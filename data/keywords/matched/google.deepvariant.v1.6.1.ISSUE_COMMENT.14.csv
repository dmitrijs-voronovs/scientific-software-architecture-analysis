id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/google/deepvariant/issues/321:1013,security,log,logs,1013,"Hi Alexey,. sure, the exact command was:. ``` sudo docker run --gpus all --cpus=25.0 -v /home/docker_input:/input -v /home/docker_output:/output google/deepvariant@sha256:fcecf5e3032245dd0b6da2c28ec0d9a9d099537af7d6df054df0e25fe4a29006 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/grch37.75.fa --reads=/input/input.bam --output_vcf=/output/output.vcf --num_shards=25 --make_examples_extra_args logging_every_n_candidates=10000 ```. I think there is nothing special regarding the used parameters. One difference to your quick start guide is the usage of ""docker run --gpus all"" instead of the deprecated nvidia-docker (see https://github.com/NVIDIA/nvidia-docker#quickstart) . I did some additional checks with ""nvidia-smi"" command, when calling is runing. It seems like it's definitly not using the GPU. Is it maybe possible that this is some driver incompatibility with our installed nvidia drivers and the one inside the docker container? I'm wondering where this 410.129.0 version in the logs is comming from. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:1013,testability,log,logs,1013,"Hi Alexey,. sure, the exact command was:. ``` sudo docker run --gpus all --cpus=25.0 -v /home/docker_input:/input -v /home/docker_output:/output google/deepvariant@sha256:fcecf5e3032245dd0b6da2c28ec0d9a9d099537af7d6df054df0e25fe4a29006 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/grch37.75.fa --reads=/input/input.bam --output_vcf=/output/output.vcf --num_shards=25 --make_examples_extra_args logging_every_n_candidates=10000 ```. I think there is nothing special regarding the used parameters. One difference to your quick start guide is the usage of ""docker run --gpus all"" instead of the deprecated nvidia-docker (see https://github.com/NVIDIA/nvidia-docker#quickstart) . I did some additional checks with ""nvidia-smi"" command, when calling is runing. It seems like it's definitly not using the GPU. Is it maybe possible that this is some driver incompatibility with our installed nvidia drivers and the one inside the docker container? I'm wondering where this 410.129.0 version in the logs is comming from. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:28,usability,command,command,28,"Hi Alexey,. sure, the exact command was:. ``` sudo docker run --gpus all --cpus=25.0 -v /home/docker_input:/input -v /home/docker_output:/output google/deepvariant@sha256:fcecf5e3032245dd0b6da2c28ec0d9a9d099537af7d6df054df0e25fe4a29006 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/grch37.75.fa --reads=/input/input.bam --output_vcf=/output/output.vcf --num_shards=25 --make_examples_extra_args logging_every_n_candidates=10000 ```. I think there is nothing special regarding the used parameters. One difference to your quick start guide is the usage of ""docker run --gpus all"" instead of the deprecated nvidia-docker (see https://github.com/NVIDIA/nvidia-docker#quickstart) . I did some additional checks with ""nvidia-smi"" command, when calling is runing. It seems like it's definitly not using the GPU. Is it maybe possible that this is some driver incompatibility with our installed nvidia drivers and the one inside the docker container? I'm wondering where this 410.129.0 version in the logs is comming from. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:108,usability,input,input,108,"Hi Alexey,. sure, the exact command was:. ``` sudo docker run --gpus all --cpus=25.0 -v /home/docker_input:/input -v /home/docker_output:/output google/deepvariant@sha256:fcecf5e3032245dd0b6da2c28ec0d9a9d099537af7d6df054df0e25fe4a29006 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/grch37.75.fa --reads=/input/input.bam --output_vcf=/output/output.vcf --num_shards=25 --make_examples_extra_args logging_every_n_candidates=10000 ```. I think there is nothing special regarding the used parameters. One difference to your quick start guide is the usage of ""docker run --gpus all"" instead of the deprecated nvidia-docker (see https://github.com/NVIDIA/nvidia-docker#quickstart) . I did some additional checks with ""nvidia-smi"" command, when calling is runing. It seems like it's definitly not using the GPU. Is it maybe possible that this is some driver incompatibility with our installed nvidia drivers and the one inside the docker container? I'm wondering where this 410.129.0 version in the logs is comming from. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:297,usability,input,input,297,"Hi Alexey,. sure, the exact command was:. ``` sudo docker run --gpus all --cpus=25.0 -v /home/docker_input:/input -v /home/docker_output:/output google/deepvariant@sha256:fcecf5e3032245dd0b6da2c28ec0d9a9d099537af7d6df054df0e25fe4a29006 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/grch37.75.fa --reads=/input/input.bam --output_vcf=/output/output.vcf --num_shards=25 --make_examples_extra_args logging_every_n_candidates=10000 ```. I think there is nothing special regarding the used parameters. One difference to your quick start guide is the usage of ""docker run --gpus all"" instead of the deprecated nvidia-docker (see https://github.com/NVIDIA/nvidia-docker#quickstart) . I did some additional checks with ""nvidia-smi"" command, when calling is runing. It seems like it's definitly not using the GPU. Is it maybe possible that this is some driver incompatibility with our installed nvidia drivers and the one inside the docker container? I'm wondering where this 410.129.0 version in the logs is comming from. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:325,usability,input,input,325,"Hi Alexey,. sure, the exact command was:. ``` sudo docker run --gpus all --cpus=25.0 -v /home/docker_input:/input -v /home/docker_output:/output google/deepvariant@sha256:fcecf5e3032245dd0b6da2c28ec0d9a9d099537af7d6df054df0e25fe4a29006 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/grch37.75.fa --reads=/input/input.bam --output_vcf=/output/output.vcf --num_shards=25 --make_examples_extra_args logging_every_n_candidates=10000 ```. I think there is nothing special regarding the used parameters. One difference to your quick start guide is the usage of ""docker run --gpus all"" instead of the deprecated nvidia-docker (see https://github.com/NVIDIA/nvidia-docker#quickstart) . I did some additional checks with ""nvidia-smi"" command, when calling is runing. It seems like it's definitly not using the GPU. Is it maybe possible that this is some driver incompatibility with our installed nvidia drivers and the one inside the docker container? I'm wondering where this 410.129.0 version in the logs is comming from. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:331,usability,input,input,331,"Hi Alexey,. sure, the exact command was:. ``` sudo docker run --gpus all --cpus=25.0 -v /home/docker_input:/input -v /home/docker_output:/output google/deepvariant@sha256:fcecf5e3032245dd0b6da2c28ec0d9a9d099537af7d6df054df0e25fe4a29006 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/grch37.75.fa --reads=/input/input.bam --output_vcf=/output/output.vcf --num_shards=25 --make_examples_extra_args logging_every_n_candidates=10000 ```. I think there is nothing special regarding the used parameters. One difference to your quick start guide is the usage of ""docker run --gpus all"" instead of the deprecated nvidia-docker (see https://github.com/NVIDIA/nvidia-docker#quickstart) . I did some additional checks with ""nvidia-smi"" command, when calling is runing. It seems like it's definitly not using the GPU. Is it maybe possible that this is some driver incompatibility with our installed nvidia drivers and the one inside the docker container? I'm wondering where this 410.129.0 version in the logs is comming from. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:553,usability,guid,guide,553,"Hi Alexey,. sure, the exact command was:. ``` sudo docker run --gpus all --cpus=25.0 -v /home/docker_input:/input -v /home/docker_output:/output google/deepvariant@sha256:fcecf5e3032245dd0b6da2c28ec0d9a9d099537af7d6df054df0e25fe4a29006 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/grch37.75.fa --reads=/input/input.bam --output_vcf=/output/output.vcf --num_shards=25 --make_examples_extra_args logging_every_n_candidates=10000 ```. I think there is nothing special regarding the used parameters. One difference to your quick start guide is the usage of ""docker run --gpus all"" instead of the deprecated nvidia-docker (see https://github.com/NVIDIA/nvidia-docker#quickstart) . I did some additional checks with ""nvidia-smi"" command, when calling is runing. It seems like it's definitly not using the GPU. Is it maybe possible that this is some driver incompatibility with our installed nvidia drivers and the one inside the docker container? I'm wondering where this 410.129.0 version in the logs is comming from. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:745,usability,command,command,745,"Hi Alexey,. sure, the exact command was:. ``` sudo docker run --gpus all --cpus=25.0 -v /home/docker_input:/input -v /home/docker_output:/output google/deepvariant@sha256:fcecf5e3032245dd0b6da2c28ec0d9a9d099537af7d6df054df0e25fe4a29006 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/grch37.75.fa --reads=/input/input.bam --output_vcf=/output/output.vcf --num_shards=25 --make_examples_extra_args logging_every_n_candidates=10000 ```. I think there is nothing special regarding the used parameters. One difference to your quick start guide is the usage of ""docker run --gpus all"" instead of the deprecated nvidia-docker (see https://github.com/NVIDIA/nvidia-docker#quickstart) . I did some additional checks with ""nvidia-smi"" command, when calling is runing. It seems like it's definitly not using the GPU. Is it maybe possible that this is some driver incompatibility with our installed nvidia drivers and the one inside the docker container? I'm wondering where this 410.129.0 version in the logs is comming from. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:376,availability,mainten,maintenance-policy,376,"Hi @segoerge . Sorry that it took a while for me to get to this again. I'm now trying to get a Debian 10 to reproduce your issue. But I'm actually stuck at getting nvidia-smi to work on Debian 10. Here is what I've done so far:. ## Get a Debian 10 machine. ```. gcloud compute instances create ""${USER}-debian-10-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image=debian-10-buster-v20200618 \. --image-project=debian-cloud \. --machine-type n1-standard-16 \. --zone ""us-west1-b"". ```. ## On the machine, install driver and docker. I did:. ```. curl https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh | bash -x. ```. to install. You can take a look at https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh to see what I did. At the end, this command failed:. ```. pichuan@pichuan-debian-10-gpu:~$ sudo docker run --gpus 1 nvidia/cuda:10.0-base nvidia-smi. docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\n\\\""\"""": unknown. ERRO[0000] error waiting for container: context canceled . ```. I saw this being reported here as well: https://github.com/NVIDIA/nvidia-container-toolkit/issues/183 . I'll need to figure this out because I can actually test DeepVariant behavior on this. If you have suggestions on how to get this work, let me know and I can proceed. Otherwise I'll take a look again later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:1170,availability,Error,Error,1170,"Hi @segoerge . Sorry that it took a while for me to get to this again. I'm now trying to get a Debian 10 to reproduce your issue. But I'm actually stuck at getting nvidia-smi to work on Debian 10. Here is what I've done so far:. ## Get a Debian 10 machine. ```. gcloud compute instances create ""${USER}-debian-10-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image=debian-10-buster-v20200618 \. --image-project=debian-cloud \. --machine-type n1-standard-16 \. --zone ""us-west1-b"". ```. ## On the machine, install driver and docker. I did:. ```. curl https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh | bash -x. ```. to install. You can take a look at https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh to see what I did. At the end, this command failed:. ```. pichuan@pichuan-debian-10-gpu:~$ sudo docker run --gpus 1 nvidia/cuda:10.0-base nvidia-smi. docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\n\\\""\"""": unknown. ERRO[0000] error waiting for container: context canceled . ```. I saw this being reported here as well: https://github.com/NVIDIA/nvidia-container-toolkit/issues/183 . I'll need to figure this out because I can actually test DeepVariant behavior on this. If you have suggestions on how to get this work, let me know and I can proceed. Otherwise I'll take a look again later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:1387,availability,error,error,1387,"Hi @segoerge . Sorry that it took a while for me to get to this again. I'm now trying to get a Debian 10 to reproduce your issue. But I'm actually stuck at getting nvidia-smi to work on Debian 10. Here is what I've done so far:. ## Get a Debian 10 machine. ```. gcloud compute instances create ""${USER}-debian-10-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image=debian-10-buster-v20200618 \. --image-project=debian-cloud \. --machine-type n1-standard-16 \. --zone ""us-west1-b"". ```. ## On the machine, install driver and docker. I did:. ```. curl https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh | bash -x. ```. to install. You can take a look at https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh to see what I did. At the end, this command failed:. ```. pichuan@pichuan-debian-10-gpu:~$ sudo docker run --gpus 1 nvidia/cuda:10.0-base nvidia-smi. docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\n\\\""\"""": unknown. ERRO[0000] error waiting for container: context canceled . ```. I saw this being reported here as well: https://github.com/NVIDIA/nvidia-container-toolkit/issues/183 . I'll need to figure this out because I can actually test DeepVariant behavior on this. If you have suggestions on how to get this work, let me know and I can proceed. Otherwise I'll take a look again later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:1477,availability,error,error,1477,"Hi @segoerge . Sorry that it took a while for me to get to this again. I'm now trying to get a Debian 10 to reproduce your issue. But I'm actually stuck at getting nvidia-smi to work on Debian 10. Here is what I've done so far:. ## Get a Debian 10 machine. ```. gcloud compute instances create ""${USER}-debian-10-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image=debian-10-buster-v20200618 \. --image-project=debian-cloud \. --machine-type n1-standard-16 \. --zone ""us-west1-b"". ```. ## On the machine, install driver and docker. I did:. ```. curl https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh | bash -x. ```. to install. You can take a look at https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh to see what I did. At the end, this command failed:. ```. pichuan@pichuan-debian-10-gpu:~$ sudo docker run --gpus 1 nvidia/cuda:10.0-base nvidia-smi. docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\n\\\""\"""": unknown. ERRO[0000] error waiting for container: context canceled . ```. I saw this being reported here as well: https://github.com/NVIDIA/nvidia-container-toolkit/issues/183 . I'll need to figure this out because I can actually test DeepVariant behavior on this. If you have suggestions on how to get this work, let me know and I can proceed. Otherwise I'll take a look again later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:1491,availability,error,error,1491,"Hi @segoerge . Sorry that it took a while for me to get to this again. I'm now trying to get a Debian 10 to reproduce your issue. But I'm actually stuck at getting nvidia-smi to work on Debian 10. Here is what I've done so far:. ## Get a Debian 10 machine. ```. gcloud compute instances create ""${USER}-debian-10-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image=debian-10-buster-v20200618 \. --image-project=debian-cloud \. --machine-type n1-standard-16 \. --zone ""us-west1-b"". ```. ## On the machine, install driver and docker. I did:. ```. curl https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh | bash -x. ```. to install. You can take a look at https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh to see what I did. At the end, this command failed:. ```. pichuan@pichuan-debian-10-gpu:~$ sudo docker run --gpus 1 nvidia/cuda:10.0-base nvidia-smi. docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\n\\\""\"""": unknown. ERRO[0000] error waiting for container: context canceled . ```. I saw this being reported here as well: https://github.com/NVIDIA/nvidia-container-toolkit/issues/183 . I'll need to figure this out because I can actually test DeepVariant behavior on this. If you have suggestions on how to get this work, let me know and I can proceed. Otherwise I'll take a look again later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:1557,availability,error,error,1557,"Hi @segoerge . Sorry that it took a while for me to get to this again. I'm now trying to get a Debian 10 to reproduce your issue. But I'm actually stuck at getting nvidia-smi to work on Debian 10. Here is what I've done so far:. ## Get a Debian 10 machine. ```. gcloud compute instances create ""${USER}-debian-10-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image=debian-10-buster-v20200618 \. --image-project=debian-cloud \. --machine-type n1-standard-16 \. --zone ""us-west1-b"". ```. ## On the machine, install driver and docker. I did:. ```. curl https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh | bash -x. ```. to install. You can take a look at https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh to see what I did. At the end, this command failed:. ```. pichuan@pichuan-debian-10-gpu:~$ sudo docker run --gpus 1 nvidia/cuda:10.0-base nvidia-smi. docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\n\\\""\"""": unknown. ERRO[0000] error waiting for container: context canceled . ```. I saw this being reported here as well: https://github.com/NVIDIA/nvidia-container-toolkit/issues/183 . I'll need to figure this out because I can actually test DeepVariant behavior on this. If you have suggestions on how to get this work, let me know and I can proceed. Otherwise I'll take a look again later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:606,deployability,instal,install,606,"Hi @segoerge . Sorry that it took a while for me to get to this again. I'm now trying to get a Debian 10 to reproduce your issue. But I'm actually stuck at getting nvidia-smi to work on Debian 10. Here is what I've done so far:. ## Get a Debian 10 machine. ```. gcloud compute instances create ""${USER}-debian-10-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image=debian-10-buster-v20200618 \. --image-project=debian-cloud \. --machine-type n1-standard-16 \. --zone ""us-west1-b"". ```. ## On the machine, install driver and docker. I did:. ```. curl https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh | bash -x. ```. to install. You can take a look at https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh to see what I did. At the end, this command failed:. ```. pichuan@pichuan-debian-10-gpu:~$ sudo docker run --gpus 1 nvidia/cuda:10.0-base nvidia-smi. docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\n\\\""\"""": unknown. ERRO[0000] error waiting for container: context canceled . ```. I saw this being reported here as well: https://github.com/NVIDIA/nvidia-container-toolkit/issues/183 . I'll need to figure this out because I can actually test DeepVariant behavior on this. If you have suggestions on how to get this work, let me know and I can proceed. Otherwise I'll take a look again later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:825,deployability,instal,install,825,"Hi @segoerge . Sorry that it took a while for me to get to this again. I'm now trying to get a Debian 10 to reproduce your issue. But I'm actually stuck at getting nvidia-smi to work on Debian 10. Here is what I've done so far:. ## Get a Debian 10 machine. ```. gcloud compute instances create ""${USER}-debian-10-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image=debian-10-buster-v20200618 \. --image-project=debian-cloud \. --machine-type n1-standard-16 \. --zone ""us-west1-b"". ```. ## On the machine, install driver and docker. I did:. ```. curl https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh | bash -x. ```. to install. You can take a look at https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh to see what I did. At the end, this command failed:. ```. pichuan@pichuan-debian-10-gpu:~$ sudo docker run --gpus 1 nvidia/cuda:10.0-base nvidia-smi. docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\n\\\""\"""": unknown. ERRO[0000] error waiting for container: context canceled . ```. I saw this being reported here as well: https://github.com/NVIDIA/nvidia-container-toolkit/issues/183 . I'll need to figure this out because I can actually test DeepVariant behavior on this. If you have suggestions on how to get this work, let me know and I can proceed. Otherwise I'll take a look again later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:1056,deployability,fail,failed,1056,"Hi @segoerge . Sorry that it took a while for me to get to this again. I'm now trying to get a Debian 10 to reproduce your issue. But I'm actually stuck at getting nvidia-smi to work on Debian 10. Here is what I've done so far:. ## Get a Debian 10 machine. ```. gcloud compute instances create ""${USER}-debian-10-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image=debian-10-buster-v20200618 \. --image-project=debian-cloud \. --machine-type n1-standard-16 \. --zone ""us-west1-b"". ```. ## On the machine, install driver and docker. I did:. ```. curl https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh | bash -x. ```. to install. You can take a look at https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh to see what I did. At the end, this command failed:. ```. pichuan@pichuan-debian-10-gpu:~$ sudo docker run --gpus 1 nvidia/cuda:10.0-base nvidia-smi. docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\n\\\""\"""": unknown. ERRO[0000] error waiting for container: context canceled . ```. I saw this being reported here as well: https://github.com/NVIDIA/nvidia-container-toolkit/issues/183 . I'll need to figure this out because I can actually test DeepVariant behavior on this. If you have suggestions on how to get this work, let me know and I can proceed. Otherwise I'll take a look again later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:1217,deployability,fail,failed,1217,"Hi @segoerge . Sorry that it took a while for me to get to this again. I'm now trying to get a Debian 10 to reproduce your issue. But I'm actually stuck at getting nvidia-smi to work on Debian 10. Here is what I've done so far:. ## Get a Debian 10 machine. ```. gcloud compute instances create ""${USER}-debian-10-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image=debian-10-buster-v20200618 \. --image-project=debian-cloud \. --machine-type n1-standard-16 \. --zone ""us-west1-b"". ```. ## On the machine, install driver and docker. I did:. ```. curl https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh | bash -x. ```. to install. You can take a look at https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh to see what I did. At the end, this command failed:. ```. pichuan@pichuan-debian-10-gpu:~$ sudo docker run --gpus 1 nvidia/cuda:10.0-base nvidia-smi. docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\n\\\""\"""": unknown. ERRO[0000] error waiting for container: context canceled . ```. I saw this being reported here as well: https://github.com/NVIDIA/nvidia-container-toolkit/issues/183 . I'll need to figure this out because I can actually test DeepVariant behavior on this. If you have suggestions on how to get this work, let me know and I can proceed. Otherwise I'll take a look again later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:1258,deployability,contain,container,1258,"Hi @segoerge . Sorry that it took a while for me to get to this again. I'm now trying to get a Debian 10 to reproduce your issue. But I'm actually stuck at getting nvidia-smi to work on Debian 10. Here is what I've done so far:. ## Get a Debian 10 machine. ```. gcloud compute instances create ""${USER}-debian-10-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image=debian-10-buster-v20200618 \. --image-project=debian-cloud \. --machine-type n1-standard-16 \. --zone ""us-west1-b"". ```. ## On the machine, install driver and docker. I did:. ```. curl https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh | bash -x. ```. to install. You can take a look at https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh to see what I did. At the end, this command failed:. ```. pichuan@pichuan-debian-10-gpu:~$ sudo docker run --gpus 1 nvidia/cuda:10.0-base nvidia-smi. docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\n\\\""\"""": unknown. ERRO[0000] error waiting for container: context canceled . ```. I saw this being reported here as well: https://github.com/NVIDIA/nvidia-container-toolkit/issues/183 . I'll need to figure this out because I can actually test DeepVariant behavior on this. If you have suggestions on how to get this work, let me know and I can proceed. Otherwise I'll take a look again later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:1306,deployability,contain,container,1306,"Hi @segoerge . Sorry that it took a while for me to get to this again. I'm now trying to get a Debian 10 to reproduce your issue. But I'm actually stuck at getting nvidia-smi to work on Debian 10. Here is what I've done so far:. ## Get a Debian 10 machine. ```. gcloud compute instances create ""${USER}-debian-10-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image=debian-10-buster-v20200618 \. --image-project=debian-cloud \. --machine-type n1-standard-16 \. --zone ""us-west1-b"". ```. ## On the machine, install driver and docker. I did:. ```. curl https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh | bash -x. ```. to install. You can take a look at https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh to see what I did. At the end, this command failed:. ```. pichuan@pichuan-debian-10-gpu:~$ sudo docker run --gpus 1 nvidia/cuda:10.0-base nvidia-smi. docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\n\\\""\"""": unknown. ERRO[0000] error waiting for container: context canceled . ```. I saw this being reported here as well: https://github.com/NVIDIA/nvidia-container-toolkit/issues/183 . I'll need to figure this out because I can actually test DeepVariant behavior on this. If you have suggestions on how to get this work, let me know and I can proceed. Otherwise I'll take a look again later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:1447,deployability,contain,container-cli,1447,"Hi @segoerge . Sorry that it took a while for me to get to this again. I'm now trying to get a Debian 10 to reproduce your issue. But I'm actually stuck at getting nvidia-smi to work on Debian 10. Here is what I've done so far:. ## Get a Debian 10 machine. ```. gcloud compute instances create ""${USER}-debian-10-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image=debian-10-buster-v20200618 \. --image-project=debian-cloud \. --machine-type n1-standard-16 \. --zone ""us-west1-b"". ```. ## On the machine, install driver and docker. I did:. ```. curl https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh | bash -x. ```. to install. You can take a look at https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh to see what I did. At the end, this command failed:. ```. pichuan@pichuan-debian-10-gpu:~$ sudo docker run --gpus 1 nvidia/cuda:10.0-base nvidia-smi. docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\n\\\""\"""": unknown. ERRO[0000] error waiting for container: context canceled . ```. I saw this being reported here as well: https://github.com/NVIDIA/nvidia-container-toolkit/issues/183 . I'll need to figure this out because I can actually test DeepVariant behavior on this. If you have suggestions on how to get this work, let me know and I can proceed. Otherwise I'll take a look again later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:1498,deployability,fail,failed,1498,"Hi @segoerge . Sorry that it took a while for me to get to this again. I'm now trying to get a Debian 10 to reproduce your issue. But I'm actually stuck at getting nvidia-smi to work on Debian 10. Here is what I've done so far:. ## Get a Debian 10 machine. ```. gcloud compute instances create ""${USER}-debian-10-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image=debian-10-buster-v20200618 \. --image-project=debian-cloud \. --machine-type n1-standard-16 \. --zone ""us-west1-b"". ```. ## On the machine, install driver and docker. I did:. ```. curl https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh | bash -x. ```. to install. You can take a look at https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh to see what I did. At the end, this command failed:. ```. pichuan@pichuan-debian-10-gpu:~$ sudo docker run --gpus 1 nvidia/cuda:10.0-base nvidia-smi. docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\n\\\""\"""": unknown. ERRO[0000] error waiting for container: context canceled . ```. I saw this being reported here as well: https://github.com/NVIDIA/nvidia-container-toolkit/issues/183 . I'll need to figure this out because I can actually test DeepVariant behavior on this. If you have suggestions on how to get this work, let me know and I can proceed. Otherwise I'll take a look again later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:1575,deployability,contain,container,1575,"Hi @segoerge . Sorry that it took a while for me to get to this again. I'm now trying to get a Debian 10 to reproduce your issue. But I'm actually stuck at getting nvidia-smi to work on Debian 10. Here is what I've done so far:. ## Get a Debian 10 machine. ```. gcloud compute instances create ""${USER}-debian-10-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image=debian-10-buster-v20200618 \. --image-project=debian-cloud \. --machine-type n1-standard-16 \. --zone ""us-west1-b"". ```. ## On the machine, install driver and docker. I did:. ```. curl https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh | bash -x. ```. to install. You can take a look at https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh to see what I did. At the end, this command failed:. ```. pichuan@pichuan-debian-10-gpu:~$ sudo docker run --gpus 1 nvidia/cuda:10.0-base nvidia-smi. docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\n\\\""\"""": unknown. ERRO[0000] error waiting for container: context canceled . ```. I saw this being reported here as well: https://github.com/NVIDIA/nvidia-container-toolkit/issues/183 . I'll need to figure this out because I can actually test DeepVariant behavior on this. If you have suggestions on how to get this work, let me know and I can proceed. Otherwise I'll take a look again later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:1683,deployability,contain,container-toolkit,1683,"Hi @segoerge . Sorry that it took a while for me to get to this again. I'm now trying to get a Debian 10 to reproduce your issue. But I'm actually stuck at getting nvidia-smi to work on Debian 10. Here is what I've done so far:. ## Get a Debian 10 machine. ```. gcloud compute instances create ""${USER}-debian-10-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image=debian-10-buster-v20200618 \. --image-project=debian-cloud \. --machine-type n1-standard-16 \. --zone ""us-west1-b"". ```. ## On the machine, install driver and docker. I did:. ```. curl https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh | bash -x. ```. to install. You can take a look at https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh to see what I did. At the end, this command failed:. ```. pichuan@pichuan-debian-10-gpu:~$ sudo docker run --gpus 1 nvidia/cuda:10.0-base nvidia-smi. docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\n\\\""\"""": unknown. ERRO[0000] error waiting for container: context canceled . ```. I saw this being reported here as well: https://github.com/NVIDIA/nvidia-container-toolkit/issues/183 . I'll need to figure this out because I can actually test DeepVariant behavior on this. If you have suggestions on how to get this work, let me know and I can proceed. Otherwise I'll take a look again later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:313,energy efficiency,gpu,gpu,313,"Hi @segoerge . Sorry that it took a while for me to get to this again. I'm now trying to get a Debian 10 to reproduce your issue. But I'm actually stuck at getting nvidia-smi to work on Debian 10. Here is what I've done so far:. ## Get a Debian 10 machine. ```. gcloud compute instances create ""${USER}-debian-10-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image=debian-10-buster-v20200618 \. --image-project=debian-cloud \. --machine-type n1-standard-16 \. --zone ""us-west1-b"". ```. ## On the machine, install driver and docker. I did:. ```. curl https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh | bash -x. ```. to install. You can take a look at https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh to see what I did. At the end, this command failed:. ```. pichuan@pichuan-debian-10-gpu:~$ sudo docker run --gpus 1 nvidia/cuda:10.0-base nvidia-smi. docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\n\\\""\"""": unknown. ERRO[0000] error waiting for container: context canceled . ```. I saw this being reported here as well: https://github.com/NVIDIA/nvidia-container-toolkit/issues/183 . I'll need to figure this out because I can actually test DeepVariant behavior on this. If you have suggestions on how to get this work, let me know and I can proceed. Otherwise I'll take a look again later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:355,energy efficiency,cloud,cloud-platform,355,"Hi @segoerge . Sorry that it took a while for me to get to this again. I'm now trying to get a Debian 10 to reproduce your issue. But I'm actually stuck at getting nvidia-smi to work on Debian 10. Here is what I've done so far:. ## Get a Debian 10 machine. ```. gcloud compute instances create ""${USER}-debian-10-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image=debian-10-buster-v20200618 \. --image-project=debian-cloud \. --machine-type n1-standard-16 \. --zone ""us-west1-b"". ```. ## On the machine, install driver and docker. I did:. ```. curl https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh | bash -x. ```. to install. You can take a look at https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh to see what I did. At the end, this command failed:. ```. pichuan@pichuan-debian-10-gpu:~$ sudo docker run --gpus 1 nvidia/cuda:10.0-base nvidia-smi. docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\n\\\""\"""": unknown. ERRO[0000] error waiting for container: context canceled . ```. I saw this being reported here as well: https://github.com/NVIDIA/nvidia-container-toolkit/issues/183 . I'll need to figure this out because I can actually test DeepVariant behavior on this. If you have suggestions on how to get this work, let me know and I can proceed. Otherwise I'll take a look again later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:519,energy efficiency,cloud,cloud,519,"Hi @segoerge . Sorry that it took a while for me to get to this again. I'm now trying to get a Debian 10 to reproduce your issue. But I'm actually stuck at getting nvidia-smi to work on Debian 10. Here is what I've done so far:. ## Get a Debian 10 machine. ```. gcloud compute instances create ""${USER}-debian-10-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image=debian-10-buster-v20200618 \. --image-project=debian-cloud \. --machine-type n1-standard-16 \. --zone ""us-west1-b"". ```. ## On the machine, install driver and docker. I did:. ```. curl https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh | bash -x. ```. to install. You can take a look at https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh to see what I did. At the end, this command failed:. ```. pichuan@pichuan-debian-10-gpu:~$ sudo docker run --gpus 1 nvidia/cuda:10.0-base nvidia-smi. docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\n\\\""\"""": unknown. ERRO[0000] error waiting for container: context canceled . ```. I saw this being reported here as well: https://github.com/NVIDIA/nvidia-container-toolkit/issues/183 . I'll need to figure this out because I can actually test DeepVariant behavior on this. If you have suggestions on how to get this work, let me know and I can proceed. Otherwise I'll take a look again later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:1096,energy efficiency,gpu,gpu,1096,"Hi @segoerge . Sorry that it took a while for me to get to this again. I'm now trying to get a Debian 10 to reproduce your issue. But I'm actually stuck at getting nvidia-smi to work on Debian 10. Here is what I've done so far:. ## Get a Debian 10 machine. ```. gcloud compute instances create ""${USER}-debian-10-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image=debian-10-buster-v20200618 \. --image-project=debian-cloud \. --machine-type n1-standard-16 \. --zone ""us-west1-b"". ```. ## On the machine, install driver and docker. I did:. ```. curl https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh | bash -x. ```. to install. You can take a look at https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh to see what I did. At the end, this command failed:. ```. pichuan@pichuan-debian-10-gpu:~$ sudo docker run --gpus 1 nvidia/cuda:10.0-base nvidia-smi. docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\n\\\""\"""": unknown. ERRO[0000] error waiting for container: context canceled . ```. I saw this being reported here as well: https://github.com/NVIDIA/nvidia-container-toolkit/issues/183 . I'll need to figure this out because I can actually test DeepVariant behavior on this. If you have suggestions on how to get this work, let me know and I can proceed. Otherwise I'll take a look again later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:1121,energy efficiency,gpu,gpus,1121,"Hi @segoerge . Sorry that it took a while for me to get to this again. I'm now trying to get a Debian 10 to reproduce your issue. But I'm actually stuck at getting nvidia-smi to work on Debian 10. Here is what I've done so far:. ## Get a Debian 10 machine. ```. gcloud compute instances create ""${USER}-debian-10-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image=debian-10-buster-v20200618 \. --image-project=debian-cloud \. --machine-type n1-standard-16 \. --zone ""us-west1-b"". ```. ## On the machine, install driver and docker. I did:. ```. curl https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh | bash -x. ```. to install. You can take a look at https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh to see what I did. At the end, this command failed:. ```. pichuan@pichuan-debian-10-gpu:~$ sudo docker run --gpus 1 nvidia/cuda:10.0-base nvidia-smi. docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\n\\\""\"""": unknown. ERRO[0000] error waiting for container: context canceled . ```. I saw this being reported here as well: https://github.com/NVIDIA/nvidia-container-toolkit/issues/183 . I'll need to figure this out because I can actually test DeepVariant behavior on this. If you have suggestions on how to get this work, let me know and I can proceed. Otherwise I'll take a look again later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:361,interoperability,platform,platform,361,"Hi @segoerge . Sorry that it took a while for me to get to this again. I'm now trying to get a Debian 10 to reproduce your issue. But I'm actually stuck at getting nvidia-smi to work on Debian 10. Here is what I've done so far:. ## Get a Debian 10 machine. ```. gcloud compute instances create ""${USER}-debian-10-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image=debian-10-buster-v20200618 \. --image-project=debian-cloud \. --machine-type n1-standard-16 \. --zone ""us-west1-b"". ```. ## On the machine, install driver and docker. I did:. ```. curl https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh | bash -x. ```. to install. You can take a look at https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh to see what I did. At the end, this command failed:. ```. pichuan@pichuan-debian-10-gpu:~$ sudo docker run --gpus 1 nvidia/cuda:10.0-base nvidia-smi. docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\n\\\""\"""": unknown. ERRO[0000] error waiting for container: context canceled . ```. I saw this being reported here as well: https://github.com/NVIDIA/nvidia-container-toolkit/issues/183 . I'll need to figure this out because I can actually test DeepVariant behavior on this. If you have suggestions on how to get this work, let me know and I can proceed. Otherwise I'll take a look again later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:546,interoperability,standard,standard-,546,"Hi @segoerge . Sorry that it took a while for me to get to this again. I'm now trying to get a Debian 10 to reproduce your issue. But I'm actually stuck at getting nvidia-smi to work on Debian 10. Here is what I've done so far:. ## Get a Debian 10 machine. ```. gcloud compute instances create ""${USER}-debian-10-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image=debian-10-buster-v20200618 \. --image-project=debian-cloud \. --machine-type n1-standard-16 \. --zone ""us-west1-b"". ```. ## On the machine, install driver and docker. I did:. ```. curl https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh | bash -x. ```. to install. You can take a look at https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh to see what I did. At the end, this command failed:. ```. pichuan@pichuan-debian-10-gpu:~$ sudo docker run --gpus 1 nvidia/cuda:10.0-base nvidia-smi. docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\n\\\""\"""": unknown. ERRO[0000] error waiting for container: context canceled . ```. I saw this being reported here as well: https://github.com/NVIDIA/nvidia-container-toolkit/issues/183 . I'll need to figure this out because I can actually test DeepVariant behavior on this. If you have suggestions on how to get this work, let me know and I can proceed. Otherwise I'll take a look again later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:313,performance,gpu,gpu,313,"Hi @segoerge . Sorry that it took a while for me to get to this again. I'm now trying to get a Debian 10 to reproduce your issue. But I'm actually stuck at getting nvidia-smi to work on Debian 10. Here is what I've done so far:. ## Get a Debian 10 machine. ```. gcloud compute instances create ""${USER}-debian-10-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image=debian-10-buster-v20200618 \. --image-project=debian-cloud \. --machine-type n1-standard-16 \. --zone ""us-west1-b"". ```. ## On the machine, install driver and docker. I did:. ```. curl https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh | bash -x. ```. to install. You can take a look at https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh to see what I did. At the end, this command failed:. ```. pichuan@pichuan-debian-10-gpu:~$ sudo docker run --gpus 1 nvidia/cuda:10.0-base nvidia-smi. docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\n\\\""\"""": unknown. ERRO[0000] error waiting for container: context canceled . ```. I saw this being reported here as well: https://github.com/NVIDIA/nvidia-container-toolkit/issues/183 . I'll need to figure this out because I can actually test DeepVariant behavior on this. If you have suggestions on how to get this work, let me know and I can proceed. Otherwise I'll take a look again later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:1096,performance,gpu,gpu,1096,"Hi @segoerge . Sorry that it took a while for me to get to this again. I'm now trying to get a Debian 10 to reproduce your issue. But I'm actually stuck at getting nvidia-smi to work on Debian 10. Here is what I've done so far:. ## Get a Debian 10 machine. ```. gcloud compute instances create ""${USER}-debian-10-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image=debian-10-buster-v20200618 \. --image-project=debian-cloud \. --machine-type n1-standard-16 \. --zone ""us-west1-b"". ```. ## On the machine, install driver and docker. I did:. ```. curl https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh | bash -x. ```. to install. You can take a look at https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh to see what I did. At the end, this command failed:. ```. pichuan@pichuan-debian-10-gpu:~$ sudo docker run --gpus 1 nvidia/cuda:10.0-base nvidia-smi. docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\n\\\""\"""": unknown. ERRO[0000] error waiting for container: context canceled . ```. I saw this being reported here as well: https://github.com/NVIDIA/nvidia-container-toolkit/issues/183 . I'll need to figure this out because I can actually test DeepVariant behavior on this. If you have suggestions on how to get this work, let me know and I can proceed. Otherwise I'll take a look again later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:1121,performance,gpu,gpus,1121,"Hi @segoerge . Sorry that it took a while for me to get to this again. I'm now trying to get a Debian 10 to reproduce your issue. But I'm actually stuck at getting nvidia-smi to work on Debian 10. Here is what I've done so far:. ## Get a Debian 10 machine. ```. gcloud compute instances create ""${USER}-debian-10-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image=debian-10-buster-v20200618 \. --image-project=debian-cloud \. --machine-type n1-standard-16 \. --zone ""us-west1-b"". ```. ## On the machine, install driver and docker. I did:. ```. curl https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh | bash -x. ```. to install. You can take a look at https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh to see what I did. At the end, this command failed:. ```. pichuan@pichuan-debian-10-gpu:~$ sudo docker run --gpus 1 nvidia/cuda:10.0-base nvidia-smi. docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\n\\\""\"""": unknown. ERRO[0000] error waiting for container: context canceled . ```. I saw this being reported here as well: https://github.com/NVIDIA/nvidia-container-toolkit/issues/183 . I'll need to figure this out because I can actually test DeepVariant behavior on this. If you have suggestions on how to get this work, let me know and I can proceed. Otherwise I'll take a look again later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:1170,performance,Error,Error,1170,"Hi @segoerge . Sorry that it took a while for me to get to this again. I'm now trying to get a Debian 10 to reproduce your issue. But I'm actually stuck at getting nvidia-smi to work on Debian 10. Here is what I've done so far:. ## Get a Debian 10 machine. ```. gcloud compute instances create ""${USER}-debian-10-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image=debian-10-buster-v20200618 \. --image-project=debian-cloud \. --machine-type n1-standard-16 \. --zone ""us-west1-b"". ```. ## On the machine, install driver and docker. I did:. ```. curl https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh | bash -x. ```. to install. You can take a look at https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh to see what I did. At the end, this command failed:. ```. pichuan@pichuan-debian-10-gpu:~$ sudo docker run --gpus 1 nvidia/cuda:10.0-base nvidia-smi. docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\n\\\""\"""": unknown. ERRO[0000] error waiting for container: context canceled . ```. I saw this being reported here as well: https://github.com/NVIDIA/nvidia-container-toolkit/issues/183 . I'll need to figure this out because I can actually test DeepVariant behavior on this. If you have suggestions on how to get this work, let me know and I can proceed. Otherwise I'll take a look again later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:1387,performance,error,error,1387,"Hi @segoerge . Sorry that it took a while for me to get to this again. I'm now trying to get a Debian 10 to reproduce your issue. But I'm actually stuck at getting nvidia-smi to work on Debian 10. Here is what I've done so far:. ## Get a Debian 10 machine. ```. gcloud compute instances create ""${USER}-debian-10-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image=debian-10-buster-v20200618 \. --image-project=debian-cloud \. --machine-type n1-standard-16 \. --zone ""us-west1-b"". ```. ## On the machine, install driver and docker. I did:. ```. curl https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh | bash -x. ```. to install. You can take a look at https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh to see what I did. At the end, this command failed:. ```. pichuan@pichuan-debian-10-gpu:~$ sudo docker run --gpus 1 nvidia/cuda:10.0-base nvidia-smi. docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\n\\\""\"""": unknown. ERRO[0000] error waiting for container: context canceled . ```. I saw this being reported here as well: https://github.com/NVIDIA/nvidia-container-toolkit/issues/183 . I'll need to figure this out because I can actually test DeepVariant behavior on this. If you have suggestions on how to get this work, let me know and I can proceed. Otherwise I'll take a look again later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:1477,performance,error,error,1477,"Hi @segoerge . Sorry that it took a while for me to get to this again. I'm now trying to get a Debian 10 to reproduce your issue. But I'm actually stuck at getting nvidia-smi to work on Debian 10. Here is what I've done so far:. ## Get a Debian 10 machine. ```. gcloud compute instances create ""${USER}-debian-10-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image=debian-10-buster-v20200618 \. --image-project=debian-cloud \. --machine-type n1-standard-16 \. --zone ""us-west1-b"". ```. ## On the machine, install driver and docker. I did:. ```. curl https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh | bash -x. ```. to install. You can take a look at https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh to see what I did. At the end, this command failed:. ```. pichuan@pichuan-debian-10-gpu:~$ sudo docker run --gpus 1 nvidia/cuda:10.0-base nvidia-smi. docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\n\\\""\"""": unknown. ERRO[0000] error waiting for container: context canceled . ```. I saw this being reported here as well: https://github.com/NVIDIA/nvidia-container-toolkit/issues/183 . I'll need to figure this out because I can actually test DeepVariant behavior on this. If you have suggestions on how to get this work, let me know and I can proceed. Otherwise I'll take a look again later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:1491,performance,error,error,1491,"Hi @segoerge . Sorry that it took a while for me to get to this again. I'm now trying to get a Debian 10 to reproduce your issue. But I'm actually stuck at getting nvidia-smi to work on Debian 10. Here is what I've done so far:. ## Get a Debian 10 machine. ```. gcloud compute instances create ""${USER}-debian-10-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image=debian-10-buster-v20200618 \. --image-project=debian-cloud \. --machine-type n1-standard-16 \. --zone ""us-west1-b"". ```. ## On the machine, install driver and docker. I did:. ```. curl https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh | bash -x. ```. to install. You can take a look at https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh to see what I did. At the end, this command failed:. ```. pichuan@pichuan-debian-10-gpu:~$ sudo docker run --gpus 1 nvidia/cuda:10.0-base nvidia-smi. docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\n\\\""\"""": unknown. ERRO[0000] error waiting for container: context canceled . ```. I saw this being reported here as well: https://github.com/NVIDIA/nvidia-container-toolkit/issues/183 . I'll need to figure this out because I can actually test DeepVariant behavior on this. If you have suggestions on how to get this work, let me know and I can proceed. Otherwise I'll take a look again later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:1557,performance,error,error,1557,"Hi @segoerge . Sorry that it took a while for me to get to this again. I'm now trying to get a Debian 10 to reproduce your issue. But I'm actually stuck at getting nvidia-smi to work on Debian 10. Here is what I've done so far:. ## Get a Debian 10 machine. ```. gcloud compute instances create ""${USER}-debian-10-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image=debian-10-buster-v20200618 \. --image-project=debian-cloud \. --machine-type n1-standard-16 \. --zone ""us-west1-b"". ```. ## On the machine, install driver and docker. I did:. ```. curl https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh | bash -x. ```. to install. You can take a look at https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh to see what I did. At the end, this command failed:. ```. pichuan@pichuan-debian-10-gpu:~$ sudo docker run --gpus 1 nvidia/cuda:10.0-base nvidia-smi. docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\n\\\""\"""": unknown. ERRO[0000] error waiting for container: context canceled . ```. I saw this being reported here as well: https://github.com/NVIDIA/nvidia-container-toolkit/issues/183 . I'll need to figure this out because I can actually test DeepVariant behavior on this. If you have suggestions on how to get this work, let me know and I can proceed. Otherwise I'll take a look again later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:376,reliability,mainten,maintenance-policy,376,"Hi @segoerge . Sorry that it took a while for me to get to this again. I'm now trying to get a Debian 10 to reproduce your issue. But I'm actually stuck at getting nvidia-smi to work on Debian 10. Here is what I've done so far:. ## Get a Debian 10 machine. ```. gcloud compute instances create ""${USER}-debian-10-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image=debian-10-buster-v20200618 \. --image-project=debian-cloud \. --machine-type n1-standard-16 \. --zone ""us-west1-b"". ```. ## On the machine, install driver and docker. I did:. ```. curl https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh | bash -x. ```. to install. You can take a look at https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh to see what I did. At the end, this command failed:. ```. pichuan@pichuan-debian-10-gpu:~$ sudo docker run --gpus 1 nvidia/cuda:10.0-base nvidia-smi. docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\n\\\""\"""": unknown. ERRO[0000] error waiting for container: context canceled . ```. I saw this being reported here as well: https://github.com/NVIDIA/nvidia-container-toolkit/issues/183 . I'll need to figure this out because I can actually test DeepVariant behavior on this. If you have suggestions on how to get this work, let me know and I can proceed. Otherwise I'll take a look again later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:1056,reliability,fail,failed,1056,"Hi @segoerge . Sorry that it took a while for me to get to this again. I'm now trying to get a Debian 10 to reproduce your issue. But I'm actually stuck at getting nvidia-smi to work on Debian 10. Here is what I've done so far:. ## Get a Debian 10 machine. ```. gcloud compute instances create ""${USER}-debian-10-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image=debian-10-buster-v20200618 \. --image-project=debian-cloud \. --machine-type n1-standard-16 \. --zone ""us-west1-b"". ```. ## On the machine, install driver and docker. I did:. ```. curl https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh | bash -x. ```. to install. You can take a look at https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh to see what I did. At the end, this command failed:. ```. pichuan@pichuan-debian-10-gpu:~$ sudo docker run --gpus 1 nvidia/cuda:10.0-base nvidia-smi. docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\n\\\""\"""": unknown. ERRO[0000] error waiting for container: context canceled . ```. I saw this being reported here as well: https://github.com/NVIDIA/nvidia-container-toolkit/issues/183 . I'll need to figure this out because I can actually test DeepVariant behavior on this. If you have suggestions on how to get this work, let me know and I can proceed. Otherwise I'll take a look again later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:1217,reliability,fail,failed,1217,"Hi @segoerge . Sorry that it took a while for me to get to this again. I'm now trying to get a Debian 10 to reproduce your issue. But I'm actually stuck at getting nvidia-smi to work on Debian 10. Here is what I've done so far:. ## Get a Debian 10 machine. ```. gcloud compute instances create ""${USER}-debian-10-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image=debian-10-buster-v20200618 \. --image-project=debian-cloud \. --machine-type n1-standard-16 \. --zone ""us-west1-b"". ```. ## On the machine, install driver and docker. I did:. ```. curl https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh | bash -x. ```. to install. You can take a look at https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh to see what I did. At the end, this command failed:. ```. pichuan@pichuan-debian-10-gpu:~$ sudo docker run --gpus 1 nvidia/cuda:10.0-base nvidia-smi. docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\n\\\""\"""": unknown. ERRO[0000] error waiting for container: context canceled . ```. I saw this being reported here as well: https://github.com/NVIDIA/nvidia-container-toolkit/issues/183 . I'll need to figure this out because I can actually test DeepVariant behavior on this. If you have suggestions on how to get this work, let me know and I can proceed. Otherwise I'll take a look again later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:1498,reliability,fail,failed,1498,"Hi @segoerge . Sorry that it took a while for me to get to this again. I'm now trying to get a Debian 10 to reproduce your issue. But I'm actually stuck at getting nvidia-smi to work on Debian 10. Here is what I've done so far:. ## Get a Debian 10 machine. ```. gcloud compute instances create ""${USER}-debian-10-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image=debian-10-buster-v20200618 \. --image-project=debian-cloud \. --machine-type n1-standard-16 \. --zone ""us-west1-b"". ```. ## On the machine, install driver and docker. I did:. ```. curl https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh | bash -x. ```. to install. You can take a look at https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh to see what I did. At the end, this command failed:. ```. pichuan@pichuan-debian-10-gpu:~$ sudo docker run --gpus 1 nvidia/cuda:10.0-base nvidia-smi. docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\n\\\""\"""": unknown. ERRO[0000] error waiting for container: context canceled . ```. I saw this being reported here as well: https://github.com/NVIDIA/nvidia-container-toolkit/issues/183 . I'll need to figure this out because I can actually test DeepVariant behavior on this. If you have suggestions on how to get this work, let me know and I can proceed. Otherwise I'll take a look again later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:1170,safety,Error,Error,1170,"Hi @segoerge . Sorry that it took a while for me to get to this again. I'm now trying to get a Debian 10 to reproduce your issue. But I'm actually stuck at getting nvidia-smi to work on Debian 10. Here is what I've done so far:. ## Get a Debian 10 machine. ```. gcloud compute instances create ""${USER}-debian-10-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image=debian-10-buster-v20200618 \. --image-project=debian-cloud \. --machine-type n1-standard-16 \. --zone ""us-west1-b"". ```. ## On the machine, install driver and docker. I did:. ```. curl https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh | bash -x. ```. to install. You can take a look at https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh to see what I did. At the end, this command failed:. ```. pichuan@pichuan-debian-10-gpu:~$ sudo docker run --gpus 1 nvidia/cuda:10.0-base nvidia-smi. docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\n\\\""\"""": unknown. ERRO[0000] error waiting for container: context canceled . ```. I saw this being reported here as well: https://github.com/NVIDIA/nvidia-container-toolkit/issues/183 . I'll need to figure this out because I can actually test DeepVariant behavior on this. If you have suggestions on how to get this work, let me know and I can proceed. Otherwise I'll take a look again later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:1387,safety,error,error,1387,"Hi @segoerge . Sorry that it took a while for me to get to this again. I'm now trying to get a Debian 10 to reproduce your issue. But I'm actually stuck at getting nvidia-smi to work on Debian 10. Here is what I've done so far:. ## Get a Debian 10 machine. ```. gcloud compute instances create ""${USER}-debian-10-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image=debian-10-buster-v20200618 \. --image-project=debian-cloud \. --machine-type n1-standard-16 \. --zone ""us-west1-b"". ```. ## On the machine, install driver and docker. I did:. ```. curl https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh | bash -x. ```. to install. You can take a look at https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh to see what I did. At the end, this command failed:. ```. pichuan@pichuan-debian-10-gpu:~$ sudo docker run --gpus 1 nvidia/cuda:10.0-base nvidia-smi. docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\n\\\""\"""": unknown. ERRO[0000] error waiting for container: context canceled . ```. I saw this being reported here as well: https://github.com/NVIDIA/nvidia-container-toolkit/issues/183 . I'll need to figure this out because I can actually test DeepVariant behavior on this. If you have suggestions on how to get this work, let me know and I can proceed. Otherwise I'll take a look again later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:1477,safety,error,error,1477,"Hi @segoerge . Sorry that it took a while for me to get to this again. I'm now trying to get a Debian 10 to reproduce your issue. But I'm actually stuck at getting nvidia-smi to work on Debian 10. Here is what I've done so far:. ## Get a Debian 10 machine. ```. gcloud compute instances create ""${USER}-debian-10-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image=debian-10-buster-v20200618 \. --image-project=debian-cloud \. --machine-type n1-standard-16 \. --zone ""us-west1-b"". ```. ## On the machine, install driver and docker. I did:. ```. curl https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh | bash -x. ```. to install. You can take a look at https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh to see what I did. At the end, this command failed:. ```. pichuan@pichuan-debian-10-gpu:~$ sudo docker run --gpus 1 nvidia/cuda:10.0-base nvidia-smi. docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\n\\\""\"""": unknown. ERRO[0000] error waiting for container: context canceled . ```. I saw this being reported here as well: https://github.com/NVIDIA/nvidia-container-toolkit/issues/183 . I'll need to figure this out because I can actually test DeepVariant behavior on this. If you have suggestions on how to get this work, let me know and I can proceed. Otherwise I'll take a look again later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:1491,safety,error,error,1491,"Hi @segoerge . Sorry that it took a while for me to get to this again. I'm now trying to get a Debian 10 to reproduce your issue. But I'm actually stuck at getting nvidia-smi to work on Debian 10. Here is what I've done so far:. ## Get a Debian 10 machine. ```. gcloud compute instances create ""${USER}-debian-10-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image=debian-10-buster-v20200618 \. --image-project=debian-cloud \. --machine-type n1-standard-16 \. --zone ""us-west1-b"". ```. ## On the machine, install driver and docker. I did:. ```. curl https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh | bash -x. ```. to install. You can take a look at https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh to see what I did. At the end, this command failed:. ```. pichuan@pichuan-debian-10-gpu:~$ sudo docker run --gpus 1 nvidia/cuda:10.0-base nvidia-smi. docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\n\\\""\"""": unknown. ERRO[0000] error waiting for container: context canceled . ```. I saw this being reported here as well: https://github.com/NVIDIA/nvidia-container-toolkit/issues/183 . I'll need to figure this out because I can actually test DeepVariant behavior on this. If you have suggestions on how to get this work, let me know and I can proceed. Otherwise I'll take a look again later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:1557,safety,error,error,1557,"Hi @segoerge . Sorry that it took a while for me to get to this again. I'm now trying to get a Debian 10 to reproduce your issue. But I'm actually stuck at getting nvidia-smi to work on Debian 10. Here is what I've done so far:. ## Get a Debian 10 machine. ```. gcloud compute instances create ""${USER}-debian-10-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image=debian-10-buster-v20200618 \. --image-project=debian-cloud \. --machine-type n1-standard-16 \. --zone ""us-west1-b"". ```. ## On the machine, install driver and docker. I did:. ```. curl https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh | bash -x. ```. to install. You can take a look at https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh to see what I did. At the end, this command failed:. ```. pichuan@pichuan-debian-10-gpu:~$ sudo docker run --gpus 1 nvidia/cuda:10.0-base nvidia-smi. docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\n\\\""\"""": unknown. ERRO[0000] error waiting for container: context canceled . ```. I saw this being reported here as well: https://github.com/NVIDIA/nvidia-container-toolkit/issues/183 . I'll need to figure this out because I can actually test DeepVariant behavior on this. If you have suggestions on how to get this work, let me know and I can proceed. Otherwise I'll take a look again later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:1766,safety,test,test,1766,"Hi @segoerge . Sorry that it took a while for me to get to this again. I'm now trying to get a Debian 10 to reproduce your issue. But I'm actually stuck at getting nvidia-smi to work on Debian 10. Here is what I've done so far:. ## Get a Debian 10 machine. ```. gcloud compute instances create ""${USER}-debian-10-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image=debian-10-buster-v20200618 \. --image-project=debian-cloud \. --machine-type n1-standard-16 \. --zone ""us-west1-b"". ```. ## On the machine, install driver and docker. I did:. ```. curl https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh | bash -x. ```. to install. You can take a look at https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh to see what I did. At the end, this command failed:. ```. pichuan@pichuan-debian-10-gpu:~$ sudo docker run --gpus 1 nvidia/cuda:10.0-base nvidia-smi. docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\n\\\""\"""": unknown. ERRO[0000] error waiting for container: context canceled . ```. I saw this being reported here as well: https://github.com/NVIDIA/nvidia-container-toolkit/issues/183 . I'll need to figure this out because I can actually test DeepVariant behavior on this. If you have suggestions on how to get this work, let me know and I can proceed. Otherwise I'll take a look again later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:388,security,polic,policy,388,"Hi @segoerge . Sorry that it took a while for me to get to this again. I'm now trying to get a Debian 10 to reproduce your issue. But I'm actually stuck at getting nvidia-smi to work on Debian 10. Here is what I've done so far:. ## Get a Debian 10 machine. ```. gcloud compute instances create ""${USER}-debian-10-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image=debian-10-buster-v20200618 \. --image-project=debian-cloud \. --machine-type n1-standard-16 \. --zone ""us-west1-b"". ```. ## On the machine, install driver and docker. I did:. ```. curl https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh | bash -x. ```. to install. You can take a look at https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh to see what I did. At the end, this command failed:. ```. pichuan@pichuan-debian-10-gpu:~$ sudo docker run --gpus 1 nvidia/cuda:10.0-base nvidia-smi. docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\n\\\""\"""": unknown. ERRO[0000] error waiting for container: context canceled . ```. I saw this being reported here as well: https://github.com/NVIDIA/nvidia-container-toolkit/issues/183 . I'll need to figure this out because I can actually test DeepVariant behavior on this. If you have suggestions on how to get this work, let me know and I can proceed. Otherwise I'll take a look again later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:1369,testability,hook,hook,1369,"Hi @segoerge . Sorry that it took a while for me to get to this again. I'm now trying to get a Debian 10 to reproduce your issue. But I'm actually stuck at getting nvidia-smi to work on Debian 10. Here is what I've done so far:. ## Get a Debian 10 machine. ```. gcloud compute instances create ""${USER}-debian-10-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image=debian-10-buster-v20200618 \. --image-project=debian-cloud \. --machine-type n1-standard-16 \. --zone ""us-west1-b"". ```. ## On the machine, install driver and docker. I did:. ```. curl https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh | bash -x. ```. to install. You can take a look at https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh to see what I did. At the end, this command failed:. ```. pichuan@pichuan-debian-10-gpu:~$ sudo docker run --gpus 1 nvidia/cuda:10.0-base nvidia-smi. docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\n\\\""\"""": unknown. ERRO[0000] error waiting for container: context canceled . ```. I saw this being reported here as well: https://github.com/NVIDIA/nvidia-container-toolkit/issues/183 . I'll need to figure this out because I can actually test DeepVariant behavior on this. If you have suggestions on how to get this work, let me know and I can proceed. Otherwise I'll take a look again later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:1401,testability,hook,hook,1401,"Hi @segoerge . Sorry that it took a while for me to get to this again. I'm now trying to get a Debian 10 to reproduce your issue. But I'm actually stuck at getting nvidia-smi to work on Debian 10. Here is what I've done so far:. ## Get a Debian 10 machine. ```. gcloud compute instances create ""${USER}-debian-10-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image=debian-10-buster-v20200618 \. --image-project=debian-cloud \. --machine-type n1-standard-16 \. --zone ""us-west1-b"". ```. ## On the machine, install driver and docker. I did:. ```. curl https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh | bash -x. ```. to install. You can take a look at https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh to see what I did. At the end, this command failed:. ```. pichuan@pichuan-debian-10-gpu:~$ sudo docker run --gpus 1 nvidia/cuda:10.0-base nvidia-smi. docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\n\\\""\"""": unknown. ERRO[0000] error waiting for container: context canceled . ```. I saw this being reported here as well: https://github.com/NVIDIA/nvidia-container-toolkit/issues/183 . I'll need to figure this out because I can actually test DeepVariant behavior on this. If you have suggestions on how to get this work, let me know and I can proceed. Otherwise I'll take a look again later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:1586,testability,context,context,1586,"Hi @segoerge . Sorry that it took a while for me to get to this again. I'm now trying to get a Debian 10 to reproduce your issue. But I'm actually stuck at getting nvidia-smi to work on Debian 10. Here is what I've done so far:. ## Get a Debian 10 machine. ```. gcloud compute instances create ""${USER}-debian-10-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image=debian-10-buster-v20200618 \. --image-project=debian-cloud \. --machine-type n1-standard-16 \. --zone ""us-west1-b"". ```. ## On the machine, install driver and docker. I did:. ```. curl https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh | bash -x. ```. to install. You can take a look at https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh to see what I did. At the end, this command failed:. ```. pichuan@pichuan-debian-10-gpu:~$ sudo docker run --gpus 1 nvidia/cuda:10.0-base nvidia-smi. docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\n\\\""\"""": unknown. ERRO[0000] error waiting for container: context canceled . ```. I saw this being reported here as well: https://github.com/NVIDIA/nvidia-container-toolkit/issues/183 . I'll need to figure this out because I can actually test DeepVariant behavior on this. If you have suggestions on how to get this work, let me know and I can proceed. Otherwise I'll take a look again later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:1766,testability,test,test,1766,"Hi @segoerge . Sorry that it took a while for me to get to this again. I'm now trying to get a Debian 10 to reproduce your issue. But I'm actually stuck at getting nvidia-smi to work on Debian 10. Here is what I've done so far:. ## Get a Debian 10 machine. ```. gcloud compute instances create ""${USER}-debian-10-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image=debian-10-buster-v20200618 \. --image-project=debian-cloud \. --machine-type n1-standard-16 \. --zone ""us-west1-b"". ```. ## On the machine, install driver and docker. I did:. ```. curl https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh | bash -x. ```. to install. You can take a look at https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh to see what I did. At the end, this command failed:. ```. pichuan@pichuan-debian-10-gpu:~$ sudo docker run --gpus 1 nvidia/cuda:10.0-base nvidia-smi. docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\n\\\""\"""": unknown. ERRO[0000] error waiting for container: context canceled . ```. I saw this being reported here as well: https://github.com/NVIDIA/nvidia-container-toolkit/issues/183 . I'll need to figure this out because I can actually test DeepVariant behavior on this. If you have suggestions on how to get this work, let me know and I can proceed. Otherwise I'll take a look again later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:297,usability,USER,USER,297,"Hi @segoerge . Sorry that it took a while for me to get to this again. I'm now trying to get a Debian 10 to reproduce your issue. But I'm actually stuck at getting nvidia-smi to work on Debian 10. Here is what I've done so far:. ## Get a Debian 10 machine. ```. gcloud compute instances create ""${USER}-debian-10-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image=debian-10-buster-v20200618 \. --image-project=debian-cloud \. --machine-type n1-standard-16 \. --zone ""us-west1-b"". ```. ## On the machine, install driver and docker. I did:. ```. curl https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh | bash -x. ```. to install. You can take a look at https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh to see what I did. At the end, this command failed:. ```. pichuan@pichuan-debian-10-gpu:~$ sudo docker run --gpus 1 nvidia/cuda:10.0-base nvidia-smi. docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\n\\\""\"""": unknown. ERRO[0000] error waiting for container: context canceled . ```. I saw this being reported here as well: https://github.com/NVIDIA/nvidia-container-toolkit/issues/183 . I'll need to figure this out because I can actually test DeepVariant behavior on this. If you have suggestions on how to get this work, let me know and I can proceed. Otherwise I'll take a look again later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:1048,usability,command,command,1048,"Hi @segoerge . Sorry that it took a while for me to get to this again. I'm now trying to get a Debian 10 to reproduce your issue. But I'm actually stuck at getting nvidia-smi to work on Debian 10. Here is what I've done so far:. ## Get a Debian 10 machine. ```. gcloud compute instances create ""${USER}-debian-10-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image=debian-10-buster-v20200618 \. --image-project=debian-cloud \. --machine-type n1-standard-16 \. --zone ""us-west1-b"". ```. ## On the machine, install driver and docker. I did:. ```. curl https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh | bash -x. ```. to install. You can take a look at https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh to see what I did. At the end, this command failed:. ```. pichuan@pichuan-debian-10-gpu:~$ sudo docker run --gpus 1 nvidia/cuda:10.0-base nvidia-smi. docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\n\\\""\"""": unknown. ERRO[0000] error waiting for container: context canceled . ```. I saw this being reported here as well: https://github.com/NVIDIA/nvidia-container-toolkit/issues/183 . I'll need to figure this out because I can actually test DeepVariant behavior on this. If you have suggestions on how to get this work, let me know and I can proceed. Otherwise I'll take a look again later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:1170,usability,Error,Error,1170,"Hi @segoerge . Sorry that it took a while for me to get to this again. I'm now trying to get a Debian 10 to reproduce your issue. But I'm actually stuck at getting nvidia-smi to work on Debian 10. Here is what I've done so far:. ## Get a Debian 10 machine. ```. gcloud compute instances create ""${USER}-debian-10-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image=debian-10-buster-v20200618 \. --image-project=debian-cloud \. --machine-type n1-standard-16 \. --zone ""us-west1-b"". ```. ## On the machine, install driver and docker. I did:. ```. curl https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh | bash -x. ```. to install. You can take a look at https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh to see what I did. At the end, this command failed:. ```. pichuan@pichuan-debian-10-gpu:~$ sudo docker run --gpus 1 nvidia/cuda:10.0-base nvidia-smi. docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\n\\\""\"""": unknown. ERRO[0000] error waiting for container: context canceled . ```. I saw this being reported here as well: https://github.com/NVIDIA/nvidia-container-toolkit/issues/183 . I'll need to figure this out because I can actually test DeepVariant behavior on this. If you have suggestions on how to get this work, let me know and I can proceed. Otherwise I'll take a look again later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:1387,usability,error,error,1387,"Hi @segoerge . Sorry that it took a while for me to get to this again. I'm now trying to get a Debian 10 to reproduce your issue. But I'm actually stuck at getting nvidia-smi to work on Debian 10. Here is what I've done so far:. ## Get a Debian 10 machine. ```. gcloud compute instances create ""${USER}-debian-10-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image=debian-10-buster-v20200618 \. --image-project=debian-cloud \. --machine-type n1-standard-16 \. --zone ""us-west1-b"". ```. ## On the machine, install driver and docker. I did:. ```. curl https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh | bash -x. ```. to install. You can take a look at https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh to see what I did. At the end, this command failed:. ```. pichuan@pichuan-debian-10-gpu:~$ sudo docker run --gpus 1 nvidia/cuda:10.0-base nvidia-smi. docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\n\\\""\"""": unknown. ERRO[0000] error waiting for container: context canceled . ```. I saw this being reported here as well: https://github.com/NVIDIA/nvidia-container-toolkit/issues/183 . I'll need to figure this out because I can actually test DeepVariant behavior on this. If you have suggestions on how to get this work, let me know and I can proceed. Otherwise I'll take a look again later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:1412,usability,statu,status,1412,"Hi @segoerge . Sorry that it took a while for me to get to this again. I'm now trying to get a Debian 10 to reproduce your issue. But I'm actually stuck at getting nvidia-smi to work on Debian 10. Here is what I've done so far:. ## Get a Debian 10 machine. ```. gcloud compute instances create ""${USER}-debian-10-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image=debian-10-buster-v20200618 \. --image-project=debian-cloud \. --machine-type n1-standard-16 \. --zone ""us-west1-b"". ```. ## On the machine, install driver and docker. I did:. ```. curl https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh | bash -x. ```. to install. You can take a look at https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh to see what I did. At the end, this command failed:. ```. pichuan@pichuan-debian-10-gpu:~$ sudo docker run --gpus 1 nvidia/cuda:10.0-base nvidia-smi. docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\n\\\""\"""": unknown. ERRO[0000] error waiting for container: context canceled . ```. I saw this being reported here as well: https://github.com/NVIDIA/nvidia-container-toolkit/issues/183 . I'll need to figure this out because I can actually test DeepVariant behavior on this. If you have suggestions on how to get this work, let me know and I can proceed. Otherwise I'll take a look again later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:1477,usability,error,error,1477,"Hi @segoerge . Sorry that it took a while for me to get to this again. I'm now trying to get a Debian 10 to reproduce your issue. But I'm actually stuck at getting nvidia-smi to work on Debian 10. Here is what I've done so far:. ## Get a Debian 10 machine. ```. gcloud compute instances create ""${USER}-debian-10-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image=debian-10-buster-v20200618 \. --image-project=debian-cloud \. --machine-type n1-standard-16 \. --zone ""us-west1-b"". ```. ## On the machine, install driver and docker. I did:. ```. curl https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh | bash -x. ```. to install. You can take a look at https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh to see what I did. At the end, this command failed:. ```. pichuan@pichuan-debian-10-gpu:~$ sudo docker run --gpus 1 nvidia/cuda:10.0-base nvidia-smi. docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\n\\\""\"""": unknown. ERRO[0000] error waiting for container: context canceled . ```. I saw this being reported here as well: https://github.com/NVIDIA/nvidia-container-toolkit/issues/183 . I'll need to figure this out because I can actually test DeepVariant behavior on this. If you have suggestions on how to get this work, let me know and I can proceed. Otherwise I'll take a look again later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:1491,usability,error,error,1491,"Hi @segoerge . Sorry that it took a while for me to get to this again. I'm now trying to get a Debian 10 to reproduce your issue. But I'm actually stuck at getting nvidia-smi to work on Debian 10. Here is what I've done so far:. ## Get a Debian 10 machine. ```. gcloud compute instances create ""${USER}-debian-10-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image=debian-10-buster-v20200618 \. --image-project=debian-cloud \. --machine-type n1-standard-16 \. --zone ""us-west1-b"". ```. ## On the machine, install driver and docker. I did:. ```. curl https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh | bash -x. ```. to install. You can take a look at https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh to see what I did. At the end, this command failed:. ```. pichuan@pichuan-debian-10-gpu:~$ sudo docker run --gpus 1 nvidia/cuda:10.0-base nvidia-smi. docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\n\\\""\"""": unknown. ERRO[0000] error waiting for container: context canceled . ```. I saw this being reported here as well: https://github.com/NVIDIA/nvidia-container-toolkit/issues/183 . I'll need to figure this out because I can actually test DeepVariant behavior on this. If you have suggestions on how to get this work, let me know and I can proceed. Otherwise I'll take a look again later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:1557,usability,error,error,1557,"Hi @segoerge . Sorry that it took a while for me to get to this again. I'm now trying to get a Debian 10 to reproduce your issue. But I'm actually stuck at getting nvidia-smi to work on Debian 10. Here is what I've done so far:. ## Get a Debian 10 machine. ```. gcloud compute instances create ""${USER}-debian-10-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image=debian-10-buster-v20200618 \. --image-project=debian-cloud \. --machine-type n1-standard-16 \. --zone ""us-west1-b"". ```. ## On the machine, install driver and docker. I did:. ```. curl https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh | bash -x. ```. to install. You can take a look at https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh to see what I did. At the end, this command failed:. ```. pichuan@pichuan-debian-10-gpu:~$ sudo docker run --gpus 1 nvidia/cuda:10.0-base nvidia-smi. docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\n\\\""\"""": unknown. ERRO[0000] error waiting for container: context canceled . ```. I saw this being reported here as well: https://github.com/NVIDIA/nvidia-container-toolkit/issues/183 . I'll need to figure this out because I can actually test DeepVariant behavior on this. If you have suggestions on how to get this work, let me know and I can proceed. Otherwise I'll take a look again later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:1594,usability,cancel,canceled,1594,"Hi @segoerge . Sorry that it took a while for me to get to this again. I'm now trying to get a Debian 10 to reproduce your issue. But I'm actually stuck at getting nvidia-smi to work on Debian 10. Here is what I've done so far:. ## Get a Debian 10 machine. ```. gcloud compute instances create ""${USER}-debian-10-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image=debian-10-buster-v20200618 \. --image-project=debian-cloud \. --machine-type n1-standard-16 \. --zone ""us-west1-b"". ```. ## On the machine, install driver and docker. I did:. ```. curl https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh | bash -x. ```. to install. You can take a look at https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh to see what I did. At the end, this command failed:. ```. pichuan@pichuan-debian-10-gpu:~$ sudo docker run --gpus 1 nvidia/cuda:10.0-base nvidia-smi. docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\n\\\""\"""": unknown. ERRO[0000] error waiting for container: context canceled . ```. I saw this being reported here as well: https://github.com/NVIDIA/nvidia-container-toolkit/issues/183 . I'll need to figure this out because I can actually test DeepVariant behavior on this. If you have suggestions on how to get this work, let me know and I can proceed. Otherwise I'll take a look again later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:1693,usability,tool,toolkit,1693,"Hi @segoerge . Sorry that it took a while for me to get to this again. I'm now trying to get a Debian 10 to reproduce your issue. But I'm actually stuck at getting nvidia-smi to work on Debian 10. Here is what I've done so far:. ## Get a Debian 10 machine. ```. gcloud compute instances create ""${USER}-debian-10-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image=debian-10-buster-v20200618 \. --image-project=debian-cloud \. --machine-type n1-standard-16 \. --zone ""us-west1-b"". ```. ## On the machine, install driver and docker. I did:. ```. curl https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh | bash -x. ```. to install. You can take a look at https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh to see what I did. At the end, this command failed:. ```. pichuan@pichuan-debian-10-gpu:~$ sudo docker run --gpus 1 nvidia/cuda:10.0-base nvidia-smi. docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\n\\\""\"""": unknown. ERRO[0000] error waiting for container: context canceled . ```. I saw this being reported here as well: https://github.com/NVIDIA/nvidia-container-toolkit/issues/183 . I'll need to figure this out because I can actually test DeepVariant behavior on this. If you have suggestions on how to get this work, let me know and I can proceed. Otherwise I'll take a look again later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:1783,usability,behavi,behavior,1783,"Hi @segoerge . Sorry that it took a while for me to get to this again. I'm now trying to get a Debian 10 to reproduce your issue. But I'm actually stuck at getting nvidia-smi to work on Debian 10. Here is what I've done so far:. ## Get a Debian 10 machine. ```. gcloud compute instances create ""${USER}-debian-10-gpu"" \. --scopes ""compute-rw,storage-full,cloud-platform"" \. --maintenance-policy ""TERMINATE"" \. --accelerator=type=nvidia-tesla-p100,count=1 \. --image=debian-10-buster-v20200618 \. --image-project=debian-cloud \. --machine-type n1-standard-16 \. --zone ""us-west1-b"". ```. ## On the machine, install driver and docker. I did:. ```. curl https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh | bash -x. ```. to install. You can take a look at https://gist.githubusercontent.com/pichuan/c538f04f08cd367c6ea2ad2df7be4de0/raw/a3c8dd11b5365dfa39351265dd53d7d986b84d8b/debian10_install_nvidia_docker.sh to see what I did. At the end, this command failed:. ```. pichuan@pichuan-debian-10-gpu:~$ sudo docker run --gpus 1 nvidia/cuda:10.0-base nvidia-smi. docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request\\\\n\\\""\"""": unknown. ERRO[0000] error waiting for container: context canceled . ```. I saw this being reported here as well: https://github.com/NVIDIA/nvidia-container-toolkit/issues/183 . I'll need to figure this out because I can actually test DeepVariant behavior on this. If you have suggestions on how to get this work, let me know and I can proceed. Otherwise I'll take a look again later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:517,availability,down,downgrade,517,"Hi @pichuan,. sorry I was on vacation. Mhm strange I've checked your test command on my Debian 10 machine and it's showing the container nvidia-smi output without any problems. . I've checked your script. Did you install any NVIDIA drivers before CUDA? If I remember right, I did something like ""nivida-detect"" and installed the proposed package. What is your version of nvidia-container-toolkit (mine is 1.1.2-1) ? I saw in your mentioned issue that people there are using a newer one. So maybe it is worth a try to downgrade. You can also try something like ""sudo modprobe nvidia-uvm"". I have had this issue with another tool that wanted to use the GPU. Somehow the card was not ready and this fixed my problem. I hope this helps. I'm quite new in the GPU world. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:127,deployability,contain,container,127,"Hi @pichuan,. sorry I was on vacation. Mhm strange I've checked your test command on my Debian 10 machine and it's showing the container nvidia-smi output without any problems. . I've checked your script. Did you install any NVIDIA drivers before CUDA? If I remember right, I did something like ""nivida-detect"" and installed the proposed package. What is your version of nvidia-container-toolkit (mine is 1.1.2-1) ? I saw in your mentioned issue that people there are using a newer one. So maybe it is worth a try to downgrade. You can also try something like ""sudo modprobe nvidia-uvm"". I have had this issue with another tool that wanted to use the GPU. Somehow the card was not ready and this fixed my problem. I hope this helps. I'm quite new in the GPU world. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:213,deployability,instal,install,213,"Hi @pichuan,. sorry I was on vacation. Mhm strange I've checked your test command on my Debian 10 machine and it's showing the container nvidia-smi output without any problems. . I've checked your script. Did you install any NVIDIA drivers before CUDA? If I remember right, I did something like ""nivida-detect"" and installed the proposed package. What is your version of nvidia-container-toolkit (mine is 1.1.2-1) ? I saw in your mentioned issue that people there are using a newer one. So maybe it is worth a try to downgrade. You can also try something like ""sudo modprobe nvidia-uvm"". I have had this issue with another tool that wanted to use the GPU. Somehow the card was not ready and this fixed my problem. I hope this helps. I'm quite new in the GPU world. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:315,deployability,instal,installed,315,"Hi @pichuan,. sorry I was on vacation. Mhm strange I've checked your test command on my Debian 10 machine and it's showing the container nvidia-smi output without any problems. . I've checked your script. Did you install any NVIDIA drivers before CUDA? If I remember right, I did something like ""nivida-detect"" and installed the proposed package. What is your version of nvidia-container-toolkit (mine is 1.1.2-1) ? I saw in your mentioned issue that people there are using a newer one. So maybe it is worth a try to downgrade. You can also try something like ""sudo modprobe nvidia-uvm"". I have had this issue with another tool that wanted to use the GPU. Somehow the card was not ready and this fixed my problem. I hope this helps. I'm quite new in the GPU world. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:360,deployability,version,version,360,"Hi @pichuan,. sorry I was on vacation. Mhm strange I've checked your test command on my Debian 10 machine and it's showing the container nvidia-smi output without any problems. . I've checked your script. Did you install any NVIDIA drivers before CUDA? If I remember right, I did something like ""nivida-detect"" and installed the proposed package. What is your version of nvidia-container-toolkit (mine is 1.1.2-1) ? I saw in your mentioned issue that people there are using a newer one. So maybe it is worth a try to downgrade. You can also try something like ""sudo modprobe nvidia-uvm"". I have had this issue with another tool that wanted to use the GPU. Somehow the card was not ready and this fixed my problem. I hope this helps. I'm quite new in the GPU world. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:378,deployability,contain,container-toolkit,378,"Hi @pichuan,. sorry I was on vacation. Mhm strange I've checked your test command on my Debian 10 machine and it's showing the container nvidia-smi output without any problems. . I've checked your script. Did you install any NVIDIA drivers before CUDA? If I remember right, I did something like ""nivida-detect"" and installed the proposed package. What is your version of nvidia-container-toolkit (mine is 1.1.2-1) ? I saw in your mentioned issue that people there are using a newer one. So maybe it is worth a try to downgrade. You can also try something like ""sudo modprobe nvidia-uvm"". I have had this issue with another tool that wanted to use the GPU. Somehow the card was not ready and this fixed my problem. I hope this helps. I'm quite new in the GPU world. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:651,energy efficiency,GPU,GPU,651,"Hi @pichuan,. sorry I was on vacation. Mhm strange I've checked your test command on my Debian 10 machine and it's showing the container nvidia-smi output without any problems. . I've checked your script. Did you install any NVIDIA drivers before CUDA? If I remember right, I did something like ""nivida-detect"" and installed the proposed package. What is your version of nvidia-container-toolkit (mine is 1.1.2-1) ? I saw in your mentioned issue that people there are using a newer one. So maybe it is worth a try to downgrade. You can also try something like ""sudo modprobe nvidia-uvm"". I have had this issue with another tool that wanted to use the GPU. Somehow the card was not ready and this fixed my problem. I hope this helps. I'm quite new in the GPU world. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:754,energy efficiency,GPU,GPU,754,"Hi @pichuan,. sorry I was on vacation. Mhm strange I've checked your test command on my Debian 10 machine and it's showing the container nvidia-smi output without any problems. . I've checked your script. Did you install any NVIDIA drivers before CUDA? If I remember right, I did something like ""nivida-detect"" and installed the proposed package. What is your version of nvidia-container-toolkit (mine is 1.1.2-1) ? I saw in your mentioned issue that people there are using a newer one. So maybe it is worth a try to downgrade. You can also try something like ""sudo modprobe nvidia-uvm"". I have had this issue with another tool that wanted to use the GPU. Somehow the card was not ready and this fixed my problem. I hope this helps. I'm quite new in the GPU world. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:360,integrability,version,version,360,"Hi @pichuan,. sorry I was on vacation. Mhm strange I've checked your test command on my Debian 10 machine and it's showing the container nvidia-smi output without any problems. . I've checked your script. Did you install any NVIDIA drivers before CUDA? If I remember right, I did something like ""nivida-detect"" and installed the proposed package. What is your version of nvidia-container-toolkit (mine is 1.1.2-1) ? I saw in your mentioned issue that people there are using a newer one. So maybe it is worth a try to downgrade. You can also try something like ""sudo modprobe nvidia-uvm"". I have had this issue with another tool that wanted to use the GPU. Somehow the card was not ready and this fixed my problem. I hope this helps. I'm quite new in the GPU world. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:338,modifiability,pac,package,338,"Hi @pichuan,. sorry I was on vacation. Mhm strange I've checked your test command on my Debian 10 machine and it's showing the container nvidia-smi output without any problems. . I've checked your script. Did you install any NVIDIA drivers before CUDA? If I remember right, I did something like ""nivida-detect"" and installed the proposed package. What is your version of nvidia-container-toolkit (mine is 1.1.2-1) ? I saw in your mentioned issue that people there are using a newer one. So maybe it is worth a try to downgrade. You can also try something like ""sudo modprobe nvidia-uvm"". I have had this issue with another tool that wanted to use the GPU. Somehow the card was not ready and this fixed my problem. I hope this helps. I'm quite new in the GPU world. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:360,modifiability,version,version,360,"Hi @pichuan,. sorry I was on vacation. Mhm strange I've checked your test command on my Debian 10 machine and it's showing the container nvidia-smi output without any problems. . I've checked your script. Did you install any NVIDIA drivers before CUDA? If I remember right, I did something like ""nivida-detect"" and installed the proposed package. What is your version of nvidia-container-toolkit (mine is 1.1.2-1) ? I saw in your mentioned issue that people there are using a newer one. So maybe it is worth a try to downgrade. You can also try something like ""sudo modprobe nvidia-uvm"". I have had this issue with another tool that wanted to use the GPU. Somehow the card was not ready and this fixed my problem. I hope this helps. I'm quite new in the GPU world. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:651,performance,GPU,GPU,651,"Hi @pichuan,. sorry I was on vacation. Mhm strange I've checked your test command on my Debian 10 machine and it's showing the container nvidia-smi output without any problems. . I've checked your script. Did you install any NVIDIA drivers before CUDA? If I remember right, I did something like ""nivida-detect"" and installed the proposed package. What is your version of nvidia-container-toolkit (mine is 1.1.2-1) ? I saw in your mentioned issue that people there are using a newer one. So maybe it is worth a try to downgrade. You can also try something like ""sudo modprobe nvidia-uvm"". I have had this issue with another tool that wanted to use the GPU. Somehow the card was not ready and this fixed my problem. I hope this helps. I'm quite new in the GPU world. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:754,performance,GPU,GPU,754,"Hi @pichuan,. sorry I was on vacation. Mhm strange I've checked your test command on my Debian 10 machine and it's showing the container nvidia-smi output without any problems. . I've checked your script. Did you install any NVIDIA drivers before CUDA? If I remember right, I did something like ""nivida-detect"" and installed the proposed package. What is your version of nvidia-container-toolkit (mine is 1.1.2-1) ? I saw in your mentioned issue that people there are using a newer one. So maybe it is worth a try to downgrade. You can also try something like ""sudo modprobe nvidia-uvm"". I have had this issue with another tool that wanted to use the GPU. Somehow the card was not ready and this fixed my problem. I hope this helps. I'm quite new in the GPU world. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:69,safety,test,test,69,"Hi @pichuan,. sorry I was on vacation. Mhm strange I've checked your test command on my Debian 10 machine and it's showing the container nvidia-smi output without any problems. . I've checked your script. Did you install any NVIDIA drivers before CUDA? If I remember right, I did something like ""nivida-detect"" and installed the proposed package. What is your version of nvidia-container-toolkit (mine is 1.1.2-1) ? I saw in your mentioned issue that people there are using a newer one. So maybe it is worth a try to downgrade. You can also try something like ""sudo modprobe nvidia-uvm"". I have had this issue with another tool that wanted to use the GPU. Somehow the card was not ready and this fixed my problem. I hope this helps. I'm quite new in the GPU world. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:258,safety,reme,remember,258,"Hi @pichuan,. sorry I was on vacation. Mhm strange I've checked your test command on my Debian 10 machine and it's showing the container nvidia-smi output without any problems. . I've checked your script. Did you install any NVIDIA drivers before CUDA? If I remember right, I did something like ""nivida-detect"" and installed the proposed package. What is your version of nvidia-container-toolkit (mine is 1.1.2-1) ? I saw in your mentioned issue that people there are using a newer one. So maybe it is worth a try to downgrade. You can also try something like ""sudo modprobe nvidia-uvm"". I have had this issue with another tool that wanted to use the GPU. Somehow the card was not ready and this fixed my problem. I hope this helps. I'm quite new in the GPU world. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:303,safety,detect,detect,303,"Hi @pichuan,. sorry I was on vacation. Mhm strange I've checked your test command on my Debian 10 machine and it's showing the container nvidia-smi output without any problems. . I've checked your script. Did you install any NVIDIA drivers before CUDA? If I remember right, I did something like ""nivida-detect"" and installed the proposed package. What is your version of nvidia-container-toolkit (mine is 1.1.2-1) ? I saw in your mentioned issue that people there are using a newer one. So maybe it is worth a try to downgrade. You can also try something like ""sudo modprobe nvidia-uvm"". I have had this issue with another tool that wanted to use the GPU. Somehow the card was not ready and this fixed my problem. I hope this helps. I'm quite new in the GPU world. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:303,security,detect,detect,303,"Hi @pichuan,. sorry I was on vacation. Mhm strange I've checked your test command on my Debian 10 machine and it's showing the container nvidia-smi output without any problems. . I've checked your script. Did you install any NVIDIA drivers before CUDA? If I remember right, I did something like ""nivida-detect"" and installed the proposed package. What is your version of nvidia-container-toolkit (mine is 1.1.2-1) ? I saw in your mentioned issue that people there are using a newer one. So maybe it is worth a try to downgrade. You can also try something like ""sudo modprobe nvidia-uvm"". I have had this issue with another tool that wanted to use the GPU. Somehow the card was not ready and this fixed my problem. I hope this helps. I'm quite new in the GPU world. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:69,testability,test,test,69,"Hi @pichuan,. sorry I was on vacation. Mhm strange I've checked your test command on my Debian 10 machine and it's showing the container nvidia-smi output without any problems. . I've checked your script. Did you install any NVIDIA drivers before CUDA? If I remember right, I did something like ""nivida-detect"" and installed the proposed package. What is your version of nvidia-container-toolkit (mine is 1.1.2-1) ? I saw in your mentioned issue that people there are using a newer one. So maybe it is worth a try to downgrade. You can also try something like ""sudo modprobe nvidia-uvm"". I have had this issue with another tool that wanted to use the GPU. Somehow the card was not ready and this fixed my problem. I hope this helps. I'm quite new in the GPU world. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:74,usability,command,command,74,"Hi @pichuan,. sorry I was on vacation. Mhm strange I've checked your test command on my Debian 10 machine and it's showing the container nvidia-smi output without any problems. . I've checked your script. Did you install any NVIDIA drivers before CUDA? If I remember right, I did something like ""nivida-detect"" and installed the proposed package. What is your version of nvidia-container-toolkit (mine is 1.1.2-1) ? I saw in your mentioned issue that people there are using a newer one. So maybe it is worth a try to downgrade. You can also try something like ""sudo modprobe nvidia-uvm"". I have had this issue with another tool that wanted to use the GPU. Somehow the card was not ready and this fixed my problem. I hope this helps. I'm quite new in the GPU world. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:388,usability,tool,toolkit,388,"Hi @pichuan,. sorry I was on vacation. Mhm strange I've checked your test command on my Debian 10 machine and it's showing the container nvidia-smi output without any problems. . I've checked your script. Did you install any NVIDIA drivers before CUDA? If I remember right, I did something like ""nivida-detect"" and installed the proposed package. What is your version of nvidia-container-toolkit (mine is 1.1.2-1) ? I saw in your mentioned issue that people there are using a newer one. So maybe it is worth a try to downgrade. You can also try something like ""sudo modprobe nvidia-uvm"". I have had this issue with another tool that wanted to use the GPU. Somehow the card was not ready and this fixed my problem. I hope this helps. I'm quite new in the GPU world. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:623,usability,tool,tool,623,"Hi @pichuan,. sorry I was on vacation. Mhm strange I've checked your test command on my Debian 10 machine and it's showing the container nvidia-smi output without any problems. . I've checked your script. Did you install any NVIDIA drivers before CUDA? If I remember right, I did something like ""nivida-detect"" and installed the proposed package. What is your version of nvidia-container-toolkit (mine is 1.1.2-1) ? I saw in your mentioned issue that people there are using a newer one. So maybe it is worth a try to downgrade. You can also try something like ""sudo modprobe nvidia-uvm"". I have had this issue with another tool that wanted to use the GPU. Somehow the card was not ready and this fixed my problem. I hope this helps. I'm quite new in the GPU world. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:726,usability,help,helps,726,"Hi @pichuan,. sorry I was on vacation. Mhm strange I've checked your test command on my Debian 10 machine and it's showing the container nvidia-smi output without any problems. . I've checked your script. Did you install any NVIDIA drivers before CUDA? If I remember right, I did something like ""nivida-detect"" and installed the proposed package. What is your version of nvidia-container-toolkit (mine is 1.1.2-1) ? I saw in your mentioned issue that people there are using a newer one. So maybe it is worth a try to downgrade. You can also try something like ""sudo modprobe nvidia-uvm"". I have had this issue with another tool that wanted to use the GPU. Somehow the card was not ready and this fixed my problem. I hope this helps. I'm quite new in the GPU world. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:261,availability,error,error,261,"Hi @pichuan,. like I mentioned before, I was also working with another tool that can make use of the GPU (SpliceAI) and I did some configurations and package installations to make this tool runable. Somehow this also fixed my problems with DeepVariant now. The error is gone and call_variants stage went through in 37 min using our GPU. Maybe there was something wrong with the CUDA drivers... I don't know. I'm very sorry for the inconveniences! Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:131,deployability,configurat,configurations,131,"Hi @pichuan,. like I mentioned before, I was also working with another tool that can make use of the GPU (SpliceAI) and I did some configurations and package installations to make this tool runable. Somehow this also fixed my problems with DeepVariant now. The error is gone and call_variants stage went through in 37 min using our GPU. Maybe there was something wrong with the CUDA drivers... I don't know. I'm very sorry for the inconveniences! Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:158,deployability,instal,installations,158,"Hi @pichuan,. like I mentioned before, I was also working with another tool that can make use of the GPU (SpliceAI) and I did some configurations and package installations to make this tool runable. Somehow this also fixed my problems with DeepVariant now. The error is gone and call_variants stage went through in 37 min using our GPU. Maybe there was something wrong with the CUDA drivers... I don't know. I'm very sorry for the inconveniences! Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:293,deployability,stage,stage,293,"Hi @pichuan,. like I mentioned before, I was also working with another tool that can make use of the GPU (SpliceAI) and I did some configurations and package installations to make this tool runable. Somehow this also fixed my problems with DeepVariant now. The error is gone and call_variants stage went through in 37 min using our GPU. Maybe there was something wrong with the CUDA drivers... I don't know. I'm very sorry for the inconveniences! Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:101,energy efficiency,GPU,GPU,101,"Hi @pichuan,. like I mentioned before, I was also working with another tool that can make use of the GPU (SpliceAI) and I did some configurations and package installations to make this tool runable. Somehow this also fixed my problems with DeepVariant now. The error is gone and call_variants stage went through in 37 min using our GPU. Maybe there was something wrong with the CUDA drivers... I don't know. I'm very sorry for the inconveniences! Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:332,energy efficiency,GPU,GPU,332,"Hi @pichuan,. like I mentioned before, I was also working with another tool that can make use of the GPU (SpliceAI) and I did some configurations and package installations to make this tool runable. Somehow this also fixed my problems with DeepVariant now. The error is gone and call_variants stage went through in 37 min using our GPU. Maybe there was something wrong with the CUDA drivers... I don't know. I'm very sorry for the inconveniences! Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:131,integrability,configur,configurations,131,"Hi @pichuan,. like I mentioned before, I was also working with another tool that can make use of the GPU (SpliceAI) and I did some configurations and package installations to make this tool runable. Somehow this also fixed my problems with DeepVariant now. The error is gone and call_variants stage went through in 37 min using our GPU. Maybe there was something wrong with the CUDA drivers... I don't know. I'm very sorry for the inconveniences! Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:131,modifiability,configur,configurations,131,"Hi @pichuan,. like I mentioned before, I was also working with another tool that can make use of the GPU (SpliceAI) and I did some configurations and package installations to make this tool runable. Somehow this also fixed my problems with DeepVariant now. The error is gone and call_variants stage went through in 37 min using our GPU. Maybe there was something wrong with the CUDA drivers... I don't know. I'm very sorry for the inconveniences! Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:150,modifiability,pac,package,150,"Hi @pichuan,. like I mentioned before, I was also working with another tool that can make use of the GPU (SpliceAI) and I did some configurations and package installations to make this tool runable. Somehow this also fixed my problems with DeepVariant now. The error is gone and call_variants stage went through in 37 min using our GPU. Maybe there was something wrong with the CUDA drivers... I don't know. I'm very sorry for the inconveniences! Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:101,performance,GPU,GPU,101,"Hi @pichuan,. like I mentioned before, I was also working with another tool that can make use of the GPU (SpliceAI) and I did some configurations and package installations to make this tool runable. Somehow this also fixed my problems with DeepVariant now. The error is gone and call_variants stage went through in 37 min using our GPU. Maybe there was something wrong with the CUDA drivers... I don't know. I'm very sorry for the inconveniences! Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:261,performance,error,error,261,"Hi @pichuan,. like I mentioned before, I was also working with another tool that can make use of the GPU (SpliceAI) and I did some configurations and package installations to make this tool runable. Somehow this also fixed my problems with DeepVariant now. The error is gone and call_variants stage went through in 37 min using our GPU. Maybe there was something wrong with the CUDA drivers... I don't know. I'm very sorry for the inconveniences! Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:332,performance,GPU,GPU,332,"Hi @pichuan,. like I mentioned before, I was also working with another tool that can make use of the GPU (SpliceAI) and I did some configurations and package installations to make this tool runable. Somehow this also fixed my problems with DeepVariant now. The error is gone and call_variants stage went through in 37 min using our GPU. Maybe there was something wrong with the CUDA drivers... I don't know. I'm very sorry for the inconveniences! Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:261,safety,error,error,261,"Hi @pichuan,. like I mentioned before, I was also working with another tool that can make use of the GPU (SpliceAI) and I did some configurations and package installations to make this tool runable. Somehow this also fixed my problems with DeepVariant now. The error is gone and call_variants stage went through in 37 min using our GPU. Maybe there was something wrong with the CUDA drivers... I don't know. I'm very sorry for the inconveniences! Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:131,security,configur,configurations,131,"Hi @pichuan,. like I mentioned before, I was also working with another tool that can make use of the GPU (SpliceAI) and I did some configurations and package installations to make this tool runable. Somehow this also fixed my problems with DeepVariant now. The error is gone and call_variants stage went through in 37 min using our GPU. Maybe there was something wrong with the CUDA drivers... I don't know. I'm very sorry for the inconveniences! Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:71,usability,tool,tool,71,"Hi @pichuan,. like I mentioned before, I was also working with another tool that can make use of the GPU (SpliceAI) and I did some configurations and package installations to make this tool runable. Somehow this also fixed my problems with DeepVariant now. The error is gone and call_variants stage went through in 37 min using our GPU. Maybe there was something wrong with the CUDA drivers... I don't know. I'm very sorry for the inconveniences! Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:185,usability,tool,tool,185,"Hi @pichuan,. like I mentioned before, I was also working with another tool that can make use of the GPU (SpliceAI) and I did some configurations and package installations to make this tool runable. Somehow this also fixed my problems with DeepVariant now. The error is gone and call_variants stage went through in 37 min using our GPU. Maybe there was something wrong with the CUDA drivers... I don't know. I'm very sorry for the inconveniences! Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/321:261,usability,error,error,261,"Hi @pichuan,. like I mentioned before, I was also working with another tool that can make use of the GPU (SpliceAI) and I did some configurations and package installations to make this tool runable. Somehow this also fixed my problems with DeepVariant now. The error is gone and call_variants stage went through in 37 min using our GPU. Maybe there was something wrong with the CUDA drivers... I don't know. I'm very sorry for the inconveniences! Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/321
https://github.com/google/deepvariant/issues/323:396,performance,network,network,396,"Hi @williamrowell, there is no flag for changing which contigs are excluded, but you could modify `exclude_contigs.py` and rebuild the Docker image / binaries. As for the MT, one of the main reasons is that MT variants can be heteroplasmic, and in these cases variant calling resembles somatic calling. It's unclear that we would be able to generate candidates well for such variants or that the network would be to handle them correctly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/323
https://github.com/google/deepvariant/issues/323:91,security,modif,modify,91,"Hi @williamrowell, there is no flag for changing which contigs are excluded, but you could modify `exclude_contigs.py` and rebuild the Docker image / binaries. As for the MT, one of the main reasons is that MT variants can be heteroplasmic, and in these cases variant calling resembles somatic calling. It's unclear that we would be able to generate candidates well for such variants or that the network would be to handle them correctly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/323
https://github.com/google/deepvariant/issues/323:396,security,network,network,396,"Hi @williamrowell, there is no flag for changing which contigs are excluded, but you could modify `exclude_contigs.py` and rebuild the Docker image / binaries. As for the MT, one of the main reasons is that MT variants can be heteroplasmic, and in these cases variant calling resembles somatic calling. It's unclear that we would be able to generate candidates well for such variants or that the network would be to handle them correctly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/323
https://github.com/google/deepvariant/issues/323:20,usability,close,close,20,"@williamrowell I'll close this issue, but feel free to reopen if there are any followup questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/323
https://github.com/google/deepvariant/issues/324:103,deployability,stage,stage,103,"Hi @leorippel, we don't recommend running VQSR on DeepVariant callsets. As for the truth sets, at what stage do you wish to use the truth sets for calling? Truth sets are required for training the model, but not needed needed for running inference.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/324
https://github.com/google/deepvariant/issues/324:197,energy efficiency,model,model,197,"Hi @leorippel, we don't recommend running VQSR on DeepVariant callsets. As for the truth sets, at what stage do you wish to use the truth sets for calling? Truth sets are required for training the model, but not needed needed for running inference.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/324
https://github.com/google/deepvariant/issues/324:197,security,model,model,197,"Hi @leorippel, we don't recommend running VQSR on DeepVariant callsets. As for the truth sets, at what stage do you wish to use the truth sets for calling? Truth sets are required for training the model, but not needed needed for running inference.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/324
https://github.com/google/deepvariant/issues/324:50,energy efficiency,model,models,50,"Hi,. I see. Yes, I was willing to try on training models. So, those models trained with truth sets are later used on inference or goes as input a single run_deepvariant run? . Cheers.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/324
https://github.com/google/deepvariant/issues/324:68,energy efficiency,model,models,68,"Hi,. I see. Yes, I was willing to try on training models. So, those models trained with truth sets are later used on inference or goes as input a single run_deepvariant run? . Cheers.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/324
https://github.com/google/deepvariant/issues/324:138,safety,input,input,138,"Hi,. I see. Yes, I was willing to try on training models. So, those models trained with truth sets are later used on inference or goes as input a single run_deepvariant run? . Cheers.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/324
https://github.com/google/deepvariant/issues/324:50,security,model,models,50,"Hi,. I see. Yes, I was willing to try on training models. So, those models trained with truth sets are later used on inference or goes as input a single run_deepvariant run? . Cheers.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/324
https://github.com/google/deepvariant/issues/324:68,security,model,models,68,"Hi,. I see. Yes, I was willing to try on training models. So, those models trained with truth sets are later used on inference or goes as input a single run_deepvariant run? . Cheers.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/324
https://github.com/google/deepvariant/issues/324:138,usability,input,input,138,"Hi,. I see. Yes, I was willing to try on training models. So, those models trained with truth sets are later used on inference or goes as input a single run_deepvariant run? . Cheers.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/324
https://github.com/google/deepvariant/issues/324:244,availability,avail,available,244,"That's correct, we provide the models trained with Genome in a Bottle truth sets as part of our Docker image and also in [this directory](https://pantheon.corp.google.com/storage/browser/deepvariant/models%2FDeepVariant%2F). The truth sets are available publicly at ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/. . All of the models we provide can be used for inference for the respective data types (WGS, WES, PacBio). When running inference, you can specify which model you would like to use. You can use any one of our provided models, or you can train your own models and use those.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/324
https://github.com/google/deepvariant/issues/324:308,deployability,releas,release,308,"That's correct, we provide the models trained with Genome in a Bottle truth sets as part of our Docker image and also in [this directory](https://pantheon.corp.google.com/storage/browser/deepvariant/models%2FDeepVariant%2F). The truth sets are available publicly at ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/. . All of the models we provide can be used for inference for the respective data types (WGS, WES, PacBio). When running inference, you can specify which model you would like to use. You can use any one of our provided models, or you can train your own models and use those.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/324
https://github.com/google/deepvariant/issues/324:31,energy efficiency,model,models,31,"That's correct, we provide the models trained with Genome in a Bottle truth sets as part of our Docker image and also in [this directory](https://pantheon.corp.google.com/storage/browser/deepvariant/models%2FDeepVariant%2F). The truth sets are available publicly at ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/. . All of the models we provide can be used for inference for the respective data types (WGS, WES, PacBio). When running inference, you can specify which model you would like to use. You can use any one of our provided models, or you can train your own models and use those.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/324
https://github.com/google/deepvariant/issues/324:199,energy efficiency,model,models,199,"That's correct, we provide the models trained with Genome in a Bottle truth sets as part of our Docker image and also in [this directory](https://pantheon.corp.google.com/storage/browser/deepvariant/models%2FDeepVariant%2F). The truth sets are available publicly at ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/. . All of the models we provide can be used for inference for the respective data types (WGS, WES, PacBio). When running inference, you can specify which model you would like to use. You can use any one of our provided models, or you can train your own models and use those.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/324
https://github.com/google/deepvariant/issues/324:331,energy efficiency,model,models,331,"That's correct, we provide the models trained with Genome in a Bottle truth sets as part of our Docker image and also in [this directory](https://pantheon.corp.google.com/storage/browser/deepvariant/models%2FDeepVariant%2F). The truth sets are available publicly at ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/. . All of the models we provide can be used for inference for the respective data types (WGS, WES, PacBio). When running inference, you can specify which model you would like to use. You can use any one of our provided models, or you can train your own models and use those.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/324
https://github.com/google/deepvariant/issues/324:471,energy efficiency,model,model,471,"That's correct, we provide the models trained with Genome in a Bottle truth sets as part of our Docker image and also in [this directory](https://pantheon.corp.google.com/storage/browser/deepvariant/models%2FDeepVariant%2F). The truth sets are available publicly at ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/. . All of the models we provide can be used for inference for the respective data types (WGS, WES, PacBio). When running inference, you can specify which model you would like to use. You can use any one of our provided models, or you can train your own models and use those.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/324
https://github.com/google/deepvariant/issues/324:536,energy efficiency,model,models,536,"That's correct, we provide the models trained with Genome in a Bottle truth sets as part of our Docker image and also in [this directory](https://pantheon.corp.google.com/storage/browser/deepvariant/models%2FDeepVariant%2F). The truth sets are available publicly at ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/. . All of the models we provide can be used for inference for the respective data types (WGS, WES, PacBio). When running inference, you can specify which model you would like to use. You can use any one of our provided models, or you can train your own models and use those.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/324
https://github.com/google/deepvariant/issues/324:570,energy efficiency,model,models,570,"That's correct, we provide the models trained with Genome in a Bottle truth sets as part of our Docker image and also in [this directory](https://pantheon.corp.google.com/storage/browser/deepvariant/models%2FDeepVariant%2F). The truth sets are available publicly at ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/. . All of the models we provide can be used for inference for the respective data types (WGS, WES, PacBio). When running inference, you can specify which model you would like to use. You can use any one of our provided models, or you can train your own models and use those.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/324
https://github.com/google/deepvariant/issues/324:254,integrability,pub,publicly,254,"That's correct, we provide the models trained with Genome in a Bottle truth sets as part of our Docker image and also in [this directory](https://pantheon.corp.google.com/storage/browser/deepvariant/models%2FDeepVariant%2F). The truth sets are available publicly at ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/. . All of the models we provide can be used for inference for the respective data types (WGS, WES, PacBio). When running inference, you can specify which model you would like to use. You can use any one of our provided models, or you can train your own models and use those.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/324
https://github.com/google/deepvariant/issues/324:457,interoperability,specif,specify,457,"That's correct, we provide the models trained with Genome in a Bottle truth sets as part of our Docker image and also in [this directory](https://pantheon.corp.google.com/storage/browser/deepvariant/models%2FDeepVariant%2F). The truth sets are available publicly at ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/. . All of the models we provide can be used for inference for the respective data types (WGS, WES, PacBio). When running inference, you can specify which model you would like to use. You can use any one of our provided models, or you can train your own models and use those.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/324
https://github.com/google/deepvariant/issues/324:416,modifiability,Pac,PacBio,416,"That's correct, we provide the models trained with Genome in a Bottle truth sets as part of our Docker image and also in [this directory](https://pantheon.corp.google.com/storage/browser/deepvariant/models%2FDeepVariant%2F). The truth sets are available publicly at ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/. . All of the models we provide can be used for inference for the respective data types (WGS, WES, PacBio). When running inference, you can specify which model you would like to use. You can use any one of our provided models, or you can train your own models and use those.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/324
https://github.com/google/deepvariant/issues/324:244,reliability,availab,available,244,"That's correct, we provide the models trained with Genome in a Bottle truth sets as part of our Docker image and also in [this directory](https://pantheon.corp.google.com/storage/browser/deepvariant/models%2FDeepVariant%2F). The truth sets are available publicly at ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/. . All of the models we provide can be used for inference for the respective data types (WGS, WES, PacBio). When running inference, you can specify which model you would like to use. You can use any one of our provided models, or you can train your own models and use those.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/324
https://github.com/google/deepvariant/issues/324:244,safety,avail,available,244,"That's correct, we provide the models trained with Genome in a Bottle truth sets as part of our Docker image and also in [this directory](https://pantheon.corp.google.com/storage/browser/deepvariant/models%2FDeepVariant%2F). The truth sets are available publicly at ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/. . All of the models we provide can be used for inference for the respective data types (WGS, WES, PacBio). When running inference, you can specify which model you would like to use. You can use any one of our provided models, or you can train your own models and use those.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/324
https://github.com/google/deepvariant/issues/324:31,security,model,models,31,"That's correct, we provide the models trained with Genome in a Bottle truth sets as part of our Docker image and also in [this directory](https://pantheon.corp.google.com/storage/browser/deepvariant/models%2FDeepVariant%2F). The truth sets are available publicly at ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/. . All of the models we provide can be used for inference for the respective data types (WGS, WES, PacBio). When running inference, you can specify which model you would like to use. You can use any one of our provided models, or you can train your own models and use those.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/324
https://github.com/google/deepvariant/issues/324:199,security,model,models,199,"That's correct, we provide the models trained with Genome in a Bottle truth sets as part of our Docker image and also in [this directory](https://pantheon.corp.google.com/storage/browser/deepvariant/models%2FDeepVariant%2F). The truth sets are available publicly at ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/. . All of the models we provide can be used for inference for the respective data types (WGS, WES, PacBio). When running inference, you can specify which model you would like to use. You can use any one of our provided models, or you can train your own models and use those.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/324
https://github.com/google/deepvariant/issues/324:244,security,availab,available,244,"That's correct, we provide the models trained with Genome in a Bottle truth sets as part of our Docker image and also in [this directory](https://pantheon.corp.google.com/storage/browser/deepvariant/models%2FDeepVariant%2F). The truth sets are available publicly at ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/. . All of the models we provide can be used for inference for the respective data types (WGS, WES, PacBio). When running inference, you can specify which model you would like to use. You can use any one of our provided models, or you can train your own models and use those.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/324
https://github.com/google/deepvariant/issues/324:331,security,model,models,331,"That's correct, we provide the models trained with Genome in a Bottle truth sets as part of our Docker image and also in [this directory](https://pantheon.corp.google.com/storage/browser/deepvariant/models%2FDeepVariant%2F). The truth sets are available publicly at ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/. . All of the models we provide can be used for inference for the respective data types (WGS, WES, PacBio). When running inference, you can specify which model you would like to use. You can use any one of our provided models, or you can train your own models and use those.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/324
https://github.com/google/deepvariant/issues/324:471,security,model,model,471,"That's correct, we provide the models trained with Genome in a Bottle truth sets as part of our Docker image and also in [this directory](https://pantheon.corp.google.com/storage/browser/deepvariant/models%2FDeepVariant%2F). The truth sets are available publicly at ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/. . All of the models we provide can be used for inference for the respective data types (WGS, WES, PacBio). When running inference, you can specify which model you would like to use. You can use any one of our provided models, or you can train your own models and use those.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/324
https://github.com/google/deepvariant/issues/324:536,security,model,models,536,"That's correct, we provide the models trained with Genome in a Bottle truth sets as part of our Docker image and also in [this directory](https://pantheon.corp.google.com/storage/browser/deepvariant/models%2FDeepVariant%2F). The truth sets are available publicly at ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/. . All of the models we provide can be used for inference for the respective data types (WGS, WES, PacBio). When running inference, you can specify which model you would like to use. You can use any one of our provided models, or you can train your own models and use those.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/324
https://github.com/google/deepvariant/issues/324:570,security,model,models,570,"That's correct, we provide the models trained with Genome in a Bottle truth sets as part of our Docker image and also in [this directory](https://pantheon.corp.google.com/storage/browser/deepvariant/models%2FDeepVariant%2F). The truth sets are available publicly at ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/. . All of the models we provide can be used for inference for the respective data types (WGS, WES, PacBio). When running inference, you can specify which model you would like to use. You can use any one of our provided models, or you can train your own models and use those.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/324
https://github.com/google/deepvariant/issues/324:276,testability,trace,trace,276,"That's correct, we provide the models trained with Genome in a Bottle truth sets as part of our Docker image and also in [this directory](https://pantheon.corp.google.com/storage/browser/deepvariant/models%2FDeepVariant%2F). The truth sets are available publicly at ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/. . All of the models we provide can be used for inference for the respective data types (WGS, WES, PacBio). When running inference, you can specify which model you would like to use. You can use any one of our provided models, or you can train your own models and use those.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/324
https://github.com/google/deepvariant/issues/324:78,energy efficiency,model,model,78,"Ok, . Thanks for clarifying that. . If I dont specify ""--genome "" or specific model using just ""--model wgs"" what model is used? I'm currently using deepvariant to variant calling for a pinus genome project. My input was Pinus.fasta and Pinus.fai for the genome part, for that I'm assuming that deepvariant is not using any of human trained data. I'm correct in assuming this? How can I train my data using a set of truth variants? Cheers. "". $ run_deepvariant.py --model_type=WGS --ref=Pinus.fasta --reads=mapped.sorted.bam --output_vcf=output/output.vcf.gz --output_gvcf=output/output.g.vcf.gz --num_shards=16. """".",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/324
https://github.com/google/deepvariant/issues/324:98,energy efficiency,model,model,98,"Ok, . Thanks for clarifying that. . If I dont specify ""--genome "" or specific model using just ""--model wgs"" what model is used? I'm currently using deepvariant to variant calling for a pinus genome project. My input was Pinus.fasta and Pinus.fai for the genome part, for that I'm assuming that deepvariant is not using any of human trained data. I'm correct in assuming this? How can I train my data using a set of truth variants? Cheers. "". $ run_deepvariant.py --model_type=WGS --ref=Pinus.fasta --reads=mapped.sorted.bam --output_vcf=output/output.vcf.gz --output_gvcf=output/output.g.vcf.gz --num_shards=16. """".",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/324
https://github.com/google/deepvariant/issues/324:114,energy efficiency,model,model,114,"Ok, . Thanks for clarifying that. . If I dont specify ""--genome "" or specific model using just ""--model wgs"" what model is used? I'm currently using deepvariant to variant calling for a pinus genome project. My input was Pinus.fasta and Pinus.fai for the genome part, for that I'm assuming that deepvariant is not using any of human trained data. I'm correct in assuming this? How can I train my data using a set of truth variants? Cheers. "". $ run_deepvariant.py --model_type=WGS --ref=Pinus.fasta --reads=mapped.sorted.bam --output_vcf=output/output.vcf.gz --output_gvcf=output/output.g.vcf.gz --num_shards=16. """".",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/324
https://github.com/google/deepvariant/issues/324:133,energy efficiency,current,currently,133,"Ok, . Thanks for clarifying that. . If I dont specify ""--genome "" or specific model using just ""--model wgs"" what model is used? I'm currently using deepvariant to variant calling for a pinus genome project. My input was Pinus.fasta and Pinus.fai for the genome part, for that I'm assuming that deepvariant is not using any of human trained data. I'm correct in assuming this? How can I train my data using a set of truth variants? Cheers. "". $ run_deepvariant.py --model_type=WGS --ref=Pinus.fasta --reads=mapped.sorted.bam --output_vcf=output/output.vcf.gz --output_gvcf=output/output.g.vcf.gz --num_shards=16. """".",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/324
https://github.com/google/deepvariant/issues/324:46,interoperability,specif,specify,46,"Ok, . Thanks for clarifying that. . If I dont specify ""--genome "" or specific model using just ""--model wgs"" what model is used? I'm currently using deepvariant to variant calling for a pinus genome project. My input was Pinus.fasta and Pinus.fai for the genome part, for that I'm assuming that deepvariant is not using any of human trained data. I'm correct in assuming this? How can I train my data using a set of truth variants? Cheers. "". $ run_deepvariant.py --model_type=WGS --ref=Pinus.fasta --reads=mapped.sorted.bam --output_vcf=output/output.vcf.gz --output_gvcf=output/output.g.vcf.gz --num_shards=16. """".",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/324
https://github.com/google/deepvariant/issues/324:69,interoperability,specif,specific,69,"Ok, . Thanks for clarifying that. . If I dont specify ""--genome "" or specific model using just ""--model wgs"" what model is used? I'm currently using deepvariant to variant calling for a pinus genome project. My input was Pinus.fasta and Pinus.fai for the genome part, for that I'm assuming that deepvariant is not using any of human trained data. I'm correct in assuming this? How can I train my data using a set of truth variants? Cheers. "". $ run_deepvariant.py --model_type=WGS --ref=Pinus.fasta --reads=mapped.sorted.bam --output_vcf=output/output.vcf.gz --output_gvcf=output/output.g.vcf.gz --num_shards=16. """".",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/324
https://github.com/google/deepvariant/issues/324:211,safety,input,input,211,"Ok, . Thanks for clarifying that. . If I dont specify ""--genome "" or specific model using just ""--model wgs"" what model is used? I'm currently using deepvariant to variant calling for a pinus genome project. My input was Pinus.fasta and Pinus.fai for the genome part, for that I'm assuming that deepvariant is not using any of human trained data. I'm correct in assuming this? How can I train my data using a set of truth variants? Cheers. "". $ run_deepvariant.py --model_type=WGS --ref=Pinus.fasta --reads=mapped.sorted.bam --output_vcf=output/output.vcf.gz --output_gvcf=output/output.g.vcf.gz --num_shards=16. """".",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/324
https://github.com/google/deepvariant/issues/324:78,security,model,model,78,"Ok, . Thanks for clarifying that. . If I dont specify ""--genome "" or specific model using just ""--model wgs"" what model is used? I'm currently using deepvariant to variant calling for a pinus genome project. My input was Pinus.fasta and Pinus.fai for the genome part, for that I'm assuming that deepvariant is not using any of human trained data. I'm correct in assuming this? How can I train my data using a set of truth variants? Cheers. "". $ run_deepvariant.py --model_type=WGS --ref=Pinus.fasta --reads=mapped.sorted.bam --output_vcf=output/output.vcf.gz --output_gvcf=output/output.g.vcf.gz --num_shards=16. """".",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/324
https://github.com/google/deepvariant/issues/324:98,security,model,model,98,"Ok, . Thanks for clarifying that. . If I dont specify ""--genome "" or specific model using just ""--model wgs"" what model is used? I'm currently using deepvariant to variant calling for a pinus genome project. My input was Pinus.fasta and Pinus.fai for the genome part, for that I'm assuming that deepvariant is not using any of human trained data. I'm correct in assuming this? How can I train my data using a set of truth variants? Cheers. "". $ run_deepvariant.py --model_type=WGS --ref=Pinus.fasta --reads=mapped.sorted.bam --output_vcf=output/output.vcf.gz --output_gvcf=output/output.g.vcf.gz --num_shards=16. """".",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/324
https://github.com/google/deepvariant/issues/324:114,security,model,model,114,"Ok, . Thanks for clarifying that. . If I dont specify ""--genome "" or specific model using just ""--model wgs"" what model is used? I'm currently using deepvariant to variant calling for a pinus genome project. My input was Pinus.fasta and Pinus.fai for the genome part, for that I'm assuming that deepvariant is not using any of human trained data. I'm correct in assuming this? How can I train my data using a set of truth variants? Cheers. "". $ run_deepvariant.py --model_type=WGS --ref=Pinus.fasta --reads=mapped.sorted.bam --output_vcf=output/output.vcf.gz --output_gvcf=output/output.g.vcf.gz --num_shards=16. """".",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/324
https://github.com/google/deepvariant/issues/324:211,usability,input,input,211,"Ok, . Thanks for clarifying that. . If I dont specify ""--genome "" or specific model using just ""--model wgs"" what model is used? I'm currently using deepvariant to variant calling for a pinus genome project. My input was Pinus.fasta and Pinus.fai for the genome part, for that I'm assuming that deepvariant is not using any of human trained data. I'm correct in assuming this? How can I train my data using a set of truth variants? Cheers. "". $ run_deepvariant.py --model_type=WGS --ref=Pinus.fasta --reads=mapped.sorted.bam --output_vcf=output/output.vcf.gz --output_gvcf=output/output.g.vcf.gz --num_shards=16. """".",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/324
https://github.com/google/deepvariant/issues/324:662,energy efficiency,model,model,662,"Hi @leorippel . If you are referring specifically to the process of VQSR, we do not recommend this step. In our benchmarks for VQSR, at both the cohort and individual level, recall and overall accuracy are substantially worse (see Figure3 of https://www.biorxiv.org/content/10.1101/2020.02.10.942086v1.full.pdf or Table 1 of https://www.nature.com/articles/nbt.4235). If you would like to further filter variant calls to increase precision, we recommend instead filtering based on genotype quality instead, with something like a cutoff of GQ>10 being reasonable (this implies the lowest retained variant has at least a 90% chance of being correctly called). The model specified in the command line here is the model used for the calling process itself. It is, in theory, possible to retrain this model, but you would need to have a set of confident sites generated through a process with better accuracy, for example deep PacBio HiFi data and using those sites to train an Illumina model. This process is technically involved as well, so my recommendation is that if you would like higher precision, you can apply a filter on the GQ value for calls. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/324
https://github.com/google/deepvariant/issues/324:710,energy efficiency,model,model,710,"Hi @leorippel . If you are referring specifically to the process of VQSR, we do not recommend this step. In our benchmarks for VQSR, at both the cohort and individual level, recall and overall accuracy are substantially worse (see Figure3 of https://www.biorxiv.org/content/10.1101/2020.02.10.942086v1.full.pdf or Table 1 of https://www.nature.com/articles/nbt.4235). If you would like to further filter variant calls to increase precision, we recommend instead filtering based on genotype quality instead, with something like a cutoff of GQ>10 being reasonable (this implies the lowest retained variant has at least a 90% chance of being correctly called). The model specified in the command line here is the model used for the calling process itself. It is, in theory, possible to retrain this model, but you would need to have a set of confident sites generated through a process with better accuracy, for example deep PacBio HiFi data and using those sites to train an Illumina model. This process is technically involved as well, so my recommendation is that if you would like higher precision, you can apply a filter on the GQ value for calls. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/324
https://github.com/google/deepvariant/issues/324:796,energy efficiency,model,model,796,"Hi @leorippel . If you are referring specifically to the process of VQSR, we do not recommend this step. In our benchmarks for VQSR, at both the cohort and individual level, recall and overall accuracy are substantially worse (see Figure3 of https://www.biorxiv.org/content/10.1101/2020.02.10.942086v1.full.pdf or Table 1 of https://www.nature.com/articles/nbt.4235). If you would like to further filter variant calls to increase precision, we recommend instead filtering based on genotype quality instead, with something like a cutoff of GQ>10 being reasonable (this implies the lowest retained variant has at least a 90% chance of being correctly called). The model specified in the command line here is the model used for the calling process itself. It is, in theory, possible to retrain this model, but you would need to have a set of confident sites generated through a process with better accuracy, for example deep PacBio HiFi data and using those sites to train an Illumina model. This process is technically involved as well, so my recommendation is that if you would like higher precision, you can apply a filter on the GQ value for calls. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/324
https://github.com/google/deepvariant/issues/324:982,energy efficiency,model,model,982,"Hi @leorippel . If you are referring specifically to the process of VQSR, we do not recommend this step. In our benchmarks for VQSR, at both the cohort and individual level, recall and overall accuracy are substantially worse (see Figure3 of https://www.biorxiv.org/content/10.1101/2020.02.10.942086v1.full.pdf or Table 1 of https://www.nature.com/articles/nbt.4235). If you would like to further filter variant calls to increase precision, we recommend instead filtering based on genotype quality instead, with something like a cutoff of GQ>10 being reasonable (this implies the lowest retained variant has at least a 90% chance of being correctly called). The model specified in the command line here is the model used for the calling process itself. It is, in theory, possible to retrain this model, but you would need to have a set of confident sites generated through a process with better accuracy, for example deep PacBio HiFi data and using those sites to train an Illumina model. This process is technically involved as well, so my recommendation is that if you would like higher precision, you can apply a filter on the GQ value for calls. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/324
https://github.com/google/deepvariant/issues/324:206,integrability,sub,substantially,206,"Hi @leorippel . If you are referring specifically to the process of VQSR, we do not recommend this step. In our benchmarks for VQSR, at both the cohort and individual level, recall and overall accuracy are substantially worse (see Figure3 of https://www.biorxiv.org/content/10.1101/2020.02.10.942086v1.full.pdf or Table 1 of https://www.nature.com/articles/nbt.4235). If you would like to further filter variant calls to increase precision, we recommend instead filtering based on genotype quality instead, with something like a cutoff of GQ>10 being reasonable (this implies the lowest retained variant has at least a 90% chance of being correctly called). The model specified in the command line here is the model used for the calling process itself. It is, in theory, possible to retrain this model, but you would need to have a set of confident sites generated through a process with better accuracy, for example deep PacBio HiFi data and using those sites to train an Illumina model. This process is technically involved as well, so my recommendation is that if you would like higher precision, you can apply a filter on the GQ value for calls. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/324
https://github.com/google/deepvariant/issues/324:397,integrability,filter,filter,397,"Hi @leorippel . If you are referring specifically to the process of VQSR, we do not recommend this step. In our benchmarks for VQSR, at both the cohort and individual level, recall and overall accuracy are substantially worse (see Figure3 of https://www.biorxiv.org/content/10.1101/2020.02.10.942086v1.full.pdf or Table 1 of https://www.nature.com/articles/nbt.4235). If you would like to further filter variant calls to increase precision, we recommend instead filtering based on genotype quality instead, with something like a cutoff of GQ>10 being reasonable (this implies the lowest retained variant has at least a 90% chance of being correctly called). The model specified in the command line here is the model used for the calling process itself. It is, in theory, possible to retrain this model, but you would need to have a set of confident sites generated through a process with better accuracy, for example deep PacBio HiFi data and using those sites to train an Illumina model. This process is technically involved as well, so my recommendation is that if you would like higher precision, you can apply a filter on the GQ value for calls. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/324
https://github.com/google/deepvariant/issues/324:462,integrability,filter,filtering,462,"Hi @leorippel . If you are referring specifically to the process of VQSR, we do not recommend this step. In our benchmarks for VQSR, at both the cohort and individual level, recall and overall accuracy are substantially worse (see Figure3 of https://www.biorxiv.org/content/10.1101/2020.02.10.942086v1.full.pdf or Table 1 of https://www.nature.com/articles/nbt.4235). If you would like to further filter variant calls to increase precision, we recommend instead filtering based on genotype quality instead, with something like a cutoff of GQ>10 being reasonable (this implies the lowest retained variant has at least a 90% chance of being correctly called). The model specified in the command line here is the model used for the calling process itself. It is, in theory, possible to retrain this model, but you would need to have a set of confident sites generated through a process with better accuracy, for example deep PacBio HiFi data and using those sites to train an Illumina model. This process is technically involved as well, so my recommendation is that if you would like higher precision, you can apply a filter on the GQ value for calls. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/324
https://github.com/google/deepvariant/issues/324:1116,integrability,filter,filter,1116,"Hi @leorippel . If you are referring specifically to the process of VQSR, we do not recommend this step. In our benchmarks for VQSR, at both the cohort and individual level, recall and overall accuracy are substantially worse (see Figure3 of https://www.biorxiv.org/content/10.1101/2020.02.10.942086v1.full.pdf or Table 1 of https://www.nature.com/articles/nbt.4235). If you would like to further filter variant calls to increase precision, we recommend instead filtering based on genotype quality instead, with something like a cutoff of GQ>10 being reasonable (this implies the lowest retained variant has at least a 90% chance of being correctly called). The model specified in the command line here is the model used for the calling process itself. It is, in theory, possible to retrain this model, but you would need to have a set of confident sites generated through a process with better accuracy, for example deep PacBio HiFi data and using those sites to train an Illumina model. This process is technically involved as well, so my recommendation is that if you would like higher precision, you can apply a filter on the GQ value for calls. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/324
https://github.com/google/deepvariant/issues/324:37,interoperability,specif,specifically,37,"Hi @leorippel . If you are referring specifically to the process of VQSR, we do not recommend this step. In our benchmarks for VQSR, at both the cohort and individual level, recall and overall accuracy are substantially worse (see Figure3 of https://www.biorxiv.org/content/10.1101/2020.02.10.942086v1.full.pdf or Table 1 of https://www.nature.com/articles/nbt.4235). If you would like to further filter variant calls to increase precision, we recommend instead filtering based on genotype quality instead, with something like a cutoff of GQ>10 being reasonable (this implies the lowest retained variant has at least a 90% chance of being correctly called). The model specified in the command line here is the model used for the calling process itself. It is, in theory, possible to retrain this model, but you would need to have a set of confident sites generated through a process with better accuracy, for example deep PacBio HiFi data and using those sites to train an Illumina model. This process is technically involved as well, so my recommendation is that if you would like higher precision, you can apply a filter on the GQ value for calls. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/324
https://github.com/google/deepvariant/issues/324:668,interoperability,specif,specified,668,"Hi @leorippel . If you are referring specifically to the process of VQSR, we do not recommend this step. In our benchmarks for VQSR, at both the cohort and individual level, recall and overall accuracy are substantially worse (see Figure3 of https://www.biorxiv.org/content/10.1101/2020.02.10.942086v1.full.pdf or Table 1 of https://www.nature.com/articles/nbt.4235). If you would like to further filter variant calls to increase precision, we recommend instead filtering based on genotype quality instead, with something like a cutoff of GQ>10 being reasonable (this implies the lowest retained variant has at least a 90% chance of being correctly called). The model specified in the command line here is the model used for the calling process itself. It is, in theory, possible to retrain this model, but you would need to have a set of confident sites generated through a process with better accuracy, for example deep PacBio HiFi data and using those sites to train an Illumina model. This process is technically involved as well, so my recommendation is that if you would like higher precision, you can apply a filter on the GQ value for calls. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/324
https://github.com/google/deepvariant/issues/324:922,modifiability,Pac,PacBio,922,"Hi @leorippel . If you are referring specifically to the process of VQSR, we do not recommend this step. In our benchmarks for VQSR, at both the cohort and individual level, recall and overall accuracy are substantially worse (see Figure3 of https://www.biorxiv.org/content/10.1101/2020.02.10.942086v1.full.pdf or Table 1 of https://www.nature.com/articles/nbt.4235). If you would like to further filter variant calls to increase precision, we recommend instead filtering based on genotype quality instead, with something like a cutoff of GQ>10 being reasonable (this implies the lowest retained variant has at least a 90% chance of being correctly called). The model specified in the command line here is the model used for the calling process itself. It is, in theory, possible to retrain this model, but you would need to have a set of confident sites generated through a process with better accuracy, for example deep PacBio HiFi data and using those sites to train an Illumina model. This process is technically involved as well, so my recommendation is that if you would like higher precision, you can apply a filter on the GQ value for calls. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/324
https://github.com/google/deepvariant/issues/324:266,performance,content,content,266,"Hi @leorippel . If you are referring specifically to the process of VQSR, we do not recommend this step. In our benchmarks for VQSR, at both the cohort and individual level, recall and overall accuracy are substantially worse (see Figure3 of https://www.biorxiv.org/content/10.1101/2020.02.10.942086v1.full.pdf or Table 1 of https://www.nature.com/articles/nbt.4235). If you would like to further filter variant calls to increase precision, we recommend instead filtering based on genotype quality instead, with something like a cutoff of GQ>10 being reasonable (this implies the lowest retained variant has at least a 90% chance of being correctly called). The model specified in the command line here is the model used for the calling process itself. It is, in theory, possible to retrain this model, but you would need to have a set of confident sites generated through a process with better accuracy, for example deep PacBio HiFi data and using those sites to train an Illumina model. This process is technically involved as well, so my recommendation is that if you would like higher precision, you can apply a filter on the GQ value for calls. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/324
https://github.com/google/deepvariant/issues/324:662,security,model,model,662,"Hi @leorippel . If you are referring specifically to the process of VQSR, we do not recommend this step. In our benchmarks for VQSR, at both the cohort and individual level, recall and overall accuracy are substantially worse (see Figure3 of https://www.biorxiv.org/content/10.1101/2020.02.10.942086v1.full.pdf or Table 1 of https://www.nature.com/articles/nbt.4235). If you would like to further filter variant calls to increase precision, we recommend instead filtering based on genotype quality instead, with something like a cutoff of GQ>10 being reasonable (this implies the lowest retained variant has at least a 90% chance of being correctly called). The model specified in the command line here is the model used for the calling process itself. It is, in theory, possible to retrain this model, but you would need to have a set of confident sites generated through a process with better accuracy, for example deep PacBio HiFi data and using those sites to train an Illumina model. This process is technically involved as well, so my recommendation is that if you would like higher precision, you can apply a filter on the GQ value for calls. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/324
https://github.com/google/deepvariant/issues/324:710,security,model,model,710,"Hi @leorippel . If you are referring specifically to the process of VQSR, we do not recommend this step. In our benchmarks for VQSR, at both the cohort and individual level, recall and overall accuracy are substantially worse (see Figure3 of https://www.biorxiv.org/content/10.1101/2020.02.10.942086v1.full.pdf or Table 1 of https://www.nature.com/articles/nbt.4235). If you would like to further filter variant calls to increase precision, we recommend instead filtering based on genotype quality instead, with something like a cutoff of GQ>10 being reasonable (this implies the lowest retained variant has at least a 90% chance of being correctly called). The model specified in the command line here is the model used for the calling process itself. It is, in theory, possible to retrain this model, but you would need to have a set of confident sites generated through a process with better accuracy, for example deep PacBio HiFi data and using those sites to train an Illumina model. This process is technically involved as well, so my recommendation is that if you would like higher precision, you can apply a filter on the GQ value for calls. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/324
https://github.com/google/deepvariant/issues/324:796,security,model,model,796,"Hi @leorippel . If you are referring specifically to the process of VQSR, we do not recommend this step. In our benchmarks for VQSR, at both the cohort and individual level, recall and overall accuracy are substantially worse (see Figure3 of https://www.biorxiv.org/content/10.1101/2020.02.10.942086v1.full.pdf or Table 1 of https://www.nature.com/articles/nbt.4235). If you would like to further filter variant calls to increase precision, we recommend instead filtering based on genotype quality instead, with something like a cutoff of GQ>10 being reasonable (this implies the lowest retained variant has at least a 90% chance of being correctly called). The model specified in the command line here is the model used for the calling process itself. It is, in theory, possible to retrain this model, but you would need to have a set of confident sites generated through a process with better accuracy, for example deep PacBio HiFi data and using those sites to train an Illumina model. This process is technically involved as well, so my recommendation is that if you would like higher precision, you can apply a filter on the GQ value for calls. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/324
https://github.com/google/deepvariant/issues/324:982,security,model,model,982,"Hi @leorippel . If you are referring specifically to the process of VQSR, we do not recommend this step. In our benchmarks for VQSR, at both the cohort and individual level, recall and overall accuracy are substantially worse (see Figure3 of https://www.biorxiv.org/content/10.1101/2020.02.10.942086v1.full.pdf or Table 1 of https://www.nature.com/articles/nbt.4235). If you would like to further filter variant calls to increase precision, we recommend instead filtering based on genotype quality instead, with something like a cutoff of GQ>10 being reasonable (this implies the lowest retained variant has at least a 90% chance of being correctly called). The model specified in the command line here is the model used for the calling process itself. It is, in theory, possible to retrain this model, but you would need to have a set of confident sites generated through a process with better accuracy, for example deep PacBio HiFi data and using those sites to train an Illumina model. This process is technically involved as well, so my recommendation is that if you would like higher precision, you can apply a filter on the GQ value for calls. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/324
https://github.com/google/deepvariant/issues/324:685,usability,command,command,685,"Hi @leorippel . If you are referring specifically to the process of VQSR, we do not recommend this step. In our benchmarks for VQSR, at both the cohort and individual level, recall and overall accuracy are substantially worse (see Figure3 of https://www.biorxiv.org/content/10.1101/2020.02.10.942086v1.full.pdf or Table 1 of https://www.nature.com/articles/nbt.4235). If you would like to further filter variant calls to increase precision, we recommend instead filtering based on genotype quality instead, with something like a cutoff of GQ>10 being reasonable (this implies the lowest retained variant has at least a 90% chance of being correctly called). The model specified in the command line here is the model used for the calling process itself. It is, in theory, possible to retrain this model, but you would need to have a set of confident sites generated through a process with better accuracy, for example deep PacBio HiFi data and using those sites to train an Illumina model. This process is technically involved as well, so my recommendation is that if you would like higher precision, you can apply a filter on the GQ value for calls. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/324
https://github.com/google/deepvariant/issues/324:227,availability,sli,slightly,227,"Hello @AndrewCarroll ,. Thanks for you know-how, valuable suggestion. VQSR seems now something to be forgot. . How much benefit would I have using a truth set for my analysis, without training? Worth the trouble? I will have a slightly refined truth set soon enough following excluding a small proportion of SNP in the QTL population maps. . Cheers.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/324
https://github.com/google/deepvariant/issues/324:227,reliability,sli,slightly,227,"Hello @AndrewCarroll ,. Thanks for you know-how, valuable suggestion. VQSR seems now something to be forgot. . How much benefit would I have using a truth set for my analysis, without training? Worth the trouble? I will have a slightly refined truth set soon enough following excluding a small proportion of SNP in the QTL population maps. . Cheers.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/324
https://github.com/google/deepvariant/issues/324:80,security,assess,assessment,80,"Hi @leorippel, I am assuming you are asking about using the truth set for final assessment of the calls produced by DeepVariant. If you have confidence in the quality of your truth set, it makes to use this for additional analysis. I'll go ahead and close this issue, but feel free to reopen if you have additional questions!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/324
https://github.com/google/deepvariant/issues/324:250,usability,close,close,250,"Hi @leorippel, I am assuming you are asking about using the truth set for final assessment of the calls produced by DeepVariant. If you have confidence in the quality of your truth set, it makes to use this for additional analysis. I'll go ahead and close this issue, but feel free to reopen if you have additional questions!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/324
https://github.com/google/deepvariant/issues/325:53,safety,test,testdata,53,"Hi @abrozzi, a quick check: where is the `quickstart-testdata` directory located? The Docker command you are using expects it to be under `/root`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/325
https://github.com/google/deepvariant/issues/325:53,testability,test,testdata,53,"Hi @abrozzi, a quick check: where is the `quickstart-testdata` directory located? The Docker command you are using expects it to be under `/root`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/325
https://github.com/google/deepvariant/issues/325:93,usability,command,command,93,"Hi @abrozzi, a quick check: where is the `quickstart-testdata` directory located? The Docker command you are using expects it to be under `/root`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/325
https://github.com/google/deepvariant/issues/325:315,availability,error,error,315,"Some other things to look in to:. * Is what you have pasted above the full stack trace? If there's any other output, it would be helpful to see that as well. * Could you try running `make_examples` directly using the below command? This is only using one shard. If this command fails, we may see a more informative error message. ```. sudo docker run \. -v ""/root/quickstart-testdata"":""/input"" \. -v ""/root/quickstart-output"":""/output"" \. google/deepvariant:latest \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" \. --examples ""/output/make_examples.tfrecord@1.gz"" \. --gvcf ""/output/gvcf.tfrecord@1.gz"" \. --regions ""chr20:10,000,000-10,010,000"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/325
https://github.com/google/deepvariant/issues/325:75,deployability,stack,stack,75,"Some other things to look in to:. * Is what you have pasted above the full stack trace? If there's any other output, it would be helpful to see that as well. * Could you try running `make_examples` directly using the below command? This is only using one shard. If this command fails, we may see a more informative error message. ```. sudo docker run \. -v ""/root/quickstart-testdata"":""/input"" \. -v ""/root/quickstart-output"":""/output"" \. google/deepvariant:latest \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" \. --examples ""/output/make_examples.tfrecord@1.gz"" \. --gvcf ""/output/gvcf.tfrecord@1.gz"" \. --regions ""chr20:10,000,000-10,010,000"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/325
https://github.com/google/deepvariant/issues/325:278,deployability,fail,fails,278,"Some other things to look in to:. * Is what you have pasted above the full stack trace? If there's any other output, it would be helpful to see that as well. * Could you try running `make_examples` directly using the below command? This is only using one shard. If this command fails, we may see a more informative error message. ```. sudo docker run \. -v ""/root/quickstart-testdata"":""/input"" \. -v ""/root/quickstart-output"":""/output"" \. google/deepvariant:latest \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" \. --examples ""/output/make_examples.tfrecord@1.gz"" \. --gvcf ""/output/gvcf.tfrecord@1.gz"" \. --regions ""chr20:10,000,000-10,010,000"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/325
https://github.com/google/deepvariant/issues/325:321,integrability,messag,message,321,"Some other things to look in to:. * Is what you have pasted above the full stack trace? If there's any other output, it would be helpful to see that as well. * Could you try running `make_examples` directly using the below command? This is only using one shard. If this command fails, we may see a more informative error message. ```. sudo docker run \. -v ""/root/quickstart-testdata"":""/input"" \. -v ""/root/quickstart-output"":""/output"" \. google/deepvariant:latest \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" \. --examples ""/output/make_examples.tfrecord@1.gz"" \. --gvcf ""/output/gvcf.tfrecord@1.gz"" \. --regions ""chr20:10,000,000-10,010,000"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/325
https://github.com/google/deepvariant/issues/325:321,interoperability,messag,message,321,"Some other things to look in to:. * Is what you have pasted above the full stack trace? If there's any other output, it would be helpful to see that as well. * Could you try running `make_examples` directly using the below command? This is only using one shard. If this command fails, we may see a more informative error message. ```. sudo docker run \. -v ""/root/quickstart-testdata"":""/input"" \. -v ""/root/quickstart-output"":""/output"" \. google/deepvariant:latest \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" \. --examples ""/output/make_examples.tfrecord@1.gz"" \. --gvcf ""/output/gvcf.tfrecord@1.gz"" \. --regions ""chr20:10,000,000-10,010,000"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/325
https://github.com/google/deepvariant/issues/325:315,performance,error,error,315,"Some other things to look in to:. * Is what you have pasted above the full stack trace? If there's any other output, it would be helpful to see that as well. * Could you try running `make_examples` directly using the below command? This is only using one shard. If this command fails, we may see a more informative error message. ```. sudo docker run \. -v ""/root/quickstart-testdata"":""/input"" \. -v ""/root/quickstart-output"":""/output"" \. google/deepvariant:latest \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" \. --examples ""/output/make_examples.tfrecord@1.gz"" \. --gvcf ""/output/gvcf.tfrecord@1.gz"" \. --regions ""chr20:10,000,000-10,010,000"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/325
https://github.com/google/deepvariant/issues/325:278,reliability,fail,fails,278,"Some other things to look in to:. * Is what you have pasted above the full stack trace? If there's any other output, it would be helpful to see that as well. * Could you try running `make_examples` directly using the below command? This is only using one shard. If this command fails, we may see a more informative error message. ```. sudo docker run \. -v ""/root/quickstart-testdata"":""/input"" \. -v ""/root/quickstart-output"":""/output"" \. google/deepvariant:latest \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" \. --examples ""/output/make_examples.tfrecord@1.gz"" \. --gvcf ""/output/gvcf.tfrecord@1.gz"" \. --regions ""chr20:10,000,000-10,010,000"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/325
https://github.com/google/deepvariant/issues/325:315,safety,error,error,315,"Some other things to look in to:. * Is what you have pasted above the full stack trace? If there's any other output, it would be helpful to see that as well. * Could you try running `make_examples` directly using the below command? This is only using one shard. If this command fails, we may see a more informative error message. ```. sudo docker run \. -v ""/root/quickstart-testdata"":""/input"" \. -v ""/root/quickstart-output"":""/output"" \. google/deepvariant:latest \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" \. --examples ""/output/make_examples.tfrecord@1.gz"" \. --gvcf ""/output/gvcf.tfrecord@1.gz"" \. --regions ""chr20:10,000,000-10,010,000"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/325
https://github.com/google/deepvariant/issues/325:375,safety,test,testdata,375,"Some other things to look in to:. * Is what you have pasted above the full stack trace? If there's any other output, it would be helpful to see that as well. * Could you try running `make_examples` directly using the below command? This is only using one shard. If this command fails, we may see a more informative error message. ```. sudo docker run \. -v ""/root/quickstart-testdata"":""/input"" \. -v ""/root/quickstart-output"":""/output"" \. google/deepvariant:latest \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" \. --examples ""/output/make_examples.tfrecord@1.gz"" \. --gvcf ""/output/gvcf.tfrecord@1.gz"" \. --regions ""chr20:10,000,000-10,010,000"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/325
https://github.com/google/deepvariant/issues/325:387,safety,input,input,387,"Some other things to look in to:. * Is what you have pasted above the full stack trace? If there's any other output, it would be helpful to see that as well. * Could you try running `make_examples` directly using the below command? This is only using one shard. If this command fails, we may see a more informative error message. ```. sudo docker run \. -v ""/root/quickstart-testdata"":""/input"" \. -v ""/root/quickstart-output"":""/output"" \. google/deepvariant:latest \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" \. --examples ""/output/make_examples.tfrecord@1.gz"" \. --gvcf ""/output/gvcf.tfrecord@1.gz"" \. --regions ""chr20:10,000,000-10,010,000"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/325
https://github.com/google/deepvariant/issues/325:532,safety,input,input,532,"Some other things to look in to:. * Is what you have pasted above the full stack trace? If there's any other output, it would be helpful to see that as well. * Could you try running `make_examples` directly using the below command? This is only using one shard. If this command fails, we may see a more informative error message. ```. sudo docker run \. -v ""/root/quickstart-testdata"":""/input"" \. -v ""/root/quickstart-output"":""/output"" \. google/deepvariant:latest \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" \. --examples ""/output/make_examples.tfrecord@1.gz"" \. --gvcf ""/output/gvcf.tfrecord@1.gz"" \. --regions ""chr20:10,000,000-10,010,000"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/325
https://github.com/google/deepvariant/issues/325:583,safety,input,input,583,"Some other things to look in to:. * Is what you have pasted above the full stack trace? If there's any other output, it would be helpful to see that as well. * Could you try running `make_examples` directly using the below command? This is only using one shard. If this command fails, we may see a more informative error message. ```. sudo docker run \. -v ""/root/quickstart-testdata"":""/input"" \. -v ""/root/quickstart-output"":""/output"" \. google/deepvariant:latest \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" \. --examples ""/output/make_examples.tfrecord@1.gz"" \. --gvcf ""/output/gvcf.tfrecord@1.gz"" \. --regions ""chr20:10,000,000-10,010,000"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/325
https://github.com/google/deepvariant/issues/325:81,testability,trace,trace,81,"Some other things to look in to:. * Is what you have pasted above the full stack trace? If there's any other output, it would be helpful to see that as well. * Could you try running `make_examples` directly using the below command? This is only using one shard. If this command fails, we may see a more informative error message. ```. sudo docker run \. -v ""/root/quickstart-testdata"":""/input"" \. -v ""/root/quickstart-output"":""/output"" \. google/deepvariant:latest \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" \. --examples ""/output/make_examples.tfrecord@1.gz"" \. --gvcf ""/output/gvcf.tfrecord@1.gz"" \. --regions ""chr20:10,000,000-10,010,000"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/325
https://github.com/google/deepvariant/issues/325:375,testability,test,testdata,375,"Some other things to look in to:. * Is what you have pasted above the full stack trace? If there's any other output, it would be helpful to see that as well. * Could you try running `make_examples` directly using the below command? This is only using one shard. If this command fails, we may see a more informative error message. ```. sudo docker run \. -v ""/root/quickstart-testdata"":""/input"" \. -v ""/root/quickstart-output"":""/output"" \. google/deepvariant:latest \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" \. --examples ""/output/make_examples.tfrecord@1.gz"" \. --gvcf ""/output/gvcf.tfrecord@1.gz"" \. --regions ""chr20:10,000,000-10,010,000"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/325
https://github.com/google/deepvariant/issues/325:554,testability,unit,unittest,554,"Some other things to look in to:. * Is what you have pasted above the full stack trace? If there's any other output, it would be helpful to see that as well. * Could you try running `make_examples` directly using the below command? This is only using one shard. If this command fails, we may see a more informative error message. ```. sudo docker run \. -v ""/root/quickstart-testdata"":""/input"" \. -v ""/root/quickstart-output"":""/output"" \. google/deepvariant:latest \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" \. --examples ""/output/make_examples.tfrecord@1.gz"" \. --gvcf ""/output/gvcf.tfrecord@1.gz"" \. --regions ""chr20:10,000,000-10,010,000"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/325
https://github.com/google/deepvariant/issues/325:129,usability,help,helpful,129,"Some other things to look in to:. * Is what you have pasted above the full stack trace? If there's any other output, it would be helpful to see that as well. * Could you try running `make_examples` directly using the below command? This is only using one shard. If this command fails, we may see a more informative error message. ```. sudo docker run \. -v ""/root/quickstart-testdata"":""/input"" \. -v ""/root/quickstart-output"":""/output"" \. google/deepvariant:latest \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" \. --examples ""/output/make_examples.tfrecord@1.gz"" \. --gvcf ""/output/gvcf.tfrecord@1.gz"" \. --regions ""chr20:10,000,000-10,010,000"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/325
https://github.com/google/deepvariant/issues/325:223,usability,command,command,223,"Some other things to look in to:. * Is what you have pasted above the full stack trace? If there's any other output, it would be helpful to see that as well. * Could you try running `make_examples` directly using the below command? This is only using one shard. If this command fails, we may see a more informative error message. ```. sudo docker run \. -v ""/root/quickstart-testdata"":""/input"" \. -v ""/root/quickstart-output"":""/output"" \. google/deepvariant:latest \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" \. --examples ""/output/make_examples.tfrecord@1.gz"" \. --gvcf ""/output/gvcf.tfrecord@1.gz"" \. --regions ""chr20:10,000,000-10,010,000"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/325
https://github.com/google/deepvariant/issues/325:270,usability,command,command,270,"Some other things to look in to:. * Is what you have pasted above the full stack trace? If there's any other output, it would be helpful to see that as well. * Could you try running `make_examples` directly using the below command? This is only using one shard. If this command fails, we may see a more informative error message. ```. sudo docker run \. -v ""/root/quickstart-testdata"":""/input"" \. -v ""/root/quickstart-output"":""/output"" \. google/deepvariant:latest \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" \. --examples ""/output/make_examples.tfrecord@1.gz"" \. --gvcf ""/output/gvcf.tfrecord@1.gz"" \. --regions ""chr20:10,000,000-10,010,000"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/325
https://github.com/google/deepvariant/issues/325:315,usability,error,error,315,"Some other things to look in to:. * Is what you have pasted above the full stack trace? If there's any other output, it would be helpful to see that as well. * Could you try running `make_examples` directly using the below command? This is only using one shard. If this command fails, we may see a more informative error message. ```. sudo docker run \. -v ""/root/quickstart-testdata"":""/input"" \. -v ""/root/quickstart-output"":""/output"" \. google/deepvariant:latest \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" \. --examples ""/output/make_examples.tfrecord@1.gz"" \. --gvcf ""/output/gvcf.tfrecord@1.gz"" \. --regions ""chr20:10,000,000-10,010,000"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/325
https://github.com/google/deepvariant/issues/325:387,usability,input,input,387,"Some other things to look in to:. * Is what you have pasted above the full stack trace? If there's any other output, it would be helpful to see that as well. * Could you try running `make_examples` directly using the below command? This is only using one shard. If this command fails, we may see a more informative error message. ```. sudo docker run \. -v ""/root/quickstart-testdata"":""/input"" \. -v ""/root/quickstart-output"":""/output"" \. google/deepvariant:latest \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" \. --examples ""/output/make_examples.tfrecord@1.gz"" \. --gvcf ""/output/gvcf.tfrecord@1.gz"" \. --regions ""chr20:10,000,000-10,010,000"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/325
https://github.com/google/deepvariant/issues/325:532,usability,input,input,532,"Some other things to look in to:. * Is what you have pasted above the full stack trace? If there's any other output, it would be helpful to see that as well. * Could you try running `make_examples` directly using the below command? This is only using one shard. If this command fails, we may see a more informative error message. ```. sudo docker run \. -v ""/root/quickstart-testdata"":""/input"" \. -v ""/root/quickstart-output"":""/output"" \. google/deepvariant:latest \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" \. --examples ""/output/make_examples.tfrecord@1.gz"" \. --gvcf ""/output/gvcf.tfrecord@1.gz"" \. --regions ""chr20:10,000,000-10,010,000"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/325
https://github.com/google/deepvariant/issues/325:583,usability,input,input,583,"Some other things to look in to:. * Is what you have pasted above the full stack trace? If there's any other output, it would be helpful to see that as well. * Could you try running `make_examples` directly using the below command? This is only using one shard. If this command fails, we may see a more informative error message. ```. sudo docker run \. -v ""/root/quickstart-testdata"":""/input"" \. -v ""/root/quickstart-output"":""/output"" \. google/deepvariant:latest \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" \. --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" \. --examples ""/output/make_examples.tfrecord@1.gz"" \. --gvcf ""/output/gvcf.tfrecord@1.gz"" \. --regions ""chr20:10,000,000-10,010,000"". ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/325
https://github.com/google/deepvariant/issues/325:220,availability,error,error,220,"Hi Gunjan,. and thanks a lot for your reply. Yes, the directories where all accessible and under ```root``` (I know it can be done better....). Indeed when I added to the docker command the option ``` --user root``` the error changed and was very clear: ``` disk full```. It remains to me to free disk or change docker location, add a docker group to allow users run it without ```sudo``` and rerun it. I am pretty confident it should work. With Regards,. -A.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/325
https://github.com/google/deepvariant/issues/325:220,performance,error,error,220,"Hi Gunjan,. and thanks a lot for your reply. Yes, the directories where all accessible and under ```root``` (I know it can be done better....). Indeed when I added to the docker command the option ``` --user root``` the error changed and was very clear: ``` disk full```. It remains to me to free disk or change docker location, add a docker group to allow users run it without ```sudo``` and rerun it. I am pretty confident it should work. With Regards,. -A.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/325
https://github.com/google/deepvariant/issues/325:258,performance,disk,disk,258,"Hi Gunjan,. and thanks a lot for your reply. Yes, the directories where all accessible and under ```root``` (I know it can be done better....). Indeed when I added to the docker command the option ``` --user root``` the error changed and was very clear: ``` disk full```. It remains to me to free disk or change docker location, add a docker group to allow users run it without ```sudo``` and rerun it. I am pretty confident it should work. With Regards,. -A.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/325
https://github.com/google/deepvariant/issues/325:297,performance,disk,disk,297,"Hi Gunjan,. and thanks a lot for your reply. Yes, the directories where all accessible and under ```root``` (I know it can be done better....). Indeed when I added to the docker command the option ``` --user root``` the error changed and was very clear: ``` disk full```. It remains to me to free disk or change docker location, add a docker group to allow users run it without ```sudo``` and rerun it. I am pretty confident it should work. With Regards,. -A.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/325
https://github.com/google/deepvariant/issues/325:220,safety,error,error,220,"Hi Gunjan,. and thanks a lot for your reply. Yes, the directories where all accessible and under ```root``` (I know it can be done better....). Indeed when I added to the docker command the option ``` --user root``` the error changed and was very clear: ``` disk full```. It remains to me to free disk or change docker location, add a docker group to allow users run it without ```sudo``` and rerun it. I am pretty confident it should work. With Regards,. -A.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/325
https://github.com/google/deepvariant/issues/325:76,security,access,accessible,76,"Hi Gunjan,. and thanks a lot for your reply. Yes, the directories where all accessible and under ```root``` (I know it can be done better....). Indeed when I added to the docker command the option ``` --user root``` the error changed and was very clear: ``` disk full```. It remains to me to free disk or change docker location, add a docker group to allow users run it without ```sudo``` and rerun it. I am pretty confident it should work. With Regards,. -A.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/325
https://github.com/google/deepvariant/issues/325:178,usability,command,command,178,"Hi Gunjan,. and thanks a lot for your reply. Yes, the directories where all accessible and under ```root``` (I know it can be done better....). Indeed when I added to the docker command the option ``` --user root``` the error changed and was very clear: ``` disk full```. It remains to me to free disk or change docker location, add a docker group to allow users run it without ```sudo``` and rerun it. I am pretty confident it should work. With Regards,. -A.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/325
https://github.com/google/deepvariant/issues/325:203,usability,user,user,203,"Hi Gunjan,. and thanks a lot for your reply. Yes, the directories where all accessible and under ```root``` (I know it can be done better....). Indeed when I added to the docker command the option ``` --user root``` the error changed and was very clear: ``` disk full```. It remains to me to free disk or change docker location, add a docker group to allow users run it without ```sudo``` and rerun it. I am pretty confident it should work. With Regards,. -A.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/325
https://github.com/google/deepvariant/issues/325:220,usability,error,error,220,"Hi Gunjan,. and thanks a lot for your reply. Yes, the directories where all accessible and under ```root``` (I know it can be done better....). Indeed when I added to the docker command the option ``` --user root``` the error changed and was very clear: ``` disk full```. It remains to me to free disk or change docker location, add a docker group to allow users run it without ```sudo``` and rerun it. I am pretty confident it should work. With Regards,. -A.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/325
https://github.com/google/deepvariant/issues/325:247,usability,clear,clear,247,"Hi Gunjan,. and thanks a lot for your reply. Yes, the directories where all accessible and under ```root``` (I know it can be done better....). Indeed when I added to the docker command the option ``` --user root``` the error changed and was very clear: ``` disk full```. It remains to me to free disk or change docker location, add a docker group to allow users run it without ```sudo``` and rerun it. I am pretty confident it should work. With Regards,. -A.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/325
https://github.com/google/deepvariant/issues/325:357,usability,user,users,357,"Hi Gunjan,. and thanks a lot for your reply. Yes, the directories where all accessible and under ```root``` (I know it can be done better....). Indeed when I added to the docker command the option ``` --user root``` the error changed and was very clear: ``` disk full```. It remains to me to free disk or change docker location, add a docker group to allow users run it without ```sudo``` and rerun it. I am pretty confident it should work. With Regards,. -A.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/325
https://github.com/google/deepvariant/issues/325:41,usability,close,close,41,"Glad to hear you found the problem! I'll close this issue, but feel free to reopen it if you have any other questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/325
https://github.com/google/deepvariant/issues/326:52,integrability,filter,filtering,52,"To give you more perspective, here is the effect of filtering, with filters >= GQ (each dot represents one of my 10 samples). ![allGQ_private_crop_github](https://user-images.githubusercontent.com/23341393/87682475-280e7980-c780-11ea-8c5e-53b487305598.PNG).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/326
https://github.com/google/deepvariant/issues/326:68,integrability,filter,filters,68,"To give you more perspective, here is the effect of filtering, with filters >= GQ (each dot represents one of my 10 samples). ![allGQ_private_crop_github](https://user-images.githubusercontent.com/23341393/87682475-280e7980-c780-11ea-8c5e-53b487305598.PNG).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/326
https://github.com/google/deepvariant/issues/326:163,usability,user,user-images,163,"To give you more perspective, here is the effect of filtering, with filters >= GQ (each dot represents one of my 10 samples). ![allGQ_private_crop_github](https://user-images.githubusercontent.com/23341393/87682475-280e7980-c780-11ea-8c5e-53b487305598.PNG).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/326
https://github.com/google/deepvariant/issues/326:1039,availability,down,download,1039,"Hi @aderzelle, thank you for your question. I believe this is coming from the cohort merging step (by GLnexus) assuming that you used `--config DeepVariantWGS` or `--config DeepVariant` for merging, not the DeepVariant itself. The merging step by GLnexus includes filtering alleles and revising genotypes and we have conducted an extensive study [1] on finding the best merging parameters for DeepVariant outputs, which was later pushed to the open-source GLnexus. The current version of the merging parameters uses `min_AQ1 = min_AQ2 = 10` (AQ means ""allele quality"") as you can see here https://github.com/dnanexus-rnd/GLnexus/blob/4d057dcf24b68b33de7a9759ae65ca2b144a3d47/src/cli_utils.cc#L874 The definitions of these parameters can be found at https://github.com/dnanexus-rnd/GLnexus/wiki/Configuration . I believe this is the reason why you are seeing the sharp drop at `GQ = 10` as the alleles corresponding to them were already filtered. If you'd like to merge DeepVariant gVCFs without any filtering or genotype revision, you can download the `.yml` file here https://gist.github.com/tedyun/1d4f57ca67fb18647b7b251f9e0b35c2 and use `--config DeepVariant_nomod.yml` instead when running GLnexus. I hope this helps and please let us know if you have any more questions/comments. [1] https://doi.org/10.1101/2020.02.10.942086. Best,. Ted.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/326
https://github.com/google/deepvariant/issues/326:477,deployability,version,version,477,"Hi @aderzelle, thank you for your question. I believe this is coming from the cohort merging step (by GLnexus) assuming that you used `--config DeepVariantWGS` or `--config DeepVariant` for merging, not the DeepVariant itself. The merging step by GLnexus includes filtering alleles and revising genotypes and we have conducted an extensive study [1] on finding the best merging parameters for DeepVariant outputs, which was later pushed to the open-source GLnexus. The current version of the merging parameters uses `min_AQ1 = min_AQ2 = 10` (AQ means ""allele quality"") as you can see here https://github.com/dnanexus-rnd/GLnexus/blob/4d057dcf24b68b33de7a9759ae65ca2b144a3d47/src/cli_utils.cc#L874 The definitions of these parameters can be found at https://github.com/dnanexus-rnd/GLnexus/wiki/Configuration . I believe this is the reason why you are seeing the sharp drop at `GQ = 10` as the alleles corresponding to them were already filtered. If you'd like to merge DeepVariant gVCFs without any filtering or genotype revision, you can download the `.yml` file here https://gist.github.com/tedyun/1d4f57ca67fb18647b7b251f9e0b35c2 and use `--config DeepVariant_nomod.yml` instead when running GLnexus. I hope this helps and please let us know if you have any more questions/comments. [1] https://doi.org/10.1101/2020.02.10.942086. Best,. Ted.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/326
https://github.com/google/deepvariant/issues/326:794,deployability,Configurat,Configuration,794,"Hi @aderzelle, thank you for your question. I believe this is coming from the cohort merging step (by GLnexus) assuming that you used `--config DeepVariantWGS` or `--config DeepVariant` for merging, not the DeepVariant itself. The merging step by GLnexus includes filtering alleles and revising genotypes and we have conducted an extensive study [1] on finding the best merging parameters for DeepVariant outputs, which was later pushed to the open-source GLnexus. The current version of the merging parameters uses `min_AQ1 = min_AQ2 = 10` (AQ means ""allele quality"") as you can see here https://github.com/dnanexus-rnd/GLnexus/blob/4d057dcf24b68b33de7a9759ae65ca2b144a3d47/src/cli_utils.cc#L874 The definitions of these parameters can be found at https://github.com/dnanexus-rnd/GLnexus/wiki/Configuration . I believe this is the reason why you are seeing the sharp drop at `GQ = 10` as the alleles corresponding to them were already filtered. If you'd like to merge DeepVariant gVCFs without any filtering or genotype revision, you can download the `.yml` file here https://gist.github.com/tedyun/1d4f57ca67fb18647b7b251f9e0b35c2 and use `--config DeepVariant_nomod.yml` instead when running GLnexus. I hope this helps and please let us know if you have any more questions/comments. [1] https://doi.org/10.1101/2020.02.10.942086. Best,. Ted.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/326
https://github.com/google/deepvariant/issues/326:469,energy efficiency,current,current,469,"Hi @aderzelle, thank you for your question. I believe this is coming from the cohort merging step (by GLnexus) assuming that you used `--config DeepVariantWGS` or `--config DeepVariant` for merging, not the DeepVariant itself. The merging step by GLnexus includes filtering alleles and revising genotypes and we have conducted an extensive study [1] on finding the best merging parameters for DeepVariant outputs, which was later pushed to the open-source GLnexus. The current version of the merging parameters uses `min_AQ1 = min_AQ2 = 10` (AQ means ""allele quality"") as you can see here https://github.com/dnanexus-rnd/GLnexus/blob/4d057dcf24b68b33de7a9759ae65ca2b144a3d47/src/cli_utils.cc#L874 The definitions of these parameters can be found at https://github.com/dnanexus-rnd/GLnexus/wiki/Configuration . I believe this is the reason why you are seeing the sharp drop at `GQ = 10` as the alleles corresponding to them were already filtered. If you'd like to merge DeepVariant gVCFs without any filtering or genotype revision, you can download the `.yml` file here https://gist.github.com/tedyun/1d4f57ca67fb18647b7b251f9e0b35c2 and use `--config DeepVariant_nomod.yml` instead when running GLnexus. I hope this helps and please let us know if you have any more questions/comments. [1] https://doi.org/10.1101/2020.02.10.942086. Best,. Ted.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/326
https://github.com/google/deepvariant/issues/326:264,integrability,filter,filtering,264,"Hi @aderzelle, thank you for your question. I believe this is coming from the cohort merging step (by GLnexus) assuming that you used `--config DeepVariantWGS` or `--config DeepVariant` for merging, not the DeepVariant itself. The merging step by GLnexus includes filtering alleles and revising genotypes and we have conducted an extensive study [1] on finding the best merging parameters for DeepVariant outputs, which was later pushed to the open-source GLnexus. The current version of the merging parameters uses `min_AQ1 = min_AQ2 = 10` (AQ means ""allele quality"") as you can see here https://github.com/dnanexus-rnd/GLnexus/blob/4d057dcf24b68b33de7a9759ae65ca2b144a3d47/src/cli_utils.cc#L874 The definitions of these parameters can be found at https://github.com/dnanexus-rnd/GLnexus/wiki/Configuration . I believe this is the reason why you are seeing the sharp drop at `GQ = 10` as the alleles corresponding to them were already filtered. If you'd like to merge DeepVariant gVCFs without any filtering or genotype revision, you can download the `.yml` file here https://gist.github.com/tedyun/1d4f57ca67fb18647b7b251f9e0b35c2 and use `--config DeepVariant_nomod.yml` instead when running GLnexus. I hope this helps and please let us know if you have any more questions/comments. [1] https://doi.org/10.1101/2020.02.10.942086. Best,. Ted.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/326
https://github.com/google/deepvariant/issues/326:477,integrability,version,version,477,"Hi @aderzelle, thank you for your question. I believe this is coming from the cohort merging step (by GLnexus) assuming that you used `--config DeepVariantWGS` or `--config DeepVariant` for merging, not the DeepVariant itself. The merging step by GLnexus includes filtering alleles and revising genotypes and we have conducted an extensive study [1] on finding the best merging parameters for DeepVariant outputs, which was later pushed to the open-source GLnexus. The current version of the merging parameters uses `min_AQ1 = min_AQ2 = 10` (AQ means ""allele quality"") as you can see here https://github.com/dnanexus-rnd/GLnexus/blob/4d057dcf24b68b33de7a9759ae65ca2b144a3d47/src/cli_utils.cc#L874 The definitions of these parameters can be found at https://github.com/dnanexus-rnd/GLnexus/wiki/Configuration . I believe this is the reason why you are seeing the sharp drop at `GQ = 10` as the alleles corresponding to them were already filtered. If you'd like to merge DeepVariant gVCFs without any filtering or genotype revision, you can download the `.yml` file here https://gist.github.com/tedyun/1d4f57ca67fb18647b7b251f9e0b35c2 and use `--config DeepVariant_nomod.yml` instead when running GLnexus. I hope this helps and please let us know if you have any more questions/comments. [1] https://doi.org/10.1101/2020.02.10.942086. Best,. Ted.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/326
https://github.com/google/deepvariant/issues/326:794,integrability,Configur,Configuration,794,"Hi @aderzelle, thank you for your question. I believe this is coming from the cohort merging step (by GLnexus) assuming that you used `--config DeepVariantWGS` or `--config DeepVariant` for merging, not the DeepVariant itself. The merging step by GLnexus includes filtering alleles and revising genotypes and we have conducted an extensive study [1] on finding the best merging parameters for DeepVariant outputs, which was later pushed to the open-source GLnexus. The current version of the merging parameters uses `min_AQ1 = min_AQ2 = 10` (AQ means ""allele quality"") as you can see here https://github.com/dnanexus-rnd/GLnexus/blob/4d057dcf24b68b33de7a9759ae65ca2b144a3d47/src/cli_utils.cc#L874 The definitions of these parameters can be found at https://github.com/dnanexus-rnd/GLnexus/wiki/Configuration . I believe this is the reason why you are seeing the sharp drop at `GQ = 10` as the alleles corresponding to them were already filtered. If you'd like to merge DeepVariant gVCFs without any filtering or genotype revision, you can download the `.yml` file here https://gist.github.com/tedyun/1d4f57ca67fb18647b7b251f9e0b35c2 and use `--config DeepVariant_nomod.yml` instead when running GLnexus. I hope this helps and please let us know if you have any more questions/comments. [1] https://doi.org/10.1101/2020.02.10.942086. Best,. Ted.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/326
https://github.com/google/deepvariant/issues/326:936,integrability,filter,filtered,936,"Hi @aderzelle, thank you for your question. I believe this is coming from the cohort merging step (by GLnexus) assuming that you used `--config DeepVariantWGS` or `--config DeepVariant` for merging, not the DeepVariant itself. The merging step by GLnexus includes filtering alleles and revising genotypes and we have conducted an extensive study [1] on finding the best merging parameters for DeepVariant outputs, which was later pushed to the open-source GLnexus. The current version of the merging parameters uses `min_AQ1 = min_AQ2 = 10` (AQ means ""allele quality"") as you can see here https://github.com/dnanexus-rnd/GLnexus/blob/4d057dcf24b68b33de7a9759ae65ca2b144a3d47/src/cli_utils.cc#L874 The definitions of these parameters can be found at https://github.com/dnanexus-rnd/GLnexus/wiki/Configuration . I believe this is the reason why you are seeing the sharp drop at `GQ = 10` as the alleles corresponding to them were already filtered. If you'd like to merge DeepVariant gVCFs without any filtering or genotype revision, you can download the `.yml` file here https://gist.github.com/tedyun/1d4f57ca67fb18647b7b251f9e0b35c2 and use `--config DeepVariant_nomod.yml` instead when running GLnexus. I hope this helps and please let us know if you have any more questions/comments. [1] https://doi.org/10.1101/2020.02.10.942086. Best,. Ted.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/326
https://github.com/google/deepvariant/issues/326:999,integrability,filter,filtering,999,"Hi @aderzelle, thank you for your question. I believe this is coming from the cohort merging step (by GLnexus) assuming that you used `--config DeepVariantWGS` or `--config DeepVariant` for merging, not the DeepVariant itself. The merging step by GLnexus includes filtering alleles and revising genotypes and we have conducted an extensive study [1] on finding the best merging parameters for DeepVariant outputs, which was later pushed to the open-source GLnexus. The current version of the merging parameters uses `min_AQ1 = min_AQ2 = 10` (AQ means ""allele quality"") as you can see here https://github.com/dnanexus-rnd/GLnexus/blob/4d057dcf24b68b33de7a9759ae65ca2b144a3d47/src/cli_utils.cc#L874 The definitions of these parameters can be found at https://github.com/dnanexus-rnd/GLnexus/wiki/Configuration . I believe this is the reason why you are seeing the sharp drop at `GQ = 10` as the alleles corresponding to them were already filtered. If you'd like to merge DeepVariant gVCFs without any filtering or genotype revision, you can download the `.yml` file here https://gist.github.com/tedyun/1d4f57ca67fb18647b7b251f9e0b35c2 and use `--config DeepVariant_nomod.yml` instead when running GLnexus. I hope this helps and please let us know if you have any more questions/comments. [1] https://doi.org/10.1101/2020.02.10.942086. Best,. Ted.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/326
https://github.com/google/deepvariant/issues/326:330,modifiability,extens,extensive,330,"Hi @aderzelle, thank you for your question. I believe this is coming from the cohort merging step (by GLnexus) assuming that you used `--config DeepVariantWGS` or `--config DeepVariant` for merging, not the DeepVariant itself. The merging step by GLnexus includes filtering alleles and revising genotypes and we have conducted an extensive study [1] on finding the best merging parameters for DeepVariant outputs, which was later pushed to the open-source GLnexus. The current version of the merging parameters uses `min_AQ1 = min_AQ2 = 10` (AQ means ""allele quality"") as you can see here https://github.com/dnanexus-rnd/GLnexus/blob/4d057dcf24b68b33de7a9759ae65ca2b144a3d47/src/cli_utils.cc#L874 The definitions of these parameters can be found at https://github.com/dnanexus-rnd/GLnexus/wiki/Configuration . I believe this is the reason why you are seeing the sharp drop at `GQ = 10` as the alleles corresponding to them were already filtered. If you'd like to merge DeepVariant gVCFs without any filtering or genotype revision, you can download the `.yml` file here https://gist.github.com/tedyun/1d4f57ca67fb18647b7b251f9e0b35c2 and use `--config DeepVariant_nomod.yml` instead when running GLnexus. I hope this helps and please let us know if you have any more questions/comments. [1] https://doi.org/10.1101/2020.02.10.942086. Best,. Ted.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/326
https://github.com/google/deepvariant/issues/326:378,modifiability,paramet,parameters,378,"Hi @aderzelle, thank you for your question. I believe this is coming from the cohort merging step (by GLnexus) assuming that you used `--config DeepVariantWGS` or `--config DeepVariant` for merging, not the DeepVariant itself. The merging step by GLnexus includes filtering alleles and revising genotypes and we have conducted an extensive study [1] on finding the best merging parameters for DeepVariant outputs, which was later pushed to the open-source GLnexus. The current version of the merging parameters uses `min_AQ1 = min_AQ2 = 10` (AQ means ""allele quality"") as you can see here https://github.com/dnanexus-rnd/GLnexus/blob/4d057dcf24b68b33de7a9759ae65ca2b144a3d47/src/cli_utils.cc#L874 The definitions of these parameters can be found at https://github.com/dnanexus-rnd/GLnexus/wiki/Configuration . I believe this is the reason why you are seeing the sharp drop at `GQ = 10` as the alleles corresponding to them were already filtered. If you'd like to merge DeepVariant gVCFs without any filtering or genotype revision, you can download the `.yml` file here https://gist.github.com/tedyun/1d4f57ca67fb18647b7b251f9e0b35c2 and use `--config DeepVariant_nomod.yml` instead when running GLnexus. I hope this helps and please let us know if you have any more questions/comments. [1] https://doi.org/10.1101/2020.02.10.942086. Best,. Ted.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/326
https://github.com/google/deepvariant/issues/326:477,modifiability,version,version,477,"Hi @aderzelle, thank you for your question. I believe this is coming from the cohort merging step (by GLnexus) assuming that you used `--config DeepVariantWGS` or `--config DeepVariant` for merging, not the DeepVariant itself. The merging step by GLnexus includes filtering alleles and revising genotypes and we have conducted an extensive study [1] on finding the best merging parameters for DeepVariant outputs, which was later pushed to the open-source GLnexus. The current version of the merging parameters uses `min_AQ1 = min_AQ2 = 10` (AQ means ""allele quality"") as you can see here https://github.com/dnanexus-rnd/GLnexus/blob/4d057dcf24b68b33de7a9759ae65ca2b144a3d47/src/cli_utils.cc#L874 The definitions of these parameters can be found at https://github.com/dnanexus-rnd/GLnexus/wiki/Configuration . I believe this is the reason why you are seeing the sharp drop at `GQ = 10` as the alleles corresponding to them were already filtered. If you'd like to merge DeepVariant gVCFs without any filtering or genotype revision, you can download the `.yml` file here https://gist.github.com/tedyun/1d4f57ca67fb18647b7b251f9e0b35c2 and use `--config DeepVariant_nomod.yml` instead when running GLnexus. I hope this helps and please let us know if you have any more questions/comments. [1] https://doi.org/10.1101/2020.02.10.942086. Best,. Ted.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/326
https://github.com/google/deepvariant/issues/326:500,modifiability,paramet,parameters,500,"Hi @aderzelle, thank you for your question. I believe this is coming from the cohort merging step (by GLnexus) assuming that you used `--config DeepVariantWGS` or `--config DeepVariant` for merging, not the DeepVariant itself. The merging step by GLnexus includes filtering alleles and revising genotypes and we have conducted an extensive study [1] on finding the best merging parameters for DeepVariant outputs, which was later pushed to the open-source GLnexus. The current version of the merging parameters uses `min_AQ1 = min_AQ2 = 10` (AQ means ""allele quality"") as you can see here https://github.com/dnanexus-rnd/GLnexus/blob/4d057dcf24b68b33de7a9759ae65ca2b144a3d47/src/cli_utils.cc#L874 The definitions of these parameters can be found at https://github.com/dnanexus-rnd/GLnexus/wiki/Configuration . I believe this is the reason why you are seeing the sharp drop at `GQ = 10` as the alleles corresponding to them were already filtered. If you'd like to merge DeepVariant gVCFs without any filtering or genotype revision, you can download the `.yml` file here https://gist.github.com/tedyun/1d4f57ca67fb18647b7b251f9e0b35c2 and use `--config DeepVariant_nomod.yml` instead when running GLnexus. I hope this helps and please let us know if you have any more questions/comments. [1] https://doi.org/10.1101/2020.02.10.942086. Best,. Ted.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/326
https://github.com/google/deepvariant/issues/326:722,modifiability,paramet,parameters,722,"Hi @aderzelle, thank you for your question. I believe this is coming from the cohort merging step (by GLnexus) assuming that you used `--config DeepVariantWGS` or `--config DeepVariant` for merging, not the DeepVariant itself. The merging step by GLnexus includes filtering alleles and revising genotypes and we have conducted an extensive study [1] on finding the best merging parameters for DeepVariant outputs, which was later pushed to the open-source GLnexus. The current version of the merging parameters uses `min_AQ1 = min_AQ2 = 10` (AQ means ""allele quality"") as you can see here https://github.com/dnanexus-rnd/GLnexus/blob/4d057dcf24b68b33de7a9759ae65ca2b144a3d47/src/cli_utils.cc#L874 The definitions of these parameters can be found at https://github.com/dnanexus-rnd/GLnexus/wiki/Configuration . I believe this is the reason why you are seeing the sharp drop at `GQ = 10` as the alleles corresponding to them were already filtered. If you'd like to merge DeepVariant gVCFs without any filtering or genotype revision, you can download the `.yml` file here https://gist.github.com/tedyun/1d4f57ca67fb18647b7b251f9e0b35c2 and use `--config DeepVariant_nomod.yml` instead when running GLnexus. I hope this helps and please let us know if you have any more questions/comments. [1] https://doi.org/10.1101/2020.02.10.942086. Best,. Ted.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/326
https://github.com/google/deepvariant/issues/326:794,modifiability,Configur,Configuration,794,"Hi @aderzelle, thank you for your question. I believe this is coming from the cohort merging step (by GLnexus) assuming that you used `--config DeepVariantWGS` or `--config DeepVariant` for merging, not the DeepVariant itself. The merging step by GLnexus includes filtering alleles and revising genotypes and we have conducted an extensive study [1] on finding the best merging parameters for DeepVariant outputs, which was later pushed to the open-source GLnexus. The current version of the merging parameters uses `min_AQ1 = min_AQ2 = 10` (AQ means ""allele quality"") as you can see here https://github.com/dnanexus-rnd/GLnexus/blob/4d057dcf24b68b33de7a9759ae65ca2b144a3d47/src/cli_utils.cc#L874 The definitions of these parameters can be found at https://github.com/dnanexus-rnd/GLnexus/wiki/Configuration . I believe this is the reason why you are seeing the sharp drop at `GQ = 10` as the alleles corresponding to them were already filtered. If you'd like to merge DeepVariant gVCFs without any filtering or genotype revision, you can download the `.yml` file here https://gist.github.com/tedyun/1d4f57ca67fb18647b7b251f9e0b35c2 and use `--config DeepVariant_nomod.yml` instead when running GLnexus. I hope this helps and please let us know if you have any more questions/comments. [1] https://doi.org/10.1101/2020.02.10.942086. Best,. Ted.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/326
https://github.com/google/deepvariant/issues/326:794,security,Configur,Configuration,794,"Hi @aderzelle, thank you for your question. I believe this is coming from the cohort merging step (by GLnexus) assuming that you used `--config DeepVariantWGS` or `--config DeepVariant` for merging, not the DeepVariant itself. The merging step by GLnexus includes filtering alleles and revising genotypes and we have conducted an extensive study [1] on finding the best merging parameters for DeepVariant outputs, which was later pushed to the open-source GLnexus. The current version of the merging parameters uses `min_AQ1 = min_AQ2 = 10` (AQ means ""allele quality"") as you can see here https://github.com/dnanexus-rnd/GLnexus/blob/4d057dcf24b68b33de7a9759ae65ca2b144a3d47/src/cli_utils.cc#L874 The definitions of these parameters can be found at https://github.com/dnanexus-rnd/GLnexus/wiki/Configuration . I believe this is the reason why you are seeing the sharp drop at `GQ = 10` as the alleles corresponding to them were already filtered. If you'd like to merge DeepVariant gVCFs without any filtering or genotype revision, you can download the `.yml` file here https://gist.github.com/tedyun/1d4f57ca67fb18647b7b251f9e0b35c2 and use `--config DeepVariant_nomod.yml` instead when running GLnexus. I hope this helps and please let us know if you have any more questions/comments. [1] https://doi.org/10.1101/2020.02.10.942086. Best,. Ted.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/326
https://github.com/google/deepvariant/issues/326:1216,usability,help,helps,1216,"Hi @aderzelle, thank you for your question. I believe this is coming from the cohort merging step (by GLnexus) assuming that you used `--config DeepVariantWGS` or `--config DeepVariant` for merging, not the DeepVariant itself. The merging step by GLnexus includes filtering alleles and revising genotypes and we have conducted an extensive study [1] on finding the best merging parameters for DeepVariant outputs, which was later pushed to the open-source GLnexus. The current version of the merging parameters uses `min_AQ1 = min_AQ2 = 10` (AQ means ""allele quality"") as you can see here https://github.com/dnanexus-rnd/GLnexus/blob/4d057dcf24b68b33de7a9759ae65ca2b144a3d47/src/cli_utils.cc#L874 The definitions of these parameters can be found at https://github.com/dnanexus-rnd/GLnexus/wiki/Configuration . I believe this is the reason why you are seeing the sharp drop at `GQ = 10` as the alleles corresponding to them were already filtered. If you'd like to merge DeepVariant gVCFs without any filtering or genotype revision, you can download the `.yml` file here https://gist.github.com/tedyun/1d4f57ca67fb18647b7b251f9e0b35c2 and use `--config DeepVariant_nomod.yml` instead when running GLnexus. I hope this helps and please let us know if you have any more questions/comments. [1] https://doi.org/10.1101/2020.02.10.942086. Best,. Ted.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/326
https://github.com/google/deepvariant/issues/326:281,deployability,observ,observed,281,"Thanks for the very helpful answer. . So, actually what I am seeing is the distribution for ""private"" seems to be centred on 10 and has been amputated of its left part by GLnexus. Interesting ... . From GLnexus definition . > Thus we may have a lower quality threshold for alleles observed in multiple individuals, compared to singletons. That means we should be more stringent on quality filtering for singletons since they are not supported by observations in more than one individual, right?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/326
https://github.com/google/deepvariant/issues/326:446,deployability,observ,observations,446,"Thanks for the very helpful answer. . So, actually what I am seeing is the distribution for ""private"" seems to be centred on 10 and has been amputated of its left part by GLnexus. Interesting ... . From GLnexus definition . > Thus we may have a lower quality threshold for alleles observed in multiple individuals, compared to singletons. That means we should be more stringent on quality filtering for singletons since they are not supported by observations in more than one individual, right?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/326
https://github.com/google/deepvariant/issues/326:389,integrability,filter,filtering,389,"Thanks for the very helpful answer. . So, actually what I am seeing is the distribution for ""private"" seems to be centred on 10 and has been amputated of its left part by GLnexus. Interesting ... . From GLnexus definition . > Thus we may have a lower quality threshold for alleles observed in multiple individuals, compared to singletons. That means we should be more stringent on quality filtering for singletons since they are not supported by observations in more than one individual, right?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/326
https://github.com/google/deepvariant/issues/326:75,interoperability,distribut,distribution,75,"Thanks for the very helpful answer. . So, actually what I am seeing is the distribution for ""private"" seems to be centred on 10 and has been amputated of its left part by GLnexus. Interesting ... . From GLnexus definition . > Thus we may have a lower quality threshold for alleles observed in multiple individuals, compared to singletons. That means we should be more stringent on quality filtering for singletons since they are not supported by observations in more than one individual, right?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/326
https://github.com/google/deepvariant/issues/326:281,testability,observ,observed,281,"Thanks for the very helpful answer. . So, actually what I am seeing is the distribution for ""private"" seems to be centred on 10 and has been amputated of its left part by GLnexus. Interesting ... . From GLnexus definition . > Thus we may have a lower quality threshold for alleles observed in multiple individuals, compared to singletons. That means we should be more stringent on quality filtering for singletons since they are not supported by observations in more than one individual, right?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/326
https://github.com/google/deepvariant/issues/326:446,testability,observ,observations,446,"Thanks for the very helpful answer. . So, actually what I am seeing is the distribution for ""private"" seems to be centred on 10 and has been amputated of its left part by GLnexus. Interesting ... . From GLnexus definition . > Thus we may have a lower quality threshold for alleles observed in multiple individuals, compared to singletons. That means we should be more stringent on quality filtering for singletons since they are not supported by observations in more than one individual, right?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/326
https://github.com/google/deepvariant/issues/326:20,usability,help,helpful,20,"Thanks for the very helpful answer. . So, actually what I am seeing is the distribution for ""private"" seems to be centred on 10 and has been amputated of its left part by GLnexus. Interesting ... . From GLnexus definition . > Thus we may have a lower quality threshold for alleles observed in multiple individuals, compared to singletons. That means we should be more stringent on quality filtering for singletons since they are not supported by observations in more than one individual, right?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/326
https://github.com/google/deepvariant/issues/326:433,usability,support,supported,433,"Thanks for the very helpful answer. . So, actually what I am seeing is the distribution for ""private"" seems to be centred on 10 and has been amputated of its left part by GLnexus. Interesting ... . From GLnexus definition . > Thus we may have a lower quality threshold for alleles observed in multiple individuals, compared to singletons. That means we should be more stringent on quality filtering for singletons since they are not supported by observations in more than one individual, right?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/326
https://github.com/google/deepvariant/issues/326:198,deployability,Depend,Depending,198,"We can be (that would imply `min_AQ1 > min_AQ2`), but based on our investigation the current settings were chosen to give the best overall quality for variety of cohort sizes and sequencing depths. Depending on how the cohort VCF is used, one may want to apply additional filters to the cohort, which can either be applied _after_ merging using a standard VCF modification tool (e.g. bcftools), or by changing the merging settings directly in the .yml file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/326
https://github.com/google/deepvariant/issues/326:85,energy efficiency,current,current,85,"We can be (that would imply `min_AQ1 > min_AQ2`), but based on our investigation the current settings were chosen to give the best overall quality for variety of cohort sizes and sequencing depths. Depending on how the cohort VCF is used, one may want to apply additional filters to the cohort, which can either be applied _after_ merging using a standard VCF modification tool (e.g. bcftools), or by changing the merging settings directly in the .yml file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/326
https://github.com/google/deepvariant/issues/326:198,integrability,Depend,Depending,198,"We can be (that would imply `min_AQ1 > min_AQ2`), but based on our investigation the current settings were chosen to give the best overall quality for variety of cohort sizes and sequencing depths. Depending on how the cohort VCF is used, one may want to apply additional filters to the cohort, which can either be applied _after_ merging using a standard VCF modification tool (e.g. bcftools), or by changing the merging settings directly in the .yml file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/326
https://github.com/google/deepvariant/issues/326:272,integrability,filter,filters,272,"We can be (that would imply `min_AQ1 > min_AQ2`), but based on our investigation the current settings were chosen to give the best overall quality for variety of cohort sizes and sequencing depths. Depending on how the cohort VCF is used, one may want to apply additional filters to the cohort, which can either be applied _after_ merging using a standard VCF modification tool (e.g. bcftools), or by changing the merging settings directly in the .yml file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/326
https://github.com/google/deepvariant/issues/326:347,interoperability,standard,standard,347,"We can be (that would imply `min_AQ1 > min_AQ2`), but based on our investigation the current settings were chosen to give the best overall quality for variety of cohort sizes and sequencing depths. Depending on how the cohort VCF is used, one may want to apply additional filters to the cohort, which can either be applied _after_ merging using a standard VCF modification tool (e.g. bcftools), or by changing the merging settings directly in the .yml file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/326
https://github.com/google/deepvariant/issues/326:198,modifiability,Depend,Depending,198,"We can be (that would imply `min_AQ1 > min_AQ2`), but based on our investigation the current settings were chosen to give the best overall quality for variety of cohort sizes and sequencing depths. Depending on how the cohort VCF is used, one may want to apply additional filters to the cohort, which can either be applied _after_ merging using a standard VCF modification tool (e.g. bcftools), or by changing the merging settings directly in the .yml file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/326
https://github.com/google/deepvariant/issues/326:198,safety,Depend,Depending,198,"We can be (that would imply `min_AQ1 > min_AQ2`), but based on our investigation the current settings were chosen to give the best overall quality for variety of cohort sizes and sequencing depths. Depending on how the cohort VCF is used, one may want to apply additional filters to the cohort, which can either be applied _after_ merging using a standard VCF modification tool (e.g. bcftools), or by changing the merging settings directly in the .yml file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/326
https://github.com/google/deepvariant/issues/326:360,security,modif,modification,360,"We can be (that would imply `min_AQ1 > min_AQ2`), but based on our investigation the current settings were chosen to give the best overall quality for variety of cohort sizes and sequencing depths. Depending on how the cohort VCF is used, one may want to apply additional filters to the cohort, which can either be applied _after_ merging using a standard VCF modification tool (e.g. bcftools), or by changing the merging settings directly in the .yml file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/326
https://github.com/google/deepvariant/issues/326:198,testability,Depend,Depending,198,"We can be (that would imply `min_AQ1 > min_AQ2`), but based on our investigation the current settings were chosen to give the best overall quality for variety of cohort sizes and sequencing depths. Depending on how the cohort VCF is used, one may want to apply additional filters to the cohort, which can either be applied _after_ merging using a standard VCF modification tool (e.g. bcftools), or by changing the merging settings directly in the .yml file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/326
https://github.com/google/deepvariant/issues/326:373,usability,tool,tool,373,"We can be (that would imply `min_AQ1 > min_AQ2`), but based on our investigation the current settings were chosen to give the best overall quality for variety of cohort sizes and sequencing depths. Depending on how the cohort VCF is used, one may want to apply additional filters to the cohort, which can either be applied _after_ merging using a standard VCF modification tool (e.g. bcftools), or by changing the merging settings directly in the .yml file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/326
https://github.com/google/deepvariant/issues/326:224,availability,down,downloading,224,"FYI I submitted a pull request to GLnexus repo for the ""nomod"" preset for merging (no filters or genotype revision). https://github.com/dnanexus-rnd/GLnexus/pull/229 If this is accepted, you'll be able to try it out without downloading an external .yml file. @aderzelle Please let me know if you have any questions/comments related to this issue. If not, please feel free to close it :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/326
https://github.com/google/deepvariant/issues/326:6,integrability,sub,submitted,6,"FYI I submitted a pull request to GLnexus repo for the ""nomod"" preset for merging (no filters or genotype revision). https://github.com/dnanexus-rnd/GLnexus/pull/229 If this is accepted, you'll be able to try it out without downloading an external .yml file. @aderzelle Please let me know if you have any questions/comments related to this issue. If not, please feel free to close it :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/326
https://github.com/google/deepvariant/issues/326:86,integrability,filter,filters,86,"FYI I submitted a pull request to GLnexus repo for the ""nomod"" preset for merging (no filters or genotype revision). https://github.com/dnanexus-rnd/GLnexus/pull/229 If this is accepted, you'll be able to try it out without downloading an external .yml file. @aderzelle Please let me know if you have any questions/comments related to this issue. If not, please feel free to close it :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/326
https://github.com/google/deepvariant/issues/326:375,usability,close,close,375,"FYI I submitted a pull request to GLnexus repo for the ""nomod"" preset for merging (no filters or genotype revision). https://github.com/dnanexus-rnd/GLnexus/pull/229 If this is accepted, you'll be able to try it out without downloading an external .yml file. @aderzelle Please let me know if you have any questions/comments related to this issue. If not, please feel free to close it :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/326
https://github.com/google/deepvariant/issues/326:254,availability,down,downloading,254,"> FYI I submitted a pull request to GLnexus repo for the ""nomod"" preset for merging (no filters or genotype revision). [dnanexus-rnd/GLnexus#229](https://github.com/dnanexus-rnd/GLnexus/pull/229) If this is accepted, you'll be able to try it out without downloading an external .yml file. > . > @aderzelle Please let me know if you have any questions/comments related to this issue. If not, please feel free to close it :). Thanks for your detail explanation! I think I have all I need ;)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/326
https://github.com/google/deepvariant/issues/326:8,integrability,sub,submitted,8,"> FYI I submitted a pull request to GLnexus repo for the ""nomod"" preset for merging (no filters or genotype revision). [dnanexus-rnd/GLnexus#229](https://github.com/dnanexus-rnd/GLnexus/pull/229) If this is accepted, you'll be able to try it out without downloading an external .yml file. > . > @aderzelle Please let me know if you have any questions/comments related to this issue. If not, please feel free to close it :). Thanks for your detail explanation! I think I have all I need ;)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/326
https://github.com/google/deepvariant/issues/326:88,integrability,filter,filters,88,"> FYI I submitted a pull request to GLnexus repo for the ""nomod"" preset for merging (no filters or genotype revision). [dnanexus-rnd/GLnexus#229](https://github.com/dnanexus-rnd/GLnexus/pull/229) If this is accepted, you'll be able to try it out without downloading an external .yml file. > . > @aderzelle Please let me know if you have any questions/comments related to this issue. If not, please feel free to close it :). Thanks for your detail explanation! I think I have all I need ;)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/326
https://github.com/google/deepvariant/issues/326:411,usability,close,close,411,"> FYI I submitted a pull request to GLnexus repo for the ""nomod"" preset for merging (no filters or genotype revision). [dnanexus-rnd/GLnexus#229](https://github.com/dnanexus-rnd/GLnexus/pull/229) If this is accepted, you'll be able to try it out without downloading an external .yml file. > . > @aderzelle Please let me know if you have any questions/comments related to this issue. If not, please feel free to close it :). Thanks for your detail explanation! I think I have all I need ;)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/326
https://github.com/google/deepvariant/issues/328:824,availability,error,error-correction,824,"Hi @X1angyang . The model is InceptionV3. You can see the layers of one of the DeepVariant models like this:. ```. import tensorflow as tf. !gsutil cp gs://deepvariant/models/DeepVariant/0.10.0/DeepVariant-inception_v3-0.10.0+data-wgs_standard/model* /tmp/. checkpoint_path = '/tmp/model.ckpt'. reader = tf.compat.v1.train.NewCheckpointReader(checkpoint_path). shape_map_for_layers = reader.get_variable_to_shape_map(). print(shape_map_for_layers). ```. I just tested that in Colab (https://colab.research.google.com/). However, reimplementing all of DeepVariant from bam to output VCF would be a huge project. If you are interested in something smaller to get started, I'd like to bring this blog post to your attention: https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/. It has an associated Colab notebook that walks through some smaller but still challenging examples of how to use genomic data in machine learning using TensorFlow and Nucleus. I hope that helps! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/328
https://github.com/google/deepvariant/issues/328:20,energy efficiency,model,model,20,"Hi @X1angyang . The model is InceptionV3. You can see the layers of one of the DeepVariant models like this:. ```. import tensorflow as tf. !gsutil cp gs://deepvariant/models/DeepVariant/0.10.0/DeepVariant-inception_v3-0.10.0+data-wgs_standard/model* /tmp/. checkpoint_path = '/tmp/model.ckpt'. reader = tf.compat.v1.train.NewCheckpointReader(checkpoint_path). shape_map_for_layers = reader.get_variable_to_shape_map(). print(shape_map_for_layers). ```. I just tested that in Colab (https://colab.research.google.com/). However, reimplementing all of DeepVariant from bam to output VCF would be a huge project. If you are interested in something smaller to get started, I'd like to bring this blog post to your attention: https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/. It has an associated Colab notebook that walks through some smaller but still challenging examples of how to use genomic data in machine learning using TensorFlow and Nucleus. I hope that helps! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/328
https://github.com/google/deepvariant/issues/328:91,energy efficiency,model,models,91,"Hi @X1angyang . The model is InceptionV3. You can see the layers of one of the DeepVariant models like this:. ```. import tensorflow as tf. !gsutil cp gs://deepvariant/models/DeepVariant/0.10.0/DeepVariant-inception_v3-0.10.0+data-wgs_standard/model* /tmp/. checkpoint_path = '/tmp/model.ckpt'. reader = tf.compat.v1.train.NewCheckpointReader(checkpoint_path). shape_map_for_layers = reader.get_variable_to_shape_map(). print(shape_map_for_layers). ```. I just tested that in Colab (https://colab.research.google.com/). However, reimplementing all of DeepVariant from bam to output VCF would be a huge project. If you are interested in something smaller to get started, I'd like to bring this blog post to your attention: https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/. It has an associated Colab notebook that walks through some smaller but still challenging examples of how to use genomic data in machine learning using TensorFlow and Nucleus. I hope that helps! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/328
https://github.com/google/deepvariant/issues/328:168,energy efficiency,model,models,168,"Hi @X1angyang . The model is InceptionV3. You can see the layers of one of the DeepVariant models like this:. ```. import tensorflow as tf. !gsutil cp gs://deepvariant/models/DeepVariant/0.10.0/DeepVariant-inception_v3-0.10.0+data-wgs_standard/model* /tmp/. checkpoint_path = '/tmp/model.ckpt'. reader = tf.compat.v1.train.NewCheckpointReader(checkpoint_path). shape_map_for_layers = reader.get_variable_to_shape_map(). print(shape_map_for_layers). ```. I just tested that in Colab (https://colab.research.google.com/). However, reimplementing all of DeepVariant from bam to output VCF would be a huge project. If you are interested in something smaller to get started, I'd like to bring this blog post to your attention: https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/. It has an associated Colab notebook that walks through some smaller but still challenging examples of how to use genomic data in machine learning using TensorFlow and Nucleus. I hope that helps! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/328
https://github.com/google/deepvariant/issues/328:244,energy efficiency,model,model,244,"Hi @X1angyang . The model is InceptionV3. You can see the layers of one of the DeepVariant models like this:. ```. import tensorflow as tf. !gsutil cp gs://deepvariant/models/DeepVariant/0.10.0/DeepVariant-inception_v3-0.10.0+data-wgs_standard/model* /tmp/. checkpoint_path = '/tmp/model.ckpt'. reader = tf.compat.v1.train.NewCheckpointReader(checkpoint_path). shape_map_for_layers = reader.get_variable_to_shape_map(). print(shape_map_for_layers). ```. I just tested that in Colab (https://colab.research.google.com/). However, reimplementing all of DeepVariant from bam to output VCF would be a huge project. If you are interested in something smaller to get started, I'd like to bring this blog post to your attention: https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/. It has an associated Colab notebook that walks through some smaller but still challenging examples of how to use genomic data in machine learning using TensorFlow and Nucleus. I hope that helps! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/328
https://github.com/google/deepvariant/issues/328:282,energy efficiency,model,model,282,"Hi @X1angyang . The model is InceptionV3. You can see the layers of one of the DeepVariant models like this:. ```. import tensorflow as tf. !gsutil cp gs://deepvariant/models/DeepVariant/0.10.0/DeepVariant-inception_v3-0.10.0+data-wgs_standard/model* /tmp/. checkpoint_path = '/tmp/model.ckpt'. reader = tf.compat.v1.train.NewCheckpointReader(checkpoint_path). shape_map_for_layers = reader.get_variable_to_shape_map(). print(shape_map_for_layers). ```. I just tested that in Colab (https://colab.research.google.com/). However, reimplementing all of DeepVariant from bam to output VCF would be a huge project. If you are interested in something smaller to get started, I'd like to bring this blog post to your attention: https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/. It has an associated Colab notebook that walks through some smaller but still challenging examples of how to use genomic data in machine learning using TensorFlow and Nucleus. I hope that helps! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/328
https://github.com/google/deepvariant/issues/328:58,modifiability,layer,layers,58,"Hi @X1angyang . The model is InceptionV3. You can see the layers of one of the DeepVariant models like this:. ```. import tensorflow as tf. !gsutil cp gs://deepvariant/models/DeepVariant/0.10.0/DeepVariant-inception_v3-0.10.0+data-wgs_standard/model* /tmp/. checkpoint_path = '/tmp/model.ckpt'. reader = tf.compat.v1.train.NewCheckpointReader(checkpoint_path). shape_map_for_layers = reader.get_variable_to_shape_map(). print(shape_map_for_layers). ```. I just tested that in Colab (https://colab.research.google.com/). However, reimplementing all of DeepVariant from bam to output VCF would be a huge project. If you are interested in something smaller to get started, I'd like to bring this blog post to your attention: https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/. It has an associated Colab notebook that walks through some smaller but still challenging examples of how to use genomic data in machine learning using TensorFlow and Nucleus. I hope that helps! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/328
https://github.com/google/deepvariant/issues/328:824,performance,error,error-correction,824,"Hi @X1angyang . The model is InceptionV3. You can see the layers of one of the DeepVariant models like this:. ```. import tensorflow as tf. !gsutil cp gs://deepvariant/models/DeepVariant/0.10.0/DeepVariant-inception_v3-0.10.0+data-wgs_standard/model* /tmp/. checkpoint_path = '/tmp/model.ckpt'. reader = tf.compat.v1.train.NewCheckpointReader(checkpoint_path). shape_map_for_layers = reader.get_variable_to_shape_map(). print(shape_map_for_layers). ```. I just tested that in Colab (https://colab.research.google.com/). However, reimplementing all of DeepVariant from bam to output VCF would be a huge project. If you are interested in something smaller to get started, I'd like to bring this blog post to your attention: https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/. It has an associated Colab notebook that walks through some smaller but still challenging examples of how to use genomic data in machine learning using TensorFlow and Nucleus. I hope that helps! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/328
https://github.com/google/deepvariant/issues/328:461,safety,test,tested,461,"Hi @X1angyang . The model is InceptionV3. You can see the layers of one of the DeepVariant models like this:. ```. import tensorflow as tf. !gsutil cp gs://deepvariant/models/DeepVariant/0.10.0/DeepVariant-inception_v3-0.10.0+data-wgs_standard/model* /tmp/. checkpoint_path = '/tmp/model.ckpt'. reader = tf.compat.v1.train.NewCheckpointReader(checkpoint_path). shape_map_for_layers = reader.get_variable_to_shape_map(). print(shape_map_for_layers). ```. I just tested that in Colab (https://colab.research.google.com/). However, reimplementing all of DeepVariant from bam to output VCF would be a huge project. If you are interested in something smaller to get started, I'd like to bring this blog post to your attention: https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/. It has an associated Colab notebook that walks through some smaller but still challenging examples of how to use genomic data in machine learning using TensorFlow and Nucleus. I hope that helps! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/328
https://github.com/google/deepvariant/issues/328:824,safety,error,error-correction,824,"Hi @X1angyang . The model is InceptionV3. You can see the layers of one of the DeepVariant models like this:. ```. import tensorflow as tf. !gsutil cp gs://deepvariant/models/DeepVariant/0.10.0/DeepVariant-inception_v3-0.10.0+data-wgs_standard/model* /tmp/. checkpoint_path = '/tmp/model.ckpt'. reader = tf.compat.v1.train.NewCheckpointReader(checkpoint_path). shape_map_for_layers = reader.get_variable_to_shape_map(). print(shape_map_for_layers). ```. I just tested that in Colab (https://colab.research.google.com/). However, reimplementing all of DeepVariant from bam to output VCF would be a huge project. If you are interested in something smaller to get started, I'd like to bring this blog post to your attention: https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/. It has an associated Colab notebook that walks through some smaller but still challenging examples of how to use genomic data in machine learning using TensorFlow and Nucleus. I hope that helps! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/328
https://github.com/google/deepvariant/issues/328:20,security,model,model,20,"Hi @X1angyang . The model is InceptionV3. You can see the layers of one of the DeepVariant models like this:. ```. import tensorflow as tf. !gsutil cp gs://deepvariant/models/DeepVariant/0.10.0/DeepVariant-inception_v3-0.10.0+data-wgs_standard/model* /tmp/. checkpoint_path = '/tmp/model.ckpt'. reader = tf.compat.v1.train.NewCheckpointReader(checkpoint_path). shape_map_for_layers = reader.get_variable_to_shape_map(). print(shape_map_for_layers). ```. I just tested that in Colab (https://colab.research.google.com/). However, reimplementing all of DeepVariant from bam to output VCF would be a huge project. If you are interested in something smaller to get started, I'd like to bring this blog post to your attention: https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/. It has an associated Colab notebook that walks through some smaller but still challenging examples of how to use genomic data in machine learning using TensorFlow and Nucleus. I hope that helps! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/328
https://github.com/google/deepvariant/issues/328:91,security,model,models,91,"Hi @X1angyang . The model is InceptionV3. You can see the layers of one of the DeepVariant models like this:. ```. import tensorflow as tf. !gsutil cp gs://deepvariant/models/DeepVariant/0.10.0/DeepVariant-inception_v3-0.10.0+data-wgs_standard/model* /tmp/. checkpoint_path = '/tmp/model.ckpt'. reader = tf.compat.v1.train.NewCheckpointReader(checkpoint_path). shape_map_for_layers = reader.get_variable_to_shape_map(). print(shape_map_for_layers). ```. I just tested that in Colab (https://colab.research.google.com/). However, reimplementing all of DeepVariant from bam to output VCF would be a huge project. If you are interested in something smaller to get started, I'd like to bring this blog post to your attention: https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/. It has an associated Colab notebook that walks through some smaller but still challenging examples of how to use genomic data in machine learning using TensorFlow and Nucleus. I hope that helps! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/328
https://github.com/google/deepvariant/issues/328:168,security,model,models,168,"Hi @X1angyang . The model is InceptionV3. You can see the layers of one of the DeepVariant models like this:. ```. import tensorflow as tf. !gsutil cp gs://deepvariant/models/DeepVariant/0.10.0/DeepVariant-inception_v3-0.10.0+data-wgs_standard/model* /tmp/. checkpoint_path = '/tmp/model.ckpt'. reader = tf.compat.v1.train.NewCheckpointReader(checkpoint_path). shape_map_for_layers = reader.get_variable_to_shape_map(). print(shape_map_for_layers). ```. I just tested that in Colab (https://colab.research.google.com/). However, reimplementing all of DeepVariant from bam to output VCF would be a huge project. If you are interested in something smaller to get started, I'd like to bring this blog post to your attention: https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/. It has an associated Colab notebook that walks through some smaller but still challenging examples of how to use genomic data in machine learning using TensorFlow and Nucleus. I hope that helps! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/328
https://github.com/google/deepvariant/issues/328:244,security,model,model,244,"Hi @X1angyang . The model is InceptionV3. You can see the layers of one of the DeepVariant models like this:. ```. import tensorflow as tf. !gsutil cp gs://deepvariant/models/DeepVariant/0.10.0/DeepVariant-inception_v3-0.10.0+data-wgs_standard/model* /tmp/. checkpoint_path = '/tmp/model.ckpt'. reader = tf.compat.v1.train.NewCheckpointReader(checkpoint_path). shape_map_for_layers = reader.get_variable_to_shape_map(). print(shape_map_for_layers). ```. I just tested that in Colab (https://colab.research.google.com/). However, reimplementing all of DeepVariant from bam to output VCF would be a huge project. If you are interested in something smaller to get started, I'd like to bring this blog post to your attention: https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/. It has an associated Colab notebook that walks through some smaller but still challenging examples of how to use genomic data in machine learning using TensorFlow and Nucleus. I hope that helps! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/328
https://github.com/google/deepvariant/issues/328:282,security,model,model,282,"Hi @X1angyang . The model is InceptionV3. You can see the layers of one of the DeepVariant models like this:. ```. import tensorflow as tf. !gsutil cp gs://deepvariant/models/DeepVariant/0.10.0/DeepVariant-inception_v3-0.10.0+data-wgs_standard/model* /tmp/. checkpoint_path = '/tmp/model.ckpt'. reader = tf.compat.v1.train.NewCheckpointReader(checkpoint_path). shape_map_for_layers = reader.get_variable_to_shape_map(). print(shape_map_for_layers). ```. I just tested that in Colab (https://colab.research.google.com/). However, reimplementing all of DeepVariant from bam to output VCF would be a huge project. If you are interested in something smaller to get started, I'd like to bring this blog post to your attention: https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/. It has an associated Colab notebook that walks through some smaller but still challenging examples of how to use genomic data in machine learning using TensorFlow and Nucleus. I hope that helps! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/328
https://github.com/google/deepvariant/issues/328:461,testability,test,tested,461,"Hi @X1angyang . The model is InceptionV3. You can see the layers of one of the DeepVariant models like this:. ```. import tensorflow as tf. !gsutil cp gs://deepvariant/models/DeepVariant/0.10.0/DeepVariant-inception_v3-0.10.0+data-wgs_standard/model* /tmp/. checkpoint_path = '/tmp/model.ckpt'. reader = tf.compat.v1.train.NewCheckpointReader(checkpoint_path). shape_map_for_layers = reader.get_variable_to_shape_map(). print(shape_map_for_layers). ```. I just tested that in Colab (https://colab.research.google.com/). However, reimplementing all of DeepVariant from bam to output VCF would be a huge project. If you are interested in something smaller to get started, I'd like to bring this blog post to your attention: https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/. It has an associated Colab notebook that walks through some smaller but still challenging examples of how to use genomic data in machine learning using TensorFlow and Nucleus. I hope that helps! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/328
https://github.com/google/deepvariant/issues/328:824,usability,error,error-correction,824,"Hi @X1angyang . The model is InceptionV3. You can see the layers of one of the DeepVariant models like this:. ```. import tensorflow as tf. !gsutil cp gs://deepvariant/models/DeepVariant/0.10.0/DeepVariant-inception_v3-0.10.0+data-wgs_standard/model* /tmp/. checkpoint_path = '/tmp/model.ckpt'. reader = tf.compat.v1.train.NewCheckpointReader(checkpoint_path). shape_map_for_layers = reader.get_variable_to_shape_map(). print(shape_map_for_layers). ```. I just tested that in Colab (https://colab.research.google.com/). However, reimplementing all of DeepVariant from bam to output VCF would be a huge project. If you are interested in something smaller to get started, I'd like to bring this blog post to your attention: https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/. It has an associated Colab notebook that walks through some smaller but still challenging examples of how to use genomic data in machine learning using TensorFlow and Nucleus. I hope that helps! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/328
https://github.com/google/deepvariant/issues/328:980,usability,learn,learning,980,"Hi @X1angyang . The model is InceptionV3. You can see the layers of one of the DeepVariant models like this:. ```. import tensorflow as tf. !gsutil cp gs://deepvariant/models/DeepVariant/0.10.0/DeepVariant-inception_v3-0.10.0+data-wgs_standard/model* /tmp/. checkpoint_path = '/tmp/model.ckpt'. reader = tf.compat.v1.train.NewCheckpointReader(checkpoint_path). shape_map_for_layers = reader.get_variable_to_shape_map(). print(shape_map_for_layers). ```. I just tested that in Colab (https://colab.research.google.com/). However, reimplementing all of DeepVariant from bam to output VCF would be a huge project. If you are interested in something smaller to get started, I'd like to bring this blog post to your attention: https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/. It has an associated Colab notebook that walks through some smaller but still challenging examples of how to use genomic data in machine learning using TensorFlow and Nucleus. I hope that helps! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/328
https://github.com/google/deepvariant/issues/328:1031,usability,help,helps,1031,"Hi @X1angyang . The model is InceptionV3. You can see the layers of one of the DeepVariant models like this:. ```. import tensorflow as tf. !gsutil cp gs://deepvariant/models/DeepVariant/0.10.0/DeepVariant-inception_v3-0.10.0+data-wgs_standard/model* /tmp/. checkpoint_path = '/tmp/model.ckpt'. reader = tf.compat.v1.train.NewCheckpointReader(checkpoint_path). shape_map_for_layers = reader.get_variable_to_shape_map(). print(shape_map_for_layers). ```. I just tested that in Colab (https://colab.research.google.com/). However, reimplementing all of DeepVariant from bam to output VCF would be a huge project. If you are interested in something smaller to get started, I'd like to bring this blog post to your attention: https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/. It has an associated Colab notebook that walks through some smaller but still challenging examples of how to use genomic data in machine learning using TensorFlow and Nucleus. I hope that helps! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/328
https://github.com/google/deepvariant/issues/328:871,availability,error,error-correction,871,"> Hi @X1angyang. > . > The model is InceptionV3. You can see the layers of one of the DeepVariant models like this:. > . > ```. > import tensorflow as tf. > . > !gsutil cp gs://deepvariant/models/DeepVariant/0.10.0/DeepVariant-inception_v3-0.10.0+data-wgs_standard/model* /tmp/. > checkpoint_path = '/tmp/model.ckpt'. > . > reader = tf.compat.v1.train.NewCheckpointReader(checkpoint_path). > shape_map_for_layers = reader.get_variable_to_shape_map(). > print(shape_map_for_layers). > ```. > . > I just tested that in Colab (https://colab.research.google.com/). > . > However, reimplementing all of DeepVariant from bam to output VCF would be a huge project. If you are interested in something smaller to get started, I'd like to bring this blog post to your attention: https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/. > It has an associated Colab notebook that walks through some smaller but still challenging examples of how to use genomic data in machine learning using TensorFlow and Nucleus. > . > I hope that helps! > Maria. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/328
https://github.com/google/deepvariant/issues/328:27,energy efficiency,model,model,27,"> Hi @X1angyang. > . > The model is InceptionV3. You can see the layers of one of the DeepVariant models like this:. > . > ```. > import tensorflow as tf. > . > !gsutil cp gs://deepvariant/models/DeepVariant/0.10.0/DeepVariant-inception_v3-0.10.0+data-wgs_standard/model* /tmp/. > checkpoint_path = '/tmp/model.ckpt'. > . > reader = tf.compat.v1.train.NewCheckpointReader(checkpoint_path). > shape_map_for_layers = reader.get_variable_to_shape_map(). > print(shape_map_for_layers). > ```. > . > I just tested that in Colab (https://colab.research.google.com/). > . > However, reimplementing all of DeepVariant from bam to output VCF would be a huge project. If you are interested in something smaller to get started, I'd like to bring this blog post to your attention: https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/. > It has an associated Colab notebook that walks through some smaller but still challenging examples of how to use genomic data in machine learning using TensorFlow and Nucleus. > . > I hope that helps! > Maria. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/328
https://github.com/google/deepvariant/issues/328:98,energy efficiency,model,models,98,"> Hi @X1angyang. > . > The model is InceptionV3. You can see the layers of one of the DeepVariant models like this:. > . > ```. > import tensorflow as tf. > . > !gsutil cp gs://deepvariant/models/DeepVariant/0.10.0/DeepVariant-inception_v3-0.10.0+data-wgs_standard/model* /tmp/. > checkpoint_path = '/tmp/model.ckpt'. > . > reader = tf.compat.v1.train.NewCheckpointReader(checkpoint_path). > shape_map_for_layers = reader.get_variable_to_shape_map(). > print(shape_map_for_layers). > ```. > . > I just tested that in Colab (https://colab.research.google.com/). > . > However, reimplementing all of DeepVariant from bam to output VCF would be a huge project. If you are interested in something smaller to get started, I'd like to bring this blog post to your attention: https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/. > It has an associated Colab notebook that walks through some smaller but still challenging examples of how to use genomic data in machine learning using TensorFlow and Nucleus. > . > I hope that helps! > Maria. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/328
https://github.com/google/deepvariant/issues/328:189,energy efficiency,model,models,189,"> Hi @X1angyang. > . > The model is InceptionV3. You can see the layers of one of the DeepVariant models like this:. > . > ```. > import tensorflow as tf. > . > !gsutil cp gs://deepvariant/models/DeepVariant/0.10.0/DeepVariant-inception_v3-0.10.0+data-wgs_standard/model* /tmp/. > checkpoint_path = '/tmp/model.ckpt'. > . > reader = tf.compat.v1.train.NewCheckpointReader(checkpoint_path). > shape_map_for_layers = reader.get_variable_to_shape_map(). > print(shape_map_for_layers). > ```. > . > I just tested that in Colab (https://colab.research.google.com/). > . > However, reimplementing all of DeepVariant from bam to output VCF would be a huge project. If you are interested in something smaller to get started, I'd like to bring this blog post to your attention: https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/. > It has an associated Colab notebook that walks through some smaller but still challenging examples of how to use genomic data in machine learning using TensorFlow and Nucleus. > . > I hope that helps! > Maria. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/328
https://github.com/google/deepvariant/issues/328:265,energy efficiency,model,model,265,"> Hi @X1angyang. > . > The model is InceptionV3. You can see the layers of one of the DeepVariant models like this:. > . > ```. > import tensorflow as tf. > . > !gsutil cp gs://deepvariant/models/DeepVariant/0.10.0/DeepVariant-inception_v3-0.10.0+data-wgs_standard/model* /tmp/. > checkpoint_path = '/tmp/model.ckpt'. > . > reader = tf.compat.v1.train.NewCheckpointReader(checkpoint_path). > shape_map_for_layers = reader.get_variable_to_shape_map(). > print(shape_map_for_layers). > ```. > . > I just tested that in Colab (https://colab.research.google.com/). > . > However, reimplementing all of DeepVariant from bam to output VCF would be a huge project. If you are interested in something smaller to get started, I'd like to bring this blog post to your attention: https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/. > It has an associated Colab notebook that walks through some smaller but still challenging examples of how to use genomic data in machine learning using TensorFlow and Nucleus. > . > I hope that helps! > Maria. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/328
https://github.com/google/deepvariant/issues/328:305,energy efficiency,model,model,305,"> Hi @X1angyang. > . > The model is InceptionV3. You can see the layers of one of the DeepVariant models like this:. > . > ```. > import tensorflow as tf. > . > !gsutil cp gs://deepvariant/models/DeepVariant/0.10.0/DeepVariant-inception_v3-0.10.0+data-wgs_standard/model* /tmp/. > checkpoint_path = '/tmp/model.ckpt'. > . > reader = tf.compat.v1.train.NewCheckpointReader(checkpoint_path). > shape_map_for_layers = reader.get_variable_to_shape_map(). > print(shape_map_for_layers). > ```. > . > I just tested that in Colab (https://colab.research.google.com/). > . > However, reimplementing all of DeepVariant from bam to output VCF would be a huge project. If you are interested in something smaller to get started, I'd like to bring this blog post to your attention: https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/. > It has an associated Colab notebook that walks through some smaller but still challenging examples of how to use genomic data in machine learning using TensorFlow and Nucleus. > . > I hope that helps! > Maria. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/328
https://github.com/google/deepvariant/issues/328:65,modifiability,layer,layers,65,"> Hi @X1angyang. > . > The model is InceptionV3. You can see the layers of one of the DeepVariant models like this:. > . > ```. > import tensorflow as tf. > . > !gsutil cp gs://deepvariant/models/DeepVariant/0.10.0/DeepVariant-inception_v3-0.10.0+data-wgs_standard/model* /tmp/. > checkpoint_path = '/tmp/model.ckpt'. > . > reader = tf.compat.v1.train.NewCheckpointReader(checkpoint_path). > shape_map_for_layers = reader.get_variable_to_shape_map(). > print(shape_map_for_layers). > ```. > . > I just tested that in Colab (https://colab.research.google.com/). > . > However, reimplementing all of DeepVariant from bam to output VCF would be a huge project. If you are interested in something smaller to get started, I'd like to bring this blog post to your attention: https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/. > It has an associated Colab notebook that walks through some smaller but still challenging examples of how to use genomic data in machine learning using TensorFlow and Nucleus. > . > I hope that helps! > Maria. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/328
https://github.com/google/deepvariant/issues/328:871,performance,error,error-correction,871,"> Hi @X1angyang. > . > The model is InceptionV3. You can see the layers of one of the DeepVariant models like this:. > . > ```. > import tensorflow as tf. > . > !gsutil cp gs://deepvariant/models/DeepVariant/0.10.0/DeepVariant-inception_v3-0.10.0+data-wgs_standard/model* /tmp/. > checkpoint_path = '/tmp/model.ckpt'. > . > reader = tf.compat.v1.train.NewCheckpointReader(checkpoint_path). > shape_map_for_layers = reader.get_variable_to_shape_map(). > print(shape_map_for_layers). > ```. > . > I just tested that in Colab (https://colab.research.google.com/). > . > However, reimplementing all of DeepVariant from bam to output VCF would be a huge project. If you are interested in something smaller to get started, I'd like to bring this blog post to your attention: https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/. > It has an associated Colab notebook that walks through some smaller but still challenging examples of how to use genomic data in machine learning using TensorFlow and Nucleus. > . > I hope that helps! > Maria. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/328
https://github.com/google/deepvariant/issues/328:502,safety,test,tested,502,"> Hi @X1angyang. > . > The model is InceptionV3. You can see the layers of one of the DeepVariant models like this:. > . > ```. > import tensorflow as tf. > . > !gsutil cp gs://deepvariant/models/DeepVariant/0.10.0/DeepVariant-inception_v3-0.10.0+data-wgs_standard/model* /tmp/. > checkpoint_path = '/tmp/model.ckpt'. > . > reader = tf.compat.v1.train.NewCheckpointReader(checkpoint_path). > shape_map_for_layers = reader.get_variable_to_shape_map(). > print(shape_map_for_layers). > ```. > . > I just tested that in Colab (https://colab.research.google.com/). > . > However, reimplementing all of DeepVariant from bam to output VCF would be a huge project. If you are interested in something smaller to get started, I'd like to bring this blog post to your attention: https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/. > It has an associated Colab notebook that walks through some smaller but still challenging examples of how to use genomic data in machine learning using TensorFlow and Nucleus. > . > I hope that helps! > Maria. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/328
https://github.com/google/deepvariant/issues/328:871,safety,error,error-correction,871,"> Hi @X1angyang. > . > The model is InceptionV3. You can see the layers of one of the DeepVariant models like this:. > . > ```. > import tensorflow as tf. > . > !gsutil cp gs://deepvariant/models/DeepVariant/0.10.0/DeepVariant-inception_v3-0.10.0+data-wgs_standard/model* /tmp/. > checkpoint_path = '/tmp/model.ckpt'. > . > reader = tf.compat.v1.train.NewCheckpointReader(checkpoint_path). > shape_map_for_layers = reader.get_variable_to_shape_map(). > print(shape_map_for_layers). > ```. > . > I just tested that in Colab (https://colab.research.google.com/). > . > However, reimplementing all of DeepVariant from bam to output VCF would be a huge project. If you are interested in something smaller to get started, I'd like to bring this blog post to your attention: https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/. > It has an associated Colab notebook that walks through some smaller but still challenging examples of how to use genomic data in machine learning using TensorFlow and Nucleus. > . > I hope that helps! > Maria. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/328
https://github.com/google/deepvariant/issues/328:27,security,model,model,27,"> Hi @X1angyang. > . > The model is InceptionV3. You can see the layers of one of the DeepVariant models like this:. > . > ```. > import tensorflow as tf. > . > !gsutil cp gs://deepvariant/models/DeepVariant/0.10.0/DeepVariant-inception_v3-0.10.0+data-wgs_standard/model* /tmp/. > checkpoint_path = '/tmp/model.ckpt'. > . > reader = tf.compat.v1.train.NewCheckpointReader(checkpoint_path). > shape_map_for_layers = reader.get_variable_to_shape_map(). > print(shape_map_for_layers). > ```. > . > I just tested that in Colab (https://colab.research.google.com/). > . > However, reimplementing all of DeepVariant from bam to output VCF would be a huge project. If you are interested in something smaller to get started, I'd like to bring this blog post to your attention: https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/. > It has an associated Colab notebook that walks through some smaller but still challenging examples of how to use genomic data in machine learning using TensorFlow and Nucleus. > . > I hope that helps! > Maria. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/328
https://github.com/google/deepvariant/issues/328:98,security,model,models,98,"> Hi @X1angyang. > . > The model is InceptionV3. You can see the layers of one of the DeepVariant models like this:. > . > ```. > import tensorflow as tf. > . > !gsutil cp gs://deepvariant/models/DeepVariant/0.10.0/DeepVariant-inception_v3-0.10.0+data-wgs_standard/model* /tmp/. > checkpoint_path = '/tmp/model.ckpt'. > . > reader = tf.compat.v1.train.NewCheckpointReader(checkpoint_path). > shape_map_for_layers = reader.get_variable_to_shape_map(). > print(shape_map_for_layers). > ```. > . > I just tested that in Colab (https://colab.research.google.com/). > . > However, reimplementing all of DeepVariant from bam to output VCF would be a huge project. If you are interested in something smaller to get started, I'd like to bring this blog post to your attention: https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/. > It has an associated Colab notebook that walks through some smaller but still challenging examples of how to use genomic data in machine learning using TensorFlow and Nucleus. > . > I hope that helps! > Maria. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/328
https://github.com/google/deepvariant/issues/328:189,security,model,models,189,"> Hi @X1angyang. > . > The model is InceptionV3. You can see the layers of one of the DeepVariant models like this:. > . > ```. > import tensorflow as tf. > . > !gsutil cp gs://deepvariant/models/DeepVariant/0.10.0/DeepVariant-inception_v3-0.10.0+data-wgs_standard/model* /tmp/. > checkpoint_path = '/tmp/model.ckpt'. > . > reader = tf.compat.v1.train.NewCheckpointReader(checkpoint_path). > shape_map_for_layers = reader.get_variable_to_shape_map(). > print(shape_map_for_layers). > ```. > . > I just tested that in Colab (https://colab.research.google.com/). > . > However, reimplementing all of DeepVariant from bam to output VCF would be a huge project. If you are interested in something smaller to get started, I'd like to bring this blog post to your attention: https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/. > It has an associated Colab notebook that walks through some smaller but still challenging examples of how to use genomic data in machine learning using TensorFlow and Nucleus. > . > I hope that helps! > Maria. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/328
https://github.com/google/deepvariant/issues/328:265,security,model,model,265,"> Hi @X1angyang. > . > The model is InceptionV3. You can see the layers of one of the DeepVariant models like this:. > . > ```. > import tensorflow as tf. > . > !gsutil cp gs://deepvariant/models/DeepVariant/0.10.0/DeepVariant-inception_v3-0.10.0+data-wgs_standard/model* /tmp/. > checkpoint_path = '/tmp/model.ckpt'. > . > reader = tf.compat.v1.train.NewCheckpointReader(checkpoint_path). > shape_map_for_layers = reader.get_variable_to_shape_map(). > print(shape_map_for_layers). > ```. > . > I just tested that in Colab (https://colab.research.google.com/). > . > However, reimplementing all of DeepVariant from bam to output VCF would be a huge project. If you are interested in something smaller to get started, I'd like to bring this blog post to your attention: https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/. > It has an associated Colab notebook that walks through some smaller but still challenging examples of how to use genomic data in machine learning using TensorFlow and Nucleus. > . > I hope that helps! > Maria. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/328
https://github.com/google/deepvariant/issues/328:305,security,model,model,305,"> Hi @X1angyang. > . > The model is InceptionV3. You can see the layers of one of the DeepVariant models like this:. > . > ```. > import tensorflow as tf. > . > !gsutil cp gs://deepvariant/models/DeepVariant/0.10.0/DeepVariant-inception_v3-0.10.0+data-wgs_standard/model* /tmp/. > checkpoint_path = '/tmp/model.ckpt'. > . > reader = tf.compat.v1.train.NewCheckpointReader(checkpoint_path). > shape_map_for_layers = reader.get_variable_to_shape_map(). > print(shape_map_for_layers). > ```. > . > I just tested that in Colab (https://colab.research.google.com/). > . > However, reimplementing all of DeepVariant from bam to output VCF would be a huge project. If you are interested in something smaller to get started, I'd like to bring this blog post to your attention: https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/. > It has an associated Colab notebook that walks through some smaller but still challenging examples of how to use genomic data in machine learning using TensorFlow and Nucleus. > . > I hope that helps! > Maria. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/328
https://github.com/google/deepvariant/issues/328:502,testability,test,tested,502,"> Hi @X1angyang. > . > The model is InceptionV3. You can see the layers of one of the DeepVariant models like this:. > . > ```. > import tensorflow as tf. > . > !gsutil cp gs://deepvariant/models/DeepVariant/0.10.0/DeepVariant-inception_v3-0.10.0+data-wgs_standard/model* /tmp/. > checkpoint_path = '/tmp/model.ckpt'. > . > reader = tf.compat.v1.train.NewCheckpointReader(checkpoint_path). > shape_map_for_layers = reader.get_variable_to_shape_map(). > print(shape_map_for_layers). > ```. > . > I just tested that in Colab (https://colab.research.google.com/). > . > However, reimplementing all of DeepVariant from bam to output VCF would be a huge project. If you are interested in something smaller to get started, I'd like to bring this blog post to your attention: https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/. > It has an associated Colab notebook that walks through some smaller but still challenging examples of how to use genomic data in machine learning using TensorFlow and Nucleus. > . > I hope that helps! > Maria. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/328
https://github.com/google/deepvariant/issues/328:871,usability,error,error-correction,871,"> Hi @X1angyang. > . > The model is InceptionV3. You can see the layers of one of the DeepVariant models like this:. > . > ```. > import tensorflow as tf. > . > !gsutil cp gs://deepvariant/models/DeepVariant/0.10.0/DeepVariant-inception_v3-0.10.0+data-wgs_standard/model* /tmp/. > checkpoint_path = '/tmp/model.ckpt'. > . > reader = tf.compat.v1.train.NewCheckpointReader(checkpoint_path). > shape_map_for_layers = reader.get_variable_to_shape_map(). > print(shape_map_for_layers). > ```. > . > I just tested that in Colab (https://colab.research.google.com/). > . > However, reimplementing all of DeepVariant from bam to output VCF would be a huge project. If you are interested in something smaller to get started, I'd like to bring this blog post to your attention: https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/. > It has an associated Colab notebook that walks through some smaller but still challenging examples of how to use genomic data in machine learning using TensorFlow and Nucleus. > . > I hope that helps! > Maria. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/328
https://github.com/google/deepvariant/issues/328:1029,usability,learn,learning,1029,"> Hi @X1angyang. > . > The model is InceptionV3. You can see the layers of one of the DeepVariant models like this:. > . > ```. > import tensorflow as tf. > . > !gsutil cp gs://deepvariant/models/DeepVariant/0.10.0/DeepVariant-inception_v3-0.10.0+data-wgs_standard/model* /tmp/. > checkpoint_path = '/tmp/model.ckpt'. > . > reader = tf.compat.v1.train.NewCheckpointReader(checkpoint_path). > shape_map_for_layers = reader.get_variable_to_shape_map(). > print(shape_map_for_layers). > ```. > . > I just tested that in Colab (https://colab.research.google.com/). > . > However, reimplementing all of DeepVariant from bam to output VCF would be a huge project. If you are interested in something smaller to get started, I'd like to bring this blog post to your attention: https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/. > It has an associated Colab notebook that walks through some smaller but still challenging examples of how to use genomic data in machine learning using TensorFlow and Nucleus. > . > I hope that helps! > Maria. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/328
https://github.com/google/deepvariant/issues/328:1086,usability,help,helps,1086,"> Hi @X1angyang. > . > The model is InceptionV3. You can see the layers of one of the DeepVariant models like this:. > . > ```. > import tensorflow as tf. > . > !gsutil cp gs://deepvariant/models/DeepVariant/0.10.0/DeepVariant-inception_v3-0.10.0+data-wgs_standard/model* /tmp/. > checkpoint_path = '/tmp/model.ckpt'. > . > reader = tf.compat.v1.train.NewCheckpointReader(checkpoint_path). > shape_map_for_layers = reader.get_variable_to_shape_map(). > print(shape_map_for_layers). > ```. > . > I just tested that in Colab (https://colab.research.google.com/). > . > However, reimplementing all of DeepVariant from bam to output VCF would be a huge project. If you are interested in something smaller to get started, I'd like to bring this blog post to your attention: https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/. > It has an associated Colab notebook that walks through some smaller but still challenging examples of how to use genomic data in machine learning using TensorFlow and Nucleus. > . > I hope that helps! > Maria. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/328
https://github.com/google/deepvariant/issues/329:65,availability,error,error,65,"Hi @anands-repo . There are a large number of differences in the error profile of WGS and WES, including capture kit efficiency, additional errors from PCR in preparation, differences in the amount of on-target reads, greater coverage variability in exomes, and probably many other factors that are not completely understood. This paper: https://www.pnas.org/content/112/17/5473 is probably a good place to start on some of the factors that differ between the assays. For deduplication, we use Picard MarkDuplicates as run by GATK. We observe only very negligible differences in variant call quality with and without MarkDuplicates, which only become observable at lower coverages (15x-22x). This is one reason we indicate MarkDuplicates as an optional step in our BestPractices.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/329
https://github.com/google/deepvariant/issues/329:140,availability,error,errors,140,"Hi @anands-repo . There are a large number of differences in the error profile of WGS and WES, including capture kit efficiency, additional errors from PCR in preparation, differences in the amount of on-target reads, greater coverage variability in exomes, and probably many other factors that are not completely understood. This paper: https://www.pnas.org/content/112/17/5473 is probably a good place to start on some of the factors that differ between the assays. For deduplication, we use Picard MarkDuplicates as run by GATK. We observe only very negligible differences in variant call quality with and without MarkDuplicates, which only become observable at lower coverages (15x-22x). This is one reason we indicate MarkDuplicates as an optional step in our BestPractices.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/329
https://github.com/google/deepvariant/issues/329:535,deployability,observ,observe,535,"Hi @anands-repo . There are a large number of differences in the error profile of WGS and WES, including capture kit efficiency, additional errors from PCR in preparation, differences in the amount of on-target reads, greater coverage variability in exomes, and probably many other factors that are not completely understood. This paper: https://www.pnas.org/content/112/17/5473 is probably a good place to start on some of the factors that differ between the assays. For deduplication, we use Picard MarkDuplicates as run by GATK. We observe only very negligible differences in variant call quality with and without MarkDuplicates, which only become observable at lower coverages (15x-22x). This is one reason we indicate MarkDuplicates as an optional step in our BestPractices.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/329
https://github.com/google/deepvariant/issues/329:651,deployability,observ,observable,651,"Hi @anands-repo . There are a large number of differences in the error profile of WGS and WES, including capture kit efficiency, additional errors from PCR in preparation, differences in the amount of on-target reads, greater coverage variability in exomes, and probably many other factors that are not completely understood. This paper: https://www.pnas.org/content/112/17/5473 is probably a good place to start on some of the factors that differ between the assays. For deduplication, we use Picard MarkDuplicates as run by GATK. We observe only very negligible differences in variant call quality with and without MarkDuplicates, which only become observable at lower coverages (15x-22x). This is one reason we indicate MarkDuplicates as an optional step in our BestPractices.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/329
https://github.com/google/deepvariant/issues/329:71,energy efficiency,profil,profile,71,"Hi @anands-repo . There are a large number of differences in the error profile of WGS and WES, including capture kit efficiency, additional errors from PCR in preparation, differences in the amount of on-target reads, greater coverage variability in exomes, and probably many other factors that are not completely understood. This paper: https://www.pnas.org/content/112/17/5473 is probably a good place to start on some of the factors that differ between the assays. For deduplication, we use Picard MarkDuplicates as run by GATK. We observe only very negligible differences in variant call quality with and without MarkDuplicates, which only become observable at lower coverages (15x-22x). This is one reason we indicate MarkDuplicates as an optional step in our BestPractices.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/329
https://github.com/google/deepvariant/issues/329:235,modifiability,variab,variability,235,"Hi @anands-repo . There are a large number of differences in the error profile of WGS and WES, including capture kit efficiency, additional errors from PCR in preparation, differences in the amount of on-target reads, greater coverage variability in exomes, and probably many other factors that are not completely understood. This paper: https://www.pnas.org/content/112/17/5473 is probably a good place to start on some of the factors that differ between the assays. For deduplication, we use Picard MarkDuplicates as run by GATK. We observe only very negligible differences in variant call quality with and without MarkDuplicates, which only become observable at lower coverages (15x-22x). This is one reason we indicate MarkDuplicates as an optional step in our BestPractices.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/329
https://github.com/google/deepvariant/issues/329:65,performance,error,error,65,"Hi @anands-repo . There are a large number of differences in the error profile of WGS and WES, including capture kit efficiency, additional errors from PCR in preparation, differences in the amount of on-target reads, greater coverage variability in exomes, and probably many other factors that are not completely understood. This paper: https://www.pnas.org/content/112/17/5473 is probably a good place to start on some of the factors that differ between the assays. For deduplication, we use Picard MarkDuplicates as run by GATK. We observe only very negligible differences in variant call quality with and without MarkDuplicates, which only become observable at lower coverages (15x-22x). This is one reason we indicate MarkDuplicates as an optional step in our BestPractices.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/329
https://github.com/google/deepvariant/issues/329:71,performance,profil,profile,71,"Hi @anands-repo . There are a large number of differences in the error profile of WGS and WES, including capture kit efficiency, additional errors from PCR in preparation, differences in the amount of on-target reads, greater coverage variability in exomes, and probably many other factors that are not completely understood. This paper: https://www.pnas.org/content/112/17/5473 is probably a good place to start on some of the factors that differ between the assays. For deduplication, we use Picard MarkDuplicates as run by GATK. We observe only very negligible differences in variant call quality with and without MarkDuplicates, which only become observable at lower coverages (15x-22x). This is one reason we indicate MarkDuplicates as an optional step in our BestPractices.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/329
https://github.com/google/deepvariant/issues/329:140,performance,error,errors,140,"Hi @anands-repo . There are a large number of differences in the error profile of WGS and WES, including capture kit efficiency, additional errors from PCR in preparation, differences in the amount of on-target reads, greater coverage variability in exomes, and probably many other factors that are not completely understood. This paper: https://www.pnas.org/content/112/17/5473 is probably a good place to start on some of the factors that differ between the assays. For deduplication, we use Picard MarkDuplicates as run by GATK. We observe only very negligible differences in variant call quality with and without MarkDuplicates, which only become observable at lower coverages (15x-22x). This is one reason we indicate MarkDuplicates as an optional step in our BestPractices.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/329
https://github.com/google/deepvariant/issues/329:359,performance,content,content,359,"Hi @anands-repo . There are a large number of differences in the error profile of WGS and WES, including capture kit efficiency, additional errors from PCR in preparation, differences in the amount of on-target reads, greater coverage variability in exomes, and probably many other factors that are not completely understood. This paper: https://www.pnas.org/content/112/17/5473 is probably a good place to start on some of the factors that differ between the assays. For deduplication, we use Picard MarkDuplicates as run by GATK. We observe only very negligible differences in variant call quality with and without MarkDuplicates, which only become observable at lower coverages (15x-22x). This is one reason we indicate MarkDuplicates as an optional step in our BestPractices.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/329
https://github.com/google/deepvariant/issues/329:65,safety,error,error,65,"Hi @anands-repo . There are a large number of differences in the error profile of WGS and WES, including capture kit efficiency, additional errors from PCR in preparation, differences in the amount of on-target reads, greater coverage variability in exomes, and probably many other factors that are not completely understood. This paper: https://www.pnas.org/content/112/17/5473 is probably a good place to start on some of the factors that differ between the assays. For deduplication, we use Picard MarkDuplicates as run by GATK. We observe only very negligible differences in variant call quality with and without MarkDuplicates, which only become observable at lower coverages (15x-22x). This is one reason we indicate MarkDuplicates as an optional step in our BestPractices.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/329
https://github.com/google/deepvariant/issues/329:140,safety,error,errors,140,"Hi @anands-repo . There are a large number of differences in the error profile of WGS and WES, including capture kit efficiency, additional errors from PCR in preparation, differences in the amount of on-target reads, greater coverage variability in exomes, and probably many other factors that are not completely understood. This paper: https://www.pnas.org/content/112/17/5473 is probably a good place to start on some of the factors that differ between the assays. For deduplication, we use Picard MarkDuplicates as run by GATK. We observe only very negligible differences in variant call quality with and without MarkDuplicates, which only become observable at lower coverages (15x-22x). This is one reason we indicate MarkDuplicates as an optional step in our BestPractices.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/329
https://github.com/google/deepvariant/issues/329:303,safety,compl,completely,303,"Hi @anands-repo . There are a large number of differences in the error profile of WGS and WES, including capture kit efficiency, additional errors from PCR in preparation, differences in the amount of on-target reads, greater coverage variability in exomes, and probably many other factors that are not completely understood. This paper: https://www.pnas.org/content/112/17/5473 is probably a good place to start on some of the factors that differ between the assays. For deduplication, we use Picard MarkDuplicates as run by GATK. We observe only very negligible differences in variant call quality with and without MarkDuplicates, which only become observable at lower coverages (15x-22x). This is one reason we indicate MarkDuplicates as an optional step in our BestPractices.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/329
https://github.com/google/deepvariant/issues/329:303,security,compl,completely,303,"Hi @anands-repo . There are a large number of differences in the error profile of WGS and WES, including capture kit efficiency, additional errors from PCR in preparation, differences in the amount of on-target reads, greater coverage variability in exomes, and probably many other factors that are not completely understood. This paper: https://www.pnas.org/content/112/17/5473 is probably a good place to start on some of the factors that differ between the assays. For deduplication, we use Picard MarkDuplicates as run by GATK. We observe only very negligible differences in variant call quality with and without MarkDuplicates, which only become observable at lower coverages (15x-22x). This is one reason we indicate MarkDuplicates as an optional step in our BestPractices.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/329
https://github.com/google/deepvariant/issues/329:226,testability,coverag,coverage,226,"Hi @anands-repo . There are a large number of differences in the error profile of WGS and WES, including capture kit efficiency, additional errors from PCR in preparation, differences in the amount of on-target reads, greater coverage variability in exomes, and probably many other factors that are not completely understood. This paper: https://www.pnas.org/content/112/17/5473 is probably a good place to start on some of the factors that differ between the assays. For deduplication, we use Picard MarkDuplicates as run by GATK. We observe only very negligible differences in variant call quality with and without MarkDuplicates, which only become observable at lower coverages (15x-22x). This is one reason we indicate MarkDuplicates as an optional step in our BestPractices.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/329
https://github.com/google/deepvariant/issues/329:535,testability,observ,observe,535,"Hi @anands-repo . There are a large number of differences in the error profile of WGS and WES, including capture kit efficiency, additional errors from PCR in preparation, differences in the amount of on-target reads, greater coverage variability in exomes, and probably many other factors that are not completely understood. This paper: https://www.pnas.org/content/112/17/5473 is probably a good place to start on some of the factors that differ between the assays. For deduplication, we use Picard MarkDuplicates as run by GATK. We observe only very negligible differences in variant call quality with and without MarkDuplicates, which only become observable at lower coverages (15x-22x). This is one reason we indicate MarkDuplicates as an optional step in our BestPractices.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/329
https://github.com/google/deepvariant/issues/329:651,testability,observ,observable,651,"Hi @anands-repo . There are a large number of differences in the error profile of WGS and WES, including capture kit efficiency, additional errors from PCR in preparation, differences in the amount of on-target reads, greater coverage variability in exomes, and probably many other factors that are not completely understood. This paper: https://www.pnas.org/content/112/17/5473 is probably a good place to start on some of the factors that differ between the assays. For deduplication, we use Picard MarkDuplicates as run by GATK. We observe only very negligible differences in variant call quality with and without MarkDuplicates, which only become observable at lower coverages (15x-22x). This is one reason we indicate MarkDuplicates as an optional step in our BestPractices.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/329
https://github.com/google/deepvariant/issues/329:671,testability,coverag,coverages,671,"Hi @anands-repo . There are a large number of differences in the error profile of WGS and WES, including capture kit efficiency, additional errors from PCR in preparation, differences in the amount of on-target reads, greater coverage variability in exomes, and probably many other factors that are not completely understood. This paper: https://www.pnas.org/content/112/17/5473 is probably a good place to start on some of the factors that differ between the assays. For deduplication, we use Picard MarkDuplicates as run by GATK. We observe only very negligible differences in variant call quality with and without MarkDuplicates, which only become observable at lower coverages (15x-22x). This is one reason we indicate MarkDuplicates as an optional step in our BestPractices.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/329
https://github.com/google/deepvariant/issues/329:65,usability,error,error,65,"Hi @anands-repo . There are a large number of differences in the error profile of WGS and WES, including capture kit efficiency, additional errors from PCR in preparation, differences in the amount of on-target reads, greater coverage variability in exomes, and probably many other factors that are not completely understood. This paper: https://www.pnas.org/content/112/17/5473 is probably a good place to start on some of the factors that differ between the assays. For deduplication, we use Picard MarkDuplicates as run by GATK. We observe only very negligible differences in variant call quality with and without MarkDuplicates, which only become observable at lower coverages (15x-22x). This is one reason we indicate MarkDuplicates as an optional step in our BestPractices.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/329
https://github.com/google/deepvariant/issues/329:117,usability,efficien,efficiency,117,"Hi @anands-repo . There are a large number of differences in the error profile of WGS and WES, including capture kit efficiency, additional errors from PCR in preparation, differences in the amount of on-target reads, greater coverage variability in exomes, and probably many other factors that are not completely understood. This paper: https://www.pnas.org/content/112/17/5473 is probably a good place to start on some of the factors that differ between the assays. For deduplication, we use Picard MarkDuplicates as run by GATK. We observe only very negligible differences in variant call quality with and without MarkDuplicates, which only become observable at lower coverages (15x-22x). This is one reason we indicate MarkDuplicates as an optional step in our BestPractices.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/329
https://github.com/google/deepvariant/issues/329:140,usability,error,errors,140,"Hi @anands-repo . There are a large number of differences in the error profile of WGS and WES, including capture kit efficiency, additional errors from PCR in preparation, differences in the amount of on-target reads, greater coverage variability in exomes, and probably many other factors that are not completely understood. This paper: https://www.pnas.org/content/112/17/5473 is probably a good place to start on some of the factors that differ between the assays. For deduplication, we use Picard MarkDuplicates as run by GATK. We observe only very negligible differences in variant call quality with and without MarkDuplicates, which only become observable at lower coverages (15x-22x). This is one reason we indicate MarkDuplicates as an optional step in our BestPractices.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/329
https://github.com/google/deepvariant/issues/329:714,usability,indicat,indicate,714,"Hi @anands-repo . There are a large number of differences in the error profile of WGS and WES, including capture kit efficiency, additional errors from PCR in preparation, differences in the amount of on-target reads, greater coverage variability in exomes, and probably many other factors that are not completely understood. This paper: https://www.pnas.org/content/112/17/5473 is probably a good place to start on some of the factors that differ between the assays. For deduplication, we use Picard MarkDuplicates as run by GATK. We observe only very negligible differences in variant call quality with and without MarkDuplicates, which only become observable at lower coverages (15x-22x). This is one reason we indicate MarkDuplicates as an optional step in our BestPractices.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/329
https://github.com/google/deepvariant/issues/331:96,deployability,releas,release,96,I should also mention that DeepVariant has had 6 (or more) channels since the first open source release (v0.4). The type of image mentioned in the 2018 paper was only used in the very first version of DeepVariant before the software was rewritten for open-source.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/331
https://github.com/google/deepvariant/issues/331:190,deployability,version,version,190,I should also mention that DeepVariant has had 6 (or more) channels since the first open source release (v0.4). The type of image mentioned in the 2018 paper was only used in the very first version of DeepVariant before the software was rewritten for open-source.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/331
https://github.com/google/deepvariant/issues/331:190,integrability,version,version,190,I should also mention that DeepVariant has had 6 (or more) channels since the first open source release (v0.4). The type of image mentioned in the 2018 paper was only used in the very first version of DeepVariant before the software was rewritten for open-source.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/331
https://github.com/google/deepvariant/issues/331:190,modifiability,version,version,190,I should also mention that DeepVariant has had 6 (or more) channels since the first open source release (v0.4). The type of image mentioned in the 2018 paper was only used in the very first version of DeepVariant before the software was rewritten for open-source.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/331
https://github.com/google/deepvariant/issues/332:42,deployability,version,version,42,"Thank you @marchoeppner . We have added a version information to the VCF file to our internal codebase, and it'll come out in the next release. After our next release, your VCF output will have a line that looks like this:. ```. ##DeepVariant_version=1.0.0. ```. What you're requesting seems to be a commandline. If I understand correctly, is this something you're looking for:. ```. docker run google/deepvariant:""${BIN_VERSION}"" --version. ```. to output the current version? I can see this being particularly helpful especially if you're trying to use google/deepvariant:latest. I can add that. Thanks for the suggestion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/332
https://github.com/google/deepvariant/issues/332:135,deployability,releas,release,135,"Thank you @marchoeppner . We have added a version information to the VCF file to our internal codebase, and it'll come out in the next release. After our next release, your VCF output will have a line that looks like this:. ```. ##DeepVariant_version=1.0.0. ```. What you're requesting seems to be a commandline. If I understand correctly, is this something you're looking for:. ```. docker run google/deepvariant:""${BIN_VERSION}"" --version. ```. to output the current version? I can see this being particularly helpful especially if you're trying to use google/deepvariant:latest. I can add that. Thanks for the suggestion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/332
https://github.com/google/deepvariant/issues/332:159,deployability,releas,release,159,"Thank you @marchoeppner . We have added a version information to the VCF file to our internal codebase, and it'll come out in the next release. After our next release, your VCF output will have a line that looks like this:. ```. ##DeepVariant_version=1.0.0. ```. What you're requesting seems to be a commandline. If I understand correctly, is this something you're looking for:. ```. docker run google/deepvariant:""${BIN_VERSION}"" --version. ```. to output the current version? I can see this being particularly helpful especially if you're trying to use google/deepvariant:latest. I can add that. Thanks for the suggestion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/332
https://github.com/google/deepvariant/issues/332:433,deployability,version,version,433,"Thank you @marchoeppner . We have added a version information to the VCF file to our internal codebase, and it'll come out in the next release. After our next release, your VCF output will have a line that looks like this:. ```. ##DeepVariant_version=1.0.0. ```. What you're requesting seems to be a commandline. If I understand correctly, is this something you're looking for:. ```. docker run google/deepvariant:""${BIN_VERSION}"" --version. ```. to output the current version? I can see this being particularly helpful especially if you're trying to use google/deepvariant:latest. I can add that. Thanks for the suggestion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/332
https://github.com/google/deepvariant/issues/332:469,deployability,version,version,469,"Thank you @marchoeppner . We have added a version information to the VCF file to our internal codebase, and it'll come out in the next release. After our next release, your VCF output will have a line that looks like this:. ```. ##DeepVariant_version=1.0.0. ```. What you're requesting seems to be a commandline. If I understand correctly, is this something you're looking for:. ```. docker run google/deepvariant:""${BIN_VERSION}"" --version. ```. to output the current version? I can see this being particularly helpful especially if you're trying to use google/deepvariant:latest. I can add that. Thanks for the suggestion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/332
https://github.com/google/deepvariant/issues/332:461,energy efficiency,current,current,461,"Thank you @marchoeppner . We have added a version information to the VCF file to our internal codebase, and it'll come out in the next release. After our next release, your VCF output will have a line that looks like this:. ```. ##DeepVariant_version=1.0.0. ```. What you're requesting seems to be a commandline. If I understand correctly, is this something you're looking for:. ```. docker run google/deepvariant:""${BIN_VERSION}"" --version. ```. to output the current version? I can see this being particularly helpful especially if you're trying to use google/deepvariant:latest. I can add that. Thanks for the suggestion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/332
https://github.com/google/deepvariant/issues/332:42,integrability,version,version,42,"Thank you @marchoeppner . We have added a version information to the VCF file to our internal codebase, and it'll come out in the next release. After our next release, your VCF output will have a line that looks like this:. ```. ##DeepVariant_version=1.0.0. ```. What you're requesting seems to be a commandline. If I understand correctly, is this something you're looking for:. ```. docker run google/deepvariant:""${BIN_VERSION}"" --version. ```. to output the current version? I can see this being particularly helpful especially if you're trying to use google/deepvariant:latest. I can add that. Thanks for the suggestion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/332
https://github.com/google/deepvariant/issues/332:433,integrability,version,version,433,"Thank you @marchoeppner . We have added a version information to the VCF file to our internal codebase, and it'll come out in the next release. After our next release, your VCF output will have a line that looks like this:. ```. ##DeepVariant_version=1.0.0. ```. What you're requesting seems to be a commandline. If I understand correctly, is this something you're looking for:. ```. docker run google/deepvariant:""${BIN_VERSION}"" --version. ```. to output the current version? I can see this being particularly helpful especially if you're trying to use google/deepvariant:latest. I can add that. Thanks for the suggestion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/332
https://github.com/google/deepvariant/issues/332:469,integrability,version,version,469,"Thank you @marchoeppner . We have added a version information to the VCF file to our internal codebase, and it'll come out in the next release. After our next release, your VCF output will have a line that looks like this:. ```. ##DeepVariant_version=1.0.0. ```. What you're requesting seems to be a commandline. If I understand correctly, is this something you're looking for:. ```. docker run google/deepvariant:""${BIN_VERSION}"" --version. ```. to output the current version? I can see this being particularly helpful especially if you're trying to use google/deepvariant:latest. I can add that. Thanks for the suggestion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/332
https://github.com/google/deepvariant/issues/332:42,modifiability,version,version,42,"Thank you @marchoeppner . We have added a version information to the VCF file to our internal codebase, and it'll come out in the next release. After our next release, your VCF output will have a line that looks like this:. ```. ##DeepVariant_version=1.0.0. ```. What you're requesting seems to be a commandline. If I understand correctly, is this something you're looking for:. ```. docker run google/deepvariant:""${BIN_VERSION}"" --version. ```. to output the current version? I can see this being particularly helpful especially if you're trying to use google/deepvariant:latest. I can add that. Thanks for the suggestion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/332
https://github.com/google/deepvariant/issues/332:433,modifiability,version,version,433,"Thank you @marchoeppner . We have added a version information to the VCF file to our internal codebase, and it'll come out in the next release. After our next release, your VCF output will have a line that looks like this:. ```. ##DeepVariant_version=1.0.0. ```. What you're requesting seems to be a commandline. If I understand correctly, is this something you're looking for:. ```. docker run google/deepvariant:""${BIN_VERSION}"" --version. ```. to output the current version? I can see this being particularly helpful especially if you're trying to use google/deepvariant:latest. I can add that. Thanks for the suggestion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/332
https://github.com/google/deepvariant/issues/332:469,modifiability,version,version,469,"Thank you @marchoeppner . We have added a version information to the VCF file to our internal codebase, and it'll come out in the next release. After our next release, your VCF output will have a line that looks like this:. ```. ##DeepVariant_version=1.0.0. ```. What you're requesting seems to be a commandline. If I understand correctly, is this something you're looking for:. ```. docker run google/deepvariant:""${BIN_VERSION}"" --version. ```. to output the current version? I can see this being particularly helpful especially if you're trying to use google/deepvariant:latest. I can add that. Thanks for the suggestion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/332
https://github.com/google/deepvariant/issues/332:318,testability,understand,understand,318,"Thank you @marchoeppner . We have added a version information to the VCF file to our internal codebase, and it'll come out in the next release. After our next release, your VCF output will have a line that looks like this:. ```. ##DeepVariant_version=1.0.0. ```. What you're requesting seems to be a commandline. If I understand correctly, is this something you're looking for:. ```. docker run google/deepvariant:""${BIN_VERSION}"" --version. ```. to output the current version? I can see this being particularly helpful especially if you're trying to use google/deepvariant:latest. I can add that. Thanks for the suggestion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/332
https://github.com/google/deepvariant/issues/332:300,usability,command,commandline,300,"Thank you @marchoeppner . We have added a version information to the VCF file to our internal codebase, and it'll come out in the next release. After our next release, your VCF output will have a line that looks like this:. ```. ##DeepVariant_version=1.0.0. ```. What you're requesting seems to be a commandline. If I understand correctly, is this something you're looking for:. ```. docker run google/deepvariant:""${BIN_VERSION}"" --version. ```. to output the current version? I can see this being particularly helpful especially if you're trying to use google/deepvariant:latest. I can add that. Thanks for the suggestion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/332
https://github.com/google/deepvariant/issues/332:512,usability,help,helpful,512,"Thank you @marchoeppner . We have added a version information to the VCF file to our internal codebase, and it'll come out in the next release. After our next release, your VCF output will have a line that looks like this:. ```. ##DeepVariant_version=1.0.0. ```. What you're requesting seems to be a commandline. If I understand correctly, is this something you're looking for:. ```. docker run google/deepvariant:""${BIN_VERSION}"" --version. ```. to output the current version? I can see this being particularly helpful especially if you're trying to use google/deepvariant:latest. I can add that. Thanks for the suggestion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/332
https://github.com/google/deepvariant/issues/332:50,deployability,releas,release,50,I filed an issue internally to track. In our code release you should have both the version in VCF and the `--version` flag.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/332
https://github.com/google/deepvariant/issues/332:83,deployability,version,version,83,I filed an issue internally to track. In our code release you should have both the version in VCF and the `--version` flag.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/332
https://github.com/google/deepvariant/issues/332:109,deployability,version,version,109,I filed an issue internally to track. In our code release you should have both the version in VCF and the `--version` flag.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/332
https://github.com/google/deepvariant/issues/332:83,integrability,version,version,83,I filed an issue internally to track. In our code release you should have both the version in VCF and the `--version` flag.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/332
https://github.com/google/deepvariant/issues/332:109,integrability,version,version,109,I filed an issue internally to track. In our code release you should have both the version in VCF and the `--version` flag.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/332
https://github.com/google/deepvariant/issues/332:83,modifiability,version,version,83,I filed an issue internally to track. In our code release you should have both the version in VCF and the `--version` flag.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/332
https://github.com/google/deepvariant/issues/332:109,modifiability,version,version,109,I filed an issue internally to track. In our code release you should have both the version in VCF and the `--version` flag.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/332
https://github.com/google/deepvariant/issues/333:398,deployability,releas,release,398,"Hi @marchoeppner . Thank you for reporting this. Currently, MT is excluded in this code:. https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/exclude_contigs.py. We actually got many requests that they want to be able to call MT. So, we have already made the internal change to remove MT and chrM from that `exclude_contigs.py` file already. In our next release, you'll be able to call MT by default! For the current codebase, if you want to call it, you'll need to remove it from code and build your own binaries. Sorry that we don't have a flag for it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/333
https://github.com/google/deepvariant/issues/333:534,deployability,build,build,534,"Hi @marchoeppner . Thank you for reporting this. Currently, MT is excluded in this code:. https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/exclude_contigs.py. We actually got many requests that they want to be able to call MT. So, we have already made the internal change to remove MT and chrM from that `exclude_contigs.py` file already. In our next release, you'll be able to call MT by default! For the current codebase, if you want to call it, you'll need to remove it from code and build your own binaries. Sorry that we don't have a flag for it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/333
https://github.com/google/deepvariant/issues/333:49,energy efficiency,Current,Currently,49,"Hi @marchoeppner . Thank you for reporting this. Currently, MT is excluded in this code:. https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/exclude_contigs.py. We actually got many requests that they want to be able to call MT. So, we have already made the internal change to remove MT and chrM from that `exclude_contigs.py` file already. In our next release, you'll be able to call MT by default! For the current codebase, if you want to call it, you'll need to remove it from code and build your own binaries. Sorry that we don't have a flag for it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/333
https://github.com/google/deepvariant/issues/333:453,energy efficiency,current,current,453,"Hi @marchoeppner . Thank you for reporting this. Currently, MT is excluded in this code:. https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/exclude_contigs.py. We actually got many requests that they want to be able to call MT. So, we have already made the internal change to remove MT and chrM from that `exclude_contigs.py` file already. In our next release, you'll be able to call MT by default! For the current codebase, if you want to call it, you'll need to remove it from code and build your own binaries. Sorry that we don't have a flag for it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/333
https://github.com/google/deepvariant/issues/333:34,deployability,updat,update,34,"Hi @marchoeppner . To give you an update, this file now doesn't have MT https://github.com/google/deepvariant/blob/r1.0/deepvariant/exclude_contigs.py, which means you should be able to call MT now with version 1.0.0. It's removed in this commit: https://github.com/google/deepvariant/commit/7f4f2885059f7efebf83a098f187e32f87b0836a.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/333
https://github.com/google/deepvariant/issues/333:203,deployability,version,version,203,"Hi @marchoeppner . To give you an update, this file now doesn't have MT https://github.com/google/deepvariant/blob/r1.0/deepvariant/exclude_contigs.py, which means you should be able to call MT now with version 1.0.0. It's removed in this commit: https://github.com/google/deepvariant/commit/7f4f2885059f7efebf83a098f187e32f87b0836a.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/333
https://github.com/google/deepvariant/issues/333:203,integrability,version,version,203,"Hi @marchoeppner . To give you an update, this file now doesn't have MT https://github.com/google/deepvariant/blob/r1.0/deepvariant/exclude_contigs.py, which means you should be able to call MT now with version 1.0.0. It's removed in this commit: https://github.com/google/deepvariant/commit/7f4f2885059f7efebf83a098f187e32f87b0836a.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/333
https://github.com/google/deepvariant/issues/333:203,modifiability,version,version,203,"Hi @marchoeppner . To give you an update, this file now doesn't have MT https://github.com/google/deepvariant/blob/r1.0/deepvariant/exclude_contigs.py, which means you should be able to call MT now with version 1.0.0. It's removed in this commit: https://github.com/google/deepvariant/commit/7f4f2885059f7efebf83a098f187e32f87b0836a.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/333
https://github.com/google/deepvariant/issues/333:56,reliability,doe,doesn,56,"Hi @marchoeppner . To give you an update, this file now doesn't have MT https://github.com/google/deepvariant/blob/r1.0/deepvariant/exclude_contigs.py, which means you should be able to call MT now with version 1.0.0. It's removed in this commit: https://github.com/google/deepvariant/commit/7f4f2885059f7efebf83a098f187e32f87b0836a.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/333
https://github.com/google/deepvariant/issues/333:34,safety,updat,update,34,"Hi @marchoeppner . To give you an update, this file now doesn't have MT https://github.com/google/deepvariant/blob/r1.0/deepvariant/exclude_contigs.py, which means you should be able to call MT now with version 1.0.0. It's removed in this commit: https://github.com/google/deepvariant/commit/7f4f2885059f7efebf83a098f187e32f87b0836a.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/333
https://github.com/google/deepvariant/issues/333:34,security,updat,update,34,"Hi @marchoeppner . To give you an update, this file now doesn't have MT https://github.com/google/deepvariant/blob/r1.0/deepvariant/exclude_contigs.py, which means you should be able to call MT now with version 1.0.0. It's removed in this commit: https://github.com/google/deepvariant/commit/7f4f2885059f7efebf83a098f187e32f87b0836a.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/333
https://github.com/google/deepvariant/issues/334:308,deployability,releas,release,308,"Thanks for the suggestion @arostamianfar . Sounds like a good suggestion and should be easy to do : adding a flag to postprocess_variants. If `sample_name` is specified for run_deepvariant.py, we'll use it for both. . I'll file an internal issue to track this, and we should be able to have this in our next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/334
https://github.com/google/deepvariant/issues/334:159,interoperability,specif,specified,159,"Thanks for the suggestion @arostamianfar . Sounds like a good suggestion and should be easy to do : adding a flag to postprocess_variants. If `sample_name` is specified for run_deepvariant.py, we'll use it for both. . I'll file an internal issue to track this, and we should be able to have this in our next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/334
https://github.com/google/deepvariant/issues/334:76,availability,avail,available,76,The functionality has been implemented in the internal codebase and will be available in the next release.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/334
https://github.com/google/deepvariant/issues/334:98,deployability,releas,release,98,The functionality has been implemented in the internal codebase and will be available in the next release.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/334
https://github.com/google/deepvariant/issues/334:76,reliability,availab,available,76,The functionality has been implemented in the internal codebase and will be available in the next release.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/334
https://github.com/google/deepvariant/issues/334:76,safety,avail,available,76,The functionality has been implemented in the internal codebase and will be available in the next release.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/334
https://github.com/google/deepvariant/issues/334:76,security,availab,available,76,The functionality has been implemented in the internal codebase and will be available in the next release.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/334
https://github.com/google/deepvariant/issues/334:199,deployability,stage,stage,199,"Hi @arostamianfar . Like @gunjanbaid said, in our internal codebase, I added the --sample_name flag to postprocess_variants as well, which will allow users to pass in a different sample_name at that stage. But, I do want to point out that even with the current version (0.10.0), if you pass in --sample_name in the one-step command like Quick Start, you should be already able to get the VCF output with the specified sample_name. Because the `--sample_name` specified in the make_examples stage does get pass on to call_variants, and then to postprocess_variants. I tried with exactly the steps in https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md , and added --sample_name:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1 \. --sample_name=FOOBAR. ```. After running this command (with the current version, 0.10.0), I do get `FOOBAR` set as my sample name. ```. $ zcat quickstart-output/output.vcf.gz | grep FOOBAR. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT FOOBAR. ```. Just an FYI, and hopefully I didn't misunderstand the issue you brought up. But, in the new code (which we plan to release soon), the --sample_name flag is added to postprocess_variants as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/334
https://github.com/google/deepvariant/issues/334:261,deployability,version,version,261,"Hi @arostamianfar . Like @gunjanbaid said, in our internal codebase, I added the --sample_name flag to postprocess_variants as well, which will allow users to pass in a different sample_name at that stage. But, I do want to point out that even with the current version (0.10.0), if you pass in --sample_name in the one-step command like Quick Start, you should be already able to get the VCF output with the specified sample_name. Because the `--sample_name` specified in the make_examples stage does get pass on to call_variants, and then to postprocess_variants. I tried with exactly the steps in https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md , and added --sample_name:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1 \. --sample_name=FOOBAR. ```. After running this command (with the current version, 0.10.0), I do get `FOOBAR` set as my sample name. ```. $ zcat quickstart-output/output.vcf.gz | grep FOOBAR. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT FOOBAR. ```. Just an FYI, and hopefully I didn't misunderstand the issue you brought up. But, in the new code (which we plan to release soon), the --sample_name flag is added to postprocess_variants as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/334
https://github.com/google/deepvariant/issues/334:490,deployability,stage,stage,490,"Hi @arostamianfar . Like @gunjanbaid said, in our internal codebase, I added the --sample_name flag to postprocess_variants as well, which will allow users to pass in a different sample_name at that stage. But, I do want to point out that even with the current version (0.10.0), if you pass in --sample_name in the one-step command like Quick Start, you should be already able to get the VCF output with the specified sample_name. Because the `--sample_name` specified in the make_examples stage does get pass on to call_variants, and then to postprocess_variants. I tried with exactly the steps in https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md , and added --sample_name:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1 \. --sample_name=FOOBAR. ```. After running this command (with the current version, 0.10.0), I do get `FOOBAR` set as my sample name. ```. $ zcat quickstart-output/output.vcf.gz | grep FOOBAR. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT FOOBAR. ```. Just an FYI, and hopefully I didn't misunderstand the issue you brought up. But, in the new code (which we plan to release soon), the --sample_name flag is added to postprocess_variants as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/334
https://github.com/google/deepvariant/issues/334:1199,deployability,version,version,1199,"Hi @arostamianfar . Like @gunjanbaid said, in our internal codebase, I added the --sample_name flag to postprocess_variants as well, which will allow users to pass in a different sample_name at that stage. But, I do want to point out that even with the current version (0.10.0), if you pass in --sample_name in the one-step command like Quick Start, you should be already able to get the VCF output with the specified sample_name. Because the `--sample_name` specified in the make_examples stage does get pass on to call_variants, and then to postprocess_variants. I tried with exactly the steps in https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md , and added --sample_name:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1 \. --sample_name=FOOBAR. ```. After running this command (with the current version, 0.10.0), I do get `FOOBAR` set as my sample name. ```. $ zcat quickstart-output/output.vcf.gz | grep FOOBAR. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT FOOBAR. ```. Just an FYI, and hopefully I didn't misunderstand the issue you brought up. But, in the new code (which we plan to release soon), the --sample_name flag is added to postprocess_variants as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/334
https://github.com/google/deepvariant/issues/334:1491,deployability,releas,release,1491,"Hi @arostamianfar . Like @gunjanbaid said, in our internal codebase, I added the --sample_name flag to postprocess_variants as well, which will allow users to pass in a different sample_name at that stage. But, I do want to point out that even with the current version (0.10.0), if you pass in --sample_name in the one-step command like Quick Start, you should be already able to get the VCF output with the specified sample_name. Because the `--sample_name` specified in the make_examples stage does get pass on to call_variants, and then to postprocess_variants. I tried with exactly the steps in https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md , and added --sample_name:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1 \. --sample_name=FOOBAR. ```. After running this command (with the current version, 0.10.0), I do get `FOOBAR` set as my sample name. ```. $ zcat quickstart-output/output.vcf.gz | grep FOOBAR. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT FOOBAR. ```. Just an FYI, and hopefully I didn't misunderstand the issue you brought up. But, in the new code (which we plan to release soon), the --sample_name flag is added to postprocess_variants as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/334
https://github.com/google/deepvariant/issues/334:253,energy efficiency,current,current,253,"Hi @arostamianfar . Like @gunjanbaid said, in our internal codebase, I added the --sample_name flag to postprocess_variants as well, which will allow users to pass in a different sample_name at that stage. But, I do want to point out that even with the current version (0.10.0), if you pass in --sample_name in the one-step command like Quick Start, you should be already able to get the VCF output with the specified sample_name. Because the `--sample_name` specified in the make_examples stage does get pass on to call_variants, and then to postprocess_variants. I tried with exactly the steps in https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md , and added --sample_name:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1 \. --sample_name=FOOBAR. ```. After running this command (with the current version, 0.10.0), I do get `FOOBAR` set as my sample name. ```. $ zcat quickstart-output/output.vcf.gz | grep FOOBAR. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT FOOBAR. ```. Just an FYI, and hopefully I didn't misunderstand the issue you brought up. But, in the new code (which we plan to release soon), the --sample_name flag is added to postprocess_variants as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/334
https://github.com/google/deepvariant/issues/334:1191,energy efficiency,current,current,1191,"Hi @arostamianfar . Like @gunjanbaid said, in our internal codebase, I added the --sample_name flag to postprocess_variants as well, which will allow users to pass in a different sample_name at that stage. But, I do want to point out that even with the current version (0.10.0), if you pass in --sample_name in the one-step command like Quick Start, you should be already able to get the VCF output with the specified sample_name. Because the `--sample_name` specified in the make_examples stage does get pass on to call_variants, and then to postprocess_variants. I tried with exactly the steps in https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md , and added --sample_name:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1 \. --sample_name=FOOBAR. ```. After running this command (with the current version, 0.10.0), I do get `FOOBAR` set as my sample name. ```. $ zcat quickstart-output/output.vcf.gz | grep FOOBAR. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT FOOBAR. ```. Just an FYI, and hopefully I didn't misunderstand the issue you brought up. But, in the new code (which we plan to release soon), the --sample_name flag is added to postprocess_variants as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/334
https://github.com/google/deepvariant/issues/334:261,integrability,version,version,261,"Hi @arostamianfar . Like @gunjanbaid said, in our internal codebase, I added the --sample_name flag to postprocess_variants as well, which will allow users to pass in a different sample_name at that stage. But, I do want to point out that even with the current version (0.10.0), if you pass in --sample_name in the one-step command like Quick Start, you should be already able to get the VCF output with the specified sample_name. Because the `--sample_name` specified in the make_examples stage does get pass on to call_variants, and then to postprocess_variants. I tried with exactly the steps in https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md , and added --sample_name:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1 \. --sample_name=FOOBAR. ```. After running this command (with the current version, 0.10.0), I do get `FOOBAR` set as my sample name. ```. $ zcat quickstart-output/output.vcf.gz | grep FOOBAR. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT FOOBAR. ```. Just an FYI, and hopefully I didn't misunderstand the issue you brought up. But, in the new code (which we plan to release soon), the --sample_name flag is added to postprocess_variants as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/334
https://github.com/google/deepvariant/issues/334:1199,integrability,version,version,1199,"Hi @arostamianfar . Like @gunjanbaid said, in our internal codebase, I added the --sample_name flag to postprocess_variants as well, which will allow users to pass in a different sample_name at that stage. But, I do want to point out that even with the current version (0.10.0), if you pass in --sample_name in the one-step command like Quick Start, you should be already able to get the VCF output with the specified sample_name. Because the `--sample_name` specified in the make_examples stage does get pass on to call_variants, and then to postprocess_variants. I tried with exactly the steps in https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md , and added --sample_name:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1 \. --sample_name=FOOBAR. ```. After running this command (with the current version, 0.10.0), I do get `FOOBAR` set as my sample name. ```. $ zcat quickstart-output/output.vcf.gz | grep FOOBAR. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT FOOBAR. ```. Just an FYI, and hopefully I didn't misunderstand the issue you brought up. But, in the new code (which we plan to release soon), the --sample_name flag is added to postprocess_variants as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/334
https://github.com/google/deepvariant/issues/334:1344,integrability,FILTER,FILTER,1344,"Hi @arostamianfar . Like @gunjanbaid said, in our internal codebase, I added the --sample_name flag to postprocess_variants as well, which will allow users to pass in a different sample_name at that stage. But, I do want to point out that even with the current version (0.10.0), if you pass in --sample_name in the one-step command like Quick Start, you should be already able to get the VCF output with the specified sample_name. Because the `--sample_name` specified in the make_examples stage does get pass on to call_variants, and then to postprocess_variants. I tried with exactly the steps in https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md , and added --sample_name:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1 \. --sample_name=FOOBAR. ```. After running this command (with the current version, 0.10.0), I do get `FOOBAR` set as my sample name. ```. $ zcat quickstart-output/output.vcf.gz | grep FOOBAR. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT FOOBAR. ```. Just an FYI, and hopefully I didn't misunderstand the issue you brought up. But, in the new code (which we plan to release soon), the --sample_name flag is added to postprocess_variants as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/334
https://github.com/google/deepvariant/issues/334:408,interoperability,specif,specified,408,"Hi @arostamianfar . Like @gunjanbaid said, in our internal codebase, I added the --sample_name flag to postprocess_variants as well, which will allow users to pass in a different sample_name at that stage. But, I do want to point out that even with the current version (0.10.0), if you pass in --sample_name in the one-step command like Quick Start, you should be already able to get the VCF output with the specified sample_name. Because the `--sample_name` specified in the make_examples stage does get pass on to call_variants, and then to postprocess_variants. I tried with exactly the steps in https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md , and added --sample_name:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1 \. --sample_name=FOOBAR. ```. After running this command (with the current version, 0.10.0), I do get `FOOBAR` set as my sample name. ```. $ zcat quickstart-output/output.vcf.gz | grep FOOBAR. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT FOOBAR. ```. Just an FYI, and hopefully I didn't misunderstand the issue you brought up. But, in the new code (which we plan to release soon), the --sample_name flag is added to postprocess_variants as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/334
https://github.com/google/deepvariant/issues/334:459,interoperability,specif,specified,459,"Hi @arostamianfar . Like @gunjanbaid said, in our internal codebase, I added the --sample_name flag to postprocess_variants as well, which will allow users to pass in a different sample_name at that stage. But, I do want to point out that even with the current version (0.10.0), if you pass in --sample_name in the one-step command like Quick Start, you should be already able to get the VCF output with the specified sample_name. Because the `--sample_name` specified in the make_examples stage does get pass on to call_variants, and then to postprocess_variants. I tried with exactly the steps in https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md , and added --sample_name:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1 \. --sample_name=FOOBAR. ```. After running this command (with the current version, 0.10.0), I do get `FOOBAR` set as my sample name. ```. $ zcat quickstart-output/output.vcf.gz | grep FOOBAR. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT FOOBAR. ```. Just an FYI, and hopefully I didn't misunderstand the issue you brought up. But, in the new code (which we plan to release soon), the --sample_name flag is added to postprocess_variants as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/334
https://github.com/google/deepvariant/issues/334:1356,interoperability,FORMAT,FORMAT,1356,"Hi @arostamianfar . Like @gunjanbaid said, in our internal codebase, I added the --sample_name flag to postprocess_variants as well, which will allow users to pass in a different sample_name at that stage. But, I do want to point out that even with the current version (0.10.0), if you pass in --sample_name in the one-step command like Quick Start, you should be already able to get the VCF output with the specified sample_name. Because the `--sample_name` specified in the make_examples stage does get pass on to call_variants, and then to postprocess_variants. I tried with exactly the steps in https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md , and added --sample_name:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1 \. --sample_name=FOOBAR. ```. After running this command (with the current version, 0.10.0), I do get `FOOBAR` set as my sample name. ```. $ zcat quickstart-output/output.vcf.gz | grep FOOBAR. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT FOOBAR. ```. Just an FYI, and hopefully I didn't misunderstand the issue you brought up. But, in the new code (which we plan to release soon), the --sample_name flag is added to postprocess_variants as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/334
https://github.com/google/deepvariant/issues/334:261,modifiability,version,version,261,"Hi @arostamianfar . Like @gunjanbaid said, in our internal codebase, I added the --sample_name flag to postprocess_variants as well, which will allow users to pass in a different sample_name at that stage. But, I do want to point out that even with the current version (0.10.0), if you pass in --sample_name in the one-step command like Quick Start, you should be already able to get the VCF output with the specified sample_name. Because the `--sample_name` specified in the make_examples stage does get pass on to call_variants, and then to postprocess_variants. I tried with exactly the steps in https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md , and added --sample_name:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1 \. --sample_name=FOOBAR. ```. After running this command (with the current version, 0.10.0), I do get `FOOBAR` set as my sample name. ```. $ zcat quickstart-output/output.vcf.gz | grep FOOBAR. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT FOOBAR. ```. Just an FYI, and hopefully I didn't misunderstand the issue you brought up. But, in the new code (which we plan to release soon), the --sample_name flag is added to postprocess_variants as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/334
https://github.com/google/deepvariant/issues/334:1199,modifiability,version,version,1199,"Hi @arostamianfar . Like @gunjanbaid said, in our internal codebase, I added the --sample_name flag to postprocess_variants as well, which will allow users to pass in a different sample_name at that stage. But, I do want to point out that even with the current version (0.10.0), if you pass in --sample_name in the one-step command like Quick Start, you should be already able to get the VCF output with the specified sample_name. Because the `--sample_name` specified in the make_examples stage does get pass on to call_variants, and then to postprocess_variants. I tried with exactly the steps in https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md , and added --sample_name:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1 \. --sample_name=FOOBAR. ```. After running this command (with the current version, 0.10.0), I do get `FOOBAR` set as my sample name. ```. $ zcat quickstart-output/output.vcf.gz | grep FOOBAR. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT FOOBAR. ```. Just an FYI, and hopefully I didn't misunderstand the issue you brought up. But, in the new code (which we plan to release soon), the --sample_name flag is added to postprocess_variants as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/334
https://github.com/google/deepvariant/issues/334:496,reliability,doe,does,496,"Hi @arostamianfar . Like @gunjanbaid said, in our internal codebase, I added the --sample_name flag to postprocess_variants as well, which will allow users to pass in a different sample_name at that stage. But, I do want to point out that even with the current version (0.10.0), if you pass in --sample_name in the one-step command like Quick Start, you should be already able to get the VCF output with the specified sample_name. Because the `--sample_name` specified in the make_examples stage does get pass on to call_variants, and then to postprocess_variants. I tried with exactly the steps in https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md , and added --sample_name:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1 \. --sample_name=FOOBAR. ```. After running this command (with the current version, 0.10.0), I do get `FOOBAR` set as my sample name. ```. $ zcat quickstart-output/output.vcf.gz | grep FOOBAR. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT FOOBAR. ```. Just an FYI, and hopefully I didn't misunderstand the issue you brought up. But, in the new code (which we plan to release soon), the --sample_name flag is added to postprocess_variants as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/334
https://github.com/google/deepvariant/issues/334:752,safety,input,input,752,"Hi @arostamianfar . Like @gunjanbaid said, in our internal codebase, I added the --sample_name flag to postprocess_variants as well, which will allow users to pass in a different sample_name at that stage. But, I do want to point out that even with the current version (0.10.0), if you pass in --sample_name in the one-step command like Quick Start, you should be already able to get the VCF output with the specified sample_name. Because the `--sample_name` specified in the make_examples stage does get pass on to call_variants, and then to postprocess_variants. I tried with exactly the steps in https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md , and added --sample_name:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1 \. --sample_name=FOOBAR. ```. After running this command (with the current version, 0.10.0), I do get `FOOBAR` set as my sample name. ```. $ zcat quickstart-output/output.vcf.gz | grep FOOBAR. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT FOOBAR. ```. Just an FYI, and hopefully I didn't misunderstand the issue you brought up. But, in the new code (which we plan to release soon), the --sample_name flag is added to postprocess_variants as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/334
https://github.com/google/deepvariant/issues/334:898,safety,input,input,898,"Hi @arostamianfar . Like @gunjanbaid said, in our internal codebase, I added the --sample_name flag to postprocess_variants as well, which will allow users to pass in a different sample_name at that stage. But, I do want to point out that even with the current version (0.10.0), if you pass in --sample_name in the one-step command like Quick Start, you should be already able to get the VCF output with the specified sample_name. Because the `--sample_name` specified in the make_examples stage does get pass on to call_variants, and then to postprocess_variants. I tried with exactly the steps in https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md , and added --sample_name:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1 \. --sample_name=FOOBAR. ```. After running this command (with the current version, 0.10.0), I do get `FOOBAR` set as my sample name. ```. $ zcat quickstart-output/output.vcf.gz | grep FOOBAR. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT FOOBAR. ```. Just an FYI, and hopefully I didn't misunderstand the issue you brought up. But, in the new code (which we plan to release soon), the --sample_name flag is added to postprocess_variants as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/334
https://github.com/google/deepvariant/issues/334:947,safety,input,input,947,"Hi @arostamianfar . Like @gunjanbaid said, in our internal codebase, I added the --sample_name flag to postprocess_variants as well, which will allow users to pass in a different sample_name at that stage. But, I do want to point out that even with the current version (0.10.0), if you pass in --sample_name in the one-step command like Quick Start, you should be already able to get the VCF output with the specified sample_name. Because the `--sample_name` specified in the make_examples stage does get pass on to call_variants, and then to postprocess_variants. I tried with exactly the steps in https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md , and added --sample_name:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1 \. --sample_name=FOOBAR. ```. After running this command (with the current version, 0.10.0), I do get `FOOBAR` set as my sample name. ```. $ zcat quickstart-output/output.vcf.gz | grep FOOBAR. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT FOOBAR. ```. Just an FYI, and hopefully I didn't misunderstand the issue you brought up. But, in the new code (which we plan to release soon), the --sample_name flag is added to postprocess_variants as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/334
https://github.com/google/deepvariant/issues/334:920,testability,unit,unittest,920,"Hi @arostamianfar . Like @gunjanbaid said, in our internal codebase, I added the --sample_name flag to postprocess_variants as well, which will allow users to pass in a different sample_name at that stage. But, I do want to point out that even with the current version (0.10.0), if you pass in --sample_name in the one-step command like Quick Start, you should be already able to get the VCF output with the specified sample_name. Because the `--sample_name` specified in the make_examples stage does get pass on to call_variants, and then to postprocess_variants. I tried with exactly the steps in https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md , and added --sample_name:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1 \. --sample_name=FOOBAR. ```. After running this command (with the current version, 0.10.0), I do get `FOOBAR` set as my sample name. ```. $ zcat quickstart-output/output.vcf.gz | grep FOOBAR. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT FOOBAR. ```. Just an FYI, and hopefully I didn't misunderstand the issue you brought up. But, in the new code (which we plan to release soon), the --sample_name flag is added to postprocess_variants as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/334
https://github.com/google/deepvariant/issues/334:1483,testability,plan,plan,1483,"Hi @arostamianfar . Like @gunjanbaid said, in our internal codebase, I added the --sample_name flag to postprocess_variants as well, which will allow users to pass in a different sample_name at that stage. But, I do want to point out that even with the current version (0.10.0), if you pass in --sample_name in the one-step command like Quick Start, you should be already able to get the VCF output with the specified sample_name. Because the `--sample_name` specified in the make_examples stage does get pass on to call_variants, and then to postprocess_variants. I tried with exactly the steps in https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md , and added --sample_name:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1 \. --sample_name=FOOBAR. ```. After running this command (with the current version, 0.10.0), I do get `FOOBAR` set as my sample name. ```. $ zcat quickstart-output/output.vcf.gz | grep FOOBAR. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT FOOBAR. ```. Just an FYI, and hopefully I didn't misunderstand the issue you brought up. But, in the new code (which we plan to release soon), the --sample_name flag is added to postprocess_variants as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/334
https://github.com/google/deepvariant/issues/334:150,usability,user,users,150,"Hi @arostamianfar . Like @gunjanbaid said, in our internal codebase, I added the --sample_name flag to postprocess_variants as well, which will allow users to pass in a different sample_name at that stage. But, I do want to point out that even with the current version (0.10.0), if you pass in --sample_name in the one-step command like Quick Start, you should be already able to get the VCF output with the specified sample_name. Because the `--sample_name` specified in the make_examples stage does get pass on to call_variants, and then to postprocess_variants. I tried with exactly the steps in https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md , and added --sample_name:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1 \. --sample_name=FOOBAR. ```. After running this command (with the current version, 0.10.0), I do get `FOOBAR` set as my sample name. ```. $ zcat quickstart-output/output.vcf.gz | grep FOOBAR. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT FOOBAR. ```. Just an FYI, and hopefully I didn't misunderstand the issue you brought up. But, in the new code (which we plan to release soon), the --sample_name flag is added to postprocess_variants as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/334
https://github.com/google/deepvariant/issues/334:324,usability,command,command,324,"Hi @arostamianfar . Like @gunjanbaid said, in our internal codebase, I added the --sample_name flag to postprocess_variants as well, which will allow users to pass in a different sample_name at that stage. But, I do want to point out that even with the current version (0.10.0), if you pass in --sample_name in the one-step command like Quick Start, you should be already able to get the VCF output with the specified sample_name. Because the `--sample_name` specified in the make_examples stage does get pass on to call_variants, and then to postprocess_variants. I tried with exactly the steps in https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md , and added --sample_name:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1 \. --sample_name=FOOBAR. ```. After running this command (with the current version, 0.10.0), I do get `FOOBAR` set as my sample name. ```. $ zcat quickstart-output/output.vcf.gz | grep FOOBAR. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT FOOBAR. ```. Just an FYI, and hopefully I didn't misunderstand the issue you brought up. But, in the new code (which we plan to release soon), the --sample_name flag is added to postprocess_variants as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/334
https://github.com/google/deepvariant/issues/334:752,usability,input,input,752,"Hi @arostamianfar . Like @gunjanbaid said, in our internal codebase, I added the --sample_name flag to postprocess_variants as well, which will allow users to pass in a different sample_name at that stage. But, I do want to point out that even with the current version (0.10.0), if you pass in --sample_name in the one-step command like Quick Start, you should be already able to get the VCF output with the specified sample_name. Because the `--sample_name` specified in the make_examples stage does get pass on to call_variants, and then to postprocess_variants. I tried with exactly the steps in https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md , and added --sample_name:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1 \. --sample_name=FOOBAR. ```. After running this command (with the current version, 0.10.0), I do get `FOOBAR` set as my sample name. ```. $ zcat quickstart-output/output.vcf.gz | grep FOOBAR. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT FOOBAR. ```. Just an FYI, and hopefully I didn't misunderstand the issue you brought up. But, in the new code (which we plan to release soon), the --sample_name flag is added to postprocess_variants as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/334
https://github.com/google/deepvariant/issues/334:898,usability,input,input,898,"Hi @arostamianfar . Like @gunjanbaid said, in our internal codebase, I added the --sample_name flag to postprocess_variants as well, which will allow users to pass in a different sample_name at that stage. But, I do want to point out that even with the current version (0.10.0), if you pass in --sample_name in the one-step command like Quick Start, you should be already able to get the VCF output with the specified sample_name. Because the `--sample_name` specified in the make_examples stage does get pass on to call_variants, and then to postprocess_variants. I tried with exactly the steps in https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md , and added --sample_name:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1 \. --sample_name=FOOBAR. ```. After running this command (with the current version, 0.10.0), I do get `FOOBAR` set as my sample name. ```. $ zcat quickstart-output/output.vcf.gz | grep FOOBAR. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT FOOBAR. ```. Just an FYI, and hopefully I didn't misunderstand the issue you brought up. But, in the new code (which we plan to release soon), the --sample_name flag is added to postprocess_variants as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/334
https://github.com/google/deepvariant/issues/334:947,usability,input,input,947,"Hi @arostamianfar . Like @gunjanbaid said, in our internal codebase, I added the --sample_name flag to postprocess_variants as well, which will allow users to pass in a different sample_name at that stage. But, I do want to point out that even with the current version (0.10.0), if you pass in --sample_name in the one-step command like Quick Start, you should be already able to get the VCF output with the specified sample_name. Because the `--sample_name` specified in the make_examples stage does get pass on to call_variants, and then to postprocess_variants. I tried with exactly the steps in https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md , and added --sample_name:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1 \. --sample_name=FOOBAR. ```. After running this command (with the current version, 0.10.0), I do get `FOOBAR` set as my sample name. ```. $ zcat quickstart-output/output.vcf.gz | grep FOOBAR. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT FOOBAR. ```. Just an FYI, and hopefully I didn't misunderstand the issue you brought up. But, in the new code (which we plan to release soon), the --sample_name flag is added to postprocess_variants as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/334
https://github.com/google/deepvariant/issues/334:1173,usability,command,command,1173,"Hi @arostamianfar . Like @gunjanbaid said, in our internal codebase, I added the --sample_name flag to postprocess_variants as well, which will allow users to pass in a different sample_name at that stage. But, I do want to point out that even with the current version (0.10.0), if you pass in --sample_name in the one-step command like Quick Start, you should be already able to get the VCF output with the specified sample_name. Because the `--sample_name` specified in the make_examples stage does get pass on to call_variants, and then to postprocess_variants. I tried with exactly the steps in https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md , and added --sample_name:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=/input/ucsc.hg19.chr20.unittest.fasta \. --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. --regions ""chr20:10,000,000-10,010,000"" \. --output_vcf=/output/output.vcf.gz \. --output_gvcf=/output/output.g.vcf.gz \. --num_shards=1 \. --sample_name=FOOBAR. ```. After running this command (with the current version, 0.10.0), I do get `FOOBAR` set as my sample name. ```. $ zcat quickstart-output/output.vcf.gz | grep FOOBAR. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT FOOBAR. ```. Just an FYI, and hopefully I didn't misunderstand the issue you brought up. But, in the new code (which we plan to release soon), the --sample_name flag is added to postprocess_variants as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/334
https://github.com/google/deepvariant/issues/334:254,deployability,releas,release,254,"Ah yes, thanks @arostamianfar for providing the `--regions ""chr20:1,000-10,000""` example and I can now reproduce the issue. It seems like my previous internal ""fix"" hasn't really fixed it, so I'll send in another fix, which will come out in the upcoming release :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/334
https://github.com/google/deepvariant/issues/335:351,availability,state,state,351,"Since you're running this via docker this sounds like more of a docker / linux permission issue than a Deepvariant issue. I personally haven't done it but later versions of Docker do allow you to run without sudo access. Please take a look here https://askubuntu.com/questions/477551/how-can-i-use-docker-without-sudo . I would, however, additionally state that you want to make sure your account does full access to those folders / file as well. I'll let the Deepvariant team also add in any other suggestions as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/335
https://github.com/google/deepvariant/issues/335:161,deployability,version,versions,161,"Since you're running this via docker this sounds like more of a docker / linux permission issue than a Deepvariant issue. I personally haven't done it but later versions of Docker do allow you to run without sudo access. Please take a look here https://askubuntu.com/questions/477551/how-can-i-use-docker-without-sudo . I would, however, additionally state that you want to make sure your account does full access to those folders / file as well. I'll let the Deepvariant team also add in any other suggestions as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/335
https://github.com/google/deepvariant/issues/335:161,integrability,version,versions,161,"Since you're running this via docker this sounds like more of a docker / linux permission issue than a Deepvariant issue. I personally haven't done it but later versions of Docker do allow you to run without sudo access. Please take a look here https://askubuntu.com/questions/477551/how-can-i-use-docker-without-sudo . I would, however, additionally state that you want to make sure your account does full access to those folders / file as well. I'll let the Deepvariant team also add in any other suggestions as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/335
https://github.com/google/deepvariant/issues/335:351,integrability,state,state,351,"Since you're running this via docker this sounds like more of a docker / linux permission issue than a Deepvariant issue. I personally haven't done it but later versions of Docker do allow you to run without sudo access. Please take a look here https://askubuntu.com/questions/477551/how-can-i-use-docker-without-sudo . I would, however, additionally state that you want to make sure your account does full access to those folders / file as well. I'll let the Deepvariant team also add in any other suggestions as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/335
https://github.com/google/deepvariant/issues/335:161,modifiability,version,versions,161,"Since you're running this via docker this sounds like more of a docker / linux permission issue than a Deepvariant issue. I personally haven't done it but later versions of Docker do allow you to run without sudo access. Please take a look here https://askubuntu.com/questions/477551/how-can-i-use-docker-without-sudo . I would, however, additionally state that you want to make sure your account does full access to those folders / file as well. I'll let the Deepvariant team also add in any other suggestions as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/335
https://github.com/google/deepvariant/issues/335:397,reliability,doe,does,397,"Since you're running this via docker this sounds like more of a docker / linux permission issue than a Deepvariant issue. I personally haven't done it but later versions of Docker do allow you to run without sudo access. Please take a look here https://askubuntu.com/questions/477551/how-can-i-use-docker-without-sudo . I would, however, additionally state that you want to make sure your account does full access to those folders / file as well. I'll let the Deepvariant team also add in any other suggestions as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/335
https://github.com/google/deepvariant/issues/335:79,safety,permiss,permission,79,"Since you're running this via docker this sounds like more of a docker / linux permission issue than a Deepvariant issue. I personally haven't done it but later versions of Docker do allow you to run without sudo access. Please take a look here https://askubuntu.com/questions/477551/how-can-i-use-docker-without-sudo . I would, however, additionally state that you want to make sure your account does full access to those folders / file as well. I'll let the Deepvariant team also add in any other suggestions as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/335
https://github.com/google/deepvariant/issues/335:213,security,access,access,213,"Since you're running this via docker this sounds like more of a docker / linux permission issue than a Deepvariant issue. I personally haven't done it but later versions of Docker do allow you to run without sudo access. Please take a look here https://askubuntu.com/questions/477551/how-can-i-use-docker-without-sudo . I would, however, additionally state that you want to make sure your account does full access to those folders / file as well. I'll let the Deepvariant team also add in any other suggestions as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/335
https://github.com/google/deepvariant/issues/335:407,security,access,access,407,"Since you're running this via docker this sounds like more of a docker / linux permission issue than a Deepvariant issue. I personally haven't done it but later versions of Docker do allow you to run without sudo access. Please take a look here https://askubuntu.com/questions/477551/how-can-i-use-docker-without-sudo . I would, however, additionally state that you want to make sure your account does full access to those folders / file as well. I'll let the Deepvariant team also add in any other suggestions as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/335
https://github.com/google/deepvariant/issues/335:472,security,team,team,472,"Since you're running this via docker this sounds like more of a docker / linux permission issue than a Deepvariant issue. I personally haven't done it but later versions of Docker do allow you to run without sudo access. Please take a look here https://askubuntu.com/questions/477551/how-can-i-use-docker-without-sudo . I would, however, additionally state that you want to make sure your account does full access to those folders / file as well. I'll let the Deepvariant team also add in any other suggestions as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/335
https://github.com/google/deepvariant/issues/335:124,usability,person,personally,124,"Since you're running this via docker this sounds like more of a docker / linux permission issue than a Deepvariant issue. I personally haven't done it but later versions of Docker do allow you to run without sudo access. Please take a look here https://askubuntu.com/questions/477551/how-can-i-use-docker-without-sudo . I would, however, additionally state that you want to make sure your account does full access to those folders / file as well. I'll let the Deepvariant team also add in any other suggestions as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/335
https://github.com/google/deepvariant/issues/335:96,deployability,instal,install,96,That works for me (Ubuntu 18.04.1-based OS with Docker 19.03.6): https://docs.docker.com/engine/install/linux-postinstall/#manage-docker-as-a-non-root-user,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/335
https://github.com/google/deepvariant/issues/335:123,deployability,manag,manage-docker-as-a-non-root-user,123,That works for me (Ubuntu 18.04.1-based OS with Docker 19.03.6): https://docs.docker.com/engine/install/linux-postinstall/#manage-docker-as-a-non-root-user,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/335
https://github.com/google/deepvariant/issues/335:123,energy efficiency,manag,manage-docker-as-a-non-root-user,123,That works for me (Ubuntu 18.04.1-based OS with Docker 19.03.6): https://docs.docker.com/engine/install/linux-postinstall/#manage-docker-as-a-non-root-user,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/335
https://github.com/google/deepvariant/issues/335:123,safety,manag,manage-docker-as-a-non-root-user,123,That works for me (Ubuntu 18.04.1-based OS with Docker 19.03.6): https://docs.docker.com/engine/install/linux-postinstall/#manage-docker-as-a-non-root-user,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/335
https://github.com/google/deepvariant/issues/335:151,usability,user,user,151,That works for me (Ubuntu 18.04.1-based OS with Docker 19.03.6): https://docs.docker.com/engine/install/linux-postinstall/#manage-docker-as-a-non-root-user,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/335
https://github.com/google/deepvariant/issues/335:50,usability,close,close,50,"Thanks for the suggestion, @Roj4ck. @PlatonB I'll close this issue, but feel free to reopen if you have other questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/335
https://github.com/google/deepvariant/issues/336:26,integrability,sub,submission,26,Found some details in the submission,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/336
https://github.com/google/deepvariant/issues/337:76,deployability,contain,container,76,"Hi @BIjoy92 . The problem is that the paths are different inside the docker container than outside. The part of the command that says `-v ""${INPUT_DIR}"":""/input""` mounts the outside $INPUT_DIR/ to the inside /input/, which means the reads can be accessed at the following path: `--reads=/input/BWA_Markdup_sort.bam`. The same change is needed for all your inputs and outputs, e.g. `/output/`. See the Quick Start for an example: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. I also noticed you're using the gcr.io docker version, which works fine, but all our newer documentation uses `google/deepvariant:""${BIN_VERSION}""`, so I just want to check: what documentation were you using for reference? Thanks! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/337
https://github.com/google/deepvariant/issues/337:557,deployability,version,version,557,"Hi @BIjoy92 . The problem is that the paths are different inside the docker container than outside. The part of the command that says `-v ""${INPUT_DIR}"":""/input""` mounts the outside $INPUT_DIR/ to the inside /input/, which means the reads can be accessed at the following path: `--reads=/input/BWA_Markdup_sort.bam`. The same change is needed for all your inputs and outputs, e.g. `/output/`. See the Quick Start for an example: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. I also noticed you're using the gcr.io docker version, which works fine, but all our newer documentation uses `google/deepvariant:""${BIN_VERSION}""`, so I just want to check: what documentation were you using for reference? Thanks! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/337
https://github.com/google/deepvariant/issues/337:557,integrability,version,version,557,"Hi @BIjoy92 . The problem is that the paths are different inside the docker container than outside. The part of the command that says `-v ""${INPUT_DIR}"":""/input""` mounts the outside $INPUT_DIR/ to the inside /input/, which means the reads can be accessed at the following path: `--reads=/input/BWA_Markdup_sort.bam`. The same change is needed for all your inputs and outputs, e.g. `/output/`. See the Quick Start for an example: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. I also noticed you're using the gcr.io docker version, which works fine, but all our newer documentation uses `google/deepvariant:""${BIN_VERSION}""`, so I just want to check: what documentation were you using for reference? Thanks! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/337
https://github.com/google/deepvariant/issues/337:557,modifiability,version,version,557,"Hi @BIjoy92 . The problem is that the paths are different inside the docker container than outside. The part of the command that says `-v ""${INPUT_DIR}"":""/input""` mounts the outside $INPUT_DIR/ to the inside /input/, which means the reads can be accessed at the following path: `--reads=/input/BWA_Markdup_sort.bam`. The same change is needed for all your inputs and outputs, e.g. `/output/`. See the Quick Start for an example: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. I also noticed you're using the gcr.io docker version, which works fine, but all our newer documentation uses `google/deepvariant:""${BIN_VERSION}""`, so I just want to check: what documentation were you using for reference? Thanks! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/337
https://github.com/google/deepvariant/issues/337:155,safety,input,input,155,"Hi @BIjoy92 . The problem is that the paths are different inside the docker container than outside. The part of the command that says `-v ""${INPUT_DIR}"":""/input""` mounts the outside $INPUT_DIR/ to the inside /input/, which means the reads can be accessed at the following path: `--reads=/input/BWA_Markdup_sort.bam`. The same change is needed for all your inputs and outputs, e.g. `/output/`. See the Quick Start for an example: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. I also noticed you're using the gcr.io docker version, which works fine, but all our newer documentation uses `google/deepvariant:""${BIN_VERSION}""`, so I just want to check: what documentation were you using for reference? Thanks! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/337
https://github.com/google/deepvariant/issues/337:209,safety,input,input,209,"Hi @BIjoy92 . The problem is that the paths are different inside the docker container than outside. The part of the command that says `-v ""${INPUT_DIR}"":""/input""` mounts the outside $INPUT_DIR/ to the inside /input/, which means the reads can be accessed at the following path: `--reads=/input/BWA_Markdup_sort.bam`. The same change is needed for all your inputs and outputs, e.g. `/output/`. See the Quick Start for an example: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. I also noticed you're using the gcr.io docker version, which works fine, but all our newer documentation uses `google/deepvariant:""${BIN_VERSION}""`, so I just want to check: what documentation were you using for reference? Thanks! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/337
https://github.com/google/deepvariant/issues/337:288,safety,input,input,288,"Hi @BIjoy92 . The problem is that the paths are different inside the docker container than outside. The part of the command that says `-v ""${INPUT_DIR}"":""/input""` mounts the outside $INPUT_DIR/ to the inside /input/, which means the reads can be accessed at the following path: `--reads=/input/BWA_Markdup_sort.bam`. The same change is needed for all your inputs and outputs, e.g. `/output/`. See the Quick Start for an example: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. I also noticed you're using the gcr.io docker version, which works fine, but all our newer documentation uses `google/deepvariant:""${BIN_VERSION}""`, so I just want to check: what documentation were you using for reference? Thanks! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/337
https://github.com/google/deepvariant/issues/337:356,safety,input,inputs,356,"Hi @BIjoy92 . The problem is that the paths are different inside the docker container than outside. The part of the command that says `-v ""${INPUT_DIR}"":""/input""` mounts the outside $INPUT_DIR/ to the inside /input/, which means the reads can be accessed at the following path: `--reads=/input/BWA_Markdup_sort.bam`. The same change is needed for all your inputs and outputs, e.g. `/output/`. See the Quick Start for an example: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. I also noticed you're using the gcr.io docker version, which works fine, but all our newer documentation uses `google/deepvariant:""${BIN_VERSION}""`, so I just want to check: what documentation were you using for reference? Thanks! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/337
https://github.com/google/deepvariant/issues/337:246,security,access,accessed,246,"Hi @BIjoy92 . The problem is that the paths are different inside the docker container than outside. The part of the command that says `-v ""${INPUT_DIR}"":""/input""` mounts the outside $INPUT_DIR/ to the inside /input/, which means the reads can be accessed at the following path: `--reads=/input/BWA_Markdup_sort.bam`. The same change is needed for all your inputs and outputs, e.g. `/output/`. See the Quick Start for an example: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. I also noticed you're using the gcr.io docker version, which works fine, but all our newer documentation uses `google/deepvariant:""${BIN_VERSION}""`, so I just want to check: what documentation were you using for reference? Thanks! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/337
https://github.com/google/deepvariant/issues/337:116,usability,command,command,116,"Hi @BIjoy92 . The problem is that the paths are different inside the docker container than outside. The part of the command that says `-v ""${INPUT_DIR}"":""/input""` mounts the outside $INPUT_DIR/ to the inside /input/, which means the reads can be accessed at the following path: `--reads=/input/BWA_Markdup_sort.bam`. The same change is needed for all your inputs and outputs, e.g. `/output/`. See the Quick Start for an example: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. I also noticed you're using the gcr.io docker version, which works fine, but all our newer documentation uses `google/deepvariant:""${BIN_VERSION}""`, so I just want to check: what documentation were you using for reference? Thanks! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/337
https://github.com/google/deepvariant/issues/337:155,usability,input,input,155,"Hi @BIjoy92 . The problem is that the paths are different inside the docker container than outside. The part of the command that says `-v ""${INPUT_DIR}"":""/input""` mounts the outside $INPUT_DIR/ to the inside /input/, which means the reads can be accessed at the following path: `--reads=/input/BWA_Markdup_sort.bam`. The same change is needed for all your inputs and outputs, e.g. `/output/`. See the Quick Start for an example: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. I also noticed you're using the gcr.io docker version, which works fine, but all our newer documentation uses `google/deepvariant:""${BIN_VERSION}""`, so I just want to check: what documentation were you using for reference? Thanks! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/337
https://github.com/google/deepvariant/issues/337:209,usability,input,input,209,"Hi @BIjoy92 . The problem is that the paths are different inside the docker container than outside. The part of the command that says `-v ""${INPUT_DIR}"":""/input""` mounts the outside $INPUT_DIR/ to the inside /input/, which means the reads can be accessed at the following path: `--reads=/input/BWA_Markdup_sort.bam`. The same change is needed for all your inputs and outputs, e.g. `/output/`. See the Quick Start for an example: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. I also noticed you're using the gcr.io docker version, which works fine, but all our newer documentation uses `google/deepvariant:""${BIN_VERSION}""`, so I just want to check: what documentation were you using for reference? Thanks! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/337
https://github.com/google/deepvariant/issues/337:288,usability,input,input,288,"Hi @BIjoy92 . The problem is that the paths are different inside the docker container than outside. The part of the command that says `-v ""${INPUT_DIR}"":""/input""` mounts the outside $INPUT_DIR/ to the inside /input/, which means the reads can be accessed at the following path: `--reads=/input/BWA_Markdup_sort.bam`. The same change is needed for all your inputs and outputs, e.g. `/output/`. See the Quick Start for an example: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. I also noticed you're using the gcr.io docker version, which works fine, but all our newer documentation uses `google/deepvariant:""${BIN_VERSION}""`, so I just want to check: what documentation were you using for reference? Thanks! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/337
https://github.com/google/deepvariant/issues/337:356,usability,input,inputs,356,"Hi @BIjoy92 . The problem is that the paths are different inside the docker container than outside. The part of the command that says `-v ""${INPUT_DIR}"":""/input""` mounts the outside $INPUT_DIR/ to the inside /input/, which means the reads can be accessed at the following path: `--reads=/input/BWA_Markdup_sort.bam`. The same change is needed for all your inputs and outputs, e.g. `/output/`. See the Quick Start for an example: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. I also noticed you're using the gcr.io docker version, which works fine, but all our newer documentation uses `google/deepvariant:""${BIN_VERSION}""`, so I just want to check: what documentation were you using for reference? Thanks! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/337
https://github.com/google/deepvariant/issues/337:602,usability,document,documentation,602,"Hi @BIjoy92 . The problem is that the paths are different inside the docker container than outside. The part of the command that says `-v ""${INPUT_DIR}"":""/input""` mounts the outside $INPUT_DIR/ to the inside /input/, which means the reads can be accessed at the following path: `--reads=/input/BWA_Markdup_sort.bam`. The same change is needed for all your inputs and outputs, e.g. `/output/`. See the Quick Start for an example: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. I also noticed you're using the gcr.io docker version, which works fine, but all our newer documentation uses `google/deepvariant:""${BIN_VERSION}""`, so I just want to check: what documentation were you using for reference? Thanks! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/337
https://github.com/google/deepvariant/issues/337:690,usability,document,documentation,690,"Hi @BIjoy92 . The problem is that the paths are different inside the docker container than outside. The part of the command that says `-v ""${INPUT_DIR}"":""/input""` mounts the outside $INPUT_DIR/ to the inside /input/, which means the reads can be accessed at the following path: `--reads=/input/BWA_Markdup_sort.bam`. The same change is needed for all your inputs and outputs, e.g. `/output/`. See the Quick Start for an example: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. I also noticed you're using the gcr.io docker version, which works fine, but all our newer documentation uses `google/deepvariant:""${BIN_VERSION}""`, so I just want to check: what documentation were you using for reference? Thanks! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/337
https://github.com/google/deepvariant/issues/337:29,usability,help,help,29,Hi Maria. Thank you for your help. I appreciate your prompt reply. Thanks. BIjoy92,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/337
https://github.com/google/deepvariant/issues/337:26,usability,close,close,26,You're very welcome! I'll close this issue.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/337
https://github.com/google/deepvariant/issues/338:288,availability,down,downsampled,288,"Since you already have 200X coverage of WGS data, the best quality variant calls would probably come from just taking the WGS data and running with that alone with `--model_type=WGS`. This is because the pileup images only fit up to 95 reads for each variant, so the 200X will already be downsampled during runtime to fit. The WES data will have some inherent biases due to the target enrichment process, so combining the datasets would carry its own potential issues. Luckily you don't need that since you have plenty of WGS data. Also I just want to check that you are only doing germline variant calling, since in my experience people with high coverage are often looking for low-frequency variants. DeepVariant is built and trained for germline variant calling only, and it can only report heterozygous, homozygous reference, and homozygous alternate genotypes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/338
https://github.com/google/deepvariant/issues/338:683,energy efficiency,frequenc,frequency,683,"Since you already have 200X coverage of WGS data, the best quality variant calls would probably come from just taking the WGS data and running with that alone with `--model_type=WGS`. This is because the pileup images only fit up to 95 reads for each variant, so the 200X will already be downsampled during runtime to fit. The WES data will have some inherent biases due to the target enrichment process, so combining the datasets would carry its own potential issues. Luckily you don't need that since you have plenty of WGS data. Also I just want to check that you are only doing germline variant calling, since in my experience people with high coverage are often looking for low-frequency variants. DeepVariant is built and trained for germline variant calling only, and it can only report heterozygous, homozygous reference, and homozygous alternate genotypes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/338
https://github.com/google/deepvariant/issues/338:28,testability,coverag,coverage,28,"Since you already have 200X coverage of WGS data, the best quality variant calls would probably come from just taking the WGS data and running with that alone with `--model_type=WGS`. This is because the pileup images only fit up to 95 reads for each variant, so the 200X will already be downsampled during runtime to fit. The WES data will have some inherent biases due to the target enrichment process, so combining the datasets would carry its own potential issues. Luckily you don't need that since you have plenty of WGS data. Also I just want to check that you are only doing germline variant calling, since in my experience people with high coverage are often looking for low-frequency variants. DeepVariant is built and trained for germline variant calling only, and it can only report heterozygous, homozygous reference, and homozygous alternate genotypes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/338
https://github.com/google/deepvariant/issues/338:648,testability,coverag,coverage,648,"Since you already have 200X coverage of WGS data, the best quality variant calls would probably come from just taking the WGS data and running with that alone with `--model_type=WGS`. This is because the pileup images only fit up to 95 reads for each variant, so the 200X will already be downsampled during runtime to fit. The WES data will have some inherent biases due to the target enrichment process, so combining the datasets would carry its own potential issues. Luckily you don't need that since you have plenty of WGS data. Also I just want to check that you are only doing germline variant calling, since in my experience people with high coverage are often looking for low-frequency variants. DeepVariant is built and trained for germline variant calling only, and it can only report heterozygous, homozygous reference, and homozygous alternate genotypes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/338
https://github.com/google/deepvariant/issues/338:620,usability,experien,experience,620,"Since you already have 200X coverage of WGS data, the best quality variant calls would probably come from just taking the WGS data and running with that alone with `--model_type=WGS`. This is because the pileup images only fit up to 95 reads for each variant, so the 200X will already be downsampled during runtime to fit. The WES data will have some inherent biases due to the target enrichment process, so combining the datasets would carry its own potential issues. Luckily you don't need that since you have plenty of WGS data. Also I just want to check that you are only doing germline variant calling, since in my experience people with high coverage are often looking for low-frequency variants. DeepVariant is built and trained for germline variant calling only, and it can only report heterozygous, homozygous reference, and homozygous alternate genotypes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/338
https://github.com/google/deepvariant/issues/338:324,availability,down,downsampled,324,"Hi Maria,. thanks for your answer. I will thus not merge WGS and WES data, but analyze them separately instead. This way I can compare and have a look at the potential issues you mention. . And thanks for checking: I am indeed looking at germline variant calling. A question regarding the pileup images you mention that are downsampled during runtime to 95x:. Here (https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-details.md#updates-on-deepvariant-since-precisionfda-truth-challenge-and-biorxiv-preprint) it says: ""_The biggest change was to move away from RGB-encoded (3-channel) pileup images and instead represent the aligned read data using a multi-channel tensor data layout. We currently represent the data as a 6-channel raw tensor in which we encode: ..._"" Is the downsampling to 95x still applied in the new design? thanks, Christian.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/338
https://github.com/google/deepvariant/issues/338:790,availability,down,downsampling,790,"Hi Maria,. thanks for your answer. I will thus not merge WGS and WES data, but analyze them separately instead. This way I can compare and have a look at the potential issues you mention. . And thanks for checking: I am indeed looking at germline variant calling. A question regarding the pileup images you mention that are downsampled during runtime to 95x:. Here (https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-details.md#updates-on-deepvariant-since-precisionfda-truth-challenge-and-biorxiv-preprint) it says: ""_The biggest change was to move away from RGB-encoded (3-channel) pileup images and instead represent the aligned read data using a multi-channel tensor data layout. We currently represent the data as a 6-channel raw tensor in which we encode: ..._"" Is the downsampling to 95x still applied in the new design? thanks, Christian.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/338
https://github.com/google/deepvariant/issues/338:443,deployability,updat,updates-on-deepvariant-since-precisionfda-truth-challenge-and-biorxiv-preprint,443,"Hi Maria,. thanks for your answer. I will thus not merge WGS and WES data, but analyze them separately instead. This way I can compare and have a look at the potential issues you mention. . And thanks for checking: I am indeed looking at germline variant calling. A question regarding the pileup images you mention that are downsampled during runtime to 95x:. Here (https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-details.md#updates-on-deepvariant-since-precisionfda-truth-challenge-and-biorxiv-preprint) it says: ""_The biggest change was to move away from RGB-encoded (3-channel) pileup images and instead represent the aligned read data using a multi-channel tensor data layout. We currently represent the data as a 6-channel raw tensor in which we encode: ..._"" Is the downsampling to 95x still applied in the new design? thanks, Christian.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/338
https://github.com/google/deepvariant/issues/338:702,energy efficiency,current,currently,702,"Hi Maria,. thanks for your answer. I will thus not merge WGS and WES data, but analyze them separately instead. This way I can compare and have a look at the potential issues you mention. . And thanks for checking: I am indeed looking at germline variant calling. A question regarding the pileup images you mention that are downsampled during runtime to 95x:. Here (https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-details.md#updates-on-deepvariant-since-precisionfda-truth-challenge-and-biorxiv-preprint) it says: ""_The biggest change was to move away from RGB-encoded (3-channel) pileup images and instead represent the aligned read data using a multi-channel tensor data layout. We currently represent the data as a 6-channel raw tensor in which we encode: ..._"" Is the downsampling to 95x still applied in the new design? thanks, Christian.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/338
https://github.com/google/deepvariant/issues/338:443,safety,updat,updates-on-deepvariant-since-precisionfda-truth-challenge-and-biorxiv-preprint,443,"Hi Maria,. thanks for your answer. I will thus not merge WGS and WES data, but analyze them separately instead. This way I can compare and have a look at the potential issues you mention. . And thanks for checking: I am indeed looking at germline variant calling. A question regarding the pileup images you mention that are downsampled during runtime to 95x:. Here (https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-details.md#updates-on-deepvariant-since-precisionfda-truth-challenge-and-biorxiv-preprint) it says: ""_The biggest change was to move away from RGB-encoded (3-channel) pileup images and instead represent the aligned read data using a multi-channel tensor data layout. We currently represent the data as a 6-channel raw tensor in which we encode: ..._"" Is the downsampling to 95x still applied in the new design? thanks, Christian.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/338
https://github.com/google/deepvariant/issues/338:443,security,updat,updates-on-deepvariant-since-precisionfda-truth-challenge-and-biorxiv-preprint,443,"Hi Maria,. thanks for your answer. I will thus not merge WGS and WES data, but analyze them separately instead. This way I can compare and have a look at the potential issues you mention. . And thanks for checking: I am indeed looking at germline variant calling. A question regarding the pileup images you mention that are downsampled during runtime to 95x:. Here (https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-details.md#updates-on-deepvariant-since-precisionfda-truth-challenge-and-biorxiv-preprint) it says: ""_The biggest change was to move away from RGB-encoded (3-channel) pileup images and instead represent the aligned read data using a multi-channel tensor data layout. We currently represent the data as a 6-channel raw tensor in which we encode: ..._"" Is the downsampling to 95x still applied in the new design? thanks, Christian.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/338
https://github.com/google/deepvariant/issues/338:196,availability,down,downsampling,196,"Sounds good! Yes, we still call it a pileup ""image"", it is just in the form of a TensorFlow tensor with 6 channels instead of an RGB image, which was limited to 3 color channels plus opacity. The downsampling to 95x is still applied and has been in all versions of DeepVariant. This blog post gives more detail on how the pileup images are structured: https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/338
https://github.com/google/deepvariant/issues/338:253,deployability,version,versions,253,"Sounds good! Yes, we still call it a pileup ""image"", it is just in the form of a TensorFlow tensor with 6 channels instead of an RGB image, which was limited to 3 color channels plus opacity. The downsampling to 95x is still applied and has been in all versions of DeepVariant. This blog post gives more detail on how the pileup images are structured: https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/338
https://github.com/google/deepvariant/issues/338:253,integrability,version,versions,253,"Sounds good! Yes, we still call it a pileup ""image"", it is just in the form of a TensorFlow tensor with 6 channels instead of an RGB image, which was limited to 3 color channels plus opacity. The downsampling to 95x is still applied and has been in all versions of DeepVariant. This blog post gives more detail on how the pileup images are structured: https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/338
https://github.com/google/deepvariant/issues/338:253,modifiability,version,versions,253,"Sounds good! Yes, we still call it a pileup ""image"", it is just in the form of a TensorFlow tensor with 6 channels instead of an RGB image, which was limited to 3 color channels plus opacity. The downsampling to 95x is still applied and has been in all versions of DeepVariant. This blog post gives more detail on how the pileup images are structured: https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/338
https://github.com/google/deepvariant/issues/338:67,energy efficiency,model,models,67,"Hi Maria (@MariaNattestad),. Do you have a comparative analysis on models trained with larger values set [here](https://github.com/google/deepvariant/blob/master/deepvariant/dv_constants.py#L43-L44) for `PILEUP_DEFAULT_HEIGHT` allowing for larger values of coverage than 95? https://github.com/google/deepvariant/blob/master/deepvariant/dv_constants.py#L43-L44. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/338
https://github.com/google/deepvariant/issues/338:67,security,model,models,67,"Hi Maria (@MariaNattestad),. Do you have a comparative analysis on models trained with larger values set [here](https://github.com/google/deepvariant/blob/master/deepvariant/dv_constants.py#L43-L44) for `PILEUP_DEFAULT_HEIGHT` allowing for larger values of coverage than 95? https://github.com/google/deepvariant/blob/master/deepvariant/dv_constants.py#L43-L44. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/338
https://github.com/google/deepvariant/issues/338:257,testability,coverag,coverage,257,"Hi Maria (@MariaNattestad),. Do you have a comparative analysis on models trained with larger values set [here](https://github.com/google/deepvariant/blob/master/deepvariant/dv_constants.py#L43-L44) for `PILEUP_DEFAULT_HEIGHT` allowing for larger values of coverage than 95? https://github.com/google/deepvariant/blob/master/deepvariant/dv_constants.py#L43-L44. Thanks,. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/338
https://github.com/google/deepvariant/issues/338:260,deployability,releas,released,260,"Hi @pgrosu . The default of `--pileup_image_height=100` was initially determined by the most common use case of Illumina WGS data. In terms of tweaking pileup_image_height, there are 2 past experiments that I can recall right now:. ## Illumina WES:. Before we released an Illumina WES model, I experimented with training a WES model with `--pileup_image_height=200`. At the time (end of 2017), I observed that the Indel F1 dropped quite a bit when increasing the height, but helped SNP F1 a bit. This investigationwas why we didn’t change this default for the Illumina WES model release. ## PacBio:. In Nov 2019, I trained a model with `--pileup_image_height=75` to see if we can decrease runtime without too much accuracy tradeoff. At the time, my results were:. * Reducing height to 75 (from 100) reduces calling time to ~85% (from 3h20m to 2h50m). * Accuracy (on case study chr20). - Indel F1: 0.983872 —> 0.982728. - SNP F1: 0.998913 —> 0.998867. Based on feedback, we decided it’s not a high priority to continue. Because:. 1. Amplicon will likely use the height=100. 2. 30min is not a big bottleneck now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/338
https://github.com/google/deepvariant/issues/338:396,deployability,observ,observed,396,"Hi @pgrosu . The default of `--pileup_image_height=100` was initially determined by the most common use case of Illumina WGS data. In terms of tweaking pileup_image_height, there are 2 past experiments that I can recall right now:. ## Illumina WES:. Before we released an Illumina WES model, I experimented with training a WES model with `--pileup_image_height=200`. At the time (end of 2017), I observed that the Indel F1 dropped quite a bit when increasing the height, but helped SNP F1 a bit. This investigationwas why we didn’t change this default for the Illumina WES model release. ## PacBio:. In Nov 2019, I trained a model with `--pileup_image_height=75` to see if we can decrease runtime without too much accuracy tradeoff. At the time, my results were:. * Reducing height to 75 (from 100) reduces calling time to ~85% (from 3h20m to 2h50m). * Accuracy (on case study chr20). - Indel F1: 0.983872 —> 0.982728. - SNP F1: 0.998913 —> 0.998867. Based on feedback, we decided it’s not a high priority to continue. Because:. 1. Amplicon will likely use the height=100. 2. 30min is not a big bottleneck now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/338
https://github.com/google/deepvariant/issues/338:579,deployability,releas,release,579,"Hi @pgrosu . The default of `--pileup_image_height=100` was initially determined by the most common use case of Illumina WGS data. In terms of tweaking pileup_image_height, there are 2 past experiments that I can recall right now:. ## Illumina WES:. Before we released an Illumina WES model, I experimented with training a WES model with `--pileup_image_height=200`. At the time (end of 2017), I observed that the Indel F1 dropped quite a bit when increasing the height, but helped SNP F1 a bit. This investigationwas why we didn’t change this default for the Illumina WES model release. ## PacBio:. In Nov 2019, I trained a model with `--pileup_image_height=75` to see if we can decrease runtime without too much accuracy tradeoff. At the time, my results were:. * Reducing height to 75 (from 100) reduces calling time to ~85% (from 3h20m to 2h50m). * Accuracy (on case study chr20). - Indel F1: 0.983872 —> 0.982728. - SNP F1: 0.998913 —> 0.998867. Based on feedback, we decided it’s not a high priority to continue. Because:. 1. Amplicon will likely use the height=100. 2. 30min is not a big bottleneck now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/338
https://github.com/google/deepvariant/issues/338:1009,deployability,continu,continue,1009,"Hi @pgrosu . The default of `--pileup_image_height=100` was initially determined by the most common use case of Illumina WGS data. In terms of tweaking pileup_image_height, there are 2 past experiments that I can recall right now:. ## Illumina WES:. Before we released an Illumina WES model, I experimented with training a WES model with `--pileup_image_height=200`. At the time (end of 2017), I observed that the Indel F1 dropped quite a bit when increasing the height, but helped SNP F1 a bit. This investigationwas why we didn’t change this default for the Illumina WES model release. ## PacBio:. In Nov 2019, I trained a model with `--pileup_image_height=75` to see if we can decrease runtime without too much accuracy tradeoff. At the time, my results were:. * Reducing height to 75 (from 100) reduces calling time to ~85% (from 3h20m to 2h50m). * Accuracy (on case study chr20). - Indel F1: 0.983872 —> 0.982728. - SNP F1: 0.998913 —> 0.998867. Based on feedback, we decided it’s not a high priority to continue. Because:. 1. Amplicon will likely use the height=100. 2. 30min is not a big bottleneck now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/338
https://github.com/google/deepvariant/issues/338:285,energy efficiency,model,model,285,"Hi @pgrosu . The default of `--pileup_image_height=100` was initially determined by the most common use case of Illumina WGS data. In terms of tweaking pileup_image_height, there are 2 past experiments that I can recall right now:. ## Illumina WES:. Before we released an Illumina WES model, I experimented with training a WES model with `--pileup_image_height=200`. At the time (end of 2017), I observed that the Indel F1 dropped quite a bit when increasing the height, but helped SNP F1 a bit. This investigationwas why we didn’t change this default for the Illumina WES model release. ## PacBio:. In Nov 2019, I trained a model with `--pileup_image_height=75` to see if we can decrease runtime without too much accuracy tradeoff. At the time, my results were:. * Reducing height to 75 (from 100) reduces calling time to ~85% (from 3h20m to 2h50m). * Accuracy (on case study chr20). - Indel F1: 0.983872 —> 0.982728. - SNP F1: 0.998913 —> 0.998867. Based on feedback, we decided it’s not a high priority to continue. Because:. 1. Amplicon will likely use the height=100. 2. 30min is not a big bottleneck now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/338
https://github.com/google/deepvariant/issues/338:327,energy efficiency,model,model,327,"Hi @pgrosu . The default of `--pileup_image_height=100` was initially determined by the most common use case of Illumina WGS data. In terms of tweaking pileup_image_height, there are 2 past experiments that I can recall right now:. ## Illumina WES:. Before we released an Illumina WES model, I experimented with training a WES model with `--pileup_image_height=200`. At the time (end of 2017), I observed that the Indel F1 dropped quite a bit when increasing the height, but helped SNP F1 a bit. This investigationwas why we didn’t change this default for the Illumina WES model release. ## PacBio:. In Nov 2019, I trained a model with `--pileup_image_height=75` to see if we can decrease runtime without too much accuracy tradeoff. At the time, my results were:. * Reducing height to 75 (from 100) reduces calling time to ~85% (from 3h20m to 2h50m). * Accuracy (on case study chr20). - Indel F1: 0.983872 —> 0.982728. - SNP F1: 0.998913 —> 0.998867. Based on feedback, we decided it’s not a high priority to continue. Because:. 1. Amplicon will likely use the height=100. 2. 30min is not a big bottleneck now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/338
https://github.com/google/deepvariant/issues/338:573,energy efficiency,model,model,573,"Hi @pgrosu . The default of `--pileup_image_height=100` was initially determined by the most common use case of Illumina WGS data. In terms of tweaking pileup_image_height, there are 2 past experiments that I can recall right now:. ## Illumina WES:. Before we released an Illumina WES model, I experimented with training a WES model with `--pileup_image_height=200`. At the time (end of 2017), I observed that the Indel F1 dropped quite a bit when increasing the height, but helped SNP F1 a bit. This investigationwas why we didn’t change this default for the Illumina WES model release. ## PacBio:. In Nov 2019, I trained a model with `--pileup_image_height=75` to see if we can decrease runtime without too much accuracy tradeoff. At the time, my results were:. * Reducing height to 75 (from 100) reduces calling time to ~85% (from 3h20m to 2h50m). * Accuracy (on case study chr20). - Indel F1: 0.983872 —> 0.982728. - SNP F1: 0.998913 —> 0.998867. Based on feedback, we decided it’s not a high priority to continue. Because:. 1. Amplicon will likely use the height=100. 2. 30min is not a big bottleneck now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/338
https://github.com/google/deepvariant/issues/338:625,energy efficiency,model,model,625,"Hi @pgrosu . The default of `--pileup_image_height=100` was initially determined by the most common use case of Illumina WGS data. In terms of tweaking pileup_image_height, there are 2 past experiments that I can recall right now:. ## Illumina WES:. Before we released an Illumina WES model, I experimented with training a WES model with `--pileup_image_height=200`. At the time (end of 2017), I observed that the Indel F1 dropped quite a bit when increasing the height, but helped SNP F1 a bit. This investigationwas why we didn’t change this default for the Illumina WES model release. ## PacBio:. In Nov 2019, I trained a model with `--pileup_image_height=75` to see if we can decrease runtime without too much accuracy tradeoff. At the time, my results were:. * Reducing height to 75 (from 100) reduces calling time to ~85% (from 3h20m to 2h50m). * Accuracy (on case study chr20). - Indel F1: 0.983872 —> 0.982728. - SNP F1: 0.998913 —> 0.998867. Based on feedback, we decided it’s not a high priority to continue. Because:. 1. Amplicon will likely use the height=100. 2. 30min is not a big bottleneck now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/338
https://github.com/google/deepvariant/issues/338:766,energy efficiency,Reduc,Reducing,766,"Hi @pgrosu . The default of `--pileup_image_height=100` was initially determined by the most common use case of Illumina WGS data. In terms of tweaking pileup_image_height, there are 2 past experiments that I can recall right now:. ## Illumina WES:. Before we released an Illumina WES model, I experimented with training a WES model with `--pileup_image_height=200`. At the time (end of 2017), I observed that the Indel F1 dropped quite a bit when increasing the height, but helped SNP F1 a bit. This investigationwas why we didn’t change this default for the Illumina WES model release. ## PacBio:. In Nov 2019, I trained a model with `--pileup_image_height=75` to see if we can decrease runtime without too much accuracy tradeoff. At the time, my results were:. * Reducing height to 75 (from 100) reduces calling time to ~85% (from 3h20m to 2h50m). * Accuracy (on case study chr20). - Indel F1: 0.983872 —> 0.982728. - SNP F1: 0.998913 —> 0.998867. Based on feedback, we decided it’s not a high priority to continue. Because:. 1. Amplicon will likely use the height=100. 2. 30min is not a big bottleneck now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/338
https://github.com/google/deepvariant/issues/338:799,energy efficiency,reduc,reduces,799,"Hi @pgrosu . The default of `--pileup_image_height=100` was initially determined by the most common use case of Illumina WGS data. In terms of tweaking pileup_image_height, there are 2 past experiments that I can recall right now:. ## Illumina WES:. Before we released an Illumina WES model, I experimented with training a WES model with `--pileup_image_height=200`. At the time (end of 2017), I observed that the Indel F1 dropped quite a bit when increasing the height, but helped SNP F1 a bit. This investigationwas why we didn’t change this default for the Illumina WES model release. ## PacBio:. In Nov 2019, I trained a model with `--pileup_image_height=75` to see if we can decrease runtime without too much accuracy tradeoff. At the time, my results were:. * Reducing height to 75 (from 100) reduces calling time to ~85% (from 3h20m to 2h50m). * Accuracy (on case study chr20). - Indel F1: 0.983872 —> 0.982728. - SNP F1: 0.998913 —> 0.998867. Based on feedback, we decided it’s not a high priority to continue. Because:. 1. Amplicon will likely use the height=100. 2. 30min is not a big bottleneck now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/338
https://github.com/google/deepvariant/issues/338:591,modifiability,Pac,PacBio,591,"Hi @pgrosu . The default of `--pileup_image_height=100` was initially determined by the most common use case of Illumina WGS data. In terms of tweaking pileup_image_height, there are 2 past experiments that I can recall right now:. ## Illumina WES:. Before we released an Illumina WES model, I experimented with training a WES model with `--pileup_image_height=200`. At the time (end of 2017), I observed that the Indel F1 dropped quite a bit when increasing the height, but helped SNP F1 a bit. This investigationwas why we didn’t change this default for the Illumina WES model release. ## PacBio:. In Nov 2019, I trained a model with `--pileup_image_height=75` to see if we can decrease runtime without too much accuracy tradeoff. At the time, my results were:. * Reducing height to 75 (from 100) reduces calling time to ~85% (from 3h20m to 2h50m). * Accuracy (on case study chr20). - Indel F1: 0.983872 —> 0.982728. - SNP F1: 0.998913 —> 0.998867. Based on feedback, we decided it’s not a high priority to continue. Because:. 1. Amplicon will likely use the height=100. 2. 30min is not a big bottleneck now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/338
https://github.com/google/deepvariant/issues/338:374,performance,time,time,374,"Hi @pgrosu . The default of `--pileup_image_height=100` was initially determined by the most common use case of Illumina WGS data. In terms of tweaking pileup_image_height, there are 2 past experiments that I can recall right now:. ## Illumina WES:. Before we released an Illumina WES model, I experimented with training a WES model with `--pileup_image_height=200`. At the time (end of 2017), I observed that the Indel F1 dropped quite a bit when increasing the height, but helped SNP F1 a bit. This investigationwas why we didn’t change this default for the Illumina WES model release. ## PacBio:. In Nov 2019, I trained a model with `--pileup_image_height=75` to see if we can decrease runtime without too much accuracy tradeoff. At the time, my results were:. * Reducing height to 75 (from 100) reduces calling time to ~85% (from 3h20m to 2h50m). * Accuracy (on case study chr20). - Indel F1: 0.983872 —> 0.982728. - SNP F1: 0.998913 —> 0.998867. Based on feedback, we decided it’s not a high priority to continue. Because:. 1. Amplicon will likely use the height=100. 2. 30min is not a big bottleneck now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/338
https://github.com/google/deepvariant/issues/338:740,performance,time,time,740,"Hi @pgrosu . The default of `--pileup_image_height=100` was initially determined by the most common use case of Illumina WGS data. In terms of tweaking pileup_image_height, there are 2 past experiments that I can recall right now:. ## Illumina WES:. Before we released an Illumina WES model, I experimented with training a WES model with `--pileup_image_height=200`. At the time (end of 2017), I observed that the Indel F1 dropped quite a bit when increasing the height, but helped SNP F1 a bit. This investigationwas why we didn’t change this default for the Illumina WES model release. ## PacBio:. In Nov 2019, I trained a model with `--pileup_image_height=75` to see if we can decrease runtime without too much accuracy tradeoff. At the time, my results were:. * Reducing height to 75 (from 100) reduces calling time to ~85% (from 3h20m to 2h50m). * Accuracy (on case study chr20). - Indel F1: 0.983872 —> 0.982728. - SNP F1: 0.998913 —> 0.998867. Based on feedback, we decided it’s not a high priority to continue. Because:. 1. Amplicon will likely use the height=100. 2. 30min is not a big bottleneck now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/338
https://github.com/google/deepvariant/issues/338:815,performance,time,time,815,"Hi @pgrosu . The default of `--pileup_image_height=100` was initially determined by the most common use case of Illumina WGS data. In terms of tweaking pileup_image_height, there are 2 past experiments that I can recall right now:. ## Illumina WES:. Before we released an Illumina WES model, I experimented with training a WES model with `--pileup_image_height=200`. At the time (end of 2017), I observed that the Indel F1 dropped quite a bit when increasing the height, but helped SNP F1 a bit. This investigationwas why we didn’t change this default for the Illumina WES model release. ## PacBio:. In Nov 2019, I trained a model with `--pileup_image_height=75` to see if we can decrease runtime without too much accuracy tradeoff. At the time, my results were:. * Reducing height to 75 (from 100) reduces calling time to ~85% (from 3h20m to 2h50m). * Accuracy (on case study chr20). - Indel F1: 0.983872 —> 0.982728. - SNP F1: 0.998913 —> 0.998867. Based on feedback, we decided it’s not a high priority to continue. Because:. 1. Amplicon will likely use the height=100. 2. 30min is not a big bottleneck now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/338
https://github.com/google/deepvariant/issues/338:1095,performance,bottleneck,bottleneck,1095,"Hi @pgrosu . The default of `--pileup_image_height=100` was initially determined by the most common use case of Illumina WGS data. In terms of tweaking pileup_image_height, there are 2 past experiments that I can recall right now:. ## Illumina WES:. Before we released an Illumina WES model, I experimented with training a WES model with `--pileup_image_height=200`. At the time (end of 2017), I observed that the Indel F1 dropped quite a bit when increasing the height, but helped SNP F1 a bit. This investigationwas why we didn’t change this default for the Illumina WES model release. ## PacBio:. In Nov 2019, I trained a model with `--pileup_image_height=75` to see if we can decrease runtime without too much accuracy tradeoff. At the time, my results were:. * Reducing height to 75 (from 100) reduces calling time to ~85% (from 3h20m to 2h50m). * Accuracy (on case study chr20). - Indel F1: 0.983872 —> 0.982728. - SNP F1: 0.998913 —> 0.998867. Based on feedback, we decided it’s not a high priority to continue. Because:. 1. Amplicon will likely use the height=100. 2. 30min is not a big bottleneck now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/338
https://github.com/google/deepvariant/issues/338:285,security,model,model,285,"Hi @pgrosu . The default of `--pileup_image_height=100` was initially determined by the most common use case of Illumina WGS data. In terms of tweaking pileup_image_height, there are 2 past experiments that I can recall right now:. ## Illumina WES:. Before we released an Illumina WES model, I experimented with training a WES model with `--pileup_image_height=200`. At the time (end of 2017), I observed that the Indel F1 dropped quite a bit when increasing the height, but helped SNP F1 a bit. This investigationwas why we didn’t change this default for the Illumina WES model release. ## PacBio:. In Nov 2019, I trained a model with `--pileup_image_height=75` to see if we can decrease runtime without too much accuracy tradeoff. At the time, my results were:. * Reducing height to 75 (from 100) reduces calling time to ~85% (from 3h20m to 2h50m). * Accuracy (on case study chr20). - Indel F1: 0.983872 —> 0.982728. - SNP F1: 0.998913 —> 0.998867. Based on feedback, we decided it’s not a high priority to continue. Because:. 1. Amplicon will likely use the height=100. 2. 30min is not a big bottleneck now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/338
https://github.com/google/deepvariant/issues/338:327,security,model,model,327,"Hi @pgrosu . The default of `--pileup_image_height=100` was initially determined by the most common use case of Illumina WGS data. In terms of tweaking pileup_image_height, there are 2 past experiments that I can recall right now:. ## Illumina WES:. Before we released an Illumina WES model, I experimented with training a WES model with `--pileup_image_height=200`. At the time (end of 2017), I observed that the Indel F1 dropped quite a bit when increasing the height, but helped SNP F1 a bit. This investigationwas why we didn’t change this default for the Illumina WES model release. ## PacBio:. In Nov 2019, I trained a model with `--pileup_image_height=75` to see if we can decrease runtime without too much accuracy tradeoff. At the time, my results were:. * Reducing height to 75 (from 100) reduces calling time to ~85% (from 3h20m to 2h50m). * Accuracy (on case study chr20). - Indel F1: 0.983872 —> 0.982728. - SNP F1: 0.998913 —> 0.998867. Based on feedback, we decided it’s not a high priority to continue. Because:. 1. Amplicon will likely use the height=100. 2. 30min is not a big bottleneck now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/338
https://github.com/google/deepvariant/issues/338:573,security,model,model,573,"Hi @pgrosu . The default of `--pileup_image_height=100` was initially determined by the most common use case of Illumina WGS data. In terms of tweaking pileup_image_height, there are 2 past experiments that I can recall right now:. ## Illumina WES:. Before we released an Illumina WES model, I experimented with training a WES model with `--pileup_image_height=200`. At the time (end of 2017), I observed that the Indel F1 dropped quite a bit when increasing the height, but helped SNP F1 a bit. This investigationwas why we didn’t change this default for the Illumina WES model release. ## PacBio:. In Nov 2019, I trained a model with `--pileup_image_height=75` to see if we can decrease runtime without too much accuracy tradeoff. At the time, my results were:. * Reducing height to 75 (from 100) reduces calling time to ~85% (from 3h20m to 2h50m). * Accuracy (on case study chr20). - Indel F1: 0.983872 —> 0.982728. - SNP F1: 0.998913 —> 0.998867. Based on feedback, we decided it’s not a high priority to continue. Because:. 1. Amplicon will likely use the height=100. 2. 30min is not a big bottleneck now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/338
https://github.com/google/deepvariant/issues/338:625,security,model,model,625,"Hi @pgrosu . The default of `--pileup_image_height=100` was initially determined by the most common use case of Illumina WGS data. In terms of tweaking pileup_image_height, there are 2 past experiments that I can recall right now:. ## Illumina WES:. Before we released an Illumina WES model, I experimented with training a WES model with `--pileup_image_height=200`. At the time (end of 2017), I observed that the Indel F1 dropped quite a bit when increasing the height, but helped SNP F1 a bit. This investigationwas why we didn’t change this default for the Illumina WES model release. ## PacBio:. In Nov 2019, I trained a model with `--pileup_image_height=75` to see if we can decrease runtime without too much accuracy tradeoff. At the time, my results were:. * Reducing height to 75 (from 100) reduces calling time to ~85% (from 3h20m to 2h50m). * Accuracy (on case study chr20). - Indel F1: 0.983872 —> 0.982728. - SNP F1: 0.998913 —> 0.998867. Based on feedback, we decided it’s not a high priority to continue. Because:. 1. Amplicon will likely use the height=100. 2. 30min is not a big bottleneck now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/338
https://github.com/google/deepvariant/issues/338:396,testability,observ,observed,396,"Hi @pgrosu . The default of `--pileup_image_height=100` was initially determined by the most common use case of Illumina WGS data. In terms of tweaking pileup_image_height, there are 2 past experiments that I can recall right now:. ## Illumina WES:. Before we released an Illumina WES model, I experimented with training a WES model with `--pileup_image_height=200`. At the time (end of 2017), I observed that the Indel F1 dropped quite a bit when increasing the height, but helped SNP F1 a bit. This investigationwas why we didn’t change this default for the Illumina WES model release. ## PacBio:. In Nov 2019, I trained a model with `--pileup_image_height=75` to see if we can decrease runtime without too much accuracy tradeoff. At the time, my results were:. * Reducing height to 75 (from 100) reduces calling time to ~85% (from 3h20m to 2h50m). * Accuracy (on case study chr20). - Indel F1: 0.983872 —> 0.982728. - SNP F1: 0.998913 —> 0.998867. Based on feedback, we decided it’s not a high priority to continue. Because:. 1. Amplicon will likely use the height=100. 2. 30min is not a big bottleneck now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/338
https://github.com/google/deepvariant/issues/338:475,usability,help,helped,475,"Hi @pgrosu . The default of `--pileup_image_height=100` was initially determined by the most common use case of Illumina WGS data. In terms of tweaking pileup_image_height, there are 2 past experiments that I can recall right now:. ## Illumina WES:. Before we released an Illumina WES model, I experimented with training a WES model with `--pileup_image_height=200`. At the time (end of 2017), I observed that the Indel F1 dropped quite a bit when increasing the height, but helped SNP F1 a bit. This investigationwas why we didn’t change this default for the Illumina WES model release. ## PacBio:. In Nov 2019, I trained a model with `--pileup_image_height=75` to see if we can decrease runtime without too much accuracy tradeoff. At the time, my results were:. * Reducing height to 75 (from 100) reduces calling time to ~85% (from 3h20m to 2h50m). * Accuracy (on case study chr20). - Indel F1: 0.983872 —> 0.982728. - SNP F1: 0.998913 —> 0.998867. Based on feedback, we decided it’s not a high priority to continue. Because:. 1. Amplicon will likely use the height=100. 2. 30min is not a big bottleneck now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/338
https://github.com/google/deepvariant/issues/338:960,usability,feedback,feedback,960,"Hi @pgrosu . The default of `--pileup_image_height=100` was initially determined by the most common use case of Illumina WGS data. In terms of tweaking pileup_image_height, there are 2 past experiments that I can recall right now:. ## Illumina WES:. Before we released an Illumina WES model, I experimented with training a WES model with `--pileup_image_height=200`. At the time (end of 2017), I observed that the Indel F1 dropped quite a bit when increasing the height, but helped SNP F1 a bit. This investigationwas why we didn’t change this default for the Illumina WES model release. ## PacBio:. In Nov 2019, I trained a model with `--pileup_image_height=75` to see if we can decrease runtime without too much accuracy tradeoff. At the time, my results were:. * Reducing height to 75 (from 100) reduces calling time to ~85% (from 3h20m to 2h50m). * Accuracy (on case study chr20). - Indel F1: 0.983872 —> 0.982728. - SNP F1: 0.998913 —> 0.998867. Based on feedback, we decided it’s not a high priority to continue. Because:. 1. Amplicon will likely use the height=100. 2. 30min is not a big bottleneck now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/338
https://github.com/google/deepvariant/issues/339:89,energy efficiency,current,currently,89,"It looks like you are trying to analyze the graph outside of DeepVariant, which we don't currently have documentation on, since it isn't needed to run DeepVariant or even train it. But we do have some interpretability research we are working on, so stay tuned for that on the blog! In the meantime, can you give a bit more context on what you are trying to do? Where did you find that code snippet?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:254,performance,tune,tuned,254,"It looks like you are trying to analyze the graph outside of DeepVariant, which we don't currently have documentation on, since it isn't needed to run DeepVariant or even train it. But we do have some interpretability research we are working on, so stay tuned for that on the blog! In the meantime, can you give a bit more context on what you are trying to do? Where did you find that code snippet?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:323,testability,context,context,323,"It looks like you are trying to analyze the graph outside of DeepVariant, which we don't currently have documentation on, since it isn't needed to run DeepVariant or even train it. But we do have some interpretability research we are working on, so stay tuned for that on the blog! In the meantime, can you give a bit more context on what you are trying to do? Where did you find that code snippet?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:104,usability,document,documentation,104,"It looks like you are trying to analyze the graph outside of DeepVariant, which we don't currently have documentation on, since it isn't needed to run DeepVariant or even train it. But we do have some interpretability research we are working on, so stay tuned for that on the blog! In the meantime, can you give a bit more context on what you are trying to do? Where did you find that code snippet?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:83,availability,avail,available,83,"Hi Maria,. Thanks for the response. My Intent is to generate frozen graph from the available checkpoints. The code snippet is my own. Do you think it needs any fixing / addition? . Is there anyone who can provide steps/methods/guidance? I tried from inside as well as outside the Docker. I even tried Google Colab with different Hardware configurations -- i.e. combination of CPU, GPU, TPU. And also with different TF versions. I am never able to import the meta graph and restore the checkpoint. I always get ""No Op Kernel was registered"" with different Op names. Additional question: for training:. - Can I train on the ""quickstart-testdata"" provided in deepvariant? . - And how do I produce a frozen graph during a training run?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:93,availability,checkpoint,checkpoints,93,"Hi Maria,. Thanks for the response. My Intent is to generate frozen graph from the available checkpoints. The code snippet is my own. Do you think it needs any fixing / addition? . Is there anyone who can provide steps/methods/guidance? I tried from inside as well as outside the Docker. I even tried Google Colab with different Hardware configurations -- i.e. combination of CPU, GPU, TPU. And also with different TF versions. I am never able to import the meta graph and restore the checkpoint. I always get ""No Op Kernel was registered"" with different Op names. Additional question: for training:. - Can I train on the ""quickstart-testdata"" provided in deepvariant? . - And how do I produce a frozen graph during a training run?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:473,availability,restor,restore,473,"Hi Maria,. Thanks for the response. My Intent is to generate frozen graph from the available checkpoints. The code snippet is my own. Do you think it needs any fixing / addition? . Is there anyone who can provide steps/methods/guidance? I tried from inside as well as outside the Docker. I even tried Google Colab with different Hardware configurations -- i.e. combination of CPU, GPU, TPU. And also with different TF versions. I am never able to import the meta graph and restore the checkpoint. I always get ""No Op Kernel was registered"" with different Op names. Additional question: for training:. - Can I train on the ""quickstart-testdata"" provided in deepvariant? . - And how do I produce a frozen graph during a training run?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:485,availability,checkpoint,checkpoint,485,"Hi Maria,. Thanks for the response. My Intent is to generate frozen graph from the available checkpoints. The code snippet is my own. Do you think it needs any fixing / addition? . Is there anyone who can provide steps/methods/guidance? I tried from inside as well as outside the Docker. I even tried Google Colab with different Hardware configurations -- i.e. combination of CPU, GPU, TPU. And also with different TF versions. I am never able to import the meta graph and restore the checkpoint. I always get ""No Op Kernel was registered"" with different Op names. Additional question: for training:. - Can I train on the ""quickstart-testdata"" provided in deepvariant? . - And how do I produce a frozen graph during a training run?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:338,deployability,configurat,configurations,338,"Hi Maria,. Thanks for the response. My Intent is to generate frozen graph from the available checkpoints. The code snippet is my own. Do you think it needs any fixing / addition? . Is there anyone who can provide steps/methods/guidance? I tried from inside as well as outside the Docker. I even tried Google Colab with different Hardware configurations -- i.e. combination of CPU, GPU, TPU. And also with different TF versions. I am never able to import the meta graph and restore the checkpoint. I always get ""No Op Kernel was registered"" with different Op names. Additional question: for training:. - Can I train on the ""quickstart-testdata"" provided in deepvariant? . - And how do I produce a frozen graph during a training run?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:418,deployability,version,versions,418,"Hi Maria,. Thanks for the response. My Intent is to generate frozen graph from the available checkpoints. The code snippet is my own. Do you think it needs any fixing / addition? . Is there anyone who can provide steps/methods/guidance? I tried from inside as well as outside the Docker. I even tried Google Colab with different Hardware configurations -- i.e. combination of CPU, GPU, TPU. And also with different TF versions. I am never able to import the meta graph and restore the checkpoint. I always get ""No Op Kernel was registered"" with different Op names. Additional question: for training:. - Can I train on the ""quickstart-testdata"" provided in deepvariant? . - And how do I produce a frozen graph during a training run?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:376,energy efficiency,CPU,CPU,376,"Hi Maria,. Thanks for the response. My Intent is to generate frozen graph from the available checkpoints. The code snippet is my own. Do you think it needs any fixing / addition? . Is there anyone who can provide steps/methods/guidance? I tried from inside as well as outside the Docker. I even tried Google Colab with different Hardware configurations -- i.e. combination of CPU, GPU, TPU. And also with different TF versions. I am never able to import the meta graph and restore the checkpoint. I always get ""No Op Kernel was registered"" with different Op names. Additional question: for training:. - Can I train on the ""quickstart-testdata"" provided in deepvariant? . - And how do I produce a frozen graph during a training run?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:381,energy efficiency,GPU,GPU,381,"Hi Maria,. Thanks for the response. My Intent is to generate frozen graph from the available checkpoints. The code snippet is my own. Do you think it needs any fixing / addition? . Is there anyone who can provide steps/methods/guidance? I tried from inside as well as outside the Docker. I even tried Google Colab with different Hardware configurations -- i.e. combination of CPU, GPU, TPU. And also with different TF versions. I am never able to import the meta graph and restore the checkpoint. I always get ""No Op Kernel was registered"" with different Op names. Additional question: for training:. - Can I train on the ""quickstart-testdata"" provided in deepvariant? . - And how do I produce a frozen graph during a training run?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:338,integrability,configur,configurations,338,"Hi Maria,. Thanks for the response. My Intent is to generate frozen graph from the available checkpoints. The code snippet is my own. Do you think it needs any fixing / addition? . Is there anyone who can provide steps/methods/guidance? I tried from inside as well as outside the Docker. I even tried Google Colab with different Hardware configurations -- i.e. combination of CPU, GPU, TPU. And also with different TF versions. I am never able to import the meta graph and restore the checkpoint. I always get ""No Op Kernel was registered"" with different Op names. Additional question: for training:. - Can I train on the ""quickstart-testdata"" provided in deepvariant? . - And how do I produce a frozen graph during a training run?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:418,integrability,version,versions,418,"Hi Maria,. Thanks for the response. My Intent is to generate frozen graph from the available checkpoints. The code snippet is my own. Do you think it needs any fixing / addition? . Is there anyone who can provide steps/methods/guidance? I tried from inside as well as outside the Docker. I even tried Google Colab with different Hardware configurations -- i.e. combination of CPU, GPU, TPU. And also with different TF versions. I am never able to import the meta graph and restore the checkpoint. I always get ""No Op Kernel was registered"" with different Op names. Additional question: for training:. - Can I train on the ""quickstart-testdata"" provided in deepvariant? . - And how do I produce a frozen graph during a training run?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:338,modifiability,configur,configurations,338,"Hi Maria,. Thanks for the response. My Intent is to generate frozen graph from the available checkpoints. The code snippet is my own. Do you think it needs any fixing / addition? . Is there anyone who can provide steps/methods/guidance? I tried from inside as well as outside the Docker. I even tried Google Colab with different Hardware configurations -- i.e. combination of CPU, GPU, TPU. And also with different TF versions. I am never able to import the meta graph and restore the checkpoint. I always get ""No Op Kernel was registered"" with different Op names. Additional question: for training:. - Can I train on the ""quickstart-testdata"" provided in deepvariant? . - And how do I produce a frozen graph during a training run?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:418,modifiability,version,versions,418,"Hi Maria,. Thanks for the response. My Intent is to generate frozen graph from the available checkpoints. The code snippet is my own. Do you think it needs any fixing / addition? . Is there anyone who can provide steps/methods/guidance? I tried from inside as well as outside the Docker. I even tried Google Colab with different Hardware configurations -- i.e. combination of CPU, GPU, TPU. And also with different TF versions. I am never able to import the meta graph and restore the checkpoint. I always get ""No Op Kernel was registered"" with different Op names. Additional question: for training:. - Can I train on the ""quickstart-testdata"" provided in deepvariant? . - And how do I produce a frozen graph during a training run?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:376,performance,CPU,CPU,376,"Hi Maria,. Thanks for the response. My Intent is to generate frozen graph from the available checkpoints. The code snippet is my own. Do you think it needs any fixing / addition? . Is there anyone who can provide steps/methods/guidance? I tried from inside as well as outside the Docker. I even tried Google Colab with different Hardware configurations -- i.e. combination of CPU, GPU, TPU. And also with different TF versions. I am never able to import the meta graph and restore the checkpoint. I always get ""No Op Kernel was registered"" with different Op names. Additional question: for training:. - Can I train on the ""quickstart-testdata"" provided in deepvariant? . - And how do I produce a frozen graph during a training run?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:381,performance,GPU,GPU,381,"Hi Maria,. Thanks for the response. My Intent is to generate frozen graph from the available checkpoints. The code snippet is my own. Do you think it needs any fixing / addition? . Is there anyone who can provide steps/methods/guidance? I tried from inside as well as outside the Docker. I even tried Google Colab with different Hardware configurations -- i.e. combination of CPU, GPU, TPU. And also with different TF versions. I am never able to import the meta graph and restore the checkpoint. I always get ""No Op Kernel was registered"" with different Op names. Additional question: for training:. - Can I train on the ""quickstart-testdata"" provided in deepvariant? . - And how do I produce a frozen graph during a training run?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:83,reliability,availab,available,83,"Hi Maria,. Thanks for the response. My Intent is to generate frozen graph from the available checkpoints. The code snippet is my own. Do you think it needs any fixing / addition? . Is there anyone who can provide steps/methods/guidance? I tried from inside as well as outside the Docker. I even tried Google Colab with different Hardware configurations -- i.e. combination of CPU, GPU, TPU. And also with different TF versions. I am never able to import the meta graph and restore the checkpoint. I always get ""No Op Kernel was registered"" with different Op names. Additional question: for training:. - Can I train on the ""quickstart-testdata"" provided in deepvariant? . - And how do I produce a frozen graph during a training run?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:93,reliability,checkpoint,checkpoints,93,"Hi Maria,. Thanks for the response. My Intent is to generate frozen graph from the available checkpoints. The code snippet is my own. Do you think it needs any fixing / addition? . Is there anyone who can provide steps/methods/guidance? I tried from inside as well as outside the Docker. I even tried Google Colab with different Hardware configurations -- i.e. combination of CPU, GPU, TPU. And also with different TF versions. I am never able to import the meta graph and restore the checkpoint. I always get ""No Op Kernel was registered"" with different Op names. Additional question: for training:. - Can I train on the ""quickstart-testdata"" provided in deepvariant? . - And how do I produce a frozen graph during a training run?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:473,reliability,restor,restore,473,"Hi Maria,. Thanks for the response. My Intent is to generate frozen graph from the available checkpoints. The code snippet is my own. Do you think it needs any fixing / addition? . Is there anyone who can provide steps/methods/guidance? I tried from inside as well as outside the Docker. I even tried Google Colab with different Hardware configurations -- i.e. combination of CPU, GPU, TPU. And also with different TF versions. I am never able to import the meta graph and restore the checkpoint. I always get ""No Op Kernel was registered"" with different Op names. Additional question: for training:. - Can I train on the ""quickstart-testdata"" provided in deepvariant? . - And how do I produce a frozen graph during a training run?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:485,reliability,checkpoint,checkpoint,485,"Hi Maria,. Thanks for the response. My Intent is to generate frozen graph from the available checkpoints. The code snippet is my own. Do you think it needs any fixing / addition? . Is there anyone who can provide steps/methods/guidance? I tried from inside as well as outside the Docker. I even tried Google Colab with different Hardware configurations -- i.e. combination of CPU, GPU, TPU. And also with different TF versions. I am never able to import the meta graph and restore the checkpoint. I always get ""No Op Kernel was registered"" with different Op names. Additional question: for training:. - Can I train on the ""quickstart-testdata"" provided in deepvariant? . - And how do I produce a frozen graph during a training run?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:83,safety,avail,available,83,"Hi Maria,. Thanks for the response. My Intent is to generate frozen graph from the available checkpoints. The code snippet is my own. Do you think it needs any fixing / addition? . Is there anyone who can provide steps/methods/guidance? I tried from inside as well as outside the Docker. I even tried Google Colab with different Hardware configurations -- i.e. combination of CPU, GPU, TPU. And also with different TF versions. I am never able to import the meta graph and restore the checkpoint. I always get ""No Op Kernel was registered"" with different Op names. Additional question: for training:. - Can I train on the ""quickstart-testdata"" provided in deepvariant? . - And how do I produce a frozen graph during a training run?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:634,safety,test,testdata,634,"Hi Maria,. Thanks for the response. My Intent is to generate frozen graph from the available checkpoints. The code snippet is my own. Do you think it needs any fixing / addition? . Is there anyone who can provide steps/methods/guidance? I tried from inside as well as outside the Docker. I even tried Google Colab with different Hardware configurations -- i.e. combination of CPU, GPU, TPU. And also with different TF versions. I am never able to import the meta graph and restore the checkpoint. I always get ""No Op Kernel was registered"" with different Op names. Additional question: for training:. - Can I train on the ""quickstart-testdata"" provided in deepvariant? . - And how do I produce a frozen graph during a training run?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:83,security,availab,available,83,"Hi Maria,. Thanks for the response. My Intent is to generate frozen graph from the available checkpoints. The code snippet is my own. Do you think it needs any fixing / addition? . Is there anyone who can provide steps/methods/guidance? I tried from inside as well as outside the Docker. I even tried Google Colab with different Hardware configurations -- i.e. combination of CPU, GPU, TPU. And also with different TF versions. I am never able to import the meta graph and restore the checkpoint. I always get ""No Op Kernel was registered"" with different Op names. Additional question: for training:. - Can I train on the ""quickstart-testdata"" provided in deepvariant? . - And how do I produce a frozen graph during a training run?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:338,security,configur,configurations,338,"Hi Maria,. Thanks for the response. My Intent is to generate frozen graph from the available checkpoints. The code snippet is my own. Do you think it needs any fixing / addition? . Is there anyone who can provide steps/methods/guidance? I tried from inside as well as outside the Docker. I even tried Google Colab with different Hardware configurations -- i.e. combination of CPU, GPU, TPU. And also with different TF versions. I am never able to import the meta graph and restore the checkpoint. I always get ""No Op Kernel was registered"" with different Op names. Additional question: for training:. - Can I train on the ""quickstart-testdata"" provided in deepvariant? . - And how do I produce a frozen graph during a training run?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:634,testability,test,testdata,634,"Hi Maria,. Thanks for the response. My Intent is to generate frozen graph from the available checkpoints. The code snippet is my own. Do you think it needs any fixing / addition? . Is there anyone who can provide steps/methods/guidance? I tried from inside as well as outside the Docker. I even tried Google Colab with different Hardware configurations -- i.e. combination of CPU, GPU, TPU. And also with different TF versions. I am never able to import the meta graph and restore the checkpoint. I always get ""No Op Kernel was registered"" with different Op names. Additional question: for training:. - Can I train on the ""quickstart-testdata"" provided in deepvariant? . - And how do I produce a frozen graph during a training run?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:227,usability,guidanc,guidance,227,"Hi Maria,. Thanks for the response. My Intent is to generate frozen graph from the available checkpoints. The code snippet is my own. Do you think it needs any fixing / addition? . Is there anyone who can provide steps/methods/guidance? I tried from inside as well as outside the Docker. I even tried Google Colab with different Hardware configurations -- i.e. combination of CPU, GPU, TPU. And also with different TF versions. I am never able to import the meta graph and restore the checkpoint. I always get ""No Op Kernel was registered"" with different Op names. Additional question: for training:. - Can I train on the ""quickstart-testdata"" provided in deepvariant? . - And how do I produce a frozen graph during a training run?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:33,availability,checkpoint,checkpoints,33,"Hi @jaydevshelat . DeepVariant's checkpoints are based on InceptionV3 and use slim, see https://github.com/google-research/tf-slim. I can share with you some code that @pichuan has written to read the checkpoints, but if you have any further questions on that, please consult the documentation for tensorflow or tf-slim. ```. ! pip install tf-slim. import tensorflow.compat.v1 as tf. import os. import tf_slim as slim. from tf_slim.nets import inception_v3. !gsutil cp gs://deepvariant/models/DeepVariant/0.8.0/DeepVariant-inception_v3-0.8.0+data-wgs_standard/model* /tmp/. ckpt_file = '/tmp/model.ckpt'. graph = tf.Graph(). with graph.as_default():. images = tf.placeholder(tf.float32, shape=(None, 100, 221, 6)). with slim.arg_scope(inception_v3.inception_v3_arg_scope()):. _, end_points = inception_v3.inception_v3(images, is_training=False, . num_classes=3,. create_aux_logits=False). . print(""end_points:""). print(end_points.keys()). # Restore the checkpoint. sess = tf.Session(graph=graph). saver = tf.train.Saver(). saver.restore(sess, ckpt_file). ```. For training, you would need a lot more training data than the quickstart-testdata, as in multiple samples of WGS worth of sequencing data. For more information on training, see the documentation: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:78,availability,sli,slim,78,"Hi @jaydevshelat . DeepVariant's checkpoints are based on InceptionV3 and use slim, see https://github.com/google-research/tf-slim. I can share with you some code that @pichuan has written to read the checkpoints, but if you have any further questions on that, please consult the documentation for tensorflow or tf-slim. ```. ! pip install tf-slim. import tensorflow.compat.v1 as tf. import os. import tf_slim as slim. from tf_slim.nets import inception_v3. !gsutil cp gs://deepvariant/models/DeepVariant/0.8.0/DeepVariant-inception_v3-0.8.0+data-wgs_standard/model* /tmp/. ckpt_file = '/tmp/model.ckpt'. graph = tf.Graph(). with graph.as_default():. images = tf.placeholder(tf.float32, shape=(None, 100, 221, 6)). with slim.arg_scope(inception_v3.inception_v3_arg_scope()):. _, end_points = inception_v3.inception_v3(images, is_training=False, . num_classes=3,. create_aux_logits=False). . print(""end_points:""). print(end_points.keys()). # Restore the checkpoint. sess = tf.Session(graph=graph). saver = tf.train.Saver(). saver.restore(sess, ckpt_file). ```. For training, you would need a lot more training data than the quickstart-testdata, as in multiple samples of WGS worth of sequencing data. For more information on training, see the documentation: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:126,availability,sli,slim,126,"Hi @jaydevshelat . DeepVariant's checkpoints are based on InceptionV3 and use slim, see https://github.com/google-research/tf-slim. I can share with you some code that @pichuan has written to read the checkpoints, but if you have any further questions on that, please consult the documentation for tensorflow or tf-slim. ```. ! pip install tf-slim. import tensorflow.compat.v1 as tf. import os. import tf_slim as slim. from tf_slim.nets import inception_v3. !gsutil cp gs://deepvariant/models/DeepVariant/0.8.0/DeepVariant-inception_v3-0.8.0+data-wgs_standard/model* /tmp/. ckpt_file = '/tmp/model.ckpt'. graph = tf.Graph(). with graph.as_default():. images = tf.placeholder(tf.float32, shape=(None, 100, 221, 6)). with slim.arg_scope(inception_v3.inception_v3_arg_scope()):. _, end_points = inception_v3.inception_v3(images, is_training=False, . num_classes=3,. create_aux_logits=False). . print(""end_points:""). print(end_points.keys()). # Restore the checkpoint. sess = tf.Session(graph=graph). saver = tf.train.Saver(). saver.restore(sess, ckpt_file). ```. For training, you would need a lot more training data than the quickstart-testdata, as in multiple samples of WGS worth of sequencing data. For more information on training, see the documentation: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:201,availability,checkpoint,checkpoints,201,"Hi @jaydevshelat . DeepVariant's checkpoints are based on InceptionV3 and use slim, see https://github.com/google-research/tf-slim. I can share with you some code that @pichuan has written to read the checkpoints, but if you have any further questions on that, please consult the documentation for tensorflow or tf-slim. ```. ! pip install tf-slim. import tensorflow.compat.v1 as tf. import os. import tf_slim as slim. from tf_slim.nets import inception_v3. !gsutil cp gs://deepvariant/models/DeepVariant/0.8.0/DeepVariant-inception_v3-0.8.0+data-wgs_standard/model* /tmp/. ckpt_file = '/tmp/model.ckpt'. graph = tf.Graph(). with graph.as_default():. images = tf.placeholder(tf.float32, shape=(None, 100, 221, 6)). with slim.arg_scope(inception_v3.inception_v3_arg_scope()):. _, end_points = inception_v3.inception_v3(images, is_training=False, . num_classes=3,. create_aux_logits=False). . print(""end_points:""). print(end_points.keys()). # Restore the checkpoint. sess = tf.Session(graph=graph). saver = tf.train.Saver(). saver.restore(sess, ckpt_file). ```. For training, you would need a lot more training data than the quickstart-testdata, as in multiple samples of WGS worth of sequencing data. For more information on training, see the documentation: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:315,availability,sli,slim,315,"Hi @jaydevshelat . DeepVariant's checkpoints are based on InceptionV3 and use slim, see https://github.com/google-research/tf-slim. I can share with you some code that @pichuan has written to read the checkpoints, but if you have any further questions on that, please consult the documentation for tensorflow or tf-slim. ```. ! pip install tf-slim. import tensorflow.compat.v1 as tf. import os. import tf_slim as slim. from tf_slim.nets import inception_v3. !gsutil cp gs://deepvariant/models/DeepVariant/0.8.0/DeepVariant-inception_v3-0.8.0+data-wgs_standard/model* /tmp/. ckpt_file = '/tmp/model.ckpt'. graph = tf.Graph(). with graph.as_default():. images = tf.placeholder(tf.float32, shape=(None, 100, 221, 6)). with slim.arg_scope(inception_v3.inception_v3_arg_scope()):. _, end_points = inception_v3.inception_v3(images, is_training=False, . num_classes=3,. create_aux_logits=False). . print(""end_points:""). print(end_points.keys()). # Restore the checkpoint. sess = tf.Session(graph=graph). saver = tf.train.Saver(). saver.restore(sess, ckpt_file). ```. For training, you would need a lot more training data than the quickstart-testdata, as in multiple samples of WGS worth of sequencing data. For more information on training, see the documentation: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:343,availability,sli,slim,343,"Hi @jaydevshelat . DeepVariant's checkpoints are based on InceptionV3 and use slim, see https://github.com/google-research/tf-slim. I can share with you some code that @pichuan has written to read the checkpoints, but if you have any further questions on that, please consult the documentation for tensorflow or tf-slim. ```. ! pip install tf-slim. import tensorflow.compat.v1 as tf. import os. import tf_slim as slim. from tf_slim.nets import inception_v3. !gsutil cp gs://deepvariant/models/DeepVariant/0.8.0/DeepVariant-inception_v3-0.8.0+data-wgs_standard/model* /tmp/. ckpt_file = '/tmp/model.ckpt'. graph = tf.Graph(). with graph.as_default():. images = tf.placeholder(tf.float32, shape=(None, 100, 221, 6)). with slim.arg_scope(inception_v3.inception_v3_arg_scope()):. _, end_points = inception_v3.inception_v3(images, is_training=False, . num_classes=3,. create_aux_logits=False). . print(""end_points:""). print(end_points.keys()). # Restore the checkpoint. sess = tf.Session(graph=graph). saver = tf.train.Saver(). saver.restore(sess, ckpt_file). ```. For training, you would need a lot more training data than the quickstart-testdata, as in multiple samples of WGS worth of sequencing data. For more information on training, see the documentation: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:413,availability,sli,slim,413,"Hi @jaydevshelat . DeepVariant's checkpoints are based on InceptionV3 and use slim, see https://github.com/google-research/tf-slim. I can share with you some code that @pichuan has written to read the checkpoints, but if you have any further questions on that, please consult the documentation for tensorflow or tf-slim. ```. ! pip install tf-slim. import tensorflow.compat.v1 as tf. import os. import tf_slim as slim. from tf_slim.nets import inception_v3. !gsutil cp gs://deepvariant/models/DeepVariant/0.8.0/DeepVariant-inception_v3-0.8.0+data-wgs_standard/model* /tmp/. ckpt_file = '/tmp/model.ckpt'. graph = tf.Graph(). with graph.as_default():. images = tf.placeholder(tf.float32, shape=(None, 100, 221, 6)). with slim.arg_scope(inception_v3.inception_v3_arg_scope()):. _, end_points = inception_v3.inception_v3(images, is_training=False, . num_classes=3,. create_aux_logits=False). . print(""end_points:""). print(end_points.keys()). # Restore the checkpoint. sess = tf.Session(graph=graph). saver = tf.train.Saver(). saver.restore(sess, ckpt_file). ```. For training, you would need a lot more training data than the quickstart-testdata, as in multiple samples of WGS worth of sequencing data. For more information on training, see the documentation: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:720,availability,sli,slim,720,"Hi @jaydevshelat . DeepVariant's checkpoints are based on InceptionV3 and use slim, see https://github.com/google-research/tf-slim. I can share with you some code that @pichuan has written to read the checkpoints, but if you have any further questions on that, please consult the documentation for tensorflow or tf-slim. ```. ! pip install tf-slim. import tensorflow.compat.v1 as tf. import os. import tf_slim as slim. from tf_slim.nets import inception_v3. !gsutil cp gs://deepvariant/models/DeepVariant/0.8.0/DeepVariant-inception_v3-0.8.0+data-wgs_standard/model* /tmp/. ckpt_file = '/tmp/model.ckpt'. graph = tf.Graph(). with graph.as_default():. images = tf.placeholder(tf.float32, shape=(None, 100, 221, 6)). with slim.arg_scope(inception_v3.inception_v3_arg_scope()):. _, end_points = inception_v3.inception_v3(images, is_training=False, . num_classes=3,. create_aux_logits=False). . print(""end_points:""). print(end_points.keys()). # Restore the checkpoint. sess = tf.Session(graph=graph). saver = tf.train.Saver(). saver.restore(sess, ckpt_file). ```. For training, you would need a lot more training data than the quickstart-testdata, as in multiple samples of WGS worth of sequencing data. For more information on training, see the documentation: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:941,availability,Restor,Restore,941,"Hi @jaydevshelat . DeepVariant's checkpoints are based on InceptionV3 and use slim, see https://github.com/google-research/tf-slim. I can share with you some code that @pichuan has written to read the checkpoints, but if you have any further questions on that, please consult the documentation for tensorflow or tf-slim. ```. ! pip install tf-slim. import tensorflow.compat.v1 as tf. import os. import tf_slim as slim. from tf_slim.nets import inception_v3. !gsutil cp gs://deepvariant/models/DeepVariant/0.8.0/DeepVariant-inception_v3-0.8.0+data-wgs_standard/model* /tmp/. ckpt_file = '/tmp/model.ckpt'. graph = tf.Graph(). with graph.as_default():. images = tf.placeholder(tf.float32, shape=(None, 100, 221, 6)). with slim.arg_scope(inception_v3.inception_v3_arg_scope()):. _, end_points = inception_v3.inception_v3(images, is_training=False, . num_classes=3,. create_aux_logits=False). . print(""end_points:""). print(end_points.keys()). # Restore the checkpoint. sess = tf.Session(graph=graph). saver = tf.train.Saver(). saver.restore(sess, ckpt_file). ```. For training, you would need a lot more training data than the quickstart-testdata, as in multiple samples of WGS worth of sequencing data. For more information on training, see the documentation: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:953,availability,checkpoint,checkpoint,953,"Hi @jaydevshelat . DeepVariant's checkpoints are based on InceptionV3 and use slim, see https://github.com/google-research/tf-slim. I can share with you some code that @pichuan has written to read the checkpoints, but if you have any further questions on that, please consult the documentation for tensorflow or tf-slim. ```. ! pip install tf-slim. import tensorflow.compat.v1 as tf. import os. import tf_slim as slim. from tf_slim.nets import inception_v3. !gsutil cp gs://deepvariant/models/DeepVariant/0.8.0/DeepVariant-inception_v3-0.8.0+data-wgs_standard/model* /tmp/. ckpt_file = '/tmp/model.ckpt'. graph = tf.Graph(). with graph.as_default():. images = tf.placeholder(tf.float32, shape=(None, 100, 221, 6)). with slim.arg_scope(inception_v3.inception_v3_arg_scope()):. _, end_points = inception_v3.inception_v3(images, is_training=False, . num_classes=3,. create_aux_logits=False). . print(""end_points:""). print(end_points.keys()). # Restore the checkpoint. sess = tf.Session(graph=graph). saver = tf.train.Saver(). saver.restore(sess, ckpt_file). ```. For training, you would need a lot more training data than the quickstart-testdata, as in multiple samples of WGS worth of sequencing data. For more information on training, see the documentation: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:1029,availability,restor,restore,1029,"Hi @jaydevshelat . DeepVariant's checkpoints are based on InceptionV3 and use slim, see https://github.com/google-research/tf-slim. I can share with you some code that @pichuan has written to read the checkpoints, but if you have any further questions on that, please consult the documentation for tensorflow or tf-slim. ```. ! pip install tf-slim. import tensorflow.compat.v1 as tf. import os. import tf_slim as slim. from tf_slim.nets import inception_v3. !gsutil cp gs://deepvariant/models/DeepVariant/0.8.0/DeepVariant-inception_v3-0.8.0+data-wgs_standard/model* /tmp/. ckpt_file = '/tmp/model.ckpt'. graph = tf.Graph(). with graph.as_default():. images = tf.placeholder(tf.float32, shape=(None, 100, 221, 6)). with slim.arg_scope(inception_v3.inception_v3_arg_scope()):. _, end_points = inception_v3.inception_v3(images, is_training=False, . num_classes=3,. create_aux_logits=False). . print(""end_points:""). print(end_points.keys()). # Restore the checkpoint. sess = tf.Session(graph=graph). saver = tf.train.Saver(). saver.restore(sess, ckpt_file). ```. For training, you would need a lot more training data than the quickstart-testdata, as in multiple samples of WGS worth of sequencing data. For more information on training, see the documentation: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:332,deployability,instal,install,332,"Hi @jaydevshelat . DeepVariant's checkpoints are based on InceptionV3 and use slim, see https://github.com/google-research/tf-slim. I can share with you some code that @pichuan has written to read the checkpoints, but if you have any further questions on that, please consult the documentation for tensorflow or tf-slim. ```. ! pip install tf-slim. import tensorflow.compat.v1 as tf. import os. import tf_slim as slim. from tf_slim.nets import inception_v3. !gsutil cp gs://deepvariant/models/DeepVariant/0.8.0/DeepVariant-inception_v3-0.8.0+data-wgs_standard/model* /tmp/. ckpt_file = '/tmp/model.ckpt'. graph = tf.Graph(). with graph.as_default():. images = tf.placeholder(tf.float32, shape=(None, 100, 221, 6)). with slim.arg_scope(inception_v3.inception_v3_arg_scope()):. _, end_points = inception_v3.inception_v3(images, is_training=False, . num_classes=3,. create_aux_logits=False). . print(""end_points:""). print(end_points.keys()). # Restore the checkpoint. sess = tf.Session(graph=graph). saver = tf.train.Saver(). saver.restore(sess, ckpt_file). ```. For training, you would need a lot more training data than the quickstart-testdata, as in multiple samples of WGS worth of sequencing data. For more information on training, see the documentation: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:486,energy efficiency,model,models,486,"Hi @jaydevshelat . DeepVariant's checkpoints are based on InceptionV3 and use slim, see https://github.com/google-research/tf-slim. I can share with you some code that @pichuan has written to read the checkpoints, but if you have any further questions on that, please consult the documentation for tensorflow or tf-slim. ```. ! pip install tf-slim. import tensorflow.compat.v1 as tf. import os. import tf_slim as slim. from tf_slim.nets import inception_v3. !gsutil cp gs://deepvariant/models/DeepVariant/0.8.0/DeepVariant-inception_v3-0.8.0+data-wgs_standard/model* /tmp/. ckpt_file = '/tmp/model.ckpt'. graph = tf.Graph(). with graph.as_default():. images = tf.placeholder(tf.float32, shape=(None, 100, 221, 6)). with slim.arg_scope(inception_v3.inception_v3_arg_scope()):. _, end_points = inception_v3.inception_v3(images, is_training=False, . num_classes=3,. create_aux_logits=False). . print(""end_points:""). print(end_points.keys()). # Restore the checkpoint. sess = tf.Session(graph=graph). saver = tf.train.Saver(). saver.restore(sess, ckpt_file). ```. For training, you would need a lot more training data than the quickstart-testdata, as in multiple samples of WGS worth of sequencing data. For more information on training, see the documentation: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:560,energy efficiency,model,model,560,"Hi @jaydevshelat . DeepVariant's checkpoints are based on InceptionV3 and use slim, see https://github.com/google-research/tf-slim. I can share with you some code that @pichuan has written to read the checkpoints, but if you have any further questions on that, please consult the documentation for tensorflow or tf-slim. ```. ! pip install tf-slim. import tensorflow.compat.v1 as tf. import os. import tf_slim as slim. from tf_slim.nets import inception_v3. !gsutil cp gs://deepvariant/models/DeepVariant/0.8.0/DeepVariant-inception_v3-0.8.0+data-wgs_standard/model* /tmp/. ckpt_file = '/tmp/model.ckpt'. graph = tf.Graph(). with graph.as_default():. images = tf.placeholder(tf.float32, shape=(None, 100, 221, 6)). with slim.arg_scope(inception_v3.inception_v3_arg_scope()):. _, end_points = inception_v3.inception_v3(images, is_training=False, . num_classes=3,. create_aux_logits=False). . print(""end_points:""). print(end_points.keys()). # Restore the checkpoint. sess = tf.Session(graph=graph). saver = tf.train.Saver(). saver.restore(sess, ckpt_file). ```. For training, you would need a lot more training data than the quickstart-testdata, as in multiple samples of WGS worth of sequencing data. For more information on training, see the documentation: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:592,energy efficiency,model,model,592,"Hi @jaydevshelat . DeepVariant's checkpoints are based on InceptionV3 and use slim, see https://github.com/google-research/tf-slim. I can share with you some code that @pichuan has written to read the checkpoints, but if you have any further questions on that, please consult the documentation for tensorflow or tf-slim. ```. ! pip install tf-slim. import tensorflow.compat.v1 as tf. import os. import tf_slim as slim. from tf_slim.nets import inception_v3. !gsutil cp gs://deepvariant/models/DeepVariant/0.8.0/DeepVariant-inception_v3-0.8.0+data-wgs_standard/model* /tmp/. ckpt_file = '/tmp/model.ckpt'. graph = tf.Graph(). with graph.as_default():. images = tf.placeholder(tf.float32, shape=(None, 100, 221, 6)). with slim.arg_scope(inception_v3.inception_v3_arg_scope()):. _, end_points = inception_v3.inception_v3(images, is_training=False, . num_classes=3,. create_aux_logits=False). . print(""end_points:""). print(end_points.keys()). # Restore the checkpoint. sess = tf.Session(graph=graph). saver = tf.train.Saver(). saver.restore(sess, ckpt_file). ```. For training, you would need a lot more training data than the quickstart-testdata, as in multiple samples of WGS worth of sequencing data. For more information on training, see the documentation: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:138,interoperability,share,share,138,"Hi @jaydevshelat . DeepVariant's checkpoints are based on InceptionV3 and use slim, see https://github.com/google-research/tf-slim. I can share with you some code that @pichuan has written to read the checkpoints, but if you have any further questions on that, please consult the documentation for tensorflow or tf-slim. ```. ! pip install tf-slim. import tensorflow.compat.v1 as tf. import os. import tf_slim as slim. from tf_slim.nets import inception_v3. !gsutil cp gs://deepvariant/models/DeepVariant/0.8.0/DeepVariant-inception_v3-0.8.0+data-wgs_standard/model* /tmp/. ckpt_file = '/tmp/model.ckpt'. graph = tf.Graph(). with graph.as_default():. images = tf.placeholder(tf.float32, shape=(None, 100, 221, 6)). with slim.arg_scope(inception_v3.inception_v3_arg_scope()):. _, end_points = inception_v3.inception_v3(images, is_training=False, . num_classes=3,. create_aux_logits=False). . print(""end_points:""). print(end_points.keys()). # Restore the checkpoint. sess = tf.Session(graph=graph). saver = tf.train.Saver(). saver.restore(sess, ckpt_file). ```. For training, you would need a lot more training data than the quickstart-testdata, as in multiple samples of WGS worth of sequencing data. For more information on training, see the documentation: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:33,reliability,checkpoint,checkpoints,33,"Hi @jaydevshelat . DeepVariant's checkpoints are based on InceptionV3 and use slim, see https://github.com/google-research/tf-slim. I can share with you some code that @pichuan has written to read the checkpoints, but if you have any further questions on that, please consult the documentation for tensorflow or tf-slim. ```. ! pip install tf-slim. import tensorflow.compat.v1 as tf. import os. import tf_slim as slim. from tf_slim.nets import inception_v3. !gsutil cp gs://deepvariant/models/DeepVariant/0.8.0/DeepVariant-inception_v3-0.8.0+data-wgs_standard/model* /tmp/. ckpt_file = '/tmp/model.ckpt'. graph = tf.Graph(). with graph.as_default():. images = tf.placeholder(tf.float32, shape=(None, 100, 221, 6)). with slim.arg_scope(inception_v3.inception_v3_arg_scope()):. _, end_points = inception_v3.inception_v3(images, is_training=False, . num_classes=3,. create_aux_logits=False). . print(""end_points:""). print(end_points.keys()). # Restore the checkpoint. sess = tf.Session(graph=graph). saver = tf.train.Saver(). saver.restore(sess, ckpt_file). ```. For training, you would need a lot more training data than the quickstart-testdata, as in multiple samples of WGS worth of sequencing data. For more information on training, see the documentation: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:78,reliability,sli,slim,78,"Hi @jaydevshelat . DeepVariant's checkpoints are based on InceptionV3 and use slim, see https://github.com/google-research/tf-slim. I can share with you some code that @pichuan has written to read the checkpoints, but if you have any further questions on that, please consult the documentation for tensorflow or tf-slim. ```. ! pip install tf-slim. import tensorflow.compat.v1 as tf. import os. import tf_slim as slim. from tf_slim.nets import inception_v3. !gsutil cp gs://deepvariant/models/DeepVariant/0.8.0/DeepVariant-inception_v3-0.8.0+data-wgs_standard/model* /tmp/. ckpt_file = '/tmp/model.ckpt'. graph = tf.Graph(). with graph.as_default():. images = tf.placeholder(tf.float32, shape=(None, 100, 221, 6)). with slim.arg_scope(inception_v3.inception_v3_arg_scope()):. _, end_points = inception_v3.inception_v3(images, is_training=False, . num_classes=3,. create_aux_logits=False). . print(""end_points:""). print(end_points.keys()). # Restore the checkpoint. sess = tf.Session(graph=graph). saver = tf.train.Saver(). saver.restore(sess, ckpt_file). ```. For training, you would need a lot more training data than the quickstart-testdata, as in multiple samples of WGS worth of sequencing data. For more information on training, see the documentation: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:126,reliability,sli,slim,126,"Hi @jaydevshelat . DeepVariant's checkpoints are based on InceptionV3 and use slim, see https://github.com/google-research/tf-slim. I can share with you some code that @pichuan has written to read the checkpoints, but if you have any further questions on that, please consult the documentation for tensorflow or tf-slim. ```. ! pip install tf-slim. import tensorflow.compat.v1 as tf. import os. import tf_slim as slim. from tf_slim.nets import inception_v3. !gsutil cp gs://deepvariant/models/DeepVariant/0.8.0/DeepVariant-inception_v3-0.8.0+data-wgs_standard/model* /tmp/. ckpt_file = '/tmp/model.ckpt'. graph = tf.Graph(). with graph.as_default():. images = tf.placeholder(tf.float32, shape=(None, 100, 221, 6)). with slim.arg_scope(inception_v3.inception_v3_arg_scope()):. _, end_points = inception_v3.inception_v3(images, is_training=False, . num_classes=3,. create_aux_logits=False). . print(""end_points:""). print(end_points.keys()). # Restore the checkpoint. sess = tf.Session(graph=graph). saver = tf.train.Saver(). saver.restore(sess, ckpt_file). ```. For training, you would need a lot more training data than the quickstart-testdata, as in multiple samples of WGS worth of sequencing data. For more information on training, see the documentation: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:201,reliability,checkpoint,checkpoints,201,"Hi @jaydevshelat . DeepVariant's checkpoints are based on InceptionV3 and use slim, see https://github.com/google-research/tf-slim. I can share with you some code that @pichuan has written to read the checkpoints, but if you have any further questions on that, please consult the documentation for tensorflow or tf-slim. ```. ! pip install tf-slim. import tensorflow.compat.v1 as tf. import os. import tf_slim as slim. from tf_slim.nets import inception_v3. !gsutil cp gs://deepvariant/models/DeepVariant/0.8.0/DeepVariant-inception_v3-0.8.0+data-wgs_standard/model* /tmp/. ckpt_file = '/tmp/model.ckpt'. graph = tf.Graph(). with graph.as_default():. images = tf.placeholder(tf.float32, shape=(None, 100, 221, 6)). with slim.arg_scope(inception_v3.inception_v3_arg_scope()):. _, end_points = inception_v3.inception_v3(images, is_training=False, . num_classes=3,. create_aux_logits=False). . print(""end_points:""). print(end_points.keys()). # Restore the checkpoint. sess = tf.Session(graph=graph). saver = tf.train.Saver(). saver.restore(sess, ckpt_file). ```. For training, you would need a lot more training data than the quickstart-testdata, as in multiple samples of WGS worth of sequencing data. For more information on training, see the documentation: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:315,reliability,sli,slim,315,"Hi @jaydevshelat . DeepVariant's checkpoints are based on InceptionV3 and use slim, see https://github.com/google-research/tf-slim. I can share with you some code that @pichuan has written to read the checkpoints, but if you have any further questions on that, please consult the documentation for tensorflow or tf-slim. ```. ! pip install tf-slim. import tensorflow.compat.v1 as tf. import os. import tf_slim as slim. from tf_slim.nets import inception_v3. !gsutil cp gs://deepvariant/models/DeepVariant/0.8.0/DeepVariant-inception_v3-0.8.0+data-wgs_standard/model* /tmp/. ckpt_file = '/tmp/model.ckpt'. graph = tf.Graph(). with graph.as_default():. images = tf.placeholder(tf.float32, shape=(None, 100, 221, 6)). with slim.arg_scope(inception_v3.inception_v3_arg_scope()):. _, end_points = inception_v3.inception_v3(images, is_training=False, . num_classes=3,. create_aux_logits=False). . print(""end_points:""). print(end_points.keys()). # Restore the checkpoint. sess = tf.Session(graph=graph). saver = tf.train.Saver(). saver.restore(sess, ckpt_file). ```. For training, you would need a lot more training data than the quickstart-testdata, as in multiple samples of WGS worth of sequencing data. For more information on training, see the documentation: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:343,reliability,sli,slim,343,"Hi @jaydevshelat . DeepVariant's checkpoints are based on InceptionV3 and use slim, see https://github.com/google-research/tf-slim. I can share with you some code that @pichuan has written to read the checkpoints, but if you have any further questions on that, please consult the documentation for tensorflow or tf-slim. ```. ! pip install tf-slim. import tensorflow.compat.v1 as tf. import os. import tf_slim as slim. from tf_slim.nets import inception_v3. !gsutil cp gs://deepvariant/models/DeepVariant/0.8.0/DeepVariant-inception_v3-0.8.0+data-wgs_standard/model* /tmp/. ckpt_file = '/tmp/model.ckpt'. graph = tf.Graph(). with graph.as_default():. images = tf.placeholder(tf.float32, shape=(None, 100, 221, 6)). with slim.arg_scope(inception_v3.inception_v3_arg_scope()):. _, end_points = inception_v3.inception_v3(images, is_training=False, . num_classes=3,. create_aux_logits=False). . print(""end_points:""). print(end_points.keys()). # Restore the checkpoint. sess = tf.Session(graph=graph). saver = tf.train.Saver(). saver.restore(sess, ckpt_file). ```. For training, you would need a lot more training data than the quickstart-testdata, as in multiple samples of WGS worth of sequencing data. For more information on training, see the documentation: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:413,reliability,sli,slim,413,"Hi @jaydevshelat . DeepVariant's checkpoints are based on InceptionV3 and use slim, see https://github.com/google-research/tf-slim. I can share with you some code that @pichuan has written to read the checkpoints, but if you have any further questions on that, please consult the documentation for tensorflow or tf-slim. ```. ! pip install tf-slim. import tensorflow.compat.v1 as tf. import os. import tf_slim as slim. from tf_slim.nets import inception_v3. !gsutil cp gs://deepvariant/models/DeepVariant/0.8.0/DeepVariant-inception_v3-0.8.0+data-wgs_standard/model* /tmp/. ckpt_file = '/tmp/model.ckpt'. graph = tf.Graph(). with graph.as_default():. images = tf.placeholder(tf.float32, shape=(None, 100, 221, 6)). with slim.arg_scope(inception_v3.inception_v3_arg_scope()):. _, end_points = inception_v3.inception_v3(images, is_training=False, . num_classes=3,. create_aux_logits=False). . print(""end_points:""). print(end_points.keys()). # Restore the checkpoint. sess = tf.Session(graph=graph). saver = tf.train.Saver(). saver.restore(sess, ckpt_file). ```. For training, you would need a lot more training data than the quickstart-testdata, as in multiple samples of WGS worth of sequencing data. For more information on training, see the documentation: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:720,reliability,sli,slim,720,"Hi @jaydevshelat . DeepVariant's checkpoints are based on InceptionV3 and use slim, see https://github.com/google-research/tf-slim. I can share with you some code that @pichuan has written to read the checkpoints, but if you have any further questions on that, please consult the documentation for tensorflow or tf-slim. ```. ! pip install tf-slim. import tensorflow.compat.v1 as tf. import os. import tf_slim as slim. from tf_slim.nets import inception_v3. !gsutil cp gs://deepvariant/models/DeepVariant/0.8.0/DeepVariant-inception_v3-0.8.0+data-wgs_standard/model* /tmp/. ckpt_file = '/tmp/model.ckpt'. graph = tf.Graph(). with graph.as_default():. images = tf.placeholder(tf.float32, shape=(None, 100, 221, 6)). with slim.arg_scope(inception_v3.inception_v3_arg_scope()):. _, end_points = inception_v3.inception_v3(images, is_training=False, . num_classes=3,. create_aux_logits=False). . print(""end_points:""). print(end_points.keys()). # Restore the checkpoint. sess = tf.Session(graph=graph). saver = tf.train.Saver(). saver.restore(sess, ckpt_file). ```. For training, you would need a lot more training data than the quickstart-testdata, as in multiple samples of WGS worth of sequencing data. For more information on training, see the documentation: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:941,reliability,Restor,Restore,941,"Hi @jaydevshelat . DeepVariant's checkpoints are based on InceptionV3 and use slim, see https://github.com/google-research/tf-slim. I can share with you some code that @pichuan has written to read the checkpoints, but if you have any further questions on that, please consult the documentation for tensorflow or tf-slim. ```. ! pip install tf-slim. import tensorflow.compat.v1 as tf. import os. import tf_slim as slim. from tf_slim.nets import inception_v3. !gsutil cp gs://deepvariant/models/DeepVariant/0.8.0/DeepVariant-inception_v3-0.8.0+data-wgs_standard/model* /tmp/. ckpt_file = '/tmp/model.ckpt'. graph = tf.Graph(). with graph.as_default():. images = tf.placeholder(tf.float32, shape=(None, 100, 221, 6)). with slim.arg_scope(inception_v3.inception_v3_arg_scope()):. _, end_points = inception_v3.inception_v3(images, is_training=False, . num_classes=3,. create_aux_logits=False). . print(""end_points:""). print(end_points.keys()). # Restore the checkpoint. sess = tf.Session(graph=graph). saver = tf.train.Saver(). saver.restore(sess, ckpt_file). ```. For training, you would need a lot more training data than the quickstart-testdata, as in multiple samples of WGS worth of sequencing data. For more information on training, see the documentation: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:953,reliability,checkpoint,checkpoint,953,"Hi @jaydevshelat . DeepVariant's checkpoints are based on InceptionV3 and use slim, see https://github.com/google-research/tf-slim. I can share with you some code that @pichuan has written to read the checkpoints, but if you have any further questions on that, please consult the documentation for tensorflow or tf-slim. ```. ! pip install tf-slim. import tensorflow.compat.v1 as tf. import os. import tf_slim as slim. from tf_slim.nets import inception_v3. !gsutil cp gs://deepvariant/models/DeepVariant/0.8.0/DeepVariant-inception_v3-0.8.0+data-wgs_standard/model* /tmp/. ckpt_file = '/tmp/model.ckpt'. graph = tf.Graph(). with graph.as_default():. images = tf.placeholder(tf.float32, shape=(None, 100, 221, 6)). with slim.arg_scope(inception_v3.inception_v3_arg_scope()):. _, end_points = inception_v3.inception_v3(images, is_training=False, . num_classes=3,. create_aux_logits=False). . print(""end_points:""). print(end_points.keys()). # Restore the checkpoint. sess = tf.Session(graph=graph). saver = tf.train.Saver(). saver.restore(sess, ckpt_file). ```. For training, you would need a lot more training data than the quickstart-testdata, as in multiple samples of WGS worth of sequencing data. For more information on training, see the documentation: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:1029,reliability,restor,restore,1029,"Hi @jaydevshelat . DeepVariant's checkpoints are based on InceptionV3 and use slim, see https://github.com/google-research/tf-slim. I can share with you some code that @pichuan has written to read the checkpoints, but if you have any further questions on that, please consult the documentation for tensorflow or tf-slim. ```. ! pip install tf-slim. import tensorflow.compat.v1 as tf. import os. import tf_slim as slim. from tf_slim.nets import inception_v3. !gsutil cp gs://deepvariant/models/DeepVariant/0.8.0/DeepVariant-inception_v3-0.8.0+data-wgs_standard/model* /tmp/. ckpt_file = '/tmp/model.ckpt'. graph = tf.Graph(). with graph.as_default():. images = tf.placeholder(tf.float32, shape=(None, 100, 221, 6)). with slim.arg_scope(inception_v3.inception_v3_arg_scope()):. _, end_points = inception_v3.inception_v3(images, is_training=False, . num_classes=3,. create_aux_logits=False). . print(""end_points:""). print(end_points.keys()). # Restore the checkpoint. sess = tf.Session(graph=graph). saver = tf.train.Saver(). saver.restore(sess, ckpt_file). ```. For training, you would need a lot more training data than the quickstart-testdata, as in multiple samples of WGS worth of sequencing data. For more information on training, see the documentation: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:1134,safety,test,testdata,1134,"Hi @jaydevshelat . DeepVariant's checkpoints are based on InceptionV3 and use slim, see https://github.com/google-research/tf-slim. I can share with you some code that @pichuan has written to read the checkpoints, but if you have any further questions on that, please consult the documentation for tensorflow or tf-slim. ```. ! pip install tf-slim. import tensorflow.compat.v1 as tf. import os. import tf_slim as slim. from tf_slim.nets import inception_v3. !gsutil cp gs://deepvariant/models/DeepVariant/0.8.0/DeepVariant-inception_v3-0.8.0+data-wgs_standard/model* /tmp/. ckpt_file = '/tmp/model.ckpt'. graph = tf.Graph(). with graph.as_default():. images = tf.placeholder(tf.float32, shape=(None, 100, 221, 6)). with slim.arg_scope(inception_v3.inception_v3_arg_scope()):. _, end_points = inception_v3.inception_v3(images, is_training=False, . num_classes=3,. create_aux_logits=False). . print(""end_points:""). print(end_points.keys()). # Restore the checkpoint. sess = tf.Session(graph=graph). saver = tf.train.Saver(). saver.restore(sess, ckpt_file). ```. For training, you would need a lot more training data than the quickstart-testdata, as in multiple samples of WGS worth of sequencing data. For more information on training, see the documentation: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:486,security,model,models,486,"Hi @jaydevshelat . DeepVariant's checkpoints are based on InceptionV3 and use slim, see https://github.com/google-research/tf-slim. I can share with you some code that @pichuan has written to read the checkpoints, but if you have any further questions on that, please consult the documentation for tensorflow or tf-slim. ```. ! pip install tf-slim. import tensorflow.compat.v1 as tf. import os. import tf_slim as slim. from tf_slim.nets import inception_v3. !gsutil cp gs://deepvariant/models/DeepVariant/0.8.0/DeepVariant-inception_v3-0.8.0+data-wgs_standard/model* /tmp/. ckpt_file = '/tmp/model.ckpt'. graph = tf.Graph(). with graph.as_default():. images = tf.placeholder(tf.float32, shape=(None, 100, 221, 6)). with slim.arg_scope(inception_v3.inception_v3_arg_scope()):. _, end_points = inception_v3.inception_v3(images, is_training=False, . num_classes=3,. create_aux_logits=False). . print(""end_points:""). print(end_points.keys()). # Restore the checkpoint. sess = tf.Session(graph=graph). saver = tf.train.Saver(). saver.restore(sess, ckpt_file). ```. For training, you would need a lot more training data than the quickstart-testdata, as in multiple samples of WGS worth of sequencing data. For more information on training, see the documentation: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:560,security,model,model,560,"Hi @jaydevshelat . DeepVariant's checkpoints are based on InceptionV3 and use slim, see https://github.com/google-research/tf-slim. I can share with you some code that @pichuan has written to read the checkpoints, but if you have any further questions on that, please consult the documentation for tensorflow or tf-slim. ```. ! pip install tf-slim. import tensorflow.compat.v1 as tf. import os. import tf_slim as slim. from tf_slim.nets import inception_v3. !gsutil cp gs://deepvariant/models/DeepVariant/0.8.0/DeepVariant-inception_v3-0.8.0+data-wgs_standard/model* /tmp/. ckpt_file = '/tmp/model.ckpt'. graph = tf.Graph(). with graph.as_default():. images = tf.placeholder(tf.float32, shape=(None, 100, 221, 6)). with slim.arg_scope(inception_v3.inception_v3_arg_scope()):. _, end_points = inception_v3.inception_v3(images, is_training=False, . num_classes=3,. create_aux_logits=False). . print(""end_points:""). print(end_points.keys()). # Restore the checkpoint. sess = tf.Session(graph=graph). saver = tf.train.Saver(). saver.restore(sess, ckpt_file). ```. For training, you would need a lot more training data than the quickstart-testdata, as in multiple samples of WGS worth of sequencing data. For more information on training, see the documentation: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:592,security,model,model,592,"Hi @jaydevshelat . DeepVariant's checkpoints are based on InceptionV3 and use slim, see https://github.com/google-research/tf-slim. I can share with you some code that @pichuan has written to read the checkpoints, but if you have any further questions on that, please consult the documentation for tensorflow or tf-slim. ```. ! pip install tf-slim. import tensorflow.compat.v1 as tf. import os. import tf_slim as slim. from tf_slim.nets import inception_v3. !gsutil cp gs://deepvariant/models/DeepVariant/0.8.0/DeepVariant-inception_v3-0.8.0+data-wgs_standard/model* /tmp/. ckpt_file = '/tmp/model.ckpt'. graph = tf.Graph(). with graph.as_default():. images = tf.placeholder(tf.float32, shape=(None, 100, 221, 6)). with slim.arg_scope(inception_v3.inception_v3_arg_scope()):. _, end_points = inception_v3.inception_v3(images, is_training=False, . num_classes=3,. create_aux_logits=False). . print(""end_points:""). print(end_points.keys()). # Restore the checkpoint. sess = tf.Session(graph=graph). saver = tf.train.Saver(). saver.restore(sess, ckpt_file). ```. For training, you would need a lot more training data than the quickstart-testdata, as in multiple samples of WGS worth of sequencing data. For more information on training, see the documentation: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:975,security,Session,Session,975,"Hi @jaydevshelat . DeepVariant's checkpoints are based on InceptionV3 and use slim, see https://github.com/google-research/tf-slim. I can share with you some code that @pichuan has written to read the checkpoints, but if you have any further questions on that, please consult the documentation for tensorflow or tf-slim. ```. ! pip install tf-slim. import tensorflow.compat.v1 as tf. import os. import tf_slim as slim. from tf_slim.nets import inception_v3. !gsutil cp gs://deepvariant/models/DeepVariant/0.8.0/DeepVariant-inception_v3-0.8.0+data-wgs_standard/model* /tmp/. ckpt_file = '/tmp/model.ckpt'. graph = tf.Graph(). with graph.as_default():. images = tf.placeholder(tf.float32, shape=(None, 100, 221, 6)). with slim.arg_scope(inception_v3.inception_v3_arg_scope()):. _, end_points = inception_v3.inception_v3(images, is_training=False, . num_classes=3,. create_aux_logits=False). . print(""end_points:""). print(end_points.keys()). # Restore the checkpoint. sess = tf.Session(graph=graph). saver = tf.train.Saver(). saver.restore(sess, ckpt_file). ```. For training, you would need a lot more training data than the quickstart-testdata, as in multiple samples of WGS worth of sequencing data. For more information on training, see the documentation: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:1134,testability,test,testdata,1134,"Hi @jaydevshelat . DeepVariant's checkpoints are based on InceptionV3 and use slim, see https://github.com/google-research/tf-slim. I can share with you some code that @pichuan has written to read the checkpoints, but if you have any further questions on that, please consult the documentation for tensorflow or tf-slim. ```. ! pip install tf-slim. import tensorflow.compat.v1 as tf. import os. import tf_slim as slim. from tf_slim.nets import inception_v3. !gsutil cp gs://deepvariant/models/DeepVariant/0.8.0/DeepVariant-inception_v3-0.8.0+data-wgs_standard/model* /tmp/. ckpt_file = '/tmp/model.ckpt'. graph = tf.Graph(). with graph.as_default():. images = tf.placeholder(tf.float32, shape=(None, 100, 221, 6)). with slim.arg_scope(inception_v3.inception_v3_arg_scope()):. _, end_points = inception_v3.inception_v3(images, is_training=False, . num_classes=3,. create_aux_logits=False). . print(""end_points:""). print(end_points.keys()). # Restore the checkpoint. sess = tf.Session(graph=graph). saver = tf.train.Saver(). saver.restore(sess, ckpt_file). ```. For training, you would need a lot more training data than the quickstart-testdata, as in multiple samples of WGS worth of sequencing data. For more information on training, see the documentation: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:280,usability,document,documentation,280,"Hi @jaydevshelat . DeepVariant's checkpoints are based on InceptionV3 and use slim, see https://github.com/google-research/tf-slim. I can share with you some code that @pichuan has written to read the checkpoints, but if you have any further questions on that, please consult the documentation for tensorflow or tf-slim. ```. ! pip install tf-slim. import tensorflow.compat.v1 as tf. import os. import tf_slim as slim. from tf_slim.nets import inception_v3. !gsutil cp gs://deepvariant/models/DeepVariant/0.8.0/DeepVariant-inception_v3-0.8.0+data-wgs_standard/model* /tmp/. ckpt_file = '/tmp/model.ckpt'. graph = tf.Graph(). with graph.as_default():. images = tf.placeholder(tf.float32, shape=(None, 100, 221, 6)). with slim.arg_scope(inception_v3.inception_v3_arg_scope()):. _, end_points = inception_v3.inception_v3(images, is_training=False, . num_classes=3,. create_aux_logits=False). . print(""end_points:""). print(end_points.keys()). # Restore the checkpoint. sess = tf.Session(graph=graph). saver = tf.train.Saver(). saver.restore(sess, ckpt_file). ```. For training, you would need a lot more training data than the quickstart-testdata, as in multiple samples of WGS worth of sequencing data. For more information on training, see the documentation: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/339:1242,usability,document,documentation,1242,"Hi @jaydevshelat . DeepVariant's checkpoints are based on InceptionV3 and use slim, see https://github.com/google-research/tf-slim. I can share with you some code that @pichuan has written to read the checkpoints, but if you have any further questions on that, please consult the documentation for tensorflow or tf-slim. ```. ! pip install tf-slim. import tensorflow.compat.v1 as tf. import os. import tf_slim as slim. from tf_slim.nets import inception_v3. !gsutil cp gs://deepvariant/models/DeepVariant/0.8.0/DeepVariant-inception_v3-0.8.0+data-wgs_standard/model* /tmp/. ckpt_file = '/tmp/model.ckpt'. graph = tf.Graph(). with graph.as_default():. images = tf.placeholder(tf.float32, shape=(None, 100, 221, 6)). with slim.arg_scope(inception_v3.inception_v3_arg_scope()):. _, end_points = inception_v3.inception_v3(images, is_training=False, . num_classes=3,. create_aux_logits=False). . print(""end_points:""). print(end_points.keys()). # Restore the checkpoint. sess = tf.Session(graph=graph). saver = tf.train.Saver(). saver.restore(sess, ckpt_file). ```. For training, you would need a lot more training data than the quickstart-testdata, as in multiple samples of WGS worth of sequencing data. For more information on training, see the documentation: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/339
https://github.com/google/deepvariant/issues/340:95,deployability,build,build-test,95,"Hi @jaydevshelat, this [doc](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-build-test.md) shows you how to build from source. Note that it mentions support for Ubuntu 14 and 16, so you may need to make some changes in order for it to work on your OS. Feel free to followup if you hit any issues.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/340
https://github.com/google/deepvariant/issues/340:127,deployability,build,build,127,"Hi @jaydevshelat, this [doc](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-build-test.md) shows you how to build from source. Note that it mentions support for Ubuntu 14 and 16, so you may need to make some changes in order for it to work on your OS. Feel free to followup if you hit any issues.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/340
https://github.com/google/deepvariant/issues/340:101,safety,test,test,101,"Hi @jaydevshelat, this [doc](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-build-test.md) shows you how to build from source. Note that it mentions support for Ubuntu 14 and 16, so you may need to make some changes in order for it to work on your OS. Feel free to followup if you hit any issues.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/340
https://github.com/google/deepvariant/issues/340:101,testability,test,test,101,"Hi @jaydevshelat, this [doc](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-build-test.md) shows you how to build from source. Note that it mentions support for Ubuntu 14 and 16, so you may need to make some changes in order for it to work on your OS. Feel free to followup if you hit any issues.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/340
https://github.com/google/deepvariant/issues/340:168,usability,support,support,168,"Hi @jaydevshelat, this [doc](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-build-test.md) shows you how to build from source. Note that it mentions support for Ubuntu 14 and 16, so you may need to make some changes in order for it to work on your OS. Feel free to followup if you hit any issues.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/340
https://github.com/google/deepvariant/issues/341:160,deployability,releas,released,160,"Hi @GuillaumeHolley, thanks for raising the issue. Could you try to run this with 1.0.0 image and let me know whether the issue persists? We haven't officially released 1.0.0 just yet, but we can use it to test this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/341
https://github.com/google/deepvariant/issues/341:206,safety,test,test,206,"Hi @GuillaumeHolley, thanks for raising the issue. Could you try to run this with 1.0.0 image and let me know whether the issue persists? We haven't officially released 1.0.0 just yet, but we can use it to test this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/341
https://github.com/google/deepvariant/issues/341:206,testability,test,test,206,"Hi @GuillaumeHolley, thanks for raising the issue. Could you try to run this with 1.0.0 image and let me know whether the issue persists? We haven't officially released 1.0.0 just yet, but we can use it to test this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/341
https://github.com/google/deepvariant/issues/341:114,availability,error,error,114,"Hi @sgoe1,. Using the 1.0.0 image, the variant calling fails on nearly all of my input samples with the following error (sites vary from one sample to another):. ```. 2020-09-03 10:01:02.237494: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmpqosj7aak/call_variants_output.tfrecord.gz. 2020-09-03 10:01:02.575525: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 63099. I0903 10:01:02.988874 140046207305472 postprocess_variants.py:1079] CVO sorting took 0.012529869874318441 minutes. I0903 10:01:02.989492 140046207305472 postprocess_variants.py:1081] Transforming call_variants_output to variants. I0903 10:01:02.989910 140046207305472 postprocess_variants.py:1108] Merging and writing variants to VCF and gVCF. I0903 10:01:02.996278 140046207305472 genomics_writer.py:172] Writing /<REDACTED>/ID.2.vcf.gz with NativeVcfWriter. I0903 10:01:02.998338 140046207305472 genomics_writer.py:172] Writing /<REDACTED>/ID.2.g.vcf.gz with NativeVcfWriter. W0903 10:01:17.865604 140046207305472 postprocess_variants.py:394] Alt allele indices found from call_variants_outputs for variant reference_bases: ""CTT"". alternate_bases: ""CT"". alternate_bases: ""CTTT"". alternate_bases: ""CTTTTTTTCT"". calls {. info {. key: ""AD"". value {. values {. int_value: 7. }. values {. int_value: 7. }. values {. int_value: 0. }. values {. int_value: 0. }. }. }. info {. key: ""DP"". value {. values {. int_value: 19. }. }. }. info {. key: ""VAF"". value {. values {. number_value: 0.3684210526315789. }. values {. number_value: 0.0. }. values {. number_value: 0.0. }. }. }. genotype: -1. genotype: -1. call_set_name: ""ID"". }. end: 21441513. reference_name: ""chr16"". start: 21441510. is [[0], [0], [0, 1], [0, 2], [1], [1, 2], [2]], which is invalid. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1143, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tens",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/341
https://github.com/google/deepvariant/issues/341:55,deployability,fail,fails,55,"Hi @sgoe1,. Using the 1.0.0 image, the variant calling fails on nearly all of my input samples with the following error (sites vary from one sample to another):. ```. 2020-09-03 10:01:02.237494: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmpqosj7aak/call_variants_output.tfrecord.gz. 2020-09-03 10:01:02.575525: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 63099. I0903 10:01:02.988874 140046207305472 postprocess_variants.py:1079] CVO sorting took 0.012529869874318441 minutes. I0903 10:01:02.989492 140046207305472 postprocess_variants.py:1081] Transforming call_variants_output to variants. I0903 10:01:02.989910 140046207305472 postprocess_variants.py:1108] Merging and writing variants to VCF and gVCF. I0903 10:01:02.996278 140046207305472 genomics_writer.py:172] Writing /<REDACTED>/ID.2.vcf.gz with NativeVcfWriter. I0903 10:01:02.998338 140046207305472 genomics_writer.py:172] Writing /<REDACTED>/ID.2.g.vcf.gz with NativeVcfWriter. W0903 10:01:17.865604 140046207305472 postprocess_variants.py:394] Alt allele indices found from call_variants_outputs for variant reference_bases: ""CTT"". alternate_bases: ""CT"". alternate_bases: ""CTTT"". alternate_bases: ""CTTTTTTTCT"". calls {. info {. key: ""AD"". value {. values {. int_value: 7. }. values {. int_value: 7. }. values {. int_value: 0. }. values {. int_value: 0. }. }. }. info {. key: ""DP"". value {. values {. int_value: 19. }. }. }. info {. key: ""VAF"". value {. values {. number_value: 0.3684210526315789. }. values {. number_value: 0.0. }. values {. number_value: 0.0. }. }. }. genotype: -1. genotype: -1. call_set_name: ""ID"". }. end: 21441513. reference_name: ""chr16"". start: 21441510. is [[0], [0], [0, 1], [0, 2], [1], [1, 2], [2]], which is invalid. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1143, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tens",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/341
https://github.com/google/deepvariant/issues/341:1918,deployability,modul,module,1918,"_writer.py:172] Writing /<REDACTED>/ID.2.g.vcf.gz with NativeVcfWriter. W0903 10:01:17.865604 140046207305472 postprocess_variants.py:394] Alt allele indices found from call_variants_outputs for variant reference_bases: ""CTT"". alternate_bases: ""CT"". alternate_bases: ""CTTT"". alternate_bases: ""CTTTTTTTCT"". calls {. info {. key: ""AD"". value {. values {. int_value: 7. }. values {. int_value: 7. }. values {. int_value: 0. }. values {. int_value: 0. }. }. }. info {. key: ""DP"". value {. values {. int_value: 19. }. }. }. info {. key: ""VAF"". value {. values {. number_value: 0.3684210526315789. }. values {. number_value: 0.0. }. values {. number_value: 0.0. }. }. }. genotype: -1. genotype: -1. call_set_name: ""ID"". }. end: 21441513. reference_name: ""chr16"". start: 21441510. is [[0], [0], [0, 1], [0, 2], [1], [1, 2], [2]], which is invalid. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1143, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1122, in main. vcf_writer, gvcf_writer). File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 962, in merge_and_write_variants_and_nonvariants. variant = next_or_none(variant_iterable). File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 948, in next_or_none. return next(iterable). File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/dee",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/341
https://github.com/google/deepvariant/issues/341:598,integrability,Transform,Transforming,598,"Hi @sgoe1,. Using the 1.0.0 image, the variant calling fails on nearly all of my input samples with the following error (sites vary from one sample to another):. ```. 2020-09-03 10:01:02.237494: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmpqosj7aak/call_variants_output.tfrecord.gz. 2020-09-03 10:01:02.575525: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 63099. I0903 10:01:02.988874 140046207305472 postprocess_variants.py:1079] CVO sorting took 0.012529869874318441 minutes. I0903 10:01:02.989492 140046207305472 postprocess_variants.py:1081] Transforming call_variants_output to variants. I0903 10:01:02.989910 140046207305472 postprocess_variants.py:1108] Merging and writing variants to VCF and gVCF. I0903 10:01:02.996278 140046207305472 genomics_writer.py:172] Writing /<REDACTED>/ID.2.vcf.gz with NativeVcfWriter. I0903 10:01:02.998338 140046207305472 genomics_writer.py:172] Writing /<REDACTED>/ID.2.g.vcf.gz with NativeVcfWriter. W0903 10:01:17.865604 140046207305472 postprocess_variants.py:394] Alt allele indices found from call_variants_outputs for variant reference_bases: ""CTT"". alternate_bases: ""CT"". alternate_bases: ""CTTT"". alternate_bases: ""CTTTTTTTCT"". calls {. info {. key: ""AD"". value {. values {. int_value: 7. }. values {. int_value: 7. }. values {. int_value: 0. }. values {. int_value: 0. }. }. }. info {. key: ""DP"". value {. values {. int_value: 19. }. }. }. info {. key: ""VAF"". value {. values {. number_value: 0.3684210526315789. }. values {. number_value: 0.0. }. values {. number_value: 0.0. }. }. }. genotype: -1. genotype: -1. call_set_name: ""ID"". }. end: 21441513. reference_name: ""chr16"". start: 21441510. is [[0], [0], [0, 1], [0, 2], [1], [1, 2], [2]], which is invalid. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1143, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tens",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/341
https://github.com/google/deepvariant/issues/341:598,interoperability,Transform,Transforming,598,"Hi @sgoe1,. Using the 1.0.0 image, the variant calling fails on nearly all of my input samples with the following error (sites vary from one sample to another):. ```. 2020-09-03 10:01:02.237494: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmpqosj7aak/call_variants_output.tfrecord.gz. 2020-09-03 10:01:02.575525: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 63099. I0903 10:01:02.988874 140046207305472 postprocess_variants.py:1079] CVO sorting took 0.012529869874318441 minutes. I0903 10:01:02.989492 140046207305472 postprocess_variants.py:1081] Transforming call_variants_output to variants. I0903 10:01:02.989910 140046207305472 postprocess_variants.py:1108] Merging and writing variants to VCF and gVCF. I0903 10:01:02.996278 140046207305472 genomics_writer.py:172] Writing /<REDACTED>/ID.2.vcf.gz with NativeVcfWriter. I0903 10:01:02.998338 140046207305472 genomics_writer.py:172] Writing /<REDACTED>/ID.2.g.vcf.gz with NativeVcfWriter. W0903 10:01:17.865604 140046207305472 postprocess_variants.py:394] Alt allele indices found from call_variants_outputs for variant reference_bases: ""CTT"". alternate_bases: ""CT"". alternate_bases: ""CTTT"". alternate_bases: ""CTTTTTTTCT"". calls {. info {. key: ""AD"". value {. values {. int_value: 7. }. values {. int_value: 7. }. values {. int_value: 0. }. values {. int_value: 0. }. }. }. info {. key: ""DP"". value {. values {. int_value: 19. }. }. }. info {. key: ""VAF"". value {. values {. number_value: 0.3684210526315789. }. values {. number_value: 0.0. }. values {. number_value: 0.0. }. }. }. genotype: -1. genotype: -1. call_set_name: ""ID"". }. end: 21441513. reference_name: ""chr16"". start: 21441510. is [[0], [0], [0, 1], [0, 2], [1], [1, 2], [2]], which is invalid. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1143, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tens",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/341
https://github.com/google/deepvariant/issues/341:2019,interoperability,platform,platform,2019,"7305472 postprocess_variants.py:394] Alt allele indices found from call_variants_outputs for variant reference_bases: ""CTT"". alternate_bases: ""CT"". alternate_bases: ""CTTT"". alternate_bases: ""CTTTTTTTCT"". calls {. info {. key: ""AD"". value {. values {. int_value: 7. }. values {. int_value: 7. }. values {. int_value: 0. }. values {. int_value: 0. }. }. }. info {. key: ""DP"". value {. values {. int_value: 19. }. }. }. info {. key: ""VAF"". value {. values {. number_value: 0.3684210526315789. }. values {. number_value: 0.0. }. values {. number_value: 0.0. }. }. }. genotype: -1. genotype: -1. call_set_name: ""ID"". }. end: 21441513. reference_name: ""chr16"". start: 21441510. is [[0], [0], [0, 1], [0, 2], [1], [1, 2], [2]], which is invalid. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1143, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1122, in main. vcf_writer, gvcf_writer). File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 962, in merge_and_write_variants_and_nonvariants. variant = next_or_none(variant_iterable). File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 948, in next_or_none. return next(iterable). File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 91, in maybe_resolve_conflicting_variants. for overlapping_candidates in",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/341
https://github.com/google/deepvariant/issues/341:1918,modifiability,modul,module,1918,"_writer.py:172] Writing /<REDACTED>/ID.2.g.vcf.gz with NativeVcfWriter. W0903 10:01:17.865604 140046207305472 postprocess_variants.py:394] Alt allele indices found from call_variants_outputs for variant reference_bases: ""CTT"". alternate_bases: ""CT"". alternate_bases: ""CTTT"". alternate_bases: ""CTTTTTTTCT"". calls {. info {. key: ""AD"". value {. values {. int_value: 7. }. values {. int_value: 7. }. values {. int_value: 0. }. values {. int_value: 0. }. }. }. info {. key: ""DP"". value {. values {. int_value: 19. }. }. }. info {. key: ""VAF"". value {. values {. number_value: 0.3684210526315789. }. values {. number_value: 0.0. }. values {. number_value: 0.0. }. }. }. genotype: -1. genotype: -1. call_set_name: ""ID"". }. end: 21441513. reference_name: ""chr16"". start: 21441510. is [[0], [0], [0, 1], [0, 2], [1], [1, 2], [2]], which is invalid. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1143, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1122, in main. vcf_writer, gvcf_writer). File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 962, in merge_and_write_variants_and_nonvariants. variant = next_or_none(variant_iterable). File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 948, in next_or_none. return next(iterable). File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/dee",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/341
https://github.com/google/deepvariant/issues/341:1987,modifiability,pac,packages,1987,". W0903 10:01:17.865604 140046207305472 postprocess_variants.py:394] Alt allele indices found from call_variants_outputs for variant reference_bases: ""CTT"". alternate_bases: ""CT"". alternate_bases: ""CTTT"". alternate_bases: ""CTTTTTTTCT"". calls {. info {. key: ""AD"". value {. values {. int_value: 7. }. values {. int_value: 7. }. values {. int_value: 0. }. values {. int_value: 0. }. }. }. info {. key: ""DP"". value {. values {. int_value: 19. }. }. }. info {. key: ""VAF"". value {. values {. number_value: 0.3684210526315789. }. values {. number_value: 0.0. }. values {. number_value: 0.0. }. }. }. genotype: -1. genotype: -1. call_set_name: ""ID"". }. end: 21441513. reference_name: ""chr16"". start: 21441510. is [[0], [0], [0, 1], [0, 2], [1], [1, 2], [2]], which is invalid. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1143, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1122, in main. vcf_writer, gvcf_writer). File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 962, in merge_and_write_variants_and_nonvariants. variant = next_or_none(variant_iterable). File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 948, in next_or_none. return next(iterable). File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 91, in maybe_resolve_conflicting_variant",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/341
https://github.com/google/deepvariant/issues/341:114,performance,error,error,114,"Hi @sgoe1,. Using the 1.0.0 image, the variant calling fails on nearly all of my input samples with the following error (sites vary from one sample to another):. ```. 2020-09-03 10:01:02.237494: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmpqosj7aak/call_variants_output.tfrecord.gz. 2020-09-03 10:01:02.575525: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 63099. I0903 10:01:02.988874 140046207305472 postprocess_variants.py:1079] CVO sorting took 0.012529869874318441 minutes. I0903 10:01:02.989492 140046207305472 postprocess_variants.py:1081] Transforming call_variants_output to variants. I0903 10:01:02.989910 140046207305472 postprocess_variants.py:1108] Merging and writing variants to VCF and gVCF. I0903 10:01:02.996278 140046207305472 genomics_writer.py:172] Writing /<REDACTED>/ID.2.vcf.gz with NativeVcfWriter. I0903 10:01:02.998338 140046207305472 genomics_writer.py:172] Writing /<REDACTED>/ID.2.g.vcf.gz with NativeVcfWriter. W0903 10:01:17.865604 140046207305472 postprocess_variants.py:394] Alt allele indices found from call_variants_outputs for variant reference_bases: ""CTT"". alternate_bases: ""CT"". alternate_bases: ""CTTT"". alternate_bases: ""CTTTTTTTCT"". calls {. info {. key: ""AD"". value {. values {. int_value: 7. }. values {. int_value: 7. }. values {. int_value: 0. }. values {. int_value: 0. }. }. }. info {. key: ""DP"". value {. values {. int_value: 19. }. }. }. info {. key: ""VAF"". value {. values {. number_value: 0.3684210526315789. }. values {. number_value: 0.0. }. values {. number_value: 0.0. }. }. }. genotype: -1. genotype: -1. call_set_name: ""ID"". }. end: 21441513. reference_name: ""chr16"". start: 21441510. is [[0], [0], [0, 1], [0, 2], [1], [1, 2], [2]], which is invalid. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1143, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tens",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/341
https://github.com/google/deepvariant/issues/341:55,reliability,fail,fails,55,"Hi @sgoe1,. Using the 1.0.0 image, the variant calling fails on nearly all of my input samples with the following error (sites vary from one sample to another):. ```. 2020-09-03 10:01:02.237494: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmpqosj7aak/call_variants_output.tfrecord.gz. 2020-09-03 10:01:02.575525: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 63099. I0903 10:01:02.988874 140046207305472 postprocess_variants.py:1079] CVO sorting took 0.012529869874318441 minutes. I0903 10:01:02.989492 140046207305472 postprocess_variants.py:1081] Transforming call_variants_output to variants. I0903 10:01:02.989910 140046207305472 postprocess_variants.py:1108] Merging and writing variants to VCF and gVCF. I0903 10:01:02.996278 140046207305472 genomics_writer.py:172] Writing /<REDACTED>/ID.2.vcf.gz with NativeVcfWriter. I0903 10:01:02.998338 140046207305472 genomics_writer.py:172] Writing /<REDACTED>/ID.2.g.vcf.gz with NativeVcfWriter. W0903 10:01:17.865604 140046207305472 postprocess_variants.py:394] Alt allele indices found from call_variants_outputs for variant reference_bases: ""CTT"". alternate_bases: ""CT"". alternate_bases: ""CTTT"". alternate_bases: ""CTTTTTTTCT"". calls {. info {. key: ""AD"". value {. values {. int_value: 7. }. values {. int_value: 7. }. values {. int_value: 0. }. values {. int_value: 0. }. }. }. info {. key: ""DP"". value {. values {. int_value: 19. }. }. }. info {. key: ""VAF"". value {. values {. number_value: 0.3684210526315789. }. values {. number_value: 0.0. }. values {. number_value: 0.0. }. }. }. genotype: -1. genotype: -1. call_set_name: ""ID"". }. end: 21441513. reference_name: ""chr16"". start: 21441510. is [[0], [0], [0, 1], [0, 2], [1], [1, 2], [2]], which is invalid. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1143, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tens",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/341
https://github.com/google/deepvariant/issues/341:81,safety,input,input,81,"Hi @sgoe1,. Using the 1.0.0 image, the variant calling fails on nearly all of my input samples with the following error (sites vary from one sample to another):. ```. 2020-09-03 10:01:02.237494: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmpqosj7aak/call_variants_output.tfrecord.gz. 2020-09-03 10:01:02.575525: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 63099. I0903 10:01:02.988874 140046207305472 postprocess_variants.py:1079] CVO sorting took 0.012529869874318441 minutes. I0903 10:01:02.989492 140046207305472 postprocess_variants.py:1081] Transforming call_variants_output to variants. I0903 10:01:02.989910 140046207305472 postprocess_variants.py:1108] Merging and writing variants to VCF and gVCF. I0903 10:01:02.996278 140046207305472 genomics_writer.py:172] Writing /<REDACTED>/ID.2.vcf.gz with NativeVcfWriter. I0903 10:01:02.998338 140046207305472 genomics_writer.py:172] Writing /<REDACTED>/ID.2.g.vcf.gz with NativeVcfWriter. W0903 10:01:17.865604 140046207305472 postprocess_variants.py:394] Alt allele indices found from call_variants_outputs for variant reference_bases: ""CTT"". alternate_bases: ""CT"". alternate_bases: ""CTTT"". alternate_bases: ""CTTTTTTTCT"". calls {. info {. key: ""AD"". value {. values {. int_value: 7. }. values {. int_value: 7. }. values {. int_value: 0. }. values {. int_value: 0. }. }. }. info {. key: ""DP"". value {. values {. int_value: 19. }. }. }. info {. key: ""VAF"". value {. values {. number_value: 0.3684210526315789. }. values {. number_value: 0.0. }. values {. number_value: 0.0. }. }. }. genotype: -1. genotype: -1. call_set_name: ""ID"". }. end: 21441513. reference_name: ""chr16"". start: 21441510. is [[0], [0], [0, 1], [0, 2], [1], [1, 2], [2]], which is invalid. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1143, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tens",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/341
https://github.com/google/deepvariant/issues/341:114,safety,error,error,114,"Hi @sgoe1,. Using the 1.0.0 image, the variant calling fails on nearly all of my input samples with the following error (sites vary from one sample to another):. ```. 2020-09-03 10:01:02.237494: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmpqosj7aak/call_variants_output.tfrecord.gz. 2020-09-03 10:01:02.575525: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 63099. I0903 10:01:02.988874 140046207305472 postprocess_variants.py:1079] CVO sorting took 0.012529869874318441 minutes. I0903 10:01:02.989492 140046207305472 postprocess_variants.py:1081] Transforming call_variants_output to variants. I0903 10:01:02.989910 140046207305472 postprocess_variants.py:1108] Merging and writing variants to VCF and gVCF. I0903 10:01:02.996278 140046207305472 genomics_writer.py:172] Writing /<REDACTED>/ID.2.vcf.gz with NativeVcfWriter. I0903 10:01:02.998338 140046207305472 genomics_writer.py:172] Writing /<REDACTED>/ID.2.g.vcf.gz with NativeVcfWriter. W0903 10:01:17.865604 140046207305472 postprocess_variants.py:394] Alt allele indices found from call_variants_outputs for variant reference_bases: ""CTT"". alternate_bases: ""CT"". alternate_bases: ""CTTT"". alternate_bases: ""CTTTTTTTCT"". calls {. info {. key: ""AD"". value {. values {. int_value: 7. }. values {. int_value: 7. }. values {. int_value: 0. }. values {. int_value: 0. }. }. }. info {. key: ""DP"". value {. values {. int_value: 19. }. }. }. info {. key: ""VAF"". value {. values {. number_value: 0.3684210526315789. }. values {. number_value: 0.0. }. values {. number_value: 0.0. }. }. }. genotype: -1. genotype: -1. call_set_name: ""ID"". }. end: 21441513. reference_name: ""chr16"". start: 21441510. is [[0], [0], [0, 1], [0, 2], [1], [1, 2], [2]], which is invalid. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1143, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tens",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/341
https://github.com/google/deepvariant/issues/341:1918,safety,modul,module,1918,"_writer.py:172] Writing /<REDACTED>/ID.2.g.vcf.gz with NativeVcfWriter. W0903 10:01:17.865604 140046207305472 postprocess_variants.py:394] Alt allele indices found from call_variants_outputs for variant reference_bases: ""CTT"". alternate_bases: ""CT"". alternate_bases: ""CTTT"". alternate_bases: ""CTTTTTTTCT"". calls {. info {. key: ""AD"". value {. values {. int_value: 7. }. values {. int_value: 7. }. values {. int_value: 0. }. values {. int_value: 0. }. }. }. info {. key: ""DP"". value {. values {. int_value: 19. }. }. }. info {. key: ""VAF"". value {. values {. number_value: 0.3684210526315789. }. values {. number_value: 0.0. }. values {. number_value: 0.0. }. }. }. genotype: -1. genotype: -1. call_set_name: ""ID"". }. end: 21441513. reference_name: ""chr16"". start: 21441510. is [[0], [0], [0, 1], [0, 2], [1], [1, 2], [2]], which is invalid. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1143, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1122, in main. vcf_writer, gvcf_writer). File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 962, in merge_and_write_variants_and_nonvariants. variant = next_or_none(variant_iterable). File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 948, in next_or_none. return next(iterable). File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/dee",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/341
https://github.com/google/deepvariant/issues/341:3636,safety,sanit,sanity,3636,", 2], [1], [1, 2], [2]], which is invalid. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1143, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1122, in main. vcf_writer, gvcf_writer). File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 962, in merge_and_write_variants_and_nonvariants. variant = next_or_none(variant_iterable). File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 948, in next_or_none. return next(iterable). File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 91, in maybe_resolve_conflicting_variants. for overlapping_candidates in _group_overlapping_variants(sorted_variants):. File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 110, in _group_overlapping_variants. for variant in sorted_variants:. File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 807, in _transform_call_variants_output_to_variants. multiallelic_model=multiallelic_model). File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 680, in merge_predictions. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/341
https://github.com/google/deepvariant/issues/341:3702,safety,sanit,sanity,3702,", 2], [1], [1, 2], [2]], which is invalid. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1143, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1122, in main. vcf_writer, gvcf_writer). File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 962, in merge_and_write_variants_and_nonvariants. variant = next_or_none(variant_iterable). File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 948, in next_or_none. return next(iterable). File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 91, in maybe_resolve_conflicting_variants. for overlapping_candidates in _group_overlapping_variants(sorted_variants):. File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 110, in _group_overlapping_variants. for variant in sorted_variants:. File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 807, in _transform_call_variants_output_to_variants. multiallelic_model=multiallelic_model). File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 680, in merge_predictions. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/341
https://github.com/google/deepvariant/issues/341:3636,security,sanit,sanity,3636,", 2], [1], [1, 2], [2]], which is invalid. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1143, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1122, in main. vcf_writer, gvcf_writer). File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 962, in merge_and_write_variants_and_nonvariants. variant = next_or_none(variant_iterable). File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 948, in next_or_none. return next(iterable). File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 91, in maybe_resolve_conflicting_variants. for overlapping_candidates in _group_overlapping_variants(sorted_variants):. File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 110, in _group_overlapping_variants. for variant in sorted_variants:. File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 807, in _transform_call_variants_output_to_variants. multiallelic_model=multiallelic_model). File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 680, in merge_predictions. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/341
https://github.com/google/deepvariant/issues/341:3702,security,sanit,sanity,3702,", 2], [1], [1, 2], [2]], which is invalid. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1143, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1122, in main. vcf_writer, gvcf_writer). File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 962, in merge_and_write_variants_and_nonvariants. variant = next_or_none(variant_iterable). File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 948, in next_or_none. return next(iterable). File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 91, in maybe_resolve_conflicting_variants. for overlapping_candidates in _group_overlapping_variants(sorted_variants):. File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 110, in _group_overlapping_variants. for variant in sorted_variants:. File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 807, in _transform_call_variants_output_to_variants. multiallelic_model=multiallelic_model). File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 680, in merge_predictions. raise ValueError('`call_variants_outputs` did not pass sanity check.'). ValueError: `call_variants_outputs` did not pass sanity check. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/341
https://github.com/google/deepvariant/issues/341:1762,testability,Trace,Traceback,1762,":01:02.996278 140046207305472 genomics_writer.py:172] Writing /<REDACTED>/ID.2.vcf.gz with NativeVcfWriter. I0903 10:01:02.998338 140046207305472 genomics_writer.py:172] Writing /<REDACTED>/ID.2.g.vcf.gz with NativeVcfWriter. W0903 10:01:17.865604 140046207305472 postprocess_variants.py:394] Alt allele indices found from call_variants_outputs for variant reference_bases: ""CTT"". alternate_bases: ""CT"". alternate_bases: ""CTTT"". alternate_bases: ""CTTTTTTTCT"". calls {. info {. key: ""AD"". value {. values {. int_value: 7. }. values {. int_value: 7. }. values {. int_value: 0. }. values {. int_value: 0. }. }. }. info {. key: ""DP"". value {. values {. int_value: 19. }. }. }. info {. key: ""VAF"". value {. values {. number_value: 0.3684210526315789. }. values {. number_value: 0.0. }. values {. number_value: 0.0. }. }. }. genotype: -1. genotype: -1. call_set_name: ""ID"". }. end: 21441513. reference_name: ""chr16"". start: 21441510. is [[0], [0], [0, 1], [0, 2], [1], [1, 2], [2]], which is invalid. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1143, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run. _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef). File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1122, in main. vcf_writer, gvcf_writer). File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 962, in merge_and_write_variants_and_nonvariants. variant = next_or_none(variant_iterable). File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deep",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/341
https://github.com/google/deepvariant/issues/341:81,usability,input,input,81,"Hi @sgoe1,. Using the 1.0.0 image, the variant calling fails on nearly all of my input samples with the following error (sites vary from one sample to another):. ```. 2020-09-03 10:01:02.237494: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmpqosj7aak/call_variants_output.tfrecord.gz. 2020-09-03 10:01:02.575525: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 63099. I0903 10:01:02.988874 140046207305472 postprocess_variants.py:1079] CVO sorting took 0.012529869874318441 minutes. I0903 10:01:02.989492 140046207305472 postprocess_variants.py:1081] Transforming call_variants_output to variants. I0903 10:01:02.989910 140046207305472 postprocess_variants.py:1108] Merging and writing variants to VCF and gVCF. I0903 10:01:02.996278 140046207305472 genomics_writer.py:172] Writing /<REDACTED>/ID.2.vcf.gz with NativeVcfWriter. I0903 10:01:02.998338 140046207305472 genomics_writer.py:172] Writing /<REDACTED>/ID.2.g.vcf.gz with NativeVcfWriter. W0903 10:01:17.865604 140046207305472 postprocess_variants.py:394] Alt allele indices found from call_variants_outputs for variant reference_bases: ""CTT"". alternate_bases: ""CT"". alternate_bases: ""CTTT"". alternate_bases: ""CTTTTTTTCT"". calls {. info {. key: ""AD"". value {. values {. int_value: 7. }. values {. int_value: 7. }. values {. int_value: 0. }. values {. int_value: 0. }. }. }. info {. key: ""DP"". value {. values {. int_value: 19. }. }. }. info {. key: ""VAF"". value {. values {. number_value: 0.3684210526315789. }. values {. number_value: 0.0. }. values {. number_value: 0.0. }. }. }. genotype: -1. genotype: -1. call_set_name: ""ID"". }. end: 21441513. reference_name: ""chr16"". start: 21441510. is [[0], [0], [0, 1], [0, 2], [1], [1, 2], [2]], which is invalid. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1143, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tens",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/341
https://github.com/google/deepvariant/issues/341:114,usability,error,error,114,"Hi @sgoe1,. Using the 1.0.0 image, the variant calling fails on nearly all of my input samples with the following error (sites vary from one sample to another):. ```. 2020-09-03 10:01:02.237494: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmpqosj7aak/call_variants_output.tfrecord.gz. 2020-09-03 10:01:02.575525: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 63099. I0903 10:01:02.988874 140046207305472 postprocess_variants.py:1079] CVO sorting took 0.012529869874318441 minutes. I0903 10:01:02.989492 140046207305472 postprocess_variants.py:1081] Transforming call_variants_output to variants. I0903 10:01:02.989910 140046207305472 postprocess_variants.py:1108] Merging and writing variants to VCF and gVCF. I0903 10:01:02.996278 140046207305472 genomics_writer.py:172] Writing /<REDACTED>/ID.2.vcf.gz with NativeVcfWriter. I0903 10:01:02.998338 140046207305472 genomics_writer.py:172] Writing /<REDACTED>/ID.2.g.vcf.gz with NativeVcfWriter. W0903 10:01:17.865604 140046207305472 postprocess_variants.py:394] Alt allele indices found from call_variants_outputs for variant reference_bases: ""CTT"". alternate_bases: ""CT"". alternate_bases: ""CTTT"". alternate_bases: ""CTTTTTTTCT"". calls {. info {. key: ""AD"". value {. values {. int_value: 7. }. values {. int_value: 7. }. values {. int_value: 0. }. values {. int_value: 0. }. }. }. info {. key: ""DP"". value {. values {. int_value: 19. }. }. }. info {. key: ""VAF"". value {. values {. number_value: 0.3684210526315789. }. values {. number_value: 0.0. }. values {. number_value: 0.0. }. }. }. genotype: -1. genotype: -1. call_set_name: ""ID"". }. end: 21441513. reference_name: ""chr16"". start: 21441510. is [[0], [0], [0, 1], [0, 2], [1], [1, 2], [2]], which is invalid. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1143, in <module>. tf.compat.v1.app.run(). File ""/usr/local/lib/python3.6/dist-packages/tens",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/341
https://github.com/google/deepvariant/issues/341:192,deployability,fail,failed,192,"Success :). Note that to make it work, I used `bcftools norm -m-any` on the VCF file of proposed variants (generated by GLnexus) to split multiallelic sites into multiple lines. Otherwise, it failed as previously reported. Thank you for the help!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/341
https://github.com/google/deepvariant/issues/341:192,reliability,fail,failed,192,"Success :). Note that to make it work, I used `bcftools norm -m-any` on the VCF file of proposed variants (generated by GLnexus) to split multiallelic sites into multiple lines. Otherwise, it failed as previously reported. Thank you for the help!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/341
https://github.com/google/deepvariant/issues/341:241,usability,help,help,241,"Success :). Note that to make it work, I used `bcftools norm -m-any` on the VCF file of proposed variants (generated by GLnexus) to split multiallelic sites into multiple lines. Otherwise, it failed as previously reported. Thank you for the help!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/341
https://github.com/google/deepvariant/issues/342:185,deployability,build,build,185,"Hi @jaydevshelat . Can you clarify - did you rebuild your docker image, or did you pull our 0.8 image? You mentioned . > Then built the docker using ""docker run ."". Did you mean docker build? If you're rebuilding your own, can you first confirm whether directly running our 0.8 image worked for you or not?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/342
https://github.com/google/deepvariant/issues/342:237,usability,confirm,confirm,237,"Hi @jaydevshelat . Can you clarify - did you rebuild your docker image, or did you pull our 0.8 image? You mentioned . > Then built the docker using ""docker run ."". Did you mean docker build? If you're rebuilding your own, can you first confirm whether directly running our 0.8 image worked for you or not?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/342
https://github.com/google/deepvariant/issues/342:102,usability,close,close,102,"Hi @jaydevshelat if you want to follow up on this question, feel free to open it again. For now, I'll close this issue due to inactivity.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/342
https://github.com/google/deepvariant/issues/342:85,availability,failur,failure,85,"Apologies for no/late response. And Thank You for following-up. . My issue was : the failure to do ""docker build"" after git clone of the repo. . However, I am successful in getting the 0.8 docker image --> install the SW pre-reqs to recompile binaries inside docker --> export to new image for future use. . This issue should be closed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/342
https://github.com/google/deepvariant/issues/342:85,deployability,fail,failure,85,"Apologies for no/late response. And Thank You for following-up. . My issue was : the failure to do ""docker build"" after git clone of the repo. . However, I am successful in getting the 0.8 docker image --> install the SW pre-reqs to recompile binaries inside docker --> export to new image for future use. . This issue should be closed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/342
https://github.com/google/deepvariant/issues/342:107,deployability,build,build,107,"Apologies for no/late response. And Thank You for following-up. . My issue was : the failure to do ""docker build"" after git clone of the repo. . However, I am successful in getting the 0.8 docker image --> install the SW pre-reqs to recompile binaries inside docker --> export to new image for future use. . This issue should be closed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/342
https://github.com/google/deepvariant/issues/342:206,deployability,instal,install,206,"Apologies for no/late response. And Thank You for following-up. . My issue was : the failure to do ""docker build"" after git clone of the repo. . However, I am successful in getting the 0.8 docker image --> install the SW pre-reqs to recompile binaries inside docker --> export to new image for future use. . This issue should be closed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/342
https://github.com/google/deepvariant/issues/342:85,performance,failur,failure,85,"Apologies for no/late response. And Thank You for following-up. . My issue was : the failure to do ""docker build"" after git clone of the repo. . However, I am successful in getting the 0.8 docker image --> install the SW pre-reqs to recompile binaries inside docker --> export to new image for future use. . This issue should be closed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/342
https://github.com/google/deepvariant/issues/342:85,reliability,fail,failure,85,"Apologies for no/late response. And Thank You for following-up. . My issue was : the failure to do ""docker build"" after git clone of the repo. . However, I am successful in getting the 0.8 docker image --> install the SW pre-reqs to recompile binaries inside docker --> export to new image for future use. . This issue should be closed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/342
https://github.com/google/deepvariant/issues/342:329,usability,close,closed,329,"Apologies for no/late response. And Thank You for following-up. . My issue was : the failure to do ""docker build"" after git clone of the repo. . However, I am successful in getting the 0.8 docker image --> install the SW pre-reqs to recompile binaries inside docker --> export to new image for future use. . This issue should be closed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/342
https://github.com/google/deepvariant/issues/342:26,deployability,updat,update,26,@Mipsology Thanks for the update. I'll keep this issue closed.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/342
https://github.com/google/deepvariant/issues/342:26,safety,updat,update,26,@Mipsology Thanks for the update. I'll keep this issue closed.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/342
https://github.com/google/deepvariant/issues/342:26,security,updat,update,26,@Mipsology Thanks for the update. I'll keep this issue closed.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/342
https://github.com/google/deepvariant/issues/342:55,usability,close,closed,55,@Mipsology Thanks for the update. I'll keep this issue closed.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/342
https://github.com/google/deepvariant/issues/343:823,deployability,pipelin,pipeline,823,"Hi @karthick1087. DeepVariant will be able to run on a BAM file for one microbial species relative to its reference genome and should be able to produce a VCF file with variants. However, DeepVariant is designed with diploid variant calling in mind, so it will call HET variants, and it also not trained with subclonal variant fractions in mind. In addition, the training data for DeepVariant is diploid human data. So it hasn't really been designed to address the problem of calling on haploid microorganisms. External groups have benchmarked DeepVariant for bacterial variant calling alongside many other callers designed for this purpose (https://academic.oup.com/gigascience/article/9/2/giaa007/5728470). Given the limitations to DeepVariant mentioned above, it performs surprisingly well, but is not the most accurate pipeline. . It does, however, seem to have a high precision (DeepVariant with NextGenMap had the highest precision of any of the methods), but with lower recall. So I would not recommend DeepVariant as the only method used to analyze microbial data. However, given its high precision and given that it is a very different approach to other methods you might run, it could be of use to use DeepVariant as a secondary caller. In this use case, you would have a high degree of confidence in variants called by both DeepVariant and the alternative method. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/343
https://github.com/google/deepvariant/issues/343:309,integrability,sub,subclonal,309,"Hi @karthick1087. DeepVariant will be able to run on a BAM file for one microbial species relative to its reference genome and should be able to produce a VCF file with variants. However, DeepVariant is designed with diploid variant calling in mind, so it will call HET variants, and it also not trained with subclonal variant fractions in mind. In addition, the training data for DeepVariant is diploid human data. So it hasn't really been designed to address the problem of calling on haploid microorganisms. External groups have benchmarked DeepVariant for bacterial variant calling alongside many other callers designed for this purpose (https://academic.oup.com/gigascience/article/9/2/giaa007/5728470). Given the limitations to DeepVariant mentioned above, it performs surprisingly well, but is not the most accurate pipeline. . It does, however, seem to have a high precision (DeepVariant with NextGenMap had the highest precision of any of the methods), but with lower recall. So I would not recommend DeepVariant as the only method used to analyze microbial data. However, given its high precision and given that it is a very different approach to other methods you might run, it could be of use to use DeepVariant as a secondary caller. In this use case, you would have a high degree of confidence in variants called by both DeepVariant and the alternative method. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/343
https://github.com/google/deepvariant/issues/343:823,integrability,pipelin,pipeline,823,"Hi @karthick1087. DeepVariant will be able to run on a BAM file for one microbial species relative to its reference genome and should be able to produce a VCF file with variants. However, DeepVariant is designed with diploid variant calling in mind, so it will call HET variants, and it also not trained with subclonal variant fractions in mind. In addition, the training data for DeepVariant is diploid human data. So it hasn't really been designed to address the problem of calling on haploid microorganisms. External groups have benchmarked DeepVariant for bacterial variant calling alongside many other callers designed for this purpose (https://academic.oup.com/gigascience/article/9/2/giaa007/5728470). Given the limitations to DeepVariant mentioned above, it performs surprisingly well, but is not the most accurate pipeline. . It does, however, seem to have a high precision (DeepVariant with NextGenMap had the highest precision of any of the methods), but with lower recall. So I would not recommend DeepVariant as the only method used to analyze microbial data. However, given its high precision and given that it is a very different approach to other methods you might run, it could be of use to use DeepVariant as a secondary caller. In this use case, you would have a high degree of confidence in variants called by both DeepVariant and the alternative method. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/343
https://github.com/google/deepvariant/issues/343:766,performance,perform,performs,766,"Hi @karthick1087. DeepVariant will be able to run on a BAM file for one microbial species relative to its reference genome and should be able to produce a VCF file with variants. However, DeepVariant is designed with diploid variant calling in mind, so it will call HET variants, and it also not trained with subclonal variant fractions in mind. In addition, the training data for DeepVariant is diploid human data. So it hasn't really been designed to address the problem of calling on haploid microorganisms. External groups have benchmarked DeepVariant for bacterial variant calling alongside many other callers designed for this purpose (https://academic.oup.com/gigascience/article/9/2/giaa007/5728470). Given the limitations to DeepVariant mentioned above, it performs surprisingly well, but is not the most accurate pipeline. . It does, however, seem to have a high precision (DeepVariant with NextGenMap had the highest precision of any of the methods), but with lower recall. So I would not recommend DeepVariant as the only method used to analyze microbial data. However, given its high precision and given that it is a very different approach to other methods you might run, it could be of use to use DeepVariant as a secondary caller. In this use case, you would have a high degree of confidence in variants called by both DeepVariant and the alternative method. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/343
https://github.com/google/deepvariant/issues/343:838,reliability,doe,does,838,"Hi @karthick1087. DeepVariant will be able to run on a BAM file for one microbial species relative to its reference genome and should be able to produce a VCF file with variants. However, DeepVariant is designed with diploid variant calling in mind, so it will call HET variants, and it also not trained with subclonal variant fractions in mind. In addition, the training data for DeepVariant is diploid human data. So it hasn't really been designed to address the problem of calling on haploid microorganisms. External groups have benchmarked DeepVariant for bacterial variant calling alongside many other callers designed for this purpose (https://academic.oup.com/gigascience/article/9/2/giaa007/5728470). Given the limitations to DeepVariant mentioned above, it performs surprisingly well, but is not the most accurate pipeline. . It does, however, seem to have a high precision (DeepVariant with NextGenMap had the highest precision of any of the methods), but with lower recall. So I would not recommend DeepVariant as the only method used to analyze microbial data. However, given its high precision and given that it is a very different approach to other methods you might run, it could be of use to use DeepVariant as a secondary caller. In this use case, you would have a high degree of confidence in variants called by both DeepVariant and the alternative method. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/343
https://github.com/google/deepvariant/issues/343:766,usability,perform,performs,766,"Hi @karthick1087. DeepVariant will be able to run on a BAM file for one microbial species relative to its reference genome and should be able to produce a VCF file with variants. However, DeepVariant is designed with diploid variant calling in mind, so it will call HET variants, and it also not trained with subclonal variant fractions in mind. In addition, the training data for DeepVariant is diploid human data. So it hasn't really been designed to address the problem of calling on haploid microorganisms. External groups have benchmarked DeepVariant for bacterial variant calling alongside many other callers designed for this purpose (https://academic.oup.com/gigascience/article/9/2/giaa007/5728470). Given the limitations to DeepVariant mentioned above, it performs surprisingly well, but is not the most accurate pipeline. . It does, however, seem to have a high precision (DeepVariant with NextGenMap had the highest precision of any of the methods), but with lower recall. So I would not recommend DeepVariant as the only method used to analyze microbial data. However, given its high precision and given that it is a very different approach to other methods you might run, it could be of use to use DeepVariant as a secondary caller. In this use case, you would have a high degree of confidence in variants called by both DeepVariant and the alternative method. Thanks,. Andrew.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/343
https://github.com/google/deepvariant/issues/344:95,deployability,build,build,95,"Hi @JakeHagen . Currently there isn't a very clean way to do this. You can modify the code and build DeepVariant from source: https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md. I'm personally interested in learning more about what you're trying to do - what is the expected input and output. If there's general enough use cases, maybe in the future we can make things easier to import, even though we don't currently have plans for that.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/344
https://github.com/google/deepvariant/issues/344:191,deployability,build,build-test,191,"Hi @JakeHagen . Currently there isn't a very clean way to do this. You can modify the code and build DeepVariant from source: https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md. I'm personally interested in learning more about what you're trying to do - what is the expected input and output. If there's general enough use cases, maybe in the future we can make things easier to import, even though we don't currently have plans for that.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/344
https://github.com/google/deepvariant/issues/344:16,energy efficiency,Current,Currently,16,"Hi @JakeHagen . Currently there isn't a very clean way to do this. You can modify the code and build DeepVariant from source: https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md. I'm personally interested in learning more about what you're trying to do - what is the expected input and output. If there's general enough use cases, maybe in the future we can make things easier to import, even though we don't currently have plans for that.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/344
https://github.com/google/deepvariant/issues/344:436,energy efficiency,current,currently,436,"Hi @JakeHagen . Currently there isn't a very clean way to do this. You can modify the code and build DeepVariant from source: https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md. I'm personally interested in learning more about what you're trying to do - what is the expected input and output. If there's general enough use cases, maybe in the future we can make things easier to import, even though we don't currently have plans for that.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/344
https://github.com/google/deepvariant/issues/344:197,safety,test,test,197,"Hi @JakeHagen . Currently there isn't a very clean way to do this. You can modify the code and build DeepVariant from source: https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md. I'm personally interested in learning more about what you're trying to do - what is the expected input and output. If there's general enough use cases, maybe in the future we can make things easier to import, even though we don't currently have plans for that.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/344
https://github.com/google/deepvariant/issues/344:303,safety,input,input,303,"Hi @JakeHagen . Currently there isn't a very clean way to do this. You can modify the code and build DeepVariant from source: https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md. I'm personally interested in learning more about what you're trying to do - what is the expected input and output. If there's general enough use cases, maybe in the future we can make things easier to import, even though we don't currently have plans for that.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/344
https://github.com/google/deepvariant/issues/344:75,security,modif,modify,75,"Hi @JakeHagen . Currently there isn't a very clean way to do this. You can modify the code and build DeepVariant from source: https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md. I'm personally interested in learning more about what you're trying to do - what is the expected input and output. If there's general enough use cases, maybe in the future we can make things easier to import, even though we don't currently have plans for that.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/344
https://github.com/google/deepvariant/issues/344:197,testability,test,test,197,"Hi @JakeHagen . Currently there isn't a very clean way to do this. You can modify the code and build DeepVariant from source: https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md. I'm personally interested in learning more about what you're trying to do - what is the expected input and output. If there's general enough use cases, maybe in the future we can make things easier to import, even though we don't currently have plans for that.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/344
https://github.com/google/deepvariant/issues/344:451,testability,plan,plans,451,"Hi @JakeHagen . Currently there isn't a very clean way to do this. You can modify the code and build DeepVariant from source: https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md. I'm personally interested in learning more about what you're trying to do - what is the expected input and output. If there's general enough use cases, maybe in the future we can make things easier to import, even though we don't currently have plans for that.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/344
https://github.com/google/deepvariant/issues/344:210,usability,person,personally,210,"Hi @JakeHagen . Currently there isn't a very clean way to do this. You can modify the code and build DeepVariant from source: https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md. I'm personally interested in learning more about what you're trying to do - what is the expected input and output. If there's general enough use cases, maybe in the future we can make things easier to import, even though we don't currently have plans for that.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/344
https://github.com/google/deepvariant/issues/344:235,usability,learn,learning,235,"Hi @JakeHagen . Currently there isn't a very clean way to do this. You can modify the code and build DeepVariant from source: https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md. I'm personally interested in learning more about what you're trying to do - what is the expected input and output. If there's general enough use cases, maybe in the future we can make things easier to import, even though we don't currently have plans for that.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/344
https://github.com/google/deepvariant/issues/344:303,usability,input,input,303,"Hi @JakeHagen . Currently there isn't a very clean way to do this. You can modify the code and build DeepVariant from source: https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md. I'm personally interested in learning more about what you're trying to do - what is the expected input and output. If there's general enough use cases, maybe in the future we can make things easier to import, even though we don't currently have plans for that.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/344
https://github.com/google/deepvariant/issues/344:311,deployability,build,building,311,I am doing some work that needs read modification based on variants. These variants come from deepvariant so I was hoping to use the realigner to modify the reads as deepvariant sees them. I often struggle with python imports so I thought I was just missing something. I will look into extracting that code and building. Thank you,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/344
https://github.com/google/deepvariant/issues/344:37,security,modif,modification,37,I am doing some work that needs read modification based on variants. These variants come from deepvariant so I was hoping to use the realigner to modify the reads as deepvariant sees them. I often struggle with python imports so I thought I was just missing something. I will look into extracting that code and building. Thank you,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/344
https://github.com/google/deepvariant/issues/344:146,security,modif,modify,146,I am doing some work that needs read modification based on variants. These variants come from deepvariant so I was hoping to use the realigner to modify the reads as deepvariant sees them. I often struggle with python imports so I thought I was just missing something. I will look into extracting that code and building. Thank you,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/344
https://github.com/google/deepvariant/issues/345:34,availability,error,error,34,"Hi @sh940202123 ,. Sorry that our error messages are still quite cryptic at this time. The last traceback is actually not quite as useful. You're seeing it because we uses Python's subprocess.check_call to call each of the commands:. https://github.com/google/deepvariant/blob/7ed651ed54d8563530d58ba80187121337fd10f2/scripts/run_deepvariant.py#L362. Usually, I hope that whatever appeared right before that is useful, though. For example, if I deliberately deleted the bam file, I got:. ```. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. [E::hts_open_format] Failed to open file ""/input/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2136, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2063, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 427, in default_options. with sam.SamReader(flags_obj.reads.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_1i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:4043,availability,error,error,4043,"20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m3.261s. user 0m5.770s. sys 0m5.974s. I0910 00:06:33.584306 140309422569216 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 1. ```. However, in your case, it seems like there's nothing useful in the log. I think I've heard about similar cases before (where there was no useful error message showing up). I previously wondered if it's possible that `parallel` somehow hid away some message. But I can't immediately think of a reproducible setting that I can debug. But speaking of ""returned non-zero exit status 252"", I did a quick search in our GitHub issues and found: https://github.com/google/deepvariant/issues/325. Can you see if this might be relevant to your issue? If so, I'd still like to find a way to reproduce on my side, so I can see if we can make a more meaningful error message... 🤔",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:4546,availability,error,error,4546,"20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m3.261s. user 0m5.770s. sys 0m5.974s. I0910 00:06:33.584306 140309422569216 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 1. ```. However, in your case, it seems like there's nothing useful in the log. I think I've heard about similar cases before (where there was no useful error message showing up). I previously wondered if it's possible that `parallel` somehow hid away some message. But I can't immediately think of a reproducible setting that I can debug. But speaking of ""returned non-zero exit status 252"", I did a quick search in our GitHub issues and found: https://github.com/google/deepvariant/issues/325. Can you see if this might be relevant to your issue? If so, I'd still like to find a way to reproduce on my side, so I can see if we can make a more meaningful error message... 🤔",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:927,deployability,Fail,Failed,927,"Hi @sh940202123 ,. Sorry that our error messages are still quite cryptic at this time. The last traceback is actually not quite as useful. You're seeing it because we uses Python's subprocess.check_call to call each of the commands:. https://github.com/google/deepvariant/blob/7ed651ed54d8563530d58ba80187121337fd10f2/scripts/run_deepvariant.py#L362. Usually, I hope that whatever appeared right before that is useful, though. For example, if I deliberately deleted the bam file, I got:. ```. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. [E::hts_open_format] Failed to open file ""/input/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2136, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2063, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 427, in default_options. with sam.SamReader(flags_obj.reads.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_1i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:1165,deployability,modul,module,1165,"ses Python's subprocess.check_call to call each of the commands:. https://github.com/google/deepvariant/blob/7ed651ed54d8563530d58ba80187121337fd10f2/scripts/run_deepvariant.py#L362. Usually, I hope that whatever appeared right before that is useful, though. For example, if I deliberately deleted the bam file, I got:. ```. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. [E::hts_open_format] Failed to open file ""/input/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2136, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2063, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 427, in default_options. with sam.SamReader(flags_obj.reads.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/tmp/Bazel.runfi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:2432,deployability,fail,failed,2432,"ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2063, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 427, in default_options. with sam.SamReader(flags_obj.reads.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 232, in __init__. use_original_base_quality_scores=use_original_base_quality_scores). ValueError: Not found: Could not open /input/NA12878_S1.chr20.10_10p1mb.bam. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m3.261s. user 0m5.770s. sys 0m5.974s. I0910 00:06:33.584306 140309422569216 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:2970,deployability,modul,module,2970,"ile ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 232, in __init__. use_original_base_quality_scores=use_original_base_quality_scores). ValueError: Not found: Could not open /input/NA12878_S1.chr20.10_10p1mb.bam. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m3.261s. user 0m5.770s. sys 0m5.974s. I0910 00:06:33.584306 140309422569216 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 1. ```. However, in your case, it seems like there's nothing useful in the log. I t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:3965,deployability,log,log,3965,"20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m3.261s. user 0m5.770s. sys 0m5.974s. I0910 00:06:33.584306 140309422569216 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 1. ```. However, in your case, it seems like there's nothing useful in the log. I think I've heard about similar cases before (where there was no useful error message showing up). I previously wondered if it's possible that `parallel` somehow hid away some message. But I can't immediately think of a reproducible setting that I can debug. But speaking of ""returned non-zero exit status 252"", I did a quick search in our GitHub issues and found: https://github.com/google/deepvariant/issues/325. Can you see if this might be relevant to your issue? If so, I'd still like to find a way to reproduce on my side, so I can see if we can make a more meaningful error message... 🤔",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:40,integrability,messag,messages,40,"Hi @sh940202123 ,. Sorry that our error messages are still quite cryptic at this time. The last traceback is actually not quite as useful. You're seeing it because we uses Python's subprocess.check_call to call each of the commands:. https://github.com/google/deepvariant/blob/7ed651ed54d8563530d58ba80187121337fd10f2/scripts/run_deepvariant.py#L362. Usually, I hope that whatever appeared right before that is useful, though. For example, if I deliberately deleted the bam file, I got:. ```. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. [E::hts_open_format] Failed to open file ""/input/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2136, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2063, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 427, in default_options. with sam.SamReader(flags_obj.reads.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_1i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:181,integrability,sub,subprocess,181,"Hi @sh940202123 ,. Sorry that our error messages are still quite cryptic at this time. The last traceback is actually not quite as useful. You're seeing it because we uses Python's subprocess.check_call to call each of the commands:. https://github.com/google/deepvariant/blob/7ed651ed54d8563530d58ba80187121337fd10f2/scripts/run_deepvariant.py#L362. Usually, I hope that whatever appeared right before that is useful, though. For example, if I deliberately deleted the bam file, I got:. ```. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. [E::hts_open_format] Failed to open file ""/input/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2136, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2063, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 427, in default_options. with sam.SamReader(flags_obj.reads.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_1i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:569,integrability,buffer,buffer,569,"Hi @sh940202123 ,. Sorry that our error messages are still quite cryptic at this time. The last traceback is actually not quite as useful. You're seeing it because we uses Python's subprocess.check_call to call each of the commands:. https://github.com/google/deepvariant/blob/7ed651ed54d8563530d58ba80187121337fd10f2/scripts/run_deepvariant.py#L362. Usually, I hope that whatever appeared right before that is useful, though. For example, if I deliberately deleted the bam file, I got:. ```. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. [E::hts_open_format] Failed to open file ""/input/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2136, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2063, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 427, in default_options. with sam.SamReader(flags_obj.reads.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_1i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:3266,integrability,sub,subprocess,3266,"_original_base_quality_scores=use_original_base_quality_scores). ValueError: Not found: Could not open /input/NA12878_S1.chr20.10_10p1mb.bam. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m3.261s. user 0m5.770s. sys 0m5.974s. I0910 00:06:33.584306 140309422569216 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 1. ```. However, in your case, it seems like there's nothing useful in the log. I think I've heard about similar cases before (where there was no useful error message showing up). I previously wondered if it's possible that `parallel` somehow hid away some message. But I can't immediately think of a reproducible setting that I can debug. But speaking of ""returned non-zero exit s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:3359,integrability,sub,subprocess,3359," not open /input/NA12878_S1.chr20.10_10p1mb.bam. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m3.261s. user 0m5.770s. sys 0m5.974s. I0910 00:06:33.584306 140309422569216 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 1. ```. However, in your case, it seems like there's nothing useful in the log. I think I've heard about similar cases before (where there was no useful error message showing up). I previously wondered if it's possible that `parallel` somehow hid away some message. But I can't immediately think of a reproducible setting that I can debug. But speaking of ""returned non-zero exit status 252"", I did a quick search in our GitHub issues and found: https://github.com/google/de",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:3440,integrability,sub,subprocess,3440,"/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m3.261s. user 0m5.770s. sys 0m5.974s. I0910 00:06:33.584306 140309422569216 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 1. ```. However, in your case, it seems like there's nothing useful in the log. I think I've heard about similar cases before (where there was no useful error message showing up). I previously wondered if it's possible that `parallel` somehow hid away some message. But I can't immediately think of a reproducible setting that I can debug. But speaking of ""returned non-zero exit status 252"", I did a quick search in our GitHub issues and found: https://github.com/google/deepvariant/issues/325. Can you see if this might be relevant to your issue? If so,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:3523,integrability,buffer,buffer,3523,"st.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m3.261s. user 0m5.770s. sys 0m5.974s. I0910 00:06:33.584306 140309422569216 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 1. ```. However, in your case, it seems like there's nothing useful in the log. I think I've heard about similar cases before (where there was no useful error message showing up). I previously wondered if it's possible that `parallel` somehow hid away some message. But I can't immediately think of a reproducible setting that I can debug. But speaking of ""returned non-zero exit status 252"", I did a quick search in our GitHub issues and found: https://github.com/google/deepvariant/issues/325. Can you see if this might be relevant to your issue? If so, I'd still like to find a way to reproduce on my side, so I can see if we can mak",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:4049,integrability,messag,message,4049,"20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m3.261s. user 0m5.770s. sys 0m5.974s. I0910 00:06:33.584306 140309422569216 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 1. ```. However, in your case, it seems like there's nothing useful in the log. I think I've heard about similar cases before (where there was no useful error message showing up). I previously wondered if it's possible that `parallel` somehow hid away some message. But I can't immediately think of a reproducible setting that I can debug. But speaking of ""returned non-zero exit status 252"", I did a quick search in our GitHub issues and found: https://github.com/google/deepvariant/issues/325. Can you see if this might be relevant to your issue? If so, I'd still like to find a way to reproduce on my side, so I can see if we can make a more meaningful error message... 🤔",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:4147,integrability,messag,message,4147,"20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m3.261s. user 0m5.770s. sys 0m5.974s. I0910 00:06:33.584306 140309422569216 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 1. ```. However, in your case, it seems like there's nothing useful in the log. I think I've heard about similar cases before (where there was no useful error message showing up). I previously wondered if it's possible that `parallel` somehow hid away some message. But I can't immediately think of a reproducible setting that I can debug. But speaking of ""returned non-zero exit status 252"", I did a quick search in our GitHub issues and found: https://github.com/google/deepvariant/issues/325. Can you see if this might be relevant to your issue? If so, I'd still like to find a way to reproduce on my side, so I can see if we can make a more meaningful error message... 🤔",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:4552,integrability,messag,message,4552,"20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m3.261s. user 0m5.770s. sys 0m5.974s. I0910 00:06:33.584306 140309422569216 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 1. ```. However, in your case, it seems like there's nothing useful in the log. I think I've heard about similar cases before (where there was no useful error message showing up). I previously wondered if it's possible that `parallel` somehow hid away some message. But I can't immediately think of a reproducible setting that I can debug. But speaking of ""returned non-zero exit status 252"", I did a quick search in our GitHub issues and found: https://github.com/google/deepvariant/issues/325. Can you see if this might be relevant to your issue? If so, I'd still like to find a way to reproduce on my side, so I can see if we can make a more meaningful error message... 🤔",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:40,interoperability,messag,messages,40,"Hi @sh940202123 ,. Sorry that our error messages are still quite cryptic at this time. The last traceback is actually not quite as useful. You're seeing it because we uses Python's subprocess.check_call to call each of the commands:. https://github.com/google/deepvariant/blob/7ed651ed54d8563530d58ba80187121337fd10f2/scripts/run_deepvariant.py#L362. Usually, I hope that whatever appeared right before that is useful, though. For example, if I deliberately deleted the bam file, I got:. ```. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. [E::hts_open_format] Failed to open file ""/input/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2136, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2063, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 427, in default_options. with sam.SamReader(flags_obj.reads.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_1i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:4049,interoperability,messag,message,4049,"20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m3.261s. user 0m5.770s. sys 0m5.974s. I0910 00:06:33.584306 140309422569216 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 1. ```. However, in your case, it seems like there's nothing useful in the log. I think I've heard about similar cases before (where there was no useful error message showing up). I previously wondered if it's possible that `parallel` somehow hid away some message. But I can't immediately think of a reproducible setting that I can debug. But speaking of ""returned non-zero exit status 252"", I did a quick search in our GitHub issues and found: https://github.com/google/deepvariant/issues/325. Can you see if this might be relevant to your issue? If so, I'd still like to find a way to reproduce on my side, so I can see if we can make a more meaningful error message... 🤔",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:4147,interoperability,messag,message,4147,"20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m3.261s. user 0m5.770s. sys 0m5.974s. I0910 00:06:33.584306 140309422569216 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 1. ```. However, in your case, it seems like there's nothing useful in the log. I think I've heard about similar cases before (where there was no useful error message showing up). I previously wondered if it's possible that `parallel` somehow hid away some message. But I can't immediately think of a reproducible setting that I can debug. But speaking of ""returned non-zero exit status 252"", I did a quick search in our GitHub issues and found: https://github.com/google/deepvariant/issues/325. Can you see if this might be relevant to your issue? If so, I'd still like to find a way to reproduce on my side, so I can see if we can make a more meaningful error message... 🤔",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:4552,interoperability,messag,message,4552,"20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m3.261s. user 0m5.770s. sys 0m5.974s. I0910 00:06:33.584306 140309422569216 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 1. ```. However, in your case, it seems like there's nothing useful in the log. I think I've heard about similar cases before (where there was no useful error message showing up). I previously wondered if it's possible that `parallel` somehow hid away some message. But I can't immediately think of a reproducible setting that I can debug. But speaking of ""returned non-zero exit status 252"", I did a quick search in our GitHub issues and found: https://github.com/google/deepvariant/issues/325. Can you see if this might be relevant to your issue? If so, I'd still like to find a way to reproduce on my side, so I can see if we can make a more meaningful error message... 🤔",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:1165,modifiability,modul,module,1165,"ses Python's subprocess.check_call to call each of the commands:. https://github.com/google/deepvariant/blob/7ed651ed54d8563530d58ba80187121337fd10f2/scripts/run_deepvariant.py#L362. Usually, I hope that whatever appeared right before that is useful, though. For example, if I deliberately deleted the bam file, I got:. ```. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. [E::hts_open_format] Failed to open file ""/input/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2136, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2063, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 427, in default_options. with sam.SamReader(flags_obj.reads.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/tmp/Bazel.runfi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:2970,modifiability,modul,module,2970,"ile ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 232, in __init__. use_original_base_quality_scores=use_original_base_quality_scores). ValueError: Not found: Could not open /input/NA12878_S1.chr20.10_10p1mb.bam. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m3.261s. user 0m5.770s. sys 0m5.974s. I0910 00:06:33.584306 140309422569216 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 1. ```. However, in your case, it seems like there's nothing useful in the log. I t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:3030,modifiability,pac,packages,3030,"iant/third_party/nucleus/io/sam.py"", line 255, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 232, in __init__. use_original_base_quality_scores=use_original_base_quality_scores). ValueError: Not found: Could not open /input/NA12878_S1.chr20.10_10p1mb.bam. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m3.261s. user 0m5.770s. sys 0m5.974s. I0910 00:06:33.584306 140309422569216 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 1. ```. However, in your case, it seems like there's nothing useful in the log. I think I've heard about similar cases before (where there was n",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:3130,modifiability,pac,packages,3130," **kwargs). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 232, in __init__. use_original_base_quality_scores=use_original_base_quality_scores). ValueError: Not found: Could not open /input/NA12878_S1.chr20.10_10p1mb.bam. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m3.261s. user 0m5.770s. sys 0m5.974s. I0910 00:06:33.584306 140309422569216 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 1. ```. However, in your case, it seems like there's nothing useful in the log. I think I've heard about similar cases before (where there was no useful error message showing up). I previously wondered if it's possible that `parallel` somehow h",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:34,performance,error,error,34,"Hi @sh940202123 ,. Sorry that our error messages are still quite cryptic at this time. The last traceback is actually not quite as useful. You're seeing it because we uses Python's subprocess.check_call to call each of the commands:. https://github.com/google/deepvariant/blob/7ed651ed54d8563530d58ba80187121337fd10f2/scripts/run_deepvariant.py#L362. Usually, I hope that whatever appeared right before that is useful, though. For example, if I deliberately deleted the bam file, I got:. ```. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. [E::hts_open_format] Failed to open file ""/input/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2136, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2063, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 427, in default_options. with sam.SamReader(flags_obj.reads.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_1i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:81,performance,time,time,81,"Hi @sh940202123 ,. Sorry that our error messages are still quite cryptic at this time. The last traceback is actually not quite as useful. You're seeing it because we uses Python's subprocess.check_call to call each of the commands:. https://github.com/google/deepvariant/blob/7ed651ed54d8563530d58ba80187121337fd10f2/scripts/run_deepvariant.py#L362. Usually, I hope that whatever appeared right before that is useful, though. For example, if I deliberately deleted the bam file, I got:. ```. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. [E::hts_open_format] Failed to open file ""/input/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2136, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2063, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 427, in default_options. with sam.SamReader(flags_obj.reads.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_1i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:526,performance,time,time,526,"Hi @sh940202123 ,. Sorry that our error messages are still quite cryptic at this time. The last traceback is actually not quite as useful. You're seeing it because we uses Python's subprocess.check_call to call each of the commands:. https://github.com/google/deepvariant/blob/7ed651ed54d8563530d58ba80187121337fd10f2/scripts/run_deepvariant.py#L362. Usually, I hope that whatever appeared right before that is useful, though. For example, if I deliberately deleted the bam file, I got:. ```. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. [E::hts_open_format] Failed to open file ""/input/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2136, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2063, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 427, in default_options. with sam.SamReader(flags_obj.reads.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_1i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:541,performance,parallel,parallel,541,"Hi @sh940202123 ,. Sorry that our error messages are still quite cryptic at this time. The last traceback is actually not quite as useful. You're seeing it because we uses Python's subprocess.check_call to call each of the commands:. https://github.com/google/deepvariant/blob/7ed651ed54d8563530d58ba80187121337fd10f2/scripts/run_deepvariant.py#L362. Usually, I hope that whatever appeared right before that is useful, though. For example, if I deliberately deleted the bam file, I got:. ```. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. [E::hts_open_format] Failed to open file ""/input/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2136, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2063, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 427, in default_options. with sam.SamReader(flags_obj.reads.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_1i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:2413,performance,parallel,parallel,2413,"p/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2063, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 427, in default_options. with sam.SamReader(flags_obj.reads.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 232, in __init__. use_original_base_quality_scores=use_original_base_quality_scores). ValueError: Not found: Could not open /input/NA12878_S1.chr20.10_10p1mb.bam. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m3.261s. user 0m5.770s. sys 0m5.974s. I0910 00:06:33.584306 140309422569216 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProce",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:3480,performance,time,time,3480," calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m3.261s. user 0m5.770s. sys 0m5.974s. I0910 00:06:33.584306 140309422569216 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 1. ```. However, in your case, it seems like there's nothing useful in the log. I think I've heard about similar cases before (where there was no useful error message showing up). I previously wondered if it's possible that `parallel` somehow hid away some message. But I can't immediately think of a reproducible setting that I can debug. But speaking of ""returned non-zero exit status 252"", I did a quick search in our GitHub issues and found: https://github.com/google/deepvariant/issues/325. Can you see if this might be relevant to your issue? If so, I'd still like to find a way to repr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:3495,performance,parallel,parallel,3495,"nput/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m3.261s. user 0m5.770s. sys 0m5.974s. I0910 00:06:33.584306 140309422569216 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 1. ```. However, in your case, it seems like there's nothing useful in the log. I think I've heard about similar cases before (where there was no useful error message showing up). I previously wondered if it's possible that `parallel` somehow hid away some message. But I can't immediately think of a reproducible setting that I can debug. But speaking of ""returned non-zero exit status 252"", I did a quick search in our GitHub issues and found: https://github.com/google/deepvariant/issues/325. Can you see if this might be relevant to your issue? If so, I'd still like to find a way to reproduce on my side,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:4043,performance,error,error,4043,"20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m3.261s. user 0m5.770s. sys 0m5.974s. I0910 00:06:33.584306 140309422569216 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 1. ```. However, in your case, it seems like there's nothing useful in the log. I think I've heard about similar cases before (where there was no useful error message showing up). I previously wondered if it's possible that `parallel` somehow hid away some message. But I can't immediately think of a reproducible setting that I can debug. But speaking of ""returned non-zero exit status 252"", I did a quick search in our GitHub issues and found: https://github.com/google/deepvariant/issues/325. Can you see if this might be relevant to your issue? If so, I'd still like to find a way to reproduce on my side, so I can see if we can make a more meaningful error message... 🤔",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:4115,performance,parallel,parallel,4115,"20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m3.261s. user 0m5.770s. sys 0m5.974s. I0910 00:06:33.584306 140309422569216 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 1. ```. However, in your case, it seems like there's nothing useful in the log. I think I've heard about similar cases before (where there was no useful error message showing up). I previously wondered if it's possible that `parallel` somehow hid away some message. But I can't immediately think of a reproducible setting that I can debug. But speaking of ""returned non-zero exit status 252"", I did a quick search in our GitHub issues and found: https://github.com/google/deepvariant/issues/325. Can you see if this might be relevant to your issue? If so, I'd still like to find a way to reproduce on my side, so I can see if we can make a more meaningful error message... 🤔",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:4546,performance,error,error,4546,"20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m3.261s. user 0m5.770s. sys 0m5.974s. I0910 00:06:33.584306 140309422569216 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 1. ```. However, in your case, it seems like there's nothing useful in the log. I think I've heard about similar cases before (where there was no useful error message showing up). I previously wondered if it's possible that `parallel` somehow hid away some message. But I can't immediately think of a reproducible setting that I can debug. But speaking of ""returned non-zero exit status 252"", I did a quick search in our GitHub issues and found: https://github.com/google/deepvariant/issues/325. Can you see if this might be relevant to your issue? If so, I'd still like to find a way to reproduce on my side, so I can see if we can make a more meaningful error message... 🤔",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:927,reliability,Fail,Failed,927,"Hi @sh940202123 ,. Sorry that our error messages are still quite cryptic at this time. The last traceback is actually not quite as useful. You're seeing it because we uses Python's subprocess.check_call to call each of the commands:. https://github.com/google/deepvariant/blob/7ed651ed54d8563530d58ba80187121337fd10f2/scripts/run_deepvariant.py#L362. Usually, I hope that whatever appeared right before that is useful, though. For example, if I deliberately deleted the bam file, I got:. ```. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. [E::hts_open_format] Failed to open file ""/input/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2136, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2063, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 427, in default_options. with sam.SamReader(flags_obj.reads.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_1i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:2432,reliability,fail,failed,2432,"ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2063, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 427, in default_options. with sam.SamReader(flags_obj.reads.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 232, in __init__. use_original_base_quality_scores=use_original_base_quality_scores). ValueError: Not found: Could not open /input/NA12878_S1.chr20.10_10p1mb.bam. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m3.261s. user 0m5.770s. sys 0m5.974s. I0910 00:06:33.584306 140309422569216 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:34,safety,error,error,34,"Hi @sh940202123 ,. Sorry that our error messages are still quite cryptic at this time. The last traceback is actually not quite as useful. You're seeing it because we uses Python's subprocess.check_call to call each of the commands:. https://github.com/google/deepvariant/blob/7ed651ed54d8563530d58ba80187121337fd10f2/scripts/run_deepvariant.py#L362. Usually, I hope that whatever appeared right before that is useful, though. For example, if I deliberately deleted the bam file, I got:. ```. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. [E::hts_open_format] Failed to open file ""/input/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2136, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2063, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 427, in default_options. with sam.SamReader(flags_obj.reads.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_1i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:634,safety,input,input,634,"Hi @sh940202123 ,. Sorry that our error messages are still quite cryptic at this time. The last traceback is actually not quite as useful. You're seeing it because we uses Python's subprocess.check_call to call each of the commands:. https://github.com/google/deepvariant/blob/7ed651ed54d8563530d58ba80187121337fd10f2/scripts/run_deepvariant.py#L362. Usually, I hope that whatever appeared right before that is useful, though. For example, if I deliberately deleted the bam file, I got:. ```. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. [E::hts_open_format] Failed to open file ""/input/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2136, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2063, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 427, in default_options. with sam.SamReader(flags_obj.reads.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_1i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:682,safety,input,input,682,"Hi @sh940202123 ,. Sorry that our error messages are still quite cryptic at this time. The last traceback is actually not quite as useful. You're seeing it because we uses Python's subprocess.check_call to call each of the commands:. https://github.com/google/deepvariant/blob/7ed651ed54d8563530d58ba80187121337fd10f2/scripts/run_deepvariant.py#L362. Usually, I hope that whatever appeared right before that is useful, though. For example, if I deliberately deleted the bam file, I got:. ```. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. [E::hts_open_format] Failed to open file ""/input/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2136, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2063, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 427, in default_options. with sam.SamReader(flags_obj.reads.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_1i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:949,safety,input,input,949,"Hi @sh940202123 ,. Sorry that our error messages are still quite cryptic at this time. The last traceback is actually not quite as useful. You're seeing it because we uses Python's subprocess.check_call to call each of the commands:. https://github.com/google/deepvariant/blob/7ed651ed54d8563530d58ba80187121337fd10f2/scripts/run_deepvariant.py#L362. Usually, I hope that whatever appeared right before that is useful, though. For example, if I deliberately deleted the bam file, I got:. ```. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. [E::hts_open_format] Failed to open file ""/input/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2136, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2063, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 427, in default_options. with sam.SamReader(flags_obj.reads.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_1i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:1165,safety,modul,module,1165,"ses Python's subprocess.check_call to call each of the commands:. https://github.com/google/deepvariant/blob/7ed651ed54d8563530d58ba80187121337fd10f2/scripts/run_deepvariant.py#L362. Usually, I hope that whatever appeared right before that is useful, though. For example, if I deliberately deleted the bam file, I got:. ```. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. [E::hts_open_format] Failed to open file ""/input/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2136, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2063, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 427, in default_options. with sam.SamReader(flags_obj.reads.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/tmp/Bazel.runfi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:2375,safety,input,input,2375,"n_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2063, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 427, in default_options. with sam.SamReader(flags_obj.reads.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 232, in __init__. use_original_base_quality_scores=use_original_base_quality_scores). ValueError: Not found: Could not open /input/NA12878_S1.chr20.10_10p1mb.bam. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m3.261s. user 0m5.770s. sys 0m5.974s. I0910 00:06:33.584306 140309422569216 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", lin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:2498,safety,input,input,2498,"py"", line 2063, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 427, in default_options. with sam.SamReader(flags_obj.reads.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 232, in __init__. use_original_base_quality_scores=use_original_base_quality_scores). ValueError: Not found: Could not open /input/NA12878_S1.chr20.10_10p1mb.bam. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m3.261s. user 0m5.770s. sys 0m5.974s. I0910 00:06:33.584306 140309422569216 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parall",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:2544,safety,input,input,2544,"ions(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 427, in default_options. with sam.SamReader(flags_obj.reads.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 232, in __init__. use_original_base_quality_scores=use_original_base_quality_scores). ValueError: Not found: Could not open /input/NA12878_S1.chr20.10_10p1mb.bam. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m3.261s. user 0m5.770s. sys 0m5.974s. I0910 00:06:33.584306 140309422569216 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:2970,safety,modul,module,2970,"ile ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 232, in __init__. use_original_base_quality_scores=use_original_base_quality_scores). ValueError: Not found: Could not open /input/NA12878_S1.chr20.10_10p1mb.bam. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m3.261s. user 0m5.770s. sys 0m5.974s. I0910 00:06:33.584306 140309422569216 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 1. ```. However, in your case, it seems like there's nothing useful in the log. I t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:3588,safety,input,input,3588,"20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m3.261s. user 0m5.770s. sys 0m5.974s. I0910 00:06:33.584306 140309422569216 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 1. ```. However, in your case, it seems like there's nothing useful in the log. I think I've heard about similar cases before (where there was no useful error message showing up). I previously wondered if it's possible that `parallel` somehow hid away some message. But I can't immediately think of a reproducible setting that I can debug. But speaking of ""returned non-zero exit status 252"", I did a quick search in our GitHub issues and found: https://github.com/google/deepvariant/issues/325. Can you see if this might be relevant to your issue? If so, I'd still like to find a way to reproduce on my side, so I can see if we can make a more meaningful error message... 🤔",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:3636,safety,input,input,3636,"20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m3.261s. user 0m5.770s. sys 0m5.974s. I0910 00:06:33.584306 140309422569216 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 1. ```. However, in your case, it seems like there's nothing useful in the log. I think I've heard about similar cases before (where there was no useful error message showing up). I previously wondered if it's possible that `parallel` somehow hid away some message. But I can't immediately think of a reproducible setting that I can debug. But speaking of ""returned non-zero exit status 252"", I did a quick search in our GitHub issues and found: https://github.com/google/deepvariant/issues/325. Can you see if this might be relevant to your issue? If so, I'd still like to find a way to reproduce on my side, so I can see if we can make a more meaningful error message... 🤔",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:3965,safety,log,log,3965,"20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m3.261s. user 0m5.770s. sys 0m5.974s. I0910 00:06:33.584306 140309422569216 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 1. ```. However, in your case, it seems like there's nothing useful in the log. I think I've heard about similar cases before (where there was no useful error message showing up). I previously wondered if it's possible that `parallel` somehow hid away some message. But I can't immediately think of a reproducible setting that I can debug. But speaking of ""returned non-zero exit status 252"", I did a quick search in our GitHub issues and found: https://github.com/google/deepvariant/issues/325. Can you see if this might be relevant to your issue? If so, I'd still like to find a way to reproduce on my side, so I can see if we can make a more meaningful error message... 🤔",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:4043,safety,error,error,4043,"20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m3.261s. user 0m5.770s. sys 0m5.974s. I0910 00:06:33.584306 140309422569216 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 1. ```. However, in your case, it seems like there's nothing useful in the log. I think I've heard about similar cases before (where there was no useful error message showing up). I previously wondered if it's possible that `parallel` somehow hid away some message. But I can't immediately think of a reproducible setting that I can debug. But speaking of ""returned non-zero exit status 252"", I did a quick search in our GitHub issues and found: https://github.com/google/deepvariant/issues/325. Can you see if this might be relevant to your issue? If so, I'd still like to find a way to reproduce on my side, so I can see if we can make a more meaningful error message... 🤔",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:4546,safety,error,error,4546,"20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m3.261s. user 0m5.770s. sys 0m5.974s. I0910 00:06:33.584306 140309422569216 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 1. ```. However, in your case, it seems like there's nothing useful in the log. I think I've heard about similar cases before (where there was no useful error message showing up). I previously wondered if it's possible that `parallel` somehow hid away some message. But I can't immediately think of a reproducible setting that I can debug. But speaking of ""returned non-zero exit status 252"", I did a quick search in our GitHub issues and found: https://github.com/google/deepvariant/issues/325. Can you see if this might be relevant to your issue? If so, I'd still like to find a way to reproduce on my side, so I can see if we can make a more meaningful error message... 🤔",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:3965,security,log,log,3965,"20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m3.261s. user 0m5.770s. sys 0m5.974s. I0910 00:06:33.584306 140309422569216 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 1. ```. However, in your case, it seems like there's nothing useful in the log. I think I've heard about similar cases before (where there was no useful error message showing up). I previously wondered if it's possible that `parallel` somehow hid away some message. But I can't immediately think of a reproducible setting that I can debug. But speaking of ""returned non-zero exit status 252"", I did a quick search in our GitHub issues and found: https://github.com/google/deepvariant/issues/325. Can you see if this might be relevant to your issue? If so, I'd still like to find a way to reproduce on my side, so I can see if we can make a more meaningful error message... 🤔",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:96,testability,trace,traceback,96,"Hi @sh940202123 ,. Sorry that our error messages are still quite cryptic at this time. The last traceback is actually not quite as useful. You're seeing it because we uses Python's subprocess.check_call to call each of the commands:. https://github.com/google/deepvariant/blob/7ed651ed54d8563530d58ba80187121337fd10f2/scripts/run_deepvariant.py#L362. Usually, I hope that whatever appeared right before that is useful, though. For example, if I deliberately deleted the bam file, I got:. ```. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. [E::hts_open_format] Failed to open file ""/input/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2136, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2063, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 427, in default_options. with sam.SamReader(flags_obj.reads.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_1i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:656,testability,unit,unittest,656,"Hi @sh940202123 ,. Sorry that our error messages are still quite cryptic at this time. The last traceback is actually not quite as useful. You're seeing it because we uses Python's subprocess.check_call to call each of the commands:. https://github.com/google/deepvariant/blob/7ed651ed54d8563530d58ba80187121337fd10f2/scripts/run_deepvariant.py#L362. Usually, I hope that whatever appeared right before that is useful, though. For example, if I deliberately deleted the bam file, I got:. ```. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. [E::hts_open_format] Failed to open file ""/input/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2136, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2063, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 427, in default_options. with sam.SamReader(flags_obj.reads.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_1i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:1016,testability,Trace,Traceback,1016,"rry that our error messages are still quite cryptic at this time. The last traceback is actually not quite as useful. You're seeing it because we uses Python's subprocess.check_call to call each of the commands:. https://github.com/google/deepvariant/blob/7ed651ed54d8563530d58ba80187121337fd10f2/scripts/run_deepvariant.py#L362. Usually, I hope that whatever appeared right before that is useful, though. For example, if I deliberately deleted the bam file, I got:. ```. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. [E::hts_open_format] Failed to open file ""/input/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2136, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2063, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 427, in default_options. with sam.SamReader(flags_obj.reads.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_g",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:2520,testability,unit,unittest,2520,". options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 427, in default_options. with sam.SamReader(flags_obj.reads.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 232, in __init__. use_original_base_quality_scores=use_original_base_quality_scores). ValueError: Not found: Could not open /input/NA12878_S1.chr20.10_10p1mb.bam. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m3.261s. user 0m5.770s. sys 0m5.974s. I0910 00:06:33.584306 140309422569216 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:2872,testability,Trace,Traceback,2872,"s_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 232, in __init__. use_original_base_quality_scores=use_original_base_quality_scores). ValueError: Not found: Could not open /input/NA12878_S1.chr20.10_10p1mb.bam. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m3.261s. user 0m5.770s. sys 0m5.974s. I0910 00:06:33.584306 140309422569216 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:3610,testability,unit,unittest,3610,"20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m3.261s. user 0m5.770s. sys 0m5.974s. I0910 00:06:33.584306 140309422569216 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 1. ```. However, in your case, it seems like there's nothing useful in the log. I think I've heard about similar cases before (where there was no useful error message showing up). I previously wondered if it's possible that `parallel` somehow hid away some message. But I can't immediately think of a reproducible setting that I can debug. But speaking of ""returned non-zero exit status 252"", I did a quick search in our GitHub issues and found: https://github.com/google/deepvariant/issues/325. Can you see if this might be relevant to your issue? If so, I'd still like to find a way to reproduce on my side, so I can see if we can make a more meaningful error message... 🤔",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:3965,testability,log,log,3965,"20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m3.261s. user 0m5.770s. sys 0m5.974s. I0910 00:06:33.584306 140309422569216 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 1. ```. However, in your case, it seems like there's nothing useful in the log. I think I've heard about similar cases before (where there was no useful error message showing up). I previously wondered if it's possible that `parallel` somehow hid away some message. But I can't immediately think of a reproducible setting that I can debug. But speaking of ""returned non-zero exit status 252"", I did a quick search in our GitHub issues and found: https://github.com/google/deepvariant/issues/325. Can you see if this might be relevant to your issue? If so, I'd still like to find a way to reproduce on my side, so I can see if we can make a more meaningful error message... 🤔",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:34,usability,error,error,34,"Hi @sh940202123 ,. Sorry that our error messages are still quite cryptic at this time. The last traceback is actually not quite as useful. You're seeing it because we uses Python's subprocess.check_call to call each of the commands:. https://github.com/google/deepvariant/blob/7ed651ed54d8563530d58ba80187121337fd10f2/scripts/run_deepvariant.py#L362. Usually, I hope that whatever appeared right before that is useful, though. For example, if I deliberately deleted the bam file, I got:. ```. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. [E::hts_open_format] Failed to open file ""/input/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2136, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2063, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 427, in default_options. with sam.SamReader(flags_obj.reads.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_1i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:223,usability,command,commands,223,"Hi @sh940202123 ,. Sorry that our error messages are still quite cryptic at this time. The last traceback is actually not quite as useful. You're seeing it because we uses Python's subprocess.check_call to call each of the commands:. https://github.com/google/deepvariant/blob/7ed651ed54d8563530d58ba80187121337fd10f2/scripts/run_deepvariant.py#L362. Usually, I hope that whatever appeared right before that is useful, though. For example, if I deliberately deleted the bam file, I got:. ```. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. [E::hts_open_format] Failed to open file ""/input/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2136, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2063, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 427, in default_options. with sam.SamReader(flags_obj.reads.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_1i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:511,usability,command,command,511,"Hi @sh940202123 ,. Sorry that our error messages are still quite cryptic at this time. The last traceback is actually not quite as useful. You're seeing it because we uses Python's subprocess.check_call to call each of the commands:. https://github.com/google/deepvariant/blob/7ed651ed54d8563530d58ba80187121337fd10f2/scripts/run_deepvariant.py#L362. Usually, I hope that whatever appeared right before that is useful, though. For example, if I deliberately deleted the bam file, I got:. ```. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. [E::hts_open_format] Failed to open file ""/input/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2136, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2063, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 427, in default_options. with sam.SamReader(flags_obj.reads.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_1i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:634,usability,input,input,634,"Hi @sh940202123 ,. Sorry that our error messages are still quite cryptic at this time. The last traceback is actually not quite as useful. You're seeing it because we uses Python's subprocess.check_call to call each of the commands:. https://github.com/google/deepvariant/blob/7ed651ed54d8563530d58ba80187121337fd10f2/scripts/run_deepvariant.py#L362. Usually, I hope that whatever appeared right before that is useful, though. For example, if I deliberately deleted the bam file, I got:. ```. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. [E::hts_open_format] Failed to open file ""/input/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2136, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2063, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 427, in default_options. with sam.SamReader(flags_obj.reads.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_1i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:682,usability,input,input,682,"Hi @sh940202123 ,. Sorry that our error messages are still quite cryptic at this time. The last traceback is actually not quite as useful. You're seeing it because we uses Python's subprocess.check_call to call each of the commands:. https://github.com/google/deepvariant/blob/7ed651ed54d8563530d58ba80187121337fd10f2/scripts/run_deepvariant.py#L362. Usually, I hope that whatever appeared right before that is useful, though. For example, if I deliberately deleted the bam file, I got:. ```. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. [E::hts_open_format] Failed to open file ""/input/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2136, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2063, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 427, in default_options. with sam.SamReader(flags_obj.reads.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_1i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:949,usability,input,input,949,"Hi @sh940202123 ,. Sorry that our error messages are still quite cryptic at this time. The last traceback is actually not quite as useful. You're seeing it because we uses Python's subprocess.check_call to call each of the commands:. https://github.com/google/deepvariant/blob/7ed651ed54d8563530d58ba80187121337fd10f2/scripts/run_deepvariant.py#L362. Usually, I hope that whatever appeared right before that is useful, though. For example, if I deliberately deleted the bam file, I got:. ```. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. [E::hts_open_format] Failed to open file ""/input/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2136, in <module>. app.run(main). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/absl_py/absl/app.py"", line 300, in run. _run_main(main, args). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/absl_py/absl/app.py"", line 251, in _run_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2063, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 427, in default_options. with sam.SamReader(flags_obj.reads.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_1i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:2375,usability,input,input,2375,"n_main. sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2063, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 427, in default_options. with sam.SamReader(flags_obj.reads.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 232, in __init__. use_original_base_quality_scores=use_original_base_quality_scores). ValueError: Not found: Could not open /input/NA12878_S1.chr20.10_10p1mb.bam. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m3.261s. user 0m5.770s. sys 0m5.974s. I0910 00:06:33.584306 140309422569216 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", lin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:2498,usability,input,input,2498,"py"", line 2063, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 427, in default_options. with sam.SamReader(flags_obj.reads.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 232, in __init__. use_original_base_quality_scores=use_original_base_quality_scores). ValueError: Not found: Could not open /input/NA12878_S1.chr20.10_10p1mb.bam. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m3.261s. user 0m5.770s. sys 0m5.974s. I0910 00:06:33.584306 140309422569216 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parall",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:2544,usability,input,input,2544,"ions(add_flags=True, flags_obj=FLAGS). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 427, in default_options. with sam.SamReader(flags_obj.reads.split(',')[0]) as sam_reader:. File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 232, in __init__. use_original_base_quality_scores=use_original_base_quality_scores). ValueError: Not found: Could not open /input/NA12878_S1.chr20.10_10p1mb.bam. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m3.261s. user 0m5.770s. sys 0m5.974s. I0910 00:06:33.584306 140309422569216 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:2775,usability,user,user,2775,":. File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__. self._reader = self._native_reader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader. return NativeSamReader(input_path, **kwargs). File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 232, in __init__. use_original_base_quality_scores=use_original_base_quality_scores). ValueError: Not found: Could not open /input/NA12878_S1.chr20.10_10p1mb.bam. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m3.261s. user 0m5.770s. sys 0m5.974s. I0910 00:06:33.584306 140309422569216 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:3288,usability,command,command,3288,"y_scores=use_original_base_quality_scores). ValueError: Not found: Could not open /input/NA12878_S1.chr20.10_10p1mb.bam. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m3.261s. user 0m5.770s. sys 0m5.974s. I0910 00:06:33.584306 140309422569216 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 1. ```. However, in your case, it seems like there's nothing useful in the log. I think I've heard about similar cases before (where there was no useful error message showing up). I previously wondered if it's possible that `parallel` somehow hid away some message. But I can't immediately think of a reproducible setting that I can debug. But speaking of ""returned non-zero exit status 252"", I did a q",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:3471,usability,Command,Command,3471," --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m3.261s. user 0m5.770s. sys 0m5.974s. I0910 00:06:33.584306 140309422569216 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 1. ```. However, in your case, it seems like there's nothing useful in the log. I think I've heard about similar cases before (where there was no useful error message showing up). I previously wondered if it's possible that `parallel` somehow hid away some message. But I can't immediately think of a reproducible setting that I can debug. But speaking of ""returned non-zero exit status 252"", I did a quick search in our GitHub issues and found: https://github.com/google/deepvariant/issues/325. Can you see if this might be relevant to your issue? If so, I'd still like to find a way ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:3588,usability,input,input,3588,"20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m3.261s. user 0m5.770s. sys 0m5.974s. I0910 00:06:33.584306 140309422569216 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 1. ```. However, in your case, it seems like there's nothing useful in the log. I think I've heard about similar cases before (where there was no useful error message showing up). I previously wondered if it's possible that `parallel` somehow hid away some message. But I can't immediately think of a reproducible setting that I can debug. But speaking of ""returned non-zero exit status 252"", I did a quick search in our GitHub issues and found: https://github.com/google/deepvariant/issues/325. Can you see if this might be relevant to your issue? If so, I'd still like to find a way to reproduce on my side, so I can see if we can make a more meaningful error message... 🤔",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:3636,usability,input,input,3636,"20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m3.261s. user 0m5.770s. sys 0m5.974s. I0910 00:06:33.584306 140309422569216 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 1. ```. However, in your case, it seems like there's nothing useful in the log. I think I've heard about similar cases before (where there was no useful error message showing up). I previously wondered if it's possible that `parallel` somehow hid away some message. But I can't immediately think of a reproducible setting that I can debug. But speaking of ""returned non-zero exit status 252"", I did a quick search in our GitHub issues and found: https://github.com/google/deepvariant/issues/325. Can you see if this might be relevant to your issue? If so, I'd still like to find a way to reproduce on my side, so I can see if we can make a more meaningful error message... 🤔",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:3883,usability,statu,status,3883,"20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m3.261s. user 0m5.770s. sys 0m5.974s. I0910 00:06:33.584306 140309422569216 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 1. ```. However, in your case, it seems like there's nothing useful in the log. I think I've heard about similar cases before (where there was no useful error message showing up). I previously wondered if it's possible that `parallel` somehow hid away some message. But I can't immediately think of a reproducible setting that I can debug. But speaking of ""returned non-zero exit status 252"", I did a quick search in our GitHub issues and found: https://github.com/google/deepvariant/issues/325. Can you see if this might be relevant to your issue? If so, I'd still like to find a way to reproduce on my side, so I can see if we can make a more meaningful error message... 🤔",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:4043,usability,error,error,4043,"20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m3.261s. user 0m5.770s. sys 0m5.974s. I0910 00:06:33.584306 140309422569216 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 1. ```. However, in your case, it seems like there's nothing useful in the log. I think I've heard about similar cases before (where there was no useful error message showing up). I previously wondered if it's possible that `parallel` somehow hid away some message. But I can't immediately think of a reproducible setting that I can debug. But speaking of ""returned non-zero exit status 252"", I did a quick search in our GitHub issues and found: https://github.com/google/deepvariant/issues/325. Can you see if this might be relevant to your issue? If so, I'd still like to find a way to reproduce on my side, so I can see if we can make a more meaningful error message... 🤔",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:4270,usability,statu,status,4270,"20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m3.261s. user 0m5.770s. sys 0m5.974s. I0910 00:06:33.584306 140309422569216 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 1. ```. However, in your case, it seems like there's nothing useful in the log. I think I've heard about similar cases before (where there was no useful error message showing up). I previously wondered if it's possible that `parallel` somehow hid away some message. But I can't immediately think of a reproducible setting that I can debug. But speaking of ""returned non-zero exit status 252"", I did a quick search in our GitHub issues and found: https://github.com/google/deepvariant/issues/325. Can you see if this might be relevant to your issue? If so, I'd still like to find a way to reproduce on my side, so I can see if we can make a more meaningful error message... 🤔",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:4546,usability,error,error,4546,"20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m3.261s. user 0m5.770s. sys 0m5.974s. I0910 00:06:33.584306 140309422569216 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 1. ```. However, in your case, it seems like there's nothing useful in the log. I think I've heard about similar cases before (where there was no useful error message showing up). I previously wondered if it's possible that `parallel` somehow hid away some message. But I can't immediately think of a reproducible setting that I can debug. But speaking of ""returned non-zero exit status 252"", I did a quick search in our GitHub issues and found: https://github.com/google/deepvariant/issues/325. Can you see if this might be relevant to your issue? If so, I'd still like to find a way to reproduce on my side, so I can see if we can make a more meaningful error message... 🤔",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:168,availability,error,error,168,"Hi @pichuan ~. Thank you for this quickly reply~. I tried to add `--user root` into docker command like the solved issue #325 . But it still didn't show any additional error message like the issue . I also check the disk and it is still have about 9 Gb free space. Here is the code and message output. ```sh. $ BIN_VERSION=""1.0.0"". $ sudo docker pull google/deepvariant:""${BIN_VERSION}"". Digest: sha256:6ef8f3b4c4465e41ee7597cd2351a7c44dd8b62a849a47766316507a6234f5f8. Status: Downloaded newer image for google/deepvariant:1.0.0. docker.io/google/deepvariant:1.0.0. $ INPUT_DIR=""${PWD}/quickstart-testdata"". $ ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.bam.bai. test_nist.b37_chr20_100kbp_at_10mb.bed. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. ucsc.hg19.chr20.unittest.fasta. ucsc.hg19.chr20.unittest.fasta.fai. ucsc.hg19.chr20.unittest.fasta.gz. ucsc.hg19.chr20.unittest.fasta.gz.fai. ucsc.hg19.chr20.unittest.fasta.gz.gzi. $ OUTPUT_DIR=""${PWD}/quickstart-output"". $ ls ${OUTPUT_DIR}. intermediate_results_dir. $ df -h. 檔案系統 容量 已用 可用 已用% 掛載點. udev 7.8G 0 7.8G 0% /dev. tmpfs 1.6G 11M 1.6G 1% /run. /dev/sda1 109G 95G 8.9G 92% /. tmpfs 7.9G 200K 7.9G 1% /dev/shm. tmpfs 5.0M 4.0K 5.0M 1% /run/lock. tmpfs 7.9G 0 7.9G 0% /sys/fs/cgroup. /dev/loop1 56M 56M 0 100% /snap/core18/1885. /dev/loop3 55M 55M 0 100% /snap/gtk-common-themes/1502. /dev/loop2 162M 162M 0 100% /snap/gnome-3-28-1804/128. /dev/loop4 161M 161M 0 100% /snap/gnome-3-28-1804/116. /dev/loop0 30M 30M 0 100% /snap/snapd/8790. /dev/loop5 63M 63M 0 100% /snap/gtk-common-themes/1506. /dev/loop6 55M 55M 0 100% /snap/core18/1880. /dev/loop7 30M 30M 0 100% /snap/snapd/8542. /dev/loop8 39M 39M 0 100% /snap/remmina/4309. /dev/loop9 40M 40M 0 100% /snap/remmina/4324. tmpfs 1.6G 40K 1.6G 1% /run/user/108. tmpfs 1.6G 12K 1.6G 1% /run/user/1000. $ sudo docker run \. > --user root \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > googl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:477,availability,Down,Downloaded,477,"Hi @pichuan ~. Thank you for this quickly reply~. I tried to add `--user root` into docker command like the solved issue #325 . But it still didn't show any additional error message like the issue . I also check the disk and it is still have about 9 Gb free space. Here is the code and message output. ```sh. $ BIN_VERSION=""1.0.0"". $ sudo docker pull google/deepvariant:""${BIN_VERSION}"". Digest: sha256:6ef8f3b4c4465e41ee7597cd2351a7c44dd8b62a849a47766316507a6234f5f8. Status: Downloaded newer image for google/deepvariant:1.0.0. docker.io/google/deepvariant:1.0.0. $ INPUT_DIR=""${PWD}/quickstart-testdata"". $ ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.bam.bai. test_nist.b37_chr20_100kbp_at_10mb.bed. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. ucsc.hg19.chr20.unittest.fasta. ucsc.hg19.chr20.unittest.fasta.fai. ucsc.hg19.chr20.unittest.fasta.gz. ucsc.hg19.chr20.unittest.fasta.gz.fai. ucsc.hg19.chr20.unittest.fasta.gz.gzi. $ OUTPUT_DIR=""${PWD}/quickstart-output"". $ ls ${OUTPUT_DIR}. intermediate_results_dir. $ df -h. 檔案系統 容量 已用 可用 已用% 掛載點. udev 7.8G 0 7.8G 0% /dev. tmpfs 1.6G 11M 1.6G 1% /run. /dev/sda1 109G 95G 8.9G 92% /. tmpfs 7.9G 200K 7.9G 1% /dev/shm. tmpfs 5.0M 4.0K 5.0M 1% /run/lock. tmpfs 7.9G 0 7.9G 0% /sys/fs/cgroup. /dev/loop1 56M 56M 0 100% /snap/core18/1885. /dev/loop3 55M 55M 0 100% /snap/gtk-common-themes/1502. /dev/loop2 162M 162M 0 100% /snap/gnome-3-28-1804/128. /dev/loop4 161M 161M 0 100% /snap/gnome-3-28-1804/116. /dev/loop0 30M 30M 0 100% /snap/snapd/8790. /dev/loop5 63M 63M 0 100% /snap/gtk-common-themes/1506. /dev/loop6 55M 55M 0 100% /snap/core18/1880. /dev/loop7 30M 30M 0 100% /snap/snapd/8542. /dev/loop8 39M 39M 0 100% /snap/remmina/4309. /dev/loop9 40M 40M 0 100% /snap/remmina/4324. tmpfs 1.6G 40K 1.6G 1% /run/user/108. tmpfs 1.6G 12K 1.6G 1% /run/user/1000. $ sudo docker run \. > --user root \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > googl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:4617,availability,error,error,4617,"cker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m1.293s. user 0m1.033s. sys 0m0.705s. I0910 01:14:39.661637 139749579556608 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 252. ```. I wonder is there any way to let me get the hidden parallel error message? Best,. Jerry.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:3089,deployability,fail,failed,3089,"GS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=1 \. >. I0910 01:14:38.364422 139749579556608 run_deepvariant.py:273] Re-using the directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m1.293s. user 0m1.033s. sys 0m0.705s. I0910 01:14:39.661637 139749579556608 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:3627,deployability,modul,module,3627,"sults_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m1.293s. user 0m1.033s. sys 0m0.705s. I0910 01:14:39.661637 139749579556608 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 252. ```. I wonder is there any way to let me get the hidden parallel error message",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:174,integrability,messag,message,174,"Hi @pichuan ~. Thank you for this quickly reply~. I tried to add `--user root` into docker command like the solved issue #325 . But it still didn't show any additional error message like the issue . I also check the disk and it is still have about 9 Gb free space. Here is the code and message output. ```sh. $ BIN_VERSION=""1.0.0"". $ sudo docker pull google/deepvariant:""${BIN_VERSION}"". Digest: sha256:6ef8f3b4c4465e41ee7597cd2351a7c44dd8b62a849a47766316507a6234f5f8. Status: Downloaded newer image for google/deepvariant:1.0.0. docker.io/google/deepvariant:1.0.0. $ INPUT_DIR=""${PWD}/quickstart-testdata"". $ ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.bam.bai. test_nist.b37_chr20_100kbp_at_10mb.bed. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. ucsc.hg19.chr20.unittest.fasta. ucsc.hg19.chr20.unittest.fasta.fai. ucsc.hg19.chr20.unittest.fasta.gz. ucsc.hg19.chr20.unittest.fasta.gz.fai. ucsc.hg19.chr20.unittest.fasta.gz.gzi. $ OUTPUT_DIR=""${PWD}/quickstart-output"". $ ls ${OUTPUT_DIR}. intermediate_results_dir. $ df -h. 檔案系統 容量 已用 可用 已用% 掛載點. udev 7.8G 0 7.8G 0% /dev. tmpfs 1.6G 11M 1.6G 1% /run. /dev/sda1 109G 95G 8.9G 92% /. tmpfs 7.9G 200K 7.9G 1% /dev/shm. tmpfs 5.0M 4.0K 5.0M 1% /run/lock. tmpfs 7.9G 0 7.9G 0% /sys/fs/cgroup. /dev/loop1 56M 56M 0 100% /snap/core18/1885. /dev/loop3 55M 55M 0 100% /snap/gtk-common-themes/1502. /dev/loop2 162M 162M 0 100% /snap/gnome-3-28-1804/128. /dev/loop4 161M 161M 0 100% /snap/gnome-3-28-1804/116. /dev/loop0 30M 30M 0 100% /snap/snapd/8790. /dev/loop5 63M 63M 0 100% /snap/gtk-common-themes/1506. /dev/loop6 55M 55M 0 100% /snap/core18/1880. /dev/loop7 30M 30M 0 100% /snap/snapd/8542. /dev/loop8 39M 39M 0 100% /snap/remmina/4309. /dev/loop9 40M 40M 0 100% /snap/remmina/4324. tmpfs 1.6G 40K 1.6G 1% /run/user/108. tmpfs 1.6G 12K 1.6G 1% /run/user/1000. $ sudo docker run \. > --user root \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > googl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:286,integrability,messag,message,286,"Hi @pichuan ~. Thank you for this quickly reply~. I tried to add `--user root` into docker command like the solved issue #325 . But it still didn't show any additional error message like the issue . I also check the disk and it is still have about 9 Gb free space. Here is the code and message output. ```sh. $ BIN_VERSION=""1.0.0"". $ sudo docker pull google/deepvariant:""${BIN_VERSION}"". Digest: sha256:6ef8f3b4c4465e41ee7597cd2351a7c44dd8b62a849a47766316507a6234f5f8. Status: Downloaded newer image for google/deepvariant:1.0.0. docker.io/google/deepvariant:1.0.0. $ INPUT_DIR=""${PWD}/quickstart-testdata"". $ ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.bam.bai. test_nist.b37_chr20_100kbp_at_10mb.bed. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. ucsc.hg19.chr20.unittest.fasta. ucsc.hg19.chr20.unittest.fasta.fai. ucsc.hg19.chr20.unittest.fasta.gz. ucsc.hg19.chr20.unittest.fasta.gz.fai. ucsc.hg19.chr20.unittest.fasta.gz.gzi. $ OUTPUT_DIR=""${PWD}/quickstart-output"". $ ls ${OUTPUT_DIR}. intermediate_results_dir. $ df -h. 檔案系統 容量 已用 可用 已用% 掛載點. udev 7.8G 0 7.8G 0% /dev. tmpfs 1.6G 11M 1.6G 1% /run. /dev/sda1 109G 95G 8.9G 92% /. tmpfs 7.9G 200K 7.9G 1% /dev/shm. tmpfs 5.0M 4.0K 5.0M 1% /run/lock. tmpfs 7.9G 0 7.9G 0% /sys/fs/cgroup. /dev/loop1 56M 56M 0 100% /snap/core18/1885. /dev/loop3 55M 55M 0 100% /snap/gtk-common-themes/1502. /dev/loop2 162M 162M 0 100% /snap/gnome-3-28-1804/128. /dev/loop4 161M 161M 0 100% /snap/gnome-3-28-1804/116. /dev/loop0 30M 30M 0 100% /snap/snapd/8790. /dev/loop5 63M 63M 0 100% /snap/gtk-common-themes/1506. /dev/loop6 55M 55M 0 100% /snap/core18/1880. /dev/loop7 30M 30M 0 100% /snap/snapd/8542. /dev/loop8 39M 39M 0 100% /snap/remmina/4309. /dev/loop9 40M 40M 0 100% /snap/remmina/4324. tmpfs 1.6G 40K 1.6G 1% /run/user/108. tmpfs 1.6G 12K 1.6G 1% /run/user/1000. $ sudo docker run \. > --user root \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > googl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:2733,integrability,buffer,buffer,2733,"9M 0 100% /snap/remmina/4309. /dev/loop9 40M 40M 0 100% /snap/remmina/4324. tmpfs 1.6G 40K 1.6G 1% /run/user/108. tmpfs 1.6G 12K 1.6G 1% /run/user/1000. $ sudo docker run \. > --user root \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=1 \. >. I0910 01:14:38.364422 139749579556608 run_deepvariant.py:273] Re-using the directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m1.293s. user 0m1.033s. sys 0m0.705s. I0910 01:14:39.661637 139749579556608 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_mai",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:3923,integrability,sub,subprocess,3923,"cker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m1.293s. user 0m1.033s. sys 0m0.705s. I0910 01:14:39.661637 139749579556608 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 252. ```. I wonder is there any way to let me get the hidden parallel error message? Best,. Jerry.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:4016,integrability,sub,subprocess,4016,"cker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m1.293s. user 0m1.033s. sys 0m0.705s. I0910 01:14:39.661637 139749579556608 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 252. ```. I wonder is there any way to let me get the hidden parallel error message? Best,. Jerry.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:4097,integrability,sub,subprocess,4097,"cker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m1.293s. user 0m1.033s. sys 0m0.705s. I0910 01:14:39.661637 139749579556608 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 252. ```. I wonder is there any way to let me get the hidden parallel error message? Best,. Jerry.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:4180,integrability,buffer,buffer,4180,"cker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m1.293s. user 0m1.033s. sys 0m0.705s. I0910 01:14:39.661637 139749579556608 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 252. ```. I wonder is there any way to let me get the hidden parallel error message? Best,. Jerry.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:4623,integrability,messag,message,4623,"cker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m1.293s. user 0m1.033s. sys 0m0.705s. I0910 01:14:39.661637 139749579556608 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 252. ```. I wonder is there any way to let me get the hidden parallel error message? Best,. Jerry.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:174,interoperability,messag,message,174,"Hi @pichuan ~. Thank you for this quickly reply~. I tried to add `--user root` into docker command like the solved issue #325 . But it still didn't show any additional error message like the issue . I also check the disk and it is still have about 9 Gb free space. Here is the code and message output. ```sh. $ BIN_VERSION=""1.0.0"". $ sudo docker pull google/deepvariant:""${BIN_VERSION}"". Digest: sha256:6ef8f3b4c4465e41ee7597cd2351a7c44dd8b62a849a47766316507a6234f5f8. Status: Downloaded newer image for google/deepvariant:1.0.0. docker.io/google/deepvariant:1.0.0. $ INPUT_DIR=""${PWD}/quickstart-testdata"". $ ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.bam.bai. test_nist.b37_chr20_100kbp_at_10mb.bed. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. ucsc.hg19.chr20.unittest.fasta. ucsc.hg19.chr20.unittest.fasta.fai. ucsc.hg19.chr20.unittest.fasta.gz. ucsc.hg19.chr20.unittest.fasta.gz.fai. ucsc.hg19.chr20.unittest.fasta.gz.gzi. $ OUTPUT_DIR=""${PWD}/quickstart-output"". $ ls ${OUTPUT_DIR}. intermediate_results_dir. $ df -h. 檔案系統 容量 已用 可用 已用% 掛載點. udev 7.8G 0 7.8G 0% /dev. tmpfs 1.6G 11M 1.6G 1% /run. /dev/sda1 109G 95G 8.9G 92% /. tmpfs 7.9G 200K 7.9G 1% /dev/shm. tmpfs 5.0M 4.0K 5.0M 1% /run/lock. tmpfs 7.9G 0 7.9G 0% /sys/fs/cgroup. /dev/loop1 56M 56M 0 100% /snap/core18/1885. /dev/loop3 55M 55M 0 100% /snap/gtk-common-themes/1502. /dev/loop2 162M 162M 0 100% /snap/gnome-3-28-1804/128. /dev/loop4 161M 161M 0 100% /snap/gnome-3-28-1804/116. /dev/loop0 30M 30M 0 100% /snap/snapd/8790. /dev/loop5 63M 63M 0 100% /snap/gtk-common-themes/1506. /dev/loop6 55M 55M 0 100% /snap/core18/1880. /dev/loop7 30M 30M 0 100% /snap/snapd/8542. /dev/loop8 39M 39M 0 100% /snap/remmina/4309. /dev/loop9 40M 40M 0 100% /snap/remmina/4324. tmpfs 1.6G 40K 1.6G 1% /run/user/108. tmpfs 1.6G 12K 1.6G 1% /run/user/1000. $ sudo docker run \. > --user root \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > googl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:286,interoperability,messag,message,286,"Hi @pichuan ~. Thank you for this quickly reply~. I tried to add `--user root` into docker command like the solved issue #325 . But it still didn't show any additional error message like the issue . I also check the disk and it is still have about 9 Gb free space. Here is the code and message output. ```sh. $ BIN_VERSION=""1.0.0"". $ sudo docker pull google/deepvariant:""${BIN_VERSION}"". Digest: sha256:6ef8f3b4c4465e41ee7597cd2351a7c44dd8b62a849a47766316507a6234f5f8. Status: Downloaded newer image for google/deepvariant:1.0.0. docker.io/google/deepvariant:1.0.0. $ INPUT_DIR=""${PWD}/quickstart-testdata"". $ ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.bam.bai. test_nist.b37_chr20_100kbp_at_10mb.bed. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. ucsc.hg19.chr20.unittest.fasta. ucsc.hg19.chr20.unittest.fasta.fai. ucsc.hg19.chr20.unittest.fasta.gz. ucsc.hg19.chr20.unittest.fasta.gz.fai. ucsc.hg19.chr20.unittest.fasta.gz.gzi. $ OUTPUT_DIR=""${PWD}/quickstart-output"". $ ls ${OUTPUT_DIR}. intermediate_results_dir. $ df -h. 檔案系統 容量 已用 可用 已用% 掛載點. udev 7.8G 0 7.8G 0% /dev. tmpfs 1.6G 11M 1.6G 1% /run. /dev/sda1 109G 95G 8.9G 92% /. tmpfs 7.9G 200K 7.9G 1% /dev/shm. tmpfs 5.0M 4.0K 5.0M 1% /run/lock. tmpfs 7.9G 0 7.9G 0% /sys/fs/cgroup. /dev/loop1 56M 56M 0 100% /snap/core18/1885. /dev/loop3 55M 55M 0 100% /snap/gtk-common-themes/1502. /dev/loop2 162M 162M 0 100% /snap/gnome-3-28-1804/128. /dev/loop4 161M 161M 0 100% /snap/gnome-3-28-1804/116. /dev/loop0 30M 30M 0 100% /snap/snapd/8790. /dev/loop5 63M 63M 0 100% /snap/gtk-common-themes/1506. /dev/loop6 55M 55M 0 100% /snap/core18/1880. /dev/loop7 30M 30M 0 100% /snap/snapd/8542. /dev/loop8 39M 39M 0 100% /snap/remmina/4309. /dev/loop9 40M 40M 0 100% /snap/remmina/4324. tmpfs 1.6G 40K 1.6G 1% /run/user/108. tmpfs 1.6G 12K 1.6G 1% /run/user/1000. $ sudo docker run \. > --user root \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > googl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:4623,interoperability,messag,message,4623,"cker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m1.293s. user 0m1.033s. sys 0m0.705s. I0910 01:14:39.661637 139749579556608 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 252. ```. I wonder is there any way to let me get the hidden parallel error message? Best,. Jerry.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:2503,modifiability,interm,intermediate,2503,"/gnome-3-28-1804/116. /dev/loop0 30M 30M 0 100% /snap/snapd/8790. /dev/loop5 63M 63M 0 100% /snap/gtk-common-themes/1506. /dev/loop6 55M 55M 0 100% /snap/core18/1880. /dev/loop7 30M 30M 0 100% /snap/snapd/8542. /dev/loop8 39M 39M 0 100% /snap/remmina/4309. /dev/loop9 40M 40M 0 100% /snap/remmina/4324. tmpfs 1.6G 40K 1.6G 1% /run/user/108. tmpfs 1.6G 12K 1.6G 1% /run/user/1000. $ sudo docker run \. > --user root \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=1 \. >. I0910 01:14:38.364422 139749579556608 run_deepvariant.py:273] Re-using the directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m1.293s. user 0m1.033s. sys 0m0.705s. I0910 01:14:39.661637 139749579556608 run_deepva",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:2567,modifiability,Interm,Intermediate,2567,". /dev/loop5 63M 63M 0 100% /snap/gtk-common-themes/1506. /dev/loop6 55M 55M 0 100% /snap/core18/1880. /dev/loop7 30M 30M 0 100% /snap/snapd/8542. /dev/loop8 39M 39M 0 100% /snap/remmina/4309. /dev/loop9 40M 40M 0 100% /snap/remmina/4324. tmpfs 1.6G 40K 1.6G 1% /run/user/108. tmpfs 1.6G 12K 1.6G 1% /run/user/1000. $ sudo docker run \. > --user root \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=1 \. >. I0910 01:14:38.364422 139749579556608 run_deepvariant.py:273] Re-using the directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m1.293s. user 0m1.033s. sys 0m0.705s. I0910 01:14:39.661637 139749579556608 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/o",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:3627,modifiability,modul,module,3627,"sults_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m1.293s. user 0m1.033s. sys 0m0.705s. I0910 01:14:39.661637 139749579556608 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 252. ```. I wonder is there any way to let me get the hidden parallel error message",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:3687,modifiability,pac,packages,3687,"cker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m1.293s. user 0m1.033s. sys 0m0.705s. I0910 01:14:39.661637 139749579556608 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 252. ```. I wonder is there any way to let me get the hidden parallel error message? Best,. Jerry.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:3787,modifiability,pac,packages,3787,"cker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m1.293s. user 0m1.033s. sys 0m0.705s. I0910 01:14:39.661637 139749579556608 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 252. ```. I wonder is there any way to let me get the hidden parallel error message? Best,. Jerry.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:168,performance,error,error,168,"Hi @pichuan ~. Thank you for this quickly reply~. I tried to add `--user root` into docker command like the solved issue #325 . But it still didn't show any additional error message like the issue . I also check the disk and it is still have about 9 Gb free space. Here is the code and message output. ```sh. $ BIN_VERSION=""1.0.0"". $ sudo docker pull google/deepvariant:""${BIN_VERSION}"". Digest: sha256:6ef8f3b4c4465e41ee7597cd2351a7c44dd8b62a849a47766316507a6234f5f8. Status: Downloaded newer image for google/deepvariant:1.0.0. docker.io/google/deepvariant:1.0.0. $ INPUT_DIR=""${PWD}/quickstart-testdata"". $ ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.bam.bai. test_nist.b37_chr20_100kbp_at_10mb.bed. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. ucsc.hg19.chr20.unittest.fasta. ucsc.hg19.chr20.unittest.fasta.fai. ucsc.hg19.chr20.unittest.fasta.gz. ucsc.hg19.chr20.unittest.fasta.gz.fai. ucsc.hg19.chr20.unittest.fasta.gz.gzi. $ OUTPUT_DIR=""${PWD}/quickstart-output"". $ ls ${OUTPUT_DIR}. intermediate_results_dir. $ df -h. 檔案系統 容量 已用 可用 已用% 掛載點. udev 7.8G 0 7.8G 0% /dev. tmpfs 1.6G 11M 1.6G 1% /run. /dev/sda1 109G 95G 8.9G 92% /. tmpfs 7.9G 200K 7.9G 1% /dev/shm. tmpfs 5.0M 4.0K 5.0M 1% /run/lock. tmpfs 7.9G 0 7.9G 0% /sys/fs/cgroup. /dev/loop1 56M 56M 0 100% /snap/core18/1885. /dev/loop3 55M 55M 0 100% /snap/gtk-common-themes/1502. /dev/loop2 162M 162M 0 100% /snap/gnome-3-28-1804/128. /dev/loop4 161M 161M 0 100% /snap/gnome-3-28-1804/116. /dev/loop0 30M 30M 0 100% /snap/snapd/8790. /dev/loop5 63M 63M 0 100% /snap/gtk-common-themes/1506. /dev/loop6 55M 55M 0 100% /snap/core18/1880. /dev/loop7 30M 30M 0 100% /snap/snapd/8542. /dev/loop8 39M 39M 0 100% /snap/remmina/4309. /dev/loop9 40M 40M 0 100% /snap/remmina/4324. tmpfs 1.6G 40K 1.6G 1% /run/user/108. tmpfs 1.6G 12K 1.6G 1% /run/user/1000. $ sudo docker run \. > --user root \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > googl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:216,performance,disk,disk,216,"Hi @pichuan ~. Thank you for this quickly reply~. I tried to add `--user root` into docker command like the solved issue #325 . But it still didn't show any additional error message like the issue . I also check the disk and it is still have about 9 Gb free space. Here is the code and message output. ```sh. $ BIN_VERSION=""1.0.0"". $ sudo docker pull google/deepvariant:""${BIN_VERSION}"". Digest: sha256:6ef8f3b4c4465e41ee7597cd2351a7c44dd8b62a849a47766316507a6234f5f8. Status: Downloaded newer image for google/deepvariant:1.0.0. docker.io/google/deepvariant:1.0.0. $ INPUT_DIR=""${PWD}/quickstart-testdata"". $ ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.bam.bai. test_nist.b37_chr20_100kbp_at_10mb.bed. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. ucsc.hg19.chr20.unittest.fasta. ucsc.hg19.chr20.unittest.fasta.fai. ucsc.hg19.chr20.unittest.fasta.gz. ucsc.hg19.chr20.unittest.fasta.gz.fai. ucsc.hg19.chr20.unittest.fasta.gz.gzi. $ OUTPUT_DIR=""${PWD}/quickstart-output"". $ ls ${OUTPUT_DIR}. intermediate_results_dir. $ df -h. 檔案系統 容量 已用 可用 已用% 掛載點. udev 7.8G 0 7.8G 0% /dev. tmpfs 1.6G 11M 1.6G 1% /run. /dev/sda1 109G 95G 8.9G 92% /. tmpfs 7.9G 200K 7.9G 1% /dev/shm. tmpfs 5.0M 4.0K 5.0M 1% /run/lock. tmpfs 7.9G 0 7.9G 0% /sys/fs/cgroup. /dev/loop1 56M 56M 0 100% /snap/core18/1885. /dev/loop3 55M 55M 0 100% /snap/gtk-common-themes/1502. /dev/loop2 162M 162M 0 100% /snap/gnome-3-28-1804/128. /dev/loop4 161M 161M 0 100% /snap/gnome-3-28-1804/116. /dev/loop0 30M 30M 0 100% /snap/snapd/8790. /dev/loop5 63M 63M 0 100% /snap/gtk-common-themes/1506. /dev/loop6 55M 55M 0 100% /snap/core18/1880. /dev/loop7 30M 30M 0 100% /snap/snapd/8542. /dev/loop8 39M 39M 0 100% /snap/remmina/4309. /dev/loop9 40M 40M 0 100% /snap/remmina/4324. tmpfs 1.6G 40K 1.6G 1% /run/user/108. tmpfs 1.6G 12K 1.6G 1% /run/user/1000. $ sudo docker run \. > --user root \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > googl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:1277,performance,lock,lock,1277,"de and message output. ```sh. $ BIN_VERSION=""1.0.0"". $ sudo docker pull google/deepvariant:""${BIN_VERSION}"". Digest: sha256:6ef8f3b4c4465e41ee7597cd2351a7c44dd8b62a849a47766316507a6234f5f8. Status: Downloaded newer image for google/deepvariant:1.0.0. docker.io/google/deepvariant:1.0.0. $ INPUT_DIR=""${PWD}/quickstart-testdata"". $ ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.bam.bai. test_nist.b37_chr20_100kbp_at_10mb.bed. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. ucsc.hg19.chr20.unittest.fasta. ucsc.hg19.chr20.unittest.fasta.fai. ucsc.hg19.chr20.unittest.fasta.gz. ucsc.hg19.chr20.unittest.fasta.gz.fai. ucsc.hg19.chr20.unittest.fasta.gz.gzi. $ OUTPUT_DIR=""${PWD}/quickstart-output"". $ ls ${OUTPUT_DIR}. intermediate_results_dir. $ df -h. 檔案系統 容量 已用 可用 已用% 掛載點. udev 7.8G 0 7.8G 0% /dev. tmpfs 1.6G 11M 1.6G 1% /run. /dev/sda1 109G 95G 8.9G 92% /. tmpfs 7.9G 200K 7.9G 1% /dev/shm. tmpfs 5.0M 4.0K 5.0M 1% /run/lock. tmpfs 7.9G 0 7.9G 0% /sys/fs/cgroup. /dev/loop1 56M 56M 0 100% /snap/core18/1885. /dev/loop3 55M 55M 0 100% /snap/gtk-common-themes/1502. /dev/loop2 162M 162M 0 100% /snap/gnome-3-28-1804/128. /dev/loop4 161M 161M 0 100% /snap/gnome-3-28-1804/116. /dev/loop0 30M 30M 0 100% /snap/snapd/8790. /dev/loop5 63M 63M 0 100% /snap/gtk-common-themes/1506. /dev/loop6 55M 55M 0 100% /snap/core18/1880. /dev/loop7 30M 30M 0 100% /snap/snapd/8542. /dev/loop8 39M 39M 0 100% /snap/remmina/4309. /dev/loop9 40M 40M 0 100% /snap/remmina/4324. tmpfs 1.6G 40K 1.6G 1% /run/user/108. tmpfs 1.6G 12K 1.6G 1% /run/user/1000. $ sudo docker run \. > --user root \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:2690,performance,time,time,2690,"0M 0 100% /snap/snapd/8542. /dev/loop8 39M 39M 0 100% /snap/remmina/4309. /dev/loop9 40M 40M 0 100% /snap/remmina/4324. tmpfs 1.6G 40K 1.6G 1% /run/user/108. tmpfs 1.6G 12K 1.6G 1% /run/user/1000. $ sudo docker run \. > --user root \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=1 \. >. I0910 01:14:38.364422 139749579556608 run_deepvariant.py:273] Re-using the directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m1.293s. user 0m1.033s. sys 0m0.705s. I0910 01:14:39.661637 139749579556608 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:2705,performance,parallel,parallel,2705,"napd/8542. /dev/loop8 39M 39M 0 100% /snap/remmina/4309. /dev/loop9 40M 40M 0 100% /snap/remmina/4324. tmpfs 1.6G 40K 1.6G 1% /run/user/108. tmpfs 1.6G 12K 1.6G 1% /run/user/1000. $ sudo docker run \. > --user root \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=1 \. >. I0910 01:14:38.364422 139749579556608 run_deepvariant.py:273] Re-using the directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m1.293s. user 0m1.033s. sys 0m0.705s. I0910 01:14:39.661637 139749579556608 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"",",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:3070,performance,parallel,parallel,3070,". > --model_type=WGS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=1 \. >. I0910 01:14:38.364422 139749579556608 run_deepvariant.py:273] Re-using the directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m1.293s. user 0m1.033s. sys 0m0.705s. I0910 01:14:39.661637 139749579556608 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProce",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:4137,performance,time,time,4137,"cker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m1.293s. user 0m1.033s. sys 0m0.705s. I0910 01:14:39.661637 139749579556608 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 252. ```. I wonder is there any way to let me get the hidden parallel error message? Best,. Jerry.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:4152,performance,parallel,parallel,4152,"cker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m1.293s. user 0m1.033s. sys 0m0.705s. I0910 01:14:39.661637 139749579556608 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 252. ```. I wonder is there any way to let me get the hidden parallel error message? Best,. Jerry.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:4608,performance,parallel,parallel,4608,"cker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m1.293s. user 0m1.033s. sys 0m0.705s. I0910 01:14:39.661637 139749579556608 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 252. ```. I wonder is there any way to let me get the hidden parallel error message? Best,. Jerry.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:4617,performance,error,error,4617,"cker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m1.293s. user 0m1.033s. sys 0m0.705s. I0910 01:14:39.661637 139749579556608 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 252. ```. I wonder is there any way to let me get the hidden parallel error message? Best,. Jerry.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:3089,reliability,fail,failed,3089,"GS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=1 \. >. I0910 01:14:38.364422 139749579556608 run_deepvariant.py:273] Re-using the directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m1.293s. user 0m1.033s. sys 0m0.705s. I0910 01:14:39.661637 139749579556608 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:168,safety,error,error,168,"Hi @pichuan ~. Thank you for this quickly reply~. I tried to add `--user root` into docker command like the solved issue #325 . But it still didn't show any additional error message like the issue . I also check the disk and it is still have about 9 Gb free space. Here is the code and message output. ```sh. $ BIN_VERSION=""1.0.0"". $ sudo docker pull google/deepvariant:""${BIN_VERSION}"". Digest: sha256:6ef8f3b4c4465e41ee7597cd2351a7c44dd8b62a849a47766316507a6234f5f8. Status: Downloaded newer image for google/deepvariant:1.0.0. docker.io/google/deepvariant:1.0.0. $ INPUT_DIR=""${PWD}/quickstart-testdata"". $ ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.bam.bai. test_nist.b37_chr20_100kbp_at_10mb.bed. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. ucsc.hg19.chr20.unittest.fasta. ucsc.hg19.chr20.unittest.fasta.fai. ucsc.hg19.chr20.unittest.fasta.gz. ucsc.hg19.chr20.unittest.fasta.gz.fai. ucsc.hg19.chr20.unittest.fasta.gz.gzi. $ OUTPUT_DIR=""${PWD}/quickstart-output"". $ ls ${OUTPUT_DIR}. intermediate_results_dir. $ df -h. 檔案系統 容量 已用 可用 已用% 掛載點. udev 7.8G 0 7.8G 0% /dev. tmpfs 1.6G 11M 1.6G 1% /run. /dev/sda1 109G 95G 8.9G 92% /. tmpfs 7.9G 200K 7.9G 1% /dev/shm. tmpfs 5.0M 4.0K 5.0M 1% /run/lock. tmpfs 7.9G 0 7.9G 0% /sys/fs/cgroup. /dev/loop1 56M 56M 0 100% /snap/core18/1885. /dev/loop3 55M 55M 0 100% /snap/gtk-common-themes/1502. /dev/loop2 162M 162M 0 100% /snap/gnome-3-28-1804/128. /dev/loop4 161M 161M 0 100% /snap/gnome-3-28-1804/116. /dev/loop0 30M 30M 0 100% /snap/snapd/8790. /dev/loop5 63M 63M 0 100% /snap/gtk-common-themes/1506. /dev/loop6 55M 55M 0 100% /snap/core18/1880. /dev/loop7 30M 30M 0 100% /snap/snapd/8542. /dev/loop8 39M 39M 0 100% /snap/remmina/4309. /dev/loop9 40M 40M 0 100% /snap/remmina/4324. tmpfs 1.6G 40K 1.6G 1% /run/user/108. tmpfs 1.6G 12K 1.6G 1% /run/user/1000. $ sudo docker run \. > --user root \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > googl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:597,safety,test,testdata,597,"Hi @pichuan ~. Thank you for this quickly reply~. I tried to add `--user root` into docker command like the solved issue #325 . But it still didn't show any additional error message like the issue . I also check the disk and it is still have about 9 Gb free space. Here is the code and message output. ```sh. $ BIN_VERSION=""1.0.0"". $ sudo docker pull google/deepvariant:""${BIN_VERSION}"". Digest: sha256:6ef8f3b4c4465e41ee7597cd2351a7c44dd8b62a849a47766316507a6234f5f8. Status: Downloaded newer image for google/deepvariant:1.0.0. docker.io/google/deepvariant:1.0.0. $ INPUT_DIR=""${PWD}/quickstart-testdata"". $ ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.bam.bai. test_nist.b37_chr20_100kbp_at_10mb.bed. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. ucsc.hg19.chr20.unittest.fasta. ucsc.hg19.chr20.unittest.fasta.fai. ucsc.hg19.chr20.unittest.fasta.gz. ucsc.hg19.chr20.unittest.fasta.gz.fai. ucsc.hg19.chr20.unittest.fasta.gz.gzi. $ OUTPUT_DIR=""${PWD}/quickstart-output"". $ ls ${OUTPUT_DIR}. intermediate_results_dir. $ df -h. 檔案系統 容量 已用 可用 已用% 掛載點. udev 7.8G 0 7.8G 0% /dev. tmpfs 1.6G 11M 1.6G 1% /run. /dev/sda1 109G 95G 8.9G 92% /. tmpfs 7.9G 200K 7.9G 1% /dev/shm. tmpfs 5.0M 4.0K 5.0M 1% /run/lock. tmpfs 7.9G 0 7.9G 0% /sys/fs/cgroup. /dev/loop1 56M 56M 0 100% /snap/core18/1885. /dev/loop3 55M 55M 0 100% /snap/gtk-common-themes/1502. /dev/loop2 162M 162M 0 100% /snap/gnome-3-28-1804/128. /dev/loop4 161M 161M 0 100% /snap/gnome-3-28-1804/116. /dev/loop0 30M 30M 0 100% /snap/snapd/8790. /dev/loop5 63M 63M 0 100% /snap/gtk-common-themes/1506. /dev/loop6 55M 55M 0 100% /snap/core18/1880. /dev/loop7 30M 30M 0 100% /snap/snapd/8542. /dev/loop8 39M 39M 0 100% /snap/remmina/4309. /dev/loop9 40M 40M 0 100% /snap/remmina/4324. tmpfs 1.6G 40K 1.6G 1% /run/user/108. tmpfs 1.6G 12K 1.6G 1% /run/user/1000. $ sudo docker run \. > --user root \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > googl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:1949,safety,input,input,1949,"est.fasta.gz.fai. ucsc.hg19.chr20.unittest.fasta.gz.gzi. $ OUTPUT_DIR=""${PWD}/quickstart-output"". $ ls ${OUTPUT_DIR}. intermediate_results_dir. $ df -h. 檔案系統 容量 已用 可用 已用% 掛載點. udev 7.8G 0 7.8G 0% /dev. tmpfs 1.6G 11M 1.6G 1% /run. /dev/sda1 109G 95G 8.9G 92% /. tmpfs 7.9G 200K 7.9G 1% /dev/shm. tmpfs 5.0M 4.0K 5.0M 1% /run/lock. tmpfs 7.9G 0 7.9G 0% /sys/fs/cgroup. /dev/loop1 56M 56M 0 100% /snap/core18/1885. /dev/loop3 55M 55M 0 100% /snap/gtk-common-themes/1502. /dev/loop2 162M 162M 0 100% /snap/gnome-3-28-1804/128. /dev/loop4 161M 161M 0 100% /snap/gnome-3-28-1804/116. /dev/loop0 30M 30M 0 100% /snap/snapd/8790. /dev/loop5 63M 63M 0 100% /snap/gtk-common-themes/1506. /dev/loop6 55M 55M 0 100% /snap/core18/1880. /dev/loop7 30M 30M 0 100% /snap/snapd/8542. /dev/loop8 39M 39M 0 100% /snap/remmina/4309. /dev/loop9 40M 40M 0 100% /snap/remmina/4324. tmpfs 1.6G 40K 1.6G 1% /run/user/108. tmpfs 1.6G 12K 1.6G 1% /run/user/1000. $ sudo docker run \. > --user root \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=1 \. >. I0910 01:14:38.364422 139749579556608 run_deepvariant.py:273] Re-using the directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:2107,safety,input,input,2107,"容量 已用 可用 已用% 掛載點. udev 7.8G 0 7.8G 0% /dev. tmpfs 1.6G 11M 1.6G 1% /run. /dev/sda1 109G 95G 8.9G 92% /. tmpfs 7.9G 200K 7.9G 1% /dev/shm. tmpfs 5.0M 4.0K 5.0M 1% /run/lock. tmpfs 7.9G 0 7.9G 0% /sys/fs/cgroup. /dev/loop1 56M 56M 0 100% /snap/core18/1885. /dev/loop3 55M 55M 0 100% /snap/gtk-common-themes/1502. /dev/loop2 162M 162M 0 100% /snap/gnome-3-28-1804/128. /dev/loop4 161M 161M 0 100% /snap/gnome-3-28-1804/116. /dev/loop0 30M 30M 0 100% /snap/snapd/8790. /dev/loop5 63M 63M 0 100% /snap/gtk-common-themes/1506. /dev/loop6 55M 55M 0 100% /snap/core18/1880. /dev/loop7 30M 30M 0 100% /snap/snapd/8542. /dev/loop8 39M 39M 0 100% /snap/remmina/4309. /dev/loop9 40M 40M 0 100% /snap/remmina/4324. tmpfs 1.6G 40K 1.6G 1% /run/user/108. tmpfs 1.6G 12K 1.6G 1% /run/user/1000. $ sudo docker run \. > --user root \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=1 \. >. I0910 01:14:38.364422 139749579556608 run_deepvariant.py:273] Re-using the directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. parallel: This job failed:. /opt/deepvar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:2158,safety,input,input,2158,".6G 11M 1.6G 1% /run. /dev/sda1 109G 95G 8.9G 92% /. tmpfs 7.9G 200K 7.9G 1% /dev/shm. tmpfs 5.0M 4.0K 5.0M 1% /run/lock. tmpfs 7.9G 0 7.9G 0% /sys/fs/cgroup. /dev/loop1 56M 56M 0 100% /snap/core18/1885. /dev/loop3 55M 55M 0 100% /snap/gtk-common-themes/1502. /dev/loop2 162M 162M 0 100% /snap/gnome-3-28-1804/128. /dev/loop4 161M 161M 0 100% /snap/gnome-3-28-1804/116. /dev/loop0 30M 30M 0 100% /snap/snapd/8790. /dev/loop5 63M 63M 0 100% /snap/gtk-common-themes/1506. /dev/loop6 55M 55M 0 100% /snap/core18/1880. /dev/loop7 30M 30M 0 100% /snap/snapd/8542. /dev/loop8 39M 39M 0 100% /snap/remmina/4309. /dev/loop9 40M 40M 0 100% /snap/remmina/4324. tmpfs 1.6G 40K 1.6G 1% /run/user/108. tmpfs 1.6G 12K 1.6G 1% /run/user/1000. $ sudo docker run \. > --user root \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=1 \. >. I0910 01:14:38.364422 139749579556608 run_deepvariant.py:273] Re-using the directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:2798,safety,input,input,2798,"mina/4324. tmpfs 1.6G 40K 1.6G 1% /run/user/108. tmpfs 1.6G 12K 1.6G 1% /run/user/1000. $ sudo docker run \. > --user root \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=1 \. >. I0910 01:14:38.364422 139749579556608 run_deepvariant.py:273] Re-using the directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m1.293s. user 0m1.033s. sys 0m0.705s. I0910 01:14:39.661637 139749579556608 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:2846,safety,input,input,2846," tmpfs 1.6G 12K 1.6G 1% /run/user/1000. $ sudo docker run \. > --user root \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=1 \. >. I0910 01:14:38.364422 139749579556608 run_deepvariant.py:273] Re-using the directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m1.293s. user 0m1.033s. sys 0m0.705s. I0910 01:14:39.661637 139749579556608 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:3155,safety,input,input,3155,"input/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=1 \. >. I0910 01:14:38.364422 139749579556608 run_deepvariant.py:273] Re-using the directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m1.293s. user 0m1.033s. sys 0m0.705s. I0910 01:14:39.661637 139749579556608 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parall",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:3201,safety,input,input,3201,"gions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=1 \. >. I0910 01:14:38.364422 139749579556608 run_deepvariant.py:273] Re-using the directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m1.293s. user 0m1.033s. sys 0m0.705s. I0910 01:14:39.661637 139749579556608 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:3627,safety,modul,module,3627,"sults_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m1.293s. user 0m1.033s. sys 0m0.705s. I0910 01:14:39.661637 139749579556608 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 252. ```. I wonder is there any way to let me get the hidden parallel error message",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:4245,safety,input,input,4245,"cker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m1.293s. user 0m1.033s. sys 0m0.705s. I0910 01:14:39.661637 139749579556608 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 252. ```. I wonder is there any way to let me get the hidden parallel error message? Best,. Jerry.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:4293,safety,input,input,4293,"cker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m1.293s. user 0m1.033s. sys 0m0.705s. I0910 01:14:39.661637 139749579556608 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 252. ```. I wonder is there any way to let me get the hidden parallel error message? Best,. Jerry.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:4617,safety,error,error,4617,"cker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m1.293s. user 0m1.033s. sys 0m0.705s. I0910 01:14:39.661637 139749579556608 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 252. ```. I wonder is there any way to let me get the hidden parallel error message? Best,. Jerry.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:1277,security,lock,lock,1277,"de and message output. ```sh. $ BIN_VERSION=""1.0.0"". $ sudo docker pull google/deepvariant:""${BIN_VERSION}"". Digest: sha256:6ef8f3b4c4465e41ee7597cd2351a7c44dd8b62a849a47766316507a6234f5f8. Status: Downloaded newer image for google/deepvariant:1.0.0. docker.io/google/deepvariant:1.0.0. $ INPUT_DIR=""${PWD}/quickstart-testdata"". $ ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.bam.bai. test_nist.b37_chr20_100kbp_at_10mb.bed. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. ucsc.hg19.chr20.unittest.fasta. ucsc.hg19.chr20.unittest.fasta.fai. ucsc.hg19.chr20.unittest.fasta.gz. ucsc.hg19.chr20.unittest.fasta.gz.fai. ucsc.hg19.chr20.unittest.fasta.gz.gzi. $ OUTPUT_DIR=""${PWD}/quickstart-output"". $ ls ${OUTPUT_DIR}. intermediate_results_dir. $ df -h. 檔案系統 容量 已用 可用 已用% 掛載點. udev 7.8G 0 7.8G 0% /dev. tmpfs 1.6G 11M 1.6G 1% /run. /dev/sda1 109G 95G 8.9G 92% /. tmpfs 7.9G 200K 7.9G 1% /dev/shm. tmpfs 5.0M 4.0K 5.0M 1% /run/lock. tmpfs 7.9G 0 7.9G 0% /sys/fs/cgroup. /dev/loop1 56M 56M 0 100% /snap/core18/1885. /dev/loop3 55M 55M 0 100% /snap/gtk-common-themes/1502. /dev/loop2 162M 162M 0 100% /snap/gnome-3-28-1804/128. /dev/loop4 161M 161M 0 100% /snap/gnome-3-28-1804/116. /dev/loop0 30M 30M 0 100% /snap/snapd/8790. /dev/loop5 63M 63M 0 100% /snap/gtk-common-themes/1506. /dev/loop6 55M 55M 0 100% /snap/core18/1880. /dev/loop7 30M 30M 0 100% /snap/snapd/8542. /dev/loop8 39M 39M 0 100% /snap/remmina/4309. /dev/loop9 40M 40M 0 100% /snap/remmina/4324. tmpfs 1.6G 40K 1.6G 1% /run/user/108. tmpfs 1.6G 12K 1.6G 1% /run/user/1000. $ sudo docker run \. > --user root \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:597,testability,test,testdata,597,"Hi @pichuan ~. Thank you for this quickly reply~. I tried to add `--user root` into docker command like the solved issue #325 . But it still didn't show any additional error message like the issue . I also check the disk and it is still have about 9 Gb free space. Here is the code and message output. ```sh. $ BIN_VERSION=""1.0.0"". $ sudo docker pull google/deepvariant:""${BIN_VERSION}"". Digest: sha256:6ef8f3b4c4465e41ee7597cd2351a7c44dd8b62a849a47766316507a6234f5f8. Status: Downloaded newer image for google/deepvariant:1.0.0. docker.io/google/deepvariant:1.0.0. $ INPUT_DIR=""${PWD}/quickstart-testdata"". $ ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.bam.bai. test_nist.b37_chr20_100kbp_at_10mb.bed. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. ucsc.hg19.chr20.unittest.fasta. ucsc.hg19.chr20.unittest.fasta.fai. ucsc.hg19.chr20.unittest.fasta.gz. ucsc.hg19.chr20.unittest.fasta.gz.fai. ucsc.hg19.chr20.unittest.fasta.gz.gzi. $ OUTPUT_DIR=""${PWD}/quickstart-output"". $ ls ${OUTPUT_DIR}. intermediate_results_dir. $ df -h. 檔案系統 容量 已用 可用 已用% 掛載點. udev 7.8G 0 7.8G 0% /dev. tmpfs 1.6G 11M 1.6G 1% /run. /dev/sda1 109G 95G 8.9G 92% /. tmpfs 7.9G 200K 7.9G 1% /dev/shm. tmpfs 5.0M 4.0K 5.0M 1% /run/lock. tmpfs 7.9G 0 7.9G 0% /sys/fs/cgroup. /dev/loop1 56M 56M 0 100% /snap/core18/1885. /dev/loop3 55M 55M 0 100% /snap/gtk-common-themes/1502. /dev/loop2 162M 162M 0 100% /snap/gnome-3-28-1804/128. /dev/loop4 161M 161M 0 100% /snap/gnome-3-28-1804/116. /dev/loop0 30M 30M 0 100% /snap/snapd/8790. /dev/loop5 63M 63M 0 100% /snap/gtk-common-themes/1506. /dev/loop6 55M 55M 0 100% /snap/core18/1880. /dev/loop7 30M 30M 0 100% /snap/snapd/8542. /dev/loop8 39M 39M 0 100% /snap/remmina/4309. /dev/loop9 40M 40M 0 100% /snap/remmina/4324. tmpfs 1.6G 40K 1.6G 1% /run/user/108. tmpfs 1.6G 12K 1.6G 1% /run/user/1000. $ sudo docker run \. > --user root \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > googl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:844,testability,unit,unittest,844,"Hi @pichuan ~. Thank you for this quickly reply~. I tried to add `--user root` into docker command like the solved issue #325 . But it still didn't show any additional error message like the issue . I also check the disk and it is still have about 9 Gb free space. Here is the code and message output. ```sh. $ BIN_VERSION=""1.0.0"". $ sudo docker pull google/deepvariant:""${BIN_VERSION}"". Digest: sha256:6ef8f3b4c4465e41ee7597cd2351a7c44dd8b62a849a47766316507a6234f5f8. Status: Downloaded newer image for google/deepvariant:1.0.0. docker.io/google/deepvariant:1.0.0. $ INPUT_DIR=""${PWD}/quickstart-testdata"". $ ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.bam.bai. test_nist.b37_chr20_100kbp_at_10mb.bed. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. ucsc.hg19.chr20.unittest.fasta. ucsc.hg19.chr20.unittest.fasta.fai. ucsc.hg19.chr20.unittest.fasta.gz. ucsc.hg19.chr20.unittest.fasta.gz.fai. ucsc.hg19.chr20.unittest.fasta.gz.gzi. $ OUTPUT_DIR=""${PWD}/quickstart-output"". $ ls ${OUTPUT_DIR}. intermediate_results_dir. $ df -h. 檔案系統 容量 已用 可用 已用% 掛載點. udev 7.8G 0 7.8G 0% /dev. tmpfs 1.6G 11M 1.6G 1% /run. /dev/sda1 109G 95G 8.9G 92% /. tmpfs 7.9G 200K 7.9G 1% /dev/shm. tmpfs 5.0M 4.0K 5.0M 1% /run/lock. tmpfs 7.9G 0 7.9G 0% /sys/fs/cgroup. /dev/loop1 56M 56M 0 100% /snap/core18/1885. /dev/loop3 55M 55M 0 100% /snap/gtk-common-themes/1502. /dev/loop2 162M 162M 0 100% /snap/gnome-3-28-1804/128. /dev/loop4 161M 161M 0 100% /snap/gnome-3-28-1804/116. /dev/loop0 30M 30M 0 100% /snap/snapd/8790. /dev/loop5 63M 63M 0 100% /snap/gtk-common-themes/1506. /dev/loop6 55M 55M 0 100% /snap/core18/1880. /dev/loop7 30M 30M 0 100% /snap/snapd/8542. /dev/loop8 39M 39M 0 100% /snap/remmina/4309. /dev/loop9 40M 40M 0 100% /snap/remmina/4324. tmpfs 1.6G 40K 1.6G 1% /run/user/108. tmpfs 1.6G 12K 1.6G 1% /run/user/1000. $ sudo docker run \. > --user root \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > googl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:876,testability,unit,unittest,876,"Hi @pichuan ~. Thank you for this quickly reply~. I tried to add `--user root` into docker command like the solved issue #325 . But it still didn't show any additional error message like the issue . I also check the disk and it is still have about 9 Gb free space. Here is the code and message output. ```sh. $ BIN_VERSION=""1.0.0"". $ sudo docker pull google/deepvariant:""${BIN_VERSION}"". Digest: sha256:6ef8f3b4c4465e41ee7597cd2351a7c44dd8b62a849a47766316507a6234f5f8. Status: Downloaded newer image for google/deepvariant:1.0.0. docker.io/google/deepvariant:1.0.0. $ INPUT_DIR=""${PWD}/quickstart-testdata"". $ ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.bam.bai. test_nist.b37_chr20_100kbp_at_10mb.bed. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. ucsc.hg19.chr20.unittest.fasta. ucsc.hg19.chr20.unittest.fasta.fai. ucsc.hg19.chr20.unittest.fasta.gz. ucsc.hg19.chr20.unittest.fasta.gz.fai. ucsc.hg19.chr20.unittest.fasta.gz.gzi. $ OUTPUT_DIR=""${PWD}/quickstart-output"". $ ls ${OUTPUT_DIR}. intermediate_results_dir. $ df -h. 檔案系統 容量 已用 可用 已用% 掛載點. udev 7.8G 0 7.8G 0% /dev. tmpfs 1.6G 11M 1.6G 1% /run. /dev/sda1 109G 95G 8.9G 92% /. tmpfs 7.9G 200K 7.9G 1% /dev/shm. tmpfs 5.0M 4.0K 5.0M 1% /run/lock. tmpfs 7.9G 0 7.9G 0% /sys/fs/cgroup. /dev/loop1 56M 56M 0 100% /snap/core18/1885. /dev/loop3 55M 55M 0 100% /snap/gtk-common-themes/1502. /dev/loop2 162M 162M 0 100% /snap/gnome-3-28-1804/128. /dev/loop4 161M 161M 0 100% /snap/gnome-3-28-1804/116. /dev/loop0 30M 30M 0 100% /snap/snapd/8790. /dev/loop5 63M 63M 0 100% /snap/gtk-common-themes/1506. /dev/loop6 55M 55M 0 100% /snap/core18/1880. /dev/loop7 30M 30M 0 100% /snap/snapd/8542. /dev/loop8 39M 39M 0 100% /snap/remmina/4309. /dev/loop9 40M 40M 0 100% /snap/remmina/4324. tmpfs 1.6G 40K 1.6G 1% /run/user/108. tmpfs 1.6G 12K 1.6G 1% /run/user/1000. $ sudo docker run \. > --user root \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > googl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:912,testability,unit,unittest,912,"Hi @pichuan ~. Thank you for this quickly reply~. I tried to add `--user root` into docker command like the solved issue #325 . But it still didn't show any additional error message like the issue . I also check the disk and it is still have about 9 Gb free space. Here is the code and message output. ```sh. $ BIN_VERSION=""1.0.0"". $ sudo docker pull google/deepvariant:""${BIN_VERSION}"". Digest: sha256:6ef8f3b4c4465e41ee7597cd2351a7c44dd8b62a849a47766316507a6234f5f8. Status: Downloaded newer image for google/deepvariant:1.0.0. docker.io/google/deepvariant:1.0.0. $ INPUT_DIR=""${PWD}/quickstart-testdata"". $ ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.bam.bai. test_nist.b37_chr20_100kbp_at_10mb.bed. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. ucsc.hg19.chr20.unittest.fasta. ucsc.hg19.chr20.unittest.fasta.fai. ucsc.hg19.chr20.unittest.fasta.gz. ucsc.hg19.chr20.unittest.fasta.gz.fai. ucsc.hg19.chr20.unittest.fasta.gz.gzi. $ OUTPUT_DIR=""${PWD}/quickstart-output"". $ ls ${OUTPUT_DIR}. intermediate_results_dir. $ df -h. 檔案系統 容量 已用 可用 已用% 掛載點. udev 7.8G 0 7.8G 0% /dev. tmpfs 1.6G 11M 1.6G 1% /run. /dev/sda1 109G 95G 8.9G 92% /. tmpfs 7.9G 200K 7.9G 1% /dev/shm. tmpfs 5.0M 4.0K 5.0M 1% /run/lock. tmpfs 7.9G 0 7.9G 0% /sys/fs/cgroup. /dev/loop1 56M 56M 0 100% /snap/core18/1885. /dev/loop3 55M 55M 0 100% /snap/gtk-common-themes/1502. /dev/loop2 162M 162M 0 100% /snap/gnome-3-28-1804/128. /dev/loop4 161M 161M 0 100% /snap/gnome-3-28-1804/116. /dev/loop0 30M 30M 0 100% /snap/snapd/8790. /dev/loop5 63M 63M 0 100% /snap/gtk-common-themes/1506. /dev/loop6 55M 55M 0 100% /snap/core18/1880. /dev/loop7 30M 30M 0 100% /snap/snapd/8542. /dev/loop8 39M 39M 0 100% /snap/remmina/4309. /dev/loop9 40M 40M 0 100% /snap/remmina/4324. tmpfs 1.6G 40K 1.6G 1% /run/user/108. tmpfs 1.6G 12K 1.6G 1% /run/user/1000. $ sudo docker run \. > --user root \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > googl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:947,testability,unit,unittest,947,"Hi @pichuan ~. Thank you for this quickly reply~. I tried to add `--user root` into docker command like the solved issue #325 . But it still didn't show any additional error message like the issue . I also check the disk and it is still have about 9 Gb free space. Here is the code and message output. ```sh. $ BIN_VERSION=""1.0.0"". $ sudo docker pull google/deepvariant:""${BIN_VERSION}"". Digest: sha256:6ef8f3b4c4465e41ee7597cd2351a7c44dd8b62a849a47766316507a6234f5f8. Status: Downloaded newer image for google/deepvariant:1.0.0. docker.io/google/deepvariant:1.0.0. $ INPUT_DIR=""${PWD}/quickstart-testdata"". $ ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.bam.bai. test_nist.b37_chr20_100kbp_at_10mb.bed. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. ucsc.hg19.chr20.unittest.fasta. ucsc.hg19.chr20.unittest.fasta.fai. ucsc.hg19.chr20.unittest.fasta.gz. ucsc.hg19.chr20.unittest.fasta.gz.fai. ucsc.hg19.chr20.unittest.fasta.gz.gzi. $ OUTPUT_DIR=""${PWD}/quickstart-output"". $ ls ${OUTPUT_DIR}. intermediate_results_dir. $ df -h. 檔案系統 容量 已用 可用 已用% 掛載點. udev 7.8G 0 7.8G 0% /dev. tmpfs 1.6G 11M 1.6G 1% /run. /dev/sda1 109G 95G 8.9G 92% /. tmpfs 7.9G 200K 7.9G 1% /dev/shm. tmpfs 5.0M 4.0K 5.0M 1% /run/lock. tmpfs 7.9G 0 7.9G 0% /sys/fs/cgroup. /dev/loop1 56M 56M 0 100% /snap/core18/1885. /dev/loop3 55M 55M 0 100% /snap/gtk-common-themes/1502. /dev/loop2 162M 162M 0 100% /snap/gnome-3-28-1804/128. /dev/loop4 161M 161M 0 100% /snap/gnome-3-28-1804/116. /dev/loop0 30M 30M 0 100% /snap/snapd/8790. /dev/loop5 63M 63M 0 100% /snap/gtk-common-themes/1506. /dev/loop6 55M 55M 0 100% /snap/core18/1880. /dev/loop7 30M 30M 0 100% /snap/snapd/8542. /dev/loop8 39M 39M 0 100% /snap/remmina/4309. /dev/loop9 40M 40M 0 100% /snap/remmina/4324. tmpfs 1.6G 40K 1.6G 1% /run/user/108. tmpfs 1.6G 12K 1.6G 1% /run/user/1000. $ sudo docker run \. > --user root \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > googl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:986,testability,unit,unittest,986,"Hi @pichuan ~. Thank you for this quickly reply~. I tried to add `--user root` into docker command like the solved issue #325 . But it still didn't show any additional error message like the issue . I also check the disk and it is still have about 9 Gb free space. Here is the code and message output. ```sh. $ BIN_VERSION=""1.0.0"". $ sudo docker pull google/deepvariant:""${BIN_VERSION}"". Digest: sha256:6ef8f3b4c4465e41ee7597cd2351a7c44dd8b62a849a47766316507a6234f5f8. Status: Downloaded newer image for google/deepvariant:1.0.0. docker.io/google/deepvariant:1.0.0. $ INPUT_DIR=""${PWD}/quickstart-testdata"". $ ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.bam.bai. test_nist.b37_chr20_100kbp_at_10mb.bed. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. ucsc.hg19.chr20.unittest.fasta. ucsc.hg19.chr20.unittest.fasta.fai. ucsc.hg19.chr20.unittest.fasta.gz. ucsc.hg19.chr20.unittest.fasta.gz.fai. ucsc.hg19.chr20.unittest.fasta.gz.gzi. $ OUTPUT_DIR=""${PWD}/quickstart-output"". $ ls ${OUTPUT_DIR}. intermediate_results_dir. $ df -h. 檔案系統 容量 已用 可用 已用% 掛載點. udev 7.8G 0 7.8G 0% /dev. tmpfs 1.6G 11M 1.6G 1% /run. /dev/sda1 109G 95G 8.9G 92% /. tmpfs 7.9G 200K 7.9G 1% /dev/shm. tmpfs 5.0M 4.0K 5.0M 1% /run/lock. tmpfs 7.9G 0 7.9G 0% /sys/fs/cgroup. /dev/loop1 56M 56M 0 100% /snap/core18/1885. /dev/loop3 55M 55M 0 100% /snap/gtk-common-themes/1502. /dev/loop2 162M 162M 0 100% /snap/gnome-3-28-1804/128. /dev/loop4 161M 161M 0 100% /snap/gnome-3-28-1804/116. /dev/loop0 30M 30M 0 100% /snap/snapd/8790. /dev/loop5 63M 63M 0 100% /snap/gtk-common-themes/1506. /dev/loop6 55M 55M 0 100% /snap/core18/1880. /dev/loop7 30M 30M 0 100% /snap/snapd/8542. /dev/loop8 39M 39M 0 100% /snap/remmina/4309. /dev/loop9 40M 40M 0 100% /snap/remmina/4324. tmpfs 1.6G 40K 1.6G 1% /run/user/108. tmpfs 1.6G 12K 1.6G 1% /run/user/1000. $ sudo docker run \. > --user root \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > googl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:2129,testability,unit,unittest,2129,"7.8G 0 7.8G 0% /dev. tmpfs 1.6G 11M 1.6G 1% /run. /dev/sda1 109G 95G 8.9G 92% /. tmpfs 7.9G 200K 7.9G 1% /dev/shm. tmpfs 5.0M 4.0K 5.0M 1% /run/lock. tmpfs 7.9G 0 7.9G 0% /sys/fs/cgroup. /dev/loop1 56M 56M 0 100% /snap/core18/1885. /dev/loop3 55M 55M 0 100% /snap/gtk-common-themes/1502. /dev/loop2 162M 162M 0 100% /snap/gnome-3-28-1804/128. /dev/loop4 161M 161M 0 100% /snap/gnome-3-28-1804/116. /dev/loop0 30M 30M 0 100% /snap/snapd/8790. /dev/loop5 63M 63M 0 100% /snap/gtk-common-themes/1506. /dev/loop6 55M 55M 0 100% /snap/core18/1880. /dev/loop7 30M 30M 0 100% /snap/snapd/8542. /dev/loop8 39M 39M 0 100% /snap/remmina/4309. /dev/loop9 40M 40M 0 100% /snap/remmina/4324. tmpfs 1.6G 40K 1.6G 1% /run/user/108. tmpfs 1.6G 12K 1.6G 1% /run/user/1000. $ sudo docker run \. > --user root \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=1 \. >. I0910 01:14:38.364422 139749579556608 run_deepvariant.py:273] Re-using the directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. parallel: This job failed:. /opt/deepvariant/bin/make_examples ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:2820,testability,unit,unittest,2820,"0K 1.6G 1% /run/user/108. tmpfs 1.6G 12K 1.6G 1% /run/user/1000. $ sudo docker run \. > --user root \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=1 \. >. I0910 01:14:38.364422 139749579556608 run_deepvariant.py:273] Re-using the directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m1.293s. user 0m1.033s. sys 0m0.705s. I0910 01:14:39.661637 139749579556608 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:3177,testability,unit,unittest,3177,"10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=1 \. >. I0910 01:14:38.364422 139749579556608 run_deepvariant.py:273] Re-using the directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m1.293s. user 0m1.033s. sys 0m0.705s. I0910 01:14:39.661637 139749579556608 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:3529,testability,Trace,Traceback,3529,"/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m1.293s. user 0m1.033s. sys 0m0.705s. I0910 01:14:39.661637 139749579556608 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:4267,testability,unit,unittest,4267,"cker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m1.293s. user 0m1.033s. sys 0m0.705s. I0910 01:14:39.661637 139749579556608 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 252. ```. I wonder is there any way to let me get the hidden parallel error message? Best,. Jerry.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:68,usability,user,user,68,"Hi @pichuan ~. Thank you for this quickly reply~. I tried to add `--user root` into docker command like the solved issue #325 . But it still didn't show any additional error message like the issue . I also check the disk and it is still have about 9 Gb free space. Here is the code and message output. ```sh. $ BIN_VERSION=""1.0.0"". $ sudo docker pull google/deepvariant:""${BIN_VERSION}"". Digest: sha256:6ef8f3b4c4465e41ee7597cd2351a7c44dd8b62a849a47766316507a6234f5f8. Status: Downloaded newer image for google/deepvariant:1.0.0. docker.io/google/deepvariant:1.0.0. $ INPUT_DIR=""${PWD}/quickstart-testdata"". $ ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.bam.bai. test_nist.b37_chr20_100kbp_at_10mb.bed. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. ucsc.hg19.chr20.unittest.fasta. ucsc.hg19.chr20.unittest.fasta.fai. ucsc.hg19.chr20.unittest.fasta.gz. ucsc.hg19.chr20.unittest.fasta.gz.fai. ucsc.hg19.chr20.unittest.fasta.gz.gzi. $ OUTPUT_DIR=""${PWD}/quickstart-output"". $ ls ${OUTPUT_DIR}. intermediate_results_dir. $ df -h. 檔案系統 容量 已用 可用 已用% 掛載點. udev 7.8G 0 7.8G 0% /dev. tmpfs 1.6G 11M 1.6G 1% /run. /dev/sda1 109G 95G 8.9G 92% /. tmpfs 7.9G 200K 7.9G 1% /dev/shm. tmpfs 5.0M 4.0K 5.0M 1% /run/lock. tmpfs 7.9G 0 7.9G 0% /sys/fs/cgroup. /dev/loop1 56M 56M 0 100% /snap/core18/1885. /dev/loop3 55M 55M 0 100% /snap/gtk-common-themes/1502. /dev/loop2 162M 162M 0 100% /snap/gnome-3-28-1804/128. /dev/loop4 161M 161M 0 100% /snap/gnome-3-28-1804/116. /dev/loop0 30M 30M 0 100% /snap/snapd/8790. /dev/loop5 63M 63M 0 100% /snap/gtk-common-themes/1506. /dev/loop6 55M 55M 0 100% /snap/core18/1880. /dev/loop7 30M 30M 0 100% /snap/snapd/8542. /dev/loop8 39M 39M 0 100% /snap/remmina/4309. /dev/loop9 40M 40M 0 100% /snap/remmina/4324. tmpfs 1.6G 40K 1.6G 1% /run/user/108. tmpfs 1.6G 12K 1.6G 1% /run/user/1000. $ sudo docker run \. > --user root \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > googl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:91,usability,command,command,91,"Hi @pichuan ~. Thank you for this quickly reply~. I tried to add `--user root` into docker command like the solved issue #325 . But it still didn't show any additional error message like the issue . I also check the disk and it is still have about 9 Gb free space. Here is the code and message output. ```sh. $ BIN_VERSION=""1.0.0"". $ sudo docker pull google/deepvariant:""${BIN_VERSION}"". Digest: sha256:6ef8f3b4c4465e41ee7597cd2351a7c44dd8b62a849a47766316507a6234f5f8. Status: Downloaded newer image for google/deepvariant:1.0.0. docker.io/google/deepvariant:1.0.0. $ INPUT_DIR=""${PWD}/quickstart-testdata"". $ ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.bam.bai. test_nist.b37_chr20_100kbp_at_10mb.bed. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. ucsc.hg19.chr20.unittest.fasta. ucsc.hg19.chr20.unittest.fasta.fai. ucsc.hg19.chr20.unittest.fasta.gz. ucsc.hg19.chr20.unittest.fasta.gz.fai. ucsc.hg19.chr20.unittest.fasta.gz.gzi. $ OUTPUT_DIR=""${PWD}/quickstart-output"". $ ls ${OUTPUT_DIR}. intermediate_results_dir. $ df -h. 檔案系統 容量 已用 可用 已用% 掛載點. udev 7.8G 0 7.8G 0% /dev. tmpfs 1.6G 11M 1.6G 1% /run. /dev/sda1 109G 95G 8.9G 92% /. tmpfs 7.9G 200K 7.9G 1% /dev/shm. tmpfs 5.0M 4.0K 5.0M 1% /run/lock. tmpfs 7.9G 0 7.9G 0% /sys/fs/cgroup. /dev/loop1 56M 56M 0 100% /snap/core18/1885. /dev/loop3 55M 55M 0 100% /snap/gtk-common-themes/1502. /dev/loop2 162M 162M 0 100% /snap/gnome-3-28-1804/128. /dev/loop4 161M 161M 0 100% /snap/gnome-3-28-1804/116. /dev/loop0 30M 30M 0 100% /snap/snapd/8790. /dev/loop5 63M 63M 0 100% /snap/gtk-common-themes/1506. /dev/loop6 55M 55M 0 100% /snap/core18/1880. /dev/loop7 30M 30M 0 100% /snap/snapd/8542. /dev/loop8 39M 39M 0 100% /snap/remmina/4309. /dev/loop9 40M 40M 0 100% /snap/remmina/4324. tmpfs 1.6G 40K 1.6G 1% /run/user/108. tmpfs 1.6G 12K 1.6G 1% /run/user/1000. $ sudo docker run \. > --user root \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > googl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:168,usability,error,error,168,"Hi @pichuan ~. Thank you for this quickly reply~. I tried to add `--user root` into docker command like the solved issue #325 . But it still didn't show any additional error message like the issue . I also check the disk and it is still have about 9 Gb free space. Here is the code and message output. ```sh. $ BIN_VERSION=""1.0.0"". $ sudo docker pull google/deepvariant:""${BIN_VERSION}"". Digest: sha256:6ef8f3b4c4465e41ee7597cd2351a7c44dd8b62a849a47766316507a6234f5f8. Status: Downloaded newer image for google/deepvariant:1.0.0. docker.io/google/deepvariant:1.0.0. $ INPUT_DIR=""${PWD}/quickstart-testdata"". $ ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.bam.bai. test_nist.b37_chr20_100kbp_at_10mb.bed. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. ucsc.hg19.chr20.unittest.fasta. ucsc.hg19.chr20.unittest.fasta.fai. ucsc.hg19.chr20.unittest.fasta.gz. ucsc.hg19.chr20.unittest.fasta.gz.fai. ucsc.hg19.chr20.unittest.fasta.gz.gzi. $ OUTPUT_DIR=""${PWD}/quickstart-output"". $ ls ${OUTPUT_DIR}. intermediate_results_dir. $ df -h. 檔案系統 容量 已用 可用 已用% 掛載點. udev 7.8G 0 7.8G 0% /dev. tmpfs 1.6G 11M 1.6G 1% /run. /dev/sda1 109G 95G 8.9G 92% /. tmpfs 7.9G 200K 7.9G 1% /dev/shm. tmpfs 5.0M 4.0K 5.0M 1% /run/lock. tmpfs 7.9G 0 7.9G 0% /sys/fs/cgroup. /dev/loop1 56M 56M 0 100% /snap/core18/1885. /dev/loop3 55M 55M 0 100% /snap/gtk-common-themes/1502. /dev/loop2 162M 162M 0 100% /snap/gnome-3-28-1804/128. /dev/loop4 161M 161M 0 100% /snap/gnome-3-28-1804/116. /dev/loop0 30M 30M 0 100% /snap/snapd/8790. /dev/loop5 63M 63M 0 100% /snap/gtk-common-themes/1506. /dev/loop6 55M 55M 0 100% /snap/core18/1880. /dev/loop7 30M 30M 0 100% /snap/snapd/8542. /dev/loop8 39M 39M 0 100% /snap/remmina/4309. /dev/loop9 40M 40M 0 100% /snap/remmina/4324. tmpfs 1.6G 40K 1.6G 1% /run/user/108. tmpfs 1.6G 12K 1.6G 1% /run/user/1000. $ sudo docker run \. > --user root \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > googl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:469,usability,Statu,Status,469,"Hi @pichuan ~. Thank you for this quickly reply~. I tried to add `--user root` into docker command like the solved issue #325 . But it still didn't show any additional error message like the issue . I also check the disk and it is still have about 9 Gb free space. Here is the code and message output. ```sh. $ BIN_VERSION=""1.0.0"". $ sudo docker pull google/deepvariant:""${BIN_VERSION}"". Digest: sha256:6ef8f3b4c4465e41ee7597cd2351a7c44dd8b62a849a47766316507a6234f5f8. Status: Downloaded newer image for google/deepvariant:1.0.0. docker.io/google/deepvariant:1.0.0. $ INPUT_DIR=""${PWD}/quickstart-testdata"". $ ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.bam.bai. test_nist.b37_chr20_100kbp_at_10mb.bed. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. ucsc.hg19.chr20.unittest.fasta. ucsc.hg19.chr20.unittest.fasta.fai. ucsc.hg19.chr20.unittest.fasta.gz. ucsc.hg19.chr20.unittest.fasta.gz.fai. ucsc.hg19.chr20.unittest.fasta.gz.gzi. $ OUTPUT_DIR=""${PWD}/quickstart-output"". $ ls ${OUTPUT_DIR}. intermediate_results_dir. $ df -h. 檔案系統 容量 已用 可用 已用% 掛載點. udev 7.8G 0 7.8G 0% /dev. tmpfs 1.6G 11M 1.6G 1% /run. /dev/sda1 109G 95G 8.9G 92% /. tmpfs 7.9G 200K 7.9G 1% /dev/shm. tmpfs 5.0M 4.0K 5.0M 1% /run/lock. tmpfs 7.9G 0 7.9G 0% /sys/fs/cgroup. /dev/loop1 56M 56M 0 100% /snap/core18/1885. /dev/loop3 55M 55M 0 100% /snap/gtk-common-themes/1502. /dev/loop2 162M 162M 0 100% /snap/gnome-3-28-1804/128. /dev/loop4 161M 161M 0 100% /snap/gnome-3-28-1804/116. /dev/loop0 30M 30M 0 100% /snap/snapd/8790. /dev/loop5 63M 63M 0 100% /snap/gtk-common-themes/1506. /dev/loop6 55M 55M 0 100% /snap/core18/1880. /dev/loop7 30M 30M 0 100% /snap/snapd/8542. /dev/loop8 39M 39M 0 100% /snap/remmina/4309. /dev/loop9 40M 40M 0 100% /snap/remmina/4324. tmpfs 1.6G 40K 1.6G 1% /run/user/108. tmpfs 1.6G 12K 1.6G 1% /run/user/1000. $ sudo docker run \. > --user root \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > googl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:1840,usability,user,user,1840,"0.unittest.fasta. ucsc.hg19.chr20.unittest.fasta.fai. ucsc.hg19.chr20.unittest.fasta.gz. ucsc.hg19.chr20.unittest.fasta.gz.fai. ucsc.hg19.chr20.unittest.fasta.gz.gzi. $ OUTPUT_DIR=""${PWD}/quickstart-output"". $ ls ${OUTPUT_DIR}. intermediate_results_dir. $ df -h. 檔案系統 容量 已用 可用 已用% 掛載點. udev 7.8G 0 7.8G 0% /dev. tmpfs 1.6G 11M 1.6G 1% /run. /dev/sda1 109G 95G 8.9G 92% /. tmpfs 7.9G 200K 7.9G 1% /dev/shm. tmpfs 5.0M 4.0K 5.0M 1% /run/lock. tmpfs 7.9G 0 7.9G 0% /sys/fs/cgroup. /dev/loop1 56M 56M 0 100% /snap/core18/1885. /dev/loop3 55M 55M 0 100% /snap/gtk-common-themes/1502. /dev/loop2 162M 162M 0 100% /snap/gnome-3-28-1804/128. /dev/loop4 161M 161M 0 100% /snap/gnome-3-28-1804/116. /dev/loop0 30M 30M 0 100% /snap/snapd/8790. /dev/loop5 63M 63M 0 100% /snap/gtk-common-themes/1506. /dev/loop6 55M 55M 0 100% /snap/core18/1880. /dev/loop7 30M 30M 0 100% /snap/snapd/8542. /dev/loop8 39M 39M 0 100% /snap/remmina/4309. /dev/loop9 40M 40M 0 100% /snap/remmina/4324. tmpfs 1.6G 40K 1.6G 1% /run/user/108. tmpfs 1.6G 12K 1.6G 1% /run/user/1000. $ sudo docker run \. > --user root \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=1 \. >. I0910 01:14:38.364422 139749579556608 run_deepvariant.py:273] Re-using the directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --read",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:1878,usability,user,user,1878,"test.fasta.fai. ucsc.hg19.chr20.unittest.fasta.gz. ucsc.hg19.chr20.unittest.fasta.gz.fai. ucsc.hg19.chr20.unittest.fasta.gz.gzi. $ OUTPUT_DIR=""${PWD}/quickstart-output"". $ ls ${OUTPUT_DIR}. intermediate_results_dir. $ df -h. 檔案系統 容量 已用 可用 已用% 掛載點. udev 7.8G 0 7.8G 0% /dev. tmpfs 1.6G 11M 1.6G 1% /run. /dev/sda1 109G 95G 8.9G 92% /. tmpfs 7.9G 200K 7.9G 1% /dev/shm. tmpfs 5.0M 4.0K 5.0M 1% /run/lock. tmpfs 7.9G 0 7.9G 0% /sys/fs/cgroup. /dev/loop1 56M 56M 0 100% /snap/core18/1885. /dev/loop3 55M 55M 0 100% /snap/gtk-common-themes/1502. /dev/loop2 162M 162M 0 100% /snap/gnome-3-28-1804/128. /dev/loop4 161M 161M 0 100% /snap/gnome-3-28-1804/116. /dev/loop0 30M 30M 0 100% /snap/snapd/8790. /dev/loop5 63M 63M 0 100% /snap/gtk-common-themes/1506. /dev/loop6 55M 55M 0 100% /snap/core18/1880. /dev/loop7 30M 30M 0 100% /snap/snapd/8542. /dev/loop8 39M 39M 0 100% /snap/remmina/4309. /dev/loop9 40M 40M 0 100% /snap/remmina/4324. tmpfs 1.6G 40K 1.6G 1% /run/user/108. tmpfs 1.6G 12K 1.6G 1% /run/user/1000. $ sudo docker run \. > --user root \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=1 \. >. I0910 01:14:38.364422 139749579556608 run_deepvariant.py:273] Re-using the directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:1914,usability,user,user,1914,"test.fasta.gz. ucsc.hg19.chr20.unittest.fasta.gz.fai. ucsc.hg19.chr20.unittest.fasta.gz.gzi. $ OUTPUT_DIR=""${PWD}/quickstart-output"". $ ls ${OUTPUT_DIR}. intermediate_results_dir. $ df -h. 檔案系統 容量 已用 可用 已用% 掛載點. udev 7.8G 0 7.8G 0% /dev. tmpfs 1.6G 11M 1.6G 1% /run. /dev/sda1 109G 95G 8.9G 92% /. tmpfs 7.9G 200K 7.9G 1% /dev/shm. tmpfs 5.0M 4.0K 5.0M 1% /run/lock. tmpfs 7.9G 0 7.9G 0% /sys/fs/cgroup. /dev/loop1 56M 56M 0 100% /snap/core18/1885. /dev/loop3 55M 55M 0 100% /snap/gtk-common-themes/1502. /dev/loop2 162M 162M 0 100% /snap/gnome-3-28-1804/128. /dev/loop4 161M 161M 0 100% /snap/gnome-3-28-1804/116. /dev/loop0 30M 30M 0 100% /snap/snapd/8790. /dev/loop5 63M 63M 0 100% /snap/gtk-common-themes/1506. /dev/loop6 55M 55M 0 100% /snap/core18/1880. /dev/loop7 30M 30M 0 100% /snap/snapd/8542. /dev/loop8 39M 39M 0 100% /snap/remmina/4309. /dev/loop9 40M 40M 0 100% /snap/remmina/4324. tmpfs 1.6G 40K 1.6G 1% /run/user/108. tmpfs 1.6G 12K 1.6G 1% /run/user/1000. $ sudo docker run \. > --user root \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=1 \. >. I0910 01:14:38.364422 139749579556608 run_deepvariant.py:273] Re-using the directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:1949,usability,input,input,1949,"est.fasta.gz.fai. ucsc.hg19.chr20.unittest.fasta.gz.gzi. $ OUTPUT_DIR=""${PWD}/quickstart-output"". $ ls ${OUTPUT_DIR}. intermediate_results_dir. $ df -h. 檔案系統 容量 已用 可用 已用% 掛載點. udev 7.8G 0 7.8G 0% /dev. tmpfs 1.6G 11M 1.6G 1% /run. /dev/sda1 109G 95G 8.9G 92% /. tmpfs 7.9G 200K 7.9G 1% /dev/shm. tmpfs 5.0M 4.0K 5.0M 1% /run/lock. tmpfs 7.9G 0 7.9G 0% /sys/fs/cgroup. /dev/loop1 56M 56M 0 100% /snap/core18/1885. /dev/loop3 55M 55M 0 100% /snap/gtk-common-themes/1502. /dev/loop2 162M 162M 0 100% /snap/gnome-3-28-1804/128. /dev/loop4 161M 161M 0 100% /snap/gnome-3-28-1804/116. /dev/loop0 30M 30M 0 100% /snap/snapd/8790. /dev/loop5 63M 63M 0 100% /snap/gtk-common-themes/1506. /dev/loop6 55M 55M 0 100% /snap/core18/1880. /dev/loop7 30M 30M 0 100% /snap/snapd/8542. /dev/loop8 39M 39M 0 100% /snap/remmina/4309. /dev/loop9 40M 40M 0 100% /snap/remmina/4324. tmpfs 1.6G 40K 1.6G 1% /run/user/108. tmpfs 1.6G 12K 1.6G 1% /run/user/1000. $ sudo docker run \. > --user root \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=1 \. >. I0910 01:14:38.364422 139749579556608 run_deepvariant.py:273] Re-using the directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:2107,usability,input,input,2107,"容量 已用 可用 已用% 掛載點. udev 7.8G 0 7.8G 0% /dev. tmpfs 1.6G 11M 1.6G 1% /run. /dev/sda1 109G 95G 8.9G 92% /. tmpfs 7.9G 200K 7.9G 1% /dev/shm. tmpfs 5.0M 4.0K 5.0M 1% /run/lock. tmpfs 7.9G 0 7.9G 0% /sys/fs/cgroup. /dev/loop1 56M 56M 0 100% /snap/core18/1885. /dev/loop3 55M 55M 0 100% /snap/gtk-common-themes/1502. /dev/loop2 162M 162M 0 100% /snap/gnome-3-28-1804/128. /dev/loop4 161M 161M 0 100% /snap/gnome-3-28-1804/116. /dev/loop0 30M 30M 0 100% /snap/snapd/8790. /dev/loop5 63M 63M 0 100% /snap/gtk-common-themes/1506. /dev/loop6 55M 55M 0 100% /snap/core18/1880. /dev/loop7 30M 30M 0 100% /snap/snapd/8542. /dev/loop8 39M 39M 0 100% /snap/remmina/4309. /dev/loop9 40M 40M 0 100% /snap/remmina/4324. tmpfs 1.6G 40K 1.6G 1% /run/user/108. tmpfs 1.6G 12K 1.6G 1% /run/user/1000. $ sudo docker run \. > --user root \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=1 \. >. I0910 01:14:38.364422 139749579556608 run_deepvariant.py:273] Re-using the directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. parallel: This job failed:. /opt/deepvar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:2158,usability,input,input,2158,".6G 11M 1.6G 1% /run. /dev/sda1 109G 95G 8.9G 92% /. tmpfs 7.9G 200K 7.9G 1% /dev/shm. tmpfs 5.0M 4.0K 5.0M 1% /run/lock. tmpfs 7.9G 0 7.9G 0% /sys/fs/cgroup. /dev/loop1 56M 56M 0 100% /snap/core18/1885. /dev/loop3 55M 55M 0 100% /snap/gtk-common-themes/1502. /dev/loop2 162M 162M 0 100% /snap/gnome-3-28-1804/128. /dev/loop4 161M 161M 0 100% /snap/gnome-3-28-1804/116. /dev/loop0 30M 30M 0 100% /snap/snapd/8790. /dev/loop5 63M 63M 0 100% /snap/gtk-common-themes/1506. /dev/loop6 55M 55M 0 100% /snap/core18/1880. /dev/loop7 30M 30M 0 100% /snap/snapd/8542. /dev/loop8 39M 39M 0 100% /snap/remmina/4309. /dev/loop9 40M 40M 0 100% /snap/remmina/4324. tmpfs 1.6G 40K 1.6G 1% /run/user/108. tmpfs 1.6G 12K 1.6G 1% /run/user/1000. $ sudo docker run \. > --user root \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=1 \. >. I0910 01:14:38.364422 139749579556608 run_deepvariant.py:273] Re-using the directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:2675,usability,command,command,2675,"v/loop7 30M 30M 0 100% /snap/snapd/8542. /dev/loop8 39M 39M 0 100% /snap/remmina/4309. /dev/loop9 40M 40M 0 100% /snap/remmina/4324. tmpfs 1.6G 40K 1.6G 1% /run/user/108. tmpfs 1.6G 12K 1.6G 1% /run/user/1000. $ sudo docker run \. > --user root \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=1 \. >. I0910 01:14:38.364422 139749579556608 run_deepvariant.py:273] Re-using the directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m1.293s. user 0m1.033s. sys 0m0.705s. I0910 01:14:39.661637 139749579556608 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:2798,usability,input,input,2798,"mina/4324. tmpfs 1.6G 40K 1.6G 1% /run/user/108. tmpfs 1.6G 12K 1.6G 1% /run/user/1000. $ sudo docker run \. > --user root \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=1 \. >. I0910 01:14:38.364422 139749579556608 run_deepvariant.py:273] Re-using the directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m1.293s. user 0m1.033s. sys 0m0.705s. I0910 01:14:39.661637 139749579556608 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:2846,usability,input,input,2846," tmpfs 1.6G 12K 1.6G 1% /run/user/1000. $ sudo docker run \. > --user root \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=1 \. >. I0910 01:14:38.364422 139749579556608 run_deepvariant.py:273] Re-using the directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m1.293s. user 0m1.033s. sys 0m0.705s. I0910 01:14:39.661637 139749579556608 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:3155,usability,input,input,3155,"input/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=1 \. >. I0910 01:14:38.364422 139749579556608 run_deepvariant.py:273] Re-using the directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m1.293s. user 0m1.033s. sys 0m0.705s. I0910 01:14:39.661637 139749579556608 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parall",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:3201,usability,input,input,3201,"gions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=1 \. >. I0910 01:14:38.364422 139749579556608 run_deepvariant.py:273] Re-using the directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m1.293s. user 0m1.033s. sys 0m0.705s. I0910 01:14:39.661637 139749579556608 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:3432,usability,user,user,3432,"2 139749579556608 run_deepvariant.py:273] Re-using the directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m1.293s. user 0m1.033s. sys 0m0.705s. I0910 01:14:39.661637 139749579556608 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:3945,usability,command,command,3945,"cker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m1.293s. user 0m1.033s. sys 0m0.705s. I0910 01:14:39.661637 139749579556608 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 252. ```. I wonder is there any way to let me get the hidden parallel error message? Best,. Jerry.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:4128,usability,Command,Command,4128,"cker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m1.293s. user 0m1.033s. sys 0m0.705s. I0910 01:14:39.661637 139749579556608 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 252. ```. I wonder is there any way to let me get the hidden parallel error message? Best,. Jerry.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:4245,usability,input,input,4245,"cker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m1.293s. user 0m1.033s. sys 0m0.705s. I0910 01:14:39.661637 139749579556608 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 252. ```. I wonder is there any way to let me get the hidden parallel error message? Best,. Jerry.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:4293,usability,input,input,4293,"cker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m1.293s. user 0m1.033s. sys 0m0.705s. I0910 01:14:39.661637 139749579556608 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 252. ```. I wonder is there any way to let me get the hidden parallel error message? Best,. Jerry.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:4540,usability,statu,status,4540,"cker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m1.293s. user 0m1.033s. sys 0m0.705s. I0910 01:14:39.661637 139749579556608 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 252. ```. I wonder is there any way to let me get the hidden parallel error message? Best,. Jerry.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:4617,usability,error,error,4617,"cker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. parallel: This job failed:. /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m1.293s. user 0m1.033s. sys 0m0.705s. I0910 01:14:39.661637 139749579556608 run_deepvariant.py:364] None. Traceback (most recent call last):. File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>. app.run(main). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run. _run_main(main, args). File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main. sys.exit(main(argv)). File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main. subprocess.check_call(command, shell=True, executable='/bin/bash'). File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call. raise CalledProcessError(retcode, cmd). subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 252. ```. I wonder is there any way to let me get the hidden parallel error message? Best,. Jerry.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:66,safety,input,input,66,"Can you try running:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta \. --reads /input/NA12878_S1.chr20.10_10p1mb.bam \. --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz \. --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz \. --regions chr20:10,000,000-10,010,000 \. --task 0. ```. directly?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:207,safety,input,input,207,"Can you try running:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta \. --reads /input/NA12878_S1.chr20.10_10p1mb.bam \. --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz \. --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz \. --regions chr20:10,000,000-10,010,000 \. --task 0. ```. directly?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:256,safety,input,input,256,"Can you try running:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta \. --reads /input/NA12878_S1.chr20.10_10p1mb.bam \. --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz \. --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz \. --regions chr20:10,000,000-10,010,000 \. --task 0. ```. directly?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:229,testability,unit,unittest,229,"Can you try running:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta \. --reads /input/NA12878_S1.chr20.10_10p1mb.bam \. --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz \. --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz \. --regions chr20:10,000,000-10,010,000 \. --task 0. ```. directly?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:66,usability,input,input,66,"Can you try running:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta \. --reads /input/NA12878_S1.chr20.10_10p1mb.bam \. --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz \. --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz \. --regions chr20:10,000,000-10,010,000 \. --task 0. ```. directly?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:207,usability,input,input,207,"Can you try running:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta \. --reads /input/NA12878_S1.chr20.10_10p1mb.bam \. --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz \. --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz \. --regions chr20:10,000,000-10,010,000 \. --task 0. ```. directly?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:256,usability,input,input,256,"Can you try running:. ```. sudo docker run \. -v ""${INPUT_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta \. --reads /input/NA12878_S1.chr20.10_10p1mb.bam \. --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz \. --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz \. --regions chr20:10,000,000-10,010,000 \. --task 0. ```. directly?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:66,availability,error,error,66,"Hi @pichuan ~. I tried the code you provided but it print nothing error. There is also nothing output file in the output folder. Here is the code:. ```sh. $ sudo docker run \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/make_examples \. > --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta \. > --reads /input/NA12878_S1.chr20.10_10p1mb.bam \. > --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz \. > --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz \. > --regions chr20:10,000,000-10,010,000 \. > --task 0. $ ls. quickstart-output quickstart-testdata. $ ls quickstart-output/. intermediate_results_dir. ```. Best,. Jerry.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:66,performance,error,error,66,"Hi @pichuan ~. I tried the code you provided but it print nothing error. There is also nothing output file in the output folder. Here is the code:. ```sh. $ sudo docker run \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/make_examples \. > --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta \. > --reads /input/NA12878_S1.chr20.10_10p1mb.bam \. > --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz \. > --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz \. > --regions chr20:10,000,000-10,010,000 \. > --task 0. $ ls. quickstart-output quickstart-testdata. $ ls quickstart-output/. intermediate_results_dir. ```. Best,. Jerry.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:66,safety,error,error,66,"Hi @pichuan ~. I tried the code you provided but it print nothing error. There is also nothing output file in the output folder. Here is the code:. ```sh. $ sudo docker run \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/make_examples \. > --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta \. > --reads /input/NA12878_S1.chr20.10_10p1mb.bam \. > --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz \. > --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz \. > --regions chr20:10,000,000-10,010,000 \. > --task 0. $ ls. quickstart-output quickstart-testdata. $ ls quickstart-output/. intermediate_results_dir. ```. Best,. Jerry.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:198,safety,input,input,198,"Hi @pichuan ~. I tried the code you provided but it print nothing error. There is also nothing output file in the output folder. Here is the code:. ```sh. $ sudo docker run \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/make_examples \. > --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta \. > --reads /input/NA12878_S1.chr20.10_10p1mb.bam \. > --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz \. > --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz \. > --regions chr20:10,000,000-10,010,000 \. > --task 0. $ ls. quickstart-output quickstart-testdata. $ ls quickstart-output/. intermediate_results_dir. ```. Best,. Jerry.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:347,safety,input,input,347,"Hi @pichuan ~. I tried the code you provided but it print nothing error. There is also nothing output file in the output folder. Here is the code:. ```sh. $ sudo docker run \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/make_examples \. > --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta \. > --reads /input/NA12878_S1.chr20.10_10p1mb.bam \. > --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz \. > --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz \. > --regions chr20:10,000,000-10,010,000 \. > --task 0. $ ls. quickstart-output quickstart-testdata. $ ls quickstart-output/. intermediate_results_dir. ```. Best,. Jerry.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:398,safety,input,input,398,"Hi @pichuan ~. I tried the code you provided but it print nothing error. There is also nothing output file in the output folder. Here is the code:. ```sh. $ sudo docker run \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/make_examples \. > --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta \. > --reads /input/NA12878_S1.chr20.10_10p1mb.bam \. > --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz \. > --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz \. > --regions chr20:10,000,000-10,010,000 \. > --task 0. $ ls. quickstart-output quickstart-testdata. $ ls quickstart-output/. intermediate_results_dir. ```. Best,. Jerry.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:669,safety,test,testdata,669,"Hi @pichuan ~. I tried the code you provided but it print nothing error. There is also nothing output file in the output folder. Here is the code:. ```sh. $ sudo docker run \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/make_examples \. > --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta \. > --reads /input/NA12878_S1.chr20.10_10p1mb.bam \. > --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz \. > --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz \. > --regions chr20:10,000,000-10,010,000 \. > --task 0. $ ls. quickstart-output quickstart-testdata. $ ls quickstart-output/. intermediate_results_dir. ```. Best,. Jerry.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:369,testability,unit,unittest,369,"Hi @pichuan ~. I tried the code you provided but it print nothing error. There is also nothing output file in the output folder. Here is the code:. ```sh. $ sudo docker run \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/make_examples \. > --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta \. > --reads /input/NA12878_S1.chr20.10_10p1mb.bam \. > --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz \. > --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz \. > --regions chr20:10,000,000-10,010,000 \. > --task 0. $ ls. quickstart-output quickstart-testdata. $ ls quickstart-output/. intermediate_results_dir. ```. Best,. Jerry.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:669,testability,test,testdata,669,"Hi @pichuan ~. I tried the code you provided but it print nothing error. There is also nothing output file in the output folder. Here is the code:. ```sh. $ sudo docker run \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/make_examples \. > --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta \. > --reads /input/NA12878_S1.chr20.10_10p1mb.bam \. > --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz \. > --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz \. > --regions chr20:10,000,000-10,010,000 \. > --task 0. $ ls. quickstart-output quickstart-testdata. $ ls quickstart-output/. intermediate_results_dir. ```. Best,. Jerry.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:66,usability,error,error,66,"Hi @pichuan ~. I tried the code you provided but it print nothing error. There is also nothing output file in the output folder. Here is the code:. ```sh. $ sudo docker run \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/make_examples \. > --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta \. > --reads /input/NA12878_S1.chr20.10_10p1mb.bam \. > --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz \. > --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz \. > --regions chr20:10,000,000-10,010,000 \. > --task 0. $ ls. quickstart-output quickstart-testdata. $ ls quickstart-output/. intermediate_results_dir. ```. Best,. Jerry.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:198,usability,input,input,198,"Hi @pichuan ~. I tried the code you provided but it print nothing error. There is also nothing output file in the output folder. Here is the code:. ```sh. $ sudo docker run \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/make_examples \. > --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta \. > --reads /input/NA12878_S1.chr20.10_10p1mb.bam \. > --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz \. > --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz \. > --regions chr20:10,000,000-10,010,000 \. > --task 0. $ ls. quickstart-output quickstart-testdata. $ ls quickstart-output/. intermediate_results_dir. ```. Best,. Jerry.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:347,usability,input,input,347,"Hi @pichuan ~. I tried the code you provided but it print nothing error. There is also nothing output file in the output folder. Here is the code:. ```sh. $ sudo docker run \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/make_examples \. > --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta \. > --reads /input/NA12878_S1.chr20.10_10p1mb.bam \. > --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz \. > --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz \. > --regions chr20:10,000,000-10,010,000 \. > --task 0. $ ls. quickstart-output quickstart-testdata. $ ls quickstart-output/. intermediate_results_dir. ```. Best,. Jerry.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:398,usability,input,input,398,"Hi @pichuan ~. I tried the code you provided but it print nothing error. There is also nothing output file in the output folder. Here is the code:. ```sh. $ sudo docker run \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/make_examples \. > --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta \. > --reads /input/NA12878_S1.chr20.10_10p1mb.bam \. > --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz \. > --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz \. > --regions chr20:10,000,000-10,010,000 \. > --task 0. $ ls. quickstart-output quickstart-testdata. $ ls quickstart-output/. intermediate_results_dir. ```. Best,. Jerry.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:90,availability,error,error,90,"Hi @sh940202123 , it's not obvious to me why this might be the case. It's strange that no error message comes out at all. Can you try something even simpler, like:. ```. sudo docker run google/deepvariant:1.0.0 /opt/deepvariant/bin/make_examples --help. ```. And see if the information comes out correctly? I'll also check with my team member tomorrow to see if anyone has other suggestions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:96,integrability,messag,message,96,"Hi @sh940202123 , it's not obvious to me why this might be the case. It's strange that no error message comes out at all. Can you try something even simpler, like:. ```. sudo docker run google/deepvariant:1.0.0 /opt/deepvariant/bin/make_examples --help. ```. And see if the information comes out correctly? I'll also check with my team member tomorrow to see if anyone has other suggestions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:96,interoperability,messag,message,96,"Hi @sh940202123 , it's not obvious to me why this might be the case. It's strange that no error message comes out at all. Can you try something even simpler, like:. ```. sudo docker run google/deepvariant:1.0.0 /opt/deepvariant/bin/make_examples --help. ```. And see if the information comes out correctly? I'll also check with my team member tomorrow to see if anyone has other suggestions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:90,performance,error,error,90,"Hi @sh940202123 , it's not obvious to me why this might be the case. It's strange that no error message comes out at all. Can you try something even simpler, like:. ```. sudo docker run google/deepvariant:1.0.0 /opt/deepvariant/bin/make_examples --help. ```. And see if the information comes out correctly? I'll also check with my team member tomorrow to see if anyone has other suggestions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:90,safety,error,error,90,"Hi @sh940202123 , it's not obvious to me why this might be the case. It's strange that no error message comes out at all. Can you try something even simpler, like:. ```. sudo docker run google/deepvariant:1.0.0 /opt/deepvariant/bin/make_examples --help. ```. And see if the information comes out correctly? I'll also check with my team member tomorrow to see if anyone has other suggestions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:331,security,team,team,331,"Hi @sh940202123 , it's not obvious to me why this might be the case. It's strange that no error message comes out at all. Can you try something even simpler, like:. ```. sudo docker run google/deepvariant:1.0.0 /opt/deepvariant/bin/make_examples --help. ```. And see if the information comes out correctly? I'll also check with my team member tomorrow to see if anyone has other suggestions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:149,testability,simpl,simpler,149,"Hi @sh940202123 , it's not obvious to me why this might be the case. It's strange that no error message comes out at all. Can you try something even simpler, like:. ```. sudo docker run google/deepvariant:1.0.0 /opt/deepvariant/bin/make_examples --help. ```. And see if the information comes out correctly? I'll also check with my team member tomorrow to see if anyone has other suggestions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:90,usability,error,error,90,"Hi @sh940202123 , it's not obvious to me why this might be the case. It's strange that no error message comes out at all. Can you try something even simpler, like:. ```. sudo docker run google/deepvariant:1.0.0 /opt/deepvariant/bin/make_examples --help. ```. And see if the information comes out correctly? I'll also check with my team member tomorrow to see if anyone has other suggestions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:149,usability,simpl,simpler,149,"Hi @sh940202123 , it's not obvious to me why this might be the case. It's strange that no error message comes out at all. Can you try something even simpler, like:. ```. sudo docker run google/deepvariant:1.0.0 /opt/deepvariant/bin/make_examples --help. ```. And see if the information comes out correctly? I'll also check with my team member tomorrow to see if anyone has other suggestions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:248,usability,help,help,248,"Hi @sh940202123 , it's not obvious to me why this might be the case. It's strange that no error message comes out at all. Can you try something even simpler, like:. ```. sudo docker run google/deepvariant:1.0.0 /opt/deepvariant/bin/make_examples --help. ```. And see if the information comes out correctly? I'll also check with my team member tomorrow to see if anyone has other suggestions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:521,availability,error,error,521,"Hi @sh940202123 it seems the CPU on the host `Intel(R) Core(TM) i7 CPU 870 @ 2.93GHz` doesn't support AVX instruction ( https://en.wikipedia.org/wiki/List_of_Intel_Core_i7_microprocessors#%22Lynnfield%22_(45_nm) ), which is required for the TensorFlow release version we use in the docker image. Please see this previous comment https://github.com/google/deepvariant/issues/217#issuecomment-530580878 for more details. That being said, I do find it strange that it's silently not producing outputs, instead of showing an error message about the unsupported instruction.. Is it possible for you to try the same command in a machine with newer CPU? For Intel CPUs, _Sandy Bridge_ (released in 2011) or later architectures should support AVX. https://en.wikipedia.org/wiki/List_of_Intel_Core_i7_microprocessors#Sandy_Bridge_microarchitecture_(2nd_generation)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:252,deployability,releas,release,252,"Hi @sh940202123 it seems the CPU on the host `Intel(R) Core(TM) i7 CPU 870 @ 2.93GHz` doesn't support AVX instruction ( https://en.wikipedia.org/wiki/List_of_Intel_Core_i7_microprocessors#%22Lynnfield%22_(45_nm) ), which is required for the TensorFlow release version we use in the docker image. Please see this previous comment https://github.com/google/deepvariant/issues/217#issuecomment-530580878 for more details. That being said, I do find it strange that it's silently not producing outputs, instead of showing an error message about the unsupported instruction.. Is it possible for you to try the same command in a machine with newer CPU? For Intel CPUs, _Sandy Bridge_ (released in 2011) or later architectures should support AVX. https://en.wikipedia.org/wiki/List_of_Intel_Core_i7_microprocessors#Sandy_Bridge_microarchitecture_(2nd_generation)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:260,deployability,version,version,260,"Hi @sh940202123 it seems the CPU on the host `Intel(R) Core(TM) i7 CPU 870 @ 2.93GHz` doesn't support AVX instruction ( https://en.wikipedia.org/wiki/List_of_Intel_Core_i7_microprocessors#%22Lynnfield%22_(45_nm) ), which is required for the TensorFlow release version we use in the docker image. Please see this previous comment https://github.com/google/deepvariant/issues/217#issuecomment-530580878 for more details. That being said, I do find it strange that it's silently not producing outputs, instead of showing an error message about the unsupported instruction.. Is it possible for you to try the same command in a machine with newer CPU? For Intel CPUs, _Sandy Bridge_ (released in 2011) or later architectures should support AVX. https://en.wikipedia.org/wiki/List_of_Intel_Core_i7_microprocessors#Sandy_Bridge_microarchitecture_(2nd_generation)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:679,deployability,releas,released,679,"Hi @sh940202123 it seems the CPU on the host `Intel(R) Core(TM) i7 CPU 870 @ 2.93GHz` doesn't support AVX instruction ( https://en.wikipedia.org/wiki/List_of_Intel_Core_i7_microprocessors#%22Lynnfield%22_(45_nm) ), which is required for the TensorFlow release version we use in the docker image. Please see this previous comment https://github.com/google/deepvariant/issues/217#issuecomment-530580878 for more details. That being said, I do find it strange that it's silently not producing outputs, instead of showing an error message about the unsupported instruction.. Is it possible for you to try the same command in a machine with newer CPU? For Intel CPUs, _Sandy Bridge_ (released in 2011) or later architectures should support AVX. https://en.wikipedia.org/wiki/List_of_Intel_Core_i7_microprocessors#Sandy_Bridge_microarchitecture_(2nd_generation)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:29,energy efficiency,CPU,CPU,29,"Hi @sh940202123 it seems the CPU on the host `Intel(R) Core(TM) i7 CPU 870 @ 2.93GHz` doesn't support AVX instruction ( https://en.wikipedia.org/wiki/List_of_Intel_Core_i7_microprocessors#%22Lynnfield%22_(45_nm) ), which is required for the TensorFlow release version we use in the docker image. Please see this previous comment https://github.com/google/deepvariant/issues/217#issuecomment-530580878 for more details. That being said, I do find it strange that it's silently not producing outputs, instead of showing an error message about the unsupported instruction.. Is it possible for you to try the same command in a machine with newer CPU? For Intel CPUs, _Sandy Bridge_ (released in 2011) or later architectures should support AVX. https://en.wikipedia.org/wiki/List_of_Intel_Core_i7_microprocessors#Sandy_Bridge_microarchitecture_(2nd_generation)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:55,energy efficiency,Core,Core,55,"Hi @sh940202123 it seems the CPU on the host `Intel(R) Core(TM) i7 CPU 870 @ 2.93GHz` doesn't support AVX instruction ( https://en.wikipedia.org/wiki/List_of_Intel_Core_i7_microprocessors#%22Lynnfield%22_(45_nm) ), which is required for the TensorFlow release version we use in the docker image. Please see this previous comment https://github.com/google/deepvariant/issues/217#issuecomment-530580878 for more details. That being said, I do find it strange that it's silently not producing outputs, instead of showing an error message about the unsupported instruction.. Is it possible for you to try the same command in a machine with newer CPU? For Intel CPUs, _Sandy Bridge_ (released in 2011) or later architectures should support AVX. https://en.wikipedia.org/wiki/List_of_Intel_Core_i7_microprocessors#Sandy_Bridge_microarchitecture_(2nd_generation)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:67,energy efficiency,CPU,CPU,67,"Hi @sh940202123 it seems the CPU on the host `Intel(R) Core(TM) i7 CPU 870 @ 2.93GHz` doesn't support AVX instruction ( https://en.wikipedia.org/wiki/List_of_Intel_Core_i7_microprocessors#%22Lynnfield%22_(45_nm) ), which is required for the TensorFlow release version we use in the docker image. Please see this previous comment https://github.com/google/deepvariant/issues/217#issuecomment-530580878 for more details. That being said, I do find it strange that it's silently not producing outputs, instead of showing an error message about the unsupported instruction.. Is it possible for you to try the same command in a machine with newer CPU? For Intel CPUs, _Sandy Bridge_ (released in 2011) or later architectures should support AVX. https://en.wikipedia.org/wiki/List_of_Intel_Core_i7_microprocessors#Sandy_Bridge_microarchitecture_(2nd_generation)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:642,energy efficiency,CPU,CPU,642,"Hi @sh940202123 it seems the CPU on the host `Intel(R) Core(TM) i7 CPU 870 @ 2.93GHz` doesn't support AVX instruction ( https://en.wikipedia.org/wiki/List_of_Intel_Core_i7_microprocessors#%22Lynnfield%22_(45_nm) ), which is required for the TensorFlow release version we use in the docker image. Please see this previous comment https://github.com/google/deepvariant/issues/217#issuecomment-530580878 for more details. That being said, I do find it strange that it's silently not producing outputs, instead of showing an error message about the unsupported instruction.. Is it possible for you to try the same command in a machine with newer CPU? For Intel CPUs, _Sandy Bridge_ (released in 2011) or later architectures should support AVX. https://en.wikipedia.org/wiki/List_of_Intel_Core_i7_microprocessors#Sandy_Bridge_microarchitecture_(2nd_generation)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:657,energy efficiency,CPU,CPUs,657,"Hi @sh940202123 it seems the CPU on the host `Intel(R) Core(TM) i7 CPU 870 @ 2.93GHz` doesn't support AVX instruction ( https://en.wikipedia.org/wiki/List_of_Intel_Core_i7_microprocessors#%22Lynnfield%22_(45_nm) ), which is required for the TensorFlow release version we use in the docker image. Please see this previous comment https://github.com/google/deepvariant/issues/217#issuecomment-530580878 for more details. That being said, I do find it strange that it's silently not producing outputs, instead of showing an error message about the unsupported instruction.. Is it possible for you to try the same command in a machine with newer CPU? For Intel CPUs, _Sandy Bridge_ (released in 2011) or later architectures should support AVX. https://en.wikipedia.org/wiki/List_of_Intel_Core_i7_microprocessors#Sandy_Bridge_microarchitecture_(2nd_generation)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:260,integrability,version,version,260,"Hi @sh940202123 it seems the CPU on the host `Intel(R) Core(TM) i7 CPU 870 @ 2.93GHz` doesn't support AVX instruction ( https://en.wikipedia.org/wiki/List_of_Intel_Core_i7_microprocessors#%22Lynnfield%22_(45_nm) ), which is required for the TensorFlow release version we use in the docker image. Please see this previous comment https://github.com/google/deepvariant/issues/217#issuecomment-530580878 for more details. That being said, I do find it strange that it's silently not producing outputs, instead of showing an error message about the unsupported instruction.. Is it possible for you to try the same command in a machine with newer CPU? For Intel CPUs, _Sandy Bridge_ (released in 2011) or later architectures should support AVX. https://en.wikipedia.org/wiki/List_of_Intel_Core_i7_microprocessors#Sandy_Bridge_microarchitecture_(2nd_generation)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:527,integrability,messag,message,527,"Hi @sh940202123 it seems the CPU on the host `Intel(R) Core(TM) i7 CPU 870 @ 2.93GHz` doesn't support AVX instruction ( https://en.wikipedia.org/wiki/List_of_Intel_Core_i7_microprocessors#%22Lynnfield%22_(45_nm) ), which is required for the TensorFlow release version we use in the docker image. Please see this previous comment https://github.com/google/deepvariant/issues/217#issuecomment-530580878 for more details. That being said, I do find it strange that it's silently not producing outputs, instead of showing an error message about the unsupported instruction.. Is it possible for you to try the same command in a machine with newer CPU? For Intel CPUs, _Sandy Bridge_ (released in 2011) or later architectures should support AVX. https://en.wikipedia.org/wiki/List_of_Intel_Core_i7_microprocessors#Sandy_Bridge_microarchitecture_(2nd_generation)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:527,interoperability,messag,message,527,"Hi @sh940202123 it seems the CPU on the host `Intel(R) Core(TM) i7 CPU 870 @ 2.93GHz` doesn't support AVX instruction ( https://en.wikipedia.org/wiki/List_of_Intel_Core_i7_microprocessors#%22Lynnfield%22_(45_nm) ), which is required for the TensorFlow release version we use in the docker image. Please see this previous comment https://github.com/google/deepvariant/issues/217#issuecomment-530580878 for more details. That being said, I do find it strange that it's silently not producing outputs, instead of showing an error message about the unsupported instruction.. Is it possible for you to try the same command in a machine with newer CPU? For Intel CPUs, _Sandy Bridge_ (released in 2011) or later architectures should support AVX. https://en.wikipedia.org/wiki/List_of_Intel_Core_i7_microprocessors#Sandy_Bridge_microarchitecture_(2nd_generation)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:706,interoperability,architectur,architectures,706,"Hi @sh940202123 it seems the CPU on the host `Intel(R) Core(TM) i7 CPU 870 @ 2.93GHz` doesn't support AVX instruction ( https://en.wikipedia.org/wiki/List_of_Intel_Core_i7_microprocessors#%22Lynnfield%22_(45_nm) ), which is required for the TensorFlow release version we use in the docker image. Please see this previous comment https://github.com/google/deepvariant/issues/217#issuecomment-530580878 for more details. That being said, I do find it strange that it's silently not producing outputs, instead of showing an error message about the unsupported instruction.. Is it possible for you to try the same command in a machine with newer CPU? For Intel CPUs, _Sandy Bridge_ (released in 2011) or later architectures should support AVX. https://en.wikipedia.org/wiki/List_of_Intel_Core_i7_microprocessors#Sandy_Bridge_microarchitecture_(2nd_generation)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:260,modifiability,version,version,260,"Hi @sh940202123 it seems the CPU on the host `Intel(R) Core(TM) i7 CPU 870 @ 2.93GHz` doesn't support AVX instruction ( https://en.wikipedia.org/wiki/List_of_Intel_Core_i7_microprocessors#%22Lynnfield%22_(45_nm) ), which is required for the TensorFlow release version we use in the docker image. Please see this previous comment https://github.com/google/deepvariant/issues/217#issuecomment-530580878 for more details. That being said, I do find it strange that it's silently not producing outputs, instead of showing an error message about the unsupported instruction.. Is it possible for you to try the same command in a machine with newer CPU? For Intel CPUs, _Sandy Bridge_ (released in 2011) or later architectures should support AVX. https://en.wikipedia.org/wiki/List_of_Intel_Core_i7_microprocessors#Sandy_Bridge_microarchitecture_(2nd_generation)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:29,performance,CPU,CPU,29,"Hi @sh940202123 it seems the CPU on the host `Intel(R) Core(TM) i7 CPU 870 @ 2.93GHz` doesn't support AVX instruction ( https://en.wikipedia.org/wiki/List_of_Intel_Core_i7_microprocessors#%22Lynnfield%22_(45_nm) ), which is required for the TensorFlow release version we use in the docker image. Please see this previous comment https://github.com/google/deepvariant/issues/217#issuecomment-530580878 for more details. That being said, I do find it strange that it's silently not producing outputs, instead of showing an error message about the unsupported instruction.. Is it possible for you to try the same command in a machine with newer CPU? For Intel CPUs, _Sandy Bridge_ (released in 2011) or later architectures should support AVX. https://en.wikipedia.org/wiki/List_of_Intel_Core_i7_microprocessors#Sandy_Bridge_microarchitecture_(2nd_generation)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:67,performance,CPU,CPU,67,"Hi @sh940202123 it seems the CPU on the host `Intel(R) Core(TM) i7 CPU 870 @ 2.93GHz` doesn't support AVX instruction ( https://en.wikipedia.org/wiki/List_of_Intel_Core_i7_microprocessors#%22Lynnfield%22_(45_nm) ), which is required for the TensorFlow release version we use in the docker image. Please see this previous comment https://github.com/google/deepvariant/issues/217#issuecomment-530580878 for more details. That being said, I do find it strange that it's silently not producing outputs, instead of showing an error message about the unsupported instruction.. Is it possible for you to try the same command in a machine with newer CPU? For Intel CPUs, _Sandy Bridge_ (released in 2011) or later architectures should support AVX. https://en.wikipedia.org/wiki/List_of_Intel_Core_i7_microprocessors#Sandy_Bridge_microarchitecture_(2nd_generation)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:521,performance,error,error,521,"Hi @sh940202123 it seems the CPU on the host `Intel(R) Core(TM) i7 CPU 870 @ 2.93GHz` doesn't support AVX instruction ( https://en.wikipedia.org/wiki/List_of_Intel_Core_i7_microprocessors#%22Lynnfield%22_(45_nm) ), which is required for the TensorFlow release version we use in the docker image. Please see this previous comment https://github.com/google/deepvariant/issues/217#issuecomment-530580878 for more details. That being said, I do find it strange that it's silently not producing outputs, instead of showing an error message about the unsupported instruction.. Is it possible for you to try the same command in a machine with newer CPU? For Intel CPUs, _Sandy Bridge_ (released in 2011) or later architectures should support AVX. https://en.wikipedia.org/wiki/List_of_Intel_Core_i7_microprocessors#Sandy_Bridge_microarchitecture_(2nd_generation)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:642,performance,CPU,CPU,642,"Hi @sh940202123 it seems the CPU on the host `Intel(R) Core(TM) i7 CPU 870 @ 2.93GHz` doesn't support AVX instruction ( https://en.wikipedia.org/wiki/List_of_Intel_Core_i7_microprocessors#%22Lynnfield%22_(45_nm) ), which is required for the TensorFlow release version we use in the docker image. Please see this previous comment https://github.com/google/deepvariant/issues/217#issuecomment-530580878 for more details. That being said, I do find it strange that it's silently not producing outputs, instead of showing an error message about the unsupported instruction.. Is it possible for you to try the same command in a machine with newer CPU? For Intel CPUs, _Sandy Bridge_ (released in 2011) or later architectures should support AVX. https://en.wikipedia.org/wiki/List_of_Intel_Core_i7_microprocessors#Sandy_Bridge_microarchitecture_(2nd_generation)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:657,performance,CPU,CPUs,657,"Hi @sh940202123 it seems the CPU on the host `Intel(R) Core(TM) i7 CPU 870 @ 2.93GHz` doesn't support AVX instruction ( https://en.wikipedia.org/wiki/List_of_Intel_Core_i7_microprocessors#%22Lynnfield%22_(45_nm) ), which is required for the TensorFlow release version we use in the docker image. Please see this previous comment https://github.com/google/deepvariant/issues/217#issuecomment-530580878 for more details. That being said, I do find it strange that it's silently not producing outputs, instead of showing an error message about the unsupported instruction.. Is it possible for you to try the same command in a machine with newer CPU? For Intel CPUs, _Sandy Bridge_ (released in 2011) or later architectures should support AVX. https://en.wikipedia.org/wiki/List_of_Intel_Core_i7_microprocessors#Sandy_Bridge_microarchitecture_(2nd_generation)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:86,reliability,doe,doesn,86,"Hi @sh940202123 it seems the CPU on the host `Intel(R) Core(TM) i7 CPU 870 @ 2.93GHz` doesn't support AVX instruction ( https://en.wikipedia.org/wiki/List_of_Intel_Core_i7_microprocessors#%22Lynnfield%22_(45_nm) ), which is required for the TensorFlow release version we use in the docker image. Please see this previous comment https://github.com/google/deepvariant/issues/217#issuecomment-530580878 for more details. That being said, I do find it strange that it's silently not producing outputs, instead of showing an error message about the unsupported instruction.. Is it possible for you to try the same command in a machine with newer CPU? For Intel CPUs, _Sandy Bridge_ (released in 2011) or later architectures should support AVX. https://en.wikipedia.org/wiki/List_of_Intel_Core_i7_microprocessors#Sandy_Bridge_microarchitecture_(2nd_generation)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:521,safety,error,error,521,"Hi @sh940202123 it seems the CPU on the host `Intel(R) Core(TM) i7 CPU 870 @ 2.93GHz` doesn't support AVX instruction ( https://en.wikipedia.org/wiki/List_of_Intel_Core_i7_microprocessors#%22Lynnfield%22_(45_nm) ), which is required for the TensorFlow release version we use in the docker image. Please see this previous comment https://github.com/google/deepvariant/issues/217#issuecomment-530580878 for more details. That being said, I do find it strange that it's silently not producing outputs, instead of showing an error message about the unsupported instruction.. Is it possible for you to try the same command in a machine with newer CPU? For Intel CPUs, _Sandy Bridge_ (released in 2011) or later architectures should support AVX. https://en.wikipedia.org/wiki/List_of_Intel_Core_i7_microprocessors#Sandy_Bridge_microarchitecture_(2nd_generation)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:94,usability,support,support,94,"Hi @sh940202123 it seems the CPU on the host `Intel(R) Core(TM) i7 CPU 870 @ 2.93GHz` doesn't support AVX instruction ( https://en.wikipedia.org/wiki/List_of_Intel_Core_i7_microprocessors#%22Lynnfield%22_(45_nm) ), which is required for the TensorFlow release version we use in the docker image. Please see this previous comment https://github.com/google/deepvariant/issues/217#issuecomment-530580878 for more details. That being said, I do find it strange that it's silently not producing outputs, instead of showing an error message about the unsupported instruction.. Is it possible for you to try the same command in a machine with newer CPU? For Intel CPUs, _Sandy Bridge_ (released in 2011) or later architectures should support AVX. https://en.wikipedia.org/wiki/List_of_Intel_Core_i7_microprocessors#Sandy_Bridge_microarchitecture_(2nd_generation)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:521,usability,error,error,521,"Hi @sh940202123 it seems the CPU on the host `Intel(R) Core(TM) i7 CPU 870 @ 2.93GHz` doesn't support AVX instruction ( https://en.wikipedia.org/wiki/List_of_Intel_Core_i7_microprocessors#%22Lynnfield%22_(45_nm) ), which is required for the TensorFlow release version we use in the docker image. Please see this previous comment https://github.com/google/deepvariant/issues/217#issuecomment-530580878 for more details. That being said, I do find it strange that it's silently not producing outputs, instead of showing an error message about the unsupported instruction.. Is it possible for you to try the same command in a machine with newer CPU? For Intel CPUs, _Sandy Bridge_ (released in 2011) or later architectures should support AVX. https://en.wikipedia.org/wiki/List_of_Intel_Core_i7_microprocessors#Sandy_Bridge_microarchitecture_(2nd_generation)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:610,usability,command,command,610,"Hi @sh940202123 it seems the CPU on the host `Intel(R) Core(TM) i7 CPU 870 @ 2.93GHz` doesn't support AVX instruction ( https://en.wikipedia.org/wiki/List_of_Intel_Core_i7_microprocessors#%22Lynnfield%22_(45_nm) ), which is required for the TensorFlow release version we use in the docker image. Please see this previous comment https://github.com/google/deepvariant/issues/217#issuecomment-530580878 for more details. That being said, I do find it strange that it's silently not producing outputs, instead of showing an error message about the unsupported instruction.. Is it possible for you to try the same command in a machine with newer CPU? For Intel CPUs, _Sandy Bridge_ (released in 2011) or later architectures should support AVX. https://en.wikipedia.org/wiki/List_of_Intel_Core_i7_microprocessors#Sandy_Bridge_microarchitecture_(2nd_generation)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:727,usability,support,support,727,"Hi @sh940202123 it seems the CPU on the host `Intel(R) Core(TM) i7 CPU 870 @ 2.93GHz` doesn't support AVX instruction ( https://en.wikipedia.org/wiki/List_of_Intel_Core_i7_microprocessors#%22Lynnfield%22_(45_nm) ), which is required for the TensorFlow release version we use in the docker image. Please see this previous comment https://github.com/google/deepvariant/issues/217#issuecomment-530580878 for more details. That being said, I do find it strange that it's silently not producing outputs, instead of showing an error message about the unsupported instruction.. Is it possible for you to try the same command in a machine with newer CPU? For Intel CPUs, _Sandy Bridge_ (released in 2011) or later architectures should support AVX. https://en.wikipedia.org/wiki/List_of_Intel_Core_i7_microprocessors#Sandy_Bridge_microarchitecture_(2nd_generation)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:1057,availability,monitor,monitor,1057,"rovided but it still print nothing on terminal. . And the docker version of this machine is `Docker version 19.03.3, build a872fc2`. So I switch to another machine which supported AVX instruction and run the test again by @tedyun 's advice. Then it run normally and output the files as same as the tutorial. Here is the environment & the command:. - Host CPU info. ```text. processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 63. model name : Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz. stepping : 2. microcode : 0x43. cpu MHz : 1199.975. cache size : 25600 KB. physical id : 0. siblings : 20. core id : 0. cpu cores : 10. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 15. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.bam.bai. test_nist.b37_chr20_100kbp_at_10mb.bed. test_nist.b3",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:2831,availability,Down,Downloaded,2831,"19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.bam.bai. test_nist.b37_chr20_100kbp_at_10mb.bed. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. ucsc.hg19.chr20.unittest.fasta. ucsc.hg19.chr20.unittest.fasta.fai. ucsc.hg19.chr20.unittest.fasta.gz. ucsc.hg19.chr20.unittest.fasta.gz.fai. ucsc.hg19.chr20.unittest.fasta.gz.gzi. # docker run \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=1 \. >. Status: Downloaded newer image for google/deepvariant:1.0.0. I0911 02:28:36.697342 140021722134272 run_deepvariant.py:269] Creating a directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. I0911 02:28:39.900765 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.914417 140366075229952 make_examples.py:587] Preparing inputs. I0911 02:28:39.915202 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.ch",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:5414,availability,checkpoint,checkpoint,5414,", note that we will decode CRAM using the reference you passed in with --ref. 2020-09-11 02:28:39.919135: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. I0911 02:28:39.922723 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.923217 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:40.247040 140366075229952 make_examples.py:587] 6 candidates (6 examples) [0.33s elapsed]. I0911 02:28:41.745102 140366075229952 make_examples.py:587] Found 78 candidate variants. I0911 02:28:41.745359 140366075229952 make_examples.py:587] Created 86 examples. real 0m5.273s. user 0m5.898s. sys 0m3.792s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0911 02:28:44.682443 139937686464256 call_variants.py:335] Shape of input examples: [100, 221, 6]. 2020-09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-09-11 02:28:44.721599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2297525000 Hz. 2020-09-11 02:28:44.724518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5886480 executing computations on platform Host. Devices:. 2020-09-11 02:28:44.724555: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-09-11 02:28:44.727738: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune us",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:5756,availability,operat,operations,5756," 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:40.247040 140366075229952 make_examples.py:587] 6 candidates (6 examples) [0.33s elapsed]. I0911 02:28:41.745102 140366075229952 make_examples.py:587] Found 78 candidate variants. I0911 02:28:41.745359 140366075229952 make_examples.py:587] Created 86 examples. real 0m5.273s. user 0m5.898s. sys 0m3.792s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0911 02:28:44.682443 139937686464256 call_variants.py:335] Shape of input examples: [100, 221, 6]. 2020-09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-09-11 02:28:44.721599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2297525000 Hz. 2020-09-11 02:28:44.724518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5886480 executing computations on platform Host. Devices:. 2020-09-11 02:28:44.724555: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-09-11 02:28:44.727738: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W0911 02:28:44.755701 139937686464256 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp8bwus8dp. I0911 02:28:44.756375 139937686464256 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp8bwus8dp', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_chec",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:5808,availability,operat,operations,5808,"put/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:40.247040 140366075229952 make_examples.py:587] 6 candidates (6 examples) [0.33s elapsed]. I0911 02:28:41.745102 140366075229952 make_examples.py:587] Found 78 candidate variants. I0911 02:28:41.745359 140366075229952 make_examples.py:587] Created 86 examples. real 0m5.273s. user 0m5.898s. sys 0m3.792s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0911 02:28:44.682443 139937686464256 call_variants.py:335] Shape of input examples: [100, 221, 6]. 2020-09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-09-11 02:28:44.721599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2297525000 Hz. 2020-09-11 02:28:44.724518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5886480 executing computations on platform Host. Devices:. 2020-09-11 02:28:44.724555: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-09-11 02:28:44.727738: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W0911 02:28:44.755701 139937686464256 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp8bwus8dp. I0911 02:28:44.756375 139937686464256 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp8bwus8dp', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:6046,availability,servic,service,6046,"e variants. I0911 02:28:41.745359 140366075229952 make_examples.py:587] Created 86 examples. real 0m5.273s. user 0m5.898s. sys 0m3.792s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0911 02:28:44.682443 139937686464256 call_variants.py:335] Shape of input examples: [100, 221, 6]. 2020-09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-09-11 02:28:44.721599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2297525000 Hz. 2020-09-11 02:28:44.724518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5886480 executing computations on platform Host. Devices:. 2020-09-11 02:28:44.724555: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-09-11 02:28:44.727738: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W0911 02:28:44.755701 139937686464256 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp8bwus8dp. I0911 02:28:44.756375 139937686464256 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp8bwus8dp', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:6054,availability,servic,service,6054,"ts. I0911 02:28:41.745359 140366075229952 make_examples.py:587] Created 86 examples. real 0m5.273s. user 0m5.898s. sys 0m3.792s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0911 02:28:44.682443 139937686464256 call_variants.py:335] Shape of input examples: [100, 221, 6]. 2020-09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-09-11 02:28:44.721599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2297525000 Hz. 2020-09-11 02:28:44.724518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5886480 executing computations on platform Host. Devices:. 2020-09-11 02:28:44.724555: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-09-11 02:28:44.727738: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W0911 02:28:44.755701 139937686464256 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp8bwus8dp. I0911 02:28:44.756375 139937686464256 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp8bwus8dp', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:6074,availability,servic,service,6074,"45359 140366075229952 make_examples.py:587] Created 86 examples. real 0m5.273s. user 0m5.898s. sys 0m3.792s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0911 02:28:44.682443 139937686464256 call_variants.py:335] Shape of input examples: [100, 221, 6]. 2020-09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-09-11 02:28:44.721599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2297525000 Hz. 2020-09-11 02:28:44.724518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5886480 executing computations on platform Host. Devices:. 2020-09-11 02:28:44.724555: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-09-11 02:28:44.727738: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W0911 02:28:44.755701 139937686464256 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp8bwus8dp. I0911 02:28:44.756375 139937686464256 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp8bwus8dp', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:6197,availability,servic,service,6197,"the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0911 02:28:44.682443 139937686464256 call_variants.py:335] Shape of input examples: [100, 221, 6]. 2020-09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-09-11 02:28:44.721599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2297525000 Hz. 2020-09-11 02:28:44.724518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5886480 executing computations on platform Host. Devices:. 2020-09-11 02:28:44.724555: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-09-11 02:28:44.727738: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W0911 02:28:44.755701 139937686464256 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp8bwus8dp. I0911 02:28:44.756375 139937686464256 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp8bwus8dp', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.pytho",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:6205,availability,servic,service,6205,"and:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0911 02:28:44.682443 139937686464256 call_variants.py:335] Shape of input examples: [100, 221, 6]. 2020-09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-09-11 02:28:44.721599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2297525000 Hz. 2020-09-11 02:28:44.724518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5886480 executing computations on platform Host. Devices:. 2020-09-11 02:28:44.724555: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-09-11 02:28:44.727738: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W0911 02:28:44.755701 139937686464256 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp8bwus8dp. I0911 02:28:44.756375 139937686464256 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp8bwus8dp', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.traini",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:7223,availability,Cluster,ClusterSpec,7223,"ecutor device (0): Host, Default Version. 2020-09-11 02:28:44.727738: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W0911 02:28:44.755701 139937686464256 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp8bwus8dp. I0911 02:28:44.756375 139937686464256 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp8bwus8dp', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f4549cfe518>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I0911 02:28:44.756798 139937686464256 call_variants.py:426] Writing calls to /output/intermediate_results_dir/call_variants_output.tfrecord.gz. W0911 02:28:44.767428 139937686464256 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version. Instructions for updating:. If using Keras pass *_constraint arguments to layers. W0911 02:28:44.788271 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:8443,availability,slo,sloppy,8443,"02:28:44.756798 139937686464256 call_variants.py:426] Writing calls to /output/intermediate_results_dir/call_variants_output.tfrecord.gz. W0911 02:28:44.767428 139937686464256 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version. Instructions for updating:. If using Keras pass *_constraint arguments to layers. W0911 02:28:44.788271 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. W0911 02:28:44.973487 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I0911 02:28:45.531516 139937686464256 estimator.py:1147] Calling model_fn. W0911 02:28:45.537137 139937686464256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0911 02:28:50.97",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:9647,availability,Restor,Restoring,9647,"iant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I0911 02:28:45.531516 139937686464256 estimator.py:1147] Calling model_fn. W0911 02:28:45.537137 139937686464256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0911 02:28:50.978360 139937686464256 estimator.py:1149] Done calling model_fn. I0911 02:28:52.395916 139937686464256 monitored_session.py:240] Graph was finalized. I0911 02:28:52.397383 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:53.423366 139937686464256 session_manager.py:500] Running local_init_op. I0911 02:28:53.495537 139937686464256 session_manager.py:502] Done running local_init_op. I0911 02:28:54.010662 139937686464256 modeling.py:415] Reloading EMA... I0911 02:28:54.011811 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:56.213706 139937686464256 call_variants.py:444] Processed 1 examples in 1 batches [1145.647 sec per 100]. I0911 02:28:56.265293 139937686464256 call_variants.py:447] Processed 86 examples in 1 batches [13.334 sec per 100]. I0911 02:28:56.265480 139937686464256 call_variants.py:450] Done calling variants from a total of 86 examples. real 0m14.761s. user 0m20.588s. sys 0m5.092s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --infile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:10001,availability,Restor,Restoring,10001," the fused implementation. I0911 02:28:45.531516 139937686464256 estimator.py:1147] Calling model_fn. W0911 02:28:45.537137 139937686464256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0911 02:28:50.978360 139937686464256 estimator.py:1149] Done calling model_fn. I0911 02:28:52.395916 139937686464256 monitored_session.py:240] Graph was finalized. I0911 02:28:52.397383 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:53.423366 139937686464256 session_manager.py:500] Running local_init_op. I0911 02:28:53.495537 139937686464256 session_manager.py:502] Done running local_init_op. I0911 02:28:54.010662 139937686464256 modeling.py:415] Reloading EMA... I0911 02:28:54.011811 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:56.213706 139937686464256 call_variants.py:444] Processed 1 examples in 1 batches [1145.647 sec per 100]. I0911 02:28:56.265293 139937686464256 call_variants.py:447] Processed 86 examples in 1 batches [13.334 sec per 100]. I0911 02:28:56.265480 139937686464256 call_variants.py:450] Done calling variants from a total of 86 examples. real 0m14.761s. user 0m20.588s. sys 0m5.092s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --infile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/output/output.g.vcf.gz"". 2020-09-11 02:28:59.543794: I deepvariant/postprocess_variants.cc:88] Read from: /output/intermediate_results_dir/call_variants_output.tfrecord.gz. 2020-09-11 02:28:59.544704: I deepvariant/post",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:126,deployability,version,version,126,"Hi @pichuan and @tedyun ~. I run the command which @pichuan provided but it still print nothing on terminal. . And the docker version of this machine is `Docker version 19.03.3, build a872fc2`. So I switch to another machine which supported AVX instruction and run the test again by @tedyun 's advice. Then it run normally and output the files as same as the tutorial. Here is the environment & the command:. - Host CPU info. ```text. processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 63. model name : Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz. stepping : 2. microcode : 0x43. cpu MHz : 1199.975. cache size : 25600 KB. physical id : 0. siblings : 20. core id : 0. cpu cores : 10. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 15. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:161,deployability,version,version,161,"Hi @pichuan and @tedyun ~. I run the command which @pichuan provided but it still print nothing on terminal. . And the docker version of this machine is `Docker version 19.03.3, build a872fc2`. So I switch to another machine which supported AVX instruction and run the test again by @tedyun 's advice. Then it run normally and output the files as same as the tutorial. Here is the environment & the command:. - Host CPU info. ```text. processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 63. model name : Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz. stepping : 2. microcode : 0x43. cpu MHz : 1199.975. cache size : 25600 KB. physical id : 0. siblings : 20. core id : 0. cpu cores : 10. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 15. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:178,deployability,build,build,178,"Hi @pichuan and @tedyun ~. I run the command which @pichuan provided but it still print nothing on terminal. . And the docker version of this machine is `Docker version 19.03.3, build a872fc2`. So I switch to another machine which supported AVX instruction and run the test again by @tedyun 's advice. Then it run normally and output the files as same as the tutorial. Here is the environment & the command:. - Host CPU info. ```text. processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 63. model name : Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz. stepping : 2. microcode : 0x43. cpu MHz : 1199.975. cache size : 25600 KB. physical id : 0. siblings : 20. core id : 0. cpu cores : 10. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 15. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:696,deployability,api,apicid,696,"Hi @pichuan and @tedyun ~. I run the command which @pichuan provided but it still print nothing on terminal. . And the docker version of this machine is `Docker version 19.03.3, build a872fc2`. So I switch to another machine which supported AVX instruction and run the test again by @tedyun 's advice. Then it run normally and output the files as same as the tutorial. Here is the environment & the command:. - Host CPU info. ```text. processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 63. model name : Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz. stepping : 2. microcode : 0x43. cpu MHz : 1199.975. cache size : 25600 KB. physical id : 0. siblings : 20. core id : 0. cpu cores : 10. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 15. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:716,deployability,api,apicid,716,"Hi @pichuan and @tedyun ~. I run the command which @pichuan provided but it still print nothing on terminal. . And the docker version of this machine is `Docker version 19.03.3, build a872fc2`. So I switch to another machine which supported AVX instruction and run the test again by @tedyun 's advice. Then it run normally and output the files as same as the tutorial. Here is the environment & the command:. - Host CPU info. ```text. processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 63. model name : Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz. stepping : 2. microcode : 0x43. cpu MHz : 1199.975. cache size : 25600 KB. physical id : 0. siblings : 20. core id : 0. cpu cores : 10. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 15. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:831,deployability,api,apic,831,"Hi @pichuan and @tedyun ~. I run the command which @pichuan provided but it still print nothing on terminal. . And the docker version of this machine is `Docker version 19.03.3, build a872fc2`. So I switch to another machine which supported AVX instruction and run the test again by @tedyun 's advice. Then it run normally and output the files as same as the tutorial. Here is the environment & the command:. - Host CPU info. ```text. processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 63. model name : Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz. stepping : 2. microcode : 0x43. cpu MHz : 1199.975. cache size : 25600 KB. physical id : 0. siblings : 20. core id : 0. cpu cores : 10. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 15. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:1057,deployability,monitor,monitor,1057,"rovided but it still print nothing on terminal. . And the docker version of this machine is `Docker version 19.03.3, build a872fc2`. So I switch to another machine which supported AVX instruction and run the test again by @tedyun 's advice. Then it run normally and output the files as same as the tutorial. Here is the environment & the command:. - Host CPU info. ```text. processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 63. model name : Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz. stepping : 2. microcode : 0x43. cpu MHz : 1199.975. cache size : 25600 KB. physical id : 0. siblings : 20. core id : 0. cpu cores : 10. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 15. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.bam.bai. test_nist.b37_chr20_100kbp_at_10mb.bed. test_nist.b3",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:1568,deployability,manag,management,1568," microcode : 0x43. cpu MHz : 1199.975. cache size : 25600 KB. physical id : 0. siblings : 20. core id : 0. cpu cores : 10. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 15. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.bam.bai. test_nist.b37_chr20_100kbp_at_10mb.bed. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. ucsc.hg19.chr20.unittest.fasta. ucsc.hg19.chr20.unittest.fasta.fai. ucsc.hg19.chr20.unittest.fasta.gz. ucsc.hg19.chr20.unittest.fasta.gz.fai. ucsc.hg19.chr20.unittest.fasta.gz.gzi. # docker run \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:1608,deployability,version,version,1608,"cache size : 25600 KB. physical id : 0. siblings : 20. core id : 0. cpu cores : 10. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 15. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.bam.bai. test_nist.b37_chr20_100kbp_at_10mb.bed. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. ucsc.hg19.chr20.unittest.fasta. ucsc.hg19.chr20.unittest.fasta.fai. ucsc.hg19.chr20.unittest.fasta.gz. ucsc.hg19.chr20.unittest.fasta.gz.fai. ucsc.hg19.chr20.unittest.fasta.gz.gzi. # docker run \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. > --r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:1761,deployability,releas,release,1761," level : 15. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.bam.bai. test_nist.b37_chr20_100kbp_at_10mb.bed. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. ucsc.hg19.chr20.unittest.fasta. ucsc.hg19.chr20.unittest.fasta.fai. ucsc.hg19.chr20.unittest.fasta.gz. ucsc.hg19.chr20.unittest.fasta.gz.fai. ucsc.hg19.chr20.unittest.fasta.gz.gzi. # docker run \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --intermediate_results_dir /",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:1783,deployability,releas,release,1783," flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.bam.bai. test_nist.b37_chr20_100kbp_at_10mb.bed. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. ucsc.hg19.chr20.unittest.fasta. ucsc.hg19.chr20.unittest.fasta.fai. ucsc.hg19.chr20.unittest.fasta.gz. ucsc.hg19.chr20.unittest.fasta.gz.fai. ucsc.hg19.chr20.unittest.fasta.gz.gzi. # docker run \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --intermediate_results_dir /output/intermediate_re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:1828,deployability,version,version,1828,"pic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.bam.bai. test_nist.b37_chr20_100kbp_at_10mb.bed. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. ucsc.hg19.chr20.unittest.fasta. ucsc.hg19.chr20.unittest.fasta.fai. ucsc.hg19.chr20.unittest.fasta.gz. ucsc.hg19.chr20.unittest.fasta.gz.fai. ucsc.hg19.chr20.unittest.fasta.gz.gzi. # docker run \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=1 \. >. Status: D",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:1846,deployability,build,build,1846,"mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.bam.bai. test_nist.b37_chr20_100kbp_at_10mb.bed. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. ucsc.hg19.chr20.unittest.fasta. ucsc.hg19.chr20.unittest.fasta.fai. ucsc.hg19.chr20.unittest.fasta.gz. ucsc.hg19.chr20.unittest.fasta.gz.fai. ucsc.hg19.chr20.unittest.fasta.gz.gzi. # docker run \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=1 \. >. Status: Downloaded newer i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:6046,deployability,servic,service,6046,"e variants. I0911 02:28:41.745359 140366075229952 make_examples.py:587] Created 86 examples. real 0m5.273s. user 0m5.898s. sys 0m3.792s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0911 02:28:44.682443 139937686464256 call_variants.py:335] Shape of input examples: [100, 221, 6]. 2020-09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-09-11 02:28:44.721599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2297525000 Hz. 2020-09-11 02:28:44.724518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5886480 executing computations on platform Host. Devices:. 2020-09-11 02:28:44.724555: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-09-11 02:28:44.727738: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W0911 02:28:44.755701 139937686464256 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp8bwus8dp. I0911 02:28:44.756375 139937686464256 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp8bwus8dp', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:6054,deployability,servic,service,6054,"ts. I0911 02:28:41.745359 140366075229952 make_examples.py:587] Created 86 examples. real 0m5.273s. user 0m5.898s. sys 0m3.792s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0911 02:28:44.682443 139937686464256 call_variants.py:335] Shape of input examples: [100, 221, 6]. 2020-09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-09-11 02:28:44.721599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2297525000 Hz. 2020-09-11 02:28:44.724518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5886480 executing computations on platform Host. Devices:. 2020-09-11 02:28:44.724555: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-09-11 02:28:44.727738: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W0911 02:28:44.755701 139937686464256 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp8bwus8dp. I0911 02:28:44.756375 139937686464256 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp8bwus8dp', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:6074,deployability,servic,service,6074,"45359 140366075229952 make_examples.py:587] Created 86 examples. real 0m5.273s. user 0m5.898s. sys 0m3.792s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0911 02:28:44.682443 139937686464256 call_variants.py:335] Shape of input examples: [100, 221, 6]. 2020-09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-09-11 02:28:44.721599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2297525000 Hz. 2020-09-11 02:28:44.724518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5886480 executing computations on platform Host. Devices:. 2020-09-11 02:28:44.724555: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-09-11 02:28:44.727738: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W0911 02:28:44.755701 139937686464256 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp8bwus8dp. I0911 02:28:44.756375 139937686464256 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp8bwus8dp', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:6197,deployability,servic,service,6197,"the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0911 02:28:44.682443 139937686464256 call_variants.py:335] Shape of input examples: [100, 221, 6]. 2020-09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-09-11 02:28:44.721599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2297525000 Hz. 2020-09-11 02:28:44.724518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5886480 executing computations on platform Host. Devices:. 2020-09-11 02:28:44.724555: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-09-11 02:28:44.727738: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W0911 02:28:44.755701 139937686464256 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp8bwus8dp. I0911 02:28:44.756375 139937686464256 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp8bwus8dp', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.pytho",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:6205,deployability,servic,service,6205,"and:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0911 02:28:44.682443 139937686464256 call_variants.py:335] Shape of input examples: [100, 221, 6]. 2020-09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-09-11 02:28:44.721599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2297525000 Hz. 2020-09-11 02:28:44.724518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5886480 executing computations on platform Host. Devices:. 2020-09-11 02:28:44.724555: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-09-11 02:28:44.727738: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W0911 02:28:44.755701 139937686464256 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp8bwus8dp. I0911 02:28:44.756375 139937686464256 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp8bwus8dp', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.traini",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:6262,deployability,Version,Version,6262,"ile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0911 02:28:44.682443 139937686464256 call_variants.py:335] Shape of input examples: [100, 221, 6]. 2020-09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-09-11 02:28:44.721599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2297525000 Hz. 2020-09-11 02:28:44.724518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5886480 executing computations on platform Host. Devices:. 2020-09-11 02:28:44.724555: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-09-11 02:28:44.727738: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W0911 02:28:44.755701 139937686464256 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp8bwus8dp. I0911 02:28:44.756375 139937686464256 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp8bwus8dp', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f4549cfe518>, '_ta",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:7223,deployability,Cluster,ClusterSpec,7223,"ecutor device (0): Host, Default Version. 2020-09-11 02:28:44.727738: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W0911 02:28:44.755701 139937686464256 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp8bwus8dp. I0911 02:28:44.756375 139937686464256 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp8bwus8dp', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f4549cfe518>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I0911 02:28:44.756798 139937686464256 call_variants.py:426] Writing calls to /output/intermediate_results_dir/call_variants_output.tfrecord.gz. W0911 02:28:44.767428 139937686464256 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version. Instructions for updating:. If using Keras pass *_constraint arguments to layers. W0911 02:28:44.788271 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:7895,deployability,version,version,7895,": 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f4549cfe518>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I0911 02:28:44.756798 139937686464256 call_variants.py:426] Writing calls to /output/intermediate_results_dir/call_variants_output.tfrecord.gz. W0911 02:28:44.767428 139937686464256 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version. Instructions for updating:. If using Keras pass *_constraint arguments to layers. W0911 02:28:44.788271 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. W0911 02:28:44.973487 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:7921,deployability,updat,updating,7921,"steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f4549cfe518>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I0911 02:28:44.756798 139937686464256 call_variants.py:426] Writing calls to /output/intermediate_results_dir/call_variants_output.tfrecord.gz. W0911 02:28:44.767428 139937686464256 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version. Instructions for updating:. If using Keras pass *_constraint arguments to layers. W0911 02:28:44.788271 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. W0911 02:28:44.973487 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:8273,deployability,version,version,8273,"worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I0911 02:28:44.756798 139937686464256 call_variants.py:426] Writing calls to /output/intermediate_results_dir/call_variants_output.tfrecord.gz. W0911 02:28:44.767428 139937686464256 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version. Instructions for updating:. If using Keras pass *_constraint arguments to layers. W0911 02:28:44.788271 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. W0911 02:28:44.973487 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I0911 02:28:45.531516 139937686464256 estimator.py:1147] Calling model_fn. W0911 02:28:45.537137 139937686464256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:8299,deployability,updat,updating,8299,"global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I0911 02:28:44.756798 139937686464256 call_variants.py:426] Writing calls to /output/intermediate_results_dir/call_variants_output.tfrecord.gz. W0911 02:28:44.767428 139937686464256 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version. Instructions for updating:. If using Keras pass *_constraint arguments to layers. W0911 02:28:44.788271 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. W0911 02:28:44.973487 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I0911 02:28:45.531516 139937686464256 estimator.py:1147] Calling model_fn. W0911 02:28:45.537137 139937686464256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:8796,deployability,version,version,8796,"ython.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version. Instructions for updating:. If using Keras pass *_constraint arguments to layers. W0911 02:28:44.788271 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. W0911 02:28:44.973487 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I0911 02:28:45.531516 139937686464256 estimator.py:1147] Calling model_fn. W0911 02:28:45.537137 139937686464256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0911 02:28:50.978360 139937686464256 estimator.py:1149] Done calling model_fn. I0911 02:28:52.395916 139937686464256 monitored_session.py:240] Graph was finalized. I0911 02:28:52.397383 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:53.423366 139937686464256 session_manager.py:500] Running local_init_op. I0911 02:28:53",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:8822,deployability,updat,updating,8822,"e_ops) with constraint is deprecated and will be removed in a future version. Instructions for updating:. If using Keras pass *_constraint arguments to layers. W0911 02:28:44.788271 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. W0911 02:28:44.973487 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I0911 02:28:45.531516 139937686464256 estimator.py:1147] Calling model_fn. W0911 02:28:45.537137 139937686464256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0911 02:28:50.978360 139937686464256 estimator.py:1149] Done calling model_fn. I0911 02:28:52.395916 139937686464256 monitored_session.py:240] Graph was finalized. I0911 02:28:52.397383 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:53.423366 139937686464256 session_manager.py:500] Running local_init_op. I0911 02:28:53.495537 139937686464256 se",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:9348,deployability,version,version,9348,"cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. W0911 02:28:44.973487 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I0911 02:28:45.531516 139937686464256 estimator.py:1147] Calling model_fn. W0911 02:28:45.537137 139937686464256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0911 02:28:50.978360 139937686464256 estimator.py:1149] Done calling model_fn. I0911 02:28:52.395916 139937686464256 monitored_session.py:240] Graph was finalized. I0911 02:28:52.397383 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:53.423366 139937686464256 session_manager.py:500] Running local_init_op. I0911 02:28:53.495537 139937686464256 session_manager.py:502] Done running local_init_op. I0911 02:28:54.010662 139937686464256 modeling.py:415] Reloading EMA... I0911 02:28:54.011811 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:56.213706 139937686464256 call_variants.py:444] Processed 1 examples in 1 batches [1145.647 sec per 100]. I0911 02:28:56.265293 139937686464256 call_variants.py:447] Processed 86 examples in 1 batches [13.334 sec per 100]. I0911 02:28:56.265480 139937686464256 call_variants.py:450] Do",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:9374,deployability,updat,updating,9374,", num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. W0911 02:28:44.973487 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I0911 02:28:45.531516 139937686464256 estimator.py:1147] Calling model_fn. W0911 02:28:45.537137 139937686464256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0911 02:28:50.978360 139937686464256 estimator.py:1149] Done calling model_fn. I0911 02:28:52.395916 139937686464256 monitored_session.py:240] Graph was finalized. I0911 02:28:52.397383 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:53.423366 139937686464256 session_manager.py:500] Running local_init_op. I0911 02:28:53.495537 139937686464256 session_manager.py:502] Done running local_init_op. I0911 02:28:54.010662 139937686464256 modeling.py:415] Reloading EMA... I0911 02:28:54.011811 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:56.213706 139937686464256 call_variants.py:444] Processed 1 examples in 1 batches [1145.647 sec per 100]. I0911 02:28:56.265293 139937686464256 call_variants.py:447] Processed 86 examples in 1 batches [13.334 sec per 100]. I0911 02:28:56.265480 139937686464256 call_variants.py:450] Done calling variants from a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:416,energy efficiency,CPU,CPU,416,"Hi @pichuan and @tedyun ~. I run the command which @pichuan provided but it still print nothing on terminal. . And the docker version of this machine is `Docker version 19.03.3, build a872fc2`. So I switch to another machine which supported AVX instruction and run the test again by @tedyun 's advice. Then it run normally and output the files as same as the tutorial. Here is the environment & the command:. - Host CPU info. ```text. processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 63. model name : Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz. stepping : 2. microcode : 0x43. cpu MHz : 1199.975. cache size : 25600 KB. physical id : 0. siblings : 20. core id : 0. cpu cores : 10. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 15. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:476,energy efficiency,cpu,cpu,476,"Hi @pichuan and @tedyun ~. I run the command which @pichuan provided but it still print nothing on terminal. . And the docker version of this machine is `Docker version 19.03.3, build a872fc2`. So I switch to another machine which supported AVX instruction and run the test again by @tedyun 's advice. Then it run normally and output the files as same as the tutorial. Here is the environment & the command:. - Host CPU info. ```text. processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 63. model name : Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz. stepping : 2. microcode : 0x43. cpu MHz : 1199.975. cache size : 25600 KB. physical id : 0. siblings : 20. core id : 0. cpu cores : 10. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 15. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:492,energy efficiency,model,model,492,"Hi @pichuan and @tedyun ~. I run the command which @pichuan provided but it still print nothing on terminal. . And the docker version of this machine is `Docker version 19.03.3, build a872fc2`. So I switch to another machine which supported AVX instruction and run the test again by @tedyun 's advice. Then it run normally and output the files as same as the tutorial. Here is the environment & the command:. - Host CPU info. ```text. processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 63. model name : Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz. stepping : 2. microcode : 0x43. cpu MHz : 1199.975. cache size : 25600 KB. physical id : 0. siblings : 20. core id : 0. cpu cores : 10. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 15. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:504,energy efficiency,model,model,504,"Hi @pichuan and @tedyun ~. I run the command which @pichuan provided but it still print nothing on terminal. . And the docker version of this machine is `Docker version 19.03.3, build a872fc2`. So I switch to another machine which supported AVX instruction and run the test again by @tedyun 's advice. Then it run normally and output the files as same as the tutorial. Here is the environment & the command:. - Host CPU info. ```text. processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 63. model name : Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz. stepping : 2. microcode : 0x43. cpu MHz : 1199.975. cache size : 25600 KB. physical id : 0. siblings : 20. core id : 0. cpu cores : 10. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 15. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:534,energy efficiency,CPU,CPU,534,"Hi @pichuan and @tedyun ~. I run the command which @pichuan provided but it still print nothing on terminal. . And the docker version of this machine is `Docker version 19.03.3, build a872fc2`. So I switch to another machine which supported AVX instruction and run the test again by @tedyun 's advice. Then it run normally and output the files as same as the tutorial. Here is the environment & the command:. - Host CPU info. ```text. processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 63. model name : Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz. stepping : 2. microcode : 0x43. cpu MHz : 1199.975. cache size : 25600 KB. physical id : 0. siblings : 20. core id : 0. cpu cores : 10. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 15. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:592,energy efficiency,cpu,cpu,592,"Hi @pichuan and @tedyun ~. I run the command which @pichuan provided but it still print nothing on terminal. . And the docker version of this machine is `Docker version 19.03.3, build a872fc2`. So I switch to another machine which supported AVX instruction and run the test again by @tedyun 's advice. Then it run normally and output the files as same as the tutorial. Here is the environment & the command:. - Host CPU info. ```text. processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 63. model name : Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz. stepping : 2. microcode : 0x43. cpu MHz : 1199.975. cache size : 25600 KB. physical id : 0. siblings : 20. core id : 0. cpu cores : 10. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 15. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:667,energy efficiency,core,core,667,"Hi @pichuan and @tedyun ~. I run the command which @pichuan provided but it still print nothing on terminal. . And the docker version of this machine is `Docker version 19.03.3, build a872fc2`. So I switch to another machine which supported AVX instruction and run the test again by @tedyun 's advice. Then it run normally and output the files as same as the tutorial. Here is the environment & the command:. - Host CPU info. ```text. processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 63. model name : Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz. stepping : 2. microcode : 0x43. cpu MHz : 1199.975. cache size : 25600 KB. physical id : 0. siblings : 20. core id : 0. cpu cores : 10. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 15. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:680,energy efficiency,cpu,cpu,680,"Hi @pichuan and @tedyun ~. I run the command which @pichuan provided but it still print nothing on terminal. . And the docker version of this machine is `Docker version 19.03.3, build a872fc2`. So I switch to another machine which supported AVX instruction and run the test again by @tedyun 's advice. Then it run normally and output the files as same as the tutorial. Here is the environment & the command:. - Host CPU info. ```text. processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 63. model name : Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz. stepping : 2. microcode : 0x43. cpu MHz : 1199.975. cache size : 25600 KB. physical id : 0. siblings : 20. core id : 0. cpu cores : 10. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 15. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:684,energy efficiency,core,cores,684,"Hi @pichuan and @tedyun ~. I run the command which @pichuan provided but it still print nothing on terminal. . And the docker version of this machine is `Docker version 19.03.3, build a872fc2`. So I switch to another machine which supported AVX instruction and run the test again by @tedyun 's advice. Then it run normally and output the files as same as the tutorial. Here is the environment & the command:. - Host CPU info. ```text. processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 63. model name : Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz. stepping : 2. microcode : 0x43. cpu MHz : 1199.975. cache size : 25600 KB. physical id : 0. siblings : 20. core id : 0. cpu cores : 10. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 15. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:760,energy efficiency,cpu,cpuid,760,"Hi @pichuan and @tedyun ~. I run the command which @pichuan provided but it still print nothing on terminal. . And the docker version of this machine is `Docker version 19.03.3, build a872fc2`. So I switch to another machine which supported AVX instruction and run the test again by @tedyun 's advice. Then it run normally and output the files as same as the tutorial. Here is the environment & the command:. - Host CPU info. ```text. processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 63. model name : Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz. stepping : 2. microcode : 0x43. cpu MHz : 1199.975. cache size : 25600 KB. physical id : 0. siblings : 20. core id : 0. cpu cores : 10. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 15. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:1057,energy efficiency,monitor,monitor,1057,"rovided but it still print nothing on terminal. . And the docker version of this machine is `Docker version 19.03.3, build a872fc2`. So I switch to another machine which supported AVX instruction and run the test again by @tedyun 's advice. Then it run normally and output the files as same as the tutorial. Here is the environment & the command:. - Host CPU info. ```text. processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 63. model name : Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz. stepping : 2. microcode : 0x43. cpu MHz : 1199.975. cache size : 25600 KB. physical id : 0. siblings : 20. core id : 0. cpu cores : 10. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 15. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.bam.bai. test_nist.b37_chr20_100kbp_at_10mb.bed. test_nist.b3",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:1562,energy efficiency,power,power,1562,"ing : 2. microcode : 0x43. cpu MHz : 1199.975. cache size : 25600 KB. physical id : 0. siblings : 20. core id : 0. cpu cores : 10. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 15. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.bam.bai. test_nist.b37_chr20_100kbp_at_10mb.bed. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. ucsc.hg19.chr20.unittest.fasta. ucsc.hg19.chr20.unittest.fasta.fai. ucsc.hg19.chr20.unittest.fasta.gz. ucsc.hg19.chr20.unittest.fasta.gz.fai. ucsc.hg19.chr20.unittest.fasta.gz.gzi. # docker run \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:1568,energy efficiency,manag,management,1568," microcode : 0x43. cpu MHz : 1199.975. cache size : 25600 KB. physical id : 0. siblings : 20. core id : 0. cpu cores : 10. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 15. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.bam.bai. test_nist.b37_chr20_100kbp_at_10mb.bed. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. ucsc.hg19.chr20.unittest.fasta. ucsc.hg19.chr20.unittest.fasta.fai. ucsc.hg19.chr20.unittest.fasta.gz. ucsc.hg19.chr20.unittest.fasta.gz.fai. ucsc.hg19.chr20.unittest.fasta.gz.gzi. # docker run \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:1642,energy efficiency,Core,CoreS,1642,"d : 0. siblings : 20. core id : 0. cpu cores : 10. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 15. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.bam.bai. test_nist.b37_chr20_100kbp_at_10mb.bed. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. ucsc.hg19.chr20.unittest.fasta. ucsc.hg19.chr20.unittest.fasta.fai. ucsc.hg19.chr20.unittest.fasta.gz. ucsc.hg19.chr20.unittest.fasta.gz.fai. ucsc.hg19.chr20.unittest.fasta.gz.gzi. # docker run \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:1801,energy efficiency,Core,Core,1801," de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.bam.bai. test_nist.b37_chr20_100kbp_at_10mb.bed. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. ucsc.hg19.chr20.unittest.fasta. ucsc.hg19.chr20.unittest.fasta.fai. ucsc.hg19.chr20.unittest.fasta.gz. ucsc.hg19.chr20.unittest.fasta.gz.fai. ucsc.hg19.chr20.unittest.fasta.gz.gzi. # docker run \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --intermediate_results_dir /output/intermediate_results_dir \. > -",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:5431,energy efficiency,model,models,5431,"will decode CRAM using the reference you passed in with --ref. 2020-09-11 02:28:39.919135: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. I0911 02:28:39.922723 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.923217 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:40.247040 140366075229952 make_examples.py:587] 6 candidates (6 examples) [0.33s elapsed]. I0911 02:28:41.745102 140366075229952 make_examples.py:587] Found 78 candidate variants. I0911 02:28:41.745359 140366075229952 make_examples.py:587] Created 86 examples. real 0m5.273s. user 0m5.898s. sys 0m3.792s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0911 02:28:44.682443 139937686464256 call_variants.py:335] Shape of input examples: [100, 221, 6]. 2020-09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-09-11 02:28:44.721599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2297525000 Hz. 2020-09-11 02:28:44.724518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5886480 executing computations on platform Host. Devices:. 2020-09-11 02:28:44.724555: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-09-11 02:28:44.727738: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_pa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:5442,energy efficiency,model,model,5442," CRAM using the reference you passed in with --ref. 2020-09-11 02:28:39.919135: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. I0911 02:28:39.922723 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.923217 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:40.247040 140366075229952 make_examples.py:587] 6 candidates (6 examples) [0.33s elapsed]. I0911 02:28:41.745102 140366075229952 make_examples.py:587] Found 78 candidate variants. I0911 02:28:41.745359 140366075229952 make_examples.py:587] Created 86 examples. real 0m5.273s. user 0m5.898s. sys 0m3.792s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0911 02:28:44.682443 139937686464256 call_variants.py:335] Shape of input examples: [100, 221, 6]. 2020-09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-09-11 02:28:44.721599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2297525000 Hz. 2020-09-11 02:28:44.724518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5886480 executing computations on platform Host. Devices:. 2020-09-11 02:28:44.724555: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-09-11 02:28:44.727738: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:5596,energy efficiency,core,core,5596," 134217728. I0911 02:28:39.922723 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.923217 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:40.247040 140366075229952 make_examples.py:587] 6 candidates (6 examples) [0.33s elapsed]. I0911 02:28:41.745102 140366075229952 make_examples.py:587] Found 78 candidate variants. I0911 02:28:41.745359 140366075229952 make_examples.py:587] Created 86 examples. real 0m5.273s. user 0m5.898s. sys 0m3.792s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0911 02:28:44.682443 139937686464256 call_variants.py:335] Shape of input examples: [100, 221, 6]. 2020-09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-09-11 02:28:44.721599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2297525000 Hz. 2020-09-11 02:28:44.724518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5886480 executing computations on platform Host. Devices:. 2020-09-11 02:28:44.724555: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-09-11 02:28:44.727738: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W0911 02:28:44.755701 139937686464256 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp8bwus8dp. I0911 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:5662,energy efficiency,optim,optimized,5662,"223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.923217 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:40.247040 140366075229952 make_examples.py:587] 6 candidates (6 examples) [0.33s elapsed]. I0911 02:28:41.745102 140366075229952 make_examples.py:587] Found 78 candidate variants. I0911 02:28:41.745359 140366075229952 make_examples.py:587] Created 86 examples. real 0m5.273s. user 0m5.898s. sys 0m3.792s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0911 02:28:44.682443 139937686464256 call_variants.py:335] Shape of input examples: [100, 221, 6]. 2020-09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-09-11 02:28:44.721599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2297525000 Hz. 2020-09-11 02:28:44.724518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5886480 executing computations on platform Host. Devices:. 2020-09-11 02:28:44.724555: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-09-11 02:28:44.727738: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W0911 02:28:44.755701 139937686464256 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp8bwus8dp. I0911 02:28:44.756375 139937686464256 estimator.py:212] Using config: {'_mo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:5715,energy efficiency,CPU,CPU,5715," with NativeSamReader. I0911 02:28:39.923217 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:40.247040 140366075229952 make_examples.py:587] 6 candidates (6 examples) [0.33s elapsed]. I0911 02:28:41.745102 140366075229952 make_examples.py:587] Found 78 candidate variants. I0911 02:28:41.745359 140366075229952 make_examples.py:587] Created 86 examples. real 0m5.273s. user 0m5.898s. sys 0m3.792s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0911 02:28:44.682443 139937686464256 call_variants.py:335] Shape of input examples: [100, 221, 6]. 2020-09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-09-11 02:28:44.721599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2297525000 Hz. 2020-09-11 02:28:44.724518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5886480 executing computations on platform Host. Devices:. 2020-09-11 02:28:44.724555: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-09-11 02:28:44.727738: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W0911 02:28:44.755701 139937686464256 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp8bwus8dp. I0911 02:28:44.756375 139937686464256 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp8bwus8dp', '_tf_random_seed': N",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:5917,energy efficiency,core,core,5917,"es.py:587] 6 candidates (6 examples) [0.33s elapsed]. I0911 02:28:41.745102 140366075229952 make_examples.py:587] Found 78 candidate variants. I0911 02:28:41.745359 140366075229952 make_examples.py:587] Created 86 examples. real 0m5.273s. user 0m5.898s. sys 0m3.792s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0911 02:28:44.682443 139937686464256 call_variants.py:335] Shape of input examples: [100, 221, 6]. 2020-09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-09-11 02:28:44.721599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2297525000 Hz. 2020-09-11 02:28:44.724518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5886480 executing computations on platform Host. Devices:. 2020-09-11 02:28:44.724555: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-09-11 02:28:44.727738: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W0911 02:28:44.755701 139937686464256 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp8bwus8dp. I0911 02:28:44.756375 139937686464256 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp8bwus8dp', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:5962,energy efficiency,CPU,CPU,5962,"lapsed]. I0911 02:28:41.745102 140366075229952 make_examples.py:587] Found 78 candidate variants. I0911 02:28:41.745359 140366075229952 make_examples.py:587] Created 86 examples. real 0m5.273s. user 0m5.898s. sys 0m3.792s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0911 02:28:44.682443 139937686464256 call_variants.py:335] Shape of input examples: [100, 221, 6]. 2020-09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-09-11 02:28:44.721599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2297525000 Hz. 2020-09-11 02:28:44.724518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5886480 executing computations on platform Host. Devices:. 2020-09-11 02:28:44.724555: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-09-11 02:28:44.727738: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W0911 02:28:44.755701 139937686464256 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp8bwus8dp. I0911 02:28:44.756375 139937686464256 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp8bwus8dp', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:5966,energy efficiency,Frequenc,Frequency,5966,". I0911 02:28:41.745102 140366075229952 make_examples.py:587] Found 78 candidate variants. I0911 02:28:41.745359 140366075229952 make_examples.py:587] Created 86 examples. real 0m5.273s. user 0m5.898s. sys 0m3.792s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0911 02:28:44.682443 139937686464256 call_variants.py:335] Shape of input examples: [100, 221, 6]. 2020-09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-09-11 02:28:44.721599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2297525000 Hz. 2020-09-11 02:28:44.724518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5886480 executing computations on platform Host. Devices:. 2020-09-11 02:28:44.724555: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-09-11 02:28:44.727738: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W0911 02:28:44.755701 139937686464256 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp8bwus8dp. I0911 02:28:44.756375 139937686464256 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp8bwus8dp', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_devi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:6312,energy efficiency,core,core,6312,"nts_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0911 02:28:44.682443 139937686464256 call_variants.py:335] Shape of input examples: [100, 221, 6]. 2020-09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-09-11 02:28:44.721599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2297525000 Hz. 2020-09-11 02:28:44.724518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5886480 executing computations on platform Host. Devices:. 2020-09-11 02:28:44.724555: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-09-11 02:28:44.727738: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W0911 02:28:44.755701 139937686464256 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp8bwus8dp. I0911 02:28:44.756375 139937686464256 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp8bwus8dp', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f4549cfe518>, '_task_type': 'worker', '_task_id': 0, '_global_id_i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:6512,energy efficiency,estimat,estimator,6512,"ape of input examples: [100, 221, 6]. 2020-09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-09-11 02:28:44.721599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2297525000 Hz. 2020-09-11 02:28:44.724518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5886480 executing computations on platform Host. Devices:. 2020-09-11 02:28:44.724555: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-09-11 02:28:44.727738: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W0911 02:28:44.755701 139937686464256 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp8bwus8dp. I0911 02:28:44.756375 139937686464256 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp8bwus8dp', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f4549cfe518>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I0911 02:28:44.756798 139937686464256 call_variants.py:426] Writing calls to ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:6557,energy efficiency,model,model,6557,"09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-09-11 02:28:44.721599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2297525000 Hz. 2020-09-11 02:28:44.724518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5886480 executing computations on platform Host. Devices:. 2020-09-11 02:28:44.724555: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-09-11 02:28:44.727738: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W0911 02:28:44.755701 139937686464256 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp8bwus8dp. I0911 02:28:44.756375 139937686464256 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp8bwus8dp', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f4549cfe518>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I0911 02:28:44.756798 139937686464256 call_variants.py:426] Writing calls to /output/intermediate_results_dir/call_varia",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:6630,energy efficiency,estimat,estimator,6630," This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-09-11 02:28:44.721599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2297525000 Hz. 2020-09-11 02:28:44.724518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5886480 executing computations on platform Host. Devices:. 2020-09-11 02:28:44.724555: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-09-11 02:28:44.727738: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W0911 02:28:44.755701 139937686464256 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp8bwus8dp. I0911 02:28:44.756375 139937686464256 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp8bwus8dp', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f4549cfe518>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I0911 02:28:44.756798 139937686464256 call_variants.py:426] Writing calls to /output/intermediate_results_dir/call_variants_output.tfrecord.gz. W0911 02:28:44.767428 139937686464256 deprecation.p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:8969,energy efficiency,optim,optimizations,8969,"o layers. W0911 02:28:44.788271 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. W0911 02:28:44.973487 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I0911 02:28:45.531516 139937686464256 estimator.py:1147] Calling model_fn. W0911 02:28:45.537137 139937686464256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0911 02:28:50.978360 139937686464256 estimator.py:1149] Done calling model_fn. I0911 02:28:52.395916 139937686464256 monitored_session.py:240] Graph was finalized. I0911 02:28:52.397383 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:53.423366 139937686464256 session_manager.py:500] Running local_init_op. I0911 02:28:53.495537 139937686464256 session_manager.py:502] Done running local_init_op. I0911 02:28:54.010662 139937686464256 modeling.py:415] Reloading EMA... I0911 02:28:54.011811 139937",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:9071,energy efficiency,estimat,estimator,9071,"s/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. W0911 02:28:44.973487 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I0911 02:28:45.531516 139937686464256 estimator.py:1147] Calling model_fn. W0911 02:28:45.537137 139937686464256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0911 02:28:50.978360 139937686464256 estimator.py:1149] Done calling model_fn. I0911 02:28:52.395916 139937686464256 monitored_session.py:240] Graph was finalized. I0911 02:28:52.397383 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:53.423366 139937686464256 session_manager.py:500] Running local_init_op. I0911 02:28:53.495537 139937686464256 session_manager.py:502] Done running local_init_op. I0911 02:28:54.010662 139937686464256 modeling.py:415] Reloading EMA... I0911 02:28:54.011811 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:56.213706",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:9467,energy efficiency,estimat,estimator,9467,"use `tf.data.Options.experimental_determinstic`. W0911 02:28:44.973487 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I0911 02:28:45.531516 139937686464256 estimator.py:1147] Calling model_fn. W0911 02:28:45.537137 139937686464256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0911 02:28:50.978360 139937686464256 estimator.py:1149] Done calling model_fn. I0911 02:28:52.395916 139937686464256 monitored_session.py:240] Graph was finalized. I0911 02:28:52.397383 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:53.423366 139937686464256 session_manager.py:500] Running local_init_op. I0911 02:28:53.495537 139937686464256 session_manager.py:502] Done running local_init_op. I0911 02:28:54.010662 139937686464256 modeling.py:415] Reloading EMA... I0911 02:28:54.011811 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:56.213706 139937686464256 call_variants.py:444] Processed 1 examples in 1 batches [1145.647 sec per 100]. I0911 02:28:56.265293 139937686464256 call_variants.py:447] Processed 86 examples in 1 batches [13.334 sec per 100]. I0911 02:28:56.265480 139937686464256 call_variants.py:450] Done calling variants from a total of 86 examples. real 0m14.761s. user 0m20.588s. sys 0m5.092s. ***** Running the command",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:9678,energy efficiency,model,models,9678,"ap_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I0911 02:28:45.531516 139937686464256 estimator.py:1147] Calling model_fn. W0911 02:28:45.537137 139937686464256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0911 02:28:50.978360 139937686464256 estimator.py:1149] Done calling model_fn. I0911 02:28:52.395916 139937686464256 monitored_session.py:240] Graph was finalized. I0911 02:28:52.397383 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:53.423366 139937686464256 session_manager.py:500] Running local_init_op. I0911 02:28:53.495537 139937686464256 session_manager.py:502] Done running local_init_op. I0911 02:28:54.010662 139937686464256 modeling.py:415] Reloading EMA... I0911 02:28:54.011811 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:56.213706 139937686464256 call_variants.py:444] Processed 1 examples in 1 batches [1145.647 sec per 100]. I0911 02:28:56.265293 139937686464256 call_variants.py:447] Processed 86 examples in 1 batches [13.334 sec per 100]. I0911 02:28:56.265480 139937686464256 call_variants.py:450] Done calling variants from a total of 86 examples. real 0m14.761s. user 0m20.588s. sys 0m5.092s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --infile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/output/output.vcf.g",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:9689,energy efficiency,model,model,9689,"h (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I0911 02:28:45.531516 139937686464256 estimator.py:1147] Calling model_fn. W0911 02:28:45.537137 139937686464256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0911 02:28:50.978360 139937686464256 estimator.py:1149] Done calling model_fn. I0911 02:28:52.395916 139937686464256 monitored_session.py:240] Graph was finalized. I0911 02:28:52.397383 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:53.423366 139937686464256 session_manager.py:500] Running local_init_op. I0911 02:28:53.495537 139937686464256 session_manager.py:502] Done running local_init_op. I0911 02:28:54.010662 139937686464256 modeling.py:415] Reloading EMA... I0911 02:28:54.011811 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:56.213706 139937686464256 call_variants.py:444] Processed 1 examples in 1 batches [1145.647 sec per 100]. I0911 02:28:56.265293 139937686464256 call_variants.py:447] Processed 86 examples in 1 batches [13.334 sec per 100]. I0911 02:28:56.265480 139937686464256 call_variants.py:450] Done calling variants from a total of 86 examples. real 0m14.761s. user 0m20.588s. sys 0m5.092s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --infile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/output/output.vcf.gz"" --nonvar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:9914,energy efficiency,model,modeling,9914,"batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I0911 02:28:45.531516 139937686464256 estimator.py:1147] Calling model_fn. W0911 02:28:45.537137 139937686464256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0911 02:28:50.978360 139937686464256 estimator.py:1149] Done calling model_fn. I0911 02:28:52.395916 139937686464256 monitored_session.py:240] Graph was finalized. I0911 02:28:52.397383 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:53.423366 139937686464256 session_manager.py:500] Running local_init_op. I0911 02:28:53.495537 139937686464256 session_manager.py:502] Done running local_init_op. I0911 02:28:54.010662 139937686464256 modeling.py:415] Reloading EMA... I0911 02:28:54.011811 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:56.213706 139937686464256 call_variants.py:444] Processed 1 examples in 1 batches [1145.647 sec per 100]. I0911 02:28:56.265293 139937686464256 call_variants.py:447] Processed 86 examples in 1 batches [13.334 sec per 100]. I0911 02:28:56.265480 139937686464256 call_variants.py:450] Done calling variants from a total of 86 examples. real 0m14.761s. user 0m20.588s. sys 0m5.092s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --infile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/output/output.g.vcf.gz"". 2020-09-11 02:28:59.543794: I deepvariant/postprocess_variants.cc:88] Read from: /output/intermediate_resu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:10032,energy efficiency,model,models,10032,"911 02:28:45.531516 139937686464256 estimator.py:1147] Calling model_fn. W0911 02:28:45.537137 139937686464256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0911 02:28:50.978360 139937686464256 estimator.py:1149] Done calling model_fn. I0911 02:28:52.395916 139937686464256 monitored_session.py:240] Graph was finalized. I0911 02:28:52.397383 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:53.423366 139937686464256 session_manager.py:500] Running local_init_op. I0911 02:28:53.495537 139937686464256 session_manager.py:502] Done running local_init_op. I0911 02:28:54.010662 139937686464256 modeling.py:415] Reloading EMA... I0911 02:28:54.011811 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:56.213706 139937686464256 call_variants.py:444] Processed 1 examples in 1 batches [1145.647 sec per 100]. I0911 02:28:56.265293 139937686464256 call_variants.py:447] Processed 86 examples in 1 batches [13.334 sec per 100]. I0911 02:28:56.265480 139937686464256 call_variants.py:450] Done calling variants from a total of 86 examples. real 0m14.761s. user 0m20.588s. sys 0m5.092s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --infile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/output/output.g.vcf.gz"". 2020-09-11 02:28:59.543794: I deepvariant/postprocess_variants.cc:88] Read from: /output/intermediate_results_dir/call_variants_output.tfrecord.gz. 2020-09-11 02:28:59.544704: I deepvariant/postprocess_variants.cc:103] Tota",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:10043,energy efficiency,model,model,10043,"5.531516 139937686464256 estimator.py:1147] Calling model_fn. W0911 02:28:45.537137 139937686464256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0911 02:28:50.978360 139937686464256 estimator.py:1149] Done calling model_fn. I0911 02:28:52.395916 139937686464256 monitored_session.py:240] Graph was finalized. I0911 02:28:52.397383 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:53.423366 139937686464256 session_manager.py:500] Running local_init_op. I0911 02:28:53.495537 139937686464256 session_manager.py:502] Done running local_init_op. I0911 02:28:54.010662 139937686464256 modeling.py:415] Reloading EMA... I0911 02:28:54.011811 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:56.213706 139937686464256 call_variants.py:444] Processed 1 examples in 1 batches [1145.647 sec per 100]. I0911 02:28:56.265293 139937686464256 call_variants.py:447] Processed 86 examples in 1 batches [13.334 sec per 100]. I0911 02:28:56.265480 139937686464256 call_variants.py:450] Done calling variants from a total of 86 examples. real 0m14.761s. user 0m20.588s. sys 0m5.092s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --infile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/output/output.g.vcf.gz"". 2020-09-11 02:28:59.543794: I deepvariant/postprocess_variants.cc:88] Read from: /output/intermediate_results_dir/call_variants_output.tfrecord.gz. 2020-09-11 02:28:59.544704: I deepvariant/postprocess_variants.cc:103] Total #entries ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:126,integrability,version,version,126,"Hi @pichuan and @tedyun ~. I run the command which @pichuan provided but it still print nothing on terminal. . And the docker version of this machine is `Docker version 19.03.3, build a872fc2`. So I switch to another machine which supported AVX instruction and run the test again by @tedyun 's advice. Then it run normally and output the files as same as the tutorial. Here is the environment & the command:. - Host CPU info. ```text. processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 63. model name : Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz. stepping : 2. microcode : 0x43. cpu MHz : 1199.975. cache size : 25600 KB. physical id : 0. siblings : 20. core id : 0. cpu cores : 10. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 15. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:161,integrability,version,version,161,"Hi @pichuan and @tedyun ~. I run the command which @pichuan provided but it still print nothing on terminal. . And the docker version of this machine is `Docker version 19.03.3, build a872fc2`. So I switch to another machine which supported AVX instruction and run the test again by @tedyun 's advice. Then it run normally and output the files as same as the tutorial. Here is the environment & the command:. - Host CPU info. ```text. processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 63. model name : Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz. stepping : 2. microcode : 0x43. cpu MHz : 1199.975. cache size : 25600 KB. physical id : 0. siblings : 20. core id : 0. cpu cores : 10. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 15. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:696,integrability,api,apicid,696,"Hi @pichuan and @tedyun ~. I run the command which @pichuan provided but it still print nothing on terminal. . And the docker version of this machine is `Docker version 19.03.3, build a872fc2`. So I switch to another machine which supported AVX instruction and run the test again by @tedyun 's advice. Then it run normally and output the files as same as the tutorial. Here is the environment & the command:. - Host CPU info. ```text. processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 63. model name : Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz. stepping : 2. microcode : 0x43. cpu MHz : 1199.975. cache size : 25600 KB. physical id : 0. siblings : 20. core id : 0. cpu cores : 10. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 15. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:716,integrability,api,apicid,716,"Hi @pichuan and @tedyun ~. I run the command which @pichuan provided but it still print nothing on terminal. . And the docker version of this machine is `Docker version 19.03.3, build a872fc2`. So I switch to another machine which supported AVX instruction and run the test again by @tedyun 's advice. Then it run normally and output the files as same as the tutorial. Here is the environment & the command:. - Host CPU info. ```text. processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 63. model name : Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz. stepping : 2. microcode : 0x43. cpu MHz : 1199.975. cache size : 25600 KB. physical id : 0. siblings : 20. core id : 0. cpu cores : 10. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 15. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:831,integrability,api,apic,831,"Hi @pichuan and @tedyun ~. I run the command which @pichuan provided but it still print nothing on terminal. . And the docker version of this machine is `Docker version 19.03.3, build a872fc2`. So I switch to another machine which supported AVX instruction and run the test again by @tedyun 's advice. Then it run normally and output the files as same as the tutorial. Here is the environment & the command:. - Host CPU info. ```text. processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 63. model name : Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz. stepping : 2. microcode : 0x43. cpu MHz : 1199.975. cache size : 25600 KB. physical id : 0. siblings : 20. core id : 0. cpu cores : 10. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 15. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:1608,integrability,version,version,1608,"cache size : 25600 KB. physical id : 0. siblings : 20. core id : 0. cpu cores : 10. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 15. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.bam.bai. test_nist.b37_chr20_100kbp_at_10mb.bed. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. ucsc.hg19.chr20.unittest.fasta. ucsc.hg19.chr20.unittest.fasta.fai. ucsc.hg19.chr20.unittest.fasta.gz. ucsc.hg19.chr20.unittest.fasta.gz.fai. ucsc.hg19.chr20.unittest.fasta.gz.gzi. # docker run \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. > --r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:1828,integrability,version,version,1828,"pic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.bam.bai. test_nist.b37_chr20_100kbp_at_10mb.bed. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. ucsc.hg19.chr20.unittest.fasta. ucsc.hg19.chr20.unittest.fasta.fai. ucsc.hg19.chr20.unittest.fasta.gz. ucsc.hg19.chr20.unittest.fasta.gz.fai. ucsc.hg19.chr20.unittest.fasta.gz.gzi. # docker run \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=1 \. >. Status: D",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:3201,integrability,buffer,buffer,3201,"i. ucsc.hg19.chr20.unittest.fasta.gz. ucsc.hg19.chr20.unittest.fasta.gz.fai. ucsc.hg19.chr20.unittest.fasta.gz.gzi. # docker run \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=1 \. >. Status: Downloaded newer image for google/deepvariant:1.0.0. I0911 02:28:36.697342 140021722134272 run_deepvariant.py:269] Creating a directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. I0911 02:28:39.900765 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.914417 140366075229952 make_examples.py:587] Preparing inputs. I0911 02:28:39.915202 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.916366 140366075229952 make_examples.py:587] Common contigs are ['chr20']. I0911 02:28:39.917786 140366075229952 make_examples.py:587] Writing examples to /output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz. I0911 02:28:39.917969 140366075229952 make_examples.py:587] Writing gvcf records to",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:6046,integrability,servic,service,6046,"e variants. I0911 02:28:41.745359 140366075229952 make_examples.py:587] Created 86 examples. real 0m5.273s. user 0m5.898s. sys 0m3.792s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0911 02:28:44.682443 139937686464256 call_variants.py:335] Shape of input examples: [100, 221, 6]. 2020-09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-09-11 02:28:44.721599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2297525000 Hz. 2020-09-11 02:28:44.724518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5886480 executing computations on platform Host. Devices:. 2020-09-11 02:28:44.724555: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-09-11 02:28:44.727738: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W0911 02:28:44.755701 139937686464256 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp8bwus8dp. I0911 02:28:44.756375 139937686464256 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp8bwus8dp', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:6054,integrability,servic,service,6054,"ts. I0911 02:28:41.745359 140366075229952 make_examples.py:587] Created 86 examples. real 0m5.273s. user 0m5.898s. sys 0m3.792s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0911 02:28:44.682443 139937686464256 call_variants.py:335] Shape of input examples: [100, 221, 6]. 2020-09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-09-11 02:28:44.721599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2297525000 Hz. 2020-09-11 02:28:44.724518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5886480 executing computations on platform Host. Devices:. 2020-09-11 02:28:44.724555: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-09-11 02:28:44.727738: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W0911 02:28:44.755701 139937686464256 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp8bwus8dp. I0911 02:28:44.756375 139937686464256 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp8bwus8dp', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:6074,integrability,servic,service,6074,"45359 140366075229952 make_examples.py:587] Created 86 examples. real 0m5.273s. user 0m5.898s. sys 0m3.792s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0911 02:28:44.682443 139937686464256 call_variants.py:335] Shape of input examples: [100, 221, 6]. 2020-09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-09-11 02:28:44.721599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2297525000 Hz. 2020-09-11 02:28:44.724518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5886480 executing computations on platform Host. Devices:. 2020-09-11 02:28:44.724555: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-09-11 02:28:44.727738: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W0911 02:28:44.755701 139937686464256 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp8bwus8dp. I0911 02:28:44.756375 139937686464256 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp8bwus8dp', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:6197,integrability,servic,service,6197,"the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0911 02:28:44.682443 139937686464256 call_variants.py:335] Shape of input examples: [100, 221, 6]. 2020-09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-09-11 02:28:44.721599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2297525000 Hz. 2020-09-11 02:28:44.724518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5886480 executing computations on platform Host. Devices:. 2020-09-11 02:28:44.724555: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-09-11 02:28:44.727738: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W0911 02:28:44.755701 139937686464256 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp8bwus8dp. I0911 02:28:44.756375 139937686464256 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp8bwus8dp', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.pytho",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:6205,integrability,servic,service,6205,"and:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0911 02:28:44.682443 139937686464256 call_variants.py:335] Shape of input examples: [100, 221, 6]. 2020-09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-09-11 02:28:44.721599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2297525000 Hz. 2020-09-11 02:28:44.724518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5886480 executing computations on platform Host. Devices:. 2020-09-11 02:28:44.724555: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-09-11 02:28:44.727738: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W0911 02:28:44.755701 139937686464256 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp8bwus8dp. I0911 02:28:44.756375 139937686464256 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp8bwus8dp', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.traini",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:6262,integrability,Version,Version,6262,"ile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0911 02:28:44.682443 139937686464256 call_variants.py:335] Shape of input examples: [100, 221, 6]. 2020-09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-09-11 02:28:44.721599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2297525000 Hz. 2020-09-11 02:28:44.724518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5886480 executing computations on platform Host. Devices:. 2020-09-11 02:28:44.724555: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-09-11 02:28:44.727738: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W0911 02:28:44.755701 139937686464256 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp8bwus8dp. I0911 02:28:44.756375 139937686464256 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp8bwus8dp', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f4549cfe518>, '_ta",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:7895,integrability,version,version,7895,": 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f4549cfe518>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I0911 02:28:44.756798 139937686464256 call_variants.py:426] Writing calls to /output/intermediate_results_dir/call_variants_output.tfrecord.gz. W0911 02:28:44.767428 139937686464256 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version. Instructions for updating:. If using Keras pass *_constraint arguments to layers. W0911 02:28:44.788271 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. W0911 02:28:44.973487 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:8273,integrability,version,version,8273,"worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I0911 02:28:44.756798 139937686464256 call_variants.py:426] Writing calls to /output/intermediate_results_dir/call_variants_output.tfrecord.gz. W0911 02:28:44.767428 139937686464256 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version. Instructions for updating:. If using Keras pass *_constraint arguments to layers. W0911 02:28:44.788271 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. W0911 02:28:44.973487 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I0911 02:28:45.531516 139937686464256 estimator.py:1147] Calling model_fn. W0911 02:28:45.537137 139937686464256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:8740,integrability,batch,batching,8740,"calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version. Instructions for updating:. If using Keras pass *_constraint arguments to layers. W0911 02:28:44.788271 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. W0911 02:28:44.973487 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I0911 02:28:45.531516 139937686464256 estimator.py:1147] Calling model_fn. W0911 02:28:45.537137 139937686464256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0911 02:28:50.978360 139937686464256 estimator.py:1149] Done calling model_fn. I0911 02:28:52.395916 139937686464256 monitored_session.py:240] Graph was finalized. I0911 02:28:52.397383 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:53.423366 139937686464256 sessi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:8796,integrability,version,version,8796,"ython.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version. Instructions for updating:. If using Keras pass *_constraint arguments to layers. W0911 02:28:44.788271 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. W0911 02:28:44.973487 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I0911 02:28:45.531516 139937686464256 estimator.py:1147] Calling model_fn. W0911 02:28:45.537137 139937686464256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0911 02:28:50.978360 139937686464256 estimator.py:1149] Done calling model_fn. I0911 02:28:52.395916 139937686464256 monitored_session.py:240] Graph was finalized. I0911 02:28:52.397383 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:53.423366 139937686464256 session_manager.py:500] Running local_init_op. I0911 02:28:53",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:8918,integrability,batch,batch,8918,"updating:. If using Keras pass *_constraint arguments to layers. W0911 02:28:44.788271 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. W0911 02:28:44.973487 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I0911 02:28:45.531516 139937686464256 estimator.py:1147] Calling model_fn. W0911 02:28:45.537137 139937686464256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0911 02:28:50.978360 139937686464256 estimator.py:1149] Done calling model_fn. I0911 02:28:52.395916 139937686464256 monitored_session.py:240] Graph was finalized. I0911 02:28:52.397383 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:53.423366 139937686464256 session_manager.py:500] Running local_init_op. I0911 02:28:53.495537 139937686464256 session_manager.py:502] Done running local_init_op. I0911 02:28:54.010662 139937686464256 modelin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:9348,integrability,version,version,9348,"cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. W0911 02:28:44.973487 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I0911 02:28:45.531516 139937686464256 estimator.py:1147] Calling model_fn. W0911 02:28:45.537137 139937686464256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0911 02:28:50.978360 139937686464256 estimator.py:1149] Done calling model_fn. I0911 02:28:52.395916 139937686464256 monitored_session.py:240] Graph was finalized. I0911 02:28:52.397383 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:53.423366 139937686464256 session_manager.py:500] Running local_init_op. I0911 02:28:53.495537 139937686464256 session_manager.py:502] Done running local_init_op. I0911 02:28:54.010662 139937686464256 modeling.py:415] Reloading EMA... I0911 02:28:54.011811 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:56.213706 139937686464256 call_variants.py:444] Processed 1 examples in 1 batches [1145.647 sec per 100]. I0911 02:28:56.265293 139937686464256 call_variants.py:447] Processed 86 examples in 1 batches [13.334 sec per 100]. I0911 02:28:56.265480 139937686464256 call_variants.py:450] Do",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:10141,integrability,batch,batches,10141," deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0911 02:28:50.978360 139937686464256 estimator.py:1149] Done calling model_fn. I0911 02:28:52.395916 139937686464256 monitored_session.py:240] Graph was finalized. I0911 02:28:52.397383 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:53.423366 139937686464256 session_manager.py:500] Running local_init_op. I0911 02:28:53.495537 139937686464256 session_manager.py:502] Done running local_init_op. I0911 02:28:54.010662 139937686464256 modeling.py:415] Reloading EMA... I0911 02:28:54.011811 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:56.213706 139937686464256 call_variants.py:444] Processed 1 examples in 1 batches [1145.647 sec per 100]. I0911 02:28:56.265293 139937686464256 call_variants.py:447] Processed 86 examples in 1 batches [13.334 sec per 100]. I0911 02:28:56.265480 139937686464256 call_variants.py:450] Done calling variants from a total of 86 examples. real 0m14.761s. user 0m20.588s. sys 0m5.092s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --infile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/output/output.g.vcf.gz"". 2020-09-11 02:28:59.543794: I deepvariant/postprocess_variants.cc:88] Read from: /output/intermediate_results_dir/call_variants_output.tfrecord.gz. 2020-09-11 02:28:59.544704: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 86. I0911 02:28:59.545160 140173302474496 postprocess_variants.py:1079] CVO ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:10260,integrability,batch,batches,10260,"rflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0911 02:28:50.978360 139937686464256 estimator.py:1149] Done calling model_fn. I0911 02:28:52.395916 139937686464256 monitored_session.py:240] Graph was finalized. I0911 02:28:52.397383 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:53.423366 139937686464256 session_manager.py:500] Running local_init_op. I0911 02:28:53.495537 139937686464256 session_manager.py:502] Done running local_init_op. I0911 02:28:54.010662 139937686464256 modeling.py:415] Reloading EMA... I0911 02:28:54.011811 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:56.213706 139937686464256 call_variants.py:444] Processed 1 examples in 1 batches [1145.647 sec per 100]. I0911 02:28:56.265293 139937686464256 call_variants.py:447] Processed 86 examples in 1 batches [13.334 sec per 100]. I0911 02:28:56.265480 139937686464256 call_variants.py:450] Done calling variants from a total of 86 examples. real 0m14.761s. user 0m20.588s. sys 0m5.092s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --infile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/output/output.g.vcf.gz"". 2020-09-11 02:28:59.543794: I deepvariant/postprocess_variants.cc:88] Read from: /output/intermediate_results_dir/call_variants_output.tfrecord.gz. 2020-09-11 02:28:59.544704: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 86. I0911 02:28:59.545160 140173302474496 postprocess_variants.py:1079] CVO sorting took 2.2590160369873047e-05 minutes. I0911 02:28:59.545950 140173302474496 postprocess_variants.py:1081] Transf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:11258,integrability,Transform,Transforming,11258,":56.265293 139937686464256 call_variants.py:447] Processed 86 examples in 1 batches [13.334 sec per 100]. I0911 02:28:56.265480 139937686464256 call_variants.py:450] Done calling variants from a total of 86 examples. real 0m14.761s. user 0m20.588s. sys 0m5.092s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --infile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/output/output.g.vcf.gz"". 2020-09-11 02:28:59.543794: I deepvariant/postprocess_variants.cc:88] Read from: /output/intermediate_results_dir/call_variants_output.tfrecord.gz. 2020-09-11 02:28:59.544704: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 86. I0911 02:28:59.545160 140173302474496 postprocess_variants.py:1079] CVO sorting took 2.2590160369873047e-05 minutes. I0911 02:28:59.545950 140173302474496 postprocess_variants.py:1081] Transforming call_variants_output to variants. I0911 02:28:59.546221 140173302474496 postprocess_variants.py:1108] Merging and writing variants to VCF and gVCF. I0911 02:28:59.547042 140173302474496 genomics_writer.py:172] Writing /output/output.vcf.gz with NativeVcfWriter. I0911 02:28:59.547737 140173302474496 genomics_writer.py:172] Writing /output/output.g.vcf.gz with NativeVcfWriter. I0911 02:29:00.128752 140173302474496 postprocess_variants.py:1128] Finished writing VCF and gVCF in 0.00970850388209025 minutes. I0911 02:29:00.129857 140173302474496 genomics_reader.py:223] Reading /output/output.vcf.gz with NativeVcfReader. real 0m3.961s. user 0m4.633s. sys 0m3.567s. # ls -1 ${OUTPUT_DIR}. intermediate_results_dir. output.g.vcf.gz. output.g.vcf.gz.tbi. output.vcf.gz. output.vcf.gz.tbi. output.visual_report.html. ```. I think it works and thanks for your rapidly help about this~ @pichuan @tedyun . Best,. Jerry.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:696,interoperability,api,apicid,696,"Hi @pichuan and @tedyun ~. I run the command which @pichuan provided but it still print nothing on terminal. . And the docker version of this machine is `Docker version 19.03.3, build a872fc2`. So I switch to another machine which supported AVX instruction and run the test again by @tedyun 's advice. Then it run normally and output the files as same as the tutorial. Here is the environment & the command:. - Host CPU info. ```text. processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 63. model name : Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz. stepping : 2. microcode : 0x43. cpu MHz : 1199.975. cache size : 25600 KB. physical id : 0. siblings : 20. core id : 0. cpu cores : 10. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 15. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:716,interoperability,api,apicid,716,"Hi @pichuan and @tedyun ~. I run the command which @pichuan provided but it still print nothing on terminal. . And the docker version of this machine is `Docker version 19.03.3, build a872fc2`. So I switch to another machine which supported AVX instruction and run the test again by @tedyun 's advice. Then it run normally and output the files as same as the tutorial. Here is the environment & the command:. - Host CPU info. ```text. processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 63. model name : Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz. stepping : 2. microcode : 0x43. cpu MHz : 1199.975. cache size : 25600 KB. physical id : 0. siblings : 20. core id : 0. cpu cores : 10. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 15. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:831,interoperability,api,apic,831,"Hi @pichuan and @tedyun ~. I run the command which @pichuan provided but it still print nothing on terminal. . And the docker version of this machine is `Docker version 19.03.3, build a872fc2`. So I switch to another machine which supported AVX instruction and run the test again by @tedyun 's advice. Then it run normally and output the files as same as the tutorial. Here is the environment & the command:. - Host CPU info. ```text. processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 63. model name : Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz. stepping : 2. microcode : 0x43. cpu MHz : 1199.975. cache size : 25600 KB. physical id : 0. siblings : 20. core id : 0. cpu cores : 10. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 15. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:5601,interoperability,platform,platform,5601,"728. I0911 02:28:39.922723 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.923217 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:40.247040 140366075229952 make_examples.py:587] 6 candidates (6 examples) [0.33s elapsed]. I0911 02:28:41.745102 140366075229952 make_examples.py:587] Found 78 candidate variants. I0911 02:28:41.745359 140366075229952 make_examples.py:587] Created 86 examples. real 0m5.273s. user 0m5.898s. sys 0m3.792s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0911 02:28:44.682443 139937686464256 call_variants.py:335] Shape of input examples: [100, 221, 6]. 2020-09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-09-11 02:28:44.721599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2297525000 Hz. 2020-09-11 02:28:44.724518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5886480 executing computations on platform Host. Devices:. 2020-09-11 02:28:44.724555: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-09-11 02:28:44.727738: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W0911 02:28:44.755701 139937686464256 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp8bwus8dp. I0911 02:28:4",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:5922,interoperability,platform,platform,5922,"87] 6 candidates (6 examples) [0.33s elapsed]. I0911 02:28:41.745102 140366075229952 make_examples.py:587] Found 78 candidate variants. I0911 02:28:41.745359 140366075229952 make_examples.py:587] Created 86 examples. real 0m5.273s. user 0m5.898s. sys 0m3.792s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0911 02:28:44.682443 139937686464256 call_variants.py:335] Shape of input examples: [100, 221, 6]. 2020-09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-09-11 02:28:44.721599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2297525000 Hz. 2020-09-11 02:28:44.724518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5886480 executing computations on platform Host. Devices:. 2020-09-11 02:28:44.724555: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-09-11 02:28:44.727738: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W0911 02:28:44.755701 139937686464256 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp8bwus8dp. I0911 02:28:44.756375 139937686464256 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp8bwus8dp', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:6118,interoperability,platform,platform,6118,"Created 86 examples. real 0m5.273s. user 0m5.898s. sys 0m3.792s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0911 02:28:44.682443 139937686464256 call_variants.py:335] Shape of input examples: [100, 221, 6]. 2020-09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-09-11 02:28:44.721599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2297525000 Hz. 2020-09-11 02:28:44.724518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5886480 executing computations on platform Host. Devices:. 2020-09-11 02:28:44.724555: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-09-11 02:28:44.727738: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W0911 02:28:44.755701 139937686464256 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp8bwus8dp. I0911 02:28:44.756375 139937686464256 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp8bwus8dp', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_cre",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:11258,interoperability,Transform,Transforming,11258,":56.265293 139937686464256 call_variants.py:447] Processed 86 examples in 1 batches [13.334 sec per 100]. I0911 02:28:56.265480 139937686464256 call_variants.py:450] Done calling variants from a total of 86 examples. real 0m14.761s. user 0m20.588s. sys 0m5.092s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --infile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/output/output.g.vcf.gz"". 2020-09-11 02:28:59.543794: I deepvariant/postprocess_variants.cc:88] Read from: /output/intermediate_results_dir/call_variants_output.tfrecord.gz. 2020-09-11 02:28:59.544704: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 86. I0911 02:28:59.545160 140173302474496 postprocess_variants.py:1079] CVO sorting took 2.2590160369873047e-05 minutes. I0911 02:28:59.545950 140173302474496 postprocess_variants.py:1081] Transforming call_variants_output to variants. I0911 02:28:59.546221 140173302474496 postprocess_variants.py:1108] Merging and writing variants to VCF and gVCF. I0911 02:28:59.547042 140173302474496 genomics_writer.py:172] Writing /output/output.vcf.gz with NativeVcfWriter. I0911 02:28:59.547737 140173302474496 genomics_writer.py:172] Writing /output/output.g.vcf.gz with NativeVcfWriter. I0911 02:29:00.128752 140173302474496 postprocess_variants.py:1128] Finished writing VCF and gVCF in 0.00970850388209025 minutes. I0911 02:29:00.129857 140173302474496 genomics_reader.py:223] Reading /output/output.vcf.gz with NativeVcfReader. real 0m3.961s. user 0m4.633s. sys 0m3.567s. # ls -1 ${OUTPUT_DIR}. intermediate_results_dir. output.g.vcf.gz. output.g.vcf.gz.tbi. output.vcf.gz. output.vcf.gz.tbi. output.visual_report.html. ```. I think it works and thanks for your rapidly help about this~ @pichuan @tedyun . Best,. Jerry.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:126,modifiability,version,version,126,"Hi @pichuan and @tedyun ~. I run the command which @pichuan provided but it still print nothing on terminal. . And the docker version of this machine is `Docker version 19.03.3, build a872fc2`. So I switch to another machine which supported AVX instruction and run the test again by @tedyun 's advice. Then it run normally and output the files as same as the tutorial. Here is the environment & the command:. - Host CPU info. ```text. processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 63. model name : Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz. stepping : 2. microcode : 0x43. cpu MHz : 1199.975. cache size : 25600 KB. physical id : 0. siblings : 20. core id : 0. cpu cores : 10. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 15. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:161,modifiability,version,version,161,"Hi @pichuan and @tedyun ~. I run the command which @pichuan provided but it still print nothing on terminal. . And the docker version of this machine is `Docker version 19.03.3, build a872fc2`. So I switch to another machine which supported AVX instruction and run the test again by @tedyun 's advice. Then it run normally and output the files as same as the tutorial. Here is the environment & the command:. - Host CPU info. ```text. processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 63. model name : Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz. stepping : 2. microcode : 0x43. cpu MHz : 1199.975. cache size : 25600 KB. physical id : 0. siblings : 20. core id : 0. cpu cores : 10. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 15. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:1608,modifiability,version,version,1608,"cache size : 25600 KB. physical id : 0. siblings : 20. core id : 0. cpu cores : 10. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 15. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.bam.bai. test_nist.b37_chr20_100kbp_at_10mb.bed. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. ucsc.hg19.chr20.unittest.fasta. ucsc.hg19.chr20.unittest.fasta.fai. ucsc.hg19.chr20.unittest.fasta.gz. ucsc.hg19.chr20.unittest.fasta.gz.fai. ucsc.hg19.chr20.unittest.fasta.gz.gzi. # docker run \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. > --r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:1828,modifiability,version,version,1828,"pic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.bam.bai. test_nist.b37_chr20_100kbp_at_10mb.bed. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. ucsc.hg19.chr20.unittest.fasta. ucsc.hg19.chr20.unittest.fasta.fai. ucsc.hg19.chr20.unittest.fasta.gz. ucsc.hg19.chr20.unittest.fasta.gz.fai. ucsc.hg19.chr20.unittest.fasta.gz.gzi. # docker run \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=1 \. >. Status: D",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:2971,modifiability,interm,intermediate,2971,"878_S1.chr20.10_10p1mb.bam.bai. test_nist.b37_chr20_100kbp_at_10mb.bed. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. ucsc.hg19.chr20.unittest.fasta. ucsc.hg19.chr20.unittest.fasta.fai. ucsc.hg19.chr20.unittest.fasta.gz. ucsc.hg19.chr20.unittest.fasta.gz.fai. ucsc.hg19.chr20.unittest.fasta.gz.gzi. # docker run \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=1 \. >. Status: Downloaded newer image for google/deepvariant:1.0.0. I0911 02:28:36.697342 140021722134272 run_deepvariant.py:269] Creating a directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. I0911 02:28:39.900765 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.914417 140366075229952 make_examples.py:587] Preparing inputs. I0911 02:28:39.915202 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.916366 140366075229952 make_examples.py:587] Common contigs are ['chr20']. I0911 02:28",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:3035,modifiability,Interm,Intermediate,3035,"mb.bed. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. ucsc.hg19.chr20.unittest.fasta. ucsc.hg19.chr20.unittest.fasta.fai. ucsc.hg19.chr20.unittest.fasta.gz. ucsc.hg19.chr20.unittest.fasta.gz.fai. ucsc.hg19.chr20.unittest.fasta.gz.gzi. # docker run \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=1 \. >. Status: Downloaded newer image for google/deepvariant:1.0.0. I0911 02:28:36.697342 140021722134272 run_deepvariant.py:269] Creating a directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. I0911 02:28:39.900765 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.914417 140366075229952 make_examples.py:587] Preparing inputs. I0911 02:28:39.915202 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.916366 140366075229952 make_examples.py:587] Common contigs are ['chr20']. I0911 02:28:39.917786 140366075229952 make_examples.py:587] Writing example",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:4439,modifiability,deco,decode,4439,"intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. I0911 02:28:39.900765 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.914417 140366075229952 make_examples.py:587] Preparing inputs. I0911 02:28:39.915202 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.916366 140366075229952 make_examples.py:587] Common contigs are ['chr20']. I0911 02:28:39.917786 140366075229952 make_examples.py:587] Writing examples to /output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz. I0911 02:28:39.917969 140366075229952 make_examples.py:587] Writing gvcf records to /output/intermediate_results_dir/gvcf.tfrecord-00000-of-00001.gz. I0911 02:28:39.918812 140366075229952 make_examples.py:587] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2020-09-11 02:28:39.919135: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. I0911 02:28:39.922723 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.923217 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:40.247040 140366075229952 make_examples.py:587] 6 candidates (6 examples) [0.33s elapsed]. I0911 02:28:41.745102 140366075229952 make_examples.py:587] Found 78 candidate variants. I0911 02:28:41.745359 140366075229952 make_examples.py:587] Created 86 examples. real 0m5.273s. user 0m5.898s. sys 0m3.792s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:6046,modifiability,servic,service,6046,"e variants. I0911 02:28:41.745359 140366075229952 make_examples.py:587] Created 86 examples. real 0m5.273s. user 0m5.898s. sys 0m3.792s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0911 02:28:44.682443 139937686464256 call_variants.py:335] Shape of input examples: [100, 221, 6]. 2020-09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-09-11 02:28:44.721599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2297525000 Hz. 2020-09-11 02:28:44.724518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5886480 executing computations on platform Host. Devices:. 2020-09-11 02:28:44.724555: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-09-11 02:28:44.727738: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W0911 02:28:44.755701 139937686464256 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp8bwus8dp. I0911 02:28:44.756375 139937686464256 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp8bwus8dp', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:6054,modifiability,servic,service,6054,"ts. I0911 02:28:41.745359 140366075229952 make_examples.py:587] Created 86 examples. real 0m5.273s. user 0m5.898s. sys 0m3.792s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0911 02:28:44.682443 139937686464256 call_variants.py:335] Shape of input examples: [100, 221, 6]. 2020-09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-09-11 02:28:44.721599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2297525000 Hz. 2020-09-11 02:28:44.724518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5886480 executing computations on platform Host. Devices:. 2020-09-11 02:28:44.724555: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-09-11 02:28:44.727738: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W0911 02:28:44.755701 139937686464256 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp8bwus8dp. I0911 02:28:44.756375 139937686464256 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp8bwus8dp', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:6074,modifiability,servic,service,6074,"45359 140366075229952 make_examples.py:587] Created 86 examples. real 0m5.273s. user 0m5.898s. sys 0m3.792s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0911 02:28:44.682443 139937686464256 call_variants.py:335] Shape of input examples: [100, 221, 6]. 2020-09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-09-11 02:28:44.721599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2297525000 Hz. 2020-09-11 02:28:44.724518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5886480 executing computations on platform Host. Devices:. 2020-09-11 02:28:44.724555: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-09-11 02:28:44.727738: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W0911 02:28:44.755701 139937686464256 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp8bwus8dp. I0911 02:28:44.756375 139937686464256 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp8bwus8dp', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:6197,modifiability,servic,service,6197,"the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0911 02:28:44.682443 139937686464256 call_variants.py:335] Shape of input examples: [100, 221, 6]. 2020-09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-09-11 02:28:44.721599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2297525000 Hz. 2020-09-11 02:28:44.724518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5886480 executing computations on platform Host. Devices:. 2020-09-11 02:28:44.724555: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-09-11 02:28:44.727738: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W0911 02:28:44.755701 139937686464256 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp8bwus8dp. I0911 02:28:44.756375 139937686464256 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp8bwus8dp', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.pytho",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:6205,modifiability,servic,service,6205,"and:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0911 02:28:44.682443 139937686464256 call_variants.py:335] Shape of input examples: [100, 221, 6]. 2020-09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-09-11 02:28:44.721599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2297525000 Hz. 2020-09-11 02:28:44.724518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5886480 executing computations on platform Host. Devices:. 2020-09-11 02:28:44.724555: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-09-11 02:28:44.727738: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W0911 02:28:44.755701 139937686464256 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp8bwus8dp. I0911 02:28:44.756375 139937686464256 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp8bwus8dp', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.traini",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:6262,modifiability,Version,Version,6262,"ile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0911 02:28:44.682443 139937686464256 call_variants.py:335] Shape of input examples: [100, 221, 6]. 2020-09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-09-11 02:28:44.721599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2297525000 Hz. 2020-09-11 02:28:44.724518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5886480 executing computations on platform Host. Devices:. 2020-09-11 02:28:44.724555: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-09-11 02:28:44.727738: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W0911 02:28:44.755701 139937686464256 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp8bwus8dp. I0911 02:28:44.756375 139937686464256 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp8bwus8dp', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f4549cfe518>, '_ta",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:7677,modifiability,pac,packages,7677,"p/tmp8bwus8dp', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f4549cfe518>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I0911 02:28:44.756798 139937686464256 call_variants.py:426] Writing calls to /output/intermediate_results_dir/call_variants_output.tfrecord.gz. W0911 02:28:44.767428 139937686464256 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version. Instructions for updating:. If using Keras pass *_constraint arguments to layers. W0911 02:28:44.788271 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. W0911 02:28:44.973487 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:7895,modifiability,version,version,7895,": 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f4549cfe518>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I0911 02:28:44.756798 139937686464256 call_variants.py:426] Writing calls to /output/intermediate_results_dir/call_variants_output.tfrecord.gz. W0911 02:28:44.767428 139937686464256 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version. Instructions for updating:. If using Keras pass *_constraint arguments to layers. W0911 02:28:44.788271 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. W0911 02:28:44.973487 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:7978,modifiability,layer,layers,7978,"ne, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f4549cfe518>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I0911 02:28:44.756798 139937686464256 call_variants.py:426] Writing calls to /output/intermediate_results_dir/call_variants_output.tfrecord.gz. W0911 02:28:44.767428 139937686464256 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version. Instructions for updating:. If using Keras pass *_constraint arguments to layers. W0911 02:28:44.788271 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. W0911 02:28:44.973487 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimization",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:8273,modifiability,version,version,8273,"worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I0911 02:28:44.756798 139937686464256 call_variants.py:426] Writing calls to /output/intermediate_results_dir/call_variants_output.tfrecord.gz. W0911 02:28:44.767428 139937686464256 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version. Instructions for updating:. If using Keras pass *_constraint arguments to layers. W0911 02:28:44.788271 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. W0911 02:28:44.973487 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I0911 02:28:45.531516 139937686464256 estimator.py:1147] Calling model_fn. W0911 02:28:45.537137 139937686464256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:8796,modifiability,version,version,8796,"ython.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version. Instructions for updating:. If using Keras pass *_constraint arguments to layers. W0911 02:28:44.788271 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. W0911 02:28:44.973487 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I0911 02:28:45.531516 139937686464256 estimator.py:1147] Calling model_fn. W0911 02:28:45.537137 139937686464256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0911 02:28:50.978360 139937686464256 estimator.py:1149] Done calling model_fn. I0911 02:28:52.395916 139937686464256 monitored_session.py:240] Graph was finalized. I0911 02:28:52.397383 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:53.423366 139937686464256 session_manager.py:500] Running local_init_op. I0911 02:28:53",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:9201,modifiability,pac,packages,9201,"l.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. W0911 02:28:44.973487 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I0911 02:28:45.531516 139937686464256 estimator.py:1147] Calling model_fn. W0911 02:28:45.537137 139937686464256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0911 02:28:50.978360 139937686464256 estimator.py:1149] Done calling model_fn. I0911 02:28:52.395916 139937686464256 monitored_session.py:240] Graph was finalized. I0911 02:28:52.397383 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:53.423366 139937686464256 session_manager.py:500] Running local_init_op. I0911 02:28:53.495537 139937686464256 session_manager.py:502] Done running local_init_op. I0911 02:28:54.010662 139937686464256 modeling.py:415] Reloading EMA... I0911 02:28:54.011811 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:56.213706 139937686464256 call_variants.py:444] Processed 1 examples in 1 batches [1145.647 sec per 100]. I0911 02:28:56.265293 1399376864",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:9218,modifiability,layer,layers,9218,"_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. W0911 02:28:44.973487 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I0911 02:28:45.531516 139937686464256 estimator.py:1147] Calling model_fn. W0911 02:28:45.537137 139937686464256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0911 02:28:50.978360 139937686464256 estimator.py:1149] Done calling model_fn. I0911 02:28:52.395916 139937686464256 monitored_session.py:240] Graph was finalized. I0911 02:28:52.397383 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:53.423366 139937686464256 session_manager.py:500] Running local_init_op. I0911 02:28:53.495537 139937686464256 session_manager.py:502] Done running local_init_op. I0911 02:28:54.010662 139937686464256 modeling.py:415] Reloading EMA... I0911 02:28:54.011811 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:56.213706 139937686464256 call_variants.py:444] Processed 1 examples in 1 batches [1145.647 sec per 100]. I0911 02:28:56.265293 139937686464256 call_varia",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:9225,modifiability,layer,layers,9225,"s deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. W0911 02:28:44.973487 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I0911 02:28:45.531516 139937686464256 estimator.py:1147] Calling model_fn. W0911 02:28:45.537137 139937686464256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0911 02:28:50.978360 139937686464256 estimator.py:1149] Done calling model_fn. I0911 02:28:52.395916 139937686464256 monitored_session.py:240] Graph was finalized. I0911 02:28:52.397383 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:53.423366 139937686464256 session_manager.py:500] Running local_init_op. I0911 02:28:53.495537 139937686464256 session_manager.py:502] Done running local_init_op. I0911 02:28:54.010662 139937686464256 modeling.py:415] Reloading EMA... I0911 02:28:54.011811 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:56.213706 139937686464256 call_variants.py:444] Processed 1 examples in 1 batches [1145.647 sec per 100]. I0911 02:28:56.265293 139937686464256 call_variants.py:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:9241,modifiability,Layer,Layer,9241," will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. W0911 02:28:44.973487 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I0911 02:28:45.531516 139937686464256 estimator.py:1147] Calling model_fn. W0911 02:28:45.537137 139937686464256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0911 02:28:50.978360 139937686464256 estimator.py:1149] Done calling model_fn. I0911 02:28:52.395916 139937686464256 monitored_session.py:240] Graph was finalized. I0911 02:28:52.397383 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:53.423366 139937686464256 session_manager.py:500] Running local_init_op. I0911 02:28:53.495537 139937686464256 session_manager.py:502] Done running local_init_op. I0911 02:28:54.010662 139937686464256 modeling.py:415] Reloading EMA... I0911 02:28:54.011811 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:56.213706 139937686464256 call_variants.py:444] Processed 1 examples in 1 batches [1145.647 sec per 100]. I0911 02:28:56.265293 139937686464256 call_variants.py:447] Processed 8",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:9348,modifiability,version,version,9348,"cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. W0911 02:28:44.973487 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I0911 02:28:45.531516 139937686464256 estimator.py:1147] Calling model_fn. W0911 02:28:45.537137 139937686464256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0911 02:28:50.978360 139937686464256 estimator.py:1149] Done calling model_fn. I0911 02:28:52.395916 139937686464256 monitored_session.py:240] Graph was finalized. I0911 02:28:52.397383 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:53.423366 139937686464256 session_manager.py:500] Running local_init_op. I0911 02:28:53.495537 139937686464256 session_manager.py:502] Done running local_init_op. I0911 02:28:54.010662 139937686464256 modeling.py:415] Reloading EMA... I0911 02:28:54.011811 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:56.213706 139937686464256 call_variants.py:444] Processed 1 examples in 1 batches [1145.647 sec per 100]. I0911 02:28:56.265293 139937686464256 call_variants.py:447] Processed 86 examples in 1 batches [13.334 sec per 100]. I0911 02:28:56.265480 139937686464256 call_variants.py:450] Do",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:9397,modifiability,layer,layer,9397,"f.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. W0911 02:28:44.973487 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I0911 02:28:45.531516 139937686464256 estimator.py:1147] Calling model_fn. W0911 02:28:45.537137 139937686464256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0911 02:28:50.978360 139937686464256 estimator.py:1149] Done calling model_fn. I0911 02:28:52.395916 139937686464256 monitored_session.py:240] Graph was finalized. I0911 02:28:52.397383 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:53.423366 139937686464256 session_manager.py:500] Running local_init_op. I0911 02:28:53.495537 139937686464256 session_manager.py:502] Done running local_init_op. I0911 02:28:54.010662 139937686464256 modeling.py:415] Reloading EMA... I0911 02:28:54.011811 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:56.213706 139937686464256 call_variants.py:444] Processed 1 examples in 1 batches [1145.647 sec per 100]. I0911 02:28:56.265293 139937686464256 call_variants.py:447] Processed 86 examples in 1 batches [13.334 sec per 100]. I0911 02:28:56.265480 139937686464256 call_variants.py:450] Done calling variants from a total of 86 examples.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:9657,modifiability,paramet,parameters,9657,"providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I0911 02:28:45.531516 139937686464256 estimator.py:1147] Calling model_fn. W0911 02:28:45.537137 139937686464256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0911 02:28:50.978360 139937686464256 estimator.py:1149] Done calling model_fn. I0911 02:28:52.395916 139937686464256 monitored_session.py:240] Graph was finalized. I0911 02:28:52.397383 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:53.423366 139937686464256 session_manager.py:500] Running local_init_op. I0911 02:28:53.495537 139937686464256 session_manager.py:502] Done running local_init_op. I0911 02:28:54.010662 139937686464256 modeling.py:415] Reloading EMA... I0911 02:28:54.011811 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:56.213706 139937686464256 call_variants.py:444] Processed 1 examples in 1 batches [1145.647 sec per 100]. I0911 02:28:56.265293 139937686464256 call_variants.py:447] Processed 86 examples in 1 batches [13.334 sec per 100]. I0911 02:28:56.265480 139937686464256 call_variants.py:450] Done calling variants from a total of 86 examples. real 0m14.761s. user 0m20.588s. sys 0m5.092s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --infile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:10011,modifiability,paramet,parameters,10011," implementation. I0911 02:28:45.531516 139937686464256 estimator.py:1147] Calling model_fn. W0911 02:28:45.537137 139937686464256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0911 02:28:50.978360 139937686464256 estimator.py:1149] Done calling model_fn. I0911 02:28:52.395916 139937686464256 monitored_session.py:240] Graph was finalized. I0911 02:28:52.397383 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:53.423366 139937686464256 session_manager.py:500] Running local_init_op. I0911 02:28:53.495537 139937686464256 session_manager.py:502] Done running local_init_op. I0911 02:28:54.010662 139937686464256 modeling.py:415] Reloading EMA... I0911 02:28:54.011811 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:56.213706 139937686464256 call_variants.py:444] Processed 1 examples in 1 batches [1145.647 sec per 100]. I0911 02:28:56.265293 139937686464256 call_variants.py:447] Processed 86 examples in 1 batches [13.334 sec per 100]. I0911 02:28:56.265480 139937686464256 call_variants.py:450] Done calling variants from a total of 86 examples. real 0m14.761s. user 0m20.588s. sys 0m5.092s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --infile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/output/output.g.vcf.gz"". 2020-09-11 02:28:59.543794: I deepvariant/postprocess_variants.cc:88] Read from: /output/intermediate_results_dir/call_variants_output.tfrecord.gz. 2020-09-11 02:28:59.544704: I deepvariant/postprocess_va",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:416,performance,CPU,CPU,416,"Hi @pichuan and @tedyun ~. I run the command which @pichuan provided but it still print nothing on terminal. . And the docker version of this machine is `Docker version 19.03.3, build a872fc2`. So I switch to another machine which supported AVX instruction and run the test again by @tedyun 's advice. Then it run normally and output the files as same as the tutorial. Here is the environment & the command:. - Host CPU info. ```text. processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 63. model name : Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz. stepping : 2. microcode : 0x43. cpu MHz : 1199.975. cache size : 25600 KB. physical id : 0. siblings : 20. core id : 0. cpu cores : 10. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 15. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:476,performance,cpu,cpu,476,"Hi @pichuan and @tedyun ~. I run the command which @pichuan provided but it still print nothing on terminal. . And the docker version of this machine is `Docker version 19.03.3, build a872fc2`. So I switch to another machine which supported AVX instruction and run the test again by @tedyun 's advice. Then it run normally and output the files as same as the tutorial. Here is the environment & the command:. - Host CPU info. ```text. processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 63. model name : Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz. stepping : 2. microcode : 0x43. cpu MHz : 1199.975. cache size : 25600 KB. physical id : 0. siblings : 20. core id : 0. cpu cores : 10. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 15. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:534,performance,CPU,CPU,534,"Hi @pichuan and @tedyun ~. I run the command which @pichuan provided but it still print nothing on terminal. . And the docker version of this machine is `Docker version 19.03.3, build a872fc2`. So I switch to another machine which supported AVX instruction and run the test again by @tedyun 's advice. Then it run normally and output the files as same as the tutorial. Here is the environment & the command:. - Host CPU info. ```text. processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 63. model name : Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz. stepping : 2. microcode : 0x43. cpu MHz : 1199.975. cache size : 25600 KB. physical id : 0. siblings : 20. core id : 0. cpu cores : 10. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 15. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:592,performance,cpu,cpu,592,"Hi @pichuan and @tedyun ~. I run the command which @pichuan provided but it still print nothing on terminal. . And the docker version of this machine is `Docker version 19.03.3, build a872fc2`. So I switch to another machine which supported AVX instruction and run the test again by @tedyun 's advice. Then it run normally and output the files as same as the tutorial. Here is the environment & the command:. - Host CPU info. ```text. processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 63. model name : Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz. stepping : 2. microcode : 0x43. cpu MHz : 1199.975. cache size : 25600 KB. physical id : 0. siblings : 20. core id : 0. cpu cores : 10. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 15. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:612,performance,cach,cache,612,"Hi @pichuan and @tedyun ~. I run the command which @pichuan provided but it still print nothing on terminal. . And the docker version of this machine is `Docker version 19.03.3, build a872fc2`. So I switch to another machine which supported AVX instruction and run the test again by @tedyun 's advice. Then it run normally and output the files as same as the tutorial. Here is the environment & the command:. - Host CPU info. ```text. processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 63. model name : Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz. stepping : 2. microcode : 0x43. cpu MHz : 1199.975. cache size : 25600 KB. physical id : 0. siblings : 20. core id : 0. cpu cores : 10. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 15. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:680,performance,cpu,cpu,680,"Hi @pichuan and @tedyun ~. I run the command which @pichuan provided but it still print nothing on terminal. . And the docker version of this machine is `Docker version 19.03.3, build a872fc2`. So I switch to another machine which supported AVX instruction and run the test again by @tedyun 's advice. Then it run normally and output the files as same as the tutorial. Here is the environment & the command:. - Host CPU info. ```text. processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 63. model name : Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz. stepping : 2. microcode : 0x43. cpu MHz : 1199.975. cache size : 25600 KB. physical id : 0. siblings : 20. core id : 0. cpu cores : 10. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 15. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:760,performance,cpu,cpuid,760,"Hi @pichuan and @tedyun ~. I run the command which @pichuan provided but it still print nothing on terminal. . And the docker version of this machine is `Docker version 19.03.3, build a872fc2`. So I switch to another machine which supported AVX instruction and run the test again by @tedyun 's advice. Then it run normally and output the files as same as the tutorial. Here is the environment & the command:. - Host CPU info. ```text. processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 63. model name : Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz. stepping : 2. microcode : 0x43. cpu MHz : 1199.975. cache size : 25600 KB. physical id : 0. siblings : 20. core id : 0. cpu cores : 10. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 15. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:3158,performance,time,time,3158,"est.fasta. ucsc.hg19.chr20.unittest.fasta.fai. ucsc.hg19.chr20.unittest.fasta.gz. ucsc.hg19.chr20.unittest.fasta.gz.fai. ucsc.hg19.chr20.unittest.fasta.gz.gzi. # docker run \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=1 \. >. Status: Downloaded newer image for google/deepvariant:1.0.0. I0911 02:28:36.697342 140021722134272 run_deepvariant.py:269] Creating a directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. I0911 02:28:39.900765 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.914417 140366075229952 make_examples.py:587] Preparing inputs. I0911 02:28:39.915202 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.916366 140366075229952 make_examples.py:587] Common contigs are ['chr20']. I0911 02:28:39.917786 140366075229952 make_examples.py:587] Writing examples to /output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz. I0911 02:28:39.917969 140366075229952 m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:3173,performance,parallel,parallel,3173,"g19.chr20.unittest.fasta.fai. ucsc.hg19.chr20.unittest.fasta.gz. ucsc.hg19.chr20.unittest.fasta.gz.fai. ucsc.hg19.chr20.unittest.fasta.gz.gzi. # docker run \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=1 \. >. Status: Downloaded newer image for google/deepvariant:1.0.0. I0911 02:28:36.697342 140021722134272 run_deepvariant.py:269] Creating a directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. I0911 02:28:39.900765 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.914417 140366075229952 make_examples.py:587] Preparing inputs. I0911 02:28:39.915202 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.916366 140366075229952 make_examples.py:587] Common contigs are ['chr20']. I0911 02:28:39.917786 140366075229952 make_examples.py:587] Writing examples to /output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz. I0911 02:28:39.917969 140366075229952 make_examples.py:5",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:5220,performance,time,time,5220,"ate_results_dir/gvcf.tfrecord-00000-of-00001.gz. I0911 02:28:39.918812 140366075229952 make_examples.py:587] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2020-09-11 02:28:39.919135: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. I0911 02:28:39.922723 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.923217 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:40.247040 140366075229952 make_examples.py:587] 6 candidates (6 examples) [0.33s elapsed]. I0911 02:28:41.745102 140366075229952 make_examples.py:587] Found 78 candidate variants. I0911 02:28:41.745359 140366075229952 make_examples.py:587] Created 86 examples. real 0m5.273s. user 0m5.898s. sys 0m3.792s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0911 02:28:44.682443 139937686464256 call_variants.py:335] Shape of input examples: [100, 221, 6]. 2020-09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-09-11 02:28:44.721599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2297525000 Hz. 2020-09-11 02:28:44.724518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5886480 executing computations on platform Host. Devices:. 2020-09-11 02:28:44.724555: I tensorflow/compiler/xla/service/service.cc:175] S",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:5662,performance,optimiz,optimized,5662,"223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.923217 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:40.247040 140366075229952 make_examples.py:587] 6 candidates (6 examples) [0.33s elapsed]. I0911 02:28:41.745102 140366075229952 make_examples.py:587] Found 78 candidate variants. I0911 02:28:41.745359 140366075229952 make_examples.py:587] Created 86 examples. real 0m5.273s. user 0m5.898s. sys 0m3.792s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0911 02:28:44.682443 139937686464256 call_variants.py:335] Shape of input examples: [100, 221, 6]. 2020-09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-09-11 02:28:44.721599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2297525000 Hz. 2020-09-11 02:28:44.724518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5886480 executing computations on platform Host. Devices:. 2020-09-11 02:28:44.724555: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-09-11 02:28:44.727738: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W0911 02:28:44.755701 139937686464256 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp8bwus8dp. I0911 02:28:44.756375 139937686464256 estimator.py:212] Using config: {'_mo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:5715,performance,CPU,CPU,5715," with NativeSamReader. I0911 02:28:39.923217 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:40.247040 140366075229952 make_examples.py:587] 6 candidates (6 examples) [0.33s elapsed]. I0911 02:28:41.745102 140366075229952 make_examples.py:587] Found 78 candidate variants. I0911 02:28:41.745359 140366075229952 make_examples.py:587] Created 86 examples. real 0m5.273s. user 0m5.898s. sys 0m3.792s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0911 02:28:44.682443 139937686464256 call_variants.py:335] Shape of input examples: [100, 221, 6]. 2020-09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-09-11 02:28:44.721599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2297525000 Hz. 2020-09-11 02:28:44.724518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5886480 executing computations on platform Host. Devices:. 2020-09-11 02:28:44.724555: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-09-11 02:28:44.727738: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W0911 02:28:44.755701 139937686464256 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp8bwus8dp. I0911 02:28:44.756375 139937686464256 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp8bwus8dp', '_tf_random_seed': N",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:5735,performance,perform,performance,5735,"0911 02:28:39.923217 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:40.247040 140366075229952 make_examples.py:587] 6 candidates (6 examples) [0.33s elapsed]. I0911 02:28:41.745102 140366075229952 make_examples.py:587] Found 78 candidate variants. I0911 02:28:41.745359 140366075229952 make_examples.py:587] Created 86 examples. real 0m5.273s. user 0m5.898s. sys 0m3.792s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0911 02:28:44.682443 139937686464256 call_variants.py:335] Shape of input examples: [100, 221, 6]. 2020-09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-09-11 02:28:44.721599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2297525000 Hz. 2020-09-11 02:28:44.724518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5886480 executing computations on platform Host. Devices:. 2020-09-11 02:28:44.724555: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-09-11 02:28:44.727738: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W0911 02:28:44.755701 139937686464256 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp8bwus8dp. I0911 02:28:44.756375 139937686464256 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp8bwus8dp', '_tf_random_seed': None, '_save_summary_step",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:5962,performance,CPU,CPU,5962,"lapsed]. I0911 02:28:41.745102 140366075229952 make_examples.py:587] Found 78 candidate variants. I0911 02:28:41.745359 140366075229952 make_examples.py:587] Created 86 examples. real 0m5.273s. user 0m5.898s. sys 0m3.792s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0911 02:28:44.682443 139937686464256 call_variants.py:335] Shape of input examples: [100, 221, 6]. 2020-09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-09-11 02:28:44.721599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2297525000 Hz. 2020-09-11 02:28:44.724518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5886480 executing computations on platform Host. Devices:. 2020-09-11 02:28:44.724555: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-09-11 02:28:44.727738: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W0911 02:28:44.755701 139937686464256 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp8bwus8dp. I0911 02:28:44.756375 139937686464256 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp8bwus8dp', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:6412,performance,Tune,Tune,6412,"checkpoint ""/opt/models/wgs/model.ckpt"". I0911 02:28:44.682443 139937686464256 call_variants.py:335] Shape of input examples: [100, 221, 6]. 2020-09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-09-11 02:28:44.721599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2297525000 Hz. 2020-09-11 02:28:44.724518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5886480 executing computations on platform Host. Devices:. 2020-09-11 02:28:44.724555: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-09-11 02:28:44.727738: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W0911 02:28:44.755701 139937686464256 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp8bwus8dp. I0911 02:28:44.756375 139937686464256 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp8bwus8dp', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f4549cfe518>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:6461,performance,perform,performance,6461,"44.682443 139937686464256 call_variants.py:335] Shape of input examples: [100, 221, 6]. 2020-09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-09-11 02:28:44.721599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2297525000 Hz. 2020-09-11 02:28:44.724518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5886480 executing computations on platform Host. Devices:. 2020-09-11 02:28:44.724555: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-09-11 02:28:44.727738: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W0911 02:28:44.755701 139937686464256 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp8bwus8dp. I0911 02:28:44.756375 139937686464256 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp8bwus8dp', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f4549cfe518>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I0911 02:28:44.756798 13993",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:8740,performance,batch,batching,8740,"calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version. Instructions for updating:. If using Keras pass *_constraint arguments to layers. W0911 02:28:44.788271 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. W0911 02:28:44.973487 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I0911 02:28:45.531516 139937686464256 estimator.py:1147] Calling model_fn. W0911 02:28:45.537137 139937686464256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0911 02:28:50.978360 139937686464256 estimator.py:1149] Done calling model_fn. I0911 02:28:52.395916 139937686464256 monitored_session.py:240] Graph was finalized. I0911 02:28:52.397383 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:53.423366 139937686464256 sessi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:8918,performance,batch,batch,8918,"updating:. If using Keras pass *_constraint arguments to layers. W0911 02:28:44.788271 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. W0911 02:28:44.973487 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I0911 02:28:45.531516 139937686464256 estimator.py:1147] Calling model_fn. W0911 02:28:45.537137 139937686464256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0911 02:28:50.978360 139937686464256 estimator.py:1149] Done calling model_fn. I0911 02:28:52.395916 139937686464256 monitored_session.py:240] Graph was finalized. I0911 02:28:52.397383 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:53.423366 139937686464256 session_manager.py:500] Running local_init_op. I0911 02:28:53.495537 139937686464256 session_manager.py:502] Done running local_init_op. I0911 02:28:54.010662 139937686464256 modelin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:8969,performance,optimiz,optimizations,8969,"o layers. W0911 02:28:44.788271 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. W0911 02:28:44.973487 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I0911 02:28:45.531516 139937686464256 estimator.py:1147] Calling model_fn. W0911 02:28:45.537137 139937686464256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0911 02:28:50.978360 139937686464256 estimator.py:1149] Done calling model_fn. I0911 02:28:52.395916 139937686464256 monitored_session.py:240] Graph was finalized. I0911 02:28:52.397383 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:53.423366 139937686464256 session_manager.py:500] Running local_init_op. I0911 02:28:53.495537 139937686464256 session_manager.py:502] Done running local_init_op. I0911 02:28:54.010662 139937686464256 modeling.py:415] Reloading EMA... I0911 02:28:54.011811 139937",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:10141,performance,batch,batches,10141," deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0911 02:28:50.978360 139937686464256 estimator.py:1149] Done calling model_fn. I0911 02:28:52.395916 139937686464256 monitored_session.py:240] Graph was finalized. I0911 02:28:52.397383 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:53.423366 139937686464256 session_manager.py:500] Running local_init_op. I0911 02:28:53.495537 139937686464256 session_manager.py:502] Done running local_init_op. I0911 02:28:54.010662 139937686464256 modeling.py:415] Reloading EMA... I0911 02:28:54.011811 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:56.213706 139937686464256 call_variants.py:444] Processed 1 examples in 1 batches [1145.647 sec per 100]. I0911 02:28:56.265293 139937686464256 call_variants.py:447] Processed 86 examples in 1 batches [13.334 sec per 100]. I0911 02:28:56.265480 139937686464256 call_variants.py:450] Done calling variants from a total of 86 examples. real 0m14.761s. user 0m20.588s. sys 0m5.092s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --infile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/output/output.g.vcf.gz"". 2020-09-11 02:28:59.543794: I deepvariant/postprocess_variants.cc:88] Read from: /output/intermediate_results_dir/call_variants_output.tfrecord.gz. 2020-09-11 02:28:59.544704: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 86. I0911 02:28:59.545160 140173302474496 postprocess_variants.py:1079] CVO ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:10260,performance,batch,batches,10260,"rflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0911 02:28:50.978360 139937686464256 estimator.py:1149] Done calling model_fn. I0911 02:28:52.395916 139937686464256 monitored_session.py:240] Graph was finalized. I0911 02:28:52.397383 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:53.423366 139937686464256 session_manager.py:500] Running local_init_op. I0911 02:28:53.495537 139937686464256 session_manager.py:502] Done running local_init_op. I0911 02:28:54.010662 139937686464256 modeling.py:415] Reloading EMA... I0911 02:28:54.011811 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:56.213706 139937686464256 call_variants.py:444] Processed 1 examples in 1 batches [1145.647 sec per 100]. I0911 02:28:56.265293 139937686464256 call_variants.py:447] Processed 86 examples in 1 batches [13.334 sec per 100]. I0911 02:28:56.265480 139937686464256 call_variants.py:450] Done calling variants from a total of 86 examples. real 0m14.761s. user 0m20.588s. sys 0m5.092s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --infile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/output/output.g.vcf.gz"". 2020-09-11 02:28:59.543794: I deepvariant/postprocess_variants.cc:88] Read from: /output/intermediate_results_dir/call_variants_output.tfrecord.gz. 2020-09-11 02:28:59.544704: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 86. I0911 02:28:59.545160 140173302474496 postprocess_variants.py:1079] CVO sorting took 2.2590160369873047e-05 minutes. I0911 02:28:59.545950 140173302474496 postprocess_variants.py:1081] Transf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:10480,performance,time,time,10480,"49] Done calling model_fn. I0911 02:28:52.395916 139937686464256 monitored_session.py:240] Graph was finalized. I0911 02:28:52.397383 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:53.423366 139937686464256 session_manager.py:500] Running local_init_op. I0911 02:28:53.495537 139937686464256 session_manager.py:502] Done running local_init_op. I0911 02:28:54.010662 139937686464256 modeling.py:415] Reloading EMA... I0911 02:28:54.011811 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:56.213706 139937686464256 call_variants.py:444] Processed 1 examples in 1 batches [1145.647 sec per 100]. I0911 02:28:56.265293 139937686464256 call_variants.py:447] Processed 86 examples in 1 batches [13.334 sec per 100]. I0911 02:28:56.265480 139937686464256 call_variants.py:450] Done calling variants from a total of 86 examples. real 0m14.761s. user 0m20.588s. sys 0m5.092s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --infile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/output/output.g.vcf.gz"". 2020-09-11 02:28:59.543794: I deepvariant/postprocess_variants.cc:88] Read from: /output/intermediate_results_dir/call_variants_output.tfrecord.gz. 2020-09-11 02:28:59.544704: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 86. I0911 02:28:59.545160 140173302474496 postprocess_variants.py:1079] CVO sorting took 2.2590160369873047e-05 minutes. I0911 02:28:59.545950 140173302474496 postprocess_variants.py:1081] Transforming call_variants_output to variants. I0911 02:28:59.546221 140173302474496 postprocess_variants.py:1108] Merging and writing variants to VCF and gVCF. I0911 02:28:59.547042 140173302474496 genomics_writer.py:172] W",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:935,reliability,rdt,rdtscp,935,"Hi @pichuan and @tedyun ~. I run the command which @pichuan provided but it still print nothing on terminal. . And the docker version of this machine is `Docker version 19.03.3, build a872fc2`. So I switch to another machine which supported AVX instruction and run the test again by @tedyun 's advice. Then it run normally and output the files as same as the tutorial. Here is the environment & the command:. - Host CPU info. ```text. processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 63. model name : Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz. stepping : 2. microcode : 0x43. cpu MHz : 1199.975. cache size : 25600 KB. physical id : 0. siblings : 20. core id : 0. cpu cores : 10. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 15. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:1057,reliability,monitor,monitor,1057,"rovided but it still print nothing on terminal. . And the docker version of this machine is `Docker version 19.03.3, build a872fc2`. So I switch to another machine which supported AVX instruction and run the test again by @tedyun 's advice. Then it run normally and output the files as same as the tutorial. Here is the environment & the command:. - Host CPU info. ```text. processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 63. model name : Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz. stepping : 2. microcode : 0x43. cpu MHz : 1199.975. cache size : 25600 KB. physical id : 0. siblings : 20. core id : 0. cpu cores : 10. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 15. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.bam.bai. test_nist.b37_chr20_100kbp_at_10mb.bed. test_nist.b3",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:5414,reliability,checkpoint,checkpoint,5414,", note that we will decode CRAM using the reference you passed in with --ref. 2020-09-11 02:28:39.919135: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. I0911 02:28:39.922723 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.923217 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:40.247040 140366075229952 make_examples.py:587] 6 candidates (6 examples) [0.33s elapsed]. I0911 02:28:41.745102 140366075229952 make_examples.py:587] Found 78 candidate variants. I0911 02:28:41.745359 140366075229952 make_examples.py:587] Created 86 examples. real 0m5.273s. user 0m5.898s. sys 0m3.792s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0911 02:28:44.682443 139937686464256 call_variants.py:335] Shape of input examples: [100, 221, 6]. 2020-09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-09-11 02:28:44.721599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2297525000 Hz. 2020-09-11 02:28:44.724518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5886480 executing computations on platform Host. Devices:. 2020-09-11 02:28:44.724555: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-09-11 02:28:44.727738: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune us",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:8443,reliability,slo,sloppy,8443,"02:28:44.756798 139937686464256 call_variants.py:426] Writing calls to /output/intermediate_results_dir/call_variants_output.tfrecord.gz. W0911 02:28:44.767428 139937686464256 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version. Instructions for updating:. If using Keras pass *_constraint arguments to layers. W0911 02:28:44.788271 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. W0911 02:28:44.973487 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I0911 02:28:45.531516 139937686464256 estimator.py:1147] Calling model_fn. W0911 02:28:45.537137 139937686464256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0911 02:28:50.97",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:9647,reliability,Restor,Restoring,9647,"iant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I0911 02:28:45.531516 139937686464256 estimator.py:1147] Calling model_fn. W0911 02:28:45.537137 139937686464256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0911 02:28:50.978360 139937686464256 estimator.py:1149] Done calling model_fn. I0911 02:28:52.395916 139937686464256 monitored_session.py:240] Graph was finalized. I0911 02:28:52.397383 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:53.423366 139937686464256 session_manager.py:500] Running local_init_op. I0911 02:28:53.495537 139937686464256 session_manager.py:502] Done running local_init_op. I0911 02:28:54.010662 139937686464256 modeling.py:415] Reloading EMA... I0911 02:28:54.011811 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:56.213706 139937686464256 call_variants.py:444] Processed 1 examples in 1 batches [1145.647 sec per 100]. I0911 02:28:56.265293 139937686464256 call_variants.py:447] Processed 86 examples in 1 batches [13.334 sec per 100]. I0911 02:28:56.265480 139937686464256 call_variants.py:450] Done calling variants from a total of 86 examples. real 0m14.761s. user 0m20.588s. sys 0m5.092s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --infile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:10001,reliability,Restor,Restoring,10001," the fused implementation. I0911 02:28:45.531516 139937686464256 estimator.py:1147] Calling model_fn. W0911 02:28:45.537137 139937686464256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0911 02:28:50.978360 139937686464256 estimator.py:1149] Done calling model_fn. I0911 02:28:52.395916 139937686464256 monitored_session.py:240] Graph was finalized. I0911 02:28:52.397383 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:53.423366 139937686464256 session_manager.py:500] Running local_init_op. I0911 02:28:53.495537 139937686464256 session_manager.py:502] Done running local_init_op. I0911 02:28:54.010662 139937686464256 modeling.py:415] Reloading EMA... I0911 02:28:54.011811 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:56.213706 139937686464256 call_variants.py:444] Processed 1 examples in 1 batches [1145.647 sec per 100]. I0911 02:28:56.265293 139937686464256 call_variants.py:447] Processed 86 examples in 1 batches [13.334 sec per 100]. I0911 02:28:56.265480 139937686464256 call_variants.py:450] Done calling variants from a total of 86 examples. real 0m14.761s. user 0m20.588s. sys 0m5.092s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --infile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/output/output.g.vcf.gz"". 2020-09-11 02:28:59.543794: I deepvariant/postprocess_variants.cc:88] Read from: /output/intermediate_results_dir/call_variants_output.tfrecord.gz. 2020-09-11 02:28:59.544704: I deepvariant/post",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:269,safety,test,test,269,"Hi @pichuan and @tedyun ~. I run the command which @pichuan provided but it still print nothing on terminal. . And the docker version of this machine is `Docker version 19.03.3, build a872fc2`. So I switch to another machine which supported AVX instruction and run the test again by @tedyun 's advice. Then it run normally and output the files as same as the tutorial. Here is the environment & the command:. - Host CPU info. ```text. processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 63. model name : Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz. stepping : 2. microcode : 0x43. cpu MHz : 1199.975. cache size : 25600 KB. physical id : 0. siblings : 20. core id : 0. cpu cores : 10. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 15. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:1057,safety,monitor,monitor,1057,"rovided but it still print nothing on terminal. . And the docker version of this machine is `Docker version 19.03.3, build a872fc2`. So I switch to another machine which supported AVX instruction and run the test again by @tedyun 's advice. Then it run normally and output the files as same as the tutorial. Here is the environment & the command:. - Host CPU info. ```text. processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 63. model name : Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz. stepping : 2. microcode : 0x43. cpu MHz : 1199.975. cache size : 25600 KB. physical id : 0. siblings : 20. core id : 0. cpu cores : 10. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 15. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.bam.bai. test_nist.b37_chr20_100kbp_at_10mb.bed. test_nist.b3",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:1568,safety,manag,management,1568," microcode : 0x43. cpu MHz : 1199.975. cache size : 25600 KB. physical id : 0. siblings : 20. core id : 0. cpu cores : 10. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 15. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.bam.bai. test_nist.b37_chr20_100kbp_at_10mb.bed. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. ucsc.hg19.chr20.unittest.fasta. ucsc.hg19.chr20.unittest.fasta.fai. ucsc.hg19.chr20.unittest.fasta.gz. ucsc.hg19.chr20.unittest.fasta.gz.fai. ucsc.hg19.chr20.unittest.fasta.gz.gzi. # docker run \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:1871,safety,Test,Test,1871,"sh dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.bam.bai. test_nist.b37_chr20_100kbp_at_10mb.bed. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. ucsc.hg19.chr20.unittest.fasta. ucsc.hg19.chr20.unittest.fasta.fai. ucsc.hg19.chr20.unittest.fasta.gz. ucsc.hg19.chr20.unittest.fasta.gz.fai. ucsc.hg19.chr20.unittest.fasta.gz.gzi. # docker run \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=1 \. >. Status: Downloaded newer image for google/deepvari",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:2358,safety,input,input,2358," cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.bam.bai. test_nist.b37_chr20_100kbp_at_10mb.bed. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. ucsc.hg19.chr20.unittest.fasta. ucsc.hg19.chr20.unittest.fasta.fai. ucsc.hg19.chr20.unittest.fasta.gz. ucsc.hg19.chr20.unittest.fasta.gz.fai. ucsc.hg19.chr20.unittest.fasta.gz.gzi. # docker run \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=1 \. >. Status: Downloaded newer image for google/deepvariant:1.0.0. I0911 02:28:36.697342 140021722134272 run_deepvariant.py:269] Creating a directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --example",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:2516,safety,input,input,2516,"sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.bam.bai. test_nist.b37_chr20_100kbp_at_10mb.bed. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. ucsc.hg19.chr20.unittest.fasta. ucsc.hg19.chr20.unittest.fasta.fai. ucsc.hg19.chr20.unittest.fasta.gz. ucsc.hg19.chr20.unittest.fasta.gz.fai. ucsc.hg19.chr20.unittest.fasta.gz.gzi. # docker run \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=1 \. >. Status: Downloaded newer image for google/deepvariant:1.0.0. I0911 02:28:36.697342 140021722134272 run_deepvariant.py:269] Creating a directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:2567,safety,input,input,2567,"nagement:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.bam.bai. test_nist.b37_chr20_100kbp_at_10mb.bed. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. ucsc.hg19.chr20.unittest.fasta. ucsc.hg19.chr20.unittest.fasta.fai. ucsc.hg19.chr20.unittest.fasta.gz. ucsc.hg19.chr20.unittest.fasta.gz.fai. ucsc.hg19.chr20.unittest.fasta.gz.gzi. # docker run \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=1 \. >. Status: Downloaded newer image for google/deepvariant:1.0.0. I0911 02:28:36.697342 140021722134272 run_deepvariant.py:269] Creating a directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. I0911 02:28:39.900765 1403660752",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:3266,safety,input,input,3266,"sta.gz.fai. ucsc.hg19.chr20.unittest.fasta.gz.gzi. # docker run \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=1 \. >. Status: Downloaded newer image for google/deepvariant:1.0.0. I0911 02:28:36.697342 140021722134272 run_deepvariant.py:269] Creating a directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. I0911 02:28:39.900765 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.914417 140366075229952 make_examples.py:587] Preparing inputs. I0911 02:28:39.915202 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.916366 140366075229952 make_examples.py:587] Common contigs are ['chr20']. I0911 02:28:39.917786 140366075229952 make_examples.py:587] Writing examples to /output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz. I0911 02:28:39.917969 140366075229952 make_examples.py:587] Writing gvcf records to /output/intermediate_results_dir/gvcf.tfrecord-00000-of-00001.gz",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:3314,safety,input,input,3314,"i. # docker run \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=1 \. >. Status: Downloaded newer image for google/deepvariant:1.0.0. I0911 02:28:36.697342 140021722134272 run_deepvariant.py:269] Creating a directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. I0911 02:28:39.900765 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.914417 140366075229952 make_examples.py:587] Preparing inputs. I0911 02:28:39.915202 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.916366 140366075229952 make_examples.py:587] Common contigs are ['chr20']. I0911 02:28:39.917786 140366075229952 make_examples.py:587] Writing examples to /output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz. I0911 02:28:39.917969 140366075229952 make_examples.py:587] Writing gvcf records to /output/intermediate_results_dir/gvcf.tfrecord-00000-of-00001.gz. I0911 02:28:39.918812 140366075229952 make_exa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:3609,safety,input,input,3609,"egions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=1 \. >. Status: Downloaded newer image for google/deepvariant:1.0.0. I0911 02:28:36.697342 140021722134272 run_deepvariant.py:269] Creating a directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. I0911 02:28:39.900765 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.914417 140366075229952 make_examples.py:587] Preparing inputs. I0911 02:28:39.915202 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.916366 140366075229952 make_examples.py:587] Common contigs are ['chr20']. I0911 02:28:39.917786 140366075229952 make_examples.py:587] Writing examples to /output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz. I0911 02:28:39.917969 140366075229952 make_examples.py:587] Writing gvcf records to /output/intermediate_results_dir/gvcf.tfrecord-00000-of-00001.gz. I0911 02:28:39.918812 140366075229952 make_examples.py:587] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2020-09-11 02:28:39.919135: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. I0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:3738,safety,input,inputs,3738,"termediate_results_dir /output/intermediate_results_dir \. > --num_shards=1 \. >. Status: Downloaded newer image for google/deepvariant:1.0.0. I0911 02:28:36.697342 140021722134272 run_deepvariant.py:269] Creating a directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. I0911 02:28:39.900765 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.914417 140366075229952 make_examples.py:587] Preparing inputs. I0911 02:28:39.915202 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.916366 140366075229952 make_examples.py:587] Common contigs are ['chr20']. I0911 02:28:39.917786 140366075229952 make_examples.py:587] Writing examples to /output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz. I0911 02:28:39.917969 140366075229952 make_examples.py:587] Writing gvcf records to /output/intermediate_results_dir/gvcf.tfrecord-00000-of-00001.gz. I0911 02:28:39.918812 140366075229952 make_examples.py:587] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2020-09-11 02:28:39.919135: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. I0911 02:28:39.922723 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:3817,safety,input,input,3817,">. Status: Downloaded newer image for google/deepvariant:1.0.0. I0911 02:28:36.697342 140021722134272 run_deepvariant.py:269] Creating a directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. I0911 02:28:39.900765 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.914417 140366075229952 make_examples.py:587] Preparing inputs. I0911 02:28:39.915202 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.916366 140366075229952 make_examples.py:587] Common contigs are ['chr20']. I0911 02:28:39.917786 140366075229952 make_examples.py:587] Writing examples to /output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz. I0911 02:28:39.917969 140366075229952 make_examples.py:587] Writing gvcf records to /output/intermediate_results_dir/gvcf.tfrecord-00000-of-00001.gz. I0911 02:28:39.918812 140366075229952 make_examples.py:587] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2020-09-11 02:28:39.919135: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. I0911 02:28:39.922723 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.923217 140366075229952 genomics_reader.py:223] Reading /input/NA1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:4414,safety,input,input,4414,"rd@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. I0911 02:28:39.900765 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.914417 140366075229952 make_examples.py:587] Preparing inputs. I0911 02:28:39.915202 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.916366 140366075229952 make_examples.py:587] Common contigs are ['chr20']. I0911 02:28:39.917786 140366075229952 make_examples.py:587] Writing examples to /output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz. I0911 02:28:39.917969 140366075229952 make_examples.py:587] Writing gvcf records to /output/intermediate_results_dir/gvcf.tfrecord-00000-of-00001.gz. I0911 02:28:39.918812 140366075229952 make_examples.py:587] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2020-09-11 02:28:39.919135: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. I0911 02:28:39.922723 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.923217 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:40.247040 140366075229952 make_examples.py:587] 6 candidates (6 examples) [0.33s elapsed]. I0911 02:28:41.745102 140366075229952 make_examples.py:587] Found 78 candidate variants. I0911 02:28:41.745359 140366075229952 make_examples.py:587] Created 86 examples. real 0m5.273s. user 0m5.898s. sys 0m3.792s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --che",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:4681,safety,input,input,4681,"14417 140366075229952 make_examples.py:587] Preparing inputs. I0911 02:28:39.915202 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.916366 140366075229952 make_examples.py:587] Common contigs are ['chr20']. I0911 02:28:39.917786 140366075229952 make_examples.py:587] Writing examples to /output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz. I0911 02:28:39.917969 140366075229952 make_examples.py:587] Writing gvcf records to /output/intermediate_results_dir/gvcf.tfrecord-00000-of-00001.gz. I0911 02:28:39.918812 140366075229952 make_examples.py:587] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2020-09-11 02:28:39.919135: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. I0911 02:28:39.922723 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.923217 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:40.247040 140366075229952 make_examples.py:587] 6 candidates (6 examples) [0.33s elapsed]. I0911 02:28:41.745102 140366075229952 make_examples.py:587] Found 78 candidate variants. I0911 02:28:41.745359 140366075229952 make_examples.py:587] Created 86 examples. real 0m5.273s. user 0m5.898s. sys 0m3.792s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0911 02:28:44.682443 139937686464256 call_variants.py:335] Shape of input examples: [100, 221, 6]. 2020-09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:4811,safety,input,input,4811,"g /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.916366 140366075229952 make_examples.py:587] Common contigs are ['chr20']. I0911 02:28:39.917786 140366075229952 make_examples.py:587] Writing examples to /output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz. I0911 02:28:39.917969 140366075229952 make_examples.py:587] Writing gvcf records to /output/intermediate_results_dir/gvcf.tfrecord-00000-of-00001.gz. I0911 02:28:39.918812 140366075229952 make_examples.py:587] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2020-09-11 02:28:39.919135: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. I0911 02:28:39.922723 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.923217 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:40.247040 140366075229952 make_examples.py:587] 6 candidates (6 examples) [0.33s elapsed]. I0911 02:28:41.745102 140366075229952 make_examples.py:587] Found 78 candidate variants. I0911 02:28:41.745359 140366075229952 make_examples.py:587] Created 86 examples. real 0m5.273s. user 0m5.898s. sys 0m3.792s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0911 02:28:44.682443 139937686464256 call_variants.py:335] Shape of input examples: [100, 221, 6]. 2020-09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operat",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:5524,safety,input,input,5524,"third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. I0911 02:28:39.922723 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.923217 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:40.247040 140366075229952 make_examples.py:587] 6 candidates (6 examples) [0.33s elapsed]. I0911 02:28:41.745102 140366075229952 make_examples.py:587] Found 78 candidate variants. I0911 02:28:41.745359 140366075229952 make_examples.py:587] Created 86 examples. real 0m5.273s. user 0m5.898s. sys 0m3.792s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0911 02:28:44.682443 139937686464256 call_variants.py:335] Shape of input examples: [100, 221, 6]. 2020-09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-09-11 02:28:44.721599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2297525000 Hz. 2020-09-11 02:28:44.724518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5886480 executing computations on platform Host. Devices:. 2020-09-11 02:28:44.724555: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-09-11 02:28:44.727738: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W0911 02:28:44.755701 139937686464256 estimator.py:18",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:7921,safety,updat,updating,7921,"steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f4549cfe518>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I0911 02:28:44.756798 139937686464256 call_variants.py:426] Writing calls to /output/intermediate_results_dir/call_variants_output.tfrecord.gz. W0911 02:28:44.767428 139937686464256 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version. Instructions for updating:. If using Keras pass *_constraint arguments to layers. W0911 02:28:44.788271 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. W0911 02:28:44.973487 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:8299,safety,updat,updating,8299,"global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I0911 02:28:44.756798 139937686464256 call_variants.py:426] Writing calls to /output/intermediate_results_dir/call_variants_output.tfrecord.gz. W0911 02:28:44.767428 139937686464256 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version. Instructions for updating:. If using Keras pass *_constraint arguments to layers. W0911 02:28:44.788271 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. W0911 02:28:44.973487 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I0911 02:28:45.531516 139937686464256 estimator.py:1147] Calling model_fn. W0911 02:28:45.537137 139937686464256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:8822,safety,updat,updating,8822,"e_ops) with constraint is deprecated and will be removed in a future version. Instructions for updating:. If using Keras pass *_constraint arguments to layers. W0911 02:28:44.788271 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. W0911 02:28:44.973487 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I0911 02:28:45.531516 139937686464256 estimator.py:1147] Calling model_fn. W0911 02:28:45.537137 139937686464256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0911 02:28:50.978360 139937686464256 estimator.py:1149] Done calling model_fn. I0911 02:28:52.395916 139937686464256 monitored_session.py:240] Graph was finalized. I0911 02:28:52.397383 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:53.423366 139937686464256 session_manager.py:500] Running local_init_op. I0911 02:28:53.495537 139937686464256 se",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:9374,safety,updat,updating,9374,", num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. W0911 02:28:44.973487 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I0911 02:28:45.531516 139937686464256 estimator.py:1147] Calling model_fn. W0911 02:28:45.537137 139937686464256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0911 02:28:50.978360 139937686464256 estimator.py:1149] Done calling model_fn. I0911 02:28:52.395916 139937686464256 monitored_session.py:240] Graph was finalized. I0911 02:28:52.397383 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:53.423366 139937686464256 session_manager.py:500] Running local_init_op. I0911 02:28:53.495537 139937686464256 session_manager.py:502] Done running local_init_op. I0911 02:28:54.010662 139937686464256 modeling.py:415] Reloading EMA... I0911 02:28:54.011811 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:56.213706 139937686464256 call_variants.py:444] Processed 1 examples in 1 batches [1145.647 sec per 100]. I0911 02:28:56.265293 139937686464256 call_variants.py:447] Processed 86 examples in 1 batches [13.334 sec per 100]. I0911 02:28:56.265480 139937686464256 call_variants.py:450] Done calling variants from a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:10535,safety,input,input,10535,"86464256 monitored_session.py:240] Graph was finalized. I0911 02:28:52.397383 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:53.423366 139937686464256 session_manager.py:500] Running local_init_op. I0911 02:28:53.495537 139937686464256 session_manager.py:502] Done running local_init_op. I0911 02:28:54.010662 139937686464256 modeling.py:415] Reloading EMA... I0911 02:28:54.011811 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:56.213706 139937686464256 call_variants.py:444] Processed 1 examples in 1 batches [1145.647 sec per 100]. I0911 02:28:56.265293 139937686464256 call_variants.py:447] Processed 86 examples in 1 batches [13.334 sec per 100]. I0911 02:28:56.265480 139937686464256 call_variants.py:450] Done calling variants from a total of 86 examples. real 0m14.761s. user 0m20.588s. sys 0m5.092s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --infile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/output/output.g.vcf.gz"". 2020-09-11 02:28:59.543794: I deepvariant/postprocess_variants.cc:88] Read from: /output/intermediate_results_dir/call_variants_output.tfrecord.gz. 2020-09-11 02:28:59.544704: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 86. I0911 02:28:59.545160 140173302474496 postprocess_variants.py:1079] CVO sorting took 2.2590160369873047e-05 minutes. I0911 02:28:59.545950 140173302474496 postprocess_variants.py:1081] Transforming call_variants_output to variants. I0911 02:28:59.546221 140173302474496 postprocess_variants.py:1108] Merging and writing variants to VCF and gVCF. I0911 02:28:59.547042 140173302474496 genomics_writer.py:172] Writing /output/output.vcf.gz with NativeVcfWriter. I0911",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:492,security,model,model,492,"Hi @pichuan and @tedyun ~. I run the command which @pichuan provided but it still print nothing on terminal. . And the docker version of this machine is `Docker version 19.03.3, build a872fc2`. So I switch to another machine which supported AVX instruction and run the test again by @tedyun 's advice. Then it run normally and output the files as same as the tutorial. Here is the environment & the command:. - Host CPU info. ```text. processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 63. model name : Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz. stepping : 2. microcode : 0x43. cpu MHz : 1199.975. cache size : 25600 KB. physical id : 0. siblings : 20. core id : 0. cpu cores : 10. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 15. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:504,security,model,model,504,"Hi @pichuan and @tedyun ~. I run the command which @pichuan provided but it still print nothing on terminal. . And the docker version of this machine is `Docker version 19.03.3, build a872fc2`. So I switch to another machine which supported AVX instruction and run the test again by @tedyun 's advice. Then it run normally and output the files as same as the tutorial. Here is the environment & the command:. - Host CPU info. ```text. processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 63. model name : Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz. stepping : 2. microcode : 0x43. cpu MHz : 1199.975. cache size : 25600 KB. physical id : 0. siblings : 20. core id : 0. cpu cores : 10. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 15. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:5431,security,model,models,5431,"will decode CRAM using the reference you passed in with --ref. 2020-09-11 02:28:39.919135: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. I0911 02:28:39.922723 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.923217 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:40.247040 140366075229952 make_examples.py:587] 6 candidates (6 examples) [0.33s elapsed]. I0911 02:28:41.745102 140366075229952 make_examples.py:587] Found 78 candidate variants. I0911 02:28:41.745359 140366075229952 make_examples.py:587] Created 86 examples. real 0m5.273s. user 0m5.898s. sys 0m3.792s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0911 02:28:44.682443 139937686464256 call_variants.py:335] Shape of input examples: [100, 221, 6]. 2020-09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-09-11 02:28:44.721599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2297525000 Hz. 2020-09-11 02:28:44.724518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5886480 executing computations on platform Host. Devices:. 2020-09-11 02:28:44.724555: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-09-11 02:28:44.727738: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_pa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:5442,security,model,model,5442," CRAM using the reference you passed in with --ref. 2020-09-11 02:28:39.919135: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. I0911 02:28:39.922723 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.923217 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:40.247040 140366075229952 make_examples.py:587] 6 candidates (6 examples) [0.33s elapsed]. I0911 02:28:41.745102 140366075229952 make_examples.py:587] Found 78 candidate variants. I0911 02:28:41.745359 140366075229952 make_examples.py:587] Created 86 examples. real 0m5.273s. user 0m5.898s. sys 0m3.792s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0911 02:28:44.682443 139937686464256 call_variants.py:335] Shape of input examples: [100, 221, 6]. 2020-09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-09-11 02:28:44.721599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2297525000 Hz. 2020-09-11 02:28:44.724518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5886480 executing computations on platform Host. Devices:. 2020-09-11 02:28:44.724555: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-09-11 02:28:44.727738: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:6557,security,model,model,6557,"09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-09-11 02:28:44.721599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2297525000 Hz. 2020-09-11 02:28:44.724518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5886480 executing computations on platform Host. Devices:. 2020-09-11 02:28:44.724555: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-09-11 02:28:44.727738: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W0911 02:28:44.755701 139937686464256 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp8bwus8dp. I0911 02:28:44.756375 139937686464256 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp8bwus8dp', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f4549cfe518>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I0911 02:28:44.756798 139937686464256 call_variants.py:426] Writing calls to /output/intermediate_results_dir/call_varia",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:7921,security,updat,updating,7921,"steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f4549cfe518>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I0911 02:28:44.756798 139937686464256 call_variants.py:426] Writing calls to /output/intermediate_results_dir/call_variants_output.tfrecord.gz. W0911 02:28:44.767428 139937686464256 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version. Instructions for updating:. If using Keras pass *_constraint arguments to layers. W0911 02:28:44.788271 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. W0911 02:28:44.973487 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:8299,security,updat,updating,8299,"global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I0911 02:28:44.756798 139937686464256 call_variants.py:426] Writing calls to /output/intermediate_results_dir/call_variants_output.tfrecord.gz. W0911 02:28:44.767428 139937686464256 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version. Instructions for updating:. If using Keras pass *_constraint arguments to layers. W0911 02:28:44.788271 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. W0911 02:28:44.973487 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I0911 02:28:45.531516 139937686464256 estimator.py:1147] Calling model_fn. W0911 02:28:45.537137 139937686464256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:8822,security,updat,updating,8822,"e_ops) with constraint is deprecated and will be removed in a future version. Instructions for updating:. If using Keras pass *_constraint arguments to layers. W0911 02:28:44.788271 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. W0911 02:28:44.973487 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I0911 02:28:45.531516 139937686464256 estimator.py:1147] Calling model_fn. W0911 02:28:45.537137 139937686464256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0911 02:28:50.978360 139937686464256 estimator.py:1149] Done calling model_fn. I0911 02:28:52.395916 139937686464256 monitored_session.py:240] Graph was finalized. I0911 02:28:52.397383 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:53.423366 139937686464256 session_manager.py:500] Running local_init_op. I0911 02:28:53.495537 139937686464256 se",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:9374,security,updat,updating,9374,", num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`. W0911 02:28:44.973487 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I0911 02:28:45.531516 139937686464256 estimator.py:1147] Calling model_fn. W0911 02:28:45.537137 139937686464256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0911 02:28:50.978360 139937686464256 estimator.py:1149] Done calling model_fn. I0911 02:28:52.395916 139937686464256 monitored_session.py:240] Graph was finalized. I0911 02:28:52.397383 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:53.423366 139937686464256 session_manager.py:500] Running local_init_op. I0911 02:28:53.495537 139937686464256 session_manager.py:502] Done running local_init_op. I0911 02:28:54.010662 139937686464256 modeling.py:415] Reloading EMA... I0911 02:28:54.011811 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:56.213706 139937686464256 call_variants.py:444] Processed 1 examples in 1 batches [1145.647 sec per 100]. I0911 02:28:56.265293 139937686464256 call_variants.py:447] Processed 86 examples in 1 batches [13.334 sec per 100]. I0911 02:28:56.265480 139937686464256 call_variants.py:450] Done calling variants from a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:9678,security,model,models,9678,"ap_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I0911 02:28:45.531516 139937686464256 estimator.py:1147] Calling model_fn. W0911 02:28:45.537137 139937686464256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0911 02:28:50.978360 139937686464256 estimator.py:1149] Done calling model_fn. I0911 02:28:52.395916 139937686464256 monitored_session.py:240] Graph was finalized. I0911 02:28:52.397383 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:53.423366 139937686464256 session_manager.py:500] Running local_init_op. I0911 02:28:53.495537 139937686464256 session_manager.py:502] Done running local_init_op. I0911 02:28:54.010662 139937686464256 modeling.py:415] Reloading EMA... I0911 02:28:54.011811 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:56.213706 139937686464256 call_variants.py:444] Processed 1 examples in 1 batches [1145.647 sec per 100]. I0911 02:28:56.265293 139937686464256 call_variants.py:447] Processed 86 examples in 1 batches [13.334 sec per 100]. I0911 02:28:56.265480 139937686464256 call_variants.py:450] Done calling variants from a total of 86 examples. real 0m14.761s. user 0m20.588s. sys 0m5.092s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --infile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/output/output.vcf.g",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:9689,security,model,model,9689,"h (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I0911 02:28:45.531516 139937686464256 estimator.py:1147] Calling model_fn. W0911 02:28:45.537137 139937686464256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0911 02:28:50.978360 139937686464256 estimator.py:1149] Done calling model_fn. I0911 02:28:52.395916 139937686464256 monitored_session.py:240] Graph was finalized. I0911 02:28:52.397383 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:53.423366 139937686464256 session_manager.py:500] Running local_init_op. I0911 02:28:53.495537 139937686464256 session_manager.py:502] Done running local_init_op. I0911 02:28:54.010662 139937686464256 modeling.py:415] Reloading EMA... I0911 02:28:54.011811 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:56.213706 139937686464256 call_variants.py:444] Processed 1 examples in 1 batches [1145.647 sec per 100]. I0911 02:28:56.265293 139937686464256 call_variants.py:447] Processed 86 examples in 1 batches [13.334 sec per 100]. I0911 02:28:56.265480 139937686464256 call_variants.py:450] Done calling variants from a total of 86 examples. real 0m14.761s. user 0m20.588s. sys 0m5.092s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --infile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/output/output.vcf.gz"" --nonvar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:9914,security,model,modeling,9914,"batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation. I0911 02:28:45.531516 139937686464256 estimator.py:1147] Calling model_fn. W0911 02:28:45.537137 139937686464256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0911 02:28:50.978360 139937686464256 estimator.py:1149] Done calling model_fn. I0911 02:28:52.395916 139937686464256 monitored_session.py:240] Graph was finalized. I0911 02:28:52.397383 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:53.423366 139937686464256 session_manager.py:500] Running local_init_op. I0911 02:28:53.495537 139937686464256 session_manager.py:502] Done running local_init_op. I0911 02:28:54.010662 139937686464256 modeling.py:415] Reloading EMA... I0911 02:28:54.011811 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:56.213706 139937686464256 call_variants.py:444] Processed 1 examples in 1 batches [1145.647 sec per 100]. I0911 02:28:56.265293 139937686464256 call_variants.py:447] Processed 86 examples in 1 batches [13.334 sec per 100]. I0911 02:28:56.265480 139937686464256 call_variants.py:450] Done calling variants from a total of 86 examples. real 0m14.761s. user 0m20.588s. sys 0m5.092s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --infile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/output/output.g.vcf.gz"". 2020-09-11 02:28:59.543794: I deepvariant/postprocess_variants.cc:88] Read from: /output/intermediate_resu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:10032,security,model,models,10032,"911 02:28:45.531516 139937686464256 estimator.py:1147] Calling model_fn. W0911 02:28:45.537137 139937686464256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0911 02:28:50.978360 139937686464256 estimator.py:1149] Done calling model_fn. I0911 02:28:52.395916 139937686464256 monitored_session.py:240] Graph was finalized. I0911 02:28:52.397383 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:53.423366 139937686464256 session_manager.py:500] Running local_init_op. I0911 02:28:53.495537 139937686464256 session_manager.py:502] Done running local_init_op. I0911 02:28:54.010662 139937686464256 modeling.py:415] Reloading EMA... I0911 02:28:54.011811 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:56.213706 139937686464256 call_variants.py:444] Processed 1 examples in 1 batches [1145.647 sec per 100]. I0911 02:28:56.265293 139937686464256 call_variants.py:447] Processed 86 examples in 1 batches [13.334 sec per 100]. I0911 02:28:56.265480 139937686464256 call_variants.py:450] Done calling variants from a total of 86 examples. real 0m14.761s. user 0m20.588s. sys 0m5.092s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --infile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/output/output.g.vcf.gz"". 2020-09-11 02:28:59.543794: I deepvariant/postprocess_variants.cc:88] Read from: /output/intermediate_results_dir/call_variants_output.tfrecord.gz. 2020-09-11 02:28:59.544704: I deepvariant/postprocess_variants.cc:103] Tota",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:10043,security,model,model,10043,"5.531516 139937686464256 estimator.py:1147] Calling model_fn. W0911 02:28:45.537137 139937686464256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version. Instructions for updating:. Please use `layer.__call__` method instead. I0911 02:28:50.978360 139937686464256 estimator.py:1149] Done calling model_fn. I0911 02:28:52.395916 139937686464256 monitored_session.py:240] Graph was finalized. I0911 02:28:52.397383 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:53.423366 139937686464256 session_manager.py:500] Running local_init_op. I0911 02:28:53.495537 139937686464256 session_manager.py:502] Done running local_init_op. I0911 02:28:54.010662 139937686464256 modeling.py:415] Reloading EMA... I0911 02:28:54.011811 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:56.213706 139937686464256 call_variants.py:444] Processed 1 examples in 1 batches [1145.647 sec per 100]. I0911 02:28:56.265293 139937686464256 call_variants.py:447] Processed 86 examples in 1 batches [13.334 sec per 100]. I0911 02:28:56.265480 139937686464256 call_variants.py:450] Done calling variants from a total of 86 examples. real 0m14.761s. user 0m20.588s. sys 0m5.092s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --infile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/output/output.g.vcf.gz"". 2020-09-11 02:28:59.543794: I deepvariant/postprocess_variants.cc:88] Read from: /output/intermediate_results_dir/call_variants_output.tfrecord.gz. 2020-09-11 02:28:59.544704: I deepvariant/postprocess_variants.cc:103] Total #entries ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:269,testability,test,test,269,"Hi @pichuan and @tedyun ~. I run the command which @pichuan provided but it still print nothing on terminal. . And the docker version of this machine is `Docker version 19.03.3, build a872fc2`. So I switch to another machine which supported AVX instruction and run the test again by @tedyun 's advice. Then it run normally and output the files as same as the tutorial. Here is the environment & the command:. - Host CPU info. ```text. processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 63. model name : Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz. stepping : 2. microcode : 0x43. cpu MHz : 1199.975. cache size : 25600 KB. physical id : 0. siblings : 20. core id : 0. cpu cores : 10. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 15. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:1057,testability,monitor,monitor,1057,"rovided but it still print nothing on terminal. . And the docker version of this machine is `Docker version 19.03.3, build a872fc2`. So I switch to another machine which supported AVX instruction and run the test again by @tedyun 's advice. Then it run normally and output the files as same as the tutorial. Here is the environment & the command:. - Host CPU info. ```text. processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 63. model name : Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz. stepping : 2. microcode : 0x43. cpu MHz : 1199.975. cache size : 25600 KB. physical id : 0. siblings : 20. core id : 0. cpu cores : 10. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 15. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.bam.bai. test_nist.b37_chr20_100kbp_at_10mb.bed. test_nist.b3",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:1871,testability,Test,Test,1871,"sh dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.bam.bai. test_nist.b37_chr20_100kbp_at_10mb.bed. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. ucsc.hg19.chr20.unittest.fasta. ucsc.hg19.chr20.unittest.fasta.fai. ucsc.hg19.chr20.unittest.fasta.gz. ucsc.hg19.chr20.unittest.fasta.gz.fai. ucsc.hg19.chr20.unittest.fasta.gz.gzi. # docker run \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=1 \. >. Status: Downloaded newer image for google/deepvari",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:2155,testability,unit,unittest,2155,"t tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.bam.bai. test_nist.b37_chr20_100kbp_at_10mb.bed. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. ucsc.hg19.chr20.unittest.fasta. ucsc.hg19.chr20.unittest.fasta.fai. ucsc.hg19.chr20.unittest.fasta.gz. ucsc.hg19.chr20.unittest.fasta.gz.fai. ucsc.hg19.chr20.unittest.fasta.gz.gzi. # docker run \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=1 \. >. Status: Downloaded newer image for google/deepvariant:1.0.0. I0911 02:28:36.697342 140021722134272 run_deepvariant.py:269] Creating a directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:2187,testability,unit,unittest,2187,"vx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.bam.bai. test_nist.b37_chr20_100kbp_at_10mb.bed. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. ucsc.hg19.chr20.unittest.fasta. ucsc.hg19.chr20.unittest.fasta.fai. ucsc.hg19.chr20.unittest.fasta.gz. ucsc.hg19.chr20.unittest.fasta.gz.fai. ucsc.hg19.chr20.unittest.fasta.gz.gzi. # docker run \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=1 \. >. Status: Downloaded newer image for google/deepvariant:1.0.0. I0911 02:28:36.697342 140021722134272 run_deepvariant.py:269] Creating a directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:2223,testability,unit,unittest,2223,"id_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.bam.bai. test_nist.b37_chr20_100kbp_at_10mb.bed. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. ucsc.hg19.chr20.unittest.fasta. ucsc.hg19.chr20.unittest.fasta.fai. ucsc.hg19.chr20.unittest.fasta.gz. ucsc.hg19.chr20.unittest.fasta.gz.fai. ucsc.hg19.chr20.unittest.fasta.gz.gzi. # docker run \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=1 \. >. Status: Downloaded newer image for google/deepvariant:1.0.0. I0911 02:28:36.697342 140021722134272 run_deepvariant.py:269] Creating a directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:2258,testability,unit,unittest,2258,"shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.bam.bai. test_nist.b37_chr20_100kbp_at_10mb.bed. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. ucsc.hg19.chr20.unittest.fasta. ucsc.hg19.chr20.unittest.fasta.fai. ucsc.hg19.chr20.unittest.fasta.gz. ucsc.hg19.chr20.unittest.fasta.gz.fai. ucsc.hg19.chr20.unittest.fasta.gz.gzi. # docker run \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=1 \. >. Status: Downloaded newer image for google/deepvariant:1.0.0. I0911 02:28:36.697342 140021722134272 run_deepvariant.py:269] Creating a directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:2297,testability,unit,unittest,2297,"ase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.bam.bai. test_nist.b37_chr20_100kbp_at_10mb.bed. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. ucsc.hg19.chr20.unittest.fasta. ucsc.hg19.chr20.unittest.fasta.fai. ucsc.hg19.chr20.unittest.fasta.gz. ucsc.hg19.chr20.unittest.fasta.gz.fai. ucsc.hg19.chr20.unittest.fasta.gz.gzi. # docker run \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=1 \. >. Status: Downloaded newer image for google/deepvariant:1.0.0. I0911 02:28:36.697342 140021722134272 run_deepvariant.py:269] Creating a directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fast",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:2538,testability,unit,unittest,2538,"l, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.bam.bai. test_nist.b37_chr20_100kbp_at_10mb.bed. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. ucsc.hg19.chr20.unittest.fasta. ucsc.hg19.chr20.unittest.fasta.fai. ucsc.hg19.chr20.unittest.fasta.gz. ucsc.hg19.chr20.unittest.fasta.gz.fai. ucsc.hg19.chr20.unittest.fasta.gz.gzi. # docker run \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=1 \. >. Status: Downloaded newer image for google/deepvariant:1.0.0. I0911 02:28:36.697342 140021722134272 run_deepvariant.py:269] Creating a directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. I091",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:3288,testability,unit,unittest,3288,"hr20.unittest.fasta.gz.gzi. # docker run \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=1 \. >. Status: Downloaded newer image for google/deepvariant:1.0.0. I0911 02:28:36.697342 140021722134272 run_deepvariant.py:269] Creating a directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. I0911 02:28:39.900765 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.914417 140366075229952 make_examples.py:587] Preparing inputs. I0911 02:28:39.915202 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.916366 140366075229952 make_examples.py:587] Common contigs are ['chr20']. I0911 02:28:39.917786 140366075229952 make_examples.py:587] Writing examples to /output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz. I0911 02:28:39.917969 140366075229952 make_examples.py:587] Writing gvcf records to /output/intermediate_results_dir/gvcf.tfrecord-00000-of-00001.gz. I0911 02:28:39.918812",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:10557,testability,unit,unittest,10557,"ion.py:240] Graph was finalized. I0911 02:28:52.397383 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:53.423366 139937686464256 session_manager.py:500] Running local_init_op. I0911 02:28:53.495537 139937686464256 session_manager.py:502] Done running local_init_op. I0911 02:28:54.010662 139937686464256 modeling.py:415] Reloading EMA... I0911 02:28:54.011811 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:56.213706 139937686464256 call_variants.py:444] Processed 1 examples in 1 batches [1145.647 sec per 100]. I0911 02:28:56.265293 139937686464256 call_variants.py:447] Processed 86 examples in 1 batches [13.334 sec per 100]. I0911 02:28:56.265480 139937686464256 call_variants.py:450] Done calling variants from a total of 86 examples. real 0m14.761s. user 0m20.588s. sys 0m5.092s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --infile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/output/output.g.vcf.gz"". 2020-09-11 02:28:59.543794: I deepvariant/postprocess_variants.cc:88] Read from: /output/intermediate_results_dir/call_variants_output.tfrecord.gz. 2020-09-11 02:28:59.544704: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 86. I0911 02:28:59.545160 140173302474496 postprocess_variants.py:1079] CVO sorting took 2.2590160369873047e-05 minutes. I0911 02:28:59.545950 140173302474496 postprocess_variants.py:1081] Transforming call_variants_output to variants. I0911 02:28:59.546221 140173302474496 postprocess_variants.py:1108] Merging and writing variants to VCF and gVCF. I0911 02:28:59.547042 140173302474496 genomics_writer.py:172] Writing /output/output.vcf.gz with NativeVcfWriter. I0911 02:28:59.547737 140173",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:37,usability,command,command,37,"Hi @pichuan and @tedyun ~. I run the command which @pichuan provided but it still print nothing on terminal. . And the docker version of this machine is `Docker version 19.03.3, build a872fc2`. So I switch to another machine which supported AVX instruction and run the test again by @tedyun 's advice. Then it run normally and output the files as same as the tutorial. Here is the environment & the command:. - Host CPU info. ```text. processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 63. model name : Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz. stepping : 2. microcode : 0x43. cpu MHz : 1199.975. cache size : 25600 KB. physical id : 0. siblings : 20. core id : 0. cpu cores : 10. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 15. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:231,usability,support,supported,231,"Hi @pichuan and @tedyun ~. I run the command which @pichuan provided but it still print nothing on terminal. . And the docker version of this machine is `Docker version 19.03.3, build a872fc2`. So I switch to another machine which supported AVX instruction and run the test again by @tedyun 's advice. Then it run normally and output the files as same as the tutorial. Here is the environment & the command:. - Host CPU info. ```text. processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 63. model name : Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz. stepping : 2. microcode : 0x43. cpu MHz : 1199.975. cache size : 25600 KB. physical id : 0. siblings : 20. core id : 0. cpu cores : 10. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 15. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:399,usability,command,command,399,"Hi @pichuan and @tedyun ~. I run the command which @pichuan provided but it still print nothing on terminal. . And the docker version of this machine is `Docker version 19.03.3, build a872fc2`. So I switch to another machine which supported AVX instruction and run the test again by @tedyun 's advice. Then it run normally and output the files as same as the tutorial. Here is the environment & the command:. - Host CPU info. ```text. processor : 0. vendor_id : GenuineIntel. cpu family : 6. model : 63. model name : Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz. stepping : 2. microcode : 0x43. cpu MHz : 1199.975. cache size : 25600 KB. physical id : 0. siblings : 20. core id : 0. cpu cores : 10. apicid : 0. initial apicid : 0. fpu : yes. fpu_exception : yes. cpuid level : 15. wp : yes. flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:1880,usability,command,command,1880," mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.bam.bai. test_nist.b37_chr20_100kbp_at_10mb.bed. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. ucsc.hg19.chr20.unittest.fasta. ucsc.hg19.chr20.unittest.fasta.fai. ucsc.hg19.chr20.unittest.fasta.gz. ucsc.hg19.chr20.unittest.fasta.gz.fai. ucsc.hg19.chr20.unittest.fasta.gz.gzi. # docker run \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=1 \. >. Status: Downloaded newer image for google/deepvariant:1.0.0. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:2358,usability,input,input,2358," cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. bogomips : 4595.05. clflush size : 64. cache_alignment : 64. address sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.bam.bai. test_nist.b37_chr20_100kbp_at_10mb.bed. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. ucsc.hg19.chr20.unittest.fasta. ucsc.hg19.chr20.unittest.fasta.fai. ucsc.hg19.chr20.unittest.fasta.gz. ucsc.hg19.chr20.unittest.fasta.gz.fai. ucsc.hg19.chr20.unittest.fasta.gz.gzi. # docker run \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=1 \. >. Status: Downloaded newer image for google/deepvariant:1.0.0. I0911 02:28:36.697342 140021722134272 run_deepvariant.py:269] Creating a directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --example",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:2516,usability,input,input,2516,"sizes : 46 bits physical, 48 bits virtual. power management:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.bam.bai. test_nist.b37_chr20_100kbp_at_10mb.bed. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. ucsc.hg19.chr20.unittest.fasta. ucsc.hg19.chr20.unittest.fasta.fai. ucsc.hg19.chr20.unittest.fasta.gz. ucsc.hg19.chr20.unittest.fasta.gz.fai. ucsc.hg19.chr20.unittest.fasta.gz.gzi. # docker run \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=1 \. >. Status: Downloaded newer image for google/deepvariant:1.0.0. I0911 02:28:36.697342 140021722134272 run_deepvariant.py:269] Creating a directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:2567,usability,input,input,2567,"nagement:. ```. - OS ,kernel & docker version. ```sh. # uname -a. Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release. CentOS Linux release 7.7.1908 (Core). # docker -v. Docker version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.bam.bai. test_nist.b37_chr20_100kbp_at_10mb.bed. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. ucsc.hg19.chr20.unittest.fasta. ucsc.hg19.chr20.unittest.fasta.fai. ucsc.hg19.chr20.unittest.fasta.gz. ucsc.hg19.chr20.unittest.fasta.gz.fai. ucsc.hg19.chr20.unittest.fasta.gz.gzi. # docker run \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=1 \. >. Status: Downloaded newer image for google/deepvariant:1.0.0. I0911 02:28:36.697342 140021722134272 run_deepvariant.py:269] Creating a directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. I0911 02:28:39.900765 1403660752",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:2823,usability,Statu,Status,2823,"r version 19.03.12, build 48a66213fe. ```. - Test run command. ```sh. # BIN_VERSION=""1.0.0"". # ls -1 ${INPUT_DIR}. NA12878_S1.chr20.10_10p1mb.bam. NA12878_S1.chr20.10_10p1mb.bam.bai. test_nist.b37_chr20_100kbp_at_10mb.bed. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz. test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi. ucsc.hg19.chr20.unittest.fasta. ucsc.hg19.chr20.unittest.fasta.fai. ucsc.hg19.chr20.unittest.fasta.gz. ucsc.hg19.chr20.unittest.fasta.gz.fai. ucsc.hg19.chr20.unittest.fasta.gz.gzi. # docker run \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=1 \. >. Status: Downloaded newer image for google/deepvariant:1.0.0. I0911 02:28:36.697342 140021722134272 run_deepvariant.py:269] Creating a directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. I0911 02:28:39.900765 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.914417 140366075229952 make_examples.py:587] Preparing inputs. I0911 02:28:39.915202 140366075229952 genomics_reader.py:223] Reading /input/NA1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:3143,usability,command,command,3143,"9.chr20.unittest.fasta. ucsc.hg19.chr20.unittest.fasta.fai. ucsc.hg19.chr20.unittest.fasta.gz. ucsc.hg19.chr20.unittest.fasta.gz.fai. ucsc.hg19.chr20.unittest.fasta.gz.gzi. # docker run \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=1 \. >. Status: Downloaded newer image for google/deepvariant:1.0.0. I0911 02:28:36.697342 140021722134272 run_deepvariant.py:269] Creating a directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. I0911 02:28:39.900765 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.914417 140366075229952 make_examples.py:587] Preparing inputs. I0911 02:28:39.915202 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.916366 140366075229952 make_examples.py:587] Common contigs are ['chr20']. I0911 02:28:39.917786 140366075229952 make_examples.py:587] Writing examples to /output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz. I0911 02:28:39.917969 1403",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:3266,usability,input,input,3266,"sta.gz.fai. ucsc.hg19.chr20.unittest.fasta.gz.gzi. # docker run \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=1 \. >. Status: Downloaded newer image for google/deepvariant:1.0.0. I0911 02:28:36.697342 140021722134272 run_deepvariant.py:269] Creating a directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. I0911 02:28:39.900765 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.914417 140366075229952 make_examples.py:587] Preparing inputs. I0911 02:28:39.915202 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.916366 140366075229952 make_examples.py:587] Common contigs are ['chr20']. I0911 02:28:39.917786 140366075229952 make_examples.py:587] Writing examples to /output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz. I0911 02:28:39.917969 140366075229952 make_examples.py:587] Writing gvcf records to /output/intermediate_results_dir/gvcf.tfrecord-00000-of-00001.gz",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:3314,usability,input,input,3314,"i. # docker run \. > -v ""${INPUT_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}"":""/output"" \. > google/deepvariant:""${BIN_VERSION}"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=/input/ucsc.hg19.chr20.unittest.fasta \. > --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \. > --regions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=1 \. >. Status: Downloaded newer image for google/deepvariant:1.0.0. I0911 02:28:36.697342 140021722134272 run_deepvariant.py:269] Creating a directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. I0911 02:28:39.900765 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.914417 140366075229952 make_examples.py:587] Preparing inputs. I0911 02:28:39.915202 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.916366 140366075229952 make_examples.py:587] Common contigs are ['chr20']. I0911 02:28:39.917786 140366075229952 make_examples.py:587] Writing examples to /output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz. I0911 02:28:39.917969 140366075229952 make_examples.py:587] Writing gvcf records to /output/intermediate_results_dir/gvcf.tfrecord-00000-of-00001.gz. I0911 02:28:39.918812 140366075229952 make_exa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:3609,usability,input,input,3609,"egions ""chr20:10,000,000-10,010,000"" \. > --output_vcf=/output/output.vcf.gz \. > --output_gvcf=/output/output.g.vcf.gz \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=1 \. >. Status: Downloaded newer image for google/deepvariant:1.0.0. I0911 02:28:36.697342 140021722134272 run_deepvariant.py:269] Creating a directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. I0911 02:28:39.900765 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.914417 140366075229952 make_examples.py:587] Preparing inputs. I0911 02:28:39.915202 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.916366 140366075229952 make_examples.py:587] Common contigs are ['chr20']. I0911 02:28:39.917786 140366075229952 make_examples.py:587] Writing examples to /output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz. I0911 02:28:39.917969 140366075229952 make_examples.py:587] Writing gvcf records to /output/intermediate_results_dir/gvcf.tfrecord-00000-of-00001.gz. I0911 02:28:39.918812 140366075229952 make_examples.py:587] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2020-09-11 02:28:39.919135: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. I0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:3738,usability,input,inputs,3738,"termediate_results_dir /output/intermediate_results_dir \. > --num_shards=1 \. >. Status: Downloaded newer image for google/deepvariant:1.0.0. I0911 02:28:36.697342 140021722134272 run_deepvariant.py:269] Creating a directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. I0911 02:28:39.900765 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.914417 140366075229952 make_examples.py:587] Preparing inputs. I0911 02:28:39.915202 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.916366 140366075229952 make_examples.py:587] Common contigs are ['chr20']. I0911 02:28:39.917786 140366075229952 make_examples.py:587] Writing examples to /output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz. I0911 02:28:39.917969 140366075229952 make_examples.py:587] Writing gvcf records to /output/intermediate_results_dir/gvcf.tfrecord-00000-of-00001.gz. I0911 02:28:39.918812 140366075229952 make_examples.py:587] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2020-09-11 02:28:39.919135: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. I0911 02:28:39.922723 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:3817,usability,input,input,3817,">. Status: Downloaded newer image for google/deepvariant:1.0.0. I0911 02:28:36.697342 140021722134272 run_deepvariant.py:269] Creating a directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****. time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. I0911 02:28:39.900765 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.914417 140366075229952 make_examples.py:587] Preparing inputs. I0911 02:28:39.915202 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.916366 140366075229952 make_examples.py:587] Common contigs are ['chr20']. I0911 02:28:39.917786 140366075229952 make_examples.py:587] Writing examples to /output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz. I0911 02:28:39.917969 140366075229952 make_examples.py:587] Writing gvcf records to /output/intermediate_results_dir/gvcf.tfrecord-00000-of-00001.gz. I0911 02:28:39.918812 140366075229952 make_examples.py:587] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2020-09-11 02:28:39.919135: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. I0911 02:28:39.922723 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.923217 140366075229952 genomics_reader.py:223] Reading /input/NA1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:4414,usability,input,input,4414,"rd@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. I0911 02:28:39.900765 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.914417 140366075229952 make_examples.py:587] Preparing inputs. I0911 02:28:39.915202 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.916366 140366075229952 make_examples.py:587] Common contigs are ['chr20']. I0911 02:28:39.917786 140366075229952 make_examples.py:587] Writing examples to /output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz. I0911 02:28:39.917969 140366075229952 make_examples.py:587] Writing gvcf records to /output/intermediate_results_dir/gvcf.tfrecord-00000-of-00001.gz. I0911 02:28:39.918812 140366075229952 make_examples.py:587] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2020-09-11 02:28:39.919135: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. I0911 02:28:39.922723 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.923217 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:40.247040 140366075229952 make_examples.py:587] 6 candidates (6 examples) [0.33s elapsed]. I0911 02:28:41.745102 140366075229952 make_examples.py:587] Found 78 candidate variants. I0911 02:28:41.745359 140366075229952 make_examples.py:587] Created 86 examples. real 0m5.273s. user 0m5.898s. sys 0m3.792s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --che",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:4681,usability,input,input,4681,"14417 140366075229952 make_examples.py:587] Preparing inputs. I0911 02:28:39.915202 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.916366 140366075229952 make_examples.py:587] Common contigs are ['chr20']. I0911 02:28:39.917786 140366075229952 make_examples.py:587] Writing examples to /output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz. I0911 02:28:39.917969 140366075229952 make_examples.py:587] Writing gvcf records to /output/intermediate_results_dir/gvcf.tfrecord-00000-of-00001.gz. I0911 02:28:39.918812 140366075229952 make_examples.py:587] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2020-09-11 02:28:39.919135: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. I0911 02:28:39.922723 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.923217 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:40.247040 140366075229952 make_examples.py:587] 6 candidates (6 examples) [0.33s elapsed]. I0911 02:28:41.745102 140366075229952 make_examples.py:587] Found 78 candidate variants. I0911 02:28:41.745359 140366075229952 make_examples.py:587] Created 86 examples. real 0m5.273s. user 0m5.898s. sys 0m3.792s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0911 02:28:44.682443 139937686464256 call_variants.py:335] Shape of input examples: [100, 221, 6]. 2020-09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:4811,usability,input,input,4811,"g /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.916366 140366075229952 make_examples.py:587] Common contigs are ['chr20']. I0911 02:28:39.917786 140366075229952 make_examples.py:587] Writing examples to /output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz. I0911 02:28:39.917969 140366075229952 make_examples.py:587] Writing gvcf records to /output/intermediate_results_dir/gvcf.tfrecord-00000-of-00001.gz. I0911 02:28:39.918812 140366075229952 make_examples.py:587] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2020-09-11 02:28:39.919135: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. I0911 02:28:39.922723 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.923217 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:40.247040 140366075229952 make_examples.py:587] 6 candidates (6 examples) [0.33s elapsed]. I0911 02:28:41.745102 140366075229952 make_examples.py:587] Found 78 candidate variants. I0911 02:28:41.745359 140366075229952 make_examples.py:587] Created 86 examples. real 0m5.273s. user 0m5.898s. sys 0m3.792s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0911 02:28:44.682443 139937686464256 call_variants.py:335] Shape of input examples: [100, 221, 6]. 2020-09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operat",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:5158,usability,user,user,5158,"ake_examples.py:587] Writing gvcf records to /output/intermediate_results_dir/gvcf.tfrecord-00000-of-00001.gz. I0911 02:28:39.918812 140366075229952 make_examples.py:587] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2020-09-11 02:28:39.919135: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. I0911 02:28:39.922723 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.923217 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:40.247040 140366075229952 make_examples.py:587] 6 candidates (6 examples) [0.33s elapsed]. I0911 02:28:41.745102 140366075229952 make_examples.py:587] Found 78 candidate variants. I0911 02:28:41.745359 140366075229952 make_examples.py:587] Created 86 examples. real 0m5.273s. user 0m5.898s. sys 0m3.792s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0911 02:28:44.682443 139937686464256 call_variants.py:335] Shape of input examples: [100, 221, 6]. 2020-09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-09-11 02:28:44.721599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2297525000 Hz. 2020-09-11 02:28:44.724518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5886480 executing computations on platform Host. Devices:. 2020-09-11 02:28:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:5205,usability,command,command,5205,"put/intermediate_results_dir/gvcf.tfrecord-00000-of-00001.gz. I0911 02:28:39.918812 140366075229952 make_examples.py:587] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref. 2020-09-11 02:28:39.919135: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. I0911 02:28:39.922723 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.923217 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:40.247040 140366075229952 make_examples.py:587] 6 candidates (6 examples) [0.33s elapsed]. I0911 02:28:41.745102 140366075229952 make_examples.py:587] Found 78 candidate variants. I0911 02:28:41.745359 140366075229952 make_examples.py:587] Created 86 examples. real 0m5.273s. user 0m5.898s. sys 0m3.792s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0911 02:28:44.682443 139937686464256 call_variants.py:335] Shape of input examples: [100, 221, 6]. 2020-09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-09-11 02:28:44.721599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2297525000 Hz. 2020-09-11 02:28:44.724518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5886480 executing computations on platform Host. Devices:. 2020-09-11 02:28:44.724555: I tensorflow/compiler/xla/service/serv",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:5524,usability,input,input,5524,"third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728. I0911 02:28:39.922723 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:39.923217 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:40.247040 140366075229952 make_examples.py:587] 6 candidates (6 examples) [0.33s elapsed]. I0911 02:28:41.745102 140366075229952 make_examples.py:587] Found 78 candidate variants. I0911 02:28:41.745359 140366075229952 make_examples.py:587] Created 86 examples. real 0m5.273s. user 0m5.898s. sys 0m3.792s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0911 02:28:44.682443 139937686464256 call_variants.py:335] Shape of input examples: [100, 221, 6]. 2020-09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-09-11 02:28:44.721599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2297525000 Hz. 2020-09-11 02:28:44.724518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5886480 executing computations on platform Host. Devices:. 2020-09-11 02:28:44.724555: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-09-11 02:28:44.727738: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W0911 02:28:44.755701 139937686464256 estimator.py:18",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:5735,usability,perform,performance,5735,"0911 02:28:39.923217 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0911 02:28:40.247040 140366075229952 make_examples.py:587] 6 candidates (6 examples) [0.33s elapsed]. I0911 02:28:41.745102 140366075229952 make_examples.py:587] Found 78 candidate variants. I0911 02:28:41.745359 140366075229952 make_examples.py:587] Created 86 examples. real 0m5.273s. user 0m5.898s. sys 0m3.792s. ***** Running the command:*****. time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0911 02:28:44.682443 139937686464256 call_variants.py:335] Shape of input examples: [100, 221, 6]. 2020-09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-09-11 02:28:44.721599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2297525000 Hz. 2020-09-11 02:28:44.724518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5886480 executing computations on platform Host. Devices:. 2020-09-11 02:28:44.724555: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-09-11 02:28:44.727738: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W0911 02:28:44.755701 139937686464256 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp8bwus8dp. I0911 02:28:44.756375 139937686464256 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp8bwus8dp', '_tf_random_seed': None, '_save_summary_step",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:6461,usability,perform,performance,6461,"44.682443 139937686464256 call_variants.py:335] Shape of input examples: [100, 221, 6]. 2020-09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA. To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags. 2020-09-11 02:28:44.721599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2297525000 Hz. 2020-09-11 02:28:44.724518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5886480 executing computations on platform Host. Devices:. 2020-09-11 02:28:44.724555: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version. 2020-09-11 02:28:44.727738: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. W0911 02:28:44.755701 139937686464256 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp8bwus8dp. I0911 02:28:44.756375 139937686464256 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp8bwus8dp', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f4549cfe518>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}. I0911 02:28:44.756798 13993",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:10417,usability,user,user,10417," instead. I0911 02:28:50.978360 139937686464256 estimator.py:1149] Done calling model_fn. I0911 02:28:52.395916 139937686464256 monitored_session.py:240] Graph was finalized. I0911 02:28:52.397383 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:53.423366 139937686464256 session_manager.py:500] Running local_init_op. I0911 02:28:53.495537 139937686464256 session_manager.py:502] Done running local_init_op. I0911 02:28:54.010662 139937686464256 modeling.py:415] Reloading EMA... I0911 02:28:54.011811 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:56.213706 139937686464256 call_variants.py:444] Processed 1 examples in 1 batches [1145.647 sec per 100]. I0911 02:28:56.265293 139937686464256 call_variants.py:447] Processed 86 examples in 1 batches [13.334 sec per 100]. I0911 02:28:56.265480 139937686464256 call_variants.py:450] Done calling variants from a total of 86 examples. real 0m14.761s. user 0m20.588s. sys 0m5.092s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --infile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/output/output.g.vcf.gz"". 2020-09-11 02:28:59.543794: I deepvariant/postprocess_variants.cc:88] Read from: /output/intermediate_results_dir/call_variants_output.tfrecord.gz. 2020-09-11 02:28:59.544704: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 86. I0911 02:28:59.545160 140173302474496 postprocess_variants.py:1079] CVO sorting took 2.2590160369873047e-05 minutes. I0911 02:28:59.545950 140173302474496 postprocess_variants.py:1081] Transforming call_variants_output to variants. I0911 02:28:59.546221 140173302474496 postprocess_variants.py:1108] Merging and writing variants to VCF and gVCF. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:10465,usability,command,command,10465,"timator.py:1149] Done calling model_fn. I0911 02:28:52.395916 139937686464256 monitored_session.py:240] Graph was finalized. I0911 02:28:52.397383 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:53.423366 139937686464256 session_manager.py:500] Running local_init_op. I0911 02:28:53.495537 139937686464256 session_manager.py:502] Done running local_init_op. I0911 02:28:54.010662 139937686464256 modeling.py:415] Reloading EMA... I0911 02:28:54.011811 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:56.213706 139937686464256 call_variants.py:444] Processed 1 examples in 1 batches [1145.647 sec per 100]. I0911 02:28:56.265293 139937686464256 call_variants.py:447] Processed 86 examples in 1 batches [13.334 sec per 100]. I0911 02:28:56.265480 139937686464256 call_variants.py:450] Done calling variants from a total of 86 examples. real 0m14.761s. user 0m20.588s. sys 0m5.092s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --infile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/output/output.g.vcf.gz"". 2020-09-11 02:28:59.543794: I deepvariant/postprocess_variants.cc:88] Read from: /output/intermediate_results_dir/call_variants_output.tfrecord.gz. 2020-09-11 02:28:59.544704: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 86. I0911 02:28:59.545160 140173302474496 postprocess_variants.py:1079] CVO sorting took 2.2590160369873047e-05 minutes. I0911 02:28:59.545950 140173302474496 postprocess_variants.py:1081] Transforming call_variants_output to variants. I0911 02:28:59.546221 140173302474496 postprocess_variants.py:1108] Merging and writing variants to VCF and gVCF. I0911 02:28:59.547042 140173302474496 genomics_wri",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:10535,usability,input,input,10535,"86464256 monitored_session.py:240] Graph was finalized. I0911 02:28:52.397383 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:53.423366 139937686464256 session_manager.py:500] Running local_init_op. I0911 02:28:53.495537 139937686464256 session_manager.py:502] Done running local_init_op. I0911 02:28:54.010662 139937686464256 modeling.py:415] Reloading EMA... I0911 02:28:54.011811 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt. I0911 02:28:56.213706 139937686464256 call_variants.py:444] Processed 1 examples in 1 batches [1145.647 sec per 100]. I0911 02:28:56.265293 139937686464256 call_variants.py:447] Processed 86 examples in 1 batches [13.334 sec per 100]. I0911 02:28:56.265480 139937686464256 call_variants.py:450] Done calling variants from a total of 86 examples. real 0m14.761s. user 0m20.588s. sys 0m5.092s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --infile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/output/output.g.vcf.gz"". 2020-09-11 02:28:59.543794: I deepvariant/postprocess_variants.cc:88] Read from: /output/intermediate_results_dir/call_variants_output.tfrecord.gz. 2020-09-11 02:28:59.544704: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 86. I0911 02:28:59.545160 140173302474496 postprocess_variants.py:1079] CVO sorting took 2.2590160369873047e-05 minutes. I0911 02:28:59.545950 140173302474496 postprocess_variants.py:1081] Transforming call_variants_output to variants. I0911 02:28:59.546221 140173302474496 postprocess_variants.py:1108] Merging and writing variants to VCF and gVCF. I0911 02:28:59.547042 140173302474496 genomics_writer.py:172] Writing /output/output.vcf.gz with NativeVcfWriter. I0911",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:11908,usability,user,user,11908,":56.265293 139937686464256 call_variants.py:447] Processed 86 examples in 1 batches [13.334 sec per 100]. I0911 02:28:56.265480 139937686464256 call_variants.py:450] Done calling variants from a total of 86 examples. real 0m14.761s. user 0m20.588s. sys 0m5.092s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --infile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/output/output.g.vcf.gz"". 2020-09-11 02:28:59.543794: I deepvariant/postprocess_variants.cc:88] Read from: /output/intermediate_results_dir/call_variants_output.tfrecord.gz. 2020-09-11 02:28:59.544704: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 86. I0911 02:28:59.545160 140173302474496 postprocess_variants.py:1079] CVO sorting took 2.2590160369873047e-05 minutes. I0911 02:28:59.545950 140173302474496 postprocess_variants.py:1081] Transforming call_variants_output to variants. I0911 02:28:59.546221 140173302474496 postprocess_variants.py:1108] Merging and writing variants to VCF and gVCF. I0911 02:28:59.547042 140173302474496 genomics_writer.py:172] Writing /output/output.vcf.gz with NativeVcfWriter. I0911 02:28:59.547737 140173302474496 genomics_writer.py:172] Writing /output/output.g.vcf.gz with NativeVcfWriter. I0911 02:29:00.128752 140173302474496 postprocess_variants.py:1128] Finished writing VCF and gVCF in 0.00970850388209025 minutes. I0911 02:29:00.129857 140173302474496 genomics_reader.py:223] Reading /output/output.vcf.gz with NativeVcfReader. real 0m3.961s. user 0m4.633s. sys 0m3.567s. # ls -1 ${OUTPUT_DIR}. intermediate_results_dir. output.g.vcf.gz. output.g.vcf.gz.tbi. output.vcf.gz. output.vcf.gz.tbi. output.visual_report.html. ```. I think it works and thanks for your rapidly help about this~ @pichuan @tedyun . Best,. Jerry.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/345:12135,usability,help,help,12135,":56.265293 139937686464256 call_variants.py:447] Processed 86 examples in 1 batches [13.334 sec per 100]. I0911 02:28:56.265480 139937686464256 call_variants.py:450] Done calling variants from a total of 86 examples. real 0m14.761s. user 0m20.588s. sys 0m5.092s. ***** Running the command:*****. time /opt/deepvariant/bin/postprocess_variants --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --infile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/output/output.g.vcf.gz"". 2020-09-11 02:28:59.543794: I deepvariant/postprocess_variants.cc:88] Read from: /output/intermediate_results_dir/call_variants_output.tfrecord.gz. 2020-09-11 02:28:59.544704: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 86. I0911 02:28:59.545160 140173302474496 postprocess_variants.py:1079] CVO sorting took 2.2590160369873047e-05 minutes. I0911 02:28:59.545950 140173302474496 postprocess_variants.py:1081] Transforming call_variants_output to variants. I0911 02:28:59.546221 140173302474496 postprocess_variants.py:1108] Merging and writing variants to VCF and gVCF. I0911 02:28:59.547042 140173302474496 genomics_writer.py:172] Writing /output/output.vcf.gz with NativeVcfWriter. I0911 02:28:59.547737 140173302474496 genomics_writer.py:172] Writing /output/output.g.vcf.gz with NativeVcfWriter. I0911 02:29:00.128752 140173302474496 postprocess_variants.py:1128] Finished writing VCF and gVCF in 0.00970850388209025 minutes. I0911 02:29:00.129857 140173302474496 genomics_reader.py:223] Reading /output/output.vcf.gz with NativeVcfReader. real 0m3.961s. user 0m4.633s. sys 0m3.567s. # ls -1 ${OUTPUT_DIR}. intermediate_results_dir. output.g.vcf.gz. output.g.vcf.gz.tbi. output.vcf.gz. output.vcf.gz.tbi. output.visual_report.html. ```. I think it works and thanks for your rapidly help about this~ @pichuan @tedyun . Best,. Jerry.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/345
https://github.com/google/deepvariant/issues/346:1135,availability,down,downstream,1135,"Hi Edoardo,. Thank you for filing this issue. This is an expected behavior in the current implementation of DeepVariant. As it is impractical & inefficient to represent each base position with hom. ref. calls in gVCF files, we consolidate multiple hom. ref. positions with similar GQ values into one gVCF block. A gVCF block with `MIN_DP=0` (but with positive GQ/PL values) will be created when the nearby hom. ref. positions also have very low GQ (but not all 0). Another thing that makes this problem more subtle is that we exclude lower quality from counting the DP. We think it makes sense to separate out blocks with no coverage in gVCF records and set GT to `./.` as you suggested. We started internal discussion about it and started tracking the issue internally. We will be sure to give an update here whenever we have any update. For your last question about GLnexus, I'm not aware of a GLnexus setting that can set DP zero outputs as missing. The only thing I can think of for now is adding an annotation for DP=0 calls using [bcftools](http://samtools.github.io/bcftools/bcftools.html), etc. and handling them separately in downstream analysis. I hope this helps and please let us know if you have any follow up questions or comments. I'll keep this issue open until we have an update. Thank you. Best,. Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:798,deployability,updat,update,798,"Hi Edoardo,. Thank you for filing this issue. This is an expected behavior in the current implementation of DeepVariant. As it is impractical & inefficient to represent each base position with hom. ref. calls in gVCF files, we consolidate multiple hom. ref. positions with similar GQ values into one gVCF block. A gVCF block with `MIN_DP=0` (but with positive GQ/PL values) will be created when the nearby hom. ref. positions also have very low GQ (but not all 0). Another thing that makes this problem more subtle is that we exclude lower quality from counting the DP. We think it makes sense to separate out blocks with no coverage in gVCF records and set GT to `./.` as you suggested. We started internal discussion about it and started tracking the issue internally. We will be sure to give an update here whenever we have any update. For your last question about GLnexus, I'm not aware of a GLnexus setting that can set DP zero outputs as missing. The only thing I can think of for now is adding an annotation for DP=0 calls using [bcftools](http://samtools.github.io/bcftools/bcftools.html), etc. and handling them separately in downstream analysis. I hope this helps and please let us know if you have any follow up questions or comments. I'll keep this issue open until we have an update. Thank you. Best,. Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:831,deployability,updat,update,831,"Hi Edoardo,. Thank you for filing this issue. This is an expected behavior in the current implementation of DeepVariant. As it is impractical & inefficient to represent each base position with hom. ref. calls in gVCF files, we consolidate multiple hom. ref. positions with similar GQ values into one gVCF block. A gVCF block with `MIN_DP=0` (but with positive GQ/PL values) will be created when the nearby hom. ref. positions also have very low GQ (but not all 0). Another thing that makes this problem more subtle is that we exclude lower quality from counting the DP. We think it makes sense to separate out blocks with no coverage in gVCF records and set GT to `./.` as you suggested. We started internal discussion about it and started tracking the issue internally. We will be sure to give an update here whenever we have any update. For your last question about GLnexus, I'm not aware of a GLnexus setting that can set DP zero outputs as missing. The only thing I can think of for now is adding an annotation for DP=0 calls using [bcftools](http://samtools.github.io/bcftools/bcftools.html), etc. and handling them separately in downstream analysis. I hope this helps and please let us know if you have any follow up questions or comments. I'll keep this issue open until we have an update. Thank you. Best,. Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:1289,deployability,updat,update,1289,"Hi Edoardo,. Thank you for filing this issue. This is an expected behavior in the current implementation of DeepVariant. As it is impractical & inefficient to represent each base position with hom. ref. calls in gVCF files, we consolidate multiple hom. ref. positions with similar GQ values into one gVCF block. A gVCF block with `MIN_DP=0` (but with positive GQ/PL values) will be created when the nearby hom. ref. positions also have very low GQ (but not all 0). Another thing that makes this problem more subtle is that we exclude lower quality from counting the DP. We think it makes sense to separate out blocks with no coverage in gVCF records and set GT to `./.` as you suggested. We started internal discussion about it and started tracking the issue internally. We will be sure to give an update here whenever we have any update. For your last question about GLnexus, I'm not aware of a GLnexus setting that can set DP zero outputs as missing. The only thing I can think of for now is adding an annotation for DP=0 calls using [bcftools](http://samtools.github.io/bcftools/bcftools.html), etc. and handling them separately in downstream analysis. I hope this helps and please let us know if you have any follow up questions or comments. I'll keep this issue open until we have an update. Thank you. Best,. Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:82,energy efficiency,current,current,82,"Hi Edoardo,. Thank you for filing this issue. This is an expected behavior in the current implementation of DeepVariant. As it is impractical & inefficient to represent each base position with hom. ref. calls in gVCF files, we consolidate multiple hom. ref. positions with similar GQ values into one gVCF block. A gVCF block with `MIN_DP=0` (but with positive GQ/PL values) will be created when the nearby hom. ref. positions also have very low GQ (but not all 0). Another thing that makes this problem more subtle is that we exclude lower quality from counting the DP. We think it makes sense to separate out blocks with no coverage in gVCF records and set GT to `./.` as you suggested. We started internal discussion about it and started tracking the issue internally. We will be sure to give an update here whenever we have any update. For your last question about GLnexus, I'm not aware of a GLnexus setting that can set DP zero outputs as missing. The only thing I can think of for now is adding an annotation for DP=0 calls using [bcftools](http://samtools.github.io/bcftools/bcftools.html), etc. and handling them separately in downstream analysis. I hope this helps and please let us know if you have any follow up questions or comments. I'll keep this issue open until we have an update. Thank you. Best,. Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:508,integrability,sub,subtle,508,"Hi Edoardo,. Thank you for filing this issue. This is an expected behavior in the current implementation of DeepVariant. As it is impractical & inefficient to represent each base position with hom. ref. calls in gVCF files, we consolidate multiple hom. ref. positions with similar GQ values into one gVCF block. A gVCF block with `MIN_DP=0` (but with positive GQ/PL values) will be created when the nearby hom. ref. positions also have very low GQ (but not all 0). Another thing that makes this problem more subtle is that we exclude lower quality from counting the DP. We think it makes sense to separate out blocks with no coverage in gVCF records and set GT to `./.` as you suggested. We started internal discussion about it and started tracking the issue internally. We will be sure to give an update here whenever we have any update. For your last question about GLnexus, I'm not aware of a GLnexus setting that can set DP zero outputs as missing. The only thing I can think of for now is adding an annotation for DP=0 calls using [bcftools](http://samtools.github.io/bcftools/bcftools.html), etc. and handling them separately in downstream analysis. I hope this helps and please let us know if you have any follow up questions or comments. I'll keep this issue open until we have an update. Thank you. Best,. Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:798,safety,updat,update,798,"Hi Edoardo,. Thank you for filing this issue. This is an expected behavior in the current implementation of DeepVariant. As it is impractical & inefficient to represent each base position with hom. ref. calls in gVCF files, we consolidate multiple hom. ref. positions with similar GQ values into one gVCF block. A gVCF block with `MIN_DP=0` (but with positive GQ/PL values) will be created when the nearby hom. ref. positions also have very low GQ (but not all 0). Another thing that makes this problem more subtle is that we exclude lower quality from counting the DP. We think it makes sense to separate out blocks with no coverage in gVCF records and set GT to `./.` as you suggested. We started internal discussion about it and started tracking the issue internally. We will be sure to give an update here whenever we have any update. For your last question about GLnexus, I'm not aware of a GLnexus setting that can set DP zero outputs as missing. The only thing I can think of for now is adding an annotation for DP=0 calls using [bcftools](http://samtools.github.io/bcftools/bcftools.html), etc. and handling them separately in downstream analysis. I hope this helps and please let us know if you have any follow up questions or comments. I'll keep this issue open until we have an update. Thank you. Best,. Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:831,safety,updat,update,831,"Hi Edoardo,. Thank you for filing this issue. This is an expected behavior in the current implementation of DeepVariant. As it is impractical & inefficient to represent each base position with hom. ref. calls in gVCF files, we consolidate multiple hom. ref. positions with similar GQ values into one gVCF block. A gVCF block with `MIN_DP=0` (but with positive GQ/PL values) will be created when the nearby hom. ref. positions also have very low GQ (but not all 0). Another thing that makes this problem more subtle is that we exclude lower quality from counting the DP. We think it makes sense to separate out blocks with no coverage in gVCF records and set GT to `./.` as you suggested. We started internal discussion about it and started tracking the issue internally. We will be sure to give an update here whenever we have any update. For your last question about GLnexus, I'm not aware of a GLnexus setting that can set DP zero outputs as missing. The only thing I can think of for now is adding an annotation for DP=0 calls using [bcftools](http://samtools.github.io/bcftools/bcftools.html), etc. and handling them separately in downstream analysis. I hope this helps and please let us know if you have any follow up questions or comments. I'll keep this issue open until we have an update. Thank you. Best,. Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:1289,safety,updat,update,1289,"Hi Edoardo,. Thank you for filing this issue. This is an expected behavior in the current implementation of DeepVariant. As it is impractical & inefficient to represent each base position with hom. ref. calls in gVCF files, we consolidate multiple hom. ref. positions with similar GQ values into one gVCF block. A gVCF block with `MIN_DP=0` (but with positive GQ/PL values) will be created when the nearby hom. ref. positions also have very low GQ (but not all 0). Another thing that makes this problem more subtle is that we exclude lower quality from counting the DP. We think it makes sense to separate out blocks with no coverage in gVCF records and set GT to `./.` as you suggested. We started internal discussion about it and started tracking the issue internally. We will be sure to give an update here whenever we have any update. For your last question about GLnexus, I'm not aware of a GLnexus setting that can set DP zero outputs as missing. The only thing I can think of for now is adding an annotation for DP=0 calls using [bcftools](http://samtools.github.io/bcftools/bcftools.html), etc. and handling them separately in downstream analysis. I hope this helps and please let us know if you have any follow up questions or comments. I'll keep this issue open until we have an update. Thank you. Best,. Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:798,security,updat,update,798,"Hi Edoardo,. Thank you for filing this issue. This is an expected behavior in the current implementation of DeepVariant. As it is impractical & inefficient to represent each base position with hom. ref. calls in gVCF files, we consolidate multiple hom. ref. positions with similar GQ values into one gVCF block. A gVCF block with `MIN_DP=0` (but with positive GQ/PL values) will be created when the nearby hom. ref. positions also have very low GQ (but not all 0). Another thing that makes this problem more subtle is that we exclude lower quality from counting the DP. We think it makes sense to separate out blocks with no coverage in gVCF records and set GT to `./.` as you suggested. We started internal discussion about it and started tracking the issue internally. We will be sure to give an update here whenever we have any update. For your last question about GLnexus, I'm not aware of a GLnexus setting that can set DP zero outputs as missing. The only thing I can think of for now is adding an annotation for DP=0 calls using [bcftools](http://samtools.github.io/bcftools/bcftools.html), etc. and handling them separately in downstream analysis. I hope this helps and please let us know if you have any follow up questions or comments. I'll keep this issue open until we have an update. Thank you. Best,. Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:831,security,updat,update,831,"Hi Edoardo,. Thank you for filing this issue. This is an expected behavior in the current implementation of DeepVariant. As it is impractical & inefficient to represent each base position with hom. ref. calls in gVCF files, we consolidate multiple hom. ref. positions with similar GQ values into one gVCF block. A gVCF block with `MIN_DP=0` (but with positive GQ/PL values) will be created when the nearby hom. ref. positions also have very low GQ (but not all 0). Another thing that makes this problem more subtle is that we exclude lower quality from counting the DP. We think it makes sense to separate out blocks with no coverage in gVCF records and set GT to `./.` as you suggested. We started internal discussion about it and started tracking the issue internally. We will be sure to give an update here whenever we have any update. For your last question about GLnexus, I'm not aware of a GLnexus setting that can set DP zero outputs as missing. The only thing I can think of for now is adding an annotation for DP=0 calls using [bcftools](http://samtools.github.io/bcftools/bcftools.html), etc. and handling them separately in downstream analysis. I hope this helps and please let us know if you have any follow up questions or comments. I'll keep this issue open until we have an update. Thank you. Best,. Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:1289,security,updat,update,1289,"Hi Edoardo,. Thank you for filing this issue. This is an expected behavior in the current implementation of DeepVariant. As it is impractical & inefficient to represent each base position with hom. ref. calls in gVCF files, we consolidate multiple hom. ref. positions with similar GQ values into one gVCF block. A gVCF block with `MIN_DP=0` (but with positive GQ/PL values) will be created when the nearby hom. ref. positions also have very low GQ (but not all 0). Another thing that makes this problem more subtle is that we exclude lower quality from counting the DP. We think it makes sense to separate out blocks with no coverage in gVCF records and set GT to `./.` as you suggested. We started internal discussion about it and started tracking the issue internally. We will be sure to give an update here whenever we have any update. For your last question about GLnexus, I'm not aware of a GLnexus setting that can set DP zero outputs as missing. The only thing I can think of for now is adding an annotation for DP=0 calls using [bcftools](http://samtools.github.io/bcftools/bcftools.html), etc. and handling them separately in downstream analysis. I hope this helps and please let us know if you have any follow up questions or comments. I'll keep this issue open until we have an update. Thank you. Best,. Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:625,testability,coverag,coverage,625,"Hi Edoardo,. Thank you for filing this issue. This is an expected behavior in the current implementation of DeepVariant. As it is impractical & inefficient to represent each base position with hom. ref. calls in gVCF files, we consolidate multiple hom. ref. positions with similar GQ values into one gVCF block. A gVCF block with `MIN_DP=0` (but with positive GQ/PL values) will be created when the nearby hom. ref. positions also have very low GQ (but not all 0). Another thing that makes this problem more subtle is that we exclude lower quality from counting the DP. We think it makes sense to separate out blocks with no coverage in gVCF records and set GT to `./.` as you suggested. We started internal discussion about it and started tracking the issue internally. We will be sure to give an update here whenever we have any update. For your last question about GLnexus, I'm not aware of a GLnexus setting that can set DP zero outputs as missing. The only thing I can think of for now is adding an annotation for DP=0 calls using [bcftools](http://samtools.github.io/bcftools/bcftools.html), etc. and handling them separately in downstream analysis. I hope this helps and please let us know if you have any follow up questions or comments. I'll keep this issue open until we have an update. Thank you. Best,. Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:66,usability,behavi,behavior,66,"Hi Edoardo,. Thank you for filing this issue. This is an expected behavior in the current implementation of DeepVariant. As it is impractical & inefficient to represent each base position with hom. ref. calls in gVCF files, we consolidate multiple hom. ref. positions with similar GQ values into one gVCF block. A gVCF block with `MIN_DP=0` (but with positive GQ/PL values) will be created when the nearby hom. ref. positions also have very low GQ (but not all 0). Another thing that makes this problem more subtle is that we exclude lower quality from counting the DP. We think it makes sense to separate out blocks with no coverage in gVCF records and set GT to `./.` as you suggested. We started internal discussion about it and started tracking the issue internally. We will be sure to give an update here whenever we have any update. For your last question about GLnexus, I'm not aware of a GLnexus setting that can set DP zero outputs as missing. The only thing I can think of for now is adding an annotation for DP=0 calls using [bcftools](http://samtools.github.io/bcftools/bcftools.html), etc. and handling them separately in downstream analysis. I hope this helps and please let us know if you have any follow up questions or comments. I'll keep this issue open until we have an update. Thank you. Best,. Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:1168,usability,help,helps,1168,"Hi Edoardo,. Thank you for filing this issue. This is an expected behavior in the current implementation of DeepVariant. As it is impractical & inefficient to represent each base position with hom. ref. calls in gVCF files, we consolidate multiple hom. ref. positions with similar GQ values into one gVCF block. A gVCF block with `MIN_DP=0` (but with positive GQ/PL values) will be created when the nearby hom. ref. positions also have very low GQ (but not all 0). Another thing that makes this problem more subtle is that we exclude lower quality from counting the DP. We think it makes sense to separate out blocks with no coverage in gVCF records and set GT to `./.` as you suggested. We started internal discussion about it and started tracking the issue internally. We will be sure to give an update here whenever we have any update. For your last question about GLnexus, I'm not aware of a GLnexus setting that can set DP zero outputs as missing. The only thing I can think of for now is adding an annotation for DP=0 calls using [bcftools](http://samtools.github.io/bcftools/bcftools.html), etc. and handling them separately in downstream analysis. I hope this helps and please let us know if you have any follow up questions or comments. I'll keep this issue open until we have an update. Thank you. Best,. Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:317,availability,slo,slower,317,"Hi Ted,. Thanks for your feedback. Just to be sure I've got this right. You confirm that variants with 0/0 genotype and zero DP in the merged VCF are actually positions with no reads in that sample and so can be set to missing? I have also an additional comment about deepvariant v1.0.0. I've noticed that it is much slower than v.0.9.0. I've used them both on the sample samples (30-60X WGS) using singularity and v0.9.0 takes 10-14h while v1.0.0 takes more than 24h per sample. Is this expected? Many thanks! Edoardo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:317,reliability,slo,slower,317,"Hi Ted,. Thanks for your feedback. Just to be sure I've got this right. You confirm that variants with 0/0 genotype and zero DP in the merged VCF are actually positions with no reads in that sample and so can be set to missing? I have also an additional comment about deepvariant v1.0.0. I've noticed that it is much slower than v.0.9.0. I've used them both on the sample samples (30-60X WGS) using singularity and v0.9.0 takes 10-14h while v1.0.0 takes more than 24h per sample. Is this expected? Many thanks! Edoardo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:25,usability,feedback,feedback,25,"Hi Ted,. Thanks for your feedback. Just to be sure I've got this right. You confirm that variants with 0/0 genotype and zero DP in the merged VCF are actually positions with no reads in that sample and so can be set to missing? I have also an additional comment about deepvariant v1.0.0. I've noticed that it is much slower than v.0.9.0. I've used them both on the sample samples (30-60X WGS) using singularity and v0.9.0 takes 10-14h while v1.0.0 takes more than 24h per sample. Is this expected? Many thanks! Edoardo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:76,usability,confirm,confirm,76,"Hi Ted,. Thanks for your feedback. Just to be sure I've got this right. You confirm that variants with 0/0 genotype and zero DP in the merged VCF are actually positions with no reads in that sample and so can be set to missing? I have also an additional comment about deepvariant v1.0.0. I've noticed that it is much slower than v.0.9.0. I've used them both on the sample samples (30-60X WGS) using singularity and v0.9.0 takes 10-14h while v1.0.0 takes more than 24h per sample. Is this expected? Many thanks! Edoardo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:351,availability,state,stated,351,"Hi Edoardo, in the current implementation, `DP=0` (which is used in variant records) will imply that there is no read covering that position, while `MIN_DP=0` (used in hom. ref. band in gVCF) does not necessarily imply that there is no read covering the reference band, due to consolidation of multiple hom.ref. records into one record for the reason stated in my previous response (as the name suggests, it means the *minimum* of DPs in all of the consolidated records were 0). Please also note that reads that don't satisfy the minimum mapping quality threshold (set to 5 by default [here](https://github.com/google/deepvariant/blob/r1.0/deepvariant/make_examples.py#L219)), don't count for DP/MIN_DP as far as I know. Again, we agree with your suggestion about separating out the `DP=0` regions into a separate record and setting the genotype to `./.`. The performance issue you mentioned is not expected as far as I know, and I asked my colleague to follow up on that aspect. I hope this help and please let me know if I can help with anything else. Best,. Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:19,energy efficiency,current,current,19,"Hi Edoardo, in the current implementation, `DP=0` (which is used in variant records) will imply that there is no read covering that position, while `MIN_DP=0` (used in hom. ref. band in gVCF) does not necessarily imply that there is no read covering the reference band, due to consolidation of multiple hom.ref. records into one record for the reason stated in my previous response (as the name suggests, it means the *minimum* of DPs in all of the consolidated records were 0). Please also note that reads that don't satisfy the minimum mapping quality threshold (set to 5 by default [here](https://github.com/google/deepvariant/blob/r1.0/deepvariant/make_examples.py#L219)), don't count for DP/MIN_DP as far as I know. Again, we agree with your suggestion about separating out the `DP=0` regions into a separate record and setting the genotype to `./.`. The performance issue you mentioned is not expected as far as I know, and I asked my colleague to follow up on that aspect. I hope this help and please let me know if I can help with anything else. Best,. Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:351,integrability,state,stated,351,"Hi Edoardo, in the current implementation, `DP=0` (which is used in variant records) will imply that there is no read covering that position, while `MIN_DP=0` (used in hom. ref. band in gVCF) does not necessarily imply that there is no read covering the reference band, due to consolidation of multiple hom.ref. records into one record for the reason stated in my previous response (as the name suggests, it means the *minimum* of DPs in all of the consolidated records were 0). Please also note that reads that don't satisfy the minimum mapping quality threshold (set to 5 by default [here](https://github.com/google/deepvariant/blob/r1.0/deepvariant/make_examples.py#L219)), don't count for DP/MIN_DP as far as I know. Again, we agree with your suggestion about separating out the `DP=0` regions into a separate record and setting the genotype to `./.`. The performance issue you mentioned is not expected as far as I know, and I asked my colleague to follow up on that aspect. I hope this help and please let me know if I can help with anything else. Best,. Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:860,performance,performance issu,performance issue,860,"Hi Edoardo, in the current implementation, `DP=0` (which is used in variant records) will imply that there is no read covering that position, while `MIN_DP=0` (used in hom. ref. band in gVCF) does not necessarily imply that there is no read covering the reference band, due to consolidation of multiple hom.ref. records into one record for the reason stated in my previous response (as the name suggests, it means the *minimum* of DPs in all of the consolidated records were 0). Please also note that reads that don't satisfy the minimum mapping quality threshold (set to 5 by default [here](https://github.com/google/deepvariant/blob/r1.0/deepvariant/make_examples.py#L219)), don't count for DP/MIN_DP as far as I know. Again, we agree with your suggestion about separating out the `DP=0` regions into a separate record and setting the genotype to `./.`. The performance issue you mentioned is not expected as far as I know, and I asked my colleague to follow up on that aspect. I hope this help and please let me know if I can help with anything else. Best,. Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:192,reliability,doe,does,192,"Hi Edoardo, in the current implementation, `DP=0` (which is used in variant records) will imply that there is no read covering that position, while `MIN_DP=0` (used in hom. ref. band in gVCF) does not necessarily imply that there is no read covering the reference band, due to consolidation of multiple hom.ref. records into one record for the reason stated in my previous response (as the name suggests, it means the *minimum* of DPs in all of the consolidated records were 0). Please also note that reads that don't satisfy the minimum mapping quality threshold (set to 5 by default [here](https://github.com/google/deepvariant/blob/r1.0/deepvariant/make_examples.py#L219)), don't count for DP/MIN_DP as far as I know. Again, we agree with your suggestion about separating out the `DP=0` regions into a separate record and setting the genotype to `./.`. The performance issue you mentioned is not expected as far as I know, and I asked my colleague to follow up on that aspect. I hope this help and please let me know if I can help with anything else. Best,. Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:419,usability,minim,minimum,419,"Hi Edoardo, in the current implementation, `DP=0` (which is used in variant records) will imply that there is no read covering that position, while `MIN_DP=0` (used in hom. ref. band in gVCF) does not necessarily imply that there is no read covering the reference band, due to consolidation of multiple hom.ref. records into one record for the reason stated in my previous response (as the name suggests, it means the *minimum* of DPs in all of the consolidated records were 0). Please also note that reads that don't satisfy the minimum mapping quality threshold (set to 5 by default [here](https://github.com/google/deepvariant/blob/r1.0/deepvariant/make_examples.py#L219)), don't count for DP/MIN_DP as far as I know. Again, we agree with your suggestion about separating out the `DP=0` regions into a separate record and setting the genotype to `./.`. The performance issue you mentioned is not expected as far as I know, and I asked my colleague to follow up on that aspect. I hope this help and please let me know if I can help with anything else. Best,. Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:530,usability,minim,minimum,530,"Hi Edoardo, in the current implementation, `DP=0` (which is used in variant records) will imply that there is no read covering that position, while `MIN_DP=0` (used in hom. ref. band in gVCF) does not necessarily imply that there is no read covering the reference band, due to consolidation of multiple hom.ref. records into one record for the reason stated in my previous response (as the name suggests, it means the *minimum* of DPs in all of the consolidated records were 0). Please also note that reads that don't satisfy the minimum mapping quality threshold (set to 5 by default [here](https://github.com/google/deepvariant/blob/r1.0/deepvariant/make_examples.py#L219)), don't count for DP/MIN_DP as far as I know. Again, we agree with your suggestion about separating out the `DP=0` regions into a separate record and setting the genotype to `./.`. The performance issue you mentioned is not expected as far as I know, and I asked my colleague to follow up on that aspect. I hope this help and please let me know if I can help with anything else. Best,. Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:860,usability,perform,performance,860,"Hi Edoardo, in the current implementation, `DP=0` (which is used in variant records) will imply that there is no read covering that position, while `MIN_DP=0` (used in hom. ref. band in gVCF) does not necessarily imply that there is no read covering the reference band, due to consolidation of multiple hom.ref. records into one record for the reason stated in my previous response (as the name suggests, it means the *minimum* of DPs in all of the consolidated records were 0). Please also note that reads that don't satisfy the minimum mapping quality threshold (set to 5 by default [here](https://github.com/google/deepvariant/blob/r1.0/deepvariant/make_examples.py#L219)), don't count for DP/MIN_DP as far as I know. Again, we agree with your suggestion about separating out the `DP=0` regions into a separate record and setting the genotype to `./.`. The performance issue you mentioned is not expected as far as I know, and I asked my colleague to follow up on that aspect. I hope this help and please let me know if I can help with anything else. Best,. Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:992,usability,help,help,992,"Hi Edoardo, in the current implementation, `DP=0` (which is used in variant records) will imply that there is no read covering that position, while `MIN_DP=0` (used in hom. ref. band in gVCF) does not necessarily imply that there is no read covering the reference band, due to consolidation of multiple hom.ref. records into one record for the reason stated in my previous response (as the name suggests, it means the *minimum* of DPs in all of the consolidated records were 0). Please also note that reads that don't satisfy the minimum mapping quality threshold (set to 5 by default [here](https://github.com/google/deepvariant/blob/r1.0/deepvariant/make_examples.py#L219)), don't count for DP/MIN_DP as far as I know. Again, we agree with your suggestion about separating out the `DP=0` regions into a separate record and setting the genotype to `./.`. The performance issue you mentioned is not expected as far as I know, and I asked my colleague to follow up on that aspect. I hope this help and please let me know if I can help with anything else. Best,. Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:1029,usability,help,help,1029,"Hi Edoardo, in the current implementation, `DP=0` (which is used in variant records) will imply that there is no read covering that position, while `MIN_DP=0` (used in hom. ref. band in gVCF) does not necessarily imply that there is no read covering the reference band, due to consolidation of multiple hom.ref. records into one record for the reason stated in my previous response (as the name suggests, it means the *minimum* of DPs in all of the consolidated records were 0). Please also note that reads that don't satisfy the minimum mapping quality threshold (set to 5 by default [here](https://github.com/google/deepvariant/blob/r1.0/deepvariant/make_examples.py#L219)), don't count for DP/MIN_DP as far as I know. Again, we agree with your suggestion about separating out the `DP=0` regions into a separate record and setting the genotype to `./.`. The performance issue you mentioned is not expected as far as I know, and I asked my colleague to follow up on that aspect. I hope this help and please let me know if I can help with anything else. Best,. Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:205,deployability,observ,observing,205,"Hi @edg1983 , I'll help with the runtime increase question. There are a few reasons I can think of that might have increased overall runtime between v0.9.0 and v1.0.0, but the magnitude of increase you're observing is not quite what I'd expected. First, let's take a quick comparison on our GitHub documentation:. - https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-case-study.md : 82m+189m+36m = 307m. - https://github.com/google/deepvariant/blob/r1.0/docs/metrics.md : 89m+258m+84m = 431m. Before we talk about code changes, note that r0.9 was done on a 50x BAM ([see script](https://raw.githubusercontent.com/google/deepvariant/r0.9/scripts/run_wgs_case_study_docker.sh)), and in r1.0 we used a 35x BAM ([see script](https://raw.githubusercontent.com/google/deepvariant/r1.0/scripts/run_wgs_case_study_docker.sh)). The lower coverage BAM is one main reason why the `postprocess_variants` took more time (84m vs 36m). There are a few known reasons why runtime would increase. 1. In [v1.0.0 release notes](https://github.com/google/deepvariant/releases/tag/v1.0.0), we mentioned that `--min_mapping_quality` is now set to 5 (previously 10). This would increase the number of reads being considered, as a result, increase the number of candidates generated. Which is why `make_examples` runtime might increase. And if there are more candidates, `call_variants` runtime can also increase. 2. In [v0.10.0 release notes](https://github.com/google/deepvariant/releases/tag/v0.10.0), we mentioned that we have turned off `--ws_use_window_selector_model` to improve accuracy but increased runtime too. . All these being said, I am surprised by your observation ""v0.9.0 takes 10-14h while v1.0.0 takes more than 24h per sample"". If you can provide the following information, it'll be very helpful for us:. 1. What is the runtime breakdown of the 3 steps on v0.9.0 and v1.0.0? This will help us understand which step (make_examples, call_variants, postprocess_variants) might be increasing the",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:1007,deployability,releas,release,1007," , I'll help with the runtime increase question. There are a few reasons I can think of that might have increased overall runtime between v0.9.0 and v1.0.0, but the magnitude of increase you're observing is not quite what I'd expected. First, let's take a quick comparison on our GitHub documentation:. - https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-case-study.md : 82m+189m+36m = 307m. - https://github.com/google/deepvariant/blob/r1.0/docs/metrics.md : 89m+258m+84m = 431m. Before we talk about code changes, note that r0.9 was done on a 50x BAM ([see script](https://raw.githubusercontent.com/google/deepvariant/r0.9/scripts/run_wgs_case_study_docker.sh)), and in r1.0 we used a 35x BAM ([see script](https://raw.githubusercontent.com/google/deepvariant/r1.0/scripts/run_wgs_case_study_docker.sh)). The lower coverage BAM is one main reason why the `postprocess_variants` took more time (84m vs 36m). There are a few known reasons why runtime would increase. 1. In [v1.0.0 release notes](https://github.com/google/deepvariant/releases/tag/v1.0.0), we mentioned that `--min_mapping_quality` is now set to 5 (previously 10). This would increase the number of reads being considered, as a result, increase the number of candidates generated. Which is why `make_examples` runtime might increase. And if there are more candidates, `call_variants` runtime can also increase. 2. In [v0.10.0 release notes](https://github.com/google/deepvariant/releases/tag/v0.10.0), we mentioned that we have turned off `--ws_use_window_selector_model` to improve accuracy but increased runtime too. . All these being said, I am surprised by your observation ""v0.9.0 takes 10-14h while v1.0.0 takes more than 24h per sample"". If you can provide the following information, it'll be very helpful for us:. 1. What is the runtime breakdown of the 3 steps on v0.9.0 and v1.0.0? This will help us understand which step (make_examples, call_variants, postprocess_variants) might be increasing the most. 2. O",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:1060,deployability,releas,releases,1060,"e are a few reasons I can think of that might have increased overall runtime between v0.9.0 and v1.0.0, but the magnitude of increase you're observing is not quite what I'd expected. First, let's take a quick comparison on our GitHub documentation:. - https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-case-study.md : 82m+189m+36m = 307m. - https://github.com/google/deepvariant/blob/r1.0/docs/metrics.md : 89m+258m+84m = 431m. Before we talk about code changes, note that r0.9 was done on a 50x BAM ([see script](https://raw.githubusercontent.com/google/deepvariant/r0.9/scripts/run_wgs_case_study_docker.sh)), and in r1.0 we used a 35x BAM ([see script](https://raw.githubusercontent.com/google/deepvariant/r1.0/scripts/run_wgs_case_study_docker.sh)). The lower coverage BAM is one main reason why the `postprocess_variants` took more time (84m vs 36m). There are a few known reasons why runtime would increase. 1. In [v1.0.0 release notes](https://github.com/google/deepvariant/releases/tag/v1.0.0), we mentioned that `--min_mapping_quality` is now set to 5 (previously 10). This would increase the number of reads being considered, as a result, increase the number of candidates generated. Which is why `make_examples` runtime might increase. And if there are more candidates, `call_variants` runtime can also increase. 2. In [v0.10.0 release notes](https://github.com/google/deepvariant/releases/tag/v0.10.0), we mentioned that we have turned off `--ws_use_window_selector_model` to improve accuracy but increased runtime too. . All these being said, I am surprised by your observation ""v0.9.0 takes 10-14h while v1.0.0 takes more than 24h per sample"". If you can provide the following information, it'll be very helpful for us:. 1. What is the runtime breakdown of the 3 steps on v0.9.0 and v1.0.0? This will help us understand which step (make_examples, call_variants, postprocess_variants) might be increasing the most. 2. On exactly the same BAM, just with different versions,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:1418,deployability,releas,release,1418,"tps://github.com/google/deepvariant/blob/r1.0/docs/metrics.md : 89m+258m+84m = 431m. Before we talk about code changes, note that r0.9 was done on a 50x BAM ([see script](https://raw.githubusercontent.com/google/deepvariant/r0.9/scripts/run_wgs_case_study_docker.sh)), and in r1.0 we used a 35x BAM ([see script](https://raw.githubusercontent.com/google/deepvariant/r1.0/scripts/run_wgs_case_study_docker.sh)). The lower coverage BAM is one main reason why the `postprocess_variants` took more time (84m vs 36m). There are a few known reasons why runtime would increase. 1. In [v1.0.0 release notes](https://github.com/google/deepvariant/releases/tag/v1.0.0), we mentioned that `--min_mapping_quality` is now set to 5 (previously 10). This would increase the number of reads being considered, as a result, increase the number of candidates generated. Which is why `make_examples` runtime might increase. And if there are more candidates, `call_variants` runtime can also increase. 2. In [v0.10.0 release notes](https://github.com/google/deepvariant/releases/tag/v0.10.0), we mentioned that we have turned off `--ws_use_window_selector_model` to improve accuracy but increased runtime too. . All these being said, I am surprised by your observation ""v0.9.0 takes 10-14h while v1.0.0 takes more than 24h per sample"". If you can provide the following information, it'll be very helpful for us:. 1. What is the runtime breakdown of the 3 steps on v0.9.0 and v1.0.0? This will help us understand which step (make_examples, call_variants, postprocess_variants) might be increasing the most. 2. On exactly the same BAM, just with different versions, how many lines are in your VCF and GVCF? For example, here is what I have after running [the wgs script](https://raw.githubusercontent.com/google/deepvariant/r1.0/scripts/run_wgs_case_study_docker.sh). ```. $ zcat HG002.output.vcf.gz | grep -v '^#' | wc -l. 7753721. $ zcat HG002.output.g.vcf.gz | grep -v '^#' | wc -l. 91714977. ```. And here is the breakd",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:1471,deployability,releas,releases,1471,"trics.md : 89m+258m+84m = 431m. Before we talk about code changes, note that r0.9 was done on a 50x BAM ([see script](https://raw.githubusercontent.com/google/deepvariant/r0.9/scripts/run_wgs_case_study_docker.sh)), and in r1.0 we used a 35x BAM ([see script](https://raw.githubusercontent.com/google/deepvariant/r1.0/scripts/run_wgs_case_study_docker.sh)). The lower coverage BAM is one main reason why the `postprocess_variants` took more time (84m vs 36m). There are a few known reasons why runtime would increase. 1. In [v1.0.0 release notes](https://github.com/google/deepvariant/releases/tag/v1.0.0), we mentioned that `--min_mapping_quality` is now set to 5 (previously 10). This would increase the number of reads being considered, as a result, increase the number of candidates generated. Which is why `make_examples` runtime might increase. And if there are more candidates, `call_variants` runtime can also increase. 2. In [v0.10.0 release notes](https://github.com/google/deepvariant/releases/tag/v0.10.0), we mentioned that we have turned off `--ws_use_window_selector_model` to improve accuracy but increased runtime too. . All these being said, I am surprised by your observation ""v0.9.0 takes 10-14h while v1.0.0 takes more than 24h per sample"". If you can provide the following information, it'll be very helpful for us:. 1. What is the runtime breakdown of the 3 steps on v0.9.0 and v1.0.0? This will help us understand which step (make_examples, call_variants, postprocess_variants) might be increasing the most. 2. On exactly the same BAM, just with different versions, how many lines are in your VCF and GVCF? For example, here is what I have after running [the wgs script](https://raw.githubusercontent.com/google/deepvariant/r1.0/scripts/run_wgs_case_study_docker.sh). ```. $ zcat HG002.output.vcf.gz | grep -v '^#' | wc -l. 7753721. $ zcat HG002.output.g.vcf.gz | grep -v '^#' | wc -l. 91714977. ```. And here is the breakdown of PASS/RefCall:. ```. $ zcat HG002.output.vcf.gz",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:1658,deployability,observ,observation,1658,"9 was done on a 50x BAM ([see script](https://raw.githubusercontent.com/google/deepvariant/r0.9/scripts/run_wgs_case_study_docker.sh)), and in r1.0 we used a 35x BAM ([see script](https://raw.githubusercontent.com/google/deepvariant/r1.0/scripts/run_wgs_case_study_docker.sh)). The lower coverage BAM is one main reason why the `postprocess_variants` took more time (84m vs 36m). There are a few known reasons why runtime would increase. 1. In [v1.0.0 release notes](https://github.com/google/deepvariant/releases/tag/v1.0.0), we mentioned that `--min_mapping_quality` is now set to 5 (previously 10). This would increase the number of reads being considered, as a result, increase the number of candidates generated. Which is why `make_examples` runtime might increase. And if there are more candidates, `call_variants` runtime can also increase. 2. In [v0.10.0 release notes](https://github.com/google/deepvariant/releases/tag/v0.10.0), we mentioned that we have turned off `--ws_use_window_selector_model` to improve accuracy but increased runtime too. . All these being said, I am surprised by your observation ""v0.9.0 takes 10-14h while v1.0.0 takes more than 24h per sample"". If you can provide the following information, it'll be very helpful for us:. 1. What is the runtime breakdown of the 3 steps on v0.9.0 and v1.0.0? This will help us understand which step (make_examples, call_variants, postprocess_variants) might be increasing the most. 2. On exactly the same BAM, just with different versions, how many lines are in your VCF and GVCF? For example, here is what I have after running [the wgs script](https://raw.githubusercontent.com/google/deepvariant/r1.0/scripts/run_wgs_case_study_docker.sh). ```. $ zcat HG002.output.vcf.gz | grep -v '^#' | wc -l. 7753721. $ zcat HG002.output.g.vcf.gz | grep -v '^#' | wc -l. 91714977. ```. And here is the breakdown of PASS/RefCall:. ```. $ zcat HG002.output.vcf.gz | grep -v '^#' | cut -f 7 | sort | uniq -c. 4845593 PASS. 2908128 RefCall. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:2055,deployability,version,versions,2055,"9 was done on a 50x BAM ([see script](https://raw.githubusercontent.com/google/deepvariant/r0.9/scripts/run_wgs_case_study_docker.sh)), and in r1.0 we used a 35x BAM ([see script](https://raw.githubusercontent.com/google/deepvariant/r1.0/scripts/run_wgs_case_study_docker.sh)). The lower coverage BAM is one main reason why the `postprocess_variants` took more time (84m vs 36m). There are a few known reasons why runtime would increase. 1. In [v1.0.0 release notes](https://github.com/google/deepvariant/releases/tag/v1.0.0), we mentioned that `--min_mapping_quality` is now set to 5 (previously 10). This would increase the number of reads being considered, as a result, increase the number of candidates generated. Which is why `make_examples` runtime might increase. And if there are more candidates, `call_variants` runtime can also increase. 2. In [v0.10.0 release notes](https://github.com/google/deepvariant/releases/tag/v0.10.0), we mentioned that we have turned off `--ws_use_window_selector_model` to improve accuracy but increased runtime too. . All these being said, I am surprised by your observation ""v0.9.0 takes 10-14h while v1.0.0 takes more than 24h per sample"". If you can provide the following information, it'll be very helpful for us:. 1. What is the runtime breakdown of the 3 steps on v0.9.0 and v1.0.0? This will help us understand which step (make_examples, call_variants, postprocess_variants) might be increasing the most. 2. On exactly the same BAM, just with different versions, how many lines are in your VCF and GVCF? For example, here is what I have after running [the wgs script](https://raw.githubusercontent.com/google/deepvariant/r1.0/scripts/run_wgs_case_study_docker.sh). ```. $ zcat HG002.output.vcf.gz | grep -v '^#' | wc -l. 7753721. $ zcat HG002.output.g.vcf.gz | grep -v '^#' | wc -l. 91714977. ```. And here is the breakdown of PASS/RefCall:. ```. $ zcat HG002.output.vcf.gz | grep -v '^#' | cut -f 7 | sort | uniq -c. 4845593 PASS. 2908128 RefCall. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:2055,integrability,version,versions,2055,"9 was done on a 50x BAM ([see script](https://raw.githubusercontent.com/google/deepvariant/r0.9/scripts/run_wgs_case_study_docker.sh)), and in r1.0 we used a 35x BAM ([see script](https://raw.githubusercontent.com/google/deepvariant/r1.0/scripts/run_wgs_case_study_docker.sh)). The lower coverage BAM is one main reason why the `postprocess_variants` took more time (84m vs 36m). There are a few known reasons why runtime would increase. 1. In [v1.0.0 release notes](https://github.com/google/deepvariant/releases/tag/v1.0.0), we mentioned that `--min_mapping_quality` is now set to 5 (previously 10). This would increase the number of reads being considered, as a result, increase the number of candidates generated. Which is why `make_examples` runtime might increase. And if there are more candidates, `call_variants` runtime can also increase. 2. In [v0.10.0 release notes](https://github.com/google/deepvariant/releases/tag/v0.10.0), we mentioned that we have turned off `--ws_use_window_selector_model` to improve accuracy but increased runtime too. . All these being said, I am surprised by your observation ""v0.9.0 takes 10-14h while v1.0.0 takes more than 24h per sample"". If you can provide the following information, it'll be very helpful for us:. 1. What is the runtime breakdown of the 3 steps on v0.9.0 and v1.0.0? This will help us understand which step (make_examples, call_variants, postprocess_variants) might be increasing the most. 2. On exactly the same BAM, just with different versions, how many lines are in your VCF and GVCF? For example, here is what I have after running [the wgs script](https://raw.githubusercontent.com/google/deepvariant/r1.0/scripts/run_wgs_case_study_docker.sh). ```. $ zcat HG002.output.vcf.gz | grep -v '^#' | wc -l. 7753721. $ zcat HG002.output.g.vcf.gz | grep -v '^#' | wc -l. 91714977. ```. And here is the breakdown of PASS/RefCall:. ```. $ zcat HG002.output.vcf.gz | grep -v '^#' | cut -f 7 | sort | uniq -c. 4845593 PASS. 2908128 RefCall. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:2055,modifiability,version,versions,2055,"9 was done on a 50x BAM ([see script](https://raw.githubusercontent.com/google/deepvariant/r0.9/scripts/run_wgs_case_study_docker.sh)), and in r1.0 we used a 35x BAM ([see script](https://raw.githubusercontent.com/google/deepvariant/r1.0/scripts/run_wgs_case_study_docker.sh)). The lower coverage BAM is one main reason why the `postprocess_variants` took more time (84m vs 36m). There are a few known reasons why runtime would increase. 1. In [v1.0.0 release notes](https://github.com/google/deepvariant/releases/tag/v1.0.0), we mentioned that `--min_mapping_quality` is now set to 5 (previously 10). This would increase the number of reads being considered, as a result, increase the number of candidates generated. Which is why `make_examples` runtime might increase. And if there are more candidates, `call_variants` runtime can also increase. 2. In [v0.10.0 release notes](https://github.com/google/deepvariant/releases/tag/v0.10.0), we mentioned that we have turned off `--ws_use_window_selector_model` to improve accuracy but increased runtime too. . All these being said, I am surprised by your observation ""v0.9.0 takes 10-14h while v1.0.0 takes more than 24h per sample"". If you can provide the following information, it'll be very helpful for us:. 1. What is the runtime breakdown of the 3 steps on v0.9.0 and v1.0.0? This will help us understand which step (make_examples, call_variants, postprocess_variants) might be increasing the most. 2. On exactly the same BAM, just with different versions, how many lines are in your VCF and GVCF? For example, here is what I have after running [the wgs script](https://raw.githubusercontent.com/google/deepvariant/r1.0/scripts/run_wgs_case_study_docker.sh). ```. $ zcat HG002.output.vcf.gz | grep -v '^#' | wc -l. 7753721. $ zcat HG002.output.g.vcf.gz | grep -v '^#' | wc -l. 91714977. ```. And here is the breakdown of PASS/RefCall:. ```. $ zcat HG002.output.vcf.gz | grep -v '^#' | cut -f 7 | sort | uniq -c. 4845593 PASS. 2908128 RefCall. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:916,performance,time,time,916,"Hi @edg1983 , I'll help with the runtime increase question. There are a few reasons I can think of that might have increased overall runtime between v0.9.0 and v1.0.0, but the magnitude of increase you're observing is not quite what I'd expected. First, let's take a quick comparison on our GitHub documentation:. - https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-case-study.md : 82m+189m+36m = 307m. - https://github.com/google/deepvariant/blob/r1.0/docs/metrics.md : 89m+258m+84m = 431m. Before we talk about code changes, note that r0.9 was done on a 50x BAM ([see script](https://raw.githubusercontent.com/google/deepvariant/r0.9/scripts/run_wgs_case_study_docker.sh)), and in r1.0 we used a 35x BAM ([see script](https://raw.githubusercontent.com/google/deepvariant/r1.0/scripts/run_wgs_case_study_docker.sh)). The lower coverage BAM is one main reason why the `postprocess_variants` took more time (84m vs 36m). There are a few known reasons why runtime would increase. 1. In [v1.0.0 release notes](https://github.com/google/deepvariant/releases/tag/v1.0.0), we mentioned that `--min_mapping_quality` is now set to 5 (previously 10). This would increase the number of reads being considered, as a result, increase the number of candidates generated. Which is why `make_examples` runtime might increase. And if there are more candidates, `call_variants` runtime can also increase. 2. In [v0.10.0 release notes](https://github.com/google/deepvariant/releases/tag/v0.10.0), we mentioned that we have turned off `--ws_use_window_selector_model` to improve accuracy but increased runtime too. . All these being said, I am surprised by your observation ""v0.9.0 takes 10-14h while v1.0.0 takes more than 24h per sample"". If you can provide the following information, it'll be very helpful for us:. 1. What is the runtime breakdown of the 3 steps on v0.9.0 and v1.0.0? This will help us understand which step (make_examples, call_variants, postprocess_variants) might be increasing the",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:205,testability,observ,observing,205,"Hi @edg1983 , I'll help with the runtime increase question. There are a few reasons I can think of that might have increased overall runtime between v0.9.0 and v1.0.0, but the magnitude of increase you're observing is not quite what I'd expected. First, let's take a quick comparison on our GitHub documentation:. - https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-case-study.md : 82m+189m+36m = 307m. - https://github.com/google/deepvariant/blob/r1.0/docs/metrics.md : 89m+258m+84m = 431m. Before we talk about code changes, note that r0.9 was done on a 50x BAM ([see script](https://raw.githubusercontent.com/google/deepvariant/r0.9/scripts/run_wgs_case_study_docker.sh)), and in r1.0 we used a 35x BAM ([see script](https://raw.githubusercontent.com/google/deepvariant/r1.0/scripts/run_wgs_case_study_docker.sh)). The lower coverage BAM is one main reason why the `postprocess_variants` took more time (84m vs 36m). There are a few known reasons why runtime would increase. 1. In [v1.0.0 release notes](https://github.com/google/deepvariant/releases/tag/v1.0.0), we mentioned that `--min_mapping_quality` is now set to 5 (previously 10). This would increase the number of reads being considered, as a result, increase the number of candidates generated. Which is why `make_examples` runtime might increase. And if there are more candidates, `call_variants` runtime can also increase. 2. In [v0.10.0 release notes](https://github.com/google/deepvariant/releases/tag/v0.10.0), we mentioned that we have turned off `--ws_use_window_selector_model` to improve accuracy but increased runtime too. . All these being said, I am surprised by your observation ""v0.9.0 takes 10-14h while v1.0.0 takes more than 24h per sample"". If you can provide the following information, it'll be very helpful for us:. 1. What is the runtime breakdown of the 3 steps on v0.9.0 and v1.0.0? This will help us understand which step (make_examples, call_variants, postprocess_variants) might be increasing the",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:843,testability,coverag,coverage,843,"Hi @edg1983 , I'll help with the runtime increase question. There are a few reasons I can think of that might have increased overall runtime between v0.9.0 and v1.0.0, but the magnitude of increase you're observing is not quite what I'd expected. First, let's take a quick comparison on our GitHub documentation:. - https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-case-study.md : 82m+189m+36m = 307m. - https://github.com/google/deepvariant/blob/r1.0/docs/metrics.md : 89m+258m+84m = 431m. Before we talk about code changes, note that r0.9 was done on a 50x BAM ([see script](https://raw.githubusercontent.com/google/deepvariant/r0.9/scripts/run_wgs_case_study_docker.sh)), and in r1.0 we used a 35x BAM ([see script](https://raw.githubusercontent.com/google/deepvariant/r1.0/scripts/run_wgs_case_study_docker.sh)). The lower coverage BAM is one main reason why the `postprocess_variants` took more time (84m vs 36m). There are a few known reasons why runtime would increase. 1. In [v1.0.0 release notes](https://github.com/google/deepvariant/releases/tag/v1.0.0), we mentioned that `--min_mapping_quality` is now set to 5 (previously 10). This would increase the number of reads being considered, as a result, increase the number of candidates generated. Which is why `make_examples` runtime might increase. And if there are more candidates, `call_variants` runtime can also increase. 2. In [v0.10.0 release notes](https://github.com/google/deepvariant/releases/tag/v0.10.0), we mentioned that we have turned off `--ws_use_window_selector_model` to improve accuracy but increased runtime too. . All these being said, I am surprised by your observation ""v0.9.0 takes 10-14h while v1.0.0 takes more than 24h per sample"". If you can provide the following information, it'll be very helpful for us:. 1. What is the runtime breakdown of the 3 steps on v0.9.0 and v1.0.0? This will help us understand which step (make_examples, call_variants, postprocess_variants) might be increasing the",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:1658,testability,observ,observation,1658,"9 was done on a 50x BAM ([see script](https://raw.githubusercontent.com/google/deepvariant/r0.9/scripts/run_wgs_case_study_docker.sh)), and in r1.0 we used a 35x BAM ([see script](https://raw.githubusercontent.com/google/deepvariant/r1.0/scripts/run_wgs_case_study_docker.sh)). The lower coverage BAM is one main reason why the `postprocess_variants` took more time (84m vs 36m). There are a few known reasons why runtime would increase. 1. In [v1.0.0 release notes](https://github.com/google/deepvariant/releases/tag/v1.0.0), we mentioned that `--min_mapping_quality` is now set to 5 (previously 10). This would increase the number of reads being considered, as a result, increase the number of candidates generated. Which is why `make_examples` runtime might increase. And if there are more candidates, `call_variants` runtime can also increase. 2. In [v0.10.0 release notes](https://github.com/google/deepvariant/releases/tag/v0.10.0), we mentioned that we have turned off `--ws_use_window_selector_model` to improve accuracy but increased runtime too. . All these being said, I am surprised by your observation ""v0.9.0 takes 10-14h while v1.0.0 takes more than 24h per sample"". If you can provide the following information, it'll be very helpful for us:. 1. What is the runtime breakdown of the 3 steps on v0.9.0 and v1.0.0? This will help us understand which step (make_examples, call_variants, postprocess_variants) might be increasing the most. 2. On exactly the same BAM, just with different versions, how many lines are in your VCF and GVCF? For example, here is what I have after running [the wgs script](https://raw.githubusercontent.com/google/deepvariant/r1.0/scripts/run_wgs_case_study_docker.sh). ```. $ zcat HG002.output.vcf.gz | grep -v '^#' | wc -l. 7753721. $ zcat HG002.output.g.vcf.gz | grep -v '^#' | wc -l. 91714977. ```. And here is the breakdown of PASS/RefCall:. ```. $ zcat HG002.output.vcf.gz | grep -v '^#' | cut -f 7 | sort | uniq -c. 4845593 PASS. 2908128 RefCall. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:1902,testability,understand,understand,1902,"9 was done on a 50x BAM ([see script](https://raw.githubusercontent.com/google/deepvariant/r0.9/scripts/run_wgs_case_study_docker.sh)), and in r1.0 we used a 35x BAM ([see script](https://raw.githubusercontent.com/google/deepvariant/r1.0/scripts/run_wgs_case_study_docker.sh)). The lower coverage BAM is one main reason why the `postprocess_variants` took more time (84m vs 36m). There are a few known reasons why runtime would increase. 1. In [v1.0.0 release notes](https://github.com/google/deepvariant/releases/tag/v1.0.0), we mentioned that `--min_mapping_quality` is now set to 5 (previously 10). This would increase the number of reads being considered, as a result, increase the number of candidates generated. Which is why `make_examples` runtime might increase. And if there are more candidates, `call_variants` runtime can also increase. 2. In [v0.10.0 release notes](https://github.com/google/deepvariant/releases/tag/v0.10.0), we mentioned that we have turned off `--ws_use_window_selector_model` to improve accuracy but increased runtime too. . All these being said, I am surprised by your observation ""v0.9.0 takes 10-14h while v1.0.0 takes more than 24h per sample"". If you can provide the following information, it'll be very helpful for us:. 1. What is the runtime breakdown of the 3 steps on v0.9.0 and v1.0.0? This will help us understand which step (make_examples, call_variants, postprocess_variants) might be increasing the most. 2. On exactly the same BAM, just with different versions, how many lines are in your VCF and GVCF? For example, here is what I have after running [the wgs script](https://raw.githubusercontent.com/google/deepvariant/r1.0/scripts/run_wgs_case_study_docker.sh). ```. $ zcat HG002.output.vcf.gz | grep -v '^#' | wc -l. 7753721. $ zcat HG002.output.g.vcf.gz | grep -v '^#' | wc -l. 91714977. ```. And here is the breakdown of PASS/RefCall:. ```. $ zcat HG002.output.vcf.gz | grep -v '^#' | cut -f 7 | sort | uniq -c. 4845593 PASS. 2908128 RefCall. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:19,usability,help,help,19,"Hi @edg1983 , I'll help with the runtime increase question. There are a few reasons I can think of that might have increased overall runtime between v0.9.0 and v1.0.0, but the magnitude of increase you're observing is not quite what I'd expected. First, let's take a quick comparison on our GitHub documentation:. - https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-case-study.md : 82m+189m+36m = 307m. - https://github.com/google/deepvariant/blob/r1.0/docs/metrics.md : 89m+258m+84m = 431m. Before we talk about code changes, note that r0.9 was done on a 50x BAM ([see script](https://raw.githubusercontent.com/google/deepvariant/r0.9/scripts/run_wgs_case_study_docker.sh)), and in r1.0 we used a 35x BAM ([see script](https://raw.githubusercontent.com/google/deepvariant/r1.0/scripts/run_wgs_case_study_docker.sh)). The lower coverage BAM is one main reason why the `postprocess_variants` took more time (84m vs 36m). There are a few known reasons why runtime would increase. 1. In [v1.0.0 release notes](https://github.com/google/deepvariant/releases/tag/v1.0.0), we mentioned that `--min_mapping_quality` is now set to 5 (previously 10). This would increase the number of reads being considered, as a result, increase the number of candidates generated. Which is why `make_examples` runtime might increase. And if there are more candidates, `call_variants` runtime can also increase. 2. In [v0.10.0 release notes](https://github.com/google/deepvariant/releases/tag/v0.10.0), we mentioned that we have turned off `--ws_use_window_selector_model` to improve accuracy but increased runtime too. . All these being said, I am surprised by your observation ""v0.9.0 takes 10-14h while v1.0.0 takes more than 24h per sample"". If you can provide the following information, it'll be very helpful for us:. 1. What is the runtime breakdown of the 3 steps on v0.9.0 and v1.0.0? This will help us understand which step (make_examples, call_variants, postprocess_variants) might be increasing the",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:298,usability,document,documentation,298,"Hi @edg1983 , I'll help with the runtime increase question. There are a few reasons I can think of that might have increased overall runtime between v0.9.0 and v1.0.0, but the magnitude of increase you're observing is not quite what I'd expected. First, let's take a quick comparison on our GitHub documentation:. - https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-case-study.md : 82m+189m+36m = 307m. - https://github.com/google/deepvariant/blob/r1.0/docs/metrics.md : 89m+258m+84m = 431m. Before we talk about code changes, note that r0.9 was done on a 50x BAM ([see script](https://raw.githubusercontent.com/google/deepvariant/r0.9/scripts/run_wgs_case_study_docker.sh)), and in r1.0 we used a 35x BAM ([see script](https://raw.githubusercontent.com/google/deepvariant/r1.0/scripts/run_wgs_case_study_docker.sh)). The lower coverage BAM is one main reason why the `postprocess_variants` took more time (84m vs 36m). There are a few known reasons why runtime would increase. 1. In [v1.0.0 release notes](https://github.com/google/deepvariant/releases/tag/v1.0.0), we mentioned that `--min_mapping_quality` is now set to 5 (previously 10). This would increase the number of reads being considered, as a result, increase the number of candidates generated. Which is why `make_examples` runtime might increase. And if there are more candidates, `call_variants` runtime can also increase. 2. In [v0.10.0 release notes](https://github.com/google/deepvariant/releases/tag/v0.10.0), we mentioned that we have turned off `--ws_use_window_selector_model` to improve accuracy but increased runtime too. . All these being said, I am surprised by your observation ""v0.9.0 takes 10-14h while v1.0.0 takes more than 24h per sample"". If you can provide the following information, it'll be very helpful for us:. 1. What is the runtime breakdown of the 3 steps on v0.9.0 and v1.0.0? This will help us understand which step (make_examples, call_variants, postprocess_variants) might be increasing the",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:1797,usability,help,helpful,1797,"9 was done on a 50x BAM ([see script](https://raw.githubusercontent.com/google/deepvariant/r0.9/scripts/run_wgs_case_study_docker.sh)), and in r1.0 we used a 35x BAM ([see script](https://raw.githubusercontent.com/google/deepvariant/r1.0/scripts/run_wgs_case_study_docker.sh)). The lower coverage BAM is one main reason why the `postprocess_variants` took more time (84m vs 36m). There are a few known reasons why runtime would increase. 1. In [v1.0.0 release notes](https://github.com/google/deepvariant/releases/tag/v1.0.0), we mentioned that `--min_mapping_quality` is now set to 5 (previously 10). This would increase the number of reads being considered, as a result, increase the number of candidates generated. Which is why `make_examples` runtime might increase. And if there are more candidates, `call_variants` runtime can also increase. 2. In [v0.10.0 release notes](https://github.com/google/deepvariant/releases/tag/v0.10.0), we mentioned that we have turned off `--ws_use_window_selector_model` to improve accuracy but increased runtime too. . All these being said, I am surprised by your observation ""v0.9.0 takes 10-14h while v1.0.0 takes more than 24h per sample"". If you can provide the following information, it'll be very helpful for us:. 1. What is the runtime breakdown of the 3 steps on v0.9.0 and v1.0.0? This will help us understand which step (make_examples, call_variants, postprocess_variants) might be increasing the most. 2. On exactly the same BAM, just with different versions, how many lines are in your VCF and GVCF? For example, here is what I have after running [the wgs script](https://raw.githubusercontent.com/google/deepvariant/r1.0/scripts/run_wgs_case_study_docker.sh). ```. $ zcat HG002.output.vcf.gz | grep -v '^#' | wc -l. 7753721. $ zcat HG002.output.g.vcf.gz | grep -v '^#' | wc -l. 91714977. ```. And here is the breakdown of PASS/RefCall:. ```. $ zcat HG002.output.vcf.gz | grep -v '^#' | cut -f 7 | sort | uniq -c. 4845593 PASS. 2908128 RefCall. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:1894,usability,help,help,1894,"9 was done on a 50x BAM ([see script](https://raw.githubusercontent.com/google/deepvariant/r0.9/scripts/run_wgs_case_study_docker.sh)), and in r1.0 we used a 35x BAM ([see script](https://raw.githubusercontent.com/google/deepvariant/r1.0/scripts/run_wgs_case_study_docker.sh)). The lower coverage BAM is one main reason why the `postprocess_variants` took more time (84m vs 36m). There are a few known reasons why runtime would increase. 1. In [v1.0.0 release notes](https://github.com/google/deepvariant/releases/tag/v1.0.0), we mentioned that `--min_mapping_quality` is now set to 5 (previously 10). This would increase the number of reads being considered, as a result, increase the number of candidates generated. Which is why `make_examples` runtime might increase. And if there are more candidates, `call_variants` runtime can also increase. 2. In [v0.10.0 release notes](https://github.com/google/deepvariant/releases/tag/v0.10.0), we mentioned that we have turned off `--ws_use_window_selector_model` to improve accuracy but increased runtime too. . All these being said, I am surprised by your observation ""v0.9.0 takes 10-14h while v1.0.0 takes more than 24h per sample"". If you can provide the following information, it'll be very helpful for us:. 1. What is the runtime breakdown of the 3 steps on v0.9.0 and v1.0.0? This will help us understand which step (make_examples, call_variants, postprocess_variants) might be increasing the most. 2. On exactly the same BAM, just with different versions, how many lines are in your VCF and GVCF? For example, here is what I have after running [the wgs script](https://raw.githubusercontent.com/google/deepvariant/r1.0/scripts/run_wgs_case_study_docker.sh). ```. $ zcat HG002.output.vcf.gz | grep -v '^#' | wc -l. 7753721. $ zcat HG002.output.g.vcf.gz | grep -v '^#' | wc -l. 91714977. ```. And here is the breakdown of PASS/RefCall:. ```. $ zcat HG002.output.vcf.gz | grep -v '^#' | cut -f 7 | sort | uniq -c. 4845593 PASS. 2908128 RefCall. ```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:1735,deployability,log,log,1735,"red reads), running deepvar 0.9.0 and 1.0.0 with singularity v.3.5.2-1.1.el7 and exactly the same command (`--num_shards=10`). The running time is now almost the same, much longer than my old test with deepvar 0.9.0 on a 30X WGS. . Below I've reported running time in min for make_example+call_variants+post_process:. - **v0.9.0**: 290+1494+281 = 2065min (~34h). - **v1.0.0**: 335+1487+300 = 2122min (~35h). - **v0.9.0 OLD TEST ON 30X WGS**: 198+456+82 = 736min (~12h). The number of variants for the 3 runs are:. - **v0.9.0**: 531371190 g.vcf.gz; 10784757 vcf.gz (4552313 PASS, 6232444 RefCall). - **v1.0.0**: 547491396 g.vcf.gz; 11892262 vcf.gz (4619350 PASS, 7272912 RefCall). - **v0.9.0 OLD TEST ON 30X WGS**: 213244705 g.vcf.gz; 9096927 vcf.gz (4661618 PASS, 4435309 RefCall). So, the first question is: are these running times expected? The running times you reported are much shorter it seems. Can it be that the running time increased from 12 to 34h just because of the lower coverage? . The exact command I've used is below (it is the same for v1.0.0 but using the corresponding singularity image):. ```. singularity exec \. --bind /data/ref/genomes/GRCh38:/genomes \. --bind /data/projects/HICF2_project/BAM:/bam_files \. /well/gel/HICF2/software/singularity/deepvariant-0.9.0.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS --ref=""/genomes/GCA_000001405.15_GRCh38_full_plus_hs38d1_analysis_set.fna"" \. --reads=""/bam_files/${bamfile}"" \. --output_vcf=""VCF/${sampleID}.vcf.gz"" \. --output_gvcf=""VCF/${sampleID}.g.vcf.gz"" \. --intermediate_results_dir=""tmp_data"" \. --num_shards=10. ```. I've uploaded the log files from the 3 runs if you want to take a look:. [deepvar_0.9.0_oldrun.log](https://github.com/google/deepvariant/files/5281370/deepvar_0.9.0_oldrun.log). [deepvar_1.0.0.log](https://github.com/google/deepvariant/files/5281371/deepvar_1.0.0.log). [deepvar_0.9.0.log](https://github.com/google/deepvariant/files/5281372/deepvar_0.9.0.log). Thanks for your support!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:1812,deployability,log,log,1812,"red reads), running deepvar 0.9.0 and 1.0.0 with singularity v.3.5.2-1.1.el7 and exactly the same command (`--num_shards=10`). The running time is now almost the same, much longer than my old test with deepvar 0.9.0 on a 30X WGS. . Below I've reported running time in min for make_example+call_variants+post_process:. - **v0.9.0**: 290+1494+281 = 2065min (~34h). - **v1.0.0**: 335+1487+300 = 2122min (~35h). - **v0.9.0 OLD TEST ON 30X WGS**: 198+456+82 = 736min (~12h). The number of variants for the 3 runs are:. - **v0.9.0**: 531371190 g.vcf.gz; 10784757 vcf.gz (4552313 PASS, 6232444 RefCall). - **v1.0.0**: 547491396 g.vcf.gz; 11892262 vcf.gz (4619350 PASS, 7272912 RefCall). - **v0.9.0 OLD TEST ON 30X WGS**: 213244705 g.vcf.gz; 9096927 vcf.gz (4661618 PASS, 4435309 RefCall). So, the first question is: are these running times expected? The running times you reported are much shorter it seems. Can it be that the running time increased from 12 to 34h just because of the lower coverage? . The exact command I've used is below (it is the same for v1.0.0 but using the corresponding singularity image):. ```. singularity exec \. --bind /data/ref/genomes/GRCh38:/genomes \. --bind /data/projects/HICF2_project/BAM:/bam_files \. /well/gel/HICF2/software/singularity/deepvariant-0.9.0.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS --ref=""/genomes/GCA_000001405.15_GRCh38_full_plus_hs38d1_analysis_set.fna"" \. --reads=""/bam_files/${bamfile}"" \. --output_vcf=""VCF/${sampleID}.vcf.gz"" \. --output_gvcf=""VCF/${sampleID}.g.vcf.gz"" \. --intermediate_results_dir=""tmp_data"" \. --num_shards=10. ```. I've uploaded the log files from the 3 runs if you want to take a look:. [deepvar_0.9.0_oldrun.log](https://github.com/google/deepvariant/files/5281370/deepvar_0.9.0_oldrun.log). [deepvar_1.0.0.log](https://github.com/google/deepvariant/files/5281371/deepvar_1.0.0.log). [deepvar_0.9.0.log](https://github.com/google/deepvariant/files/5281372/deepvar_0.9.0.log). Thanks for your support!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:1890,deployability,log,log,1890,"red reads), running deepvar 0.9.0 and 1.0.0 with singularity v.3.5.2-1.1.el7 and exactly the same command (`--num_shards=10`). The running time is now almost the same, much longer than my old test with deepvar 0.9.0 on a 30X WGS. . Below I've reported running time in min for make_example+call_variants+post_process:. - **v0.9.0**: 290+1494+281 = 2065min (~34h). - **v1.0.0**: 335+1487+300 = 2122min (~35h). - **v0.9.0 OLD TEST ON 30X WGS**: 198+456+82 = 736min (~12h). The number of variants for the 3 runs are:. - **v0.9.0**: 531371190 g.vcf.gz; 10784757 vcf.gz (4552313 PASS, 6232444 RefCall). - **v1.0.0**: 547491396 g.vcf.gz; 11892262 vcf.gz (4619350 PASS, 7272912 RefCall). - **v0.9.0 OLD TEST ON 30X WGS**: 213244705 g.vcf.gz; 9096927 vcf.gz (4661618 PASS, 4435309 RefCall). So, the first question is: are these running times expected? The running times you reported are much shorter it seems. Can it be that the running time increased from 12 to 34h just because of the lower coverage? . The exact command I've used is below (it is the same for v1.0.0 but using the corresponding singularity image):. ```. singularity exec \. --bind /data/ref/genomes/GRCh38:/genomes \. --bind /data/projects/HICF2_project/BAM:/bam_files \. /well/gel/HICF2/software/singularity/deepvariant-0.9.0.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS --ref=""/genomes/GCA_000001405.15_GRCh38_full_plus_hs38d1_analysis_set.fna"" \. --reads=""/bam_files/${bamfile}"" \. --output_vcf=""VCF/${sampleID}.vcf.gz"" \. --output_gvcf=""VCF/${sampleID}.g.vcf.gz"" \. --intermediate_results_dir=""tmp_data"" \. --num_shards=10. ```. I've uploaded the log files from the 3 runs if you want to take a look:. [deepvar_0.9.0_oldrun.log](https://github.com/google/deepvariant/files/5281370/deepvar_0.9.0_oldrun.log). [deepvar_1.0.0.log](https://github.com/google/deepvariant/files/5281371/deepvar_1.0.0.log). [deepvar_0.9.0.log](https://github.com/google/deepvariant/files/5281372/deepvar_0.9.0.log). Thanks for your support!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:1911,deployability,log,log,1911,"red reads), running deepvar 0.9.0 and 1.0.0 with singularity v.3.5.2-1.1.el7 and exactly the same command (`--num_shards=10`). The running time is now almost the same, much longer than my old test with deepvar 0.9.0 on a 30X WGS. . Below I've reported running time in min for make_example+call_variants+post_process:. - **v0.9.0**: 290+1494+281 = 2065min (~34h). - **v1.0.0**: 335+1487+300 = 2122min (~35h). - **v0.9.0 OLD TEST ON 30X WGS**: 198+456+82 = 736min (~12h). The number of variants for the 3 runs are:. - **v0.9.0**: 531371190 g.vcf.gz; 10784757 vcf.gz (4552313 PASS, 6232444 RefCall). - **v1.0.0**: 547491396 g.vcf.gz; 11892262 vcf.gz (4619350 PASS, 7272912 RefCall). - **v0.9.0 OLD TEST ON 30X WGS**: 213244705 g.vcf.gz; 9096927 vcf.gz (4661618 PASS, 4435309 RefCall). So, the first question is: are these running times expected? The running times you reported are much shorter it seems. Can it be that the running time increased from 12 to 34h just because of the lower coverage? . The exact command I've used is below (it is the same for v1.0.0 but using the corresponding singularity image):. ```. singularity exec \. --bind /data/ref/genomes/GRCh38:/genomes \. --bind /data/projects/HICF2_project/BAM:/bam_files \. /well/gel/HICF2/software/singularity/deepvariant-0.9.0.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS --ref=""/genomes/GCA_000001405.15_GRCh38_full_plus_hs38d1_analysis_set.fna"" \. --reads=""/bam_files/${bamfile}"" \. --output_vcf=""VCF/${sampleID}.vcf.gz"" \. --output_gvcf=""VCF/${sampleID}.g.vcf.gz"" \. --intermediate_results_dir=""tmp_data"" \. --num_shards=10. ```. I've uploaded the log files from the 3 runs if you want to take a look:. [deepvar_0.9.0_oldrun.log](https://github.com/google/deepvariant/files/5281370/deepvar_0.9.0_oldrun.log). [deepvar_1.0.0.log](https://github.com/google/deepvariant/files/5281371/deepvar_1.0.0.log). [deepvar_0.9.0.log](https://github.com/google/deepvariant/files/5281372/deepvar_0.9.0.log). Thanks for your support!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:1982,deployability,log,log,1982,"red reads), running deepvar 0.9.0 and 1.0.0 with singularity v.3.5.2-1.1.el7 and exactly the same command (`--num_shards=10`). The running time is now almost the same, much longer than my old test with deepvar 0.9.0 on a 30X WGS. . Below I've reported running time in min for make_example+call_variants+post_process:. - **v0.9.0**: 290+1494+281 = 2065min (~34h). - **v1.0.0**: 335+1487+300 = 2122min (~35h). - **v0.9.0 OLD TEST ON 30X WGS**: 198+456+82 = 736min (~12h). The number of variants for the 3 runs are:. - **v0.9.0**: 531371190 g.vcf.gz; 10784757 vcf.gz (4552313 PASS, 6232444 RefCall). - **v1.0.0**: 547491396 g.vcf.gz; 11892262 vcf.gz (4619350 PASS, 7272912 RefCall). - **v0.9.0 OLD TEST ON 30X WGS**: 213244705 g.vcf.gz; 9096927 vcf.gz (4661618 PASS, 4435309 RefCall). So, the first question is: are these running times expected? The running times you reported are much shorter it seems. Can it be that the running time increased from 12 to 34h just because of the lower coverage? . The exact command I've used is below (it is the same for v1.0.0 but using the corresponding singularity image):. ```. singularity exec \. --bind /data/ref/genomes/GRCh38:/genomes \. --bind /data/projects/HICF2_project/BAM:/bam_files \. /well/gel/HICF2/software/singularity/deepvariant-0.9.0.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS --ref=""/genomes/GCA_000001405.15_GRCh38_full_plus_hs38d1_analysis_set.fna"" \. --reads=""/bam_files/${bamfile}"" \. --output_vcf=""VCF/${sampleID}.vcf.gz"" \. --output_gvcf=""VCF/${sampleID}.g.vcf.gz"" \. --intermediate_results_dir=""tmp_data"" \. --num_shards=10. ```. I've uploaded the log files from the 3 runs if you want to take a look:. [deepvar_0.9.0_oldrun.log](https://github.com/google/deepvariant/files/5281370/deepvar_0.9.0_oldrun.log). [deepvar_1.0.0.log](https://github.com/google/deepvariant/files/5281371/deepvar_1.0.0.log). [deepvar_0.9.0.log](https://github.com/google/deepvariant/files/5281372/deepvar_0.9.0.log). Thanks for your support!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:2003,deployability,log,log,2003,"red reads), running deepvar 0.9.0 and 1.0.0 with singularity v.3.5.2-1.1.el7 and exactly the same command (`--num_shards=10`). The running time is now almost the same, much longer than my old test with deepvar 0.9.0 on a 30X WGS. . Below I've reported running time in min for make_example+call_variants+post_process:. - **v0.9.0**: 290+1494+281 = 2065min (~34h). - **v1.0.0**: 335+1487+300 = 2122min (~35h). - **v0.9.0 OLD TEST ON 30X WGS**: 198+456+82 = 736min (~12h). The number of variants for the 3 runs are:. - **v0.9.0**: 531371190 g.vcf.gz; 10784757 vcf.gz (4552313 PASS, 6232444 RefCall). - **v1.0.0**: 547491396 g.vcf.gz; 11892262 vcf.gz (4619350 PASS, 7272912 RefCall). - **v0.9.0 OLD TEST ON 30X WGS**: 213244705 g.vcf.gz; 9096927 vcf.gz (4661618 PASS, 4435309 RefCall). So, the first question is: are these running times expected? The running times you reported are much shorter it seems. Can it be that the running time increased from 12 to 34h just because of the lower coverage? . The exact command I've used is below (it is the same for v1.0.0 but using the corresponding singularity image):. ```. singularity exec \. --bind /data/ref/genomes/GRCh38:/genomes \. --bind /data/projects/HICF2_project/BAM:/bam_files \. /well/gel/HICF2/software/singularity/deepvariant-0.9.0.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS --ref=""/genomes/GCA_000001405.15_GRCh38_full_plus_hs38d1_analysis_set.fna"" \. --reads=""/bam_files/${bamfile}"" \. --output_vcf=""VCF/${sampleID}.vcf.gz"" \. --output_gvcf=""VCF/${sampleID}.g.vcf.gz"" \. --intermediate_results_dir=""tmp_data"" \. --num_shards=10. ```. I've uploaded the log files from the 3 runs if you want to take a look:. [deepvar_0.9.0_oldrun.log](https://github.com/google/deepvariant/files/5281370/deepvar_0.9.0_oldrun.log). [deepvar_1.0.0.log](https://github.com/google/deepvariant/files/5281371/deepvar_1.0.0.log). [deepvar_0.9.0.log](https://github.com/google/deepvariant/files/5281372/deepvar_0.9.0.log). Thanks for your support!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:2074,deployability,log,log,2074,"red reads), running deepvar 0.9.0 and 1.0.0 with singularity v.3.5.2-1.1.el7 and exactly the same command (`--num_shards=10`). The running time is now almost the same, much longer than my old test with deepvar 0.9.0 on a 30X WGS. . Below I've reported running time in min for make_example+call_variants+post_process:. - **v0.9.0**: 290+1494+281 = 2065min (~34h). - **v1.0.0**: 335+1487+300 = 2122min (~35h). - **v0.9.0 OLD TEST ON 30X WGS**: 198+456+82 = 736min (~12h). The number of variants for the 3 runs are:. - **v0.9.0**: 531371190 g.vcf.gz; 10784757 vcf.gz (4552313 PASS, 6232444 RefCall). - **v1.0.0**: 547491396 g.vcf.gz; 11892262 vcf.gz (4619350 PASS, 7272912 RefCall). - **v0.9.0 OLD TEST ON 30X WGS**: 213244705 g.vcf.gz; 9096927 vcf.gz (4661618 PASS, 4435309 RefCall). So, the first question is: are these running times expected? The running times you reported are much shorter it seems. Can it be that the running time increased from 12 to 34h just because of the lower coverage? . The exact command I've used is below (it is the same for v1.0.0 but using the corresponding singularity image):. ```. singularity exec \. --bind /data/ref/genomes/GRCh38:/genomes \. --bind /data/projects/HICF2_project/BAM:/bam_files \. /well/gel/HICF2/software/singularity/deepvariant-0.9.0.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS --ref=""/genomes/GCA_000001405.15_GRCh38_full_plus_hs38d1_analysis_set.fna"" \. --reads=""/bam_files/${bamfile}"" \. --output_vcf=""VCF/${sampleID}.vcf.gz"" \. --output_gvcf=""VCF/${sampleID}.g.vcf.gz"" \. --intermediate_results_dir=""tmp_data"" \. --num_shards=10. ```. I've uploaded the log files from the 3 runs if you want to take a look:. [deepvar_0.9.0_oldrun.log](https://github.com/google/deepvariant/files/5281370/deepvar_0.9.0_oldrun.log). [deepvar_1.0.0.log](https://github.com/google/deepvariant/files/5281371/deepvar_1.0.0.log). [deepvar_0.9.0.log](https://github.com/google/deepvariant/files/5281372/deepvar_0.9.0.log). Thanks for your support!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:1240,interoperability,bind,bind,1240,"red reads), running deepvar 0.9.0 and 1.0.0 with singularity v.3.5.2-1.1.el7 and exactly the same command (`--num_shards=10`). The running time is now almost the same, much longer than my old test with deepvar 0.9.0 on a 30X WGS. . Below I've reported running time in min for make_example+call_variants+post_process:. - **v0.9.0**: 290+1494+281 = 2065min (~34h). - **v1.0.0**: 335+1487+300 = 2122min (~35h). - **v0.9.0 OLD TEST ON 30X WGS**: 198+456+82 = 736min (~12h). The number of variants for the 3 runs are:. - **v0.9.0**: 531371190 g.vcf.gz; 10784757 vcf.gz (4552313 PASS, 6232444 RefCall). - **v1.0.0**: 547491396 g.vcf.gz; 11892262 vcf.gz (4619350 PASS, 7272912 RefCall). - **v0.9.0 OLD TEST ON 30X WGS**: 213244705 g.vcf.gz; 9096927 vcf.gz (4661618 PASS, 4435309 RefCall). So, the first question is: are these running times expected? The running times you reported are much shorter it seems. Can it be that the running time increased from 12 to 34h just because of the lower coverage? . The exact command I've used is below (it is the same for v1.0.0 but using the corresponding singularity image):. ```. singularity exec \. --bind /data/ref/genomes/GRCh38:/genomes \. --bind /data/projects/HICF2_project/BAM:/bam_files \. /well/gel/HICF2/software/singularity/deepvariant-0.9.0.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS --ref=""/genomes/GCA_000001405.15_GRCh38_full_plus_hs38d1_analysis_set.fna"" \. --reads=""/bam_files/${bamfile}"" \. --output_vcf=""VCF/${sampleID}.vcf.gz"" \. --output_gvcf=""VCF/${sampleID}.g.vcf.gz"" \. --intermediate_results_dir=""tmp_data"" \. --num_shards=10. ```. I've uploaded the log files from the 3 runs if you want to take a look:. [deepvar_0.9.0_oldrun.log](https://github.com/google/deepvariant/files/5281370/deepvar_0.9.0_oldrun.log). [deepvar_1.0.0.log](https://github.com/google/deepvariant/files/5281371/deepvar_1.0.0.log). [deepvar_0.9.0.log](https://github.com/google/deepvariant/files/5281372/deepvar_0.9.0.log). Thanks for your support!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:1284,interoperability,bind,bind,1284,"red reads), running deepvar 0.9.0 and 1.0.0 with singularity v.3.5.2-1.1.el7 and exactly the same command (`--num_shards=10`). The running time is now almost the same, much longer than my old test with deepvar 0.9.0 on a 30X WGS. . Below I've reported running time in min for make_example+call_variants+post_process:. - **v0.9.0**: 290+1494+281 = 2065min (~34h). - **v1.0.0**: 335+1487+300 = 2122min (~35h). - **v0.9.0 OLD TEST ON 30X WGS**: 198+456+82 = 736min (~12h). The number of variants for the 3 runs are:. - **v0.9.0**: 531371190 g.vcf.gz; 10784757 vcf.gz (4552313 PASS, 6232444 RefCall). - **v1.0.0**: 547491396 g.vcf.gz; 11892262 vcf.gz (4619350 PASS, 7272912 RefCall). - **v0.9.0 OLD TEST ON 30X WGS**: 213244705 g.vcf.gz; 9096927 vcf.gz (4661618 PASS, 4435309 RefCall). So, the first question is: are these running times expected? The running times you reported are much shorter it seems. Can it be that the running time increased from 12 to 34h just because of the lower coverage? . The exact command I've used is below (it is the same for v1.0.0 but using the corresponding singularity image):. ```. singularity exec \. --bind /data/ref/genomes/GRCh38:/genomes \. --bind /data/projects/HICF2_project/BAM:/bam_files \. /well/gel/HICF2/software/singularity/deepvariant-0.9.0.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS --ref=""/genomes/GCA_000001405.15_GRCh38_full_plus_hs38d1_analysis_set.fna"" \. --reads=""/bam_files/${bamfile}"" \. --output_vcf=""VCF/${sampleID}.vcf.gz"" \. --output_gvcf=""VCF/${sampleID}.g.vcf.gz"" \. --intermediate_results_dir=""tmp_data"" \. --num_shards=10. ```. I've uploaded the log files from the 3 runs if you want to take a look:. [deepvar_0.9.0_oldrun.log](https://github.com/google/deepvariant/files/5281370/deepvar_0.9.0_oldrun.log). [deepvar_1.0.0.log](https://github.com/google/deepvariant/files/5281371/deepvar_1.0.0.log). [deepvar_0.9.0.log](https://github.com/google/deepvariant/files/5281372/deepvar_0.9.0.log). Thanks for your support!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:1240,modifiability,bind,bind,1240,"red reads), running deepvar 0.9.0 and 1.0.0 with singularity v.3.5.2-1.1.el7 and exactly the same command (`--num_shards=10`). The running time is now almost the same, much longer than my old test with deepvar 0.9.0 on a 30X WGS. . Below I've reported running time in min for make_example+call_variants+post_process:. - **v0.9.0**: 290+1494+281 = 2065min (~34h). - **v1.0.0**: 335+1487+300 = 2122min (~35h). - **v0.9.0 OLD TEST ON 30X WGS**: 198+456+82 = 736min (~12h). The number of variants for the 3 runs are:. - **v0.9.0**: 531371190 g.vcf.gz; 10784757 vcf.gz (4552313 PASS, 6232444 RefCall). - **v1.0.0**: 547491396 g.vcf.gz; 11892262 vcf.gz (4619350 PASS, 7272912 RefCall). - **v0.9.0 OLD TEST ON 30X WGS**: 213244705 g.vcf.gz; 9096927 vcf.gz (4661618 PASS, 4435309 RefCall). So, the first question is: are these running times expected? The running times you reported are much shorter it seems. Can it be that the running time increased from 12 to 34h just because of the lower coverage? . The exact command I've used is below (it is the same for v1.0.0 but using the corresponding singularity image):. ```. singularity exec \. --bind /data/ref/genomes/GRCh38:/genomes \. --bind /data/projects/HICF2_project/BAM:/bam_files \. /well/gel/HICF2/software/singularity/deepvariant-0.9.0.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS --ref=""/genomes/GCA_000001405.15_GRCh38_full_plus_hs38d1_analysis_set.fna"" \. --reads=""/bam_files/${bamfile}"" \. --output_vcf=""VCF/${sampleID}.vcf.gz"" \. --output_gvcf=""VCF/${sampleID}.g.vcf.gz"" \. --intermediate_results_dir=""tmp_data"" \. --num_shards=10. ```. I've uploaded the log files from the 3 runs if you want to take a look:. [deepvar_0.9.0_oldrun.log](https://github.com/google/deepvariant/files/5281370/deepvar_0.9.0_oldrun.log). [deepvar_1.0.0.log](https://github.com/google/deepvariant/files/5281371/deepvar_1.0.0.log). [deepvar_0.9.0.log](https://github.com/google/deepvariant/files/5281372/deepvar_0.9.0.log). Thanks for your support!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:1284,modifiability,bind,bind,1284,"red reads), running deepvar 0.9.0 and 1.0.0 with singularity v.3.5.2-1.1.el7 and exactly the same command (`--num_shards=10`). The running time is now almost the same, much longer than my old test with deepvar 0.9.0 on a 30X WGS. . Below I've reported running time in min for make_example+call_variants+post_process:. - **v0.9.0**: 290+1494+281 = 2065min (~34h). - **v1.0.0**: 335+1487+300 = 2122min (~35h). - **v0.9.0 OLD TEST ON 30X WGS**: 198+456+82 = 736min (~12h). The number of variants for the 3 runs are:. - **v0.9.0**: 531371190 g.vcf.gz; 10784757 vcf.gz (4552313 PASS, 6232444 RefCall). - **v1.0.0**: 547491396 g.vcf.gz; 11892262 vcf.gz (4619350 PASS, 7272912 RefCall). - **v0.9.0 OLD TEST ON 30X WGS**: 213244705 g.vcf.gz; 9096927 vcf.gz (4661618 PASS, 4435309 RefCall). So, the first question is: are these running times expected? The running times you reported are much shorter it seems. Can it be that the running time increased from 12 to 34h just because of the lower coverage? . The exact command I've used is below (it is the same for v1.0.0 but using the corresponding singularity image):. ```. singularity exec \. --bind /data/ref/genomes/GRCh38:/genomes \. --bind /data/projects/HICF2_project/BAM:/bam_files \. /well/gel/HICF2/software/singularity/deepvariant-0.9.0.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS --ref=""/genomes/GCA_000001405.15_GRCh38_full_plus_hs38d1_analysis_set.fna"" \. --reads=""/bam_files/${bamfile}"" \. --output_vcf=""VCF/${sampleID}.vcf.gz"" \. --output_gvcf=""VCF/${sampleID}.g.vcf.gz"" \. --intermediate_results_dir=""tmp_data"" \. --num_shards=10. ```. I've uploaded the log files from the 3 runs if you want to take a look:. [deepvar_0.9.0_oldrun.log](https://github.com/google/deepvariant/files/5281370/deepvar_0.9.0_oldrun.log). [deepvar_1.0.0.log](https://github.com/google/deepvariant/files/5281371/deepvar_1.0.0.log). [deepvar_0.9.0.log](https://github.com/google/deepvariant/files/5281372/deepvar_0.9.0.log). Thanks for your support!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:243,performance,time,time,243,"Hi. After your comment I've repeated the test using the same input BAM (18X WGS sequenced with 150bp paired reads), running deepvar 0.9.0 and 1.0.0 with singularity v.3.5.2-1.1.el7 and exactly the same command (`--num_shards=10`). The running time is now almost the same, much longer than my old test with deepvar 0.9.0 on a 30X WGS. . Below I've reported running time in min for make_example+call_variants+post_process:. - **v0.9.0**: 290+1494+281 = 2065min (~34h). - **v1.0.0**: 335+1487+300 = 2122min (~35h). - **v0.9.0 OLD TEST ON 30X WGS**: 198+456+82 = 736min (~12h). The number of variants for the 3 runs are:. - **v0.9.0**: 531371190 g.vcf.gz; 10784757 vcf.gz (4552313 PASS, 6232444 RefCall). - **v1.0.0**: 547491396 g.vcf.gz; 11892262 vcf.gz (4619350 PASS, 7272912 RefCall). - **v0.9.0 OLD TEST ON 30X WGS**: 213244705 g.vcf.gz; 9096927 vcf.gz (4661618 PASS, 4435309 RefCall). So, the first question is: are these running times expected? The running times you reported are much shorter it seems. Can it be that the running time increased from 12 to 34h just because of the lower coverage? . The exact command I've used is below (it is the same for v1.0.0 but using the corresponding singularity image):. ```. singularity exec \. --bind /data/ref/genomes/GRCh38:/genomes \. --bind /data/projects/HICF2_project/BAM:/bam_files \. /well/gel/HICF2/software/singularity/deepvariant-0.9.0.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS --ref=""/genomes/GCA_000001405.15_GRCh38_full_plus_hs38d1_analysis_set.fna"" \. --reads=""/bam_files/${bamfile}"" \. --output_vcf=""VCF/${sampleID}.vcf.gz"" \. --output_gvcf=""VCF/${sampleID}.g.vcf.gz"" \. --intermediate_results_dir=""tmp_data"" \. --num_shards=10. ```. I've uploaded the log files from the 3 runs if you want to take a look:. [deepvar_0.9.0_oldrun.log](https://github.com/google/deepvariant/files/5281370/deepvar_0.9.0_oldrun.log). [deepvar_1.0.0.log](https://github.com/google/deepvariant/files/5281371/deepvar_1.0.0.log). [deepvar_0.9",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:364,performance,time,time,364,"Hi. After your comment I've repeated the test using the same input BAM (18X WGS sequenced with 150bp paired reads), running deepvar 0.9.0 and 1.0.0 with singularity v.3.5.2-1.1.el7 and exactly the same command (`--num_shards=10`). The running time is now almost the same, much longer than my old test with deepvar 0.9.0 on a 30X WGS. . Below I've reported running time in min for make_example+call_variants+post_process:. - **v0.9.0**: 290+1494+281 = 2065min (~34h). - **v1.0.0**: 335+1487+300 = 2122min (~35h). - **v0.9.0 OLD TEST ON 30X WGS**: 198+456+82 = 736min (~12h). The number of variants for the 3 runs are:. - **v0.9.0**: 531371190 g.vcf.gz; 10784757 vcf.gz (4552313 PASS, 6232444 RefCall). - **v1.0.0**: 547491396 g.vcf.gz; 11892262 vcf.gz (4619350 PASS, 7272912 RefCall). - **v0.9.0 OLD TEST ON 30X WGS**: 213244705 g.vcf.gz; 9096927 vcf.gz (4661618 PASS, 4435309 RefCall). So, the first question is: are these running times expected? The running times you reported are much shorter it seems. Can it be that the running time increased from 12 to 34h just because of the lower coverage? . The exact command I've used is below (it is the same for v1.0.0 but using the corresponding singularity image):. ```. singularity exec \. --bind /data/ref/genomes/GRCh38:/genomes \. --bind /data/projects/HICF2_project/BAM:/bam_files \. /well/gel/HICF2/software/singularity/deepvariant-0.9.0.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS --ref=""/genomes/GCA_000001405.15_GRCh38_full_plus_hs38d1_analysis_set.fna"" \. --reads=""/bam_files/${bamfile}"" \. --output_vcf=""VCF/${sampleID}.vcf.gz"" \. --output_gvcf=""VCF/${sampleID}.g.vcf.gz"" \. --intermediate_results_dir=""tmp_data"" \. --num_shards=10. ```. I've uploaded the log files from the 3 runs if you want to take a look:. [deepvar_0.9.0_oldrun.log](https://github.com/google/deepvariant/files/5281370/deepvar_0.9.0_oldrun.log). [deepvar_1.0.0.log](https://github.com/google/deepvariant/files/5281371/deepvar_1.0.0.log). [deepvar_0.9",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:931,performance,time,times,931,"Hi. After your comment I've repeated the test using the same input BAM (18X WGS sequenced with 150bp paired reads), running deepvar 0.9.0 and 1.0.0 with singularity v.3.5.2-1.1.el7 and exactly the same command (`--num_shards=10`). The running time is now almost the same, much longer than my old test with deepvar 0.9.0 on a 30X WGS. . Below I've reported running time in min for make_example+call_variants+post_process:. - **v0.9.0**: 290+1494+281 = 2065min (~34h). - **v1.0.0**: 335+1487+300 = 2122min (~35h). - **v0.9.0 OLD TEST ON 30X WGS**: 198+456+82 = 736min (~12h). The number of variants for the 3 runs are:. - **v0.9.0**: 531371190 g.vcf.gz; 10784757 vcf.gz (4552313 PASS, 6232444 RefCall). - **v1.0.0**: 547491396 g.vcf.gz; 11892262 vcf.gz (4619350 PASS, 7272912 RefCall). - **v0.9.0 OLD TEST ON 30X WGS**: 213244705 g.vcf.gz; 9096927 vcf.gz (4661618 PASS, 4435309 RefCall). So, the first question is: are these running times expected? The running times you reported are much shorter it seems. Can it be that the running time increased from 12 to 34h just because of the lower coverage? . The exact command I've used is below (it is the same for v1.0.0 but using the corresponding singularity image):. ```. singularity exec \. --bind /data/ref/genomes/GRCh38:/genomes \. --bind /data/projects/HICF2_project/BAM:/bam_files \. /well/gel/HICF2/software/singularity/deepvariant-0.9.0.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS --ref=""/genomes/GCA_000001405.15_GRCh38_full_plus_hs38d1_analysis_set.fna"" \. --reads=""/bam_files/${bamfile}"" \. --output_vcf=""VCF/${sampleID}.vcf.gz"" \. --output_gvcf=""VCF/${sampleID}.g.vcf.gz"" \. --intermediate_results_dir=""tmp_data"" \. --num_shards=10. ```. I've uploaded the log files from the 3 runs if you want to take a look:. [deepvar_0.9.0_oldrun.log](https://github.com/google/deepvariant/files/5281370/deepvar_0.9.0_oldrun.log). [deepvar_1.0.0.log](https://github.com/google/deepvariant/files/5281371/deepvar_1.0.0.log). [deepvar_0.9",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:959,performance,time,times,959,"Hi. After your comment I've repeated the test using the same input BAM (18X WGS sequenced with 150bp paired reads), running deepvar 0.9.0 and 1.0.0 with singularity v.3.5.2-1.1.el7 and exactly the same command (`--num_shards=10`). The running time is now almost the same, much longer than my old test with deepvar 0.9.0 on a 30X WGS. . Below I've reported running time in min for make_example+call_variants+post_process:. - **v0.9.0**: 290+1494+281 = 2065min (~34h). - **v1.0.0**: 335+1487+300 = 2122min (~35h). - **v0.9.0 OLD TEST ON 30X WGS**: 198+456+82 = 736min (~12h). The number of variants for the 3 runs are:. - **v0.9.0**: 531371190 g.vcf.gz; 10784757 vcf.gz (4552313 PASS, 6232444 RefCall). - **v1.0.0**: 547491396 g.vcf.gz; 11892262 vcf.gz (4619350 PASS, 7272912 RefCall). - **v0.9.0 OLD TEST ON 30X WGS**: 213244705 g.vcf.gz; 9096927 vcf.gz (4661618 PASS, 4435309 RefCall). So, the first question is: are these running times expected? The running times you reported are much shorter it seems. Can it be that the running time increased from 12 to 34h just because of the lower coverage? . The exact command I've used is below (it is the same for v1.0.0 but using the corresponding singularity image):. ```. singularity exec \. --bind /data/ref/genomes/GRCh38:/genomes \. --bind /data/projects/HICF2_project/BAM:/bam_files \. /well/gel/HICF2/software/singularity/deepvariant-0.9.0.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS --ref=""/genomes/GCA_000001405.15_GRCh38_full_plus_hs38d1_analysis_set.fna"" \. --reads=""/bam_files/${bamfile}"" \. --output_vcf=""VCF/${sampleID}.vcf.gz"" \. --output_gvcf=""VCF/${sampleID}.g.vcf.gz"" \. --intermediate_results_dir=""tmp_data"" \. --num_shards=10. ```. I've uploaded the log files from the 3 runs if you want to take a look:. [deepvar_0.9.0_oldrun.log](https://github.com/google/deepvariant/files/5281370/deepvar_0.9.0_oldrun.log). [deepvar_1.0.0.log](https://github.com/google/deepvariant/files/5281371/deepvar_1.0.0.log). [deepvar_0.9",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:1032,performance,time,time,1032,"ed the test using the same input BAM (18X WGS sequenced with 150bp paired reads), running deepvar 0.9.0 and 1.0.0 with singularity v.3.5.2-1.1.el7 and exactly the same command (`--num_shards=10`). The running time is now almost the same, much longer than my old test with deepvar 0.9.0 on a 30X WGS. . Below I've reported running time in min for make_example+call_variants+post_process:. - **v0.9.0**: 290+1494+281 = 2065min (~34h). - **v1.0.0**: 335+1487+300 = 2122min (~35h). - **v0.9.0 OLD TEST ON 30X WGS**: 198+456+82 = 736min (~12h). The number of variants for the 3 runs are:. - **v0.9.0**: 531371190 g.vcf.gz; 10784757 vcf.gz (4552313 PASS, 6232444 RefCall). - **v1.0.0**: 547491396 g.vcf.gz; 11892262 vcf.gz (4619350 PASS, 7272912 RefCall). - **v0.9.0 OLD TEST ON 30X WGS**: 213244705 g.vcf.gz; 9096927 vcf.gz (4661618 PASS, 4435309 RefCall). So, the first question is: are these running times expected? The running times you reported are much shorter it seems. Can it be that the running time increased from 12 to 34h just because of the lower coverage? . The exact command I've used is below (it is the same for v1.0.0 but using the corresponding singularity image):. ```. singularity exec \. --bind /data/ref/genomes/GRCh38:/genomes \. --bind /data/projects/HICF2_project/BAM:/bam_files \. /well/gel/HICF2/software/singularity/deepvariant-0.9.0.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS --ref=""/genomes/GCA_000001405.15_GRCh38_full_plus_hs38d1_analysis_set.fna"" \. --reads=""/bam_files/${bamfile}"" \. --output_vcf=""VCF/${sampleID}.vcf.gz"" \. --output_gvcf=""VCF/${sampleID}.g.vcf.gz"" \. --intermediate_results_dir=""tmp_data"" \. --num_shards=10. ```. I've uploaded the log files from the 3 runs if you want to take a look:. [deepvar_0.9.0_oldrun.log](https://github.com/google/deepvariant/files/5281370/deepvar_0.9.0_oldrun.log). [deepvar_1.0.0.log](https://github.com/google/deepvariant/files/5281371/deepvar_1.0.0.log). [deepvar_0.9.0.log](https://github.com/google/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:41,safety,test,test,41,"Hi. After your comment I've repeated the test using the same input BAM (18X WGS sequenced with 150bp paired reads), running deepvar 0.9.0 and 1.0.0 with singularity v.3.5.2-1.1.el7 and exactly the same command (`--num_shards=10`). The running time is now almost the same, much longer than my old test with deepvar 0.9.0 on a 30X WGS. . Below I've reported running time in min for make_example+call_variants+post_process:. - **v0.9.0**: 290+1494+281 = 2065min (~34h). - **v1.0.0**: 335+1487+300 = 2122min (~35h). - **v0.9.0 OLD TEST ON 30X WGS**: 198+456+82 = 736min (~12h). The number of variants for the 3 runs are:. - **v0.9.0**: 531371190 g.vcf.gz; 10784757 vcf.gz (4552313 PASS, 6232444 RefCall). - **v1.0.0**: 547491396 g.vcf.gz; 11892262 vcf.gz (4619350 PASS, 7272912 RefCall). - **v0.9.0 OLD TEST ON 30X WGS**: 213244705 g.vcf.gz; 9096927 vcf.gz (4661618 PASS, 4435309 RefCall). So, the first question is: are these running times expected? The running times you reported are much shorter it seems. Can it be that the running time increased from 12 to 34h just because of the lower coverage? . The exact command I've used is below (it is the same for v1.0.0 but using the corresponding singularity image):. ```. singularity exec \. --bind /data/ref/genomes/GRCh38:/genomes \. --bind /data/projects/HICF2_project/BAM:/bam_files \. /well/gel/HICF2/software/singularity/deepvariant-0.9.0.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS --ref=""/genomes/GCA_000001405.15_GRCh38_full_plus_hs38d1_analysis_set.fna"" \. --reads=""/bam_files/${bamfile}"" \. --output_vcf=""VCF/${sampleID}.vcf.gz"" \. --output_gvcf=""VCF/${sampleID}.g.vcf.gz"" \. --intermediate_results_dir=""tmp_data"" \. --num_shards=10. ```. I've uploaded the log files from the 3 runs if you want to take a look:. [deepvar_0.9.0_oldrun.log](https://github.com/google/deepvariant/files/5281370/deepvar_0.9.0_oldrun.log). [deepvar_1.0.0.log](https://github.com/google/deepvariant/files/5281371/deepvar_1.0.0.log). [deepvar_0.9",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:61,safety,input,input,61,"Hi. After your comment I've repeated the test using the same input BAM (18X WGS sequenced with 150bp paired reads), running deepvar 0.9.0 and 1.0.0 with singularity v.3.5.2-1.1.el7 and exactly the same command (`--num_shards=10`). The running time is now almost the same, much longer than my old test with deepvar 0.9.0 on a 30X WGS. . Below I've reported running time in min for make_example+call_variants+post_process:. - **v0.9.0**: 290+1494+281 = 2065min (~34h). - **v1.0.0**: 335+1487+300 = 2122min (~35h). - **v0.9.0 OLD TEST ON 30X WGS**: 198+456+82 = 736min (~12h). The number of variants for the 3 runs are:. - **v0.9.0**: 531371190 g.vcf.gz; 10784757 vcf.gz (4552313 PASS, 6232444 RefCall). - **v1.0.0**: 547491396 g.vcf.gz; 11892262 vcf.gz (4619350 PASS, 7272912 RefCall). - **v0.9.0 OLD TEST ON 30X WGS**: 213244705 g.vcf.gz; 9096927 vcf.gz (4661618 PASS, 4435309 RefCall). So, the first question is: are these running times expected? The running times you reported are much shorter it seems. Can it be that the running time increased from 12 to 34h just because of the lower coverage? . The exact command I've used is below (it is the same for v1.0.0 but using the corresponding singularity image):. ```. singularity exec \. --bind /data/ref/genomes/GRCh38:/genomes \. --bind /data/projects/HICF2_project/BAM:/bam_files \. /well/gel/HICF2/software/singularity/deepvariant-0.9.0.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS --ref=""/genomes/GCA_000001405.15_GRCh38_full_plus_hs38d1_analysis_set.fna"" \. --reads=""/bam_files/${bamfile}"" \. --output_vcf=""VCF/${sampleID}.vcf.gz"" \. --output_gvcf=""VCF/${sampleID}.g.vcf.gz"" \. --intermediate_results_dir=""tmp_data"" \. --num_shards=10. ```. I've uploaded the log files from the 3 runs if you want to take a look:. [deepvar_0.9.0_oldrun.log](https://github.com/google/deepvariant/files/5281370/deepvar_0.9.0_oldrun.log). [deepvar_1.0.0.log](https://github.com/google/deepvariant/files/5281371/deepvar_1.0.0.log). [deepvar_0.9",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:296,safety,test,test,296,"Hi. After your comment I've repeated the test using the same input BAM (18X WGS sequenced with 150bp paired reads), running deepvar 0.9.0 and 1.0.0 with singularity v.3.5.2-1.1.el7 and exactly the same command (`--num_shards=10`). The running time is now almost the same, much longer than my old test with deepvar 0.9.0 on a 30X WGS. . Below I've reported running time in min for make_example+call_variants+post_process:. - **v0.9.0**: 290+1494+281 = 2065min (~34h). - **v1.0.0**: 335+1487+300 = 2122min (~35h). - **v0.9.0 OLD TEST ON 30X WGS**: 198+456+82 = 736min (~12h). The number of variants for the 3 runs are:. - **v0.9.0**: 531371190 g.vcf.gz; 10784757 vcf.gz (4552313 PASS, 6232444 RefCall). - **v1.0.0**: 547491396 g.vcf.gz; 11892262 vcf.gz (4619350 PASS, 7272912 RefCall). - **v0.9.0 OLD TEST ON 30X WGS**: 213244705 g.vcf.gz; 9096927 vcf.gz (4661618 PASS, 4435309 RefCall). So, the first question is: are these running times expected? The running times you reported are much shorter it seems. Can it be that the running time increased from 12 to 34h just because of the lower coverage? . The exact command I've used is below (it is the same for v1.0.0 but using the corresponding singularity image):. ```. singularity exec \. --bind /data/ref/genomes/GRCh38:/genomes \. --bind /data/projects/HICF2_project/BAM:/bam_files \. /well/gel/HICF2/software/singularity/deepvariant-0.9.0.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS --ref=""/genomes/GCA_000001405.15_GRCh38_full_plus_hs38d1_analysis_set.fna"" \. --reads=""/bam_files/${bamfile}"" \. --output_vcf=""VCF/${sampleID}.vcf.gz"" \. --output_gvcf=""VCF/${sampleID}.g.vcf.gz"" \. --intermediate_results_dir=""tmp_data"" \. --num_shards=10. ```. I've uploaded the log files from the 3 runs if you want to take a look:. [deepvar_0.9.0_oldrun.log](https://github.com/google/deepvariant/files/5281370/deepvar_0.9.0_oldrun.log). [deepvar_1.0.0.log](https://github.com/google/deepvariant/files/5281371/deepvar_1.0.0.log). [deepvar_0.9",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:527,safety,TEST,TEST,527,"Hi. After your comment I've repeated the test using the same input BAM (18X WGS sequenced with 150bp paired reads), running deepvar 0.9.0 and 1.0.0 with singularity v.3.5.2-1.1.el7 and exactly the same command (`--num_shards=10`). The running time is now almost the same, much longer than my old test with deepvar 0.9.0 on a 30X WGS. . Below I've reported running time in min for make_example+call_variants+post_process:. - **v0.9.0**: 290+1494+281 = 2065min (~34h). - **v1.0.0**: 335+1487+300 = 2122min (~35h). - **v0.9.0 OLD TEST ON 30X WGS**: 198+456+82 = 736min (~12h). The number of variants for the 3 runs are:. - **v0.9.0**: 531371190 g.vcf.gz; 10784757 vcf.gz (4552313 PASS, 6232444 RefCall). - **v1.0.0**: 547491396 g.vcf.gz; 11892262 vcf.gz (4619350 PASS, 7272912 RefCall). - **v0.9.0 OLD TEST ON 30X WGS**: 213244705 g.vcf.gz; 9096927 vcf.gz (4661618 PASS, 4435309 RefCall). So, the first question is: are these running times expected? The running times you reported are much shorter it seems. Can it be that the running time increased from 12 to 34h just because of the lower coverage? . The exact command I've used is below (it is the same for v1.0.0 but using the corresponding singularity image):. ```. singularity exec \. --bind /data/ref/genomes/GRCh38:/genomes \. --bind /data/projects/HICF2_project/BAM:/bam_files \. /well/gel/HICF2/software/singularity/deepvariant-0.9.0.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS --ref=""/genomes/GCA_000001405.15_GRCh38_full_plus_hs38d1_analysis_set.fna"" \. --reads=""/bam_files/${bamfile}"" \. --output_vcf=""VCF/${sampleID}.vcf.gz"" \. --output_gvcf=""VCF/${sampleID}.g.vcf.gz"" \. --intermediate_results_dir=""tmp_data"" \. --num_shards=10. ```. I've uploaded the log files from the 3 runs if you want to take a look:. [deepvar_0.9.0_oldrun.log](https://github.com/google/deepvariant/files/5281370/deepvar_0.9.0_oldrun.log). [deepvar_1.0.0.log](https://github.com/google/deepvariant/files/5281371/deepvar_1.0.0.log). [deepvar_0.9",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:799,safety,TEST,TEST,799,"Hi. After your comment I've repeated the test using the same input BAM (18X WGS sequenced with 150bp paired reads), running deepvar 0.9.0 and 1.0.0 with singularity v.3.5.2-1.1.el7 and exactly the same command (`--num_shards=10`). The running time is now almost the same, much longer than my old test with deepvar 0.9.0 on a 30X WGS. . Below I've reported running time in min for make_example+call_variants+post_process:. - **v0.9.0**: 290+1494+281 = 2065min (~34h). - **v1.0.0**: 335+1487+300 = 2122min (~35h). - **v0.9.0 OLD TEST ON 30X WGS**: 198+456+82 = 736min (~12h). The number of variants for the 3 runs are:. - **v0.9.0**: 531371190 g.vcf.gz; 10784757 vcf.gz (4552313 PASS, 6232444 RefCall). - **v1.0.0**: 547491396 g.vcf.gz; 11892262 vcf.gz (4619350 PASS, 7272912 RefCall). - **v0.9.0 OLD TEST ON 30X WGS**: 213244705 g.vcf.gz; 9096927 vcf.gz (4661618 PASS, 4435309 RefCall). So, the first question is: are these running times expected? The running times you reported are much shorter it seems. Can it be that the running time increased from 12 to 34h just because of the lower coverage? . The exact command I've used is below (it is the same for v1.0.0 but using the corresponding singularity image):. ```. singularity exec \. --bind /data/ref/genomes/GRCh38:/genomes \. --bind /data/projects/HICF2_project/BAM:/bam_files \. /well/gel/HICF2/software/singularity/deepvariant-0.9.0.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS --ref=""/genomes/GCA_000001405.15_GRCh38_full_plus_hs38d1_analysis_set.fna"" \. --reads=""/bam_files/${bamfile}"" \. --output_vcf=""VCF/${sampleID}.vcf.gz"" \. --output_gvcf=""VCF/${sampleID}.g.vcf.gz"" \. --intermediate_results_dir=""tmp_data"" \. --num_shards=10. ```. I've uploaded the log files from the 3 runs if you want to take a look:. [deepvar_0.9.0_oldrun.log](https://github.com/google/deepvariant/files/5281370/deepvar_0.9.0_oldrun.log). [deepvar_1.0.0.log](https://github.com/google/deepvariant/files/5281371/deepvar_1.0.0.log). [deepvar_0.9",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:1735,safety,log,log,1735,"red reads), running deepvar 0.9.0 and 1.0.0 with singularity v.3.5.2-1.1.el7 and exactly the same command (`--num_shards=10`). The running time is now almost the same, much longer than my old test with deepvar 0.9.0 on a 30X WGS. . Below I've reported running time in min for make_example+call_variants+post_process:. - **v0.9.0**: 290+1494+281 = 2065min (~34h). - **v1.0.0**: 335+1487+300 = 2122min (~35h). - **v0.9.0 OLD TEST ON 30X WGS**: 198+456+82 = 736min (~12h). The number of variants for the 3 runs are:. - **v0.9.0**: 531371190 g.vcf.gz; 10784757 vcf.gz (4552313 PASS, 6232444 RefCall). - **v1.0.0**: 547491396 g.vcf.gz; 11892262 vcf.gz (4619350 PASS, 7272912 RefCall). - **v0.9.0 OLD TEST ON 30X WGS**: 213244705 g.vcf.gz; 9096927 vcf.gz (4661618 PASS, 4435309 RefCall). So, the first question is: are these running times expected? The running times you reported are much shorter it seems. Can it be that the running time increased from 12 to 34h just because of the lower coverage? . The exact command I've used is below (it is the same for v1.0.0 but using the corresponding singularity image):. ```. singularity exec \. --bind /data/ref/genomes/GRCh38:/genomes \. --bind /data/projects/HICF2_project/BAM:/bam_files \. /well/gel/HICF2/software/singularity/deepvariant-0.9.0.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS --ref=""/genomes/GCA_000001405.15_GRCh38_full_plus_hs38d1_analysis_set.fna"" \. --reads=""/bam_files/${bamfile}"" \. --output_vcf=""VCF/${sampleID}.vcf.gz"" \. --output_gvcf=""VCF/${sampleID}.g.vcf.gz"" \. --intermediate_results_dir=""tmp_data"" \. --num_shards=10. ```. I've uploaded the log files from the 3 runs if you want to take a look:. [deepvar_0.9.0_oldrun.log](https://github.com/google/deepvariant/files/5281370/deepvar_0.9.0_oldrun.log). [deepvar_1.0.0.log](https://github.com/google/deepvariant/files/5281371/deepvar_1.0.0.log). [deepvar_0.9.0.log](https://github.com/google/deepvariant/files/5281372/deepvar_0.9.0.log). Thanks for your support!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:1812,safety,log,log,1812,"red reads), running deepvar 0.9.0 and 1.0.0 with singularity v.3.5.2-1.1.el7 and exactly the same command (`--num_shards=10`). The running time is now almost the same, much longer than my old test with deepvar 0.9.0 on a 30X WGS. . Below I've reported running time in min for make_example+call_variants+post_process:. - **v0.9.0**: 290+1494+281 = 2065min (~34h). - **v1.0.0**: 335+1487+300 = 2122min (~35h). - **v0.9.0 OLD TEST ON 30X WGS**: 198+456+82 = 736min (~12h). The number of variants for the 3 runs are:. - **v0.9.0**: 531371190 g.vcf.gz; 10784757 vcf.gz (4552313 PASS, 6232444 RefCall). - **v1.0.0**: 547491396 g.vcf.gz; 11892262 vcf.gz (4619350 PASS, 7272912 RefCall). - **v0.9.0 OLD TEST ON 30X WGS**: 213244705 g.vcf.gz; 9096927 vcf.gz (4661618 PASS, 4435309 RefCall). So, the first question is: are these running times expected? The running times you reported are much shorter it seems. Can it be that the running time increased from 12 to 34h just because of the lower coverage? . The exact command I've used is below (it is the same for v1.0.0 but using the corresponding singularity image):. ```. singularity exec \. --bind /data/ref/genomes/GRCh38:/genomes \. --bind /data/projects/HICF2_project/BAM:/bam_files \. /well/gel/HICF2/software/singularity/deepvariant-0.9.0.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS --ref=""/genomes/GCA_000001405.15_GRCh38_full_plus_hs38d1_analysis_set.fna"" \. --reads=""/bam_files/${bamfile}"" \. --output_vcf=""VCF/${sampleID}.vcf.gz"" \. --output_gvcf=""VCF/${sampleID}.g.vcf.gz"" \. --intermediate_results_dir=""tmp_data"" \. --num_shards=10. ```. I've uploaded the log files from the 3 runs if you want to take a look:. [deepvar_0.9.0_oldrun.log](https://github.com/google/deepvariant/files/5281370/deepvar_0.9.0_oldrun.log). [deepvar_1.0.0.log](https://github.com/google/deepvariant/files/5281371/deepvar_1.0.0.log). [deepvar_0.9.0.log](https://github.com/google/deepvariant/files/5281372/deepvar_0.9.0.log). Thanks for your support!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:1890,safety,log,log,1890,"red reads), running deepvar 0.9.0 and 1.0.0 with singularity v.3.5.2-1.1.el7 and exactly the same command (`--num_shards=10`). The running time is now almost the same, much longer than my old test with deepvar 0.9.0 on a 30X WGS. . Below I've reported running time in min for make_example+call_variants+post_process:. - **v0.9.0**: 290+1494+281 = 2065min (~34h). - **v1.0.0**: 335+1487+300 = 2122min (~35h). - **v0.9.0 OLD TEST ON 30X WGS**: 198+456+82 = 736min (~12h). The number of variants for the 3 runs are:. - **v0.9.0**: 531371190 g.vcf.gz; 10784757 vcf.gz (4552313 PASS, 6232444 RefCall). - **v1.0.0**: 547491396 g.vcf.gz; 11892262 vcf.gz (4619350 PASS, 7272912 RefCall). - **v0.9.0 OLD TEST ON 30X WGS**: 213244705 g.vcf.gz; 9096927 vcf.gz (4661618 PASS, 4435309 RefCall). So, the first question is: are these running times expected? The running times you reported are much shorter it seems. Can it be that the running time increased from 12 to 34h just because of the lower coverage? . The exact command I've used is below (it is the same for v1.0.0 but using the corresponding singularity image):. ```. singularity exec \. --bind /data/ref/genomes/GRCh38:/genomes \. --bind /data/projects/HICF2_project/BAM:/bam_files \. /well/gel/HICF2/software/singularity/deepvariant-0.9.0.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS --ref=""/genomes/GCA_000001405.15_GRCh38_full_plus_hs38d1_analysis_set.fna"" \. --reads=""/bam_files/${bamfile}"" \. --output_vcf=""VCF/${sampleID}.vcf.gz"" \. --output_gvcf=""VCF/${sampleID}.g.vcf.gz"" \. --intermediate_results_dir=""tmp_data"" \. --num_shards=10. ```. I've uploaded the log files from the 3 runs if you want to take a look:. [deepvar_0.9.0_oldrun.log](https://github.com/google/deepvariant/files/5281370/deepvar_0.9.0_oldrun.log). [deepvar_1.0.0.log](https://github.com/google/deepvariant/files/5281371/deepvar_1.0.0.log). [deepvar_0.9.0.log](https://github.com/google/deepvariant/files/5281372/deepvar_0.9.0.log). Thanks for your support!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:1911,safety,log,log,1911,"red reads), running deepvar 0.9.0 and 1.0.0 with singularity v.3.5.2-1.1.el7 and exactly the same command (`--num_shards=10`). The running time is now almost the same, much longer than my old test with deepvar 0.9.0 on a 30X WGS. . Below I've reported running time in min for make_example+call_variants+post_process:. - **v0.9.0**: 290+1494+281 = 2065min (~34h). - **v1.0.0**: 335+1487+300 = 2122min (~35h). - **v0.9.0 OLD TEST ON 30X WGS**: 198+456+82 = 736min (~12h). The number of variants for the 3 runs are:. - **v0.9.0**: 531371190 g.vcf.gz; 10784757 vcf.gz (4552313 PASS, 6232444 RefCall). - **v1.0.0**: 547491396 g.vcf.gz; 11892262 vcf.gz (4619350 PASS, 7272912 RefCall). - **v0.9.0 OLD TEST ON 30X WGS**: 213244705 g.vcf.gz; 9096927 vcf.gz (4661618 PASS, 4435309 RefCall). So, the first question is: are these running times expected? The running times you reported are much shorter it seems. Can it be that the running time increased from 12 to 34h just because of the lower coverage? . The exact command I've used is below (it is the same for v1.0.0 but using the corresponding singularity image):. ```. singularity exec \. --bind /data/ref/genomes/GRCh38:/genomes \. --bind /data/projects/HICF2_project/BAM:/bam_files \. /well/gel/HICF2/software/singularity/deepvariant-0.9.0.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS --ref=""/genomes/GCA_000001405.15_GRCh38_full_plus_hs38d1_analysis_set.fna"" \. --reads=""/bam_files/${bamfile}"" \. --output_vcf=""VCF/${sampleID}.vcf.gz"" \. --output_gvcf=""VCF/${sampleID}.g.vcf.gz"" \. --intermediate_results_dir=""tmp_data"" \. --num_shards=10. ```. I've uploaded the log files from the 3 runs if you want to take a look:. [deepvar_0.9.0_oldrun.log](https://github.com/google/deepvariant/files/5281370/deepvar_0.9.0_oldrun.log). [deepvar_1.0.0.log](https://github.com/google/deepvariant/files/5281371/deepvar_1.0.0.log). [deepvar_0.9.0.log](https://github.com/google/deepvariant/files/5281372/deepvar_0.9.0.log). Thanks for your support!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:1982,safety,log,log,1982,"red reads), running deepvar 0.9.0 and 1.0.0 with singularity v.3.5.2-1.1.el7 and exactly the same command (`--num_shards=10`). The running time is now almost the same, much longer than my old test with deepvar 0.9.0 on a 30X WGS. . Below I've reported running time in min for make_example+call_variants+post_process:. - **v0.9.0**: 290+1494+281 = 2065min (~34h). - **v1.0.0**: 335+1487+300 = 2122min (~35h). - **v0.9.0 OLD TEST ON 30X WGS**: 198+456+82 = 736min (~12h). The number of variants for the 3 runs are:. - **v0.9.0**: 531371190 g.vcf.gz; 10784757 vcf.gz (4552313 PASS, 6232444 RefCall). - **v1.0.0**: 547491396 g.vcf.gz; 11892262 vcf.gz (4619350 PASS, 7272912 RefCall). - **v0.9.0 OLD TEST ON 30X WGS**: 213244705 g.vcf.gz; 9096927 vcf.gz (4661618 PASS, 4435309 RefCall). So, the first question is: are these running times expected? The running times you reported are much shorter it seems. Can it be that the running time increased from 12 to 34h just because of the lower coverage? . The exact command I've used is below (it is the same for v1.0.0 but using the corresponding singularity image):. ```. singularity exec \. --bind /data/ref/genomes/GRCh38:/genomes \. --bind /data/projects/HICF2_project/BAM:/bam_files \. /well/gel/HICF2/software/singularity/deepvariant-0.9.0.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS --ref=""/genomes/GCA_000001405.15_GRCh38_full_plus_hs38d1_analysis_set.fna"" \. --reads=""/bam_files/${bamfile}"" \. --output_vcf=""VCF/${sampleID}.vcf.gz"" \. --output_gvcf=""VCF/${sampleID}.g.vcf.gz"" \. --intermediate_results_dir=""tmp_data"" \. --num_shards=10. ```. I've uploaded the log files from the 3 runs if you want to take a look:. [deepvar_0.9.0_oldrun.log](https://github.com/google/deepvariant/files/5281370/deepvar_0.9.0_oldrun.log). [deepvar_1.0.0.log](https://github.com/google/deepvariant/files/5281371/deepvar_1.0.0.log). [deepvar_0.9.0.log](https://github.com/google/deepvariant/files/5281372/deepvar_0.9.0.log). Thanks for your support!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:2003,safety,log,log,2003,"red reads), running deepvar 0.9.0 and 1.0.0 with singularity v.3.5.2-1.1.el7 and exactly the same command (`--num_shards=10`). The running time is now almost the same, much longer than my old test with deepvar 0.9.0 on a 30X WGS. . Below I've reported running time in min for make_example+call_variants+post_process:. - **v0.9.0**: 290+1494+281 = 2065min (~34h). - **v1.0.0**: 335+1487+300 = 2122min (~35h). - **v0.9.0 OLD TEST ON 30X WGS**: 198+456+82 = 736min (~12h). The number of variants for the 3 runs are:. - **v0.9.0**: 531371190 g.vcf.gz; 10784757 vcf.gz (4552313 PASS, 6232444 RefCall). - **v1.0.0**: 547491396 g.vcf.gz; 11892262 vcf.gz (4619350 PASS, 7272912 RefCall). - **v0.9.0 OLD TEST ON 30X WGS**: 213244705 g.vcf.gz; 9096927 vcf.gz (4661618 PASS, 4435309 RefCall). So, the first question is: are these running times expected? The running times you reported are much shorter it seems. Can it be that the running time increased from 12 to 34h just because of the lower coverage? . The exact command I've used is below (it is the same for v1.0.0 but using the corresponding singularity image):. ```. singularity exec \. --bind /data/ref/genomes/GRCh38:/genomes \. --bind /data/projects/HICF2_project/BAM:/bam_files \. /well/gel/HICF2/software/singularity/deepvariant-0.9.0.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS --ref=""/genomes/GCA_000001405.15_GRCh38_full_plus_hs38d1_analysis_set.fna"" \. --reads=""/bam_files/${bamfile}"" \. --output_vcf=""VCF/${sampleID}.vcf.gz"" \. --output_gvcf=""VCF/${sampleID}.g.vcf.gz"" \. --intermediate_results_dir=""tmp_data"" \. --num_shards=10. ```. I've uploaded the log files from the 3 runs if you want to take a look:. [deepvar_0.9.0_oldrun.log](https://github.com/google/deepvariant/files/5281370/deepvar_0.9.0_oldrun.log). [deepvar_1.0.0.log](https://github.com/google/deepvariant/files/5281371/deepvar_1.0.0.log). [deepvar_0.9.0.log](https://github.com/google/deepvariant/files/5281372/deepvar_0.9.0.log). Thanks for your support!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:2074,safety,log,log,2074,"red reads), running deepvar 0.9.0 and 1.0.0 with singularity v.3.5.2-1.1.el7 and exactly the same command (`--num_shards=10`). The running time is now almost the same, much longer than my old test with deepvar 0.9.0 on a 30X WGS. . Below I've reported running time in min for make_example+call_variants+post_process:. - **v0.9.0**: 290+1494+281 = 2065min (~34h). - **v1.0.0**: 335+1487+300 = 2122min (~35h). - **v0.9.0 OLD TEST ON 30X WGS**: 198+456+82 = 736min (~12h). The number of variants for the 3 runs are:. - **v0.9.0**: 531371190 g.vcf.gz; 10784757 vcf.gz (4552313 PASS, 6232444 RefCall). - **v1.0.0**: 547491396 g.vcf.gz; 11892262 vcf.gz (4619350 PASS, 7272912 RefCall). - **v0.9.0 OLD TEST ON 30X WGS**: 213244705 g.vcf.gz; 9096927 vcf.gz (4661618 PASS, 4435309 RefCall). So, the first question is: are these running times expected? The running times you reported are much shorter it seems. Can it be that the running time increased from 12 to 34h just because of the lower coverage? . The exact command I've used is below (it is the same for v1.0.0 but using the corresponding singularity image):. ```. singularity exec \. --bind /data/ref/genomes/GRCh38:/genomes \. --bind /data/projects/HICF2_project/BAM:/bam_files \. /well/gel/HICF2/software/singularity/deepvariant-0.9.0.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS --ref=""/genomes/GCA_000001405.15_GRCh38_full_plus_hs38d1_analysis_set.fna"" \. --reads=""/bam_files/${bamfile}"" \. --output_vcf=""VCF/${sampleID}.vcf.gz"" \. --output_gvcf=""VCF/${sampleID}.g.vcf.gz"" \. --intermediate_results_dir=""tmp_data"" \. --num_shards=10. ```. I've uploaded the log files from the 3 runs if you want to take a look:. [deepvar_0.9.0_oldrun.log](https://github.com/google/deepvariant/files/5281370/deepvar_0.9.0_oldrun.log). [deepvar_1.0.0.log](https://github.com/google/deepvariant/files/5281371/deepvar_1.0.0.log). [deepvar_0.9.0.log](https://github.com/google/deepvariant/files/5281372/deepvar_0.9.0.log). Thanks for your support!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:1735,security,log,log,1735,"red reads), running deepvar 0.9.0 and 1.0.0 with singularity v.3.5.2-1.1.el7 and exactly the same command (`--num_shards=10`). The running time is now almost the same, much longer than my old test with deepvar 0.9.0 on a 30X WGS. . Below I've reported running time in min for make_example+call_variants+post_process:. - **v0.9.0**: 290+1494+281 = 2065min (~34h). - **v1.0.0**: 335+1487+300 = 2122min (~35h). - **v0.9.0 OLD TEST ON 30X WGS**: 198+456+82 = 736min (~12h). The number of variants for the 3 runs are:. - **v0.9.0**: 531371190 g.vcf.gz; 10784757 vcf.gz (4552313 PASS, 6232444 RefCall). - **v1.0.0**: 547491396 g.vcf.gz; 11892262 vcf.gz (4619350 PASS, 7272912 RefCall). - **v0.9.0 OLD TEST ON 30X WGS**: 213244705 g.vcf.gz; 9096927 vcf.gz (4661618 PASS, 4435309 RefCall). So, the first question is: are these running times expected? The running times you reported are much shorter it seems. Can it be that the running time increased from 12 to 34h just because of the lower coverage? . The exact command I've used is below (it is the same for v1.0.0 but using the corresponding singularity image):. ```. singularity exec \. --bind /data/ref/genomes/GRCh38:/genomes \. --bind /data/projects/HICF2_project/BAM:/bam_files \. /well/gel/HICF2/software/singularity/deepvariant-0.9.0.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS --ref=""/genomes/GCA_000001405.15_GRCh38_full_plus_hs38d1_analysis_set.fna"" \. --reads=""/bam_files/${bamfile}"" \. --output_vcf=""VCF/${sampleID}.vcf.gz"" \. --output_gvcf=""VCF/${sampleID}.g.vcf.gz"" \. --intermediate_results_dir=""tmp_data"" \. --num_shards=10. ```. I've uploaded the log files from the 3 runs if you want to take a look:. [deepvar_0.9.0_oldrun.log](https://github.com/google/deepvariant/files/5281370/deepvar_0.9.0_oldrun.log). [deepvar_1.0.0.log](https://github.com/google/deepvariant/files/5281371/deepvar_1.0.0.log). [deepvar_0.9.0.log](https://github.com/google/deepvariant/files/5281372/deepvar_0.9.0.log). Thanks for your support!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:1812,security,log,log,1812,"red reads), running deepvar 0.9.0 and 1.0.0 with singularity v.3.5.2-1.1.el7 and exactly the same command (`--num_shards=10`). The running time is now almost the same, much longer than my old test with deepvar 0.9.0 on a 30X WGS. . Below I've reported running time in min for make_example+call_variants+post_process:. - **v0.9.0**: 290+1494+281 = 2065min (~34h). - **v1.0.0**: 335+1487+300 = 2122min (~35h). - **v0.9.0 OLD TEST ON 30X WGS**: 198+456+82 = 736min (~12h). The number of variants for the 3 runs are:. - **v0.9.0**: 531371190 g.vcf.gz; 10784757 vcf.gz (4552313 PASS, 6232444 RefCall). - **v1.0.0**: 547491396 g.vcf.gz; 11892262 vcf.gz (4619350 PASS, 7272912 RefCall). - **v0.9.0 OLD TEST ON 30X WGS**: 213244705 g.vcf.gz; 9096927 vcf.gz (4661618 PASS, 4435309 RefCall). So, the first question is: are these running times expected? The running times you reported are much shorter it seems. Can it be that the running time increased from 12 to 34h just because of the lower coverage? . The exact command I've used is below (it is the same for v1.0.0 but using the corresponding singularity image):. ```. singularity exec \. --bind /data/ref/genomes/GRCh38:/genomes \. --bind /data/projects/HICF2_project/BAM:/bam_files \. /well/gel/HICF2/software/singularity/deepvariant-0.9.0.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS --ref=""/genomes/GCA_000001405.15_GRCh38_full_plus_hs38d1_analysis_set.fna"" \. --reads=""/bam_files/${bamfile}"" \. --output_vcf=""VCF/${sampleID}.vcf.gz"" \. --output_gvcf=""VCF/${sampleID}.g.vcf.gz"" \. --intermediate_results_dir=""tmp_data"" \. --num_shards=10. ```. I've uploaded the log files from the 3 runs if you want to take a look:. [deepvar_0.9.0_oldrun.log](https://github.com/google/deepvariant/files/5281370/deepvar_0.9.0_oldrun.log). [deepvar_1.0.0.log](https://github.com/google/deepvariant/files/5281371/deepvar_1.0.0.log). [deepvar_0.9.0.log](https://github.com/google/deepvariant/files/5281372/deepvar_0.9.0.log). Thanks for your support!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:1890,security,log,log,1890,"red reads), running deepvar 0.9.0 and 1.0.0 with singularity v.3.5.2-1.1.el7 and exactly the same command (`--num_shards=10`). The running time is now almost the same, much longer than my old test with deepvar 0.9.0 on a 30X WGS. . Below I've reported running time in min for make_example+call_variants+post_process:. - **v0.9.0**: 290+1494+281 = 2065min (~34h). - **v1.0.0**: 335+1487+300 = 2122min (~35h). - **v0.9.0 OLD TEST ON 30X WGS**: 198+456+82 = 736min (~12h). The number of variants for the 3 runs are:. - **v0.9.0**: 531371190 g.vcf.gz; 10784757 vcf.gz (4552313 PASS, 6232444 RefCall). - **v1.0.0**: 547491396 g.vcf.gz; 11892262 vcf.gz (4619350 PASS, 7272912 RefCall). - **v0.9.0 OLD TEST ON 30X WGS**: 213244705 g.vcf.gz; 9096927 vcf.gz (4661618 PASS, 4435309 RefCall). So, the first question is: are these running times expected? The running times you reported are much shorter it seems. Can it be that the running time increased from 12 to 34h just because of the lower coverage? . The exact command I've used is below (it is the same for v1.0.0 but using the corresponding singularity image):. ```. singularity exec \. --bind /data/ref/genomes/GRCh38:/genomes \. --bind /data/projects/HICF2_project/BAM:/bam_files \. /well/gel/HICF2/software/singularity/deepvariant-0.9.0.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS --ref=""/genomes/GCA_000001405.15_GRCh38_full_plus_hs38d1_analysis_set.fna"" \. --reads=""/bam_files/${bamfile}"" \. --output_vcf=""VCF/${sampleID}.vcf.gz"" \. --output_gvcf=""VCF/${sampleID}.g.vcf.gz"" \. --intermediate_results_dir=""tmp_data"" \. --num_shards=10. ```. I've uploaded the log files from the 3 runs if you want to take a look:. [deepvar_0.9.0_oldrun.log](https://github.com/google/deepvariant/files/5281370/deepvar_0.9.0_oldrun.log). [deepvar_1.0.0.log](https://github.com/google/deepvariant/files/5281371/deepvar_1.0.0.log). [deepvar_0.9.0.log](https://github.com/google/deepvariant/files/5281372/deepvar_0.9.0.log). Thanks for your support!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:1911,security,log,log,1911,"red reads), running deepvar 0.9.0 and 1.0.0 with singularity v.3.5.2-1.1.el7 and exactly the same command (`--num_shards=10`). The running time is now almost the same, much longer than my old test with deepvar 0.9.0 on a 30X WGS. . Below I've reported running time in min for make_example+call_variants+post_process:. - **v0.9.0**: 290+1494+281 = 2065min (~34h). - **v1.0.0**: 335+1487+300 = 2122min (~35h). - **v0.9.0 OLD TEST ON 30X WGS**: 198+456+82 = 736min (~12h). The number of variants for the 3 runs are:. - **v0.9.0**: 531371190 g.vcf.gz; 10784757 vcf.gz (4552313 PASS, 6232444 RefCall). - **v1.0.0**: 547491396 g.vcf.gz; 11892262 vcf.gz (4619350 PASS, 7272912 RefCall). - **v0.9.0 OLD TEST ON 30X WGS**: 213244705 g.vcf.gz; 9096927 vcf.gz (4661618 PASS, 4435309 RefCall). So, the first question is: are these running times expected? The running times you reported are much shorter it seems. Can it be that the running time increased from 12 to 34h just because of the lower coverage? . The exact command I've used is below (it is the same for v1.0.0 but using the corresponding singularity image):. ```. singularity exec \. --bind /data/ref/genomes/GRCh38:/genomes \. --bind /data/projects/HICF2_project/BAM:/bam_files \. /well/gel/HICF2/software/singularity/deepvariant-0.9.0.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS --ref=""/genomes/GCA_000001405.15_GRCh38_full_plus_hs38d1_analysis_set.fna"" \. --reads=""/bam_files/${bamfile}"" \. --output_vcf=""VCF/${sampleID}.vcf.gz"" \. --output_gvcf=""VCF/${sampleID}.g.vcf.gz"" \. --intermediate_results_dir=""tmp_data"" \. --num_shards=10. ```. I've uploaded the log files from the 3 runs if you want to take a look:. [deepvar_0.9.0_oldrun.log](https://github.com/google/deepvariant/files/5281370/deepvar_0.9.0_oldrun.log). [deepvar_1.0.0.log](https://github.com/google/deepvariant/files/5281371/deepvar_1.0.0.log). [deepvar_0.9.0.log](https://github.com/google/deepvariant/files/5281372/deepvar_0.9.0.log). Thanks for your support!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:1982,security,log,log,1982,"red reads), running deepvar 0.9.0 and 1.0.0 with singularity v.3.5.2-1.1.el7 and exactly the same command (`--num_shards=10`). The running time is now almost the same, much longer than my old test with deepvar 0.9.0 on a 30X WGS. . Below I've reported running time in min for make_example+call_variants+post_process:. - **v0.9.0**: 290+1494+281 = 2065min (~34h). - **v1.0.0**: 335+1487+300 = 2122min (~35h). - **v0.9.0 OLD TEST ON 30X WGS**: 198+456+82 = 736min (~12h). The number of variants for the 3 runs are:. - **v0.9.0**: 531371190 g.vcf.gz; 10784757 vcf.gz (4552313 PASS, 6232444 RefCall). - **v1.0.0**: 547491396 g.vcf.gz; 11892262 vcf.gz (4619350 PASS, 7272912 RefCall). - **v0.9.0 OLD TEST ON 30X WGS**: 213244705 g.vcf.gz; 9096927 vcf.gz (4661618 PASS, 4435309 RefCall). So, the first question is: are these running times expected? The running times you reported are much shorter it seems. Can it be that the running time increased from 12 to 34h just because of the lower coverage? . The exact command I've used is below (it is the same for v1.0.0 but using the corresponding singularity image):. ```. singularity exec \. --bind /data/ref/genomes/GRCh38:/genomes \. --bind /data/projects/HICF2_project/BAM:/bam_files \. /well/gel/HICF2/software/singularity/deepvariant-0.9.0.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS --ref=""/genomes/GCA_000001405.15_GRCh38_full_plus_hs38d1_analysis_set.fna"" \. --reads=""/bam_files/${bamfile}"" \. --output_vcf=""VCF/${sampleID}.vcf.gz"" \. --output_gvcf=""VCF/${sampleID}.g.vcf.gz"" \. --intermediate_results_dir=""tmp_data"" \. --num_shards=10. ```. I've uploaded the log files from the 3 runs if you want to take a look:. [deepvar_0.9.0_oldrun.log](https://github.com/google/deepvariant/files/5281370/deepvar_0.9.0_oldrun.log). [deepvar_1.0.0.log](https://github.com/google/deepvariant/files/5281371/deepvar_1.0.0.log). [deepvar_0.9.0.log](https://github.com/google/deepvariant/files/5281372/deepvar_0.9.0.log). Thanks for your support!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:2003,security,log,log,2003,"red reads), running deepvar 0.9.0 and 1.0.0 with singularity v.3.5.2-1.1.el7 and exactly the same command (`--num_shards=10`). The running time is now almost the same, much longer than my old test with deepvar 0.9.0 on a 30X WGS. . Below I've reported running time in min for make_example+call_variants+post_process:. - **v0.9.0**: 290+1494+281 = 2065min (~34h). - **v1.0.0**: 335+1487+300 = 2122min (~35h). - **v0.9.0 OLD TEST ON 30X WGS**: 198+456+82 = 736min (~12h). The number of variants for the 3 runs are:. - **v0.9.0**: 531371190 g.vcf.gz; 10784757 vcf.gz (4552313 PASS, 6232444 RefCall). - **v1.0.0**: 547491396 g.vcf.gz; 11892262 vcf.gz (4619350 PASS, 7272912 RefCall). - **v0.9.0 OLD TEST ON 30X WGS**: 213244705 g.vcf.gz; 9096927 vcf.gz (4661618 PASS, 4435309 RefCall). So, the first question is: are these running times expected? The running times you reported are much shorter it seems. Can it be that the running time increased from 12 to 34h just because of the lower coverage? . The exact command I've used is below (it is the same for v1.0.0 but using the corresponding singularity image):. ```. singularity exec \. --bind /data/ref/genomes/GRCh38:/genomes \. --bind /data/projects/HICF2_project/BAM:/bam_files \. /well/gel/HICF2/software/singularity/deepvariant-0.9.0.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS --ref=""/genomes/GCA_000001405.15_GRCh38_full_plus_hs38d1_analysis_set.fna"" \. --reads=""/bam_files/${bamfile}"" \. --output_vcf=""VCF/${sampleID}.vcf.gz"" \. --output_gvcf=""VCF/${sampleID}.g.vcf.gz"" \. --intermediate_results_dir=""tmp_data"" \. --num_shards=10. ```. I've uploaded the log files from the 3 runs if you want to take a look:. [deepvar_0.9.0_oldrun.log](https://github.com/google/deepvariant/files/5281370/deepvar_0.9.0_oldrun.log). [deepvar_1.0.0.log](https://github.com/google/deepvariant/files/5281371/deepvar_1.0.0.log). [deepvar_0.9.0.log](https://github.com/google/deepvariant/files/5281372/deepvar_0.9.0.log). Thanks for your support!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:2074,security,log,log,2074,"red reads), running deepvar 0.9.0 and 1.0.0 with singularity v.3.5.2-1.1.el7 and exactly the same command (`--num_shards=10`). The running time is now almost the same, much longer than my old test with deepvar 0.9.0 on a 30X WGS. . Below I've reported running time in min for make_example+call_variants+post_process:. - **v0.9.0**: 290+1494+281 = 2065min (~34h). - **v1.0.0**: 335+1487+300 = 2122min (~35h). - **v0.9.0 OLD TEST ON 30X WGS**: 198+456+82 = 736min (~12h). The number of variants for the 3 runs are:. - **v0.9.0**: 531371190 g.vcf.gz; 10784757 vcf.gz (4552313 PASS, 6232444 RefCall). - **v1.0.0**: 547491396 g.vcf.gz; 11892262 vcf.gz (4619350 PASS, 7272912 RefCall). - **v0.9.0 OLD TEST ON 30X WGS**: 213244705 g.vcf.gz; 9096927 vcf.gz (4661618 PASS, 4435309 RefCall). So, the first question is: are these running times expected? The running times you reported are much shorter it seems. Can it be that the running time increased from 12 to 34h just because of the lower coverage? . The exact command I've used is below (it is the same for v1.0.0 but using the corresponding singularity image):. ```. singularity exec \. --bind /data/ref/genomes/GRCh38:/genomes \. --bind /data/projects/HICF2_project/BAM:/bam_files \. /well/gel/HICF2/software/singularity/deepvariant-0.9.0.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS --ref=""/genomes/GCA_000001405.15_GRCh38_full_plus_hs38d1_analysis_set.fna"" \. --reads=""/bam_files/${bamfile}"" \. --output_vcf=""VCF/${sampleID}.vcf.gz"" \. --output_gvcf=""VCF/${sampleID}.g.vcf.gz"" \. --intermediate_results_dir=""tmp_data"" \. --num_shards=10. ```. I've uploaded the log files from the 3 runs if you want to take a look:. [deepvar_0.9.0_oldrun.log](https://github.com/google/deepvariant/files/5281370/deepvar_0.9.0_oldrun.log). [deepvar_1.0.0.log](https://github.com/google/deepvariant/files/5281371/deepvar_1.0.0.log). [deepvar_0.9.0.log](https://github.com/google/deepvariant/files/5281372/deepvar_0.9.0.log). Thanks for your support!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:41,testability,test,test,41,"Hi. After your comment I've repeated the test using the same input BAM (18X WGS sequenced with 150bp paired reads), running deepvar 0.9.0 and 1.0.0 with singularity v.3.5.2-1.1.el7 and exactly the same command (`--num_shards=10`). The running time is now almost the same, much longer than my old test with deepvar 0.9.0 on a 30X WGS. . Below I've reported running time in min for make_example+call_variants+post_process:. - **v0.9.0**: 290+1494+281 = 2065min (~34h). - **v1.0.0**: 335+1487+300 = 2122min (~35h). - **v0.9.0 OLD TEST ON 30X WGS**: 198+456+82 = 736min (~12h). The number of variants for the 3 runs are:. - **v0.9.0**: 531371190 g.vcf.gz; 10784757 vcf.gz (4552313 PASS, 6232444 RefCall). - **v1.0.0**: 547491396 g.vcf.gz; 11892262 vcf.gz (4619350 PASS, 7272912 RefCall). - **v0.9.0 OLD TEST ON 30X WGS**: 213244705 g.vcf.gz; 9096927 vcf.gz (4661618 PASS, 4435309 RefCall). So, the first question is: are these running times expected? The running times you reported are much shorter it seems. Can it be that the running time increased from 12 to 34h just because of the lower coverage? . The exact command I've used is below (it is the same for v1.0.0 but using the corresponding singularity image):. ```. singularity exec \. --bind /data/ref/genomes/GRCh38:/genomes \. --bind /data/projects/HICF2_project/BAM:/bam_files \. /well/gel/HICF2/software/singularity/deepvariant-0.9.0.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS --ref=""/genomes/GCA_000001405.15_GRCh38_full_plus_hs38d1_analysis_set.fna"" \. --reads=""/bam_files/${bamfile}"" \. --output_vcf=""VCF/${sampleID}.vcf.gz"" \. --output_gvcf=""VCF/${sampleID}.g.vcf.gz"" \. --intermediate_results_dir=""tmp_data"" \. --num_shards=10. ```. I've uploaded the log files from the 3 runs if you want to take a look:. [deepvar_0.9.0_oldrun.log](https://github.com/google/deepvariant/files/5281370/deepvar_0.9.0_oldrun.log). [deepvar_1.0.0.log](https://github.com/google/deepvariant/files/5281371/deepvar_1.0.0.log). [deepvar_0.9",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:296,testability,test,test,296,"Hi. After your comment I've repeated the test using the same input BAM (18X WGS sequenced with 150bp paired reads), running deepvar 0.9.0 and 1.0.0 with singularity v.3.5.2-1.1.el7 and exactly the same command (`--num_shards=10`). The running time is now almost the same, much longer than my old test with deepvar 0.9.0 on a 30X WGS. . Below I've reported running time in min for make_example+call_variants+post_process:. - **v0.9.0**: 290+1494+281 = 2065min (~34h). - **v1.0.0**: 335+1487+300 = 2122min (~35h). - **v0.9.0 OLD TEST ON 30X WGS**: 198+456+82 = 736min (~12h). The number of variants for the 3 runs are:. - **v0.9.0**: 531371190 g.vcf.gz; 10784757 vcf.gz (4552313 PASS, 6232444 RefCall). - **v1.0.0**: 547491396 g.vcf.gz; 11892262 vcf.gz (4619350 PASS, 7272912 RefCall). - **v0.9.0 OLD TEST ON 30X WGS**: 213244705 g.vcf.gz; 9096927 vcf.gz (4661618 PASS, 4435309 RefCall). So, the first question is: are these running times expected? The running times you reported are much shorter it seems. Can it be that the running time increased from 12 to 34h just because of the lower coverage? . The exact command I've used is below (it is the same for v1.0.0 but using the corresponding singularity image):. ```. singularity exec \. --bind /data/ref/genomes/GRCh38:/genomes \. --bind /data/projects/HICF2_project/BAM:/bam_files \. /well/gel/HICF2/software/singularity/deepvariant-0.9.0.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS --ref=""/genomes/GCA_000001405.15_GRCh38_full_plus_hs38d1_analysis_set.fna"" \. --reads=""/bam_files/${bamfile}"" \. --output_vcf=""VCF/${sampleID}.vcf.gz"" \. --output_gvcf=""VCF/${sampleID}.g.vcf.gz"" \. --intermediate_results_dir=""tmp_data"" \. --num_shards=10. ```. I've uploaded the log files from the 3 runs if you want to take a look:. [deepvar_0.9.0_oldrun.log](https://github.com/google/deepvariant/files/5281370/deepvar_0.9.0_oldrun.log). [deepvar_1.0.0.log](https://github.com/google/deepvariant/files/5281371/deepvar_1.0.0.log). [deepvar_0.9",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:527,testability,TEST,TEST,527,"Hi. After your comment I've repeated the test using the same input BAM (18X WGS sequenced with 150bp paired reads), running deepvar 0.9.0 and 1.0.0 with singularity v.3.5.2-1.1.el7 and exactly the same command (`--num_shards=10`). The running time is now almost the same, much longer than my old test with deepvar 0.9.0 on a 30X WGS. . Below I've reported running time in min for make_example+call_variants+post_process:. - **v0.9.0**: 290+1494+281 = 2065min (~34h). - **v1.0.0**: 335+1487+300 = 2122min (~35h). - **v0.9.0 OLD TEST ON 30X WGS**: 198+456+82 = 736min (~12h). The number of variants for the 3 runs are:. - **v0.9.0**: 531371190 g.vcf.gz; 10784757 vcf.gz (4552313 PASS, 6232444 RefCall). - **v1.0.0**: 547491396 g.vcf.gz; 11892262 vcf.gz (4619350 PASS, 7272912 RefCall). - **v0.9.0 OLD TEST ON 30X WGS**: 213244705 g.vcf.gz; 9096927 vcf.gz (4661618 PASS, 4435309 RefCall). So, the first question is: are these running times expected? The running times you reported are much shorter it seems. Can it be that the running time increased from 12 to 34h just because of the lower coverage? . The exact command I've used is below (it is the same for v1.0.0 but using the corresponding singularity image):. ```. singularity exec \. --bind /data/ref/genomes/GRCh38:/genomes \. --bind /data/projects/HICF2_project/BAM:/bam_files \. /well/gel/HICF2/software/singularity/deepvariant-0.9.0.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS --ref=""/genomes/GCA_000001405.15_GRCh38_full_plus_hs38d1_analysis_set.fna"" \. --reads=""/bam_files/${bamfile}"" \. --output_vcf=""VCF/${sampleID}.vcf.gz"" \. --output_gvcf=""VCF/${sampleID}.g.vcf.gz"" \. --intermediate_results_dir=""tmp_data"" \. --num_shards=10. ```. I've uploaded the log files from the 3 runs if you want to take a look:. [deepvar_0.9.0_oldrun.log](https://github.com/google/deepvariant/files/5281370/deepvar_0.9.0_oldrun.log). [deepvar_1.0.0.log](https://github.com/google/deepvariant/files/5281371/deepvar_1.0.0.log). [deepvar_0.9",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:799,testability,TEST,TEST,799,"Hi. After your comment I've repeated the test using the same input BAM (18X WGS sequenced with 150bp paired reads), running deepvar 0.9.0 and 1.0.0 with singularity v.3.5.2-1.1.el7 and exactly the same command (`--num_shards=10`). The running time is now almost the same, much longer than my old test with deepvar 0.9.0 on a 30X WGS. . Below I've reported running time in min for make_example+call_variants+post_process:. - **v0.9.0**: 290+1494+281 = 2065min (~34h). - **v1.0.0**: 335+1487+300 = 2122min (~35h). - **v0.9.0 OLD TEST ON 30X WGS**: 198+456+82 = 736min (~12h). The number of variants for the 3 runs are:. - **v0.9.0**: 531371190 g.vcf.gz; 10784757 vcf.gz (4552313 PASS, 6232444 RefCall). - **v1.0.0**: 547491396 g.vcf.gz; 11892262 vcf.gz (4619350 PASS, 7272912 RefCall). - **v0.9.0 OLD TEST ON 30X WGS**: 213244705 g.vcf.gz; 9096927 vcf.gz (4661618 PASS, 4435309 RefCall). So, the first question is: are these running times expected? The running times you reported are much shorter it seems. Can it be that the running time increased from 12 to 34h just because of the lower coverage? . The exact command I've used is below (it is the same for v1.0.0 but using the corresponding singularity image):. ```. singularity exec \. --bind /data/ref/genomes/GRCh38:/genomes \. --bind /data/projects/HICF2_project/BAM:/bam_files \. /well/gel/HICF2/software/singularity/deepvariant-0.9.0.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS --ref=""/genomes/GCA_000001405.15_GRCh38_full_plus_hs38d1_analysis_set.fna"" \. --reads=""/bam_files/${bamfile}"" \. --output_vcf=""VCF/${sampleID}.vcf.gz"" \. --output_gvcf=""VCF/${sampleID}.g.vcf.gz"" \. --intermediate_results_dir=""tmp_data"" \. --num_shards=10. ```. I've uploaded the log files from the 3 runs if you want to take a look:. [deepvar_0.9.0_oldrun.log](https://github.com/google/deepvariant/files/5281370/deepvar_0.9.0_oldrun.log). [deepvar_1.0.0.log](https://github.com/google/deepvariant/files/5281371/deepvar_1.0.0.log). [deepvar_0.9",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:1088,testability,coverag,coverage,1088,"th 150bp paired reads), running deepvar 0.9.0 and 1.0.0 with singularity v.3.5.2-1.1.el7 and exactly the same command (`--num_shards=10`). The running time is now almost the same, much longer than my old test with deepvar 0.9.0 on a 30X WGS. . Below I've reported running time in min for make_example+call_variants+post_process:. - **v0.9.0**: 290+1494+281 = 2065min (~34h). - **v1.0.0**: 335+1487+300 = 2122min (~35h). - **v0.9.0 OLD TEST ON 30X WGS**: 198+456+82 = 736min (~12h). The number of variants for the 3 runs are:. - **v0.9.0**: 531371190 g.vcf.gz; 10784757 vcf.gz (4552313 PASS, 6232444 RefCall). - **v1.0.0**: 547491396 g.vcf.gz; 11892262 vcf.gz (4619350 PASS, 7272912 RefCall). - **v0.9.0 OLD TEST ON 30X WGS**: 213244705 g.vcf.gz; 9096927 vcf.gz (4661618 PASS, 4435309 RefCall). So, the first question is: are these running times expected? The running times you reported are much shorter it seems. Can it be that the running time increased from 12 to 34h just because of the lower coverage? . The exact command I've used is below (it is the same for v1.0.0 but using the corresponding singularity image):. ```. singularity exec \. --bind /data/ref/genomes/GRCh38:/genomes \. --bind /data/projects/HICF2_project/BAM:/bam_files \. /well/gel/HICF2/software/singularity/deepvariant-0.9.0.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS --ref=""/genomes/GCA_000001405.15_GRCh38_full_plus_hs38d1_analysis_set.fna"" \. --reads=""/bam_files/${bamfile}"" \. --output_vcf=""VCF/${sampleID}.vcf.gz"" \. --output_gvcf=""VCF/${sampleID}.g.vcf.gz"" \. --intermediate_results_dir=""tmp_data"" \. --num_shards=10. ```. I've uploaded the log files from the 3 runs if you want to take a look:. [deepvar_0.9.0_oldrun.log](https://github.com/google/deepvariant/files/5281370/deepvar_0.9.0_oldrun.log). [deepvar_1.0.0.log](https://github.com/google/deepvariant/files/5281371/deepvar_1.0.0.log). [deepvar_0.9.0.log](https://github.com/google/deepvariant/files/5281372/deepvar_0.9.0.log). Thanks for y",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:1735,testability,log,log,1735,"red reads), running deepvar 0.9.0 and 1.0.0 with singularity v.3.5.2-1.1.el7 and exactly the same command (`--num_shards=10`). The running time is now almost the same, much longer than my old test with deepvar 0.9.0 on a 30X WGS. . Below I've reported running time in min for make_example+call_variants+post_process:. - **v0.9.0**: 290+1494+281 = 2065min (~34h). - **v1.0.0**: 335+1487+300 = 2122min (~35h). - **v0.9.0 OLD TEST ON 30X WGS**: 198+456+82 = 736min (~12h). The number of variants for the 3 runs are:. - **v0.9.0**: 531371190 g.vcf.gz; 10784757 vcf.gz (4552313 PASS, 6232444 RefCall). - **v1.0.0**: 547491396 g.vcf.gz; 11892262 vcf.gz (4619350 PASS, 7272912 RefCall). - **v0.9.0 OLD TEST ON 30X WGS**: 213244705 g.vcf.gz; 9096927 vcf.gz (4661618 PASS, 4435309 RefCall). So, the first question is: are these running times expected? The running times you reported are much shorter it seems. Can it be that the running time increased from 12 to 34h just because of the lower coverage? . The exact command I've used is below (it is the same for v1.0.0 but using the corresponding singularity image):. ```. singularity exec \. --bind /data/ref/genomes/GRCh38:/genomes \. --bind /data/projects/HICF2_project/BAM:/bam_files \. /well/gel/HICF2/software/singularity/deepvariant-0.9.0.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS --ref=""/genomes/GCA_000001405.15_GRCh38_full_plus_hs38d1_analysis_set.fna"" \. --reads=""/bam_files/${bamfile}"" \. --output_vcf=""VCF/${sampleID}.vcf.gz"" \. --output_gvcf=""VCF/${sampleID}.g.vcf.gz"" \. --intermediate_results_dir=""tmp_data"" \. --num_shards=10. ```. I've uploaded the log files from the 3 runs if you want to take a look:. [deepvar_0.9.0_oldrun.log](https://github.com/google/deepvariant/files/5281370/deepvar_0.9.0_oldrun.log). [deepvar_1.0.0.log](https://github.com/google/deepvariant/files/5281371/deepvar_1.0.0.log). [deepvar_0.9.0.log](https://github.com/google/deepvariant/files/5281372/deepvar_0.9.0.log). Thanks for your support!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:1812,testability,log,log,1812,"red reads), running deepvar 0.9.0 and 1.0.0 with singularity v.3.5.2-1.1.el7 and exactly the same command (`--num_shards=10`). The running time is now almost the same, much longer than my old test with deepvar 0.9.0 on a 30X WGS. . Below I've reported running time in min for make_example+call_variants+post_process:. - **v0.9.0**: 290+1494+281 = 2065min (~34h). - **v1.0.0**: 335+1487+300 = 2122min (~35h). - **v0.9.0 OLD TEST ON 30X WGS**: 198+456+82 = 736min (~12h). The number of variants for the 3 runs are:. - **v0.9.0**: 531371190 g.vcf.gz; 10784757 vcf.gz (4552313 PASS, 6232444 RefCall). - **v1.0.0**: 547491396 g.vcf.gz; 11892262 vcf.gz (4619350 PASS, 7272912 RefCall). - **v0.9.0 OLD TEST ON 30X WGS**: 213244705 g.vcf.gz; 9096927 vcf.gz (4661618 PASS, 4435309 RefCall). So, the first question is: are these running times expected? The running times you reported are much shorter it seems. Can it be that the running time increased from 12 to 34h just because of the lower coverage? . The exact command I've used is below (it is the same for v1.0.0 but using the corresponding singularity image):. ```. singularity exec \. --bind /data/ref/genomes/GRCh38:/genomes \. --bind /data/projects/HICF2_project/BAM:/bam_files \. /well/gel/HICF2/software/singularity/deepvariant-0.9.0.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS --ref=""/genomes/GCA_000001405.15_GRCh38_full_plus_hs38d1_analysis_set.fna"" \. --reads=""/bam_files/${bamfile}"" \. --output_vcf=""VCF/${sampleID}.vcf.gz"" \. --output_gvcf=""VCF/${sampleID}.g.vcf.gz"" \. --intermediate_results_dir=""tmp_data"" \. --num_shards=10. ```. I've uploaded the log files from the 3 runs if you want to take a look:. [deepvar_0.9.0_oldrun.log](https://github.com/google/deepvariant/files/5281370/deepvar_0.9.0_oldrun.log). [deepvar_1.0.0.log](https://github.com/google/deepvariant/files/5281371/deepvar_1.0.0.log). [deepvar_0.9.0.log](https://github.com/google/deepvariant/files/5281372/deepvar_0.9.0.log). Thanks for your support!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:1890,testability,log,log,1890,"red reads), running deepvar 0.9.0 and 1.0.0 with singularity v.3.5.2-1.1.el7 and exactly the same command (`--num_shards=10`). The running time is now almost the same, much longer than my old test with deepvar 0.9.0 on a 30X WGS. . Below I've reported running time in min for make_example+call_variants+post_process:. - **v0.9.0**: 290+1494+281 = 2065min (~34h). - **v1.0.0**: 335+1487+300 = 2122min (~35h). - **v0.9.0 OLD TEST ON 30X WGS**: 198+456+82 = 736min (~12h). The number of variants for the 3 runs are:. - **v0.9.0**: 531371190 g.vcf.gz; 10784757 vcf.gz (4552313 PASS, 6232444 RefCall). - **v1.0.0**: 547491396 g.vcf.gz; 11892262 vcf.gz (4619350 PASS, 7272912 RefCall). - **v0.9.0 OLD TEST ON 30X WGS**: 213244705 g.vcf.gz; 9096927 vcf.gz (4661618 PASS, 4435309 RefCall). So, the first question is: are these running times expected? The running times you reported are much shorter it seems. Can it be that the running time increased from 12 to 34h just because of the lower coverage? . The exact command I've used is below (it is the same for v1.0.0 but using the corresponding singularity image):. ```. singularity exec \. --bind /data/ref/genomes/GRCh38:/genomes \. --bind /data/projects/HICF2_project/BAM:/bam_files \. /well/gel/HICF2/software/singularity/deepvariant-0.9.0.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS --ref=""/genomes/GCA_000001405.15_GRCh38_full_plus_hs38d1_analysis_set.fna"" \. --reads=""/bam_files/${bamfile}"" \. --output_vcf=""VCF/${sampleID}.vcf.gz"" \. --output_gvcf=""VCF/${sampleID}.g.vcf.gz"" \. --intermediate_results_dir=""tmp_data"" \. --num_shards=10. ```. I've uploaded the log files from the 3 runs if you want to take a look:. [deepvar_0.9.0_oldrun.log](https://github.com/google/deepvariant/files/5281370/deepvar_0.9.0_oldrun.log). [deepvar_1.0.0.log](https://github.com/google/deepvariant/files/5281371/deepvar_1.0.0.log). [deepvar_0.9.0.log](https://github.com/google/deepvariant/files/5281372/deepvar_0.9.0.log). Thanks for your support!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:1911,testability,log,log,1911,"red reads), running deepvar 0.9.0 and 1.0.0 with singularity v.3.5.2-1.1.el7 and exactly the same command (`--num_shards=10`). The running time is now almost the same, much longer than my old test with deepvar 0.9.0 on a 30X WGS. . Below I've reported running time in min for make_example+call_variants+post_process:. - **v0.9.0**: 290+1494+281 = 2065min (~34h). - **v1.0.0**: 335+1487+300 = 2122min (~35h). - **v0.9.0 OLD TEST ON 30X WGS**: 198+456+82 = 736min (~12h). The number of variants for the 3 runs are:. - **v0.9.0**: 531371190 g.vcf.gz; 10784757 vcf.gz (4552313 PASS, 6232444 RefCall). - **v1.0.0**: 547491396 g.vcf.gz; 11892262 vcf.gz (4619350 PASS, 7272912 RefCall). - **v0.9.0 OLD TEST ON 30X WGS**: 213244705 g.vcf.gz; 9096927 vcf.gz (4661618 PASS, 4435309 RefCall). So, the first question is: are these running times expected? The running times you reported are much shorter it seems. Can it be that the running time increased from 12 to 34h just because of the lower coverage? . The exact command I've used is below (it is the same for v1.0.0 but using the corresponding singularity image):. ```. singularity exec \. --bind /data/ref/genomes/GRCh38:/genomes \. --bind /data/projects/HICF2_project/BAM:/bam_files \. /well/gel/HICF2/software/singularity/deepvariant-0.9.0.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS --ref=""/genomes/GCA_000001405.15_GRCh38_full_plus_hs38d1_analysis_set.fna"" \. --reads=""/bam_files/${bamfile}"" \. --output_vcf=""VCF/${sampleID}.vcf.gz"" \. --output_gvcf=""VCF/${sampleID}.g.vcf.gz"" \. --intermediate_results_dir=""tmp_data"" \. --num_shards=10. ```. I've uploaded the log files from the 3 runs if you want to take a look:. [deepvar_0.9.0_oldrun.log](https://github.com/google/deepvariant/files/5281370/deepvar_0.9.0_oldrun.log). [deepvar_1.0.0.log](https://github.com/google/deepvariant/files/5281371/deepvar_1.0.0.log). [deepvar_0.9.0.log](https://github.com/google/deepvariant/files/5281372/deepvar_0.9.0.log). Thanks for your support!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:1982,testability,log,log,1982,"red reads), running deepvar 0.9.0 and 1.0.0 with singularity v.3.5.2-1.1.el7 and exactly the same command (`--num_shards=10`). The running time is now almost the same, much longer than my old test with deepvar 0.9.0 on a 30X WGS. . Below I've reported running time in min for make_example+call_variants+post_process:. - **v0.9.0**: 290+1494+281 = 2065min (~34h). - **v1.0.0**: 335+1487+300 = 2122min (~35h). - **v0.9.0 OLD TEST ON 30X WGS**: 198+456+82 = 736min (~12h). The number of variants for the 3 runs are:. - **v0.9.0**: 531371190 g.vcf.gz; 10784757 vcf.gz (4552313 PASS, 6232444 RefCall). - **v1.0.0**: 547491396 g.vcf.gz; 11892262 vcf.gz (4619350 PASS, 7272912 RefCall). - **v0.9.0 OLD TEST ON 30X WGS**: 213244705 g.vcf.gz; 9096927 vcf.gz (4661618 PASS, 4435309 RefCall). So, the first question is: are these running times expected? The running times you reported are much shorter it seems. Can it be that the running time increased from 12 to 34h just because of the lower coverage? . The exact command I've used is below (it is the same for v1.0.0 but using the corresponding singularity image):. ```. singularity exec \. --bind /data/ref/genomes/GRCh38:/genomes \. --bind /data/projects/HICF2_project/BAM:/bam_files \. /well/gel/HICF2/software/singularity/deepvariant-0.9.0.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS --ref=""/genomes/GCA_000001405.15_GRCh38_full_plus_hs38d1_analysis_set.fna"" \. --reads=""/bam_files/${bamfile}"" \. --output_vcf=""VCF/${sampleID}.vcf.gz"" \. --output_gvcf=""VCF/${sampleID}.g.vcf.gz"" \. --intermediate_results_dir=""tmp_data"" \. --num_shards=10. ```. I've uploaded the log files from the 3 runs if you want to take a look:. [deepvar_0.9.0_oldrun.log](https://github.com/google/deepvariant/files/5281370/deepvar_0.9.0_oldrun.log). [deepvar_1.0.0.log](https://github.com/google/deepvariant/files/5281371/deepvar_1.0.0.log). [deepvar_0.9.0.log](https://github.com/google/deepvariant/files/5281372/deepvar_0.9.0.log). Thanks for your support!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:2003,testability,log,log,2003,"red reads), running deepvar 0.9.0 and 1.0.0 with singularity v.3.5.2-1.1.el7 and exactly the same command (`--num_shards=10`). The running time is now almost the same, much longer than my old test with deepvar 0.9.0 on a 30X WGS. . Below I've reported running time in min for make_example+call_variants+post_process:. - **v0.9.0**: 290+1494+281 = 2065min (~34h). - **v1.0.0**: 335+1487+300 = 2122min (~35h). - **v0.9.0 OLD TEST ON 30X WGS**: 198+456+82 = 736min (~12h). The number of variants for the 3 runs are:. - **v0.9.0**: 531371190 g.vcf.gz; 10784757 vcf.gz (4552313 PASS, 6232444 RefCall). - **v1.0.0**: 547491396 g.vcf.gz; 11892262 vcf.gz (4619350 PASS, 7272912 RefCall). - **v0.9.0 OLD TEST ON 30X WGS**: 213244705 g.vcf.gz; 9096927 vcf.gz (4661618 PASS, 4435309 RefCall). So, the first question is: are these running times expected? The running times you reported are much shorter it seems. Can it be that the running time increased from 12 to 34h just because of the lower coverage? . The exact command I've used is below (it is the same for v1.0.0 but using the corresponding singularity image):. ```. singularity exec \. --bind /data/ref/genomes/GRCh38:/genomes \. --bind /data/projects/HICF2_project/BAM:/bam_files \. /well/gel/HICF2/software/singularity/deepvariant-0.9.0.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS --ref=""/genomes/GCA_000001405.15_GRCh38_full_plus_hs38d1_analysis_set.fna"" \. --reads=""/bam_files/${bamfile}"" \. --output_vcf=""VCF/${sampleID}.vcf.gz"" \. --output_gvcf=""VCF/${sampleID}.g.vcf.gz"" \. --intermediate_results_dir=""tmp_data"" \. --num_shards=10. ```. I've uploaded the log files from the 3 runs if you want to take a look:. [deepvar_0.9.0_oldrun.log](https://github.com/google/deepvariant/files/5281370/deepvar_0.9.0_oldrun.log). [deepvar_1.0.0.log](https://github.com/google/deepvariant/files/5281371/deepvar_1.0.0.log). [deepvar_0.9.0.log](https://github.com/google/deepvariant/files/5281372/deepvar_0.9.0.log). Thanks for your support!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:2074,testability,log,log,2074,"red reads), running deepvar 0.9.0 and 1.0.0 with singularity v.3.5.2-1.1.el7 and exactly the same command (`--num_shards=10`). The running time is now almost the same, much longer than my old test with deepvar 0.9.0 on a 30X WGS. . Below I've reported running time in min for make_example+call_variants+post_process:. - **v0.9.0**: 290+1494+281 = 2065min (~34h). - **v1.0.0**: 335+1487+300 = 2122min (~35h). - **v0.9.0 OLD TEST ON 30X WGS**: 198+456+82 = 736min (~12h). The number of variants for the 3 runs are:. - **v0.9.0**: 531371190 g.vcf.gz; 10784757 vcf.gz (4552313 PASS, 6232444 RefCall). - **v1.0.0**: 547491396 g.vcf.gz; 11892262 vcf.gz (4619350 PASS, 7272912 RefCall). - **v0.9.0 OLD TEST ON 30X WGS**: 213244705 g.vcf.gz; 9096927 vcf.gz (4661618 PASS, 4435309 RefCall). So, the first question is: are these running times expected? The running times you reported are much shorter it seems. Can it be that the running time increased from 12 to 34h just because of the lower coverage? . The exact command I've used is below (it is the same for v1.0.0 but using the corresponding singularity image):. ```. singularity exec \. --bind /data/ref/genomes/GRCh38:/genomes \. --bind /data/projects/HICF2_project/BAM:/bam_files \. /well/gel/HICF2/software/singularity/deepvariant-0.9.0.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS --ref=""/genomes/GCA_000001405.15_GRCh38_full_plus_hs38d1_analysis_set.fna"" \. --reads=""/bam_files/${bamfile}"" \. --output_vcf=""VCF/${sampleID}.vcf.gz"" \. --output_gvcf=""VCF/${sampleID}.g.vcf.gz"" \. --intermediate_results_dir=""tmp_data"" \. --num_shards=10. ```. I've uploaded the log files from the 3 runs if you want to take a look:. [deepvar_0.9.0_oldrun.log](https://github.com/google/deepvariant/files/5281370/deepvar_0.9.0_oldrun.log). [deepvar_1.0.0.log](https://github.com/google/deepvariant/files/5281371/deepvar_1.0.0.log). [deepvar_0.9.0.log](https://github.com/google/deepvariant/files/5281372/deepvar_0.9.0.log). Thanks for your support!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:61,usability,input,input,61,"Hi. After your comment I've repeated the test using the same input BAM (18X WGS sequenced with 150bp paired reads), running deepvar 0.9.0 and 1.0.0 with singularity v.3.5.2-1.1.el7 and exactly the same command (`--num_shards=10`). The running time is now almost the same, much longer than my old test with deepvar 0.9.0 on a 30X WGS. . Below I've reported running time in min for make_example+call_variants+post_process:. - **v0.9.0**: 290+1494+281 = 2065min (~34h). - **v1.0.0**: 335+1487+300 = 2122min (~35h). - **v0.9.0 OLD TEST ON 30X WGS**: 198+456+82 = 736min (~12h). The number of variants for the 3 runs are:. - **v0.9.0**: 531371190 g.vcf.gz; 10784757 vcf.gz (4552313 PASS, 6232444 RefCall). - **v1.0.0**: 547491396 g.vcf.gz; 11892262 vcf.gz (4619350 PASS, 7272912 RefCall). - **v0.9.0 OLD TEST ON 30X WGS**: 213244705 g.vcf.gz; 9096927 vcf.gz (4661618 PASS, 4435309 RefCall). So, the first question is: are these running times expected? The running times you reported are much shorter it seems. Can it be that the running time increased from 12 to 34h just because of the lower coverage? . The exact command I've used is below (it is the same for v1.0.0 but using the corresponding singularity image):. ```. singularity exec \. --bind /data/ref/genomes/GRCh38:/genomes \. --bind /data/projects/HICF2_project/BAM:/bam_files \. /well/gel/HICF2/software/singularity/deepvariant-0.9.0.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS --ref=""/genomes/GCA_000001405.15_GRCh38_full_plus_hs38d1_analysis_set.fna"" \. --reads=""/bam_files/${bamfile}"" \. --output_vcf=""VCF/${sampleID}.vcf.gz"" \. --output_gvcf=""VCF/${sampleID}.g.vcf.gz"" \. --intermediate_results_dir=""tmp_data"" \. --num_shards=10. ```. I've uploaded the log files from the 3 runs if you want to take a look:. [deepvar_0.9.0_oldrun.log](https://github.com/google/deepvariant/files/5281370/deepvar_0.9.0_oldrun.log). [deepvar_1.0.0.log](https://github.com/google/deepvariant/files/5281371/deepvar_1.0.0.log). [deepvar_0.9",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:202,usability,command,command,202,"Hi. After your comment I've repeated the test using the same input BAM (18X WGS sequenced with 150bp paired reads), running deepvar 0.9.0 and 1.0.0 with singularity v.3.5.2-1.1.el7 and exactly the same command (`--num_shards=10`). The running time is now almost the same, much longer than my old test with deepvar 0.9.0 on a 30X WGS. . Below I've reported running time in min for make_example+call_variants+post_process:. - **v0.9.0**: 290+1494+281 = 2065min (~34h). - **v1.0.0**: 335+1487+300 = 2122min (~35h). - **v0.9.0 OLD TEST ON 30X WGS**: 198+456+82 = 736min (~12h). The number of variants for the 3 runs are:. - **v0.9.0**: 531371190 g.vcf.gz; 10784757 vcf.gz (4552313 PASS, 6232444 RefCall). - **v1.0.0**: 547491396 g.vcf.gz; 11892262 vcf.gz (4619350 PASS, 7272912 RefCall). - **v0.9.0 OLD TEST ON 30X WGS**: 213244705 g.vcf.gz; 9096927 vcf.gz (4661618 PASS, 4435309 RefCall). So, the first question is: are these running times expected? The running times you reported are much shorter it seems. Can it be that the running time increased from 12 to 34h just because of the lower coverage? . The exact command I've used is below (it is the same for v1.0.0 but using the corresponding singularity image):. ```. singularity exec \. --bind /data/ref/genomes/GRCh38:/genomes \. --bind /data/projects/HICF2_project/BAM:/bam_files \. /well/gel/HICF2/software/singularity/deepvariant-0.9.0.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS --ref=""/genomes/GCA_000001405.15_GRCh38_full_plus_hs38d1_analysis_set.fna"" \. --reads=""/bam_files/${bamfile}"" \. --output_vcf=""VCF/${sampleID}.vcf.gz"" \. --output_gvcf=""VCF/${sampleID}.g.vcf.gz"" \. --intermediate_results_dir=""tmp_data"" \. --num_shards=10. ```. I've uploaded the log files from the 3 runs if you want to take a look:. [deepvar_0.9.0_oldrun.log](https://github.com/google/deepvariant/files/5281370/deepvar_0.9.0_oldrun.log). [deepvar_1.0.0.log](https://github.com/google/deepvariant/files/5281371/deepvar_1.0.0.log). [deepvar_0.9",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:1110,usability,command,command,1110,"red reads), running deepvar 0.9.0 and 1.0.0 with singularity v.3.5.2-1.1.el7 and exactly the same command (`--num_shards=10`). The running time is now almost the same, much longer than my old test with deepvar 0.9.0 on a 30X WGS. . Below I've reported running time in min for make_example+call_variants+post_process:. - **v0.9.0**: 290+1494+281 = 2065min (~34h). - **v1.0.0**: 335+1487+300 = 2122min (~35h). - **v0.9.0 OLD TEST ON 30X WGS**: 198+456+82 = 736min (~12h). The number of variants for the 3 runs are:. - **v0.9.0**: 531371190 g.vcf.gz; 10784757 vcf.gz (4552313 PASS, 6232444 RefCall). - **v1.0.0**: 547491396 g.vcf.gz; 11892262 vcf.gz (4619350 PASS, 7272912 RefCall). - **v0.9.0 OLD TEST ON 30X WGS**: 213244705 g.vcf.gz; 9096927 vcf.gz (4661618 PASS, 4435309 RefCall). So, the first question is: are these running times expected? The running times you reported are much shorter it seems. Can it be that the running time increased from 12 to 34h just because of the lower coverage? . The exact command I've used is below (it is the same for v1.0.0 but using the corresponding singularity image):. ```. singularity exec \. --bind /data/ref/genomes/GRCh38:/genomes \. --bind /data/projects/HICF2_project/BAM:/bam_files \. /well/gel/HICF2/software/singularity/deepvariant-0.9.0.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS --ref=""/genomes/GCA_000001405.15_GRCh38_full_plus_hs38d1_analysis_set.fna"" \. --reads=""/bam_files/${bamfile}"" \. --output_vcf=""VCF/${sampleID}.vcf.gz"" \. --output_gvcf=""VCF/${sampleID}.g.vcf.gz"" \. --intermediate_results_dir=""tmp_data"" \. --num_shards=10. ```. I've uploaded the log files from the 3 runs if you want to take a look:. [deepvar_0.9.0_oldrun.log](https://github.com/google/deepvariant/files/5281370/deepvar_0.9.0_oldrun.log). [deepvar_1.0.0.log](https://github.com/google/deepvariant/files/5281371/deepvar_1.0.0.log). [deepvar_0.9.0.log](https://github.com/google/deepvariant/files/5281372/deepvar_0.9.0.log). Thanks for your support!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:2096,usability,support,support,2096,"red reads), running deepvar 0.9.0 and 1.0.0 with singularity v.3.5.2-1.1.el7 and exactly the same command (`--num_shards=10`). The running time is now almost the same, much longer than my old test with deepvar 0.9.0 on a 30X WGS. . Below I've reported running time in min for make_example+call_variants+post_process:. - **v0.9.0**: 290+1494+281 = 2065min (~34h). - **v1.0.0**: 335+1487+300 = 2122min (~35h). - **v0.9.0 OLD TEST ON 30X WGS**: 198+456+82 = 736min (~12h). The number of variants for the 3 runs are:. - **v0.9.0**: 531371190 g.vcf.gz; 10784757 vcf.gz (4552313 PASS, 6232444 RefCall). - **v1.0.0**: 547491396 g.vcf.gz; 11892262 vcf.gz (4619350 PASS, 7272912 RefCall). - **v0.9.0 OLD TEST ON 30X WGS**: 213244705 g.vcf.gz; 9096927 vcf.gz (4661618 PASS, 4435309 RefCall). So, the first question is: are these running times expected? The running times you reported are much shorter it seems. Can it be that the running time increased from 12 to 34h just because of the lower coverage? . The exact command I've used is below (it is the same for v1.0.0 but using the corresponding singularity image):. ```. singularity exec \. --bind /data/ref/genomes/GRCh38:/genomes \. --bind /data/projects/HICF2_project/BAM:/bam_files \. /well/gel/HICF2/software/singularity/deepvariant-0.9.0.simg \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS --ref=""/genomes/GCA_000001405.15_GRCh38_full_plus_hs38d1_analysis_set.fna"" \. --reads=""/bam_files/${bamfile}"" \. --output_vcf=""VCF/${sampleID}.vcf.gz"" \. --output_gvcf=""VCF/${sampleID}.g.vcf.gz"" \. --intermediate_results_dir=""tmp_data"" \. --num_shards=10. ```. I've uploaded the log files from the 3 runs if you want to take a look:. [deepvar_0.9.0_oldrun.log](https://github.com/google/deepvariant/files/5281370/deepvar_0.9.0_oldrun.log). [deepvar_1.0.0.log](https://github.com/google/deepvariant/files/5281371/deepvar_1.0.0.log). [deepvar_0.9.0.log](https://github.com/google/deepvariant/files/5281372/deepvar_0.9.0.log). Thanks for your support!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:65,availability,slo,slower,65,"Thanks @edg1983 . Looking at your numbers. Comparing **v0.9.0** (slower) and **v0.9.0 OLD TEST ON 30X WGS**. 1. make_examples runtime: 290/198 = about 1.5 times. 2. #entries in VCF: 10784757/9096927 = about 1.2 times entries. 3. #entries in gVCF: 531371190/213244705 = about 2.5 times. 4. call_variants runtime: 1494/456 = about 3.3 times. 5. postprocess_variants runtime: 281/82 = about 3.4 times. My observations:. (1) gVCF entries is higher, which is not unexpected on lower coverage BAMs. (2) The increase on call_variants runtime should be linear to the number of examples presented to the classifier, which should be roughly similar to the #entries in VCF. One reason this could change significantly is: if you end up having too many multi-allelic entries. I guess it is possible with lower coverage files, but I'm still surprised by this. I'll see if I can find some internal benchmarking runs to see if we observe this. This one is currently a surprise to me. (3) The increase on postprocess_variants runtime - We know that lower coverage will increase gVCF entries and postprocess_variants runtime. Given that the #entries in gVCF has increased to 2.5 times, this number might not be unexpected here.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:402,deployability,observ,observations,402,"Thanks @edg1983 . Looking at your numbers. Comparing **v0.9.0** (slower) and **v0.9.0 OLD TEST ON 30X WGS**. 1. make_examples runtime: 290/198 = about 1.5 times. 2. #entries in VCF: 10784757/9096927 = about 1.2 times entries. 3. #entries in gVCF: 531371190/213244705 = about 2.5 times. 4. call_variants runtime: 1494/456 = about 3.3 times. 5. postprocess_variants runtime: 281/82 = about 3.4 times. My observations:. (1) gVCF entries is higher, which is not unexpected on lower coverage BAMs. (2) The increase on call_variants runtime should be linear to the number of examples presented to the classifier, which should be roughly similar to the #entries in VCF. One reason this could change significantly is: if you end up having too many multi-allelic entries. I guess it is possible with lower coverage files, but I'm still surprised by this. I'll see if I can find some internal benchmarking runs to see if we observe this. This one is currently a surprise to me. (3) The increase on postprocess_variants runtime - We know that lower coverage will increase gVCF entries and postprocess_variants runtime. Given that the #entries in gVCF has increased to 2.5 times, this number might not be unexpected here.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:914,deployability,observ,observe,914,"Thanks @edg1983 . Looking at your numbers. Comparing **v0.9.0** (slower) and **v0.9.0 OLD TEST ON 30X WGS**. 1. make_examples runtime: 290/198 = about 1.5 times. 2. #entries in VCF: 10784757/9096927 = about 1.2 times entries. 3. #entries in gVCF: 531371190/213244705 = about 2.5 times. 4. call_variants runtime: 1494/456 = about 3.3 times. 5. postprocess_variants runtime: 281/82 = about 3.4 times. My observations:. (1) gVCF entries is higher, which is not unexpected on lower coverage BAMs. (2) The increase on call_variants runtime should be linear to the number of examples presented to the classifier, which should be roughly similar to the #entries in VCF. One reason this could change significantly is: if you end up having too many multi-allelic entries. I guess it is possible with lower coverage files, but I'm still surprised by this. I'll see if I can find some internal benchmarking runs to see if we observe this. This one is currently a surprise to me. (3) The increase on postprocess_variants runtime - We know that lower coverage will increase gVCF entries and postprocess_variants runtime. Given that the #entries in gVCF has increased to 2.5 times, this number might not be unexpected here.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:940,energy efficiency,current,currently,940,"Thanks @edg1983 . Looking at your numbers. Comparing **v0.9.0** (slower) and **v0.9.0 OLD TEST ON 30X WGS**. 1. make_examples runtime: 290/198 = about 1.5 times. 2. #entries in VCF: 10784757/9096927 = about 1.2 times entries. 3. #entries in gVCF: 531371190/213244705 = about 2.5 times. 4. call_variants runtime: 1494/456 = about 3.3 times. 5. postprocess_variants runtime: 281/82 = about 3.4 times. My observations:. (1) gVCF entries is higher, which is not unexpected on lower coverage BAMs. (2) The increase on call_variants runtime should be linear to the number of examples presented to the classifier, which should be roughly similar to the #entries in VCF. One reason this could change significantly is: if you end up having too many multi-allelic entries. I guess it is possible with lower coverage files, but I'm still surprised by this. I'll see if I can find some internal benchmarking runs to see if we observe this. This one is currently a surprise to me. (3) The increase on postprocess_variants runtime - We know that lower coverage will increase gVCF entries and postprocess_variants runtime. Given that the #entries in gVCF has increased to 2.5 times, this number might not be unexpected here.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:155,performance,time,times,155,"Thanks @edg1983 . Looking at your numbers. Comparing **v0.9.0** (slower) and **v0.9.0 OLD TEST ON 30X WGS**. 1. make_examples runtime: 290/198 = about 1.5 times. 2. #entries in VCF: 10784757/9096927 = about 1.2 times entries. 3. #entries in gVCF: 531371190/213244705 = about 2.5 times. 4. call_variants runtime: 1494/456 = about 3.3 times. 5. postprocess_variants runtime: 281/82 = about 3.4 times. My observations:. (1) gVCF entries is higher, which is not unexpected on lower coverage BAMs. (2) The increase on call_variants runtime should be linear to the number of examples presented to the classifier, which should be roughly similar to the #entries in VCF. One reason this could change significantly is: if you end up having too many multi-allelic entries. I guess it is possible with lower coverage files, but I'm still surprised by this. I'll see if I can find some internal benchmarking runs to see if we observe this. This one is currently a surprise to me. (3) The increase on postprocess_variants runtime - We know that lower coverage will increase gVCF entries and postprocess_variants runtime. Given that the #entries in gVCF has increased to 2.5 times, this number might not be unexpected here.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:211,performance,time,times,211,"Thanks @edg1983 . Looking at your numbers. Comparing **v0.9.0** (slower) and **v0.9.0 OLD TEST ON 30X WGS**. 1. make_examples runtime: 290/198 = about 1.5 times. 2. #entries in VCF: 10784757/9096927 = about 1.2 times entries. 3. #entries in gVCF: 531371190/213244705 = about 2.5 times. 4. call_variants runtime: 1494/456 = about 3.3 times. 5. postprocess_variants runtime: 281/82 = about 3.4 times. My observations:. (1) gVCF entries is higher, which is not unexpected on lower coverage BAMs. (2) The increase on call_variants runtime should be linear to the number of examples presented to the classifier, which should be roughly similar to the #entries in VCF. One reason this could change significantly is: if you end up having too many multi-allelic entries. I guess it is possible with lower coverage files, but I'm still surprised by this. I'll see if I can find some internal benchmarking runs to see if we observe this. This one is currently a surprise to me. (3) The increase on postprocess_variants runtime - We know that lower coverage will increase gVCF entries and postprocess_variants runtime. Given that the #entries in gVCF has increased to 2.5 times, this number might not be unexpected here.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:279,performance,time,times,279,"Thanks @edg1983 . Looking at your numbers. Comparing **v0.9.0** (slower) and **v0.9.0 OLD TEST ON 30X WGS**. 1. make_examples runtime: 290/198 = about 1.5 times. 2. #entries in VCF: 10784757/9096927 = about 1.2 times entries. 3. #entries in gVCF: 531371190/213244705 = about 2.5 times. 4. call_variants runtime: 1494/456 = about 3.3 times. 5. postprocess_variants runtime: 281/82 = about 3.4 times. My observations:. (1) gVCF entries is higher, which is not unexpected on lower coverage BAMs. (2) The increase on call_variants runtime should be linear to the number of examples presented to the classifier, which should be roughly similar to the #entries in VCF. One reason this could change significantly is: if you end up having too many multi-allelic entries. I guess it is possible with lower coverage files, but I'm still surprised by this. I'll see if I can find some internal benchmarking runs to see if we observe this. This one is currently a surprise to me. (3) The increase on postprocess_variants runtime - We know that lower coverage will increase gVCF entries and postprocess_variants runtime. Given that the #entries in gVCF has increased to 2.5 times, this number might not be unexpected here.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:333,performance,time,times,333,"Thanks @edg1983 . Looking at your numbers. Comparing **v0.9.0** (slower) and **v0.9.0 OLD TEST ON 30X WGS**. 1. make_examples runtime: 290/198 = about 1.5 times. 2. #entries in VCF: 10784757/9096927 = about 1.2 times entries. 3. #entries in gVCF: 531371190/213244705 = about 2.5 times. 4. call_variants runtime: 1494/456 = about 3.3 times. 5. postprocess_variants runtime: 281/82 = about 3.4 times. My observations:. (1) gVCF entries is higher, which is not unexpected on lower coverage BAMs. (2) The increase on call_variants runtime should be linear to the number of examples presented to the classifier, which should be roughly similar to the #entries in VCF. One reason this could change significantly is: if you end up having too many multi-allelic entries. I guess it is possible with lower coverage files, but I'm still surprised by this. I'll see if I can find some internal benchmarking runs to see if we observe this. This one is currently a surprise to me. (3) The increase on postprocess_variants runtime - We know that lower coverage will increase gVCF entries and postprocess_variants runtime. Given that the #entries in gVCF has increased to 2.5 times, this number might not be unexpected here.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:392,performance,time,times,392,"Thanks @edg1983 . Looking at your numbers. Comparing **v0.9.0** (slower) and **v0.9.0 OLD TEST ON 30X WGS**. 1. make_examples runtime: 290/198 = about 1.5 times. 2. #entries in VCF: 10784757/9096927 = about 1.2 times entries. 3. #entries in gVCF: 531371190/213244705 = about 2.5 times. 4. call_variants runtime: 1494/456 = about 3.3 times. 5. postprocess_variants runtime: 281/82 = about 3.4 times. My observations:. (1) gVCF entries is higher, which is not unexpected on lower coverage BAMs. (2) The increase on call_variants runtime should be linear to the number of examples presented to the classifier, which should be roughly similar to the #entries in VCF. One reason this could change significantly is: if you end up having too many multi-allelic entries. I guess it is possible with lower coverage files, but I'm still surprised by this. I'll see if I can find some internal benchmarking runs to see if we observe this. This one is currently a surprise to me. (3) The increase on postprocess_variants runtime - We know that lower coverage will increase gVCF entries and postprocess_variants runtime. Given that the #entries in gVCF has increased to 2.5 times, this number might not be unexpected here.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:1161,performance,time,times,1161,"Thanks @edg1983 . Looking at your numbers. Comparing **v0.9.0** (slower) and **v0.9.0 OLD TEST ON 30X WGS**. 1. make_examples runtime: 290/198 = about 1.5 times. 2. #entries in VCF: 10784757/9096927 = about 1.2 times entries. 3. #entries in gVCF: 531371190/213244705 = about 2.5 times. 4. call_variants runtime: 1494/456 = about 3.3 times. 5. postprocess_variants runtime: 281/82 = about 3.4 times. My observations:. (1) gVCF entries is higher, which is not unexpected on lower coverage BAMs. (2) The increase on call_variants runtime should be linear to the number of examples presented to the classifier, which should be roughly similar to the #entries in VCF. One reason this could change significantly is: if you end up having too many multi-allelic entries. I guess it is possible with lower coverage files, but I'm still surprised by this. I'll see if I can find some internal benchmarking runs to see if we observe this. This one is currently a surprise to me. (3) The increase on postprocess_variants runtime - We know that lower coverage will increase gVCF entries and postprocess_variants runtime. Given that the #entries in gVCF has increased to 2.5 times, this number might not be unexpected here.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:65,reliability,slo,slower,65,"Thanks @edg1983 . Looking at your numbers. Comparing **v0.9.0** (slower) and **v0.9.0 OLD TEST ON 30X WGS**. 1. make_examples runtime: 290/198 = about 1.5 times. 2. #entries in VCF: 10784757/9096927 = about 1.2 times entries. 3. #entries in gVCF: 531371190/213244705 = about 2.5 times. 4. call_variants runtime: 1494/456 = about 3.3 times. 5. postprocess_variants runtime: 281/82 = about 3.4 times. My observations:. (1) gVCF entries is higher, which is not unexpected on lower coverage BAMs. (2) The increase on call_variants runtime should be linear to the number of examples presented to the classifier, which should be roughly similar to the #entries in VCF. One reason this could change significantly is: if you end up having too many multi-allelic entries. I guess it is possible with lower coverage files, but I'm still surprised by this. I'll see if I can find some internal benchmarking runs to see if we observe this. This one is currently a surprise to me. (3) The increase on postprocess_variants runtime - We know that lower coverage will increase gVCF entries and postprocess_variants runtime. Given that the #entries in gVCF has increased to 2.5 times, this number might not be unexpected here.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:90,safety,TEST,TEST,90,"Thanks @edg1983 . Looking at your numbers. Comparing **v0.9.0** (slower) and **v0.9.0 OLD TEST ON 30X WGS**. 1. make_examples runtime: 290/198 = about 1.5 times. 2. #entries in VCF: 10784757/9096927 = about 1.2 times entries. 3. #entries in gVCF: 531371190/213244705 = about 2.5 times. 4. call_variants runtime: 1494/456 = about 3.3 times. 5. postprocess_variants runtime: 281/82 = about 3.4 times. My observations:. (1) gVCF entries is higher, which is not unexpected on lower coverage BAMs. (2) The increase on call_variants runtime should be linear to the number of examples presented to the classifier, which should be roughly similar to the #entries in VCF. One reason this could change significantly is: if you end up having too many multi-allelic entries. I guess it is possible with lower coverage files, but I'm still surprised by this. I'll see if I can find some internal benchmarking runs to see if we observe this. This one is currently a surprise to me. (3) The increase on postprocess_variants runtime - We know that lower coverage will increase gVCF entries and postprocess_variants runtime. Given that the #entries in gVCF has increased to 2.5 times, this number might not be unexpected here.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:692,security,sign,significantly,692,"Thanks @edg1983 . Looking at your numbers. Comparing **v0.9.0** (slower) and **v0.9.0 OLD TEST ON 30X WGS**. 1. make_examples runtime: 290/198 = about 1.5 times. 2. #entries in VCF: 10784757/9096927 = about 1.2 times entries. 3. #entries in gVCF: 531371190/213244705 = about 2.5 times. 4. call_variants runtime: 1494/456 = about 3.3 times. 5. postprocess_variants runtime: 281/82 = about 3.4 times. My observations:. (1) gVCF entries is higher, which is not unexpected on lower coverage BAMs. (2) The increase on call_variants runtime should be linear to the number of examples presented to the classifier, which should be roughly similar to the #entries in VCF. One reason this could change significantly is: if you end up having too many multi-allelic entries. I guess it is possible with lower coverage files, but I'm still surprised by this. I'll see if I can find some internal benchmarking runs to see if we observe this. This one is currently a surprise to me. (3) The increase on postprocess_variants runtime - We know that lower coverage will increase gVCF entries and postprocess_variants runtime. Given that the #entries in gVCF has increased to 2.5 times, this number might not be unexpected here.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:90,testability,TEST,TEST,90,"Thanks @edg1983 . Looking at your numbers. Comparing **v0.9.0** (slower) and **v0.9.0 OLD TEST ON 30X WGS**. 1. make_examples runtime: 290/198 = about 1.5 times. 2. #entries in VCF: 10784757/9096927 = about 1.2 times entries. 3. #entries in gVCF: 531371190/213244705 = about 2.5 times. 4. call_variants runtime: 1494/456 = about 3.3 times. 5. postprocess_variants runtime: 281/82 = about 3.4 times. My observations:. (1) gVCF entries is higher, which is not unexpected on lower coverage BAMs. (2) The increase on call_variants runtime should be linear to the number of examples presented to the classifier, which should be roughly similar to the #entries in VCF. One reason this could change significantly is: if you end up having too many multi-allelic entries. I guess it is possible with lower coverage files, but I'm still surprised by this. I'll see if I can find some internal benchmarking runs to see if we observe this. This one is currently a surprise to me. (3) The increase on postprocess_variants runtime - We know that lower coverage will increase gVCF entries and postprocess_variants runtime. Given that the #entries in gVCF has increased to 2.5 times, this number might not be unexpected here.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:402,testability,observ,observations,402,"Thanks @edg1983 . Looking at your numbers. Comparing **v0.9.0** (slower) and **v0.9.0 OLD TEST ON 30X WGS**. 1. make_examples runtime: 290/198 = about 1.5 times. 2. #entries in VCF: 10784757/9096927 = about 1.2 times entries. 3. #entries in gVCF: 531371190/213244705 = about 2.5 times. 4. call_variants runtime: 1494/456 = about 3.3 times. 5. postprocess_variants runtime: 281/82 = about 3.4 times. My observations:. (1) gVCF entries is higher, which is not unexpected on lower coverage BAMs. (2) The increase on call_variants runtime should be linear to the number of examples presented to the classifier, which should be roughly similar to the #entries in VCF. One reason this could change significantly is: if you end up having too many multi-allelic entries. I guess it is possible with lower coverage files, but I'm still surprised by this. I'll see if I can find some internal benchmarking runs to see if we observe this. This one is currently a surprise to me. (3) The increase on postprocess_variants runtime - We know that lower coverage will increase gVCF entries and postprocess_variants runtime. Given that the #entries in gVCF has increased to 2.5 times, this number might not be unexpected here.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:478,testability,coverag,coverage,478,"Thanks @edg1983 . Looking at your numbers. Comparing **v0.9.0** (slower) and **v0.9.0 OLD TEST ON 30X WGS**. 1. make_examples runtime: 290/198 = about 1.5 times. 2. #entries in VCF: 10784757/9096927 = about 1.2 times entries. 3. #entries in gVCF: 531371190/213244705 = about 2.5 times. 4. call_variants runtime: 1494/456 = about 3.3 times. 5. postprocess_variants runtime: 281/82 = about 3.4 times. My observations:. (1) gVCF entries is higher, which is not unexpected on lower coverage BAMs. (2) The increase on call_variants runtime should be linear to the number of examples presented to the classifier, which should be roughly similar to the #entries in VCF. One reason this could change significantly is: if you end up having too many multi-allelic entries. I guess it is possible with lower coverage files, but I'm still surprised by this. I'll see if I can find some internal benchmarking runs to see if we observe this. This one is currently a surprise to me. (3) The increase on postprocess_variants runtime - We know that lower coverage will increase gVCF entries and postprocess_variants runtime. Given that the #entries in gVCF has increased to 2.5 times, this number might not be unexpected here.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:797,testability,coverag,coverage,797,"Thanks @edg1983 . Looking at your numbers. Comparing **v0.9.0** (slower) and **v0.9.0 OLD TEST ON 30X WGS**. 1. make_examples runtime: 290/198 = about 1.5 times. 2. #entries in VCF: 10784757/9096927 = about 1.2 times entries. 3. #entries in gVCF: 531371190/213244705 = about 2.5 times. 4. call_variants runtime: 1494/456 = about 3.3 times. 5. postprocess_variants runtime: 281/82 = about 3.4 times. My observations:. (1) gVCF entries is higher, which is not unexpected on lower coverage BAMs. (2) The increase on call_variants runtime should be linear to the number of examples presented to the classifier, which should be roughly similar to the #entries in VCF. One reason this could change significantly is: if you end up having too many multi-allelic entries. I guess it is possible with lower coverage files, but I'm still surprised by this. I'll see if I can find some internal benchmarking runs to see if we observe this. This one is currently a surprise to me. (3) The increase on postprocess_variants runtime - We know that lower coverage will increase gVCF entries and postprocess_variants runtime. Given that the #entries in gVCF has increased to 2.5 times, this number might not be unexpected here.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:914,testability,observ,observe,914,"Thanks @edg1983 . Looking at your numbers. Comparing **v0.9.0** (slower) and **v0.9.0 OLD TEST ON 30X WGS**. 1. make_examples runtime: 290/198 = about 1.5 times. 2. #entries in VCF: 10784757/9096927 = about 1.2 times entries. 3. #entries in gVCF: 531371190/213244705 = about 2.5 times. 4. call_variants runtime: 1494/456 = about 3.3 times. 5. postprocess_variants runtime: 281/82 = about 3.4 times. My observations:. (1) gVCF entries is higher, which is not unexpected on lower coverage BAMs. (2) The increase on call_variants runtime should be linear to the number of examples presented to the classifier, which should be roughly similar to the #entries in VCF. One reason this could change significantly is: if you end up having too many multi-allelic entries. I guess it is possible with lower coverage files, but I'm still surprised by this. I'll see if I can find some internal benchmarking runs to see if we observe this. This one is currently a surprise to me. (3) The increase on postprocess_variants runtime - We know that lower coverage will increase gVCF entries and postprocess_variants runtime. Given that the #entries in gVCF has increased to 2.5 times, this number might not be unexpected here.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:1038,testability,coverag,coverage,1038,"Thanks @edg1983 . Looking at your numbers. Comparing **v0.9.0** (slower) and **v0.9.0 OLD TEST ON 30X WGS**. 1. make_examples runtime: 290/198 = about 1.5 times. 2. #entries in VCF: 10784757/9096927 = about 1.2 times entries. 3. #entries in gVCF: 531371190/213244705 = about 2.5 times. 4. call_variants runtime: 1494/456 = about 3.3 times. 5. postprocess_variants runtime: 281/82 = about 3.4 times. My observations:. (1) gVCF entries is higher, which is not unexpected on lower coverage BAMs. (2) The increase on call_variants runtime should be linear to the number of examples presented to the classifier, which should be roughly similar to the #entries in VCF. One reason this could change significantly is: if you end up having too many multi-allelic entries. I guess it is possible with lower coverage files, but I'm still surprised by this. I'll see if I can find some internal benchmarking runs to see if we observe this. This one is currently a surprise to me. (3) The increase on postprocess_variants runtime - We know that lower coverage will increase gVCF entries and postprocess_variants runtime. Given that the #entries in gVCF has increased to 2.5 times, this number might not be unexpected here.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:269,availability,slo,slower,269,"@edg1983 one other possibility: was the run corresponding to `deepvar_0.9.0_oldrun.log` on different hardware than the other two runs? If you are running the other two runs on older hardware that does not support AVX2 or AVX512 instructions, runtime will be noticeably slower because the code will not be able to take advantage of TensorFlow MKL optimizations.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:83,deployability,log,log,83,"@edg1983 one other possibility: was the run corresponding to `deepvar_0.9.0_oldrun.log` on different hardware than the other two runs? If you are running the other two runs on older hardware that does not support AVX2 or AVX512 instructions, runtime will be noticeably slower because the code will not be able to take advantage of TensorFlow MKL optimizations.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:346,energy efficiency,optim,optimizations,346,"@edg1983 one other possibility: was the run corresponding to `deepvar_0.9.0_oldrun.log` on different hardware than the other two runs? If you are running the other two runs on older hardware that does not support AVX2 or AVX512 instructions, runtime will be noticeably slower because the code will not be able to take advantage of TensorFlow MKL optimizations.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:346,performance,optimiz,optimizations,346,"@edg1983 one other possibility: was the run corresponding to `deepvar_0.9.0_oldrun.log` on different hardware than the other two runs? If you are running the other two runs on older hardware that does not support AVX2 or AVX512 instructions, runtime will be noticeably slower because the code will not be able to take advantage of TensorFlow MKL optimizations.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:196,reliability,doe,does,196,"@edg1983 one other possibility: was the run corresponding to `deepvar_0.9.0_oldrun.log` on different hardware than the other two runs? If you are running the other two runs on older hardware that does not support AVX2 or AVX512 instructions, runtime will be noticeably slower because the code will not be able to take advantage of TensorFlow MKL optimizations.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:269,reliability,slo,slower,269,"@edg1983 one other possibility: was the run corresponding to `deepvar_0.9.0_oldrun.log` on different hardware than the other two runs? If you are running the other two runs on older hardware that does not support AVX2 or AVX512 instructions, runtime will be noticeably slower because the code will not be able to take advantage of TensorFlow MKL optimizations.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:83,safety,log,log,83,"@edg1983 one other possibility: was the run corresponding to `deepvar_0.9.0_oldrun.log` on different hardware than the other two runs? If you are running the other two runs on older hardware that does not support AVX2 or AVX512 instructions, runtime will be noticeably slower because the code will not be able to take advantage of TensorFlow MKL optimizations.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:83,security,log,log,83,"@edg1983 one other possibility: was the run corresponding to `deepvar_0.9.0_oldrun.log` on different hardware than the other two runs? If you are running the other two runs on older hardware that does not support AVX2 or AVX512 instructions, runtime will be noticeably slower because the code will not be able to take advantage of TensorFlow MKL optimizations.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:83,testability,log,log,83,"@edg1983 one other possibility: was the run corresponding to `deepvar_0.9.0_oldrun.log` on different hardware than the other two runs? If you are running the other two runs on older hardware that does not support AVX2 or AVX512 instructions, runtime will be noticeably slower because the code will not be able to take advantage of TensorFlow MKL optimizations.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:205,usability,support,support,205,"@edg1983 one other possibility: was the run corresponding to `deepvar_0.9.0_oldrun.log` on different hardware than the other two runs? If you are running the other two runs on older hardware that does not support AVX2 or AVX512 instructions, runtime will be noticeably slower because the code will not be able to take advantage of TensorFlow MKL optimizations.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:161,energy efficiency,reduc,reduce,161,"And an follow up on gVCF file size, (thanks to @tedyun for reminding me of the flag):. There is a flag in `gvcf_gq_binsize` in make_examples that you can set to reduce the size of gVCF file, the tradeoff being the gVCF file size & runtime vs. more accurate representation of GQ in each hom. ref. site.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:166,availability,cluster,cluster,166,"I want to give you an update on the situation. Following your suggestions, I've discussed with the system administrators and I found out that some older nodes on the cluster do not have AVX2 or AVX512 instructions. I've now completed a test running on nodes supporting those and the running time on the same BAM file is ~14h (290+326+134). So I've discussed with them and found a way to allocate all deepvar jobs to the new nodes supporting the AVX instruction. . Thanks for your support!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:22,deployability,updat,update,22,"I want to give you an update on the situation. Following your suggestions, I've discussed with the system administrators and I found out that some older nodes on the cluster do not have AVX2 or AVX512 instructions. I've now completed a test running on nodes supporting those and the running time on the same BAM file is ~14h (290+326+134). So I've discussed with them and found a way to allocate all deepvar jobs to the new nodes supporting the AVX instruction. . Thanks for your support!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:166,deployability,cluster,cluster,166,"I want to give you an update on the situation. Following your suggestions, I've discussed with the system administrators and I found out that some older nodes on the cluster do not have AVX2 or AVX512 instructions. I've now completed a test running on nodes supporting those and the running time on the same BAM file is ~14h (290+326+134). So I've discussed with them and found a way to allocate all deepvar jobs to the new nodes supporting the AVX instruction. . Thanks for your support!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:387,energy efficiency,alloc,allocate,387,"I want to give you an update on the situation. Following your suggestions, I've discussed with the system administrators and I found out that some older nodes on the cluster do not have AVX2 or AVX512 instructions. I've now completed a test running on nodes supporting those and the running time on the same BAM file is ~14h (290+326+134). So I've discussed with them and found a way to allocate all deepvar jobs to the new nodes supporting the AVX instruction. . Thanks for your support!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:291,performance,time,time,291,"I want to give you an update on the situation. Following your suggestions, I've discussed with the system administrators and I found out that some older nodes on the cluster do not have AVX2 or AVX512 instructions. I've now completed a test running on nodes supporting those and the running time on the same BAM file is ~14h (290+326+134). So I've discussed with them and found a way to allocate all deepvar jobs to the new nodes supporting the AVX instruction. . Thanks for your support!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:22,safety,updat,update,22,"I want to give you an update on the situation. Following your suggestions, I've discussed with the system administrators and I found out that some older nodes on the cluster do not have AVX2 or AVX512 instructions. I've now completed a test running on nodes supporting those and the running time on the same BAM file is ~14h (290+326+134). So I've discussed with them and found a way to allocate all deepvar jobs to the new nodes supporting the AVX instruction. . Thanks for your support!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:224,safety,compl,completed,224,"I want to give you an update on the situation. Following your suggestions, I've discussed with the system administrators and I found out that some older nodes on the cluster do not have AVX2 or AVX512 instructions. I've now completed a test running on nodes supporting those and the running time on the same BAM file is ~14h (290+326+134). So I've discussed with them and found a way to allocate all deepvar jobs to the new nodes supporting the AVX instruction. . Thanks for your support!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:236,safety,test,test,236,"I want to give you an update on the situation. Following your suggestions, I've discussed with the system administrators and I found out that some older nodes on the cluster do not have AVX2 or AVX512 instructions. I've now completed a test running on nodes supporting those and the running time on the same BAM file is ~14h (290+326+134). So I've discussed with them and found a way to allocate all deepvar jobs to the new nodes supporting the AVX instruction. . Thanks for your support!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:22,security,updat,update,22,"I want to give you an update on the situation. Following your suggestions, I've discussed with the system administrators and I found out that some older nodes on the cluster do not have AVX2 or AVX512 instructions. I've now completed a test running on nodes supporting those and the running time on the same BAM file is ~14h (290+326+134). So I've discussed with them and found a way to allocate all deepvar jobs to the new nodes supporting the AVX instruction. . Thanks for your support!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:224,security,compl,completed,224,"I want to give you an update on the situation. Following your suggestions, I've discussed with the system administrators and I found out that some older nodes on the cluster do not have AVX2 or AVX512 instructions. I've now completed a test running on nodes supporting those and the running time on the same BAM file is ~14h (290+326+134). So I've discussed with them and found a way to allocate all deepvar jobs to the new nodes supporting the AVX instruction. . Thanks for your support!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:236,testability,test,test,236,"I want to give you an update on the situation. Following your suggestions, I've discussed with the system administrators and I found out that some older nodes on the cluster do not have AVX2 or AVX512 instructions. I've now completed a test running on nodes supporting those and the running time on the same BAM file is ~14h (290+326+134). So I've discussed with them and found a way to allocate all deepvar jobs to the new nodes supporting the AVX instruction. . Thanks for your support!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:258,usability,support,supporting,258,"I want to give you an update on the situation. Following your suggestions, I've discussed with the system administrators and I found out that some older nodes on the cluster do not have AVX2 or AVX512 instructions. I've now completed a test running on nodes supporting those and the running time on the same BAM file is ~14h (290+326+134). So I've discussed with them and found a way to allocate all deepvar jobs to the new nodes supporting the AVX instruction. . Thanks for your support!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:430,usability,support,supporting,430,"I want to give you an update on the situation. Following your suggestions, I've discussed with the system administrators and I found out that some older nodes on the cluster do not have AVX2 or AVX512 instructions. I've now completed a test running on nodes supporting those and the running time on the same BAM file is ~14h (290+326+134). So I've discussed with them and found a way to allocate all deepvar jobs to the new nodes supporting the AVX instruction. . Thanks for your support!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:480,usability,support,support,480,"I want to give you an update on the situation. Following your suggestions, I've discussed with the system administrators and I found out that some older nodes on the cluster do not have AVX2 or AVX512 instructions. I've now completed a test running on nodes supporting those and the running time on the same BAM file is ~14h (290+326+134). So I've discussed with them and found a way to allocate all deepvar jobs to the new nodes supporting the AVX instruction. . Thanks for your support!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/346:154,testability,coverag,coverage,154,"Hi Edoardo,. I'm glad to hear that you got the runtime issue sorted out! I'll keep this ticket open for your original issue about hom. ref. calls in zero coverage regions. Please let us know if you have any other issues/questions. Thanks,. Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/346
https://github.com/google/deepvariant/issues/347:407,availability,error,error,407,"Hi @jdmontenegro . For the question about multi-allelic heterozygous calls - yes, DeepVariant is able to all 1/2 events, and will represent these in one line as a GT 1/2 call in the VCF. For CLR calling in DeepVariant. It is theoretically possible for us to make a model for DeepVariant that can call CLR data. However, this requires us to write a special candidate generation logic to deal with the higher error rate. Based on what we perceive for the direction of future use in the genomics community, we think that data generated will be increasingly HiFi, so we have not been able to highly prioritize CLR models. Feedback from users like yourself will be useful to us in evaluating if that prioritization makes sense. For now, I can't commit to a timeframe under which we would support a PacBio CLR model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/347
https://github.com/google/deepvariant/issues/347:377,deployability,log,logic,377,"Hi @jdmontenegro . For the question about multi-allelic heterozygous calls - yes, DeepVariant is able to all 1/2 events, and will represent these in one line as a GT 1/2 call in the VCF. For CLR calling in DeepVariant. It is theoretically possible for us to make a model for DeepVariant that can call CLR data. However, this requires us to write a special candidate generation logic to deal with the higher error rate. Based on what we perceive for the direction of future use in the genomics community, we think that data generated will be increasingly HiFi, so we have not been able to highly prioritize CLR models. Feedback from users like yourself will be useful to us in evaluating if that prioritization makes sense. For now, I can't commit to a timeframe under which we would support a PacBio CLR model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/347
https://github.com/google/deepvariant/issues/347:265,energy efficiency,model,model,265,"Hi @jdmontenegro . For the question about multi-allelic heterozygous calls - yes, DeepVariant is able to all 1/2 events, and will represent these in one line as a GT 1/2 call in the VCF. For CLR calling in DeepVariant. It is theoretically possible for us to make a model for DeepVariant that can call CLR data. However, this requires us to write a special candidate generation logic to deal with the higher error rate. Based on what we perceive for the direction of future use in the genomics community, we think that data generated will be increasingly HiFi, so we have not been able to highly prioritize CLR models. Feedback from users like yourself will be useful to us in evaluating if that prioritization makes sense. For now, I can't commit to a timeframe under which we would support a PacBio CLR model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/347
https://github.com/google/deepvariant/issues/347:610,energy efficiency,model,models,610,"Hi @jdmontenegro . For the question about multi-allelic heterozygous calls - yes, DeepVariant is able to all 1/2 events, and will represent these in one line as a GT 1/2 call in the VCF. For CLR calling in DeepVariant. It is theoretically possible for us to make a model for DeepVariant that can call CLR data. However, this requires us to write a special candidate generation logic to deal with the higher error rate. Based on what we perceive for the direction of future use in the genomics community, we think that data generated will be increasingly HiFi, so we have not been able to highly prioritize CLR models. Feedback from users like yourself will be useful to us in evaluating if that prioritization makes sense. For now, I can't commit to a timeframe under which we would support a PacBio CLR model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/347
https://github.com/google/deepvariant/issues/347:804,energy efficiency,model,model,804,"Hi @jdmontenegro . For the question about multi-allelic heterozygous calls - yes, DeepVariant is able to all 1/2 events, and will represent these in one line as a GT 1/2 call in the VCF. For CLR calling in DeepVariant. It is theoretically possible for us to make a model for DeepVariant that can call CLR data. However, this requires us to write a special candidate generation logic to deal with the higher error rate. Based on what we perceive for the direction of future use in the genomics community, we think that data generated will be increasingly HiFi, so we have not been able to highly prioritize CLR models. Feedback from users like yourself will be useful to us in evaluating if that prioritization makes sense. For now, I can't commit to a timeframe under which we would support a PacBio CLR model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/347
https://github.com/google/deepvariant/issues/347:113,integrability,event,events,113,"Hi @jdmontenegro . For the question about multi-allelic heterozygous calls - yes, DeepVariant is able to all 1/2 events, and will represent these in one line as a GT 1/2 call in the VCF. For CLR calling in DeepVariant. It is theoretically possible for us to make a model for DeepVariant that can call CLR data. However, this requires us to write a special candidate generation logic to deal with the higher error rate. Based on what we perceive for the direction of future use in the genomics community, we think that data generated will be increasingly HiFi, so we have not been able to highly prioritize CLR models. Feedback from users like yourself will be useful to us in evaluating if that prioritization makes sense. For now, I can't commit to a timeframe under which we would support a PacBio CLR model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/347
https://github.com/google/deepvariant/issues/347:793,modifiability,Pac,PacBio,793,"Hi @jdmontenegro . For the question about multi-allelic heterozygous calls - yes, DeepVariant is able to all 1/2 events, and will represent these in one line as a GT 1/2 call in the VCF. For CLR calling in DeepVariant. It is theoretically possible for us to make a model for DeepVariant that can call CLR data. However, this requires us to write a special candidate generation logic to deal with the higher error rate. Based on what we perceive for the direction of future use in the genomics community, we think that data generated will be increasingly HiFi, so we have not been able to highly prioritize CLR models. Feedback from users like yourself will be useful to us in evaluating if that prioritization makes sense. For now, I can't commit to a timeframe under which we would support a PacBio CLR model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/347
https://github.com/google/deepvariant/issues/347:407,performance,error,error,407,"Hi @jdmontenegro . For the question about multi-allelic heterozygous calls - yes, DeepVariant is able to all 1/2 events, and will represent these in one line as a GT 1/2 call in the VCF. For CLR calling in DeepVariant. It is theoretically possible for us to make a model for DeepVariant that can call CLR data. However, this requires us to write a special candidate generation logic to deal with the higher error rate. Based on what we perceive for the direction of future use in the genomics community, we think that data generated will be increasingly HiFi, so we have not been able to highly prioritize CLR models. Feedback from users like yourself will be useful to us in evaluating if that prioritization makes sense. For now, I can't commit to a timeframe under which we would support a PacBio CLR model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/347
https://github.com/google/deepvariant/issues/347:752,performance,time,timeframe,752,"Hi @jdmontenegro . For the question about multi-allelic heterozygous calls - yes, DeepVariant is able to all 1/2 events, and will represent these in one line as a GT 1/2 call in the VCF. For CLR calling in DeepVariant. It is theoretically possible for us to make a model for DeepVariant that can call CLR data. However, this requires us to write a special candidate generation logic to deal with the higher error rate. Based on what we perceive for the direction of future use in the genomics community, we think that data generated will be increasingly HiFi, so we have not been able to highly prioritize CLR models. Feedback from users like yourself will be useful to us in evaluating if that prioritization makes sense. For now, I can't commit to a timeframe under which we would support a PacBio CLR model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/347
https://github.com/google/deepvariant/issues/347:377,safety,log,logic,377,"Hi @jdmontenegro . For the question about multi-allelic heterozygous calls - yes, DeepVariant is able to all 1/2 events, and will represent these in one line as a GT 1/2 call in the VCF. For CLR calling in DeepVariant. It is theoretically possible for us to make a model for DeepVariant that can call CLR data. However, this requires us to write a special candidate generation logic to deal with the higher error rate. Based on what we perceive for the direction of future use in the genomics community, we think that data generated will be increasingly HiFi, so we have not been able to highly prioritize CLR models. Feedback from users like yourself will be useful to us in evaluating if that prioritization makes sense. For now, I can't commit to a timeframe under which we would support a PacBio CLR model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/347
https://github.com/google/deepvariant/issues/347:407,safety,error,error,407,"Hi @jdmontenegro . For the question about multi-allelic heterozygous calls - yes, DeepVariant is able to all 1/2 events, and will represent these in one line as a GT 1/2 call in the VCF. For CLR calling in DeepVariant. It is theoretically possible for us to make a model for DeepVariant that can call CLR data. However, this requires us to write a special candidate generation logic to deal with the higher error rate. Based on what we perceive for the direction of future use in the genomics community, we think that data generated will be increasingly HiFi, so we have not been able to highly prioritize CLR models. Feedback from users like yourself will be useful to us in evaluating if that prioritization makes sense. For now, I can't commit to a timeframe under which we would support a PacBio CLR model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/347
https://github.com/google/deepvariant/issues/347:265,security,model,model,265,"Hi @jdmontenegro . For the question about multi-allelic heterozygous calls - yes, DeepVariant is able to all 1/2 events, and will represent these in one line as a GT 1/2 call in the VCF. For CLR calling in DeepVariant. It is theoretically possible for us to make a model for DeepVariant that can call CLR data. However, this requires us to write a special candidate generation logic to deal with the higher error rate. Based on what we perceive for the direction of future use in the genomics community, we think that data generated will be increasingly HiFi, so we have not been able to highly prioritize CLR models. Feedback from users like yourself will be useful to us in evaluating if that prioritization makes sense. For now, I can't commit to a timeframe under which we would support a PacBio CLR model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/347
https://github.com/google/deepvariant/issues/347:377,security,log,logic,377,"Hi @jdmontenegro . For the question about multi-allelic heterozygous calls - yes, DeepVariant is able to all 1/2 events, and will represent these in one line as a GT 1/2 call in the VCF. For CLR calling in DeepVariant. It is theoretically possible for us to make a model for DeepVariant that can call CLR data. However, this requires us to write a special candidate generation logic to deal with the higher error rate. Based on what we perceive for the direction of future use in the genomics community, we think that data generated will be increasingly HiFi, so we have not been able to highly prioritize CLR models. Feedback from users like yourself will be useful to us in evaluating if that prioritization makes sense. For now, I can't commit to a timeframe under which we would support a PacBio CLR model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/347
https://github.com/google/deepvariant/issues/347:610,security,model,models,610,"Hi @jdmontenegro . For the question about multi-allelic heterozygous calls - yes, DeepVariant is able to all 1/2 events, and will represent these in one line as a GT 1/2 call in the VCF. For CLR calling in DeepVariant. It is theoretically possible for us to make a model for DeepVariant that can call CLR data. However, this requires us to write a special candidate generation logic to deal with the higher error rate. Based on what we perceive for the direction of future use in the genomics community, we think that data generated will be increasingly HiFi, so we have not been able to highly prioritize CLR models. Feedback from users like yourself will be useful to us in evaluating if that prioritization makes sense. For now, I can't commit to a timeframe under which we would support a PacBio CLR model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/347
https://github.com/google/deepvariant/issues/347:804,security,model,model,804,"Hi @jdmontenegro . For the question about multi-allelic heterozygous calls - yes, DeepVariant is able to all 1/2 events, and will represent these in one line as a GT 1/2 call in the VCF. For CLR calling in DeepVariant. It is theoretically possible for us to make a model for DeepVariant that can call CLR data. However, this requires us to write a special candidate generation logic to deal with the higher error rate. Based on what we perceive for the direction of future use in the genomics community, we think that data generated will be increasingly HiFi, so we have not been able to highly prioritize CLR models. Feedback from users like yourself will be useful to us in evaluating if that prioritization makes sense. For now, I can't commit to a timeframe under which we would support a PacBio CLR model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/347
https://github.com/google/deepvariant/issues/347:377,testability,log,logic,377,"Hi @jdmontenegro . For the question about multi-allelic heterozygous calls - yes, DeepVariant is able to all 1/2 events, and will represent these in one line as a GT 1/2 call in the VCF. For CLR calling in DeepVariant. It is theoretically possible for us to make a model for DeepVariant that can call CLR data. However, this requires us to write a special candidate generation logic to deal with the higher error rate. Based on what we perceive for the direction of future use in the genomics community, we think that data generated will be increasingly HiFi, so we have not been able to highly prioritize CLR models. Feedback from users like yourself will be useful to us in evaluating if that prioritization makes sense. For now, I can't commit to a timeframe under which we would support a PacBio CLR model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/347
https://github.com/google/deepvariant/issues/347:407,usability,error,error,407,"Hi @jdmontenegro . For the question about multi-allelic heterozygous calls - yes, DeepVariant is able to all 1/2 events, and will represent these in one line as a GT 1/2 call in the VCF. For CLR calling in DeepVariant. It is theoretically possible for us to make a model for DeepVariant that can call CLR data. However, this requires us to write a special candidate generation logic to deal with the higher error rate. Based on what we perceive for the direction of future use in the genomics community, we think that data generated will be increasingly HiFi, so we have not been able to highly prioritize CLR models. Feedback from users like yourself will be useful to us in evaluating if that prioritization makes sense. For now, I can't commit to a timeframe under which we would support a PacBio CLR model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/347
https://github.com/google/deepvariant/issues/347:618,usability,Feedback,Feedback,618,"Hi @jdmontenegro . For the question about multi-allelic heterozygous calls - yes, DeepVariant is able to all 1/2 events, and will represent these in one line as a GT 1/2 call in the VCF. For CLR calling in DeepVariant. It is theoretically possible for us to make a model for DeepVariant that can call CLR data. However, this requires us to write a special candidate generation logic to deal with the higher error rate. Based on what we perceive for the direction of future use in the genomics community, we think that data generated will be increasingly HiFi, so we have not been able to highly prioritize CLR models. Feedback from users like yourself will be useful to us in evaluating if that prioritization makes sense. For now, I can't commit to a timeframe under which we would support a PacBio CLR model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/347
https://github.com/google/deepvariant/issues/347:632,usability,user,users,632,"Hi @jdmontenegro . For the question about multi-allelic heterozygous calls - yes, DeepVariant is able to all 1/2 events, and will represent these in one line as a GT 1/2 call in the VCF. For CLR calling in DeepVariant. It is theoretically possible for us to make a model for DeepVariant that can call CLR data. However, this requires us to write a special candidate generation logic to deal with the higher error rate. Based on what we perceive for the direction of future use in the genomics community, we think that data generated will be increasingly HiFi, so we have not been able to highly prioritize CLR models. Feedback from users like yourself will be useful to us in evaluating if that prioritization makes sense. For now, I can't commit to a timeframe under which we would support a PacBio CLR model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/347
https://github.com/google/deepvariant/issues/347:783,usability,support,support,783,"Hi @jdmontenegro . For the question about multi-allelic heterozygous calls - yes, DeepVariant is able to all 1/2 events, and will represent these in one line as a GT 1/2 call in the VCF. For CLR calling in DeepVariant. It is theoretically possible for us to make a model for DeepVariant that can call CLR data. However, this requires us to write a special candidate generation logic to deal with the higher error rate. Based on what we perceive for the direction of future use in the genomics community, we think that data generated will be increasingly HiFi, so we have not been able to highly prioritize CLR models. Feedback from users like yourself will be useful to us in evaluating if that prioritization makes sense. For now, I can't commit to a timeframe under which we would support a PacBio CLR model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/347
https://github.com/google/deepvariant/issues/347:1251,availability,error,error,1251,"Dear Andrew,. Thank you for your quick reply. I agree with you that most sequencing and. resequencing projects will move towards HiFi reads rather than CLR reads. However, there is a lot of CLR sequencing data that has been generated in. the past couple of years and continues to be produced currently and could. still be useful for groups without the means to resequence using the novel. HiFi reads. So, I definitely see a niche in a large part of the. bioinformatics community that do a lot of data reusing (nowadays data. parasites). So, if there is anything we can do to help you n development,. please feel free to let me know how we can collaborate. Kind regards,. Juan D. Montenegro. El mar., 15 sept. 2020 a las 18:37, Andrew Carroll (<. notifications@github.com>) escribió:. > Hi @jdmontenegro <https://github.com/jdmontenegro>. >. > For the question about multi-allelic heterozygous calls - yes, DeepVariant. > is able to all 1/2 events, and will represent these in one line as a GT 1/2. > call in the VCF. >. > For CLR calling in DeepVariant. It is theoretically possible for us to. > make a model for DeepVariant that can call CLR data. However, this requires. > us to write a special candidate generation logic to deal with the higher. > error rate. Based on what we perceive for the direction of future use in. > the genomics community, we think that data generated will be increasingly. > HiFi, so we have not been able to highly prioritize CLR models. Feedback. > from users like yourself will be useful to us in evaluating if that. > prioritization makes sense. For now, I can't commit to a timeframe under. > which we would support a PacBio CLR model. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/347#issuecomment-693053180>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ACHSLOV5RPVLTVGDW2A44X3SF73E7ANCNFSM4RNQJZYQ>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/347
https://github.com/google/deepvariant/issues/347:267,deployability,continu,continues,267,"Dear Andrew,. Thank you for your quick reply. I agree with you that most sequencing and. resequencing projects will move towards HiFi reads rather than CLR reads. However, there is a lot of CLR sequencing data that has been generated in. the past couple of years and continues to be produced currently and could. still be useful for groups without the means to resequence using the novel. HiFi reads. So, I definitely see a niche in a large part of the. bioinformatics community that do a lot of data reusing (nowadays data. parasites). So, if there is anything we can do to help you n development,. please feel free to let me know how we can collaborate. Kind regards,. Juan D. Montenegro. El mar., 15 sept. 2020 a las 18:37, Andrew Carroll (<. notifications@github.com>) escribió:. > Hi @jdmontenegro <https://github.com/jdmontenegro>. >. > For the question about multi-allelic heterozygous calls - yes, DeepVariant. > is able to all 1/2 events, and will represent these in one line as a GT 1/2. > call in the VCF. >. > For CLR calling in DeepVariant. It is theoretically possible for us to. > make a model for DeepVariant that can call CLR data. However, this requires. > us to write a special candidate generation logic to deal with the higher. > error rate. Based on what we perceive for the direction of future use in. > the genomics community, we think that data generated will be increasingly. > HiFi, so we have not been able to highly prioritize CLR models. Feedback. > from users like yourself will be useful to us in evaluating if that. > prioritization makes sense. For now, I can't commit to a timeframe under. > which we would support a PacBio CLR model. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/347#issuecomment-693053180>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ACHSLOV5RPVLTVGDW2A44X3SF73E7ANCNFSM4RNQJZYQ>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/347
https://github.com/google/deepvariant/issues/347:1218,deployability,log,logic,1218,"Dear Andrew,. Thank you for your quick reply. I agree with you that most sequencing and. resequencing projects will move towards HiFi reads rather than CLR reads. However, there is a lot of CLR sequencing data that has been generated in. the past couple of years and continues to be produced currently and could. still be useful for groups without the means to resequence using the novel. HiFi reads. So, I definitely see a niche in a large part of the. bioinformatics community that do a lot of data reusing (nowadays data. parasites). So, if there is anything we can do to help you n development,. please feel free to let me know how we can collaborate. Kind regards,. Juan D. Montenegro. El mar., 15 sept. 2020 a las 18:37, Andrew Carroll (<. notifications@github.com>) escribió:. > Hi @jdmontenegro <https://github.com/jdmontenegro>. >. > For the question about multi-allelic heterozygous calls - yes, DeepVariant. > is able to all 1/2 events, and will represent these in one line as a GT 1/2. > call in the VCF. >. > For CLR calling in DeepVariant. It is theoretically possible for us to. > make a model for DeepVariant that can call CLR data. However, this requires. > us to write a special candidate generation logic to deal with the higher. > error rate. Based on what we perceive for the direction of future use in. > the genomics community, we think that data generated will be increasingly. > HiFi, so we have not been able to highly prioritize CLR models. Feedback. > from users like yourself will be useful to us in evaluating if that. > prioritization makes sense. For now, I can't commit to a timeframe under. > which we would support a PacBio CLR model. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/347#issuecomment-693053180>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ACHSLOV5RPVLTVGDW2A44X3SF73E7ANCNFSM4RNQJZYQ>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/347
https://github.com/google/deepvariant/issues/347:292,energy efficiency,current,currently,292,"Dear Andrew,. Thank you for your quick reply. I agree with you that most sequencing and. resequencing projects will move towards HiFi reads rather than CLR reads. However, there is a lot of CLR sequencing data that has been generated in. the past couple of years and continues to be produced currently and could. still be useful for groups without the means to resequence using the novel. HiFi reads. So, I definitely see a niche in a large part of the. bioinformatics community that do a lot of data reusing (nowadays data. parasites). So, if there is anything we can do to help you n development,. please feel free to let me know how we can collaborate. Kind regards,. Juan D. Montenegro. El mar., 15 sept. 2020 a las 18:37, Andrew Carroll (<. notifications@github.com>) escribió:. > Hi @jdmontenegro <https://github.com/jdmontenegro>. >. > For the question about multi-allelic heterozygous calls - yes, DeepVariant. > is able to all 1/2 events, and will represent these in one line as a GT 1/2. > call in the VCF. >. > For CLR calling in DeepVariant. It is theoretically possible for us to. > make a model for DeepVariant that can call CLR data. However, this requires. > us to write a special candidate generation logic to deal with the higher. > error rate. Based on what we perceive for the direction of future use in. > the genomics community, we think that data generated will be increasingly. > HiFi, so we have not been able to highly prioritize CLR models. Feedback. > from users like yourself will be useful to us in evaluating if that. > prioritization makes sense. For now, I can't commit to a timeframe under. > which we would support a PacBio CLR model. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/347#issuecomment-693053180>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ACHSLOV5RPVLTVGDW2A44X3SF73E7ANCNFSM4RNQJZYQ>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/347
https://github.com/google/deepvariant/issues/347:1103,energy efficiency,model,model,1103,"Dear Andrew,. Thank you for your quick reply. I agree with you that most sequencing and. resequencing projects will move towards HiFi reads rather than CLR reads. However, there is a lot of CLR sequencing data that has been generated in. the past couple of years and continues to be produced currently and could. still be useful for groups without the means to resequence using the novel. HiFi reads. So, I definitely see a niche in a large part of the. bioinformatics community that do a lot of data reusing (nowadays data. parasites). So, if there is anything we can do to help you n development,. please feel free to let me know how we can collaborate. Kind regards,. Juan D. Montenegro. El mar., 15 sept. 2020 a las 18:37, Andrew Carroll (<. notifications@github.com>) escribió:. > Hi @jdmontenegro <https://github.com/jdmontenegro>. >. > For the question about multi-allelic heterozygous calls - yes, DeepVariant. > is able to all 1/2 events, and will represent these in one line as a GT 1/2. > call in the VCF. >. > For CLR calling in DeepVariant. It is theoretically possible for us to. > make a model for DeepVariant that can call CLR data. However, this requires. > us to write a special candidate generation logic to deal with the higher. > error rate. Based on what we perceive for the direction of future use in. > the genomics community, we think that data generated will be increasingly. > HiFi, so we have not been able to highly prioritize CLR models. Feedback. > from users like yourself will be useful to us in evaluating if that. > prioritization makes sense. For now, I can't commit to a timeframe under. > which we would support a PacBio CLR model. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/347#issuecomment-693053180>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ACHSLOV5RPVLTVGDW2A44X3SF73E7ANCNFSM4RNQJZYQ>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/347
https://github.com/google/deepvariant/issues/347:1460,energy efficiency,model,models,1460,"Dear Andrew,. Thank you for your quick reply. I agree with you that most sequencing and. resequencing projects will move towards HiFi reads rather than CLR reads. However, there is a lot of CLR sequencing data that has been generated in. the past couple of years and continues to be produced currently and could. still be useful for groups without the means to resequence using the novel. HiFi reads. So, I definitely see a niche in a large part of the. bioinformatics community that do a lot of data reusing (nowadays data. parasites). So, if there is anything we can do to help you n development,. please feel free to let me know how we can collaborate. Kind regards,. Juan D. Montenegro. El mar., 15 sept. 2020 a las 18:37, Andrew Carroll (<. notifications@github.com>) escribió:. > Hi @jdmontenegro <https://github.com/jdmontenegro>. >. > For the question about multi-allelic heterozygous calls - yes, DeepVariant. > is able to all 1/2 events, and will represent these in one line as a GT 1/2. > call in the VCF. >. > For CLR calling in DeepVariant. It is theoretically possible for us to. > make a model for DeepVariant that can call CLR data. However, this requires. > us to write a special candidate generation logic to deal with the higher. > error rate. Based on what we perceive for the direction of future use in. > the genomics community, we think that data generated will be increasingly. > HiFi, so we have not been able to highly prioritize CLR models. Feedback. > from users like yourself will be useful to us in evaluating if that. > prioritization makes sense. For now, I can't commit to a timeframe under. > which we would support a PacBio CLR model. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/347#issuecomment-693053180>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ACHSLOV5RPVLTVGDW2A44X3SF73E7ANCNFSM4RNQJZYQ>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/347
https://github.com/google/deepvariant/issues/347:1663,energy efficiency,model,model,1663,"Dear Andrew,. Thank you for your quick reply. I agree with you that most sequencing and. resequencing projects will move towards HiFi reads rather than CLR reads. However, there is a lot of CLR sequencing data that has been generated in. the past couple of years and continues to be produced currently and could. still be useful for groups without the means to resequence using the novel. HiFi reads. So, I definitely see a niche in a large part of the. bioinformatics community that do a lot of data reusing (nowadays data. parasites). So, if there is anything we can do to help you n development,. please feel free to let me know how we can collaborate. Kind regards,. Juan D. Montenegro. El mar., 15 sept. 2020 a las 18:37, Andrew Carroll (<. notifications@github.com>) escribió:. > Hi @jdmontenegro <https://github.com/jdmontenegro>. >. > For the question about multi-allelic heterozygous calls - yes, DeepVariant. > is able to all 1/2 events, and will represent these in one line as a GT 1/2. > call in the VCF. >. > For CLR calling in DeepVariant. It is theoretically possible for us to. > make a model for DeepVariant that can call CLR data. However, this requires. > us to write a special candidate generation logic to deal with the higher. > error rate. Based on what we perceive for the direction of future use in. > the genomics community, we think that data generated will be increasingly. > HiFi, so we have not been able to highly prioritize CLR models. Feedback. > from users like yourself will be useful to us in evaluating if that. > prioritization makes sense. For now, I can't commit to a timeframe under. > which we would support a PacBio CLR model. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/347#issuecomment-693053180>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ACHSLOV5RPVLTVGDW2A44X3SF73E7ANCNFSM4RNQJZYQ>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/347
https://github.com/google/deepvariant/issues/347:247,integrability,coupl,couple,247,"Dear Andrew,. Thank you for your quick reply. I agree with you that most sequencing and. resequencing projects will move towards HiFi reads rather than CLR reads. However, there is a lot of CLR sequencing data that has been generated in. the past couple of years and continues to be produced currently and could. still be useful for groups without the means to resequence using the novel. HiFi reads. So, I definitely see a niche in a large part of the. bioinformatics community that do a lot of data reusing (nowadays data. parasites). So, if there is anything we can do to help you n development,. please feel free to let me know how we can collaborate. Kind regards,. Juan D. Montenegro. El mar., 15 sept. 2020 a las 18:37, Andrew Carroll (<. notifications@github.com>) escribió:. > Hi @jdmontenegro <https://github.com/jdmontenegro>. >. > For the question about multi-allelic heterozygous calls - yes, DeepVariant. > is able to all 1/2 events, and will represent these in one line as a GT 1/2. > call in the VCF. >. > For CLR calling in DeepVariant. It is theoretically possible for us to. > make a model for DeepVariant that can call CLR data. However, this requires. > us to write a special candidate generation logic to deal with the higher. > error rate. Based on what we perceive for the direction of future use in. > the genomics community, we think that data generated will be increasingly. > HiFi, so we have not been able to highly prioritize CLR models. Feedback. > from users like yourself will be useful to us in evaluating if that. > prioritization makes sense. For now, I can't commit to a timeframe under. > which we would support a PacBio CLR model. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/347#issuecomment-693053180>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ACHSLOV5RPVLTVGDW2A44X3SF73E7ANCNFSM4RNQJZYQ>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/347
https://github.com/google/deepvariant/issues/347:940,integrability,event,events,940,"Dear Andrew,. Thank you for your quick reply. I agree with you that most sequencing and. resequencing projects will move towards HiFi reads rather than CLR reads. However, there is a lot of CLR sequencing data that has been generated in. the past couple of years and continues to be produced currently and could. still be useful for groups without the means to resequence using the novel. HiFi reads. So, I definitely see a niche in a large part of the. bioinformatics community that do a lot of data reusing (nowadays data. parasites). So, if there is anything we can do to help you n development,. please feel free to let me know how we can collaborate. Kind regards,. Juan D. Montenegro. El mar., 15 sept. 2020 a las 18:37, Andrew Carroll (<. notifications@github.com>) escribió:. > Hi @jdmontenegro <https://github.com/jdmontenegro>. >. > For the question about multi-allelic heterozygous calls - yes, DeepVariant. > is able to all 1/2 events, and will represent these in one line as a GT 1/2. > call in the VCF. >. > For CLR calling in DeepVariant. It is theoretically possible for us to. > make a model for DeepVariant that can call CLR data. However, this requires. > us to write a special candidate generation logic to deal with the higher. > error rate. Based on what we perceive for the direction of future use in. > the genomics community, we think that data generated will be increasingly. > HiFi, so we have not been able to highly prioritize CLR models. Feedback. > from users like yourself will be useful to us in evaluating if that. > prioritization makes sense. For now, I can't commit to a timeframe under. > which we would support a PacBio CLR model. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/347#issuecomment-693053180>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ACHSLOV5RPVLTVGDW2A44X3SF73E7ANCNFSM4RNQJZYQ>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/347
https://github.com/google/deepvariant/issues/347:247,modifiability,coupl,couple,247,"Dear Andrew,. Thank you for your quick reply. I agree with you that most sequencing and. resequencing projects will move towards HiFi reads rather than CLR reads. However, there is a lot of CLR sequencing data that has been generated in. the past couple of years and continues to be produced currently and could. still be useful for groups without the means to resequence using the novel. HiFi reads. So, I definitely see a niche in a large part of the. bioinformatics community that do a lot of data reusing (nowadays data. parasites). So, if there is anything we can do to help you n development,. please feel free to let me know how we can collaborate. Kind regards,. Juan D. Montenegro. El mar., 15 sept. 2020 a las 18:37, Andrew Carroll (<. notifications@github.com>) escribió:. > Hi @jdmontenegro <https://github.com/jdmontenegro>. >. > For the question about multi-allelic heterozygous calls - yes, DeepVariant. > is able to all 1/2 events, and will represent these in one line as a GT 1/2. > call in the VCF. >. > For CLR calling in DeepVariant. It is theoretically possible for us to. > make a model for DeepVariant that can call CLR data. However, this requires. > us to write a special candidate generation logic to deal with the higher. > error rate. Based on what we perceive for the direction of future use in. > the genomics community, we think that data generated will be increasingly. > HiFi, so we have not been able to highly prioritize CLR models. Feedback. > from users like yourself will be useful to us in evaluating if that. > prioritization makes sense. For now, I can't commit to a timeframe under. > which we would support a PacBio CLR model. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/347#issuecomment-693053180>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ACHSLOV5RPVLTVGDW2A44X3SF73E7ANCNFSM4RNQJZYQ>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/347
https://github.com/google/deepvariant/issues/347:501,modifiability,reu,reusing,501,"Dear Andrew,. Thank you for your quick reply. I agree with you that most sequencing and. resequencing projects will move towards HiFi reads rather than CLR reads. However, there is a lot of CLR sequencing data that has been generated in. the past couple of years and continues to be produced currently and could. still be useful for groups without the means to resequence using the novel. HiFi reads. So, I definitely see a niche in a large part of the. bioinformatics community that do a lot of data reusing (nowadays data. parasites). So, if there is anything we can do to help you n development,. please feel free to let me know how we can collaborate. Kind regards,. Juan D. Montenegro. El mar., 15 sept. 2020 a las 18:37, Andrew Carroll (<. notifications@github.com>) escribió:. > Hi @jdmontenegro <https://github.com/jdmontenegro>. >. > For the question about multi-allelic heterozygous calls - yes, DeepVariant. > is able to all 1/2 events, and will represent these in one line as a GT 1/2. > call in the VCF. >. > For CLR calling in DeepVariant. It is theoretically possible for us to. > make a model for DeepVariant that can call CLR data. However, this requires. > us to write a special candidate generation logic to deal with the higher. > error rate. Based on what we perceive for the direction of future use in. > the genomics community, we think that data generated will be increasingly. > HiFi, so we have not been able to highly prioritize CLR models. Feedback. > from users like yourself will be useful to us in evaluating if that. > prioritization makes sense. For now, I can't commit to a timeframe under. > which we would support a PacBio CLR model. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/347#issuecomment-693053180>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ACHSLOV5RPVLTVGDW2A44X3SF73E7ANCNFSM4RNQJZYQ>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/347
https://github.com/google/deepvariant/issues/347:1652,modifiability,Pac,PacBio,1652,"Dear Andrew,. Thank you for your quick reply. I agree with you that most sequencing and. resequencing projects will move towards HiFi reads rather than CLR reads. However, there is a lot of CLR sequencing data that has been generated in. the past couple of years and continues to be produced currently and could. still be useful for groups without the means to resequence using the novel. HiFi reads. So, I definitely see a niche in a large part of the. bioinformatics community that do a lot of data reusing (nowadays data. parasites). So, if there is anything we can do to help you n development,. please feel free to let me know how we can collaborate. Kind regards,. Juan D. Montenegro. El mar., 15 sept. 2020 a las 18:37, Andrew Carroll (<. notifications@github.com>) escribió:. > Hi @jdmontenegro <https://github.com/jdmontenegro>. >. > For the question about multi-allelic heterozygous calls - yes, DeepVariant. > is able to all 1/2 events, and will represent these in one line as a GT 1/2. > call in the VCF. >. > For CLR calling in DeepVariant. It is theoretically possible for us to. > make a model for DeepVariant that can call CLR data. However, this requires. > us to write a special candidate generation logic to deal with the higher. > error rate. Based on what we perceive for the direction of future use in. > the genomics community, we think that data generated will be increasingly. > HiFi, so we have not been able to highly prioritize CLR models. Feedback. > from users like yourself will be useful to us in evaluating if that. > prioritization makes sense. For now, I can't commit to a timeframe under. > which we would support a PacBio CLR model. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/347#issuecomment-693053180>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ACHSLOV5RPVLTVGDW2A44X3SF73E7ANCNFSM4RNQJZYQ>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/347
https://github.com/google/deepvariant/issues/347:1251,performance,error,error,1251,"Dear Andrew,. Thank you for your quick reply. I agree with you that most sequencing and. resequencing projects will move towards HiFi reads rather than CLR reads. However, there is a lot of CLR sequencing data that has been generated in. the past couple of years and continues to be produced currently and could. still be useful for groups without the means to resequence using the novel. HiFi reads. So, I definitely see a niche in a large part of the. bioinformatics community that do a lot of data reusing (nowadays data. parasites). So, if there is anything we can do to help you n development,. please feel free to let me know how we can collaborate. Kind regards,. Juan D. Montenegro. El mar., 15 sept. 2020 a las 18:37, Andrew Carroll (<. notifications@github.com>) escribió:. > Hi @jdmontenegro <https://github.com/jdmontenegro>. >. > For the question about multi-allelic heterozygous calls - yes, DeepVariant. > is able to all 1/2 events, and will represent these in one line as a GT 1/2. > call in the VCF. >. > For CLR calling in DeepVariant. It is theoretically possible for us to. > make a model for DeepVariant that can call CLR data. However, this requires. > us to write a special candidate generation logic to deal with the higher. > error rate. Based on what we perceive for the direction of future use in. > the genomics community, we think that data generated will be increasingly. > HiFi, so we have not been able to highly prioritize CLR models. Feedback. > from users like yourself will be useful to us in evaluating if that. > prioritization makes sense. For now, I can't commit to a timeframe under. > which we would support a PacBio CLR model. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/347#issuecomment-693053180>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ACHSLOV5RPVLTVGDW2A44X3SF73E7ANCNFSM4RNQJZYQ>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/347
https://github.com/google/deepvariant/issues/347:1608,performance,time,timeframe,1608,"Dear Andrew,. Thank you for your quick reply. I agree with you that most sequencing and. resequencing projects will move towards HiFi reads rather than CLR reads. However, there is a lot of CLR sequencing data that has been generated in. the past couple of years and continues to be produced currently and could. still be useful for groups without the means to resequence using the novel. HiFi reads. So, I definitely see a niche in a large part of the. bioinformatics community that do a lot of data reusing (nowadays data. parasites). So, if there is anything we can do to help you n development,. please feel free to let me know how we can collaborate. Kind regards,. Juan D. Montenegro. El mar., 15 sept. 2020 a las 18:37, Andrew Carroll (<. notifications@github.com>) escribió:. > Hi @jdmontenegro <https://github.com/jdmontenegro>. >. > For the question about multi-allelic heterozygous calls - yes, DeepVariant. > is able to all 1/2 events, and will represent these in one line as a GT 1/2. > call in the VCF. >. > For CLR calling in DeepVariant. It is theoretically possible for us to. > make a model for DeepVariant that can call CLR data. However, this requires. > us to write a special candidate generation logic to deal with the higher. > error rate. Based on what we perceive for the direction of future use in. > the genomics community, we think that data generated will be increasingly. > HiFi, so we have not been able to highly prioritize CLR models. Feedback. > from users like yourself will be useful to us in evaluating if that. > prioritization makes sense. For now, I can't commit to a timeframe under. > which we would support a PacBio CLR model. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/347#issuecomment-693053180>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ACHSLOV5RPVLTVGDW2A44X3SF73E7ANCNFSM4RNQJZYQ>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/347
https://github.com/google/deepvariant/issues/347:1218,safety,log,logic,1218,"Dear Andrew,. Thank you for your quick reply. I agree with you that most sequencing and. resequencing projects will move towards HiFi reads rather than CLR reads. However, there is a lot of CLR sequencing data that has been generated in. the past couple of years and continues to be produced currently and could. still be useful for groups without the means to resequence using the novel. HiFi reads. So, I definitely see a niche in a large part of the. bioinformatics community that do a lot of data reusing (nowadays data. parasites). So, if there is anything we can do to help you n development,. please feel free to let me know how we can collaborate. Kind regards,. Juan D. Montenegro. El mar., 15 sept. 2020 a las 18:37, Andrew Carroll (<. notifications@github.com>) escribió:. > Hi @jdmontenegro <https://github.com/jdmontenegro>. >. > For the question about multi-allelic heterozygous calls - yes, DeepVariant. > is able to all 1/2 events, and will represent these in one line as a GT 1/2. > call in the VCF. >. > For CLR calling in DeepVariant. It is theoretically possible for us to. > make a model for DeepVariant that can call CLR data. However, this requires. > us to write a special candidate generation logic to deal with the higher. > error rate. Based on what we perceive for the direction of future use in. > the genomics community, we think that data generated will be increasingly. > HiFi, so we have not been able to highly prioritize CLR models. Feedback. > from users like yourself will be useful to us in evaluating if that. > prioritization makes sense. For now, I can't commit to a timeframe under. > which we would support a PacBio CLR model. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/347#issuecomment-693053180>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ACHSLOV5RPVLTVGDW2A44X3SF73E7ANCNFSM4RNQJZYQ>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/347
https://github.com/google/deepvariant/issues/347:1251,safety,error,error,1251,"Dear Andrew,. Thank you for your quick reply. I agree with you that most sequencing and. resequencing projects will move towards HiFi reads rather than CLR reads. However, there is a lot of CLR sequencing data that has been generated in. the past couple of years and continues to be produced currently and could. still be useful for groups without the means to resequence using the novel. HiFi reads. So, I definitely see a niche in a large part of the. bioinformatics community that do a lot of data reusing (nowadays data. parasites). So, if there is anything we can do to help you n development,. please feel free to let me know how we can collaborate. Kind regards,. Juan D. Montenegro. El mar., 15 sept. 2020 a las 18:37, Andrew Carroll (<. notifications@github.com>) escribió:. > Hi @jdmontenegro <https://github.com/jdmontenegro>. >. > For the question about multi-allelic heterozygous calls - yes, DeepVariant. > is able to all 1/2 events, and will represent these in one line as a GT 1/2. > call in the VCF. >. > For CLR calling in DeepVariant. It is theoretically possible for us to. > make a model for DeepVariant that can call CLR data. However, this requires. > us to write a special candidate generation logic to deal with the higher. > error rate. Based on what we perceive for the direction of future use in. > the genomics community, we think that data generated will be increasingly. > HiFi, so we have not been able to highly prioritize CLR models. Feedback. > from users like yourself will be useful to us in evaluating if that. > prioritization makes sense. For now, I can't commit to a timeframe under. > which we would support a PacBio CLR model. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/347#issuecomment-693053180>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ACHSLOV5RPVLTVGDW2A44X3SF73E7ANCNFSM4RNQJZYQ>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/347
https://github.com/google/deepvariant/issues/347:1103,security,model,model,1103,"Dear Andrew,. Thank you for your quick reply. I agree with you that most sequencing and. resequencing projects will move towards HiFi reads rather than CLR reads. However, there is a lot of CLR sequencing data that has been generated in. the past couple of years and continues to be produced currently and could. still be useful for groups without the means to resequence using the novel. HiFi reads. So, I definitely see a niche in a large part of the. bioinformatics community that do a lot of data reusing (nowadays data. parasites). So, if there is anything we can do to help you n development,. please feel free to let me know how we can collaborate. Kind regards,. Juan D. Montenegro. El mar., 15 sept. 2020 a las 18:37, Andrew Carroll (<. notifications@github.com>) escribió:. > Hi @jdmontenegro <https://github.com/jdmontenegro>. >. > For the question about multi-allelic heterozygous calls - yes, DeepVariant. > is able to all 1/2 events, and will represent these in one line as a GT 1/2. > call in the VCF. >. > For CLR calling in DeepVariant. It is theoretically possible for us to. > make a model for DeepVariant that can call CLR data. However, this requires. > us to write a special candidate generation logic to deal with the higher. > error rate. Based on what we perceive for the direction of future use in. > the genomics community, we think that data generated will be increasingly. > HiFi, so we have not been able to highly prioritize CLR models. Feedback. > from users like yourself will be useful to us in evaluating if that. > prioritization makes sense. For now, I can't commit to a timeframe under. > which we would support a PacBio CLR model. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/347#issuecomment-693053180>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ACHSLOV5RPVLTVGDW2A44X3SF73E7ANCNFSM4RNQJZYQ>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/347
https://github.com/google/deepvariant/issues/347:1218,security,log,logic,1218,"Dear Andrew,. Thank you for your quick reply. I agree with you that most sequencing and. resequencing projects will move towards HiFi reads rather than CLR reads. However, there is a lot of CLR sequencing data that has been generated in. the past couple of years and continues to be produced currently and could. still be useful for groups without the means to resequence using the novel. HiFi reads. So, I definitely see a niche in a large part of the. bioinformatics community that do a lot of data reusing (nowadays data. parasites). So, if there is anything we can do to help you n development,. please feel free to let me know how we can collaborate. Kind regards,. Juan D. Montenegro. El mar., 15 sept. 2020 a las 18:37, Andrew Carroll (<. notifications@github.com>) escribió:. > Hi @jdmontenegro <https://github.com/jdmontenegro>. >. > For the question about multi-allelic heterozygous calls - yes, DeepVariant. > is able to all 1/2 events, and will represent these in one line as a GT 1/2. > call in the VCF. >. > For CLR calling in DeepVariant. It is theoretically possible for us to. > make a model for DeepVariant that can call CLR data. However, this requires. > us to write a special candidate generation logic to deal with the higher. > error rate. Based on what we perceive for the direction of future use in. > the genomics community, we think that data generated will be increasingly. > HiFi, so we have not been able to highly prioritize CLR models. Feedback. > from users like yourself will be useful to us in evaluating if that. > prioritization makes sense. For now, I can't commit to a timeframe under. > which we would support a PacBio CLR model. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/347#issuecomment-693053180>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ACHSLOV5RPVLTVGDW2A44X3SF73E7ANCNFSM4RNQJZYQ>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/347
https://github.com/google/deepvariant/issues/347:1460,security,model,models,1460,"Dear Andrew,. Thank you for your quick reply. I agree with you that most sequencing and. resequencing projects will move towards HiFi reads rather than CLR reads. However, there is a lot of CLR sequencing data that has been generated in. the past couple of years and continues to be produced currently and could. still be useful for groups without the means to resequence using the novel. HiFi reads. So, I definitely see a niche in a large part of the. bioinformatics community that do a lot of data reusing (nowadays data. parasites). So, if there is anything we can do to help you n development,. please feel free to let me know how we can collaborate. Kind regards,. Juan D. Montenegro. El mar., 15 sept. 2020 a las 18:37, Andrew Carroll (<. notifications@github.com>) escribió:. > Hi @jdmontenegro <https://github.com/jdmontenegro>. >. > For the question about multi-allelic heterozygous calls - yes, DeepVariant. > is able to all 1/2 events, and will represent these in one line as a GT 1/2. > call in the VCF. >. > For CLR calling in DeepVariant. It is theoretically possible for us to. > make a model for DeepVariant that can call CLR data. However, this requires. > us to write a special candidate generation logic to deal with the higher. > error rate. Based on what we perceive for the direction of future use in. > the genomics community, we think that data generated will be increasingly. > HiFi, so we have not been able to highly prioritize CLR models. Feedback. > from users like yourself will be useful to us in evaluating if that. > prioritization makes sense. For now, I can't commit to a timeframe under. > which we would support a PacBio CLR model. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/347#issuecomment-693053180>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ACHSLOV5RPVLTVGDW2A44X3SF73E7ANCNFSM4RNQJZYQ>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/347
https://github.com/google/deepvariant/issues/347:1663,security,model,model,1663,"Dear Andrew,. Thank you for your quick reply. I agree with you that most sequencing and. resequencing projects will move towards HiFi reads rather than CLR reads. However, there is a lot of CLR sequencing data that has been generated in. the past couple of years and continues to be produced currently and could. still be useful for groups without the means to resequence using the novel. HiFi reads. So, I definitely see a niche in a large part of the. bioinformatics community that do a lot of data reusing (nowadays data. parasites). So, if there is anything we can do to help you n development,. please feel free to let me know how we can collaborate. Kind regards,. Juan D. Montenegro. El mar., 15 sept. 2020 a las 18:37, Andrew Carroll (<. notifications@github.com>) escribió:. > Hi @jdmontenegro <https://github.com/jdmontenegro>. >. > For the question about multi-allelic heterozygous calls - yes, DeepVariant. > is able to all 1/2 events, and will represent these in one line as a GT 1/2. > call in the VCF. >. > For CLR calling in DeepVariant. It is theoretically possible for us to. > make a model for DeepVariant that can call CLR data. However, this requires. > us to write a special candidate generation logic to deal with the higher. > error rate. Based on what we perceive for the direction of future use in. > the genomics community, we think that data generated will be increasingly. > HiFi, so we have not been able to highly prioritize CLR models. Feedback. > from users like yourself will be useful to us in evaluating if that. > prioritization makes sense. For now, I can't commit to a timeframe under. > which we would support a PacBio CLR model. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/347#issuecomment-693053180>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ACHSLOV5RPVLTVGDW2A44X3SF73E7ANCNFSM4RNQJZYQ>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/347
https://github.com/google/deepvariant/issues/347:1926,security,auth,auth,1926,"Dear Andrew,. Thank you for your quick reply. I agree with you that most sequencing and. resequencing projects will move towards HiFi reads rather than CLR reads. However, there is a lot of CLR sequencing data that has been generated in. the past couple of years and continues to be produced currently and could. still be useful for groups without the means to resequence using the novel. HiFi reads. So, I definitely see a niche in a large part of the. bioinformatics community that do a lot of data reusing (nowadays data. parasites). So, if there is anything we can do to help you n development,. please feel free to let me know how we can collaborate. Kind regards,. Juan D. Montenegro. El mar., 15 sept. 2020 a las 18:37, Andrew Carroll (<. notifications@github.com>) escribió:. > Hi @jdmontenegro <https://github.com/jdmontenegro>. >. > For the question about multi-allelic heterozygous calls - yes, DeepVariant. > is able to all 1/2 events, and will represent these in one line as a GT 1/2. > call in the VCF. >. > For CLR calling in DeepVariant. It is theoretically possible for us to. > make a model for DeepVariant that can call CLR data. However, this requires. > us to write a special candidate generation logic to deal with the higher. > error rate. Based on what we perceive for the direction of future use in. > the genomics community, we think that data generated will be increasingly. > HiFi, so we have not been able to highly prioritize CLR models. Feedback. > from users like yourself will be useful to us in evaluating if that. > prioritization makes sense. For now, I can't commit to a timeframe under. > which we would support a PacBio CLR model. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/347#issuecomment-693053180>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ACHSLOV5RPVLTVGDW2A44X3SF73E7ANCNFSM4RNQJZYQ>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/347
https://github.com/google/deepvariant/issues/347:247,testability,coupl,couple,247,"Dear Andrew,. Thank you for your quick reply. I agree with you that most sequencing and. resequencing projects will move towards HiFi reads rather than CLR reads. However, there is a lot of CLR sequencing data that has been generated in. the past couple of years and continues to be produced currently and could. still be useful for groups without the means to resequence using the novel. HiFi reads. So, I definitely see a niche in a large part of the. bioinformatics community that do a lot of data reusing (nowadays data. parasites). So, if there is anything we can do to help you n development,. please feel free to let me know how we can collaborate. Kind regards,. Juan D. Montenegro. El mar., 15 sept. 2020 a las 18:37, Andrew Carroll (<. notifications@github.com>) escribió:. > Hi @jdmontenegro <https://github.com/jdmontenegro>. >. > For the question about multi-allelic heterozygous calls - yes, DeepVariant. > is able to all 1/2 events, and will represent these in one line as a GT 1/2. > call in the VCF. >. > For CLR calling in DeepVariant. It is theoretically possible for us to. > make a model for DeepVariant that can call CLR data. However, this requires. > us to write a special candidate generation logic to deal with the higher. > error rate. Based on what we perceive for the direction of future use in. > the genomics community, we think that data generated will be increasingly. > HiFi, so we have not been able to highly prioritize CLR models. Feedback. > from users like yourself will be useful to us in evaluating if that. > prioritization makes sense. For now, I can't commit to a timeframe under. > which we would support a PacBio CLR model. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/347#issuecomment-693053180>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ACHSLOV5RPVLTVGDW2A44X3SF73E7ANCNFSM4RNQJZYQ>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/347
https://github.com/google/deepvariant/issues/347:1218,testability,log,logic,1218,"Dear Andrew,. Thank you for your quick reply. I agree with you that most sequencing and. resequencing projects will move towards HiFi reads rather than CLR reads. However, there is a lot of CLR sequencing data that has been generated in. the past couple of years and continues to be produced currently and could. still be useful for groups without the means to resequence using the novel. HiFi reads. So, I definitely see a niche in a large part of the. bioinformatics community that do a lot of data reusing (nowadays data. parasites). So, if there is anything we can do to help you n development,. please feel free to let me know how we can collaborate. Kind regards,. Juan D. Montenegro. El mar., 15 sept. 2020 a las 18:37, Andrew Carroll (<. notifications@github.com>) escribió:. > Hi @jdmontenegro <https://github.com/jdmontenegro>. >. > For the question about multi-allelic heterozygous calls - yes, DeepVariant. > is able to all 1/2 events, and will represent these in one line as a GT 1/2. > call in the VCF. >. > For CLR calling in DeepVariant. It is theoretically possible for us to. > make a model for DeepVariant that can call CLR data. However, this requires. > us to write a special candidate generation logic to deal with the higher. > error rate. Based on what we perceive for the direction of future use in. > the genomics community, we think that data generated will be increasingly. > HiFi, so we have not been able to highly prioritize CLR models. Feedback. > from users like yourself will be useful to us in evaluating if that. > prioritization makes sense. For now, I can't commit to a timeframe under. > which we would support a PacBio CLR model. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/347#issuecomment-693053180>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ACHSLOV5RPVLTVGDW2A44X3SF73E7ANCNFSM4RNQJZYQ>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/347
https://github.com/google/deepvariant/issues/347:575,usability,help,help,575,"Dear Andrew,. Thank you for your quick reply. I agree with you that most sequencing and. resequencing projects will move towards HiFi reads rather than CLR reads. However, there is a lot of CLR sequencing data that has been generated in. the past couple of years and continues to be produced currently and could. still be useful for groups without the means to resequence using the novel. HiFi reads. So, I definitely see a niche in a large part of the. bioinformatics community that do a lot of data reusing (nowadays data. parasites). So, if there is anything we can do to help you n development,. please feel free to let me know how we can collaborate. Kind regards,. Juan D. Montenegro. El mar., 15 sept. 2020 a las 18:37, Andrew Carroll (<. notifications@github.com>) escribió:. > Hi @jdmontenegro <https://github.com/jdmontenegro>. >. > For the question about multi-allelic heterozygous calls - yes, DeepVariant. > is able to all 1/2 events, and will represent these in one line as a GT 1/2. > call in the VCF. >. > For CLR calling in DeepVariant. It is theoretically possible for us to. > make a model for DeepVariant that can call CLR data. However, this requires. > us to write a special candidate generation logic to deal with the higher. > error rate. Based on what we perceive for the direction of future use in. > the genomics community, we think that data generated will be increasingly. > HiFi, so we have not been able to highly prioritize CLR models. Feedback. > from users like yourself will be useful to us in evaluating if that. > prioritization makes sense. For now, I can't commit to a timeframe under. > which we would support a PacBio CLR model. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/347#issuecomment-693053180>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ACHSLOV5RPVLTVGDW2A44X3SF73E7ANCNFSM4RNQJZYQ>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/347
https://github.com/google/deepvariant/issues/347:1251,usability,error,error,1251,"Dear Andrew,. Thank you for your quick reply. I agree with you that most sequencing and. resequencing projects will move towards HiFi reads rather than CLR reads. However, there is a lot of CLR sequencing data that has been generated in. the past couple of years and continues to be produced currently and could. still be useful for groups without the means to resequence using the novel. HiFi reads. So, I definitely see a niche in a large part of the. bioinformatics community that do a lot of data reusing (nowadays data. parasites). So, if there is anything we can do to help you n development,. please feel free to let me know how we can collaborate. Kind regards,. Juan D. Montenegro. El mar., 15 sept. 2020 a las 18:37, Andrew Carroll (<. notifications@github.com>) escribió:. > Hi @jdmontenegro <https://github.com/jdmontenegro>. >. > For the question about multi-allelic heterozygous calls - yes, DeepVariant. > is able to all 1/2 events, and will represent these in one line as a GT 1/2. > call in the VCF. >. > For CLR calling in DeepVariant. It is theoretically possible for us to. > make a model for DeepVariant that can call CLR data. However, this requires. > us to write a special candidate generation logic to deal with the higher. > error rate. Based on what we perceive for the direction of future use in. > the genomics community, we think that data generated will be increasingly. > HiFi, so we have not been able to highly prioritize CLR models. Feedback. > from users like yourself will be useful to us in evaluating if that. > prioritization makes sense. For now, I can't commit to a timeframe under. > which we would support a PacBio CLR model. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/347#issuecomment-693053180>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ACHSLOV5RPVLTVGDW2A44X3SF73E7ANCNFSM4RNQJZYQ>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/347
https://github.com/google/deepvariant/issues/347:1468,usability,Feedback,Feedback,1468,"Dear Andrew,. Thank you for your quick reply. I agree with you that most sequencing and. resequencing projects will move towards HiFi reads rather than CLR reads. However, there is a lot of CLR sequencing data that has been generated in. the past couple of years and continues to be produced currently and could. still be useful for groups without the means to resequence using the novel. HiFi reads. So, I definitely see a niche in a large part of the. bioinformatics community that do a lot of data reusing (nowadays data. parasites). So, if there is anything we can do to help you n development,. please feel free to let me know how we can collaborate. Kind regards,. Juan D. Montenegro. El mar., 15 sept. 2020 a las 18:37, Andrew Carroll (<. notifications@github.com>) escribió:. > Hi @jdmontenegro <https://github.com/jdmontenegro>. >. > For the question about multi-allelic heterozygous calls - yes, DeepVariant. > is able to all 1/2 events, and will represent these in one line as a GT 1/2. > call in the VCF. >. > For CLR calling in DeepVariant. It is theoretically possible for us to. > make a model for DeepVariant that can call CLR data. However, this requires. > us to write a special candidate generation logic to deal with the higher. > error rate. Based on what we perceive for the direction of future use in. > the genomics community, we think that data generated will be increasingly. > HiFi, so we have not been able to highly prioritize CLR models. Feedback. > from users like yourself will be useful to us in evaluating if that. > prioritization makes sense. For now, I can't commit to a timeframe under. > which we would support a PacBio CLR model. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/347#issuecomment-693053180>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ACHSLOV5RPVLTVGDW2A44X3SF73E7ANCNFSM4RNQJZYQ>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/347
https://github.com/google/deepvariant/issues/347:1485,usability,user,users,1485,"Dear Andrew,. Thank you for your quick reply. I agree with you that most sequencing and. resequencing projects will move towards HiFi reads rather than CLR reads. However, there is a lot of CLR sequencing data that has been generated in. the past couple of years and continues to be produced currently and could. still be useful for groups without the means to resequence using the novel. HiFi reads. So, I definitely see a niche in a large part of the. bioinformatics community that do a lot of data reusing (nowadays data. parasites). So, if there is anything we can do to help you n development,. please feel free to let me know how we can collaborate. Kind regards,. Juan D. Montenegro. El mar., 15 sept. 2020 a las 18:37, Andrew Carroll (<. notifications@github.com>) escribió:. > Hi @jdmontenegro <https://github.com/jdmontenegro>. >. > For the question about multi-allelic heterozygous calls - yes, DeepVariant. > is able to all 1/2 events, and will represent these in one line as a GT 1/2. > call in the VCF. >. > For CLR calling in DeepVariant. It is theoretically possible for us to. > make a model for DeepVariant that can call CLR data. However, this requires. > us to write a special candidate generation logic to deal with the higher. > error rate. Based on what we perceive for the direction of future use in. > the genomics community, we think that data generated will be increasingly. > HiFi, so we have not been able to highly prioritize CLR models. Feedback. > from users like yourself will be useful to us in evaluating if that. > prioritization makes sense. For now, I can't commit to a timeframe under. > which we would support a PacBio CLR model. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/347#issuecomment-693053180>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ACHSLOV5RPVLTVGDW2A44X3SF73E7ANCNFSM4RNQJZYQ>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/347
https://github.com/google/deepvariant/issues/347:1642,usability,support,support,1642,"Dear Andrew,. Thank you for your quick reply. I agree with you that most sequencing and. resequencing projects will move towards HiFi reads rather than CLR reads. However, there is a lot of CLR sequencing data that has been generated in. the past couple of years and continues to be produced currently and could. still be useful for groups without the means to resequence using the novel. HiFi reads. So, I definitely see a niche in a large part of the. bioinformatics community that do a lot of data reusing (nowadays data. parasites). So, if there is anything we can do to help you n development,. please feel free to let me know how we can collaborate. Kind regards,. Juan D. Montenegro. El mar., 15 sept. 2020 a las 18:37, Andrew Carroll (<. notifications@github.com>) escribió:. > Hi @jdmontenegro <https://github.com/jdmontenegro>. >. > For the question about multi-allelic heterozygous calls - yes, DeepVariant. > is able to all 1/2 events, and will represent these in one line as a GT 1/2. > call in the VCF. >. > For CLR calling in DeepVariant. It is theoretically possible for us to. > make a model for DeepVariant that can call CLR data. However, this requires. > us to write a special candidate generation logic to deal with the higher. > error rate. Based on what we perceive for the direction of future use in. > the genomics community, we think that data generated will be increasingly. > HiFi, so we have not been able to highly prioritize CLR models. Feedback. > from users like yourself will be useful to us in evaluating if that. > prioritization makes sense. For now, I can't commit to a timeframe under. > which we would support a PacBio CLR model. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/google/deepvariant/issues/347#issuecomment-693053180>,. > or unsubscribe. > <https://github.com/notifications/unsubscribe-auth/ACHSLOV5RPVLTVGDW2A44X3SF73E7ANCNFSM4RNQJZYQ>. > . >.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/347
https://github.com/google/deepvariant/issues/347:131,energy efficiency,model,model,131,Hi @jdmontenegro . I am going to close this issue for now. I will make a note to send you a message if/when we can revisit the CLR model. Thank you for your perspective on what data the community has and what will be valuable to them.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/347
https://github.com/google/deepvariant/issues/347:92,integrability,messag,message,92,Hi @jdmontenegro . I am going to close this issue for now. I will make a note to send you a message if/when we can revisit the CLR model. Thank you for your perspective on what data the community has and what will be valuable to them.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/347
https://github.com/google/deepvariant/issues/347:92,interoperability,messag,message,92,Hi @jdmontenegro . I am going to close this issue for now. I will make a note to send you a message if/when we can revisit the CLR model. Thank you for your perspective on what data the community has and what will be valuable to them.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/347
https://github.com/google/deepvariant/issues/347:131,security,model,model,131,Hi @jdmontenegro . I am going to close this issue for now. I will make a note to send you a message if/when we can revisit the CLR model. Thank you for your perspective on what data the community has and what will be valuable to them.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/347
https://github.com/google/deepvariant/issues/347:33,usability,close,close,33,Hi @jdmontenegro . I am going to close this issue for now. I will make a note to send you a message if/when we can revisit the CLR model. Thank you for your perspective on what data the community has and what will be valuable to them.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/347
https://github.com/google/deepvariant/issues/347:120,deployability,contain,contains,120,"Hi, @AndrewCarroll. Any new about CLR PacBio reads? . Recently we meet a same situation to process published data which contains many CLR PacBio reads. Thanks a low.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/347
https://github.com/google/deepvariant/issues/347:99,integrability,pub,published,99,"Hi, @AndrewCarroll. Any new about CLR PacBio reads? . Recently we meet a same situation to process published data which contains many CLR PacBio reads. Thanks a low.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/347
https://github.com/google/deepvariant/issues/347:38,modifiability,Pac,PacBio,38,"Hi, @AndrewCarroll. Any new about CLR PacBio reads? . Recently we meet a same situation to process published data which contains many CLR PacBio reads. Thanks a low.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/347
https://github.com/google/deepvariant/issues/347:138,modifiability,Pac,PacBio,138,"Hi, @AndrewCarroll. Any new about CLR PacBio reads? . Recently we meet a same situation to process published data which contains many CLR PacBio reads. Thanks a low.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/347
https://github.com/google/deepvariant/issues/351:193,availability,avail,available,193,"Hi @ahgillmo . We do have a prototype implementation for somatic calling, which can take a tumor and normal BAM and call subclonal variants. However, we don't yet have enough confidence in the available truth sets, and that they come from a diverse enough sampling of cancers with mutational profiles, for us be certain in releasing something of high quality. . We're watching developments in the area of these truth sets and hope to be able to further develop the somatic caller in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/351
https://github.com/google/deepvariant/issues/351:323,deployability,releas,releasing,323,"Hi @ahgillmo . We do have a prototype implementation for somatic calling, which can take a tumor and normal BAM and call subclonal variants. However, we don't yet have enough confidence in the available truth sets, and that they come from a diverse enough sampling of cancers with mutational profiles, for us be certain in releasing something of high quality. . We're watching developments in the area of these truth sets and hope to be able to further develop the somatic caller in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/351
https://github.com/google/deepvariant/issues/351:292,energy efficiency,profil,profiles,292,"Hi @ahgillmo . We do have a prototype implementation for somatic calling, which can take a tumor and normal BAM and call subclonal variants. However, we don't yet have enough confidence in the available truth sets, and that they come from a diverse enough sampling of cancers with mutational profiles, for us be certain in releasing something of high quality. . We're watching developments in the area of these truth sets and hope to be able to further develop the somatic caller in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/351
https://github.com/google/deepvariant/issues/351:121,integrability,sub,subclonal,121,"Hi @ahgillmo . We do have a prototype implementation for somatic calling, which can take a tumor and normal BAM and call subclonal variants. However, we don't yet have enough confidence in the available truth sets, and that they come from a diverse enough sampling of cancers with mutational profiles, for us be certain in releasing something of high quality. . We're watching developments in the area of these truth sets and hope to be able to further develop the somatic caller in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/351
https://github.com/google/deepvariant/issues/351:292,performance,profil,profiles,292,"Hi @ahgillmo . We do have a prototype implementation for somatic calling, which can take a tumor and normal BAM and call subclonal variants. However, we don't yet have enough confidence in the available truth sets, and that they come from a diverse enough sampling of cancers with mutational profiles, for us be certain in releasing something of high quality. . We're watching developments in the area of these truth sets and hope to be able to further develop the somatic caller in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/351
https://github.com/google/deepvariant/issues/351:193,reliability,availab,available,193,"Hi @ahgillmo . We do have a prototype implementation for somatic calling, which can take a tumor and normal BAM and call subclonal variants. However, we don't yet have enough confidence in the available truth sets, and that they come from a diverse enough sampling of cancers with mutational profiles, for us be certain in releasing something of high quality. . We're watching developments in the area of these truth sets and hope to be able to further develop the somatic caller in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/351
https://github.com/google/deepvariant/issues/351:193,safety,avail,available,193,"Hi @ahgillmo . We do have a prototype implementation for somatic calling, which can take a tumor and normal BAM and call subclonal variants. However, we don't yet have enough confidence in the available truth sets, and that they come from a diverse enough sampling of cancers with mutational profiles, for us be certain in releasing something of high quality. . We're watching developments in the area of these truth sets and hope to be able to further develop the somatic caller in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/351
https://github.com/google/deepvariant/issues/351:193,security,availab,available,193,"Hi @ahgillmo . We do have a prototype implementation for somatic calling, which can take a tumor and normal BAM and call subclonal variants. However, we don't yet have enough confidence in the available truth sets, and that they come from a diverse enough sampling of cancers with mutational profiles, for us be certain in releasing something of high quality. . We're watching developments in the area of these truth sets and hope to be able to further develop the somatic caller in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/351
https://github.com/google/deepvariant/issues/351:28,usability,prototyp,prototype,28,"Hi @ahgillmo . We do have a prototype implementation for somatic calling, which can take a tumor and normal BAM and call subclonal variants. However, we don't yet have enough confidence in the available truth sets, and that they come from a diverse enough sampling of cancers with mutational profiles, for us be certain in releasing something of high quality. . We're watching developments in the area of these truth sets and hope to be able to further develop the somatic caller in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/351
https://github.com/google/deepvariant/issues/351:200,availability,avail,available,200,"> Hi @ahgillmo. > . > We do have a prototype implementation for somatic calling, which can take a tumor and normal BAM and call subclonal variants. However, we don't yet have enough confidence in the available truth sets, and that they come from a diverse enough sampling of cancers with mutational profiles, for us be certain in releasing something of high quality. > . > We're watching developments in the area of these truth sets and hope to be able to further develop the somatic caller in the future. Where can I find the prototype and try it out?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/351
https://github.com/google/deepvariant/issues/351:330,deployability,releas,releasing,330,"> Hi @ahgillmo. > . > We do have a prototype implementation for somatic calling, which can take a tumor and normal BAM and call subclonal variants. However, we don't yet have enough confidence in the available truth sets, and that they come from a diverse enough sampling of cancers with mutational profiles, for us be certain in releasing something of high quality. > . > We're watching developments in the area of these truth sets and hope to be able to further develop the somatic caller in the future. Where can I find the prototype and try it out?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/351
https://github.com/google/deepvariant/issues/351:299,energy efficiency,profil,profiles,299,"> Hi @ahgillmo. > . > We do have a prototype implementation for somatic calling, which can take a tumor and normal BAM and call subclonal variants. However, we don't yet have enough confidence in the available truth sets, and that they come from a diverse enough sampling of cancers with mutational profiles, for us be certain in releasing something of high quality. > . > We're watching developments in the area of these truth sets and hope to be able to further develop the somatic caller in the future. Where can I find the prototype and try it out?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/351
https://github.com/google/deepvariant/issues/351:128,integrability,sub,subclonal,128,"> Hi @ahgillmo. > . > We do have a prototype implementation for somatic calling, which can take a tumor and normal BAM and call subclonal variants. However, we don't yet have enough confidence in the available truth sets, and that they come from a diverse enough sampling of cancers with mutational profiles, for us be certain in releasing something of high quality. > . > We're watching developments in the area of these truth sets and hope to be able to further develop the somatic caller in the future. Where can I find the prototype and try it out?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/351
https://github.com/google/deepvariant/issues/351:299,performance,profil,profiles,299,"> Hi @ahgillmo. > . > We do have a prototype implementation for somatic calling, which can take a tumor and normal BAM and call subclonal variants. However, we don't yet have enough confidence in the available truth sets, and that they come from a diverse enough sampling of cancers with mutational profiles, for us be certain in releasing something of high quality. > . > We're watching developments in the area of these truth sets and hope to be able to further develop the somatic caller in the future. Where can I find the prototype and try it out?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/351
https://github.com/google/deepvariant/issues/351:200,reliability,availab,available,200,"> Hi @ahgillmo. > . > We do have a prototype implementation for somatic calling, which can take a tumor and normal BAM and call subclonal variants. However, we don't yet have enough confidence in the available truth sets, and that they come from a diverse enough sampling of cancers with mutational profiles, for us be certain in releasing something of high quality. > . > We're watching developments in the area of these truth sets and hope to be able to further develop the somatic caller in the future. Where can I find the prototype and try it out?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/351
https://github.com/google/deepvariant/issues/351:200,safety,avail,available,200,"> Hi @ahgillmo. > . > We do have a prototype implementation for somatic calling, which can take a tumor and normal BAM and call subclonal variants. However, we don't yet have enough confidence in the available truth sets, and that they come from a diverse enough sampling of cancers with mutational profiles, for us be certain in releasing something of high quality. > . > We're watching developments in the area of these truth sets and hope to be able to further develop the somatic caller in the future. Where can I find the prototype and try it out?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/351
https://github.com/google/deepvariant/issues/351:200,security,availab,available,200,"> Hi @ahgillmo. > . > We do have a prototype implementation for somatic calling, which can take a tumor and normal BAM and call subclonal variants. However, we don't yet have enough confidence in the available truth sets, and that they come from a diverse enough sampling of cancers with mutational profiles, for us be certain in releasing something of high quality. > . > We're watching developments in the area of these truth sets and hope to be able to further develop the somatic caller in the future. Where can I find the prototype and try it out?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/351
https://github.com/google/deepvariant/issues/351:35,usability,prototyp,prototype,35,"> Hi @ahgillmo. > . > We do have a prototype implementation for somatic calling, which can take a tumor and normal BAM and call subclonal variants. However, we don't yet have enough confidence in the available truth sets, and that they come from a diverse enough sampling of cancers with mutational profiles, for us be certain in releasing something of high quality. > . > We're watching developments in the area of these truth sets and hope to be able to further develop the somatic caller in the future. Where can I find the prototype and try it out?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/351
https://github.com/google/deepvariant/issues/351:527,usability,prototyp,prototype,527,"> Hi @ahgillmo. > . > We do have a prototype implementation for somatic calling, which can take a tumor and normal BAM and call subclonal variants. However, we don't yet have enough confidence in the available truth sets, and that they come from a diverse enough sampling of cancers with mutational profiles, for us be certain in releasing something of high quality. > . > We're watching developments in the area of these truth sets and hope to be able to further develop the somatic caller in the future. Where can I find the prototype and try it out?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/351
https://github.com/google/deepvariant/issues/351:35,usability,prototyp,prototype,35,"Hi there, I also wanted to try the prototype and have reached out to Dr. Carroll :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/351
https://github.com/google/deepvariant/issues/351:439,deployability,version,version,439,">We're watching developments in the area of these truth sets and hope to be able to further develop the somatic caller in the future. Hi, I think I have a decent set of multi-tumor samples that would allow us to define ground truth sets for somatic clonal and somatic sub-clonal mutations, (and also pick up on potential noise from our inhouse variant caller). . @AndrewCarroll, @pichuan - I would be interested in testing out a prototype version of your somatic caller, or perhaps collaborating to see how it performs.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/351
https://github.com/google/deepvariant/issues/351:268,integrability,sub,sub-clonal,268,">We're watching developments in the area of these truth sets and hope to be able to further develop the somatic caller in the future. Hi, I think I have a decent set of multi-tumor samples that would allow us to define ground truth sets for somatic clonal and somatic sub-clonal mutations, (and also pick up on potential noise from our inhouse variant caller). . @AndrewCarroll, @pichuan - I would be interested in testing out a prototype version of your somatic caller, or perhaps collaborating to see how it performs.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/351
https://github.com/google/deepvariant/issues/351:439,integrability,version,version,439,">We're watching developments in the area of these truth sets and hope to be able to further develop the somatic caller in the future. Hi, I think I have a decent set of multi-tumor samples that would allow us to define ground truth sets for somatic clonal and somatic sub-clonal mutations, (and also pick up on potential noise from our inhouse variant caller). . @AndrewCarroll, @pichuan - I would be interested in testing out a prototype version of your somatic caller, or perhaps collaborating to see how it performs.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/351
https://github.com/google/deepvariant/issues/351:439,modifiability,version,version,439,">We're watching developments in the area of these truth sets and hope to be able to further develop the somatic caller in the future. Hi, I think I have a decent set of multi-tumor samples that would allow us to define ground truth sets for somatic clonal and somatic sub-clonal mutations, (and also pick up on potential noise from our inhouse variant caller). . @AndrewCarroll, @pichuan - I would be interested in testing out a prototype version of your somatic caller, or perhaps collaborating to see how it performs.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/351
https://github.com/google/deepvariant/issues/351:510,performance,perform,performs,510,">We're watching developments in the area of these truth sets and hope to be able to further develop the somatic caller in the future. Hi, I think I have a decent set of multi-tumor samples that would allow us to define ground truth sets for somatic clonal and somatic sub-clonal mutations, (and also pick up on potential noise from our inhouse variant caller). . @AndrewCarroll, @pichuan - I would be interested in testing out a prototype version of your somatic caller, or perhaps collaborating to see how it performs.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/351
https://github.com/google/deepvariant/issues/351:415,safety,test,testing,415,">We're watching developments in the area of these truth sets and hope to be able to further develop the somatic caller in the future. Hi, I think I have a decent set of multi-tumor samples that would allow us to define ground truth sets for somatic clonal and somatic sub-clonal mutations, (and also pick up on potential noise from our inhouse variant caller). . @AndrewCarroll, @pichuan - I would be interested in testing out a prototype version of your somatic caller, or perhaps collaborating to see how it performs.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/351
https://github.com/google/deepvariant/issues/351:415,testability,test,testing,415,">We're watching developments in the area of these truth sets and hope to be able to further develop the somatic caller in the future. Hi, I think I have a decent set of multi-tumor samples that would allow us to define ground truth sets for somatic clonal and somatic sub-clonal mutations, (and also pick up on potential noise from our inhouse variant caller). . @AndrewCarroll, @pichuan - I would be interested in testing out a prototype version of your somatic caller, or perhaps collaborating to see how it performs.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/351
https://github.com/google/deepvariant/issues/351:429,usability,prototyp,prototype,429,">We're watching developments in the area of these truth sets and hope to be able to further develop the somatic caller in the future. Hi, I think I have a decent set of multi-tumor samples that would allow us to define ground truth sets for somatic clonal and somatic sub-clonal mutations, (and also pick up on potential noise from our inhouse variant caller). . @AndrewCarroll, @pichuan - I would be interested in testing out a prototype version of your somatic caller, or perhaps collaborating to see how it performs.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/351
https://github.com/google/deepvariant/issues/351:510,usability,perform,performs,510,">We're watching developments in the area of these truth sets and hope to be able to further develop the somatic caller in the future. Hi, I think I have a decent set of multi-tumor samples that would allow us to define ground truth sets for somatic clonal and somatic sub-clonal mutations, (and also pick up on potential noise from our inhouse variant caller). . @AndrewCarroll, @pichuan - I would be interested in testing out a prototype version of your somatic caller, or perhaps collaborating to see how it performs.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/351
https://github.com/google/deepvariant/issues/351:149,deployability,resourc,resource,149,"Hi @chilampoon and @forericksonetalsubmission1 . Thank you to both for your interest, and @forericksonetalsubmission1 this does seem like a valuable resource. Unfortunately, I'll have to defer an offer to help with somatic calling for at least awhile. We currently have too many active demands for near-term development work, and I have to preserve more bandwidth to make sure we can deliver well on those. I am sorry and hope we can revisit this question sometime in the future. Thanks and apologies,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/351
https://github.com/google/deepvariant/issues/351:149,energy efficiency,resourc,resource,149,"Hi @chilampoon and @forericksonetalsubmission1 . Thank you to both for your interest, and @forericksonetalsubmission1 this does seem like a valuable resource. Unfortunately, I'll have to defer an offer to help with somatic calling for at least awhile. We currently have too many active demands for near-term development work, and I have to preserve more bandwidth to make sure we can deliver well on those. I am sorry and hope we can revisit this question sometime in the future. Thanks and apologies,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/351
https://github.com/google/deepvariant/issues/351:255,energy efficiency,current,currently,255,"Hi @chilampoon and @forericksonetalsubmission1 . Thank you to both for your interest, and @forericksonetalsubmission1 this does seem like a valuable resource. Unfortunately, I'll have to defer an offer to help with somatic calling for at least awhile. We currently have too many active demands for near-term development work, and I have to preserve more bandwidth to make sure we can deliver well on those. I am sorry and hope we can revisit this question sometime in the future. Thanks and apologies,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/351
https://github.com/google/deepvariant/issues/351:149,performance,resourc,resource,149,"Hi @chilampoon and @forericksonetalsubmission1 . Thank you to both for your interest, and @forericksonetalsubmission1 this does seem like a valuable resource. Unfortunately, I'll have to defer an offer to help with somatic calling for at least awhile. We currently have too many active demands for near-term development work, and I have to preserve more bandwidth to make sure we can deliver well on those. I am sorry and hope we can revisit this question sometime in the future. Thanks and apologies,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/351
https://github.com/google/deepvariant/issues/351:123,reliability,doe,does,123,"Hi @chilampoon and @forericksonetalsubmission1 . Thank you to both for your interest, and @forericksonetalsubmission1 this does seem like a valuable resource. Unfortunately, I'll have to defer an offer to help with somatic calling for at least awhile. We currently have too many active demands for near-term development work, and I have to preserve more bandwidth to make sure we can deliver well on those. I am sorry and hope we can revisit this question sometime in the future. Thanks and apologies,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/351
https://github.com/google/deepvariant/issues/351:149,safety,resourc,resource,149,"Hi @chilampoon and @forericksonetalsubmission1 . Thank you to both for your interest, and @forericksonetalsubmission1 this does seem like a valuable resource. Unfortunately, I'll have to defer an offer to help with somatic calling for at least awhile. We currently have too many active demands for near-term development work, and I have to preserve more bandwidth to make sure we can deliver well on those. I am sorry and hope we can revisit this question sometime in the future. Thanks and apologies,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/351
https://github.com/google/deepvariant/issues/351:149,testability,resourc,resource,149,"Hi @chilampoon and @forericksonetalsubmission1 . Thank you to both for your interest, and @forericksonetalsubmission1 this does seem like a valuable resource. Unfortunately, I'll have to defer an offer to help with somatic calling for at least awhile. We currently have too many active demands for near-term development work, and I have to preserve more bandwidth to make sure we can deliver well on those. I am sorry and hope we can revisit this question sometime in the future. Thanks and apologies,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/351
https://github.com/google/deepvariant/issues/351:205,usability,help,help,205,"Hi @chilampoon and @forericksonetalsubmission1 . Thank you to both for your interest, and @forericksonetalsubmission1 this does seem like a valuable resource. Unfortunately, I'll have to defer an offer to help with somatic calling for at least awhile. We currently have too many active demands for near-term development work, and I have to preserve more bandwidth to make sure we can deliver well on those. I am sorry and hope we can revisit this question sometime in the future. Thanks and apologies,. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/351
https://github.com/google/deepvariant/issues/352:191,availability,avail,available,191,"Hi @ndesar . We do have a prototype implementation for somatic calling, which can take a tumor and normal BAM and call subclonal variants. However, we don't yet have enough confidence in the available truth sets, and that they come from a diverse enough sampling of cancers with mutational profiles, for us be certain in releasing something of high quality. From the title of your issue, is it correct that you have PacBio HiFi data for a tumor line? This is interesting, we're keeping an eye on adoption of HiFi data for cancer sequencing, since it has potential to improve some of the truth sets. For the present, we don't have a widely released method you can use for somatic calling, and this isn't on the immediate roadmap. However, hopefully in the medium-to-long term this is an area we'll be able to get to.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/352
https://github.com/google/deepvariant/issues/352:321,deployability,releas,releasing,321,"Hi @ndesar . We do have a prototype implementation for somatic calling, which can take a tumor and normal BAM and call subclonal variants. However, we don't yet have enough confidence in the available truth sets, and that they come from a diverse enough sampling of cancers with mutational profiles, for us be certain in releasing something of high quality. From the title of your issue, is it correct that you have PacBio HiFi data for a tumor line? This is interesting, we're keeping an eye on adoption of HiFi data for cancer sequencing, since it has potential to improve some of the truth sets. For the present, we don't have a widely released method you can use for somatic calling, and this isn't on the immediate roadmap. However, hopefully in the medium-to-long term this is an area we'll be able to get to.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/352
https://github.com/google/deepvariant/issues/352:639,deployability,releas,released,639,"Hi @ndesar . We do have a prototype implementation for somatic calling, which can take a tumor and normal BAM and call subclonal variants. However, we don't yet have enough confidence in the available truth sets, and that they come from a diverse enough sampling of cancers with mutational profiles, for us be certain in releasing something of high quality. From the title of your issue, is it correct that you have PacBio HiFi data for a tumor line? This is interesting, we're keeping an eye on adoption of HiFi data for cancer sequencing, since it has potential to improve some of the truth sets. For the present, we don't have a widely released method you can use for somatic calling, and this isn't on the immediate roadmap. However, hopefully in the medium-to-long term this is an area we'll be able to get to.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/352
https://github.com/google/deepvariant/issues/352:290,energy efficiency,profil,profiles,290,"Hi @ndesar . We do have a prototype implementation for somatic calling, which can take a tumor and normal BAM and call subclonal variants. However, we don't yet have enough confidence in the available truth sets, and that they come from a diverse enough sampling of cancers with mutational profiles, for us be certain in releasing something of high quality. From the title of your issue, is it correct that you have PacBio HiFi data for a tumor line? This is interesting, we're keeping an eye on adoption of HiFi data for cancer sequencing, since it has potential to improve some of the truth sets. For the present, we don't have a widely released method you can use for somatic calling, and this isn't on the immediate roadmap. However, hopefully in the medium-to-long term this is an area we'll be able to get to.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/352
https://github.com/google/deepvariant/issues/352:119,integrability,sub,subclonal,119,"Hi @ndesar . We do have a prototype implementation for somatic calling, which can take a tumor and normal BAM and call subclonal variants. However, we don't yet have enough confidence in the available truth sets, and that they come from a diverse enough sampling of cancers with mutational profiles, for us be certain in releasing something of high quality. From the title of your issue, is it correct that you have PacBio HiFi data for a tumor line? This is interesting, we're keeping an eye on adoption of HiFi data for cancer sequencing, since it has potential to improve some of the truth sets. For the present, we don't have a widely released method you can use for somatic calling, and this isn't on the immediate roadmap. However, hopefully in the medium-to-long term this is an area we'll be able to get to.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/352
https://github.com/google/deepvariant/issues/352:416,modifiability,Pac,PacBio,416,"Hi @ndesar . We do have a prototype implementation for somatic calling, which can take a tumor and normal BAM and call subclonal variants. However, we don't yet have enough confidence in the available truth sets, and that they come from a diverse enough sampling of cancers with mutational profiles, for us be certain in releasing something of high quality. From the title of your issue, is it correct that you have PacBio HiFi data for a tumor line? This is interesting, we're keeping an eye on adoption of HiFi data for cancer sequencing, since it has potential to improve some of the truth sets. For the present, we don't have a widely released method you can use for somatic calling, and this isn't on the immediate roadmap. However, hopefully in the medium-to-long term this is an area we'll be able to get to.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/352
https://github.com/google/deepvariant/issues/352:290,performance,profil,profiles,290,"Hi @ndesar . We do have a prototype implementation for somatic calling, which can take a tumor and normal BAM and call subclonal variants. However, we don't yet have enough confidence in the available truth sets, and that they come from a diverse enough sampling of cancers with mutational profiles, for us be certain in releasing something of high quality. From the title of your issue, is it correct that you have PacBio HiFi data for a tumor line? This is interesting, we're keeping an eye on adoption of HiFi data for cancer sequencing, since it has potential to improve some of the truth sets. For the present, we don't have a widely released method you can use for somatic calling, and this isn't on the immediate roadmap. However, hopefully in the medium-to-long term this is an area we'll be able to get to.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/352
https://github.com/google/deepvariant/issues/352:191,reliability,availab,available,191,"Hi @ndesar . We do have a prototype implementation for somatic calling, which can take a tumor and normal BAM and call subclonal variants. However, we don't yet have enough confidence in the available truth sets, and that they come from a diverse enough sampling of cancers with mutational profiles, for us be certain in releasing something of high quality. From the title of your issue, is it correct that you have PacBio HiFi data for a tumor line? This is interesting, we're keeping an eye on adoption of HiFi data for cancer sequencing, since it has potential to improve some of the truth sets. For the present, we don't have a widely released method you can use for somatic calling, and this isn't on the immediate roadmap. However, hopefully in the medium-to-long term this is an area we'll be able to get to.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/352
https://github.com/google/deepvariant/issues/352:191,safety,avail,available,191,"Hi @ndesar . We do have a prototype implementation for somatic calling, which can take a tumor and normal BAM and call subclonal variants. However, we don't yet have enough confidence in the available truth sets, and that they come from a diverse enough sampling of cancers with mutational profiles, for us be certain in releasing something of high quality. From the title of your issue, is it correct that you have PacBio HiFi data for a tumor line? This is interesting, we're keeping an eye on adoption of HiFi data for cancer sequencing, since it has potential to improve some of the truth sets. For the present, we don't have a widely released method you can use for somatic calling, and this isn't on the immediate roadmap. However, hopefully in the medium-to-long term this is an area we'll be able to get to.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/352
https://github.com/google/deepvariant/issues/352:191,security,availab,available,191,"Hi @ndesar . We do have a prototype implementation for somatic calling, which can take a tumor and normal BAM and call subclonal variants. However, we don't yet have enough confidence in the available truth sets, and that they come from a diverse enough sampling of cancers with mutational profiles, for us be certain in releasing something of high quality. From the title of your issue, is it correct that you have PacBio HiFi data for a tumor line? This is interesting, we're keeping an eye on adoption of HiFi data for cancer sequencing, since it has potential to improve some of the truth sets. For the present, we don't have a widely released method you can use for somatic calling, and this isn't on the immediate roadmap. However, hopefully in the medium-to-long term this is an area we'll be able to get to.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/352
https://github.com/google/deepvariant/issues/352:26,usability,prototyp,prototype,26,"Hi @ndesar . We do have a prototype implementation for somatic calling, which can take a tumor and normal BAM and call subclonal variants. However, we don't yet have enough confidence in the available truth sets, and that they come from a diverse enough sampling of cancers with mutational profiles, for us be certain in releasing something of high quality. From the title of your issue, is it correct that you have PacBio HiFi data for a tumor line? This is interesting, we're keeping an eye on adoption of HiFi data for cancer sequencing, since it has potential to improve some of the truth sets. For the present, we don't have a widely released method you can use for somatic calling, and this isn't on the immediate roadmap. However, hopefully in the medium-to-long term this is an area we'll be able to get to.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/352
https://github.com/google/deepvariant/issues/352:99,performance,content,content,99,"@AndrewCarroll . Have you tried DeepSSV, a somatic small variants caller ? https://www.biorxiv.org/content/10.1101/555680v1.full.pdf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/352
https://github.com/google/deepvariant/issues/353:389,deployability,contain,containers,389,"Hi @DiDeoxy, here are [our instructions](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md#notes-on-singularity) for running DeepVariant 1.0.0 with Singularity. Specifically, the below commands should get you the 1.0.0 image. . ```. BIN_VERSION=""1.0.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". ```. Where are you seeing the singularity containers at 0.9.0? If you could share a link or paste the commands used, I'll look into that. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/353
https://github.com/google/deepvariant/issues/353:191,interoperability,Specif,Specifically,191,"Hi @DiDeoxy, here are [our instructions](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md#notes-on-singularity) for running DeepVariant 1.0.0 with Singularity. Specifically, the below commands should get you the 1.0.0 image. . ```. BIN_VERSION=""1.0.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". ```. Where are you seeing the singularity containers at 0.9.0? If you could share a link or paste the commands used, I'll look into that. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/353
https://github.com/google/deepvariant/issues/353:423,interoperability,share,share,423,"Hi @DiDeoxy, here are [our instructions](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md#notes-on-singularity) for running DeepVariant 1.0.0 with Singularity. Specifically, the below commands should get you the 1.0.0 image. . ```. BIN_VERSION=""1.0.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". ```. Where are you seeing the singularity containers at 0.9.0? If you could share a link or paste the commands used, I'll look into that. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/353
https://github.com/google/deepvariant/issues/353:215,usability,command,commands,215,"Hi @DiDeoxy, here are [our instructions](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md#notes-on-singularity) for running DeepVariant 1.0.0 with Singularity. Specifically, the below commands should get you the 1.0.0 image. . ```. BIN_VERSION=""1.0.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". ```. Where are you seeing the singularity containers at 0.9.0? If you could share a link or paste the commands used, I'll look into that. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/353
https://github.com/google/deepvariant/issues/353:449,usability,command,commands,449,"Hi @DiDeoxy, here are [our instructions](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md#notes-on-singularity) for running DeepVariant 1.0.0 with Singularity. Specifically, the below commands should get you the 1.0.0 image. . ```. BIN_VERSION=""1.0.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". ```. Where are you seeing the singularity containers at 0.9.0? If you could share a link or paste the commands used, I'll look into that. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/353
https://github.com/google/deepvariant/issues/353:121,energy efficiency,cloud,cloud,121,"Hey,. Thanks, I did that yesterday and got it working. . I was finding the 0.9.0 singularity imagers at: https://console.cloud.google.com/storage/browser/deepvariant/singularity_images?pageState=(%22StorageObjectListTable%22:(%22f%22:%22%255B%255D%22))&prefix=&forceOnObjectsSortingFiltering=false. Cheers,. Max H.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/353
https://github.com/google/deepvariant/issues/353:91,testability,plan,plan,91,"@DiDeoxy glad to hear you were able to get it working, and thanks for the pointer! We will plan remove the `gs://deepvariant/singularity_images` folder to make things less confusing. The recommended approach is the set of commands mentioned earlier:. ```. BIN_VERSION=""1.0.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". ```. I'll close this issue now, but feel free to reopen if you have any other questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/353
https://github.com/google/deepvariant/issues/353:222,usability,command,commands,222,"@DiDeoxy glad to hear you were able to get it working, and thanks for the pointer! We will plan remove the `gs://deepvariant/singularity_images` folder to make things less confusing. The recommended approach is the set of commands mentioned earlier:. ```. BIN_VERSION=""1.0.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". ```. I'll close this issue now, but feel free to reopen if you have any other questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/353
https://github.com/google/deepvariant/issues/353:350,usability,close,close,350,"@DiDeoxy glad to hear you were able to get it working, and thanks for the pointer! We will plan remove the `gs://deepvariant/singularity_images` folder to make things less confusing. The recommended approach is the set of commands mentioned earlier:. ```. BIN_VERSION=""1.0.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". ```. I'll close this issue now, but feel free to reopen if you have any other questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/353
https://github.com/google/deepvariant/issues/353:92,safety,avoid,avoid,92,Thanks for reporting this. . I removed the gs://deepvariant/singularity_images directory to avoid future confusion.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/353
https://github.com/google/deepvariant/issues/354:9,availability,error,error,9,The same error happens with `google/deepvariant:1.0.0`,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/354
https://github.com/google/deepvariant/issues/354:9,performance,error,error,9,The same error happens with `google/deepvariant:1.0.0`,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/354
https://github.com/google/deepvariant/issues/354:9,safety,error,error,9,The same error happens with `google/deepvariant:1.0.0`,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/354
https://github.com/google/deepvariant/issues/354:9,usability,error,error,9,The same error happens with `google/deepvariant:1.0.0`,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/354
https://github.com/google/deepvariant/issues/354:92,deployability,releas,release,92,"Hi @Redmar-van-den-Berg, thanks for reporting this! We will look into the issue and plan to release a fix in the next version of DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/354
https://github.com/google/deepvariant/issues/354:118,deployability,version,version,118,"Hi @Redmar-van-den-Berg, thanks for reporting this! We will look into the issue and plan to release a fix in the next version of DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/354
https://github.com/google/deepvariant/issues/354:118,integrability,version,version,118,"Hi @Redmar-van-den-Berg, thanks for reporting this! We will look into the issue and plan to release a fix in the next version of DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/354
https://github.com/google/deepvariant/issues/354:118,modifiability,version,version,118,"Hi @Redmar-van-den-Berg, thanks for reporting this! We will look into the issue and plan to release a fix in the next version of DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/354
https://github.com/google/deepvariant/issues/354:84,testability,plan,plan,84,"Hi @Redmar-van-den-Berg, thanks for reporting this! We will look into the issue and plan to release a fix in the next version of DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/354
https://github.com/google/deepvariant/issues/354:97,deployability,releas,release,97,"@Redmar-van-den-Berg this issue has been fixed internally, and the fix will be out with the next release. When gVCF records are present, we will try to extract the sample name from those, instead of using 'default'. I'll close this issue for now, but feel free to reopen if you have any other questions. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/354
https://github.com/google/deepvariant/issues/354:221,usability,close,close,221,"@Redmar-van-den-Berg this issue has been fixed internally, and the fix will be out with the next release. When gVCF records are present, we will try to extract the sample name from those, instead of using 'default'. I'll close this issue for now, but feel free to reopen if you have any other questions. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/354
https://github.com/google/deepvariant/issues/355:64,availability,Operat,Operating,64,"Hi @anands-repo, can you share more details on the following? - Operating system. - DeepVariant version - I'm assuming you are building 1.0.0 from source, but please let me know if it's a different version. - Command used to build. - Error trace after passing `--host_javabase=@local_jdk//:jdk`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/355:234,availability,Error,Error,234,"Hi @anands-repo, can you share more details on the following? - Operating system. - DeepVariant version - I'm assuming you are building 1.0.0 from source, but please let me know if it's a different version. - Command used to build. - Error trace after passing `--host_javabase=@local_jdk//:jdk`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/355:96,deployability,version,version,96,"Hi @anands-repo, can you share more details on the following? - Operating system. - DeepVariant version - I'm assuming you are building 1.0.0 from source, but please let me know if it's a different version. - Command used to build. - Error trace after passing `--host_javabase=@local_jdk//:jdk`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/355:127,deployability,build,building,127,"Hi @anands-repo, can you share more details on the following? - Operating system. - DeepVariant version - I'm assuming you are building 1.0.0 from source, but please let me know if it's a different version. - Command used to build. - Error trace after passing `--host_javabase=@local_jdk//:jdk`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/355:198,deployability,version,version,198,"Hi @anands-repo, can you share more details on the following? - Operating system. - DeepVariant version - I'm assuming you are building 1.0.0 from source, but please let me know if it's a different version. - Command used to build. - Error trace after passing `--host_javabase=@local_jdk//:jdk`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/355:225,deployability,build,build,225,"Hi @anands-repo, can you share more details on the following? - Operating system. - DeepVariant version - I'm assuming you are building 1.0.0 from source, but please let me know if it's a different version. - Command used to build. - Error trace after passing `--host_javabase=@local_jdk//:jdk`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/355:96,integrability,version,version,96,"Hi @anands-repo, can you share more details on the following? - Operating system. - DeepVariant version - I'm assuming you are building 1.0.0 from source, but please let me know if it's a different version. - Command used to build. - Error trace after passing `--host_javabase=@local_jdk//:jdk`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/355:198,integrability,version,version,198,"Hi @anands-repo, can you share more details on the following? - Operating system. - DeepVariant version - I'm assuming you are building 1.0.0 from source, but please let me know if it's a different version. - Command used to build. - Error trace after passing `--host_javabase=@local_jdk//:jdk`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/355:25,interoperability,share,share,25,"Hi @anands-repo, can you share more details on the following? - Operating system. - DeepVariant version - I'm assuming you are building 1.0.0 from source, but please let me know if it's a different version. - Command used to build. - Error trace after passing `--host_javabase=@local_jdk//:jdk`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/355:96,modifiability,version,version,96,"Hi @anands-repo, can you share more details on the following? - Operating system. - DeepVariant version - I'm assuming you are building 1.0.0 from source, but please let me know if it's a different version. - Command used to build. - Error trace after passing `--host_javabase=@local_jdk//:jdk`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/355:198,modifiability,version,version,198,"Hi @anands-repo, can you share more details on the following? - Operating system. - DeepVariant version - I'm assuming you are building 1.0.0 from source, but please let me know if it's a different version. - Command used to build. - Error trace after passing `--host_javabase=@local_jdk//:jdk`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/355:234,performance,Error,Error,234,"Hi @anands-repo, can you share more details on the following? - Operating system. - DeepVariant version - I'm assuming you are building 1.0.0 from source, but please let me know if it's a different version. - Command used to build. - Error trace after passing `--host_javabase=@local_jdk//:jdk`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/355:234,safety,Error,Error,234,"Hi @anands-repo, can you share more details on the following? - Operating system. - DeepVariant version - I'm assuming you are building 1.0.0 from source, but please let me know if it's a different version. - Command used to build. - Error trace after passing `--host_javabase=@local_jdk//:jdk`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/355:240,testability,trace,trace,240,"Hi @anands-repo, can you share more details on the following? - Operating system. - DeepVariant version - I'm assuming you are building 1.0.0 from source, but please let me know if it's a different version. - Command used to build. - Error trace after passing `--host_javabase=@local_jdk//:jdk`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/355:209,usability,Command,Command,209,"Hi @anands-repo, can you share more details on the following? - Operating system. - DeepVariant version - I'm assuming you are building 1.0.0 from source, but please let me know if it's a different version. - Command used to build. - Error trace after passing `--host_javabase=@local_jdk//:jdk`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/355:234,usability,Error,Error,234,"Hi @anands-repo, can you share more details on the following? - Operating system. - DeepVariant version - I'm assuming you are building 1.0.0 from source, but please let me know if it's a different version. - Command used to build. - Error trace after passing `--host_javabase=@local_jdk//:jdk`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/355:251,availability,ERROR,ERROR,251,"Hi @gunjanbaid I do not see the same issue now. I switched to openjdk-11. However, I still need to pass the --host_javabase option, which is not present by default in the build scripts. I do see another issue. The trace is as follows. ```. (02:26:24) ERROR: /root/deepvariant/third_party/nucleus/protos/BUILD:413:1: ClifProtoLibraryGeneration third_party/nucleus/protos/bedgraph_pyclif.h failed (Exit 1): proto failed: error executing command. (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \. exec env - \. bazel-out/host/bin/external/clif/proto -c bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.cc -h bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.h '--strip_dir=bazel-out/ppc-opt/bin' '--source_dir='\''.'\''' third_party/nucleus/protos/bedgraph.proto). Execution platform: @bazel_tools//platforms:host_platform . Traceback (most recent call last): . File ""bazel-out/host/bin/external/clif/proto"", line 5, in <module>. from clif.python.proto import start . File ""/root/opt/clif/lib64/python3.6/site-packages/clif/python/proto.py"", line 29, in <module>. from clif.python.utils import proto_util . ImportError: libprotobuf.so.24: cannot open shared object file: No such file or directory. ```. This is for the following command from build_and_test.sh:. ```. bazel test --host_javabase=@local_jdk//:jdk -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" \. deepvariant/... ```. Why is bazel running ```exec env -``` here? This is invalidating library paths (LD_LIBRARY_PATH variable) where it would find ```libprotobuf.so```. As expected, when I run the failing command from the trace without ```exec env -```, it is fine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/355:419,availability,error,error,419,"Hi @gunjanbaid I do not see the same issue now. I switched to openjdk-11. However, I still need to pass the --host_javabase option, which is not present by default in the build scripts. I do see another issue. The trace is as follows. ```. (02:26:24) ERROR: /root/deepvariant/third_party/nucleus/protos/BUILD:413:1: ClifProtoLibraryGeneration third_party/nucleus/protos/bedgraph_pyclif.h failed (Exit 1): proto failed: error executing command. (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \. exec env - \. bazel-out/host/bin/external/clif/proto -c bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.cc -h bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.h '--strip_dir=bazel-out/ppc-opt/bin' '--source_dir='\''.'\''' third_party/nucleus/protos/bedgraph.proto). Execution platform: @bazel_tools//platforms:host_platform . Traceback (most recent call last): . File ""bazel-out/host/bin/external/clif/proto"", line 5, in <module>. from clif.python.proto import start . File ""/root/opt/clif/lib64/python3.6/site-packages/clif/python/proto.py"", line 29, in <module>. from clif.python.utils import proto_util . ImportError: libprotobuf.so.24: cannot open shared object file: No such file or directory. ```. This is for the following command from build_and_test.sh:. ```. bazel test --host_javabase=@local_jdk//:jdk -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" \. deepvariant/... ```. Why is bazel running ```exec env -``` here? This is invalidating library paths (LD_LIBRARY_PATH variable) where it would find ```libprotobuf.so```. As expected, when I run the failing command from the trace without ```exec env -```, it is fine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/355:171,deployability,build,build,171,"Hi @gunjanbaid I do not see the same issue now. I switched to openjdk-11. However, I still need to pass the --host_javabase option, which is not present by default in the build scripts. I do see another issue. The trace is as follows. ```. (02:26:24) ERROR: /root/deepvariant/third_party/nucleus/protos/BUILD:413:1: ClifProtoLibraryGeneration third_party/nucleus/protos/bedgraph_pyclif.h failed (Exit 1): proto failed: error executing command. (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \. exec env - \. bazel-out/host/bin/external/clif/proto -c bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.cc -h bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.h '--strip_dir=bazel-out/ppc-opt/bin' '--source_dir='\''.'\''' third_party/nucleus/protos/bedgraph.proto). Execution platform: @bazel_tools//platforms:host_platform . Traceback (most recent call last): . File ""bazel-out/host/bin/external/clif/proto"", line 5, in <module>. from clif.python.proto import start . File ""/root/opt/clif/lib64/python3.6/site-packages/clif/python/proto.py"", line 29, in <module>. from clif.python.utils import proto_util . ImportError: libprotobuf.so.24: cannot open shared object file: No such file or directory. ```. This is for the following command from build_and_test.sh:. ```. bazel test --host_javabase=@local_jdk//:jdk -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" \. deepvariant/... ```. Why is bazel running ```exec env -``` here? This is invalidating library paths (LD_LIBRARY_PATH variable) where it would find ```libprotobuf.so```. As expected, when I run the failing command from the trace without ```exec env -```, it is fine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/355:303,deployability,BUILD,BUILD,303,"Hi @gunjanbaid I do not see the same issue now. I switched to openjdk-11. However, I still need to pass the --host_javabase option, which is not present by default in the build scripts. I do see another issue. The trace is as follows. ```. (02:26:24) ERROR: /root/deepvariant/third_party/nucleus/protos/BUILD:413:1: ClifProtoLibraryGeneration third_party/nucleus/protos/bedgraph_pyclif.h failed (Exit 1): proto failed: error executing command. (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \. exec env - \. bazel-out/host/bin/external/clif/proto -c bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.cc -h bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.h '--strip_dir=bazel-out/ppc-opt/bin' '--source_dir='\''.'\''' third_party/nucleus/protos/bedgraph.proto). Execution platform: @bazel_tools//platforms:host_platform . Traceback (most recent call last): . File ""bazel-out/host/bin/external/clif/proto"", line 5, in <module>. from clif.python.proto import start . File ""/root/opt/clif/lib64/python3.6/site-packages/clif/python/proto.py"", line 29, in <module>. from clif.python.utils import proto_util . ImportError: libprotobuf.so.24: cannot open shared object file: No such file or directory. ```. This is for the following command from build_and_test.sh:. ```. bazel test --host_javabase=@local_jdk//:jdk -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" \. deepvariant/... ```. Why is bazel running ```exec env -``` here? This is invalidating library paths (LD_LIBRARY_PATH variable) where it would find ```libprotobuf.so```. As expected, when I run the failing command from the trace without ```exec env -```, it is fine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/355:388,deployability,fail,failed,388,"Hi @gunjanbaid I do not see the same issue now. I switched to openjdk-11. However, I still need to pass the --host_javabase option, which is not present by default in the build scripts. I do see another issue. The trace is as follows. ```. (02:26:24) ERROR: /root/deepvariant/third_party/nucleus/protos/BUILD:413:1: ClifProtoLibraryGeneration third_party/nucleus/protos/bedgraph_pyclif.h failed (Exit 1): proto failed: error executing command. (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \. exec env - \. bazel-out/host/bin/external/clif/proto -c bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.cc -h bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.h '--strip_dir=bazel-out/ppc-opt/bin' '--source_dir='\''.'\''' third_party/nucleus/protos/bedgraph.proto). Execution platform: @bazel_tools//platforms:host_platform . Traceback (most recent call last): . File ""bazel-out/host/bin/external/clif/proto"", line 5, in <module>. from clif.python.proto import start . File ""/root/opt/clif/lib64/python3.6/site-packages/clif/python/proto.py"", line 29, in <module>. from clif.python.utils import proto_util . ImportError: libprotobuf.so.24: cannot open shared object file: No such file or directory. ```. This is for the following command from build_and_test.sh:. ```. bazel test --host_javabase=@local_jdk//:jdk -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" \. deepvariant/... ```. Why is bazel running ```exec env -``` here? This is invalidating library paths (LD_LIBRARY_PATH variable) where it would find ```libprotobuf.so```. As expected, when I run the failing command from the trace without ```exec env -```, it is fine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/355:411,deployability,fail,failed,411,"Hi @gunjanbaid I do not see the same issue now. I switched to openjdk-11. However, I still need to pass the --host_javabase option, which is not present by default in the build scripts. I do see another issue. The trace is as follows. ```. (02:26:24) ERROR: /root/deepvariant/third_party/nucleus/protos/BUILD:413:1: ClifProtoLibraryGeneration third_party/nucleus/protos/bedgraph_pyclif.h failed (Exit 1): proto failed: error executing command. (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \. exec env - \. bazel-out/host/bin/external/clif/proto -c bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.cc -h bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.h '--strip_dir=bazel-out/ppc-opt/bin' '--source_dir='\''.'\''' third_party/nucleus/protos/bedgraph.proto). Execution platform: @bazel_tools//platforms:host_platform . Traceback (most recent call last): . File ""bazel-out/host/bin/external/clif/proto"", line 5, in <module>. from clif.python.proto import start . File ""/root/opt/clif/lib64/python3.6/site-packages/clif/python/proto.py"", line 29, in <module>. from clif.python.utils import proto_util . ImportError: libprotobuf.so.24: cannot open shared object file: No such file or directory. ```. This is for the following command from build_and_test.sh:. ```. bazel test --host_javabase=@local_jdk//:jdk -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" \. deepvariant/... ```. Why is bazel running ```exec env -``` here? This is invalidating library paths (LD_LIBRARY_PATH variable) where it would find ```libprotobuf.so```. As expected, when I run the failing command from the trace without ```exec env -```, it is fine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/355:1005,deployability,modul,module,1005,"Hi @gunjanbaid I do not see the same issue now. I switched to openjdk-11. However, I still need to pass the --host_javabase option, which is not present by default in the build scripts. I do see another issue. The trace is as follows. ```. (02:26:24) ERROR: /root/deepvariant/third_party/nucleus/protos/BUILD:413:1: ClifProtoLibraryGeneration third_party/nucleus/protos/bedgraph_pyclif.h failed (Exit 1): proto failed: error executing command. (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \. exec env - \. bazel-out/host/bin/external/clif/proto -c bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.cc -h bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.h '--strip_dir=bazel-out/ppc-opt/bin' '--source_dir='\''.'\''' third_party/nucleus/protos/bedgraph.proto). Execution platform: @bazel_tools//platforms:host_platform . Traceback (most recent call last): . File ""bazel-out/host/bin/external/clif/proto"", line 5, in <module>. from clif.python.proto import start . File ""/root/opt/clif/lib64/python3.6/site-packages/clif/python/proto.py"", line 29, in <module>. from clif.python.utils import proto_util . ImportError: libprotobuf.so.24: cannot open shared object file: No such file or directory. ```. This is for the following command from build_and_test.sh:. ```. bazel test --host_javabase=@local_jdk//:jdk -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" \. deepvariant/... ```. Why is bazel running ```exec env -``` here? This is invalidating library paths (LD_LIBRARY_PATH variable) where it would find ```libprotobuf.so```. As expected, when I run the failing command from the trace without ```exec env -```, it is fine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/355:1139,deployability,modul,module,1139,"Hi @gunjanbaid I do not see the same issue now. I switched to openjdk-11. However, I still need to pass the --host_javabase option, which is not present by default in the build scripts. I do see another issue. The trace is as follows. ```. (02:26:24) ERROR: /root/deepvariant/third_party/nucleus/protos/BUILD:413:1: ClifProtoLibraryGeneration third_party/nucleus/protos/bedgraph_pyclif.h failed (Exit 1): proto failed: error executing command. (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \. exec env - \. bazel-out/host/bin/external/clif/proto -c bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.cc -h bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.h '--strip_dir=bazel-out/ppc-opt/bin' '--source_dir='\''.'\''' third_party/nucleus/protos/bedgraph.proto). Execution platform: @bazel_tools//platforms:host_platform . Traceback (most recent call last): . File ""bazel-out/host/bin/external/clif/proto"", line 5, in <module>. from clif.python.proto import start . File ""/root/opt/clif/lib64/python3.6/site-packages/clif/python/proto.py"", line 29, in <module>. from clif.python.utils import proto_util . ImportError: libprotobuf.so.24: cannot open shared object file: No such file or directory. ```. This is for the following command from build_and_test.sh:. ```. bazel test --host_javabase=@local_jdk//:jdk -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" \. deepvariant/... ```. Why is bazel running ```exec env -``` here? This is invalidating library paths (LD_LIBRARY_PATH variable) where it would find ```libprotobuf.so```. As expected, when I run the failing command from the trace without ```exec env -```, it is fine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/355:1644,deployability,fail,failing,1644,"Hi @gunjanbaid I do not see the same issue now. I switched to openjdk-11. However, I still need to pass the --host_javabase option, which is not present by default in the build scripts. I do see another issue. The trace is as follows. ```. (02:26:24) ERROR: /root/deepvariant/third_party/nucleus/protos/BUILD:413:1: ClifProtoLibraryGeneration third_party/nucleus/protos/bedgraph_pyclif.h failed (Exit 1): proto failed: error executing command. (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \. exec env - \. bazel-out/host/bin/external/clif/proto -c bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.cc -h bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.h '--strip_dir=bazel-out/ppc-opt/bin' '--source_dir='\''.'\''' third_party/nucleus/protos/bedgraph.proto). Execution platform: @bazel_tools//platforms:host_platform . Traceback (most recent call last): . File ""bazel-out/host/bin/external/clif/proto"", line 5, in <module>. from clif.python.proto import start . File ""/root/opt/clif/lib64/python3.6/site-packages/clif/python/proto.py"", line 29, in <module>. from clif.python.utils import proto_util . ImportError: libprotobuf.so.24: cannot open shared object file: No such file or directory. ```. This is for the following command from build_and_test.sh:. ```. bazel test --host_javabase=@local_jdk//:jdk -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" \. deepvariant/... ```. Why is bazel running ```exec env -``` here? This is invalidating library paths (LD_LIBRARY_PATH variable) where it would find ```libprotobuf.so```. As expected, when I run the failing command from the trace without ```exec env -```, it is fine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/355:859,interoperability,platform,platform,859,"Hi @gunjanbaid I do not see the same issue now. I switched to openjdk-11. However, I still need to pass the --host_javabase option, which is not present by default in the build scripts. I do see another issue. The trace is as follows. ```. (02:26:24) ERROR: /root/deepvariant/third_party/nucleus/protos/BUILD:413:1: ClifProtoLibraryGeneration third_party/nucleus/protos/bedgraph_pyclif.h failed (Exit 1): proto failed: error executing command. (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \. exec env - \. bazel-out/host/bin/external/clif/proto -c bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.cc -h bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.h '--strip_dir=bazel-out/ppc-opt/bin' '--source_dir='\''.'\''' third_party/nucleus/protos/bedgraph.proto). Execution platform: @bazel_tools//platforms:host_platform . Traceback (most recent call last): . File ""bazel-out/host/bin/external/clif/proto"", line 5, in <module>. from clif.python.proto import start . File ""/root/opt/clif/lib64/python3.6/site-packages/clif/python/proto.py"", line 29, in <module>. from clif.python.utils import proto_util . ImportError: libprotobuf.so.24: cannot open shared object file: No such file or directory. ```. This is for the following command from build_and_test.sh:. ```. bazel test --host_javabase=@local_jdk//:jdk -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" \. deepvariant/... ```. Why is bazel running ```exec env -``` here? This is invalidating library paths (LD_LIBRARY_PATH variable) where it would find ```libprotobuf.so```. As expected, when I run the failing command from the trace without ```exec env -```, it is fine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/355:883,interoperability,platform,platforms,883,"Hi @gunjanbaid I do not see the same issue now. I switched to openjdk-11. However, I still need to pass the --host_javabase option, which is not present by default in the build scripts. I do see another issue. The trace is as follows. ```. (02:26:24) ERROR: /root/deepvariant/third_party/nucleus/protos/BUILD:413:1: ClifProtoLibraryGeneration third_party/nucleus/protos/bedgraph_pyclif.h failed (Exit 1): proto failed: error executing command. (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \. exec env - \. bazel-out/host/bin/external/clif/proto -c bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.cc -h bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.h '--strip_dir=bazel-out/ppc-opt/bin' '--source_dir='\''.'\''' third_party/nucleus/protos/bedgraph.proto). Execution platform: @bazel_tools//platforms:host_platform . Traceback (most recent call last): . File ""bazel-out/host/bin/external/clif/proto"", line 5, in <module>. from clif.python.proto import start . File ""/root/opt/clif/lib64/python3.6/site-packages/clif/python/proto.py"", line 29, in <module>. from clif.python.utils import proto_util . ImportError: libprotobuf.so.24: cannot open shared object file: No such file or directory. ```. This is for the following command from build_and_test.sh:. ```. bazel test --host_javabase=@local_jdk//:jdk -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" \. deepvariant/... ```. Why is bazel running ```exec env -``` here? This is invalidating library paths (LD_LIBRARY_PATH variable) where it would find ```libprotobuf.so```. As expected, when I run the failing command from the trace without ```exec env -```, it is fine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/355:1235,interoperability,share,shared,1235,"Hi @gunjanbaid I do not see the same issue now. I switched to openjdk-11. However, I still need to pass the --host_javabase option, which is not present by default in the build scripts. I do see another issue. The trace is as follows. ```. (02:26:24) ERROR: /root/deepvariant/third_party/nucleus/protos/BUILD:413:1: ClifProtoLibraryGeneration third_party/nucleus/protos/bedgraph_pyclif.h failed (Exit 1): proto failed: error executing command. (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \. exec env - \. bazel-out/host/bin/external/clif/proto -c bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.cc -h bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.h '--strip_dir=bazel-out/ppc-opt/bin' '--source_dir='\''.'\''' third_party/nucleus/protos/bedgraph.proto). Execution platform: @bazel_tools//platforms:host_platform . Traceback (most recent call last): . File ""bazel-out/host/bin/external/clif/proto"", line 5, in <module>. from clif.python.proto import start . File ""/root/opt/clif/lib64/python3.6/site-packages/clif/python/proto.py"", line 29, in <module>. from clif.python.utils import proto_util . ImportError: libprotobuf.so.24: cannot open shared object file: No such file or directory. ```. This is for the following command from build_and_test.sh:. ```. bazel test --host_javabase=@local_jdk//:jdk -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" \. deepvariant/... ```. Why is bazel running ```exec env -``` here? This is invalidating library paths (LD_LIBRARY_PATH variable) where it would find ```libprotobuf.so```. As expected, when I run the failing command from the trace without ```exec env -```, it is fine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/355:1005,modifiability,modul,module,1005,"Hi @gunjanbaid I do not see the same issue now. I switched to openjdk-11. However, I still need to pass the --host_javabase option, which is not present by default in the build scripts. I do see another issue. The trace is as follows. ```. (02:26:24) ERROR: /root/deepvariant/third_party/nucleus/protos/BUILD:413:1: ClifProtoLibraryGeneration third_party/nucleus/protos/bedgraph_pyclif.h failed (Exit 1): proto failed: error executing command. (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \. exec env - \. bazel-out/host/bin/external/clif/proto -c bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.cc -h bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.h '--strip_dir=bazel-out/ppc-opt/bin' '--source_dir='\''.'\''' third_party/nucleus/protos/bedgraph.proto). Execution platform: @bazel_tools//platforms:host_platform . Traceback (most recent call last): . File ""bazel-out/host/bin/external/clif/proto"", line 5, in <module>. from clif.python.proto import start . File ""/root/opt/clif/lib64/python3.6/site-packages/clif/python/proto.py"", line 29, in <module>. from clif.python.utils import proto_util . ImportError: libprotobuf.so.24: cannot open shared object file: No such file or directory. ```. This is for the following command from build_and_test.sh:. ```. bazel test --host_javabase=@local_jdk//:jdk -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" \. deepvariant/... ```. Why is bazel running ```exec env -``` here? This is invalidating library paths (LD_LIBRARY_PATH variable) where it would find ```libprotobuf.so```. As expected, when I run the failing command from the trace without ```exec env -```, it is fine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/355:1094,modifiability,pac,packages,1094,"Hi @gunjanbaid I do not see the same issue now. I switched to openjdk-11. However, I still need to pass the --host_javabase option, which is not present by default in the build scripts. I do see another issue. The trace is as follows. ```. (02:26:24) ERROR: /root/deepvariant/third_party/nucleus/protos/BUILD:413:1: ClifProtoLibraryGeneration third_party/nucleus/protos/bedgraph_pyclif.h failed (Exit 1): proto failed: error executing command. (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \. exec env - \. bazel-out/host/bin/external/clif/proto -c bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.cc -h bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.h '--strip_dir=bazel-out/ppc-opt/bin' '--source_dir='\''.'\''' third_party/nucleus/protos/bedgraph.proto). Execution platform: @bazel_tools//platforms:host_platform . Traceback (most recent call last): . File ""bazel-out/host/bin/external/clif/proto"", line 5, in <module>. from clif.python.proto import start . File ""/root/opt/clif/lib64/python3.6/site-packages/clif/python/proto.py"", line 29, in <module>. from clif.python.utils import proto_util . ImportError: libprotobuf.so.24: cannot open shared object file: No such file or directory. ```. This is for the following command from build_and_test.sh:. ```. bazel test --host_javabase=@local_jdk//:jdk -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" \. deepvariant/... ```. Why is bazel running ```exec env -``` here? This is invalidating library paths (LD_LIBRARY_PATH variable) where it would find ```libprotobuf.so```. As expected, when I run the failing command from the trace without ```exec env -```, it is fine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/355:1139,modifiability,modul,module,1139,"Hi @gunjanbaid I do not see the same issue now. I switched to openjdk-11. However, I still need to pass the --host_javabase option, which is not present by default in the build scripts. I do see another issue. The trace is as follows. ```. (02:26:24) ERROR: /root/deepvariant/third_party/nucleus/protos/BUILD:413:1: ClifProtoLibraryGeneration third_party/nucleus/protos/bedgraph_pyclif.h failed (Exit 1): proto failed: error executing command. (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \. exec env - \. bazel-out/host/bin/external/clif/proto -c bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.cc -h bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.h '--strip_dir=bazel-out/ppc-opt/bin' '--source_dir='\''.'\''' third_party/nucleus/protos/bedgraph.proto). Execution platform: @bazel_tools//platforms:host_platform . Traceback (most recent call last): . File ""bazel-out/host/bin/external/clif/proto"", line 5, in <module>. from clif.python.proto import start . File ""/root/opt/clif/lib64/python3.6/site-packages/clif/python/proto.py"", line 29, in <module>. from clif.python.utils import proto_util . ImportError: libprotobuf.so.24: cannot open shared object file: No such file or directory. ```. This is for the following command from build_and_test.sh:. ```. bazel test --host_javabase=@local_jdk//:jdk -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" \. deepvariant/... ```. Why is bazel running ```exec env -``` here? This is invalidating library paths (LD_LIBRARY_PATH variable) where it would find ```libprotobuf.so```. As expected, when I run the failing command from the trace without ```exec env -```, it is fine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/355:1564,modifiability,variab,variable,1564,"Hi @gunjanbaid I do not see the same issue now. I switched to openjdk-11. However, I still need to pass the --host_javabase option, which is not present by default in the build scripts. I do see another issue. The trace is as follows. ```. (02:26:24) ERROR: /root/deepvariant/third_party/nucleus/protos/BUILD:413:1: ClifProtoLibraryGeneration third_party/nucleus/protos/bedgraph_pyclif.h failed (Exit 1): proto failed: error executing command. (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \. exec env - \. bazel-out/host/bin/external/clif/proto -c bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.cc -h bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.h '--strip_dir=bazel-out/ppc-opt/bin' '--source_dir='\''.'\''' third_party/nucleus/protos/bedgraph.proto). Execution platform: @bazel_tools//platforms:host_platform . Traceback (most recent call last): . File ""bazel-out/host/bin/external/clif/proto"", line 5, in <module>. from clif.python.proto import start . File ""/root/opt/clif/lib64/python3.6/site-packages/clif/python/proto.py"", line 29, in <module>. from clif.python.utils import proto_util . ImportError: libprotobuf.so.24: cannot open shared object file: No such file or directory. ```. This is for the following command from build_and_test.sh:. ```. bazel test --host_javabase=@local_jdk//:jdk -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" \. deepvariant/... ```. Why is bazel running ```exec env -``` here? This is invalidating library paths (LD_LIBRARY_PATH variable) where it would find ```libprotobuf.so```. As expected, when I run the failing command from the trace without ```exec env -```, it is fine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/355:251,performance,ERROR,ERROR,251,"Hi @gunjanbaid I do not see the same issue now. I switched to openjdk-11. However, I still need to pass the --host_javabase option, which is not present by default in the build scripts. I do see another issue. The trace is as follows. ```. (02:26:24) ERROR: /root/deepvariant/third_party/nucleus/protos/BUILD:413:1: ClifProtoLibraryGeneration third_party/nucleus/protos/bedgraph_pyclif.h failed (Exit 1): proto failed: error executing command. (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \. exec env - \. bazel-out/host/bin/external/clif/proto -c bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.cc -h bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.h '--strip_dir=bazel-out/ppc-opt/bin' '--source_dir='\''.'\''' third_party/nucleus/protos/bedgraph.proto). Execution platform: @bazel_tools//platforms:host_platform . Traceback (most recent call last): . File ""bazel-out/host/bin/external/clif/proto"", line 5, in <module>. from clif.python.proto import start . File ""/root/opt/clif/lib64/python3.6/site-packages/clif/python/proto.py"", line 29, in <module>. from clif.python.utils import proto_util . ImportError: libprotobuf.so.24: cannot open shared object file: No such file or directory. ```. This is for the following command from build_and_test.sh:. ```. bazel test --host_javabase=@local_jdk//:jdk -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" \. deepvariant/... ```. Why is bazel running ```exec env -``` here? This is invalidating library paths (LD_LIBRARY_PATH variable) where it would find ```libprotobuf.so```. As expected, when I run the failing command from the trace without ```exec env -```, it is fine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/355:419,performance,error,error,419,"Hi @gunjanbaid I do not see the same issue now. I switched to openjdk-11. However, I still need to pass the --host_javabase option, which is not present by default in the build scripts. I do see another issue. The trace is as follows. ```. (02:26:24) ERROR: /root/deepvariant/third_party/nucleus/protos/BUILD:413:1: ClifProtoLibraryGeneration third_party/nucleus/protos/bedgraph_pyclif.h failed (Exit 1): proto failed: error executing command. (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \. exec env - \. bazel-out/host/bin/external/clif/proto -c bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.cc -h bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.h '--strip_dir=bazel-out/ppc-opt/bin' '--source_dir='\''.'\''' third_party/nucleus/protos/bedgraph.proto). Execution platform: @bazel_tools//platforms:host_platform . Traceback (most recent call last): . File ""bazel-out/host/bin/external/clif/proto"", line 5, in <module>. from clif.python.proto import start . File ""/root/opt/clif/lib64/python3.6/site-packages/clif/python/proto.py"", line 29, in <module>. from clif.python.utils import proto_util . ImportError: libprotobuf.so.24: cannot open shared object file: No such file or directory. ```. This is for the following command from build_and_test.sh:. ```. bazel test --host_javabase=@local_jdk//:jdk -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" \. deepvariant/... ```. Why is bazel running ```exec env -``` here? This is invalidating library paths (LD_LIBRARY_PATH variable) where it would find ```libprotobuf.so```. As expected, when I run the failing command from the trace without ```exec env -```, it is fine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/355:455,performance,cach,cache,455,"Hi @gunjanbaid I do not see the same issue now. I switched to openjdk-11. However, I still need to pass the --host_javabase option, which is not present by default in the build scripts. I do see another issue. The trace is as follows. ```. (02:26:24) ERROR: /root/deepvariant/third_party/nucleus/protos/BUILD:413:1: ClifProtoLibraryGeneration third_party/nucleus/protos/bedgraph_pyclif.h failed (Exit 1): proto failed: error executing command. (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \. exec env - \. bazel-out/host/bin/external/clif/proto -c bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.cc -h bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.h '--strip_dir=bazel-out/ppc-opt/bin' '--source_dir='\''.'\''' third_party/nucleus/protos/bedgraph.proto). Execution platform: @bazel_tools//platforms:host_platform . Traceback (most recent call last): . File ""bazel-out/host/bin/external/clif/proto"", line 5, in <module>. from clif.python.proto import start . File ""/root/opt/clif/lib64/python3.6/site-packages/clif/python/proto.py"", line 29, in <module>. from clif.python.utils import proto_util . ImportError: libprotobuf.so.24: cannot open shared object file: No such file or directory. ```. This is for the following command from build_and_test.sh:. ```. bazel test --host_javabase=@local_jdk//:jdk -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" \. deepvariant/... ```. Why is bazel running ```exec env -``` here? This is invalidating library paths (LD_LIBRARY_PATH variable) where it would find ```libprotobuf.so```. As expected, when I run the failing command from the trace without ```exec env -```, it is fine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/355:388,reliability,fail,failed,388,"Hi @gunjanbaid I do not see the same issue now. I switched to openjdk-11. However, I still need to pass the --host_javabase option, which is not present by default in the build scripts. I do see another issue. The trace is as follows. ```. (02:26:24) ERROR: /root/deepvariant/third_party/nucleus/protos/BUILD:413:1: ClifProtoLibraryGeneration third_party/nucleus/protos/bedgraph_pyclif.h failed (Exit 1): proto failed: error executing command. (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \. exec env - \. bazel-out/host/bin/external/clif/proto -c bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.cc -h bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.h '--strip_dir=bazel-out/ppc-opt/bin' '--source_dir='\''.'\''' third_party/nucleus/protos/bedgraph.proto). Execution platform: @bazel_tools//platforms:host_platform . Traceback (most recent call last): . File ""bazel-out/host/bin/external/clif/proto"", line 5, in <module>. from clif.python.proto import start . File ""/root/opt/clif/lib64/python3.6/site-packages/clif/python/proto.py"", line 29, in <module>. from clif.python.utils import proto_util . ImportError: libprotobuf.so.24: cannot open shared object file: No such file or directory. ```. This is for the following command from build_and_test.sh:. ```. bazel test --host_javabase=@local_jdk//:jdk -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" \. deepvariant/... ```. Why is bazel running ```exec env -``` here? This is invalidating library paths (LD_LIBRARY_PATH variable) where it would find ```libprotobuf.so```. As expected, when I run the failing command from the trace without ```exec env -```, it is fine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/355:411,reliability,fail,failed,411,"Hi @gunjanbaid I do not see the same issue now. I switched to openjdk-11. However, I still need to pass the --host_javabase option, which is not present by default in the build scripts. I do see another issue. The trace is as follows. ```. (02:26:24) ERROR: /root/deepvariant/third_party/nucleus/protos/BUILD:413:1: ClifProtoLibraryGeneration third_party/nucleus/protos/bedgraph_pyclif.h failed (Exit 1): proto failed: error executing command. (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \. exec env - \. bazel-out/host/bin/external/clif/proto -c bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.cc -h bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.h '--strip_dir=bazel-out/ppc-opt/bin' '--source_dir='\''.'\''' third_party/nucleus/protos/bedgraph.proto). Execution platform: @bazel_tools//platforms:host_platform . Traceback (most recent call last): . File ""bazel-out/host/bin/external/clif/proto"", line 5, in <module>. from clif.python.proto import start . File ""/root/opt/clif/lib64/python3.6/site-packages/clif/python/proto.py"", line 29, in <module>. from clif.python.utils import proto_util . ImportError: libprotobuf.so.24: cannot open shared object file: No such file or directory. ```. This is for the following command from build_and_test.sh:. ```. bazel test --host_javabase=@local_jdk//:jdk -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" \. deepvariant/... ```. Why is bazel running ```exec env -``` here? This is invalidating library paths (LD_LIBRARY_PATH variable) where it would find ```libprotobuf.so```. As expected, when I run the failing command from the trace without ```exec env -```, it is fine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/355:1644,reliability,fail,failing,1644,"Hi @gunjanbaid I do not see the same issue now. I switched to openjdk-11. However, I still need to pass the --host_javabase option, which is not present by default in the build scripts. I do see another issue. The trace is as follows. ```. (02:26:24) ERROR: /root/deepvariant/third_party/nucleus/protos/BUILD:413:1: ClifProtoLibraryGeneration third_party/nucleus/protos/bedgraph_pyclif.h failed (Exit 1): proto failed: error executing command. (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \. exec env - \. bazel-out/host/bin/external/clif/proto -c bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.cc -h bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.h '--strip_dir=bazel-out/ppc-opt/bin' '--source_dir='\''.'\''' third_party/nucleus/protos/bedgraph.proto). Execution platform: @bazel_tools//platforms:host_platform . Traceback (most recent call last): . File ""bazel-out/host/bin/external/clif/proto"", line 5, in <module>. from clif.python.proto import start . File ""/root/opt/clif/lib64/python3.6/site-packages/clif/python/proto.py"", line 29, in <module>. from clif.python.utils import proto_util . ImportError: libprotobuf.so.24: cannot open shared object file: No such file or directory. ```. This is for the following command from build_and_test.sh:. ```. bazel test --host_javabase=@local_jdk//:jdk -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" \. deepvariant/... ```. Why is bazel running ```exec env -``` here? This is invalidating library paths (LD_LIBRARY_PATH variable) where it would find ```libprotobuf.so```. As expected, when I run the failing command from the trace without ```exec env -```, it is fine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/355:251,safety,ERROR,ERROR,251,"Hi @gunjanbaid I do not see the same issue now. I switched to openjdk-11. However, I still need to pass the --host_javabase option, which is not present by default in the build scripts. I do see another issue. The trace is as follows. ```. (02:26:24) ERROR: /root/deepvariant/third_party/nucleus/protos/BUILD:413:1: ClifProtoLibraryGeneration third_party/nucleus/protos/bedgraph_pyclif.h failed (Exit 1): proto failed: error executing command. (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \. exec env - \. bazel-out/host/bin/external/clif/proto -c bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.cc -h bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.h '--strip_dir=bazel-out/ppc-opt/bin' '--source_dir='\''.'\''' third_party/nucleus/protos/bedgraph.proto). Execution platform: @bazel_tools//platforms:host_platform . Traceback (most recent call last): . File ""bazel-out/host/bin/external/clif/proto"", line 5, in <module>. from clif.python.proto import start . File ""/root/opt/clif/lib64/python3.6/site-packages/clif/python/proto.py"", line 29, in <module>. from clif.python.utils import proto_util . ImportError: libprotobuf.so.24: cannot open shared object file: No such file or directory. ```. This is for the following command from build_and_test.sh:. ```. bazel test --host_javabase=@local_jdk//:jdk -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" \. deepvariant/... ```. Why is bazel running ```exec env -``` here? This is invalidating library paths (LD_LIBRARY_PATH variable) where it would find ```libprotobuf.so```. As expected, when I run the failing command from the trace without ```exec env -```, it is fine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/355:419,safety,error,error,419,"Hi @gunjanbaid I do not see the same issue now. I switched to openjdk-11. However, I still need to pass the --host_javabase option, which is not present by default in the build scripts. I do see another issue. The trace is as follows. ```. (02:26:24) ERROR: /root/deepvariant/third_party/nucleus/protos/BUILD:413:1: ClifProtoLibraryGeneration third_party/nucleus/protos/bedgraph_pyclif.h failed (Exit 1): proto failed: error executing command. (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \. exec env - \. bazel-out/host/bin/external/clif/proto -c bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.cc -h bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.h '--strip_dir=bazel-out/ppc-opt/bin' '--source_dir='\''.'\''' third_party/nucleus/protos/bedgraph.proto). Execution platform: @bazel_tools//platforms:host_platform . Traceback (most recent call last): . File ""bazel-out/host/bin/external/clif/proto"", line 5, in <module>. from clif.python.proto import start . File ""/root/opt/clif/lib64/python3.6/site-packages/clif/python/proto.py"", line 29, in <module>. from clif.python.utils import proto_util . ImportError: libprotobuf.so.24: cannot open shared object file: No such file or directory. ```. This is for the following command from build_and_test.sh:. ```. bazel test --host_javabase=@local_jdk//:jdk -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" \. deepvariant/... ```. Why is bazel running ```exec env -``` here? This is invalidating library paths (LD_LIBRARY_PATH variable) where it would find ```libprotobuf.so```. As expected, when I run the failing command from the trace without ```exec env -```, it is fine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/355:1005,safety,modul,module,1005,"Hi @gunjanbaid I do not see the same issue now. I switched to openjdk-11. However, I still need to pass the --host_javabase option, which is not present by default in the build scripts. I do see another issue. The trace is as follows. ```. (02:26:24) ERROR: /root/deepvariant/third_party/nucleus/protos/BUILD:413:1: ClifProtoLibraryGeneration third_party/nucleus/protos/bedgraph_pyclif.h failed (Exit 1): proto failed: error executing command. (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \. exec env - \. bazel-out/host/bin/external/clif/proto -c bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.cc -h bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.h '--strip_dir=bazel-out/ppc-opt/bin' '--source_dir='\''.'\''' third_party/nucleus/protos/bedgraph.proto). Execution platform: @bazel_tools//platforms:host_platform . Traceback (most recent call last): . File ""bazel-out/host/bin/external/clif/proto"", line 5, in <module>. from clif.python.proto import start . File ""/root/opt/clif/lib64/python3.6/site-packages/clif/python/proto.py"", line 29, in <module>. from clif.python.utils import proto_util . ImportError: libprotobuf.so.24: cannot open shared object file: No such file or directory. ```. This is for the following command from build_and_test.sh:. ```. bazel test --host_javabase=@local_jdk//:jdk -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" \. deepvariant/... ```. Why is bazel running ```exec env -``` here? This is invalidating library paths (LD_LIBRARY_PATH variable) where it would find ```libprotobuf.so```. As expected, when I run the failing command from the trace without ```exec env -```, it is fine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/355:1139,safety,modul,module,1139,"Hi @gunjanbaid I do not see the same issue now. I switched to openjdk-11. However, I still need to pass the --host_javabase option, which is not present by default in the build scripts. I do see another issue. The trace is as follows. ```. (02:26:24) ERROR: /root/deepvariant/third_party/nucleus/protos/BUILD:413:1: ClifProtoLibraryGeneration third_party/nucleus/protos/bedgraph_pyclif.h failed (Exit 1): proto failed: error executing command. (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \. exec env - \. bazel-out/host/bin/external/clif/proto -c bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.cc -h bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.h '--strip_dir=bazel-out/ppc-opt/bin' '--source_dir='\''.'\''' third_party/nucleus/protos/bedgraph.proto). Execution platform: @bazel_tools//platforms:host_platform . Traceback (most recent call last): . File ""bazel-out/host/bin/external/clif/proto"", line 5, in <module>. from clif.python.proto import start . File ""/root/opt/clif/lib64/python3.6/site-packages/clif/python/proto.py"", line 29, in <module>. from clif.python.utils import proto_util . ImportError: libprotobuf.so.24: cannot open shared object file: No such file or directory. ```. This is for the following command from build_and_test.sh:. ```. bazel test --host_javabase=@local_jdk//:jdk -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" \. deepvariant/... ```. Why is bazel running ```exec env -``` here? This is invalidating library paths (LD_LIBRARY_PATH variable) where it would find ```libprotobuf.so```. As expected, when I run the failing command from the trace without ```exec env -```, it is fine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/355:1357,safety,test,test,1357,"Hi @gunjanbaid I do not see the same issue now. I switched to openjdk-11. However, I still need to pass the --host_javabase option, which is not present by default in the build scripts. I do see another issue. The trace is as follows. ```. (02:26:24) ERROR: /root/deepvariant/third_party/nucleus/protos/BUILD:413:1: ClifProtoLibraryGeneration third_party/nucleus/protos/bedgraph_pyclif.h failed (Exit 1): proto failed: error executing command. (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \. exec env - \. bazel-out/host/bin/external/clif/proto -c bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.cc -h bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.h '--strip_dir=bazel-out/ppc-opt/bin' '--source_dir='\''.'\''' third_party/nucleus/protos/bedgraph.proto). Execution platform: @bazel_tools//platforms:host_platform . Traceback (most recent call last): . File ""bazel-out/host/bin/external/clif/proto"", line 5, in <module>. from clif.python.proto import start . File ""/root/opt/clif/lib64/python3.6/site-packages/clif/python/proto.py"", line 29, in <module>. from clif.python.utils import proto_util . ImportError: libprotobuf.so.24: cannot open shared object file: No such file or directory. ```. This is for the following command from build_and_test.sh:. ```. bazel test --host_javabase=@local_jdk//:jdk -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" \. deepvariant/... ```. Why is bazel running ```exec env -``` here? This is invalidating library paths (LD_LIBRARY_PATH variable) where it would find ```libprotobuf.so```. As expected, when I run the failing command from the trace without ```exec env -```, it is fine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/355:214,testability,trace,trace,214,"Hi @gunjanbaid I do not see the same issue now. I switched to openjdk-11. However, I still need to pass the --host_javabase option, which is not present by default in the build scripts. I do see another issue. The trace is as follows. ```. (02:26:24) ERROR: /root/deepvariant/third_party/nucleus/protos/BUILD:413:1: ClifProtoLibraryGeneration third_party/nucleus/protos/bedgraph_pyclif.h failed (Exit 1): proto failed: error executing command. (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \. exec env - \. bazel-out/host/bin/external/clif/proto -c bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.cc -h bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.h '--strip_dir=bazel-out/ppc-opt/bin' '--source_dir='\''.'\''' third_party/nucleus/protos/bedgraph.proto). Execution platform: @bazel_tools//platforms:host_platform . Traceback (most recent call last): . File ""bazel-out/host/bin/external/clif/proto"", line 5, in <module>. from clif.python.proto import start . File ""/root/opt/clif/lib64/python3.6/site-packages/clif/python/proto.py"", line 29, in <module>. from clif.python.utils import proto_util . ImportError: libprotobuf.so.24: cannot open shared object file: No such file or directory. ```. This is for the following command from build_and_test.sh:. ```. bazel test --host_javabase=@local_jdk//:jdk -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" \. deepvariant/... ```. Why is bazel running ```exec env -``` here? This is invalidating library paths (LD_LIBRARY_PATH variable) where it would find ```libprotobuf.so```. As expected, when I run the failing command from the trace without ```exec env -```, it is fine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/355:909,testability,Trace,Traceback,909,"Hi @gunjanbaid I do not see the same issue now. I switched to openjdk-11. However, I still need to pass the --host_javabase option, which is not present by default in the build scripts. I do see another issue. The trace is as follows. ```. (02:26:24) ERROR: /root/deepvariant/third_party/nucleus/protos/BUILD:413:1: ClifProtoLibraryGeneration third_party/nucleus/protos/bedgraph_pyclif.h failed (Exit 1): proto failed: error executing command. (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \. exec env - \. bazel-out/host/bin/external/clif/proto -c bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.cc -h bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.h '--strip_dir=bazel-out/ppc-opt/bin' '--source_dir='\''.'\''' third_party/nucleus/protos/bedgraph.proto). Execution platform: @bazel_tools//platforms:host_platform . Traceback (most recent call last): . File ""bazel-out/host/bin/external/clif/proto"", line 5, in <module>. from clif.python.proto import start . File ""/root/opt/clif/lib64/python3.6/site-packages/clif/python/proto.py"", line 29, in <module>. from clif.python.utils import proto_util . ImportError: libprotobuf.so.24: cannot open shared object file: No such file or directory. ```. This is for the following command from build_and_test.sh:. ```. bazel test --host_javabase=@local_jdk//:jdk -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" \. deepvariant/... ```. Why is bazel running ```exec env -``` here? This is invalidating library paths (LD_LIBRARY_PATH variable) where it would find ```libprotobuf.so```. As expected, when I run the failing command from the trace without ```exec env -```, it is fine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/355:1357,testability,test,test,1357,"Hi @gunjanbaid I do not see the same issue now. I switched to openjdk-11. However, I still need to pass the --host_javabase option, which is not present by default in the build scripts. I do see another issue. The trace is as follows. ```. (02:26:24) ERROR: /root/deepvariant/third_party/nucleus/protos/BUILD:413:1: ClifProtoLibraryGeneration third_party/nucleus/protos/bedgraph_pyclif.h failed (Exit 1): proto failed: error executing command. (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \. exec env - \. bazel-out/host/bin/external/clif/proto -c bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.cc -h bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.h '--strip_dir=bazel-out/ppc-opt/bin' '--source_dir='\''.'\''' third_party/nucleus/protos/bedgraph.proto). Execution platform: @bazel_tools//platforms:host_platform . Traceback (most recent call last): . File ""bazel-out/host/bin/external/clif/proto"", line 5, in <module>. from clif.python.proto import start . File ""/root/opt/clif/lib64/python3.6/site-packages/clif/python/proto.py"", line 29, in <module>. from clif.python.utils import proto_util . ImportError: libprotobuf.so.24: cannot open shared object file: No such file or directory. ```. This is for the following command from build_and_test.sh:. ```. bazel test --host_javabase=@local_jdk//:jdk -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" \. deepvariant/... ```. Why is bazel running ```exec env -``` here? This is invalidating library paths (LD_LIBRARY_PATH variable) where it would find ```libprotobuf.so```. As expected, when I run the failing command from the trace without ```exec env -```, it is fine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/355:1669,testability,trace,trace,1669,"Hi @gunjanbaid I do not see the same issue now. I switched to openjdk-11. However, I still need to pass the --host_javabase option, which is not present by default in the build scripts. I do see another issue. The trace is as follows. ```. (02:26:24) ERROR: /root/deepvariant/third_party/nucleus/protos/BUILD:413:1: ClifProtoLibraryGeneration third_party/nucleus/protos/bedgraph_pyclif.h failed (Exit 1): proto failed: error executing command. (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \. exec env - \. bazel-out/host/bin/external/clif/proto -c bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.cc -h bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.h '--strip_dir=bazel-out/ppc-opt/bin' '--source_dir='\''.'\''' third_party/nucleus/protos/bedgraph.proto). Execution platform: @bazel_tools//platforms:host_platform . Traceback (most recent call last): . File ""bazel-out/host/bin/external/clif/proto"", line 5, in <module>. from clif.python.proto import start . File ""/root/opt/clif/lib64/python3.6/site-packages/clif/python/proto.py"", line 29, in <module>. from clif.python.utils import proto_util . ImportError: libprotobuf.so.24: cannot open shared object file: No such file or directory. ```. This is for the following command from build_and_test.sh:. ```. bazel test --host_javabase=@local_jdk//:jdk -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" \. deepvariant/... ```. Why is bazel running ```exec env -``` here? This is invalidating library paths (LD_LIBRARY_PATH variable) where it would find ```libprotobuf.so```. As expected, when I run the failing command from the trace without ```exec env -```, it is fine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/355:251,usability,ERROR,ERROR,251,"Hi @gunjanbaid I do not see the same issue now. I switched to openjdk-11. However, I still need to pass the --host_javabase option, which is not present by default in the build scripts. I do see another issue. The trace is as follows. ```. (02:26:24) ERROR: /root/deepvariant/third_party/nucleus/protos/BUILD:413:1: ClifProtoLibraryGeneration third_party/nucleus/protos/bedgraph_pyclif.h failed (Exit 1): proto failed: error executing command. (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \. exec env - \. bazel-out/host/bin/external/clif/proto -c bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.cc -h bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.h '--strip_dir=bazel-out/ppc-opt/bin' '--source_dir='\''.'\''' third_party/nucleus/protos/bedgraph.proto). Execution platform: @bazel_tools//platforms:host_platform . Traceback (most recent call last): . File ""bazel-out/host/bin/external/clif/proto"", line 5, in <module>. from clif.python.proto import start . File ""/root/opt/clif/lib64/python3.6/site-packages/clif/python/proto.py"", line 29, in <module>. from clif.python.utils import proto_util . ImportError: libprotobuf.so.24: cannot open shared object file: No such file or directory. ```. This is for the following command from build_and_test.sh:. ```. bazel test --host_javabase=@local_jdk//:jdk -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" \. deepvariant/... ```. Why is bazel running ```exec env -``` here? This is invalidating library paths (LD_LIBRARY_PATH variable) where it would find ```libprotobuf.so```. As expected, when I run the failing command from the trace without ```exec env -```, it is fine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/355:419,usability,error,error,419,"Hi @gunjanbaid I do not see the same issue now. I switched to openjdk-11. However, I still need to pass the --host_javabase option, which is not present by default in the build scripts. I do see another issue. The trace is as follows. ```. (02:26:24) ERROR: /root/deepvariant/third_party/nucleus/protos/BUILD:413:1: ClifProtoLibraryGeneration third_party/nucleus/protos/bedgraph_pyclif.h failed (Exit 1): proto failed: error executing command. (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \. exec env - \. bazel-out/host/bin/external/clif/proto -c bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.cc -h bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.h '--strip_dir=bazel-out/ppc-opt/bin' '--source_dir='\''.'\''' third_party/nucleus/protos/bedgraph.proto). Execution platform: @bazel_tools//platforms:host_platform . Traceback (most recent call last): . File ""bazel-out/host/bin/external/clif/proto"", line 5, in <module>. from clif.python.proto import start . File ""/root/opt/clif/lib64/python3.6/site-packages/clif/python/proto.py"", line 29, in <module>. from clif.python.utils import proto_util . ImportError: libprotobuf.so.24: cannot open shared object file: No such file or directory. ```. This is for the following command from build_and_test.sh:. ```. bazel test --host_javabase=@local_jdk//:jdk -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" \. deepvariant/... ```. Why is bazel running ```exec env -``` here? This is invalidating library paths (LD_LIBRARY_PATH variable) where it would find ```libprotobuf.so```. As expected, when I run the failing command from the trace without ```exec env -```, it is fine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/355:435,usability,command,command,435,"Hi @gunjanbaid I do not see the same issue now. I switched to openjdk-11. However, I still need to pass the --host_javabase option, which is not present by default in the build scripts. I do see another issue. The trace is as follows. ```. (02:26:24) ERROR: /root/deepvariant/third_party/nucleus/protos/BUILD:413:1: ClifProtoLibraryGeneration third_party/nucleus/protos/bedgraph_pyclif.h failed (Exit 1): proto failed: error executing command. (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \. exec env - \. bazel-out/host/bin/external/clif/proto -c bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.cc -h bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.h '--strip_dir=bazel-out/ppc-opt/bin' '--source_dir='\''.'\''' third_party/nucleus/protos/bedgraph.proto). Execution platform: @bazel_tools//platforms:host_platform . Traceback (most recent call last): . File ""bazel-out/host/bin/external/clif/proto"", line 5, in <module>. from clif.python.proto import start . File ""/root/opt/clif/lib64/python3.6/site-packages/clif/python/proto.py"", line 29, in <module>. from clif.python.utils import proto_util . ImportError: libprotobuf.so.24: cannot open shared object file: No such file or directory. ```. This is for the following command from build_and_test.sh:. ```. bazel test --host_javabase=@local_jdk//:jdk -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" \. deepvariant/... ```. Why is bazel running ```exec env -``` here? This is invalidating library paths (LD_LIBRARY_PATH variable) where it would find ```libprotobuf.so```. As expected, when I run the failing command from the trace without ```exec env -```, it is fine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/355:1313,usability,command,command,1313,"Hi @gunjanbaid I do not see the same issue now. I switched to openjdk-11. However, I still need to pass the --host_javabase option, which is not present by default in the build scripts. I do see another issue. The trace is as follows. ```. (02:26:24) ERROR: /root/deepvariant/third_party/nucleus/protos/BUILD:413:1: ClifProtoLibraryGeneration third_party/nucleus/protos/bedgraph_pyclif.h failed (Exit 1): proto failed: error executing command. (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \. exec env - \. bazel-out/host/bin/external/clif/proto -c bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.cc -h bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.h '--strip_dir=bazel-out/ppc-opt/bin' '--source_dir='\''.'\''' third_party/nucleus/protos/bedgraph.proto). Execution platform: @bazel_tools//platforms:host_platform . Traceback (most recent call last): . File ""bazel-out/host/bin/external/clif/proto"", line 5, in <module>. from clif.python.proto import start . File ""/root/opt/clif/lib64/python3.6/site-packages/clif/python/proto.py"", line 29, in <module>. from clif.python.utils import proto_util . ImportError: libprotobuf.so.24: cannot open shared object file: No such file or directory. ```. This is for the following command from build_and_test.sh:. ```. bazel test --host_javabase=@local_jdk//:jdk -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" \. deepvariant/... ```. Why is bazel running ```exec env -``` here? This is invalidating library paths (LD_LIBRARY_PATH variable) where it would find ```libprotobuf.so```. As expected, when I run the failing command from the trace without ```exec env -```, it is fine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/355:1652,usability,command,command,1652,"Hi @gunjanbaid I do not see the same issue now. I switched to openjdk-11. However, I still need to pass the --host_javabase option, which is not present by default in the build scripts. I do see another issue. The trace is as follows. ```. (02:26:24) ERROR: /root/deepvariant/third_party/nucleus/protos/BUILD:413:1: ClifProtoLibraryGeneration third_party/nucleus/protos/bedgraph_pyclif.h failed (Exit 1): proto failed: error executing command. (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \. exec env - \. bazel-out/host/bin/external/clif/proto -c bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.cc -h bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.h '--strip_dir=bazel-out/ppc-opt/bin' '--source_dir='\''.'\''' third_party/nucleus/protos/bedgraph.proto). Execution platform: @bazel_tools//platforms:host_platform . Traceback (most recent call last): . File ""bazel-out/host/bin/external/clif/proto"", line 5, in <module>. from clif.python.proto import start . File ""/root/opt/clif/lib64/python3.6/site-packages/clif/python/proto.py"", line 29, in <module>. from clif.python.utils import proto_util . ImportError: libprotobuf.so.24: cannot open shared object file: No such file or directory. ```. This is for the following command from build_and_test.sh:. ```. bazel test --host_javabase=@local_jdk//:jdk -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" \. deepvariant/... ```. Why is bazel running ```exec env -``` here? This is invalidating library paths (LD_LIBRARY_PATH variable) where it would find ```libprotobuf.so```. As expected, when I run the failing command from the trace without ```exec env -```, it is fine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/355:0,deployability,Instal,Installing,0,"Installing everything to standard path or updating ldconf solves the issue. I still do not have the whole build working, because bazel is looking for standard python libraries (which is not what I am using) to be used. Overall it seems the installation expects everything to be in standard paths which I believe is a little too stringent, and it may be possible to fix that without too much trouble may be. But my original issues are no longer issues, so closing this ...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/355:42,deployability,updat,updating,42,"Installing everything to standard path or updating ldconf solves the issue. I still do not have the whole build working, because bazel is looking for standard python libraries (which is not what I am using) to be used. Overall it seems the installation expects everything to be in standard paths which I believe is a little too stringent, and it may be possible to fix that without too much trouble may be. But my original issues are no longer issues, so closing this ...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/355:106,deployability,build,build,106,"Installing everything to standard path or updating ldconf solves the issue. I still do not have the whole build working, because bazel is looking for standard python libraries (which is not what I am using) to be used. Overall it seems the installation expects everything to be in standard paths which I believe is a little too stringent, and it may be possible to fix that without too much trouble may be. But my original issues are no longer issues, so closing this ...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/355:240,deployability,instal,installation,240,"Installing everything to standard path or updating ldconf solves the issue. I still do not have the whole build working, because bazel is looking for standard python libraries (which is not what I am using) to be used. Overall it seems the installation expects everything to be in standard paths which I believe is a little too stringent, and it may be possible to fix that without too much trouble may be. But my original issues are no longer issues, so closing this ...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/355:25,interoperability,standard,standard,25,"Installing everything to standard path or updating ldconf solves the issue. I still do not have the whole build working, because bazel is looking for standard python libraries (which is not what I am using) to be used. Overall it seems the installation expects everything to be in standard paths which I believe is a little too stringent, and it may be possible to fix that without too much trouble may be. But my original issues are no longer issues, so closing this ...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/355:150,interoperability,standard,standard,150,"Installing everything to standard path or updating ldconf solves the issue. I still do not have the whole build working, because bazel is looking for standard python libraries (which is not what I am using) to be used. Overall it seems the installation expects everything to be in standard paths which I believe is a little too stringent, and it may be possible to fix that without too much trouble may be. But my original issues are no longer issues, so closing this ...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/355:281,interoperability,standard,standard,281,"Installing everything to standard path or updating ldconf solves the issue. I still do not have the whole build working, because bazel is looking for standard python libraries (which is not what I am using) to be used. Overall it seems the installation expects everything to be in standard paths which I believe is a little too stringent, and it may be possible to fix that without too much trouble may be. But my original issues are no longer issues, so closing this ...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/355:42,safety,updat,updating,42,"Installing everything to standard path or updating ldconf solves the issue. I still do not have the whole build working, because bazel is looking for standard python libraries (which is not what I am using) to be used. Overall it seems the installation expects everything to be in standard paths which I believe is a little too stringent, and it may be possible to fix that without too much trouble may be. But my original issues are no longer issues, so closing this ...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/355:42,security,updat,updating,42,"Installing everything to standard path or updating ldconf solves the issue. I still do not have the whole build working, because bazel is looking for standard python libraries (which is not what I am using) to be used. Overall it seems the installation expects everything to be in standard paths which I believe is a little too stringent, and it may be possible to fix that without too much trouble may be. But my original issues are no longer issues, so closing this ...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/355
https://github.com/google/deepvariant/issues/356:103,usability,tool,tools,103,Upon further examination I find that the file ```bazel-out/host/bin/external/org_tensorflow/tensorflow/tools/git/gen_git_source``` has the following hard-coded into it:. ```PYTHON_BINARY = '/usr/bin/python3.6'```. I am not sure where this is generated from.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/356
https://github.com/google/deepvariant/issues/356:240,deployability,build,building,240,"Hi @anands-repo, glad you were able to get it working! I don't have any other comments on the fix and will defer to the relevant bazel issue. In general, I would recommend running DeepVariant using Docker for the simplest setup. If you are building from source because you want to experiment with changes to the codebase, I'd still recommend Docker. You can clone the DeepVariant repo, modify the source code, and build a Docker image with your changes using [the provided Dockerfile](https://github.com/google/deepvariant/blob/r1.0/Dockerfile).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/356
https://github.com/google/deepvariant/issues/356:414,deployability,build,build,414,"Hi @anands-repo, glad you were able to get it working! I don't have any other comments on the fix and will defer to the relevant bazel issue. In general, I would recommend running DeepVariant using Docker for the simplest setup. If you are building from source because you want to experiment with changes to the codebase, I'd still recommend Docker. You can clone the DeepVariant repo, modify the source code, and build a Docker image with your changes using [the provided Dockerfile](https://github.com/google/deepvariant/blob/r1.0/Dockerfile).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/356
https://github.com/google/deepvariant/issues/356:386,security,modif,modify,386,"Hi @anands-repo, glad you were able to get it working! I don't have any other comments on the fix and will defer to the relevant bazel issue. In general, I would recommend running DeepVariant using Docker for the simplest setup. If you are building from source because you want to experiment with changes to the codebase, I'd still recommend Docker. You can clone the DeepVariant repo, modify the source code, and build a Docker image with your changes using [the provided Dockerfile](https://github.com/google/deepvariant/blob/r1.0/Dockerfile).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/356
https://github.com/google/deepvariant/issues/356:213,testability,simpl,simplest,213,"Hi @anands-repo, glad you were able to get it working! I don't have any other comments on the fix and will defer to the relevant bazel issue. In general, I would recommend running DeepVariant using Docker for the simplest setup. If you are building from source because you want to experiment with changes to the codebase, I'd still recommend Docker. You can clone the DeepVariant repo, modify the source code, and build a Docker image with your changes using [the provided Dockerfile](https://github.com/google/deepvariant/blob/r1.0/Dockerfile).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/356
https://github.com/google/deepvariant/issues/356:213,usability,simpl,simplest,213,"Hi @anands-repo, glad you were able to get it working! I don't have any other comments on the fix and will defer to the relevant bazel issue. In general, I would recommend running DeepVariant using Docker for the simplest setup. If you are building from source because you want to experiment with changes to the codebase, I'd still recommend Docker. You can clone the DeepVariant repo, modify the source code, and build a Docker image with your changes using [the provided Dockerfile](https://github.com/google/deepvariant/blob/r1.0/Dockerfile).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/356
https://github.com/google/deepvariant/issues/356:999,availability,error,error,999,"Hi @gunjanbaid . Unfortunately I am not compiling for x86, but for IBM power, so most of the installation scripts need to be discarded, and packages need to be manually compiled from source using IBM's Advance Toolchain gcc compilers. I have finally gotten all bazel tests to complete as well as the build to complete. I was wondering whether you could explain one piece of the build files though - this is just out of curiosity. In build_release_binaries, there is a function that starts as follows - which seems to be performing a hack to fix something:. ```. # Bazel's --build_python_zip replaces our carefully engineered symbolic links. # with copies. This function puts the symbolic links back. function fix_zip_file {. orig_zip_file=$1. # Step 1: Copy the zip file to a temporary place. TMPDIR=$(mktemp -d -t tmp.XXXXXXXXXXX). # The .zip version of the binary doesn't have the header that makes it. # self-executable. We use that version because otherwise unzip would. # complain and raise an error code. cp ""${orig_zip_file}.zip"" ""${TMPDIR}"". ```. Would you be able to give a quick explanation of what the problem is? I understand what it does, but I do not understand why it is needed, or whether it is just for convenience. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/356
https://github.com/google/deepvariant/issues/356:93,deployability,instal,installation,93,"Hi @gunjanbaid . Unfortunately I am not compiling for x86, but for IBM power, so most of the installation scripts need to be discarded, and packages need to be manually compiled from source using IBM's Advance Toolchain gcc compilers. I have finally gotten all bazel tests to complete as well as the build to complete. I was wondering whether you could explain one piece of the build files though - this is just out of curiosity. In build_release_binaries, there is a function that starts as follows - which seems to be performing a hack to fix something:. ```. # Bazel's --build_python_zip replaces our carefully engineered symbolic links. # with copies. This function puts the symbolic links back. function fix_zip_file {. orig_zip_file=$1. # Step 1: Copy the zip file to a temporary place. TMPDIR=$(mktemp -d -t tmp.XXXXXXXXXXX). # The .zip version of the binary doesn't have the header that makes it. # self-executable. We use that version because otherwise unzip would. # complain and raise an error code. cp ""${orig_zip_file}.zip"" ""${TMPDIR}"". ```. Would you be able to give a quick explanation of what the problem is? I understand what it does, but I do not understand why it is needed, or whether it is just for convenience. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/356
https://github.com/google/deepvariant/issues/356:210,deployability,Toolchain,Toolchain,210,"Hi @gunjanbaid . Unfortunately I am not compiling for x86, but for IBM power, so most of the installation scripts need to be discarded, and packages need to be manually compiled from source using IBM's Advance Toolchain gcc compilers. I have finally gotten all bazel tests to complete as well as the build to complete. I was wondering whether you could explain one piece of the build files though - this is just out of curiosity. In build_release_binaries, there is a function that starts as follows - which seems to be performing a hack to fix something:. ```. # Bazel's --build_python_zip replaces our carefully engineered symbolic links. # with copies. This function puts the symbolic links back. function fix_zip_file {. orig_zip_file=$1. # Step 1: Copy the zip file to a temporary place. TMPDIR=$(mktemp -d -t tmp.XXXXXXXXXXX). # The .zip version of the binary doesn't have the header that makes it. # self-executable. We use that version because otherwise unzip would. # complain and raise an error code. cp ""${orig_zip_file}.zip"" ""${TMPDIR}"". ```. Would you be able to give a quick explanation of what the problem is? I understand what it does, but I do not understand why it is needed, or whether it is just for convenience. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/356
https://github.com/google/deepvariant/issues/356:300,deployability,build,build,300,"Hi @gunjanbaid . Unfortunately I am not compiling for x86, but for IBM power, so most of the installation scripts need to be discarded, and packages need to be manually compiled from source using IBM's Advance Toolchain gcc compilers. I have finally gotten all bazel tests to complete as well as the build to complete. I was wondering whether you could explain one piece of the build files though - this is just out of curiosity. In build_release_binaries, there is a function that starts as follows - which seems to be performing a hack to fix something:. ```. # Bazel's --build_python_zip replaces our carefully engineered symbolic links. # with copies. This function puts the symbolic links back. function fix_zip_file {. orig_zip_file=$1. # Step 1: Copy the zip file to a temporary place. TMPDIR=$(mktemp -d -t tmp.XXXXXXXXXXX). # The .zip version of the binary doesn't have the header that makes it. # self-executable. We use that version because otherwise unzip would. # complain and raise an error code. cp ""${orig_zip_file}.zip"" ""${TMPDIR}"". ```. Would you be able to give a quick explanation of what the problem is? I understand what it does, but I do not understand why it is needed, or whether it is just for convenience. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/356
https://github.com/google/deepvariant/issues/356:378,deployability,build,build,378,"Hi @gunjanbaid . Unfortunately I am not compiling for x86, but for IBM power, so most of the installation scripts need to be discarded, and packages need to be manually compiled from source using IBM's Advance Toolchain gcc compilers. I have finally gotten all bazel tests to complete as well as the build to complete. I was wondering whether you could explain one piece of the build files though - this is just out of curiosity. In build_release_binaries, there is a function that starts as follows - which seems to be performing a hack to fix something:. ```. # Bazel's --build_python_zip replaces our carefully engineered symbolic links. # with copies. This function puts the symbolic links back. function fix_zip_file {. orig_zip_file=$1. # Step 1: Copy the zip file to a temporary place. TMPDIR=$(mktemp -d -t tmp.XXXXXXXXXXX). # The .zip version of the binary doesn't have the header that makes it. # self-executable. We use that version because otherwise unzip would. # complain and raise an error code. cp ""${orig_zip_file}.zip"" ""${TMPDIR}"". ```. Would you be able to give a quick explanation of what the problem is? I understand what it does, but I do not understand why it is needed, or whether it is just for convenience. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/356
https://github.com/google/deepvariant/issues/356:844,deployability,version,version,844,"Hi @gunjanbaid . Unfortunately I am not compiling for x86, but for IBM power, so most of the installation scripts need to be discarded, and packages need to be manually compiled from source using IBM's Advance Toolchain gcc compilers. I have finally gotten all bazel tests to complete as well as the build to complete. I was wondering whether you could explain one piece of the build files though - this is just out of curiosity. In build_release_binaries, there is a function that starts as follows - which seems to be performing a hack to fix something:. ```. # Bazel's --build_python_zip replaces our carefully engineered symbolic links. # with copies. This function puts the symbolic links back. function fix_zip_file {. orig_zip_file=$1. # Step 1: Copy the zip file to a temporary place. TMPDIR=$(mktemp -d -t tmp.XXXXXXXXXXX). # The .zip version of the binary doesn't have the header that makes it. # self-executable. We use that version because otherwise unzip would. # complain and raise an error code. cp ""${orig_zip_file}.zip"" ""${TMPDIR}"". ```. Would you be able to give a quick explanation of what the problem is? I understand what it does, but I do not understand why it is needed, or whether it is just for convenience. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/356
https://github.com/google/deepvariant/issues/356:936,deployability,version,version,936,"Hi @gunjanbaid . Unfortunately I am not compiling for x86, but for IBM power, so most of the installation scripts need to be discarded, and packages need to be manually compiled from source using IBM's Advance Toolchain gcc compilers. I have finally gotten all bazel tests to complete as well as the build to complete. I was wondering whether you could explain one piece of the build files though - this is just out of curiosity. In build_release_binaries, there is a function that starts as follows - which seems to be performing a hack to fix something:. ```. # Bazel's --build_python_zip replaces our carefully engineered symbolic links. # with copies. This function puts the symbolic links back. function fix_zip_file {. orig_zip_file=$1. # Step 1: Copy the zip file to a temporary place. TMPDIR=$(mktemp -d -t tmp.XXXXXXXXXXX). # The .zip version of the binary doesn't have the header that makes it. # self-executable. We use that version because otherwise unzip would. # complain and raise an error code. cp ""${orig_zip_file}.zip"" ""${TMPDIR}"". ```. Would you be able to give a quick explanation of what the problem is? I understand what it does, but I do not understand why it is needed, or whether it is just for convenience. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/356
https://github.com/google/deepvariant/issues/356:71,energy efficiency,power,power,71,"Hi @gunjanbaid . Unfortunately I am not compiling for x86, but for IBM power, so most of the installation scripts need to be discarded, and packages need to be manually compiled from source using IBM's Advance Toolchain gcc compilers. I have finally gotten all bazel tests to complete as well as the build to complete. I was wondering whether you could explain one piece of the build files though - this is just out of curiosity. In build_release_binaries, there is a function that starts as follows - which seems to be performing a hack to fix something:. ```. # Bazel's --build_python_zip replaces our carefully engineered symbolic links. # with copies. This function puts the symbolic links back. function fix_zip_file {. orig_zip_file=$1. # Step 1: Copy the zip file to a temporary place. TMPDIR=$(mktemp -d -t tmp.XXXXXXXXXXX). # The .zip version of the binary doesn't have the header that makes it. # self-executable. We use that version because otherwise unzip would. # complain and raise an error code. cp ""${orig_zip_file}.zip"" ""${TMPDIR}"". ```. Would you be able to give a quick explanation of what the problem is? I understand what it does, but I do not understand why it is needed, or whether it is just for convenience. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/356
https://github.com/google/deepvariant/issues/356:844,integrability,version,version,844,"Hi @gunjanbaid . Unfortunately I am not compiling for x86, but for IBM power, so most of the installation scripts need to be discarded, and packages need to be manually compiled from source using IBM's Advance Toolchain gcc compilers. I have finally gotten all bazel tests to complete as well as the build to complete. I was wondering whether you could explain one piece of the build files though - this is just out of curiosity. In build_release_binaries, there is a function that starts as follows - which seems to be performing a hack to fix something:. ```. # Bazel's --build_python_zip replaces our carefully engineered symbolic links. # with copies. This function puts the symbolic links back. function fix_zip_file {. orig_zip_file=$1. # Step 1: Copy the zip file to a temporary place. TMPDIR=$(mktemp -d -t tmp.XXXXXXXXXXX). # The .zip version of the binary doesn't have the header that makes it. # self-executable. We use that version because otherwise unzip would. # complain and raise an error code. cp ""${orig_zip_file}.zip"" ""${TMPDIR}"". ```. Would you be able to give a quick explanation of what the problem is? I understand what it does, but I do not understand why it is needed, or whether it is just for convenience. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/356
https://github.com/google/deepvariant/issues/356:936,integrability,version,version,936,"Hi @gunjanbaid . Unfortunately I am not compiling for x86, but for IBM power, so most of the installation scripts need to be discarded, and packages need to be manually compiled from source using IBM's Advance Toolchain gcc compilers. I have finally gotten all bazel tests to complete as well as the build to complete. I was wondering whether you could explain one piece of the build files though - this is just out of curiosity. In build_release_binaries, there is a function that starts as follows - which seems to be performing a hack to fix something:. ```. # Bazel's --build_python_zip replaces our carefully engineered symbolic links. # with copies. This function puts the symbolic links back. function fix_zip_file {. orig_zip_file=$1. # Step 1: Copy the zip file to a temporary place. TMPDIR=$(mktemp -d -t tmp.XXXXXXXXXXX). # The .zip version of the binary doesn't have the header that makes it. # self-executable. We use that version because otherwise unzip would. # complain and raise an error code. cp ""${orig_zip_file}.zip"" ""${TMPDIR}"". ```. Would you be able to give a quick explanation of what the problem is? I understand what it does, but I do not understand why it is needed, or whether it is just for convenience. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/356
https://github.com/google/deepvariant/issues/356:140,modifiability,pac,packages,140,"Hi @gunjanbaid . Unfortunately I am not compiling for x86, but for IBM power, so most of the installation scripts need to be discarded, and packages need to be manually compiled from source using IBM's Advance Toolchain gcc compilers. I have finally gotten all bazel tests to complete as well as the build to complete. I was wondering whether you could explain one piece of the build files though - this is just out of curiosity. In build_release_binaries, there is a function that starts as follows - which seems to be performing a hack to fix something:. ```. # Bazel's --build_python_zip replaces our carefully engineered symbolic links. # with copies. This function puts the symbolic links back. function fix_zip_file {. orig_zip_file=$1. # Step 1: Copy the zip file to a temporary place. TMPDIR=$(mktemp -d -t tmp.XXXXXXXXXXX). # The .zip version of the binary doesn't have the header that makes it. # self-executable. We use that version because otherwise unzip would. # complain and raise an error code. cp ""${orig_zip_file}.zip"" ""${TMPDIR}"". ```. Would you be able to give a quick explanation of what the problem is? I understand what it does, but I do not understand why it is needed, or whether it is just for convenience. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/356
https://github.com/google/deepvariant/issues/356:844,modifiability,version,version,844,"Hi @gunjanbaid . Unfortunately I am not compiling for x86, but for IBM power, so most of the installation scripts need to be discarded, and packages need to be manually compiled from source using IBM's Advance Toolchain gcc compilers. I have finally gotten all bazel tests to complete as well as the build to complete. I was wondering whether you could explain one piece of the build files though - this is just out of curiosity. In build_release_binaries, there is a function that starts as follows - which seems to be performing a hack to fix something:. ```. # Bazel's --build_python_zip replaces our carefully engineered symbolic links. # with copies. This function puts the symbolic links back. function fix_zip_file {. orig_zip_file=$1. # Step 1: Copy the zip file to a temporary place. TMPDIR=$(mktemp -d -t tmp.XXXXXXXXXXX). # The .zip version of the binary doesn't have the header that makes it. # self-executable. We use that version because otherwise unzip would. # complain and raise an error code. cp ""${orig_zip_file}.zip"" ""${TMPDIR}"". ```. Would you be able to give a quick explanation of what the problem is? I understand what it does, but I do not understand why it is needed, or whether it is just for convenience. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/356
https://github.com/google/deepvariant/issues/356:936,modifiability,version,version,936,"Hi @gunjanbaid . Unfortunately I am not compiling for x86, but for IBM power, so most of the installation scripts need to be discarded, and packages need to be manually compiled from source using IBM's Advance Toolchain gcc compilers. I have finally gotten all bazel tests to complete as well as the build to complete. I was wondering whether you could explain one piece of the build files though - this is just out of curiosity. In build_release_binaries, there is a function that starts as follows - which seems to be performing a hack to fix something:. ```. # Bazel's --build_python_zip replaces our carefully engineered symbolic links. # with copies. This function puts the symbolic links back. function fix_zip_file {. orig_zip_file=$1. # Step 1: Copy the zip file to a temporary place. TMPDIR=$(mktemp -d -t tmp.XXXXXXXXXXX). # The .zip version of the binary doesn't have the header that makes it. # self-executable. We use that version because otherwise unzip would. # complain and raise an error code. cp ""${orig_zip_file}.zip"" ""${TMPDIR}"". ```. Would you be able to give a quick explanation of what the problem is? I understand what it does, but I do not understand why it is needed, or whether it is just for convenience. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/356
https://github.com/google/deepvariant/issues/356:520,performance,perform,performing,520,"Hi @gunjanbaid . Unfortunately I am not compiling for x86, but for IBM power, so most of the installation scripts need to be discarded, and packages need to be manually compiled from source using IBM's Advance Toolchain gcc compilers. I have finally gotten all bazel tests to complete as well as the build to complete. I was wondering whether you could explain one piece of the build files though - this is just out of curiosity. In build_release_binaries, there is a function that starts as follows - which seems to be performing a hack to fix something:. ```. # Bazel's --build_python_zip replaces our carefully engineered symbolic links. # with copies. This function puts the symbolic links back. function fix_zip_file {. orig_zip_file=$1. # Step 1: Copy the zip file to a temporary place. TMPDIR=$(mktemp -d -t tmp.XXXXXXXXXXX). # The .zip version of the binary doesn't have the header that makes it. # self-executable. We use that version because otherwise unzip would. # complain and raise an error code. cp ""${orig_zip_file}.zip"" ""${TMPDIR}"". ```. Would you be able to give a quick explanation of what the problem is? I understand what it does, but I do not understand why it is needed, or whether it is just for convenience. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/356
https://github.com/google/deepvariant/issues/356:999,performance,error,error,999,"Hi @gunjanbaid . Unfortunately I am not compiling for x86, but for IBM power, so most of the installation scripts need to be discarded, and packages need to be manually compiled from source using IBM's Advance Toolchain gcc compilers. I have finally gotten all bazel tests to complete as well as the build to complete. I was wondering whether you could explain one piece of the build files though - this is just out of curiosity. In build_release_binaries, there is a function that starts as follows - which seems to be performing a hack to fix something:. ```. # Bazel's --build_python_zip replaces our carefully engineered symbolic links. # with copies. This function puts the symbolic links back. function fix_zip_file {. orig_zip_file=$1. # Step 1: Copy the zip file to a temporary place. TMPDIR=$(mktemp -d -t tmp.XXXXXXXXXXX). # The .zip version of the binary doesn't have the header that makes it. # self-executable. We use that version because otherwise unzip would. # complain and raise an error code. cp ""${orig_zip_file}.zip"" ""${TMPDIR}"". ```. Would you be able to give a quick explanation of what the problem is? I understand what it does, but I do not understand why it is needed, or whether it is just for convenience. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/356
https://github.com/google/deepvariant/issues/356:866,reliability,doe,doesn,866,"Hi @gunjanbaid . Unfortunately I am not compiling for x86, but for IBM power, so most of the installation scripts need to be discarded, and packages need to be manually compiled from source using IBM's Advance Toolchain gcc compilers. I have finally gotten all bazel tests to complete as well as the build to complete. I was wondering whether you could explain one piece of the build files though - this is just out of curiosity. In build_release_binaries, there is a function that starts as follows - which seems to be performing a hack to fix something:. ```. # Bazel's --build_python_zip replaces our carefully engineered symbolic links. # with copies. This function puts the symbolic links back. function fix_zip_file {. orig_zip_file=$1. # Step 1: Copy the zip file to a temporary place. TMPDIR=$(mktemp -d -t tmp.XXXXXXXXXXX). # The .zip version of the binary doesn't have the header that makes it. # self-executable. We use that version because otherwise unzip would. # complain and raise an error code. cp ""${orig_zip_file}.zip"" ""${TMPDIR}"". ```. Would you be able to give a quick explanation of what the problem is? I understand what it does, but I do not understand why it is needed, or whether it is just for convenience. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/356
https://github.com/google/deepvariant/issues/356:1146,reliability,doe,does,1146,"Hi @gunjanbaid . Unfortunately I am not compiling for x86, but for IBM power, so most of the installation scripts need to be discarded, and packages need to be manually compiled from source using IBM's Advance Toolchain gcc compilers. I have finally gotten all bazel tests to complete as well as the build to complete. I was wondering whether you could explain one piece of the build files though - this is just out of curiosity. In build_release_binaries, there is a function that starts as follows - which seems to be performing a hack to fix something:. ```. # Bazel's --build_python_zip replaces our carefully engineered symbolic links. # with copies. This function puts the symbolic links back. function fix_zip_file {. orig_zip_file=$1. # Step 1: Copy the zip file to a temporary place. TMPDIR=$(mktemp -d -t tmp.XXXXXXXXXXX). # The .zip version of the binary doesn't have the header that makes it. # self-executable. We use that version because otherwise unzip would. # complain and raise an error code. cp ""${orig_zip_file}.zip"" ""${TMPDIR}"". ```. Would you be able to give a quick explanation of what the problem is? I understand what it does, but I do not understand why it is needed, or whether it is just for convenience. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/356
https://github.com/google/deepvariant/issues/356:267,safety,test,tests,267,"Hi @gunjanbaid . Unfortunately I am not compiling for x86, but for IBM power, so most of the installation scripts need to be discarded, and packages need to be manually compiled from source using IBM's Advance Toolchain gcc compilers. I have finally gotten all bazel tests to complete as well as the build to complete. I was wondering whether you could explain one piece of the build files though - this is just out of curiosity. In build_release_binaries, there is a function that starts as follows - which seems to be performing a hack to fix something:. ```. # Bazel's --build_python_zip replaces our carefully engineered symbolic links. # with copies. This function puts the symbolic links back. function fix_zip_file {. orig_zip_file=$1. # Step 1: Copy the zip file to a temporary place. TMPDIR=$(mktemp -d -t tmp.XXXXXXXXXXX). # The .zip version of the binary doesn't have the header that makes it. # self-executable. We use that version because otherwise unzip would. # complain and raise an error code. cp ""${orig_zip_file}.zip"" ""${TMPDIR}"". ```. Would you be able to give a quick explanation of what the problem is? I understand what it does, but I do not understand why it is needed, or whether it is just for convenience. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/356
https://github.com/google/deepvariant/issues/356:276,safety,compl,complete,276,"Hi @gunjanbaid . Unfortunately I am not compiling for x86, but for IBM power, so most of the installation scripts need to be discarded, and packages need to be manually compiled from source using IBM's Advance Toolchain gcc compilers. I have finally gotten all bazel tests to complete as well as the build to complete. I was wondering whether you could explain one piece of the build files though - this is just out of curiosity. In build_release_binaries, there is a function that starts as follows - which seems to be performing a hack to fix something:. ```. # Bazel's --build_python_zip replaces our carefully engineered symbolic links. # with copies. This function puts the symbolic links back. function fix_zip_file {. orig_zip_file=$1. # Step 1: Copy the zip file to a temporary place. TMPDIR=$(mktemp -d -t tmp.XXXXXXXXXXX). # The .zip version of the binary doesn't have the header that makes it. # self-executable. We use that version because otherwise unzip would. # complain and raise an error code. cp ""${orig_zip_file}.zip"" ""${TMPDIR}"". ```. Would you be able to give a quick explanation of what the problem is? I understand what it does, but I do not understand why it is needed, or whether it is just for convenience. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/356
https://github.com/google/deepvariant/issues/356:309,safety,compl,complete,309,"Hi @gunjanbaid . Unfortunately I am not compiling for x86, but for IBM power, so most of the installation scripts need to be discarded, and packages need to be manually compiled from source using IBM's Advance Toolchain gcc compilers. I have finally gotten all bazel tests to complete as well as the build to complete. I was wondering whether you could explain one piece of the build files though - this is just out of curiosity. In build_release_binaries, there is a function that starts as follows - which seems to be performing a hack to fix something:. ```. # Bazel's --build_python_zip replaces our carefully engineered symbolic links. # with copies. This function puts the symbolic links back. function fix_zip_file {. orig_zip_file=$1. # Step 1: Copy the zip file to a temporary place. TMPDIR=$(mktemp -d -t tmp.XXXXXXXXXXX). # The .zip version of the binary doesn't have the header that makes it. # self-executable. We use that version because otherwise unzip would. # complain and raise an error code. cp ""${orig_zip_file}.zip"" ""${TMPDIR}"". ```. Would you be able to give a quick explanation of what the problem is? I understand what it does, but I do not understand why it is needed, or whether it is just for convenience. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/356
https://github.com/google/deepvariant/issues/356:977,safety,compl,complain,977,"Hi @gunjanbaid . Unfortunately I am not compiling for x86, but for IBM power, so most of the installation scripts need to be discarded, and packages need to be manually compiled from source using IBM's Advance Toolchain gcc compilers. I have finally gotten all bazel tests to complete as well as the build to complete. I was wondering whether you could explain one piece of the build files though - this is just out of curiosity. In build_release_binaries, there is a function that starts as follows - which seems to be performing a hack to fix something:. ```. # Bazel's --build_python_zip replaces our carefully engineered symbolic links. # with copies. This function puts the symbolic links back. function fix_zip_file {. orig_zip_file=$1. # Step 1: Copy the zip file to a temporary place. TMPDIR=$(mktemp -d -t tmp.XXXXXXXXXXX). # The .zip version of the binary doesn't have the header that makes it. # self-executable. We use that version because otherwise unzip would. # complain and raise an error code. cp ""${orig_zip_file}.zip"" ""${TMPDIR}"". ```. Would you be able to give a quick explanation of what the problem is? I understand what it does, but I do not understand why it is needed, or whether it is just for convenience. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/356
https://github.com/google/deepvariant/issues/356:999,safety,error,error,999,"Hi @gunjanbaid . Unfortunately I am not compiling for x86, but for IBM power, so most of the installation scripts need to be discarded, and packages need to be manually compiled from source using IBM's Advance Toolchain gcc compilers. I have finally gotten all bazel tests to complete as well as the build to complete. I was wondering whether you could explain one piece of the build files though - this is just out of curiosity. In build_release_binaries, there is a function that starts as follows - which seems to be performing a hack to fix something:. ```. # Bazel's --build_python_zip replaces our carefully engineered symbolic links. # with copies. This function puts the symbolic links back. function fix_zip_file {. orig_zip_file=$1. # Step 1: Copy the zip file to a temporary place. TMPDIR=$(mktemp -d -t tmp.XXXXXXXXXXX). # The .zip version of the binary doesn't have the header that makes it. # self-executable. We use that version because otherwise unzip would. # complain and raise an error code. cp ""${orig_zip_file}.zip"" ""${TMPDIR}"". ```. Would you be able to give a quick explanation of what the problem is? I understand what it does, but I do not understand why it is needed, or whether it is just for convenience. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/356
https://github.com/google/deepvariant/issues/356:276,security,compl,complete,276,"Hi @gunjanbaid . Unfortunately I am not compiling for x86, but for IBM power, so most of the installation scripts need to be discarded, and packages need to be manually compiled from source using IBM's Advance Toolchain gcc compilers. I have finally gotten all bazel tests to complete as well as the build to complete. I was wondering whether you could explain one piece of the build files though - this is just out of curiosity. In build_release_binaries, there is a function that starts as follows - which seems to be performing a hack to fix something:. ```. # Bazel's --build_python_zip replaces our carefully engineered symbolic links. # with copies. This function puts the symbolic links back. function fix_zip_file {. orig_zip_file=$1. # Step 1: Copy the zip file to a temporary place. TMPDIR=$(mktemp -d -t tmp.XXXXXXXXXXX). # The .zip version of the binary doesn't have the header that makes it. # self-executable. We use that version because otherwise unzip would. # complain and raise an error code. cp ""${orig_zip_file}.zip"" ""${TMPDIR}"". ```. Would you be able to give a quick explanation of what the problem is? I understand what it does, but I do not understand why it is needed, or whether it is just for convenience. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/356
https://github.com/google/deepvariant/issues/356:309,security,compl,complete,309,"Hi @gunjanbaid . Unfortunately I am not compiling for x86, but for IBM power, so most of the installation scripts need to be discarded, and packages need to be manually compiled from source using IBM's Advance Toolchain gcc compilers. I have finally gotten all bazel tests to complete as well as the build to complete. I was wondering whether you could explain one piece of the build files though - this is just out of curiosity. In build_release_binaries, there is a function that starts as follows - which seems to be performing a hack to fix something:. ```. # Bazel's --build_python_zip replaces our carefully engineered symbolic links. # with copies. This function puts the symbolic links back. function fix_zip_file {. orig_zip_file=$1. # Step 1: Copy the zip file to a temporary place. TMPDIR=$(mktemp -d -t tmp.XXXXXXXXXXX). # The .zip version of the binary doesn't have the header that makes it. # self-executable. We use that version because otherwise unzip would. # complain and raise an error code. cp ""${orig_zip_file}.zip"" ""${TMPDIR}"". ```. Would you be able to give a quick explanation of what the problem is? I understand what it does, but I do not understand why it is needed, or whether it is just for convenience. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/356
https://github.com/google/deepvariant/issues/356:533,security,hack,hack,533,"Hi @gunjanbaid . Unfortunately I am not compiling for x86, but for IBM power, so most of the installation scripts need to be discarded, and packages need to be manually compiled from source using IBM's Advance Toolchain gcc compilers. I have finally gotten all bazel tests to complete as well as the build to complete. I was wondering whether you could explain one piece of the build files though - this is just out of curiosity. In build_release_binaries, there is a function that starts as follows - which seems to be performing a hack to fix something:. ```. # Bazel's --build_python_zip replaces our carefully engineered symbolic links. # with copies. This function puts the symbolic links back. function fix_zip_file {. orig_zip_file=$1. # Step 1: Copy the zip file to a temporary place. TMPDIR=$(mktemp -d -t tmp.XXXXXXXXXXX). # The .zip version of the binary doesn't have the header that makes it. # self-executable. We use that version because otherwise unzip would. # complain and raise an error code. cp ""${orig_zip_file}.zip"" ""${TMPDIR}"". ```. Would you be able to give a quick explanation of what the problem is? I understand what it does, but I do not understand why it is needed, or whether it is just for convenience. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/356
https://github.com/google/deepvariant/issues/356:977,security,compl,complain,977,"Hi @gunjanbaid . Unfortunately I am not compiling for x86, but for IBM power, so most of the installation scripts need to be discarded, and packages need to be manually compiled from source using IBM's Advance Toolchain gcc compilers. I have finally gotten all bazel tests to complete as well as the build to complete. I was wondering whether you could explain one piece of the build files though - this is just out of curiosity. In build_release_binaries, there is a function that starts as follows - which seems to be performing a hack to fix something:. ```. # Bazel's --build_python_zip replaces our carefully engineered symbolic links. # with copies. This function puts the symbolic links back. function fix_zip_file {. orig_zip_file=$1. # Step 1: Copy the zip file to a temporary place. TMPDIR=$(mktemp -d -t tmp.XXXXXXXXXXX). # The .zip version of the binary doesn't have the header that makes it. # self-executable. We use that version because otherwise unzip would. # complain and raise an error code. cp ""${orig_zip_file}.zip"" ""${TMPDIR}"". ```. Would you be able to give a quick explanation of what the problem is? I understand what it does, but I do not understand why it is needed, or whether it is just for convenience. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/356
https://github.com/google/deepvariant/issues/356:267,testability,test,tests,267,"Hi @gunjanbaid . Unfortunately I am not compiling for x86, but for IBM power, so most of the installation scripts need to be discarded, and packages need to be manually compiled from source using IBM's Advance Toolchain gcc compilers. I have finally gotten all bazel tests to complete as well as the build to complete. I was wondering whether you could explain one piece of the build files though - this is just out of curiosity. In build_release_binaries, there is a function that starts as follows - which seems to be performing a hack to fix something:. ```. # Bazel's --build_python_zip replaces our carefully engineered symbolic links. # with copies. This function puts the symbolic links back. function fix_zip_file {. orig_zip_file=$1. # Step 1: Copy the zip file to a temporary place. TMPDIR=$(mktemp -d -t tmp.XXXXXXXXXXX). # The .zip version of the binary doesn't have the header that makes it. # self-executable. We use that version because otherwise unzip would. # complain and raise an error code. cp ""${orig_zip_file}.zip"" ""${TMPDIR}"". ```. Would you be able to give a quick explanation of what the problem is? I understand what it does, but I do not understand why it is needed, or whether it is just for convenience. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/356
https://github.com/google/deepvariant/issues/356:1127,testability,understand,understand,1127,"Hi @gunjanbaid . Unfortunately I am not compiling for x86, but for IBM power, so most of the installation scripts need to be discarded, and packages need to be manually compiled from source using IBM's Advance Toolchain gcc compilers. I have finally gotten all bazel tests to complete as well as the build to complete. I was wondering whether you could explain one piece of the build files though - this is just out of curiosity. In build_release_binaries, there is a function that starts as follows - which seems to be performing a hack to fix something:. ```. # Bazel's --build_python_zip replaces our carefully engineered symbolic links. # with copies. This function puts the symbolic links back. function fix_zip_file {. orig_zip_file=$1. # Step 1: Copy the zip file to a temporary place. TMPDIR=$(mktemp -d -t tmp.XXXXXXXXXXX). # The .zip version of the binary doesn't have the header that makes it. # self-executable. We use that version because otherwise unzip would. # complain and raise an error code. cp ""${orig_zip_file}.zip"" ""${TMPDIR}"". ```. Would you be able to give a quick explanation of what the problem is? I understand what it does, but I do not understand why it is needed, or whether it is just for convenience. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/356
https://github.com/google/deepvariant/issues/356:1165,testability,understand,understand,1165,"Hi @gunjanbaid . Unfortunately I am not compiling for x86, but for IBM power, so most of the installation scripts need to be discarded, and packages need to be manually compiled from source using IBM's Advance Toolchain gcc compilers. I have finally gotten all bazel tests to complete as well as the build to complete. I was wondering whether you could explain one piece of the build files though - this is just out of curiosity. In build_release_binaries, there is a function that starts as follows - which seems to be performing a hack to fix something:. ```. # Bazel's --build_python_zip replaces our carefully engineered symbolic links. # with copies. This function puts the symbolic links back. function fix_zip_file {. orig_zip_file=$1. # Step 1: Copy the zip file to a temporary place. TMPDIR=$(mktemp -d -t tmp.XXXXXXXXXXX). # The .zip version of the binary doesn't have the header that makes it. # self-executable. We use that version because otherwise unzip would. # complain and raise an error code. cp ""${orig_zip_file}.zip"" ""${TMPDIR}"". ```. Would you be able to give a quick explanation of what the problem is? I understand what it does, but I do not understand why it is needed, or whether it is just for convenience. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/356
https://github.com/google/deepvariant/issues/356:210,usability,Tool,Toolchain,210,"Hi @gunjanbaid . Unfortunately I am not compiling for x86, but for IBM power, so most of the installation scripts need to be discarded, and packages need to be manually compiled from source using IBM's Advance Toolchain gcc compilers. I have finally gotten all bazel tests to complete as well as the build to complete. I was wondering whether you could explain one piece of the build files though - this is just out of curiosity. In build_release_binaries, there is a function that starts as follows - which seems to be performing a hack to fix something:. ```. # Bazel's --build_python_zip replaces our carefully engineered symbolic links. # with copies. This function puts the symbolic links back. function fix_zip_file {. orig_zip_file=$1. # Step 1: Copy the zip file to a temporary place. TMPDIR=$(mktemp -d -t tmp.XXXXXXXXXXX). # The .zip version of the binary doesn't have the header that makes it. # self-executable. We use that version because otherwise unzip would. # complain and raise an error code. cp ""${orig_zip_file}.zip"" ""${TMPDIR}"". ```. Would you be able to give a quick explanation of what the problem is? I understand what it does, but I do not understand why it is needed, or whether it is just for convenience. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/356
https://github.com/google/deepvariant/issues/356:520,usability,perform,performing,520,"Hi @gunjanbaid . Unfortunately I am not compiling for x86, but for IBM power, so most of the installation scripts need to be discarded, and packages need to be manually compiled from source using IBM's Advance Toolchain gcc compilers. I have finally gotten all bazel tests to complete as well as the build to complete. I was wondering whether you could explain one piece of the build files though - this is just out of curiosity. In build_release_binaries, there is a function that starts as follows - which seems to be performing a hack to fix something:. ```. # Bazel's --build_python_zip replaces our carefully engineered symbolic links. # with copies. This function puts the symbolic links back. function fix_zip_file {. orig_zip_file=$1. # Step 1: Copy the zip file to a temporary place. TMPDIR=$(mktemp -d -t tmp.XXXXXXXXXXX). # The .zip version of the binary doesn't have the header that makes it. # self-executable. We use that version because otherwise unzip would. # complain and raise an error code. cp ""${orig_zip_file}.zip"" ""${TMPDIR}"". ```. Would you be able to give a quick explanation of what the problem is? I understand what it does, but I do not understand why it is needed, or whether it is just for convenience. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/356
https://github.com/google/deepvariant/issues/356:999,usability,error,error,999,"Hi @gunjanbaid . Unfortunately I am not compiling for x86, but for IBM power, so most of the installation scripts need to be discarded, and packages need to be manually compiled from source using IBM's Advance Toolchain gcc compilers. I have finally gotten all bazel tests to complete as well as the build to complete. I was wondering whether you could explain one piece of the build files though - this is just out of curiosity. In build_release_binaries, there is a function that starts as follows - which seems to be performing a hack to fix something:. ```. # Bazel's --build_python_zip replaces our carefully engineered symbolic links. # with copies. This function puts the symbolic links back. function fix_zip_file {. orig_zip_file=$1. # Step 1: Copy the zip file to a temporary place. TMPDIR=$(mktemp -d -t tmp.XXXXXXXXXXX). # The .zip version of the binary doesn't have the header that makes it. # self-executable. We use that version because otherwise unzip would. # complain and raise an error code. cp ""${orig_zip_file}.zip"" ""${TMPDIR}"". ```. Would you be able to give a quick explanation of what the problem is? I understand what it does, but I do not understand why it is needed, or whether it is just for convenience. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/356
https://github.com/google/deepvariant/issues/357:189,energy efficiency,cloud,cloud,189,"Hi @leorippel, DeepVariant has previously been applied to plant species. In the case of rice, there was good evidence of high accuracy. You can see [some results in this blog post](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant). However, these rice genomes were diploid and with a similar variant density of humans. DeepVariant is currently written to be a diploid variant caller. So if the plant species you are working with is polyploid, it is not yet clear how DeepVariant will perform. That said, I am also not sure how other variant callers perform on polyploid samples. It would be possible to train DeepVariant models for a specific genome, but this would require a gold set for the training. We have a previous example of this in mosquitos [in this blog post](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/357
https://github.com/google/deepvariant/issues/357:396,energy efficiency,current,currently,396,"Hi @leorippel, DeepVariant has previously been applied to plant species. In the case of rice, there was good evidence of high accuracy. You can see [some results in this blog post](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant). However, these rice genomes were diploid and with a similar variant density of humans. DeepVariant is currently written to be a diploid variant caller. So if the plant species you are working with is polyploid, it is not yet clear how DeepVariant will perform. That said, I am also not sure how other variant callers perform on polyploid samples. It would be possible to train DeepVariant models for a specific genome, but this would require a gold set for the training. We have a previous example of this in mosquitos [in this blog post](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/357
https://github.com/google/deepvariant/issues/357:683,energy efficiency,model,models,683,"Hi @leorippel, DeepVariant has previously been applied to plant species. In the case of rice, there was good evidence of high accuracy. You can see [some results in this blog post](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant). However, these rice genomes were diploid and with a similar variant density of humans. DeepVariant is currently written to be a diploid variant caller. So if the plant species you are working with is polyploid, it is not yet clear how DeepVariant will perform. That said, I am also not sure how other variant callers perform on polyploid samples. It would be possible to train DeepVariant models for a specific genome, but this would require a gold set for the training. We have a previous example of this in mosquitos [in this blog post](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/357
https://github.com/google/deepvariant/issues/357:957,energy efficiency,model,models,957,"Hi @leorippel, DeepVariant has previously been applied to plant species. In the case of rice, there was good evidence of high accuracy. You can see [some results in this blog post](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant). However, these rice genomes were diploid and with a similar variant density of humans. DeepVariant is currently written to be a diploid variant caller. So if the plant species you are working with is polyploid, it is not yet clear how DeepVariant will perform. That said, I am also not sure how other variant callers perform on polyploid samples. It would be possible to train DeepVariant models for a specific genome, but this would require a gold set for the training. We have a previous example of this in mosquitos [in this blog post](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/357
https://github.com/google/deepvariant/issues/357:696,interoperability,specif,specific,696,"Hi @leorippel, DeepVariant has previously been applied to plant species. In the case of rice, there was good evidence of high accuracy. You can see [some results in this blog post](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant). However, these rice genomes were diploid and with a similar variant density of humans. DeepVariant is currently written to be a diploid variant caller. So if the plant species you are working with is polyploid, it is not yet clear how DeepVariant will perform. That said, I am also not sure how other variant callers perform on polyploid samples. It would be possible to train DeepVariant models for a specific genome, but this would require a gold set for the training. We have a previous example of this in mosquitos [in this blog post](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/357
https://github.com/google/deepvariant/issues/357:936,interoperability,specif,specific-deepvariant-models,936,"Hi @leorippel, DeepVariant has previously been applied to plant species. In the case of rice, there was good evidence of high accuracy. You can see [some results in this blog post](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant). However, these rice genomes were diploid and with a similar variant density of humans. DeepVariant is currently written to be a diploid variant caller. So if the plant species you are working with is polyploid, it is not yet clear how DeepVariant will perform. That said, I am also not sure how other variant callers perform on polyploid samples. It would be possible to train DeepVariant models for a specific genome, but this would require a gold set for the training. We have a previous example of this in mosquitos [in this blog post](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/357
https://github.com/google/deepvariant/issues/357:546,performance,perform,perform,546,"Hi @leorippel, DeepVariant has previously been applied to plant species. In the case of rice, there was good evidence of high accuracy. You can see [some results in this blog post](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant). However, these rice genomes were diploid and with a similar variant density of humans. DeepVariant is currently written to be a diploid variant caller. So if the plant species you are working with is polyploid, it is not yet clear how DeepVariant will perform. That said, I am also not sure how other variant callers perform on polyploid samples. It would be possible to train DeepVariant models for a specific genome, but this would require a gold set for the training. We have a previous example of this in mosquitos [in this blog post](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/357
https://github.com/google/deepvariant/issues/357:611,performance,perform,perform,611,"Hi @leorippel, DeepVariant has previously been applied to plant species. In the case of rice, there was good evidence of high accuracy. You can see [some results in this blog post](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant). However, these rice genomes were diploid and with a similar variant density of humans. DeepVariant is currently written to be a diploid variant caller. So if the plant species you are working with is polyploid, it is not yet clear how DeepVariant will perform. That said, I am also not sure how other variant callers perform on polyploid samples. It would be possible to train DeepVariant models for a specific genome, but this would require a gold set for the training. We have a previous example of this in mosquitos [in this blog post](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/357
https://github.com/google/deepvariant/issues/357:683,security,model,models,683,"Hi @leorippel, DeepVariant has previously been applied to plant species. In the case of rice, there was good evidence of high accuracy. You can see [some results in this blog post](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant). However, these rice genomes were diploid and with a similar variant density of humans. DeepVariant is currently written to be a diploid variant caller. So if the plant species you are working with is polyploid, it is not yet clear how DeepVariant will perform. That said, I am also not sure how other variant callers perform on polyploid samples. It would be possible to train DeepVariant models for a specific genome, but this would require a gold set for the training. We have a previous example of this in mosquitos [in this blog post](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/357
https://github.com/google/deepvariant/issues/357:957,security,model,models,957,"Hi @leorippel, DeepVariant has previously been applied to plant species. In the case of rice, there was good evidence of high accuracy. You can see [some results in this blog post](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant). However, these rice genomes were diploid and with a similar variant density of humans. DeepVariant is currently written to be a diploid variant caller. So if the plant species you are working with is polyploid, it is not yet clear how DeepVariant will perform. That said, I am also not sure how other variant callers perform on polyploid samples. It would be possible to train DeepVariant models for a specific genome, but this would require a gold set for the training. We have a previous example of this in mosquitos [in this blog post](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/357
https://github.com/google/deepvariant/issues/357:58,testability,plan,plant,58,"Hi @leorippel, DeepVariant has previously been applied to plant species. In the case of rice, there was good evidence of high accuracy. You can see [some results in this blog post](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant). However, these rice genomes were diploid and with a similar variant density of humans. DeepVariant is currently written to be a diploid variant caller. So if the plant species you are working with is polyploid, it is not yet clear how DeepVariant will perform. That said, I am also not sure how other variant callers perform on polyploid samples. It would be possible to train DeepVariant models for a specific genome, but this would require a gold set for the training. We have a previous example of this in mosquitos [in this blog post](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/357
https://github.com/google/deepvariant/issues/357:456,testability,plan,plant,456,"Hi @leorippel, DeepVariant has previously been applied to plant species. In the case of rice, there was good evidence of high accuracy. You can see [some results in this blog post](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant). However, these rice genomes were diploid and with a similar variant density of humans. DeepVariant is currently written to be a diploid variant caller. So if the plant species you are working with is polyploid, it is not yet clear how DeepVariant will perform. That said, I am also not sure how other variant callers perform on polyploid samples. It would be possible to train DeepVariant models for a specific genome, but this would require a gold set for the training. We have a previous example of this in mosquitos [in this blog post](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/357
https://github.com/google/deepvariant/issues/357:519,usability,clear,clear,519,"Hi @leorippel, DeepVariant has previously been applied to plant species. In the case of rice, there was good evidence of high accuracy. You can see [some results in this blog post](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant). However, these rice genomes were diploid and with a similar variant density of humans. DeepVariant is currently written to be a diploid variant caller. So if the plant species you are working with is polyploid, it is not yet clear how DeepVariant will perform. That said, I am also not sure how other variant callers perform on polyploid samples. It would be possible to train DeepVariant models for a specific genome, but this would require a gold set for the training. We have a previous example of this in mosquitos [in this blog post](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/357
https://github.com/google/deepvariant/issues/357:546,usability,perform,perform,546,"Hi @leorippel, DeepVariant has previously been applied to plant species. In the case of rice, there was good evidence of high accuracy. You can see [some results in this blog post](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant). However, these rice genomes were diploid and with a similar variant density of humans. DeepVariant is currently written to be a diploid variant caller. So if the plant species you are working with is polyploid, it is not yet clear how DeepVariant will perform. That said, I am also not sure how other variant callers perform on polyploid samples. It would be possible to train DeepVariant models for a specific genome, but this would require a gold set for the training. We have a previous example of this in mosquitos [in this blog post](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/357
https://github.com/google/deepvariant/issues/357:611,usability,perform,perform,611,"Hi @leorippel, DeepVariant has previously been applied to plant species. In the case of rice, there was good evidence of high accuracy. You can see [some results in this blog post](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant). However, these rice genomes were diploid and with a similar variant density of humans. DeepVariant is currently written to be a diploid variant caller. So if the plant species you are working with is polyploid, it is not yet clear how DeepVariant will perform. That said, I am also not sure how other variant callers perform on polyploid samples. It would be possible to train DeepVariant models for a specific genome, but this would require a gold set for the training. We have a previous example of this in mosquitos [in this blog post](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/357
https://github.com/google/deepvariant/issues/357:113,availability,robust,robust,113,"Hi! I read both of blog posts, and I could see that even the primary goal is working with human data, DV is very robust. My specie is diploid, so Iguess it helps, and the variant density should no be too different from humam or rice. . I Actually have two datasets; one is the megagametophyte (1n) of the same individuals (2n). Unfortunatelly, at this point, I don't have a gold dataset to train the data. I'm trying to get at least a truth set to help me in this quest. . Thanks for enlightening.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/357
https://github.com/google/deepvariant/issues/357:113,reliability,robust,robust,113,"Hi! I read both of blog posts, and I could see that even the primary goal is working with human data, DV is very robust. My specie is diploid, so Iguess it helps, and the variant density should no be too different from humam or rice. . I Actually have two datasets; one is the megagametophyte (1n) of the same individuals (2n). Unfortunatelly, at this point, I don't have a gold dataset to train the data. I'm trying to get at least a truth set to help me in this quest. . Thanks for enlightening.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/357
https://github.com/google/deepvariant/issues/357:113,safety,robust,robust,113,"Hi! I read both of blog posts, and I could see that even the primary goal is working with human data, DV is very robust. My specie is diploid, so Iguess it helps, and the variant density should no be too different from humam or rice. . I Actually have two datasets; one is the megagametophyte (1n) of the same individuals (2n). Unfortunatelly, at this point, I don't have a gold dataset to train the data. I'm trying to get at least a truth set to help me in this quest. . Thanks for enlightening.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/357
https://github.com/google/deepvariant/issues/357:156,usability,help,helps,156,"Hi! I read both of blog posts, and I could see that even the primary goal is working with human data, DV is very robust. My specie is diploid, so Iguess it helps, and the variant density should no be too different from humam or rice. . I Actually have two datasets; one is the megagametophyte (1n) of the same individuals (2n). Unfortunatelly, at this point, I don't have a gold dataset to train the data. I'm trying to get at least a truth set to help me in this quest. . Thanks for enlightening.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/357
https://github.com/google/deepvariant/issues/357:448,usability,help,help,448,"Hi! I read both of blog posts, and I could see that even the primary goal is working with human data, DV is very robust. My specie is diploid, so Iguess it helps, and the variant density should no be too different from humam or rice. . I Actually have two datasets; one is the megagametophyte (1n) of the same individuals (2n). Unfortunatelly, at this point, I don't have a gold dataset to train the data. I'm trying to get at least a truth set to help me in this quest. . Thanks for enlightening.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/357
https://github.com/google/deepvariant/issues/357:111,deployability,releas,released,111,"@leorippel since your data is diploid and of similar variant density to human data, you could try applying the released DeepVariant models. That said, we can't be sure how well the models will perform on this data. I'll close this issue for now, but feel free to reopen if you have any other questions. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/357
https://github.com/google/deepvariant/issues/357:132,energy efficiency,model,models,132,"@leorippel since your data is diploid and of similar variant density to human data, you could try applying the released DeepVariant models. That said, we can't be sure how well the models will perform on this data. I'll close this issue for now, but feel free to reopen if you have any other questions. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/357
https://github.com/google/deepvariant/issues/357:181,energy efficiency,model,models,181,"@leorippel since your data is diploid and of similar variant density to human data, you could try applying the released DeepVariant models. That said, we can't be sure how well the models will perform on this data. I'll close this issue for now, but feel free to reopen if you have any other questions. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/357
https://github.com/google/deepvariant/issues/357:193,performance,perform,perform,193,"@leorippel since your data is diploid and of similar variant density to human data, you could try applying the released DeepVariant models. That said, we can't be sure how well the models will perform on this data. I'll close this issue for now, but feel free to reopen if you have any other questions. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/357
https://github.com/google/deepvariant/issues/357:132,security,model,models,132,"@leorippel since your data is diploid and of similar variant density to human data, you could try applying the released DeepVariant models. That said, we can't be sure how well the models will perform on this data. I'll close this issue for now, but feel free to reopen if you have any other questions. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/357
https://github.com/google/deepvariant/issues/357:181,security,model,models,181,"@leorippel since your data is diploid and of similar variant density to human data, you could try applying the released DeepVariant models. That said, we can't be sure how well the models will perform on this data. I'll close this issue for now, but feel free to reopen if you have any other questions. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/357
https://github.com/google/deepvariant/issues/357:193,usability,perform,perform,193,"@leorippel since your data is diploid and of similar variant density to human data, you could try applying the released DeepVariant models. That said, we can't be sure how well the models will perform on this data. I'll close this issue for now, but feel free to reopen if you have any other questions. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/357
https://github.com/google/deepvariant/issues/357:220,usability,close,close,220,"@leorippel since your data is diploid and of similar variant density to human data, you could try applying the released DeepVariant models. That said, we can't be sure how well the models will perform on this data. I'll close this issue for now, but feel free to reopen if you have any other questions. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/357
https://github.com/google/deepvariant/issues/358:122,energy efficiency,gpu,gpus,122,"Hi @leorippel, you can pass the desired argument to `run_deepvariant.py` using the below command:. ```. sudo docker run --gpus 1 \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""/input/${REF2}"" \. --reads=""/input/${BAM2}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=30 \. --call_variants_extra_args=""config_string='gpu_options: {allow_growth: True}'"". ```. Specifically, I added: ` --call_variants_extra_args=""config_string='gpu_options: {allow_growth: True}'""` . Does adding this fix the memory issue?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:226,energy efficiency,gpu,gpu,226,"Hi @leorippel, you can pass the desired argument to `run_deepvariant.py` using the below command:. ```. sudo docker run --gpus 1 \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""/input/${REF2}"" \. --reads=""/input/${BAM2}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=30 \. --call_variants_extra_args=""config_string='gpu_options: {allow_growth: True}'"". ```. Specifically, I added: ` --call_variants_extra_args=""config_string='gpu_options: {allow_growth: True}'""` . Does adding this fix the memory issue?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:593,interoperability,Specif,Specifically,593,"Hi @leorippel, you can pass the desired argument to `run_deepvariant.py` using the below command:. ```. sudo docker run --gpus 1 \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""/input/${REF2}"" \. --reads=""/input/${BAM2}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=30 \. --call_variants_extra_args=""config_string='gpu_options: {allow_growth: True}'"". ```. Specifically, I added: ` --call_variants_extra_args=""config_string='gpu_options: {allow_growth: True}'""` . Does adding this fix the memory issue?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:122,performance,gpu,gpus,122,"Hi @leorippel, you can pass the desired argument to `run_deepvariant.py` using the below command:. ```. sudo docker run --gpus 1 \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""/input/${REF2}"" \. --reads=""/input/${BAM2}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=30 \. --call_variants_extra_args=""config_string='gpu_options: {allow_growth: True}'"". ```. Specifically, I added: ` --call_variants_extra_args=""config_string='gpu_options: {allow_growth: True}'""` . Does adding this fix the memory issue?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:226,performance,gpu,gpu,226,"Hi @leorippel, you can pass the desired argument to `run_deepvariant.py` using the below command:. ```. sudo docker run --gpus 1 \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""/input/${REF2}"" \. --reads=""/input/${BAM2}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=30 \. --call_variants_extra_args=""config_string='gpu_options: {allow_growth: True}'"". ```. Specifically, I added: ` --call_variants_extra_args=""config_string='gpu_options: {allow_growth: True}'""` . Does adding this fix the memory issue?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:725,performance,memor,memory,725,"Hi @leorippel, you can pass the desired argument to `run_deepvariant.py` using the below command:. ```. sudo docker run --gpus 1 \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""/input/${REF2}"" \. --reads=""/input/${BAM2}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=30 \. --call_variants_extra_args=""config_string='gpu_options: {allow_growth: True}'"". ```. Specifically, I added: ` --call_variants_extra_args=""config_string='gpu_options: {allow_growth: True}'""` . Does adding this fix the memory issue?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:700,reliability,Doe,Does,700,"Hi @leorippel, you can pass the desired argument to `run_deepvariant.py` using the below command:. ```. sudo docker run --gpus 1 \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""/input/${REF2}"" \. --reads=""/input/${BAM2}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=30 \. --call_variants_extra_args=""config_string='gpu_options: {allow_growth: True}'"". ```. Specifically, I added: ` --call_variants_extra_args=""config_string='gpu_options: {allow_growth: True}'""` . Does adding this fix the memory issue?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:151,safety,input,input,151,"Hi @leorippel, you can pass the desired argument to `run_deepvariant.py` using the below command:. ```. sudo docker run --gpus 1 \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""/input/${REF2}"" \. --reads=""/input/${BAM2}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=30 \. --call_variants_extra_args=""config_string='gpu_options: {allow_growth: True}'"". ```. Specifically, I added: ` --call_variants_extra_args=""config_string='gpu_options: {allow_growth: True}'""` . Does adding this fix the memory issue?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:302,safety,input,input,302,"Hi @leorippel, you can pass the desired argument to `run_deepvariant.py` using the below command:. ```. sudo docker run --gpus 1 \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""/input/${REF2}"" \. --reads=""/input/${BAM2}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=30 \. --call_variants_extra_args=""config_string='gpu_options: {allow_growth: True}'"". ```. Specifically, I added: ` --call_variants_extra_args=""config_string='gpu_options: {allow_growth: True}'""` . Does adding this fix the memory issue?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:330,safety,input,input,330,"Hi @leorippel, you can pass the desired argument to `run_deepvariant.py` using the below command:. ```. sudo docker run --gpus 1 \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""/input/${REF2}"" \. --reads=""/input/${BAM2}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=30 \. --call_variants_extra_args=""config_string='gpu_options: {allow_growth: True}'"". ```. Specifically, I added: ` --call_variants_extra_args=""config_string='gpu_options: {allow_growth: True}'""` . Does adding this fix the memory issue?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:89,usability,command,command,89,"Hi @leorippel, you can pass the desired argument to `run_deepvariant.py` using the below command:. ```. sudo docker run --gpus 1 \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""/input/${REF2}"" \. --reads=""/input/${BAM2}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=30 \. --call_variants_extra_args=""config_string='gpu_options: {allow_growth: True}'"". ```. Specifically, I added: ` --call_variants_extra_args=""config_string='gpu_options: {allow_growth: True}'""` . Does adding this fix the memory issue?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:151,usability,input,input,151,"Hi @leorippel, you can pass the desired argument to `run_deepvariant.py` using the below command:. ```. sudo docker run --gpus 1 \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""/input/${REF2}"" \. --reads=""/input/${BAM2}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=30 \. --call_variants_extra_args=""config_string='gpu_options: {allow_growth: True}'"". ```. Specifically, I added: ` --call_variants_extra_args=""config_string='gpu_options: {allow_growth: True}'""` . Does adding this fix the memory issue?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:302,usability,input,input,302,"Hi @leorippel, you can pass the desired argument to `run_deepvariant.py` using the below command:. ```. sudo docker run --gpus 1 \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""/input/${REF2}"" \. --reads=""/input/${BAM2}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=30 \. --call_variants_extra_args=""config_string='gpu_options: {allow_growth: True}'"". ```. Specifically, I added: ` --call_variants_extra_args=""config_string='gpu_options: {allow_growth: True}'""` . Does adding this fix the memory issue?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:330,usability,input,input,330,"Hi @leorippel, you can pass the desired argument to `run_deepvariant.py` using the below command:. ```. sudo docker run --gpus 1 \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""/input/${REF2}"" \. --reads=""/input/${BAM2}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=30 \. --call_variants_extra_args=""config_string='gpu_options: {allow_growth: True}'"". ```. Specifically, I added: ` --call_variants_extra_args=""config_string='gpu_options: {allow_growth: True}'""` . Does adding this fix the memory issue?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:725,usability,memor,memory,725,"Hi @leorippel, you can pass the desired argument to `run_deepvariant.py` using the below command:. ```. sudo docker run --gpus 1 \. -v ""${DATA_DIR}"":""/input"" \. -v ""${OUTPUT_DIR}:/output"" \. google/deepvariant:""${BIN_VERSION}-gpu"" \. /opt/deepvariant/bin/run_deepvariant \. --model_type=WGS \. --ref=""/input/${REF2}"" \. --reads=""/input/${BAM2}"" \. --output_vcf=/output/${OUTPUT_VCF} \. --output_gvcf=/output/${OUTPUT_GVCF} \. --intermediate_results_dir /output/intermediate_results_dir \. --num_shards=30 \. --call_variants_extra_args=""config_string='gpu_options: {allow_growth: True}'"". ```. Specifically, I added: ` --call_variants_extra_args=""config_string='gpu_options: {allow_growth: True}'""` . Does adding this fix the memory issue?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:48,energy efficiency,CPU,CPU,48,"Hi @gunjanbaid, . I'm running one individual on CPU mode, as sson this run finishes, I will try. Thanks for the help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:48,performance,CPU,CPU,48,"Hi @gunjanbaid, . I'm running one individual on CPU mode, as sson this run finishes, I will try. Thanks for the help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:112,usability,help,help,112,"Hi @gunjanbaid, . I'm running one individual on CPU mode, as sson this run finishes, I will try. Thanks for the help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:9849,availability,avail,available,9849,f-00030.gz. -rw-r--r-- 1 root root 52K Sep 24 15:47 make_examples.tfrecord-00000-of-00030.gz. -rw-r--r-- 1 root root 5.9K Sep 24 15:47 gvcf.tfrecord-00003-of-00030.gz. -rw-r--r-- 1 root root 5.7K Sep 24 15:47 gvcf.tfrecord-00004-of-00030.gz. -rw-r--r-- 1 root root 3.6K Sep 24 15:47 gvcf.tfrecord-00022-of-00030.gz. -rw-r--r-- 1 root root 4.5K Sep 24 15:47 gvcf.tfrecord-00025-of-00030.gz. -rw-r--r-- 1 root root 22K Sep 24 15:47 make_examples.tfrecord-00004-of-00030.gz. -rw-r--r-- 1 root root 31K Sep 24 15:47 make_examples.tfrecord-00003-of-00030.gz. -rw-r--r-- 1 root root 16K Sep 24 15:47 make_examples.tfrecord-00022-of-00030.gz. -rw-r--r-- 1 root root 6.2K Sep 24 15:47 gvcf.tfrecord-00008-of-00030.gz. -rw-r--r-- 1 root root 56K Sep 24 15:47 make_examples.tfrecord-00008-of-00030.gz. -rw-r--r-- 1 root root 7.5K Sep 24 15:47 make_examples.tfrecord-00025-of-00030.gz. -rw-r--r-- 1 root root 4.6K Sep 24 15:47 gvcf.tfrecord-00026-of-00030.gz. -rw-r--r-- 1 root root 38K Sep 24 15:47 make_examples.tfrecord-00026-of-00030.gz. -rw-r--r-- 1 root root 5.2K Sep 24 15:47 gvcf.tfrecord-00002-of-00030.gz. -rw-r--r-- 1 root root 32K Sep 24 15:47 make_examples.tfrecord-00002-of-00030.gz. -rw-r--r-- 1 root root 7.0K Sep 24 15:47 gvcf.tfrecord-00007-of-00030.gz. -rw-r--r-- 1 root root 3.0K Sep 24 15:47 gvcf.tfrecord-00024-of-00030.gz. -rw-r--r-- 1 root root 44K Sep 24 15:47 make_examples.tfrecord-00007-of-00030.gz. -rw-r--r-- 1 root root 11K Sep 24 15:47 make_examples.tfrecord-00024-of-00030.gz. -rw-r--r-- 1 root root 5.3K Sep 24 15:47 gvcf.tfrecord-00013-of-00030.gz. -rw-r--r-- 1 root root 27K Sep 24 15:47 make_examples.tfrecord-00013-of-00030.gz. -rw-r--r-- 1 root root 4.9K Sep 24 15:47 gvcf.tfrecord-00005-of-00030.gz. -rw-r--r-- 1 root root 20K Sep 24 15:47 make_examples.tfrecord-00005-of-00030.gz. ```. This is how my `free -h` is looking like right now:. ```. total used free shared buff/cache available. Mem: 125G 123G 776M 224M 933M 354M. Swap: 101G 100G 1.7G. ```. Thank you. Cheers.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:868,deployability,manag,manage,868,"Hi @gunjanbaid,. I came across anothe memmory problem, not to GPU, but the CPU. The postprocess_variants.py is consuming a ridiculous amount of memory: . This is how I'm runing: . `sudo docker run --memory=""100g"" --memory-swap=""101g"" --oom-kill-disable -v ""${DATA_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/postprocess_variants --ref ""/input/10consensus.fasta"" --infile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/output/M10.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/output/intermediate_results_dir/gvcf.tfrecord@25.gz"" --gvcf_outfile ""/output/M10.output.g.vcf.gz"". `. And the process is consuming all the designed memory the is autmatic killed. When I set the flag ` --oom-kill-disable ` get stucked on `D - uninterruptible sleep `. There is any argument I can use to manage this? I understand that the makeexamples should consume the most of the memory, but something else is happening. . This is how my deepvariant-run/output/intermediate_results_dir/ is looking . ```. -rw-r--r-- 1 root root 7.3G Oct 5 06:53 call_variants_output.tfrecord.gz. -rw-r--r-- 1 root root 577M Sep 29 23:42 gvcf.tfrecord-00004-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:42 make_examples.tfrecord-00004-of-00025.gz. -rw-r--r-- 1 root root 576M Sep 29 23:33 gvcf.tfrecord-00020-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:33 make_examples.tfrecord-00020-of-00025.gz. -rw-r--r-- 1 root root 576M Sep 29 23:33 gvcf.tfrecord-00017-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:33 make_examples.tfrecord-00017-of-00025.gz. -rw-r--r-- 1 root root 575M Sep 29 23:33 gvcf.tfrecord-00000-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:33 make_examples.tfrecord-00000-of-00025.gz. -rw-r--r-- 1 root root 577M Sep 29 23:32 gvcf.tfrecord-00009-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:32 make_examples.tfrecord-00009-of-00025.gz. -rw-r--r-- 1 root root 576M Sep 29 23:31 gvcf.tfrecord-00003-of-00025",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:62,energy efficiency,GPU,GPU,62,"Hi @gunjanbaid,. I came across anothe memmory problem, not to GPU, but the CPU. The postprocess_variants.py is consuming a ridiculous amount of memory: . This is how I'm runing: . `sudo docker run --memory=""100g"" --memory-swap=""101g"" --oom-kill-disable -v ""${DATA_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/postprocess_variants --ref ""/input/10consensus.fasta"" --infile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/output/M10.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/output/intermediate_results_dir/gvcf.tfrecord@25.gz"" --gvcf_outfile ""/output/M10.output.g.vcf.gz"". `. And the process is consuming all the designed memory the is autmatic killed. When I set the flag ` --oom-kill-disable ` get stucked on `D - uninterruptible sleep `. There is any argument I can use to manage this? I understand that the makeexamples should consume the most of the memory, but something else is happening. . This is how my deepvariant-run/output/intermediate_results_dir/ is looking . ```. -rw-r--r-- 1 root root 7.3G Oct 5 06:53 call_variants_output.tfrecord.gz. -rw-r--r-- 1 root root 577M Sep 29 23:42 gvcf.tfrecord-00004-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:42 make_examples.tfrecord-00004-of-00025.gz. -rw-r--r-- 1 root root 576M Sep 29 23:33 gvcf.tfrecord-00020-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:33 make_examples.tfrecord-00020-of-00025.gz. -rw-r--r-- 1 root root 576M Sep 29 23:33 gvcf.tfrecord-00017-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:33 make_examples.tfrecord-00017-of-00025.gz. -rw-r--r-- 1 root root 575M Sep 29 23:33 gvcf.tfrecord-00000-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:33 make_examples.tfrecord-00000-of-00025.gz. -rw-r--r-- 1 root root 577M Sep 29 23:32 gvcf.tfrecord-00009-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:32 make_examples.tfrecord-00009-of-00025.gz. -rw-r--r-- 1 root root 576M Sep 29 23:31 gvcf.tfrecord-00003-of-00025",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:75,energy efficiency,CPU,CPU,75,"Hi @gunjanbaid,. I came across anothe memmory problem, not to GPU, but the CPU. The postprocess_variants.py is consuming a ridiculous amount of memory: . This is how I'm runing: . `sudo docker run --memory=""100g"" --memory-swap=""101g"" --oom-kill-disable -v ""${DATA_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/postprocess_variants --ref ""/input/10consensus.fasta"" --infile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/output/M10.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/output/intermediate_results_dir/gvcf.tfrecord@25.gz"" --gvcf_outfile ""/output/M10.output.g.vcf.gz"". `. And the process is consuming all the designed memory the is autmatic killed. When I set the flag ` --oom-kill-disable ` get stucked on `D - uninterruptible sleep `. There is any argument I can use to manage this? I understand that the makeexamples should consume the most of the memory, but something else is happening. . This is how my deepvariant-run/output/intermediate_results_dir/ is looking . ```. -rw-r--r-- 1 root root 7.3G Oct 5 06:53 call_variants_output.tfrecord.gz. -rw-r--r-- 1 root root 577M Sep 29 23:42 gvcf.tfrecord-00004-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:42 make_examples.tfrecord-00004-of-00025.gz. -rw-r--r-- 1 root root 576M Sep 29 23:33 gvcf.tfrecord-00020-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:33 make_examples.tfrecord-00020-of-00025.gz. -rw-r--r-- 1 root root 576M Sep 29 23:33 gvcf.tfrecord-00017-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:33 make_examples.tfrecord-00017-of-00025.gz. -rw-r--r-- 1 root root 575M Sep 29 23:33 gvcf.tfrecord-00000-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:33 make_examples.tfrecord-00000-of-00025.gz. -rw-r--r-- 1 root root 577M Sep 29 23:32 gvcf.tfrecord-00009-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:32 make_examples.tfrecord-00009-of-00025.gz. -rw-r--r-- 1 root root 576M Sep 29 23:31 gvcf.tfrecord-00003-of-00025",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:868,energy efficiency,manag,manage,868,"Hi @gunjanbaid,. I came across anothe memmory problem, not to GPU, but the CPU. The postprocess_variants.py is consuming a ridiculous amount of memory: . This is how I'm runing: . `sudo docker run --memory=""100g"" --memory-swap=""101g"" --oom-kill-disable -v ""${DATA_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/postprocess_variants --ref ""/input/10consensus.fasta"" --infile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/output/M10.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/output/intermediate_results_dir/gvcf.tfrecord@25.gz"" --gvcf_outfile ""/output/M10.output.g.vcf.gz"". `. And the process is consuming all the designed memory the is autmatic killed. When I set the flag ` --oom-kill-disable ` get stucked on `D - uninterruptible sleep `. There is any argument I can use to manage this? I understand that the makeexamples should consume the most of the memory, but something else is happening. . This is how my deepvariant-run/output/intermediate_results_dir/ is looking . ```. -rw-r--r-- 1 root root 7.3G Oct 5 06:53 call_variants_output.tfrecord.gz. -rw-r--r-- 1 root root 577M Sep 29 23:42 gvcf.tfrecord-00004-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:42 make_examples.tfrecord-00004-of-00025.gz. -rw-r--r-- 1 root root 576M Sep 29 23:33 gvcf.tfrecord-00020-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:33 make_examples.tfrecord-00020-of-00025.gz. -rw-r--r-- 1 root root 576M Sep 29 23:33 gvcf.tfrecord-00017-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:33 make_examples.tfrecord-00017-of-00025.gz. -rw-r--r-- 1 root root 575M Sep 29 23:33 gvcf.tfrecord-00000-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:33 make_examples.tfrecord-00000-of-00025.gz. -rw-r--r-- 1 root root 577M Sep 29 23:32 gvcf.tfrecord-00009-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:32 make_examples.tfrecord-00009-of-00025.gz. -rw-r--r-- 1 root root 576M Sep 29 23:31 gvcf.tfrecord-00003-of-00025",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:9831,interoperability,share,shared,9831,f-00030.gz. -rw-r--r-- 1 root root 52K Sep 24 15:47 make_examples.tfrecord-00000-of-00030.gz. -rw-r--r-- 1 root root 5.9K Sep 24 15:47 gvcf.tfrecord-00003-of-00030.gz. -rw-r--r-- 1 root root 5.7K Sep 24 15:47 gvcf.tfrecord-00004-of-00030.gz. -rw-r--r-- 1 root root 3.6K Sep 24 15:47 gvcf.tfrecord-00022-of-00030.gz. -rw-r--r-- 1 root root 4.5K Sep 24 15:47 gvcf.tfrecord-00025-of-00030.gz. -rw-r--r-- 1 root root 22K Sep 24 15:47 make_examples.tfrecord-00004-of-00030.gz. -rw-r--r-- 1 root root 31K Sep 24 15:47 make_examples.tfrecord-00003-of-00030.gz. -rw-r--r-- 1 root root 16K Sep 24 15:47 make_examples.tfrecord-00022-of-00030.gz. -rw-r--r-- 1 root root 6.2K Sep 24 15:47 gvcf.tfrecord-00008-of-00030.gz. -rw-r--r-- 1 root root 56K Sep 24 15:47 make_examples.tfrecord-00008-of-00030.gz. -rw-r--r-- 1 root root 7.5K Sep 24 15:47 make_examples.tfrecord-00025-of-00030.gz. -rw-r--r-- 1 root root 4.6K Sep 24 15:47 gvcf.tfrecord-00026-of-00030.gz. -rw-r--r-- 1 root root 38K Sep 24 15:47 make_examples.tfrecord-00026-of-00030.gz. -rw-r--r-- 1 root root 5.2K Sep 24 15:47 gvcf.tfrecord-00002-of-00030.gz. -rw-r--r-- 1 root root 32K Sep 24 15:47 make_examples.tfrecord-00002-of-00030.gz. -rw-r--r-- 1 root root 7.0K Sep 24 15:47 gvcf.tfrecord-00007-of-00030.gz. -rw-r--r-- 1 root root 3.0K Sep 24 15:47 gvcf.tfrecord-00024-of-00030.gz. -rw-r--r-- 1 root root 44K Sep 24 15:47 make_examples.tfrecord-00007-of-00030.gz. -rw-r--r-- 1 root root 11K Sep 24 15:47 make_examples.tfrecord-00024-of-00030.gz. -rw-r--r-- 1 root root 5.3K Sep 24 15:47 gvcf.tfrecord-00013-of-00030.gz. -rw-r--r-- 1 root root 27K Sep 24 15:47 make_examples.tfrecord-00013-of-00030.gz. -rw-r--r-- 1 root root 4.9K Sep 24 15:47 gvcf.tfrecord-00005-of-00030.gz. -rw-r--r-- 1 root root 20K Sep 24 15:47 make_examples.tfrecord-00005-of-00030.gz. ```. This is how my `free -h` is looking like right now:. ```. total used free shared buff/cache available. Mem: 125G 123G 776M 224M 933M 354M. Swap: 101G 100G 1.7G. ```. Thank you. Cheers.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:62,performance,GPU,GPU,62,"Hi @gunjanbaid,. I came across anothe memmory problem, not to GPU, but the CPU. The postprocess_variants.py is consuming a ridiculous amount of memory: . This is how I'm runing: . `sudo docker run --memory=""100g"" --memory-swap=""101g"" --oom-kill-disable -v ""${DATA_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/postprocess_variants --ref ""/input/10consensus.fasta"" --infile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/output/M10.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/output/intermediate_results_dir/gvcf.tfrecord@25.gz"" --gvcf_outfile ""/output/M10.output.g.vcf.gz"". `. And the process is consuming all the designed memory the is autmatic killed. When I set the flag ` --oom-kill-disable ` get stucked on `D - uninterruptible sleep `. There is any argument I can use to manage this? I understand that the makeexamples should consume the most of the memory, but something else is happening. . This is how my deepvariant-run/output/intermediate_results_dir/ is looking . ```. -rw-r--r-- 1 root root 7.3G Oct 5 06:53 call_variants_output.tfrecord.gz. -rw-r--r-- 1 root root 577M Sep 29 23:42 gvcf.tfrecord-00004-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:42 make_examples.tfrecord-00004-of-00025.gz. -rw-r--r-- 1 root root 576M Sep 29 23:33 gvcf.tfrecord-00020-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:33 make_examples.tfrecord-00020-of-00025.gz. -rw-r--r-- 1 root root 576M Sep 29 23:33 gvcf.tfrecord-00017-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:33 make_examples.tfrecord-00017-of-00025.gz. -rw-r--r-- 1 root root 575M Sep 29 23:33 gvcf.tfrecord-00000-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:33 make_examples.tfrecord-00000-of-00025.gz. -rw-r--r-- 1 root root 577M Sep 29 23:32 gvcf.tfrecord-00009-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:32 make_examples.tfrecord-00009-of-00025.gz. -rw-r--r-- 1 root root 576M Sep 29 23:31 gvcf.tfrecord-00003-of-00025",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:75,performance,CPU,CPU,75,"Hi @gunjanbaid,. I came across anothe memmory problem, not to GPU, but the CPU. The postprocess_variants.py is consuming a ridiculous amount of memory: . This is how I'm runing: . `sudo docker run --memory=""100g"" --memory-swap=""101g"" --oom-kill-disable -v ""${DATA_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/postprocess_variants --ref ""/input/10consensus.fasta"" --infile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/output/M10.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/output/intermediate_results_dir/gvcf.tfrecord@25.gz"" --gvcf_outfile ""/output/M10.output.g.vcf.gz"". `. And the process is consuming all the designed memory the is autmatic killed. When I set the flag ` --oom-kill-disable ` get stucked on `D - uninterruptible sleep `. There is any argument I can use to manage this? I understand that the makeexamples should consume the most of the memory, but something else is happening. . This is how my deepvariant-run/output/intermediate_results_dir/ is looking . ```. -rw-r--r-- 1 root root 7.3G Oct 5 06:53 call_variants_output.tfrecord.gz. -rw-r--r-- 1 root root 577M Sep 29 23:42 gvcf.tfrecord-00004-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:42 make_examples.tfrecord-00004-of-00025.gz. -rw-r--r-- 1 root root 576M Sep 29 23:33 gvcf.tfrecord-00020-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:33 make_examples.tfrecord-00020-of-00025.gz. -rw-r--r-- 1 root root 576M Sep 29 23:33 gvcf.tfrecord-00017-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:33 make_examples.tfrecord-00017-of-00025.gz. -rw-r--r-- 1 root root 575M Sep 29 23:33 gvcf.tfrecord-00000-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:33 make_examples.tfrecord-00000-of-00025.gz. -rw-r--r-- 1 root root 577M Sep 29 23:32 gvcf.tfrecord-00009-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:32 make_examples.tfrecord-00009-of-00025.gz. -rw-r--r-- 1 root root 576M Sep 29 23:31 gvcf.tfrecord-00003-of-00025",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:144,performance,memor,memory,144,"Hi @gunjanbaid,. I came across anothe memmory problem, not to GPU, but the CPU. The postprocess_variants.py is consuming a ridiculous amount of memory: . This is how I'm runing: . `sudo docker run --memory=""100g"" --memory-swap=""101g"" --oom-kill-disable -v ""${DATA_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/postprocess_variants --ref ""/input/10consensus.fasta"" --infile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/output/M10.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/output/intermediate_results_dir/gvcf.tfrecord@25.gz"" --gvcf_outfile ""/output/M10.output.g.vcf.gz"". `. And the process is consuming all the designed memory the is autmatic killed. When I set the flag ` --oom-kill-disable ` get stucked on `D - uninterruptible sleep `. There is any argument I can use to manage this? I understand that the makeexamples should consume the most of the memory, but something else is happening. . This is how my deepvariant-run/output/intermediate_results_dir/ is looking . ```. -rw-r--r-- 1 root root 7.3G Oct 5 06:53 call_variants_output.tfrecord.gz. -rw-r--r-- 1 root root 577M Sep 29 23:42 gvcf.tfrecord-00004-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:42 make_examples.tfrecord-00004-of-00025.gz. -rw-r--r-- 1 root root 576M Sep 29 23:33 gvcf.tfrecord-00020-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:33 make_examples.tfrecord-00020-of-00025.gz. -rw-r--r-- 1 root root 576M Sep 29 23:33 gvcf.tfrecord-00017-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:33 make_examples.tfrecord-00017-of-00025.gz. -rw-r--r-- 1 root root 575M Sep 29 23:33 gvcf.tfrecord-00000-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:33 make_examples.tfrecord-00000-of-00025.gz. -rw-r--r-- 1 root root 577M Sep 29 23:32 gvcf.tfrecord-00009-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:32 make_examples.tfrecord-00009-of-00025.gz. -rw-r--r-- 1 root root 576M Sep 29 23:31 gvcf.tfrecord-00003-of-00025",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:199,performance,memor,memory,199,"Hi @gunjanbaid,. I came across anothe memmory problem, not to GPU, but the CPU. The postprocess_variants.py is consuming a ridiculous amount of memory: . This is how I'm runing: . `sudo docker run --memory=""100g"" --memory-swap=""101g"" --oom-kill-disable -v ""${DATA_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/postprocess_variants --ref ""/input/10consensus.fasta"" --infile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/output/M10.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/output/intermediate_results_dir/gvcf.tfrecord@25.gz"" --gvcf_outfile ""/output/M10.output.g.vcf.gz"". `. And the process is consuming all the designed memory the is autmatic killed. When I set the flag ` --oom-kill-disable ` get stucked on `D - uninterruptible sleep `. There is any argument I can use to manage this? I understand that the makeexamples should consume the most of the memory, but something else is happening. . This is how my deepvariant-run/output/intermediate_results_dir/ is looking . ```. -rw-r--r-- 1 root root 7.3G Oct 5 06:53 call_variants_output.tfrecord.gz. -rw-r--r-- 1 root root 577M Sep 29 23:42 gvcf.tfrecord-00004-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:42 make_examples.tfrecord-00004-of-00025.gz. -rw-r--r-- 1 root root 576M Sep 29 23:33 gvcf.tfrecord-00020-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:33 make_examples.tfrecord-00020-of-00025.gz. -rw-r--r-- 1 root root 576M Sep 29 23:33 gvcf.tfrecord-00017-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:33 make_examples.tfrecord-00017-of-00025.gz. -rw-r--r-- 1 root root 575M Sep 29 23:33 gvcf.tfrecord-00000-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:33 make_examples.tfrecord-00000-of-00025.gz. -rw-r--r-- 1 root root 577M Sep 29 23:32 gvcf.tfrecord-00009-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:32 make_examples.tfrecord-00009-of-00025.gz. -rw-r--r-- 1 root root 576M Sep 29 23:31 gvcf.tfrecord-00003-of-00025",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:215,performance,memor,memory-swap,215,"Hi @gunjanbaid,. I came across anothe memmory problem, not to GPU, but the CPU. The postprocess_variants.py is consuming a ridiculous amount of memory: . This is how I'm runing: . `sudo docker run --memory=""100g"" --memory-swap=""101g"" --oom-kill-disable -v ""${DATA_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/postprocess_variants --ref ""/input/10consensus.fasta"" --infile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/output/M10.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/output/intermediate_results_dir/gvcf.tfrecord@25.gz"" --gvcf_outfile ""/output/M10.output.g.vcf.gz"". `. And the process is consuming all the designed memory the is autmatic killed. When I set the flag ` --oom-kill-disable ` get stucked on `D - uninterruptible sleep `. There is any argument I can use to manage this? I understand that the makeexamples should consume the most of the memory, but something else is happening. . This is how my deepvariant-run/output/intermediate_results_dir/ is looking . ```. -rw-r--r-- 1 root root 7.3G Oct 5 06:53 call_variants_output.tfrecord.gz. -rw-r--r-- 1 root root 577M Sep 29 23:42 gvcf.tfrecord-00004-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:42 make_examples.tfrecord-00004-of-00025.gz. -rw-r--r-- 1 root root 576M Sep 29 23:33 gvcf.tfrecord-00020-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:33 make_examples.tfrecord-00020-of-00025.gz. -rw-r--r-- 1 root root 576M Sep 29 23:33 gvcf.tfrecord-00017-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:33 make_examples.tfrecord-00017-of-00025.gz. -rw-r--r-- 1 root root 575M Sep 29 23:33 gvcf.tfrecord-00000-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:33 make_examples.tfrecord-00000-of-00025.gz. -rw-r--r-- 1 root root 577M Sep 29 23:32 gvcf.tfrecord-00009-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:32 make_examples.tfrecord-00009-of-00025.gz. -rw-r--r-- 1 root root 576M Sep 29 23:31 gvcf.tfrecord-00003-of-00025",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:714,performance,memor,memory,714,"Hi @gunjanbaid,. I came across anothe memmory problem, not to GPU, but the CPU. The postprocess_variants.py is consuming a ridiculous amount of memory: . This is how I'm runing: . `sudo docker run --memory=""100g"" --memory-swap=""101g"" --oom-kill-disable -v ""${DATA_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/postprocess_variants --ref ""/input/10consensus.fasta"" --infile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/output/M10.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/output/intermediate_results_dir/gvcf.tfrecord@25.gz"" --gvcf_outfile ""/output/M10.output.g.vcf.gz"". `. And the process is consuming all the designed memory the is autmatic killed. When I set the flag ` --oom-kill-disable ` get stucked on `D - uninterruptible sleep `. There is any argument I can use to manage this? I understand that the makeexamples should consume the most of the memory, but something else is happening. . This is how my deepvariant-run/output/intermediate_results_dir/ is looking . ```. -rw-r--r-- 1 root root 7.3G Oct 5 06:53 call_variants_output.tfrecord.gz. -rw-r--r-- 1 root root 577M Sep 29 23:42 gvcf.tfrecord-00004-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:42 make_examples.tfrecord-00004-of-00025.gz. -rw-r--r-- 1 root root 576M Sep 29 23:33 gvcf.tfrecord-00020-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:33 make_examples.tfrecord-00020-of-00025.gz. -rw-r--r-- 1 root root 576M Sep 29 23:33 gvcf.tfrecord-00017-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:33 make_examples.tfrecord-00017-of-00025.gz. -rw-r--r-- 1 root root 575M Sep 29 23:33 gvcf.tfrecord-00000-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:33 make_examples.tfrecord-00000-of-00025.gz. -rw-r--r-- 1 root root 577M Sep 29 23:32 gvcf.tfrecord-00009-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:32 make_examples.tfrecord-00009-of-00025.gz. -rw-r--r-- 1 root root 576M Sep 29 23:31 gvcf.tfrecord-00003-of-00025",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:947,performance,memor,memory,947,"Hi @gunjanbaid,. I came across anothe memmory problem, not to GPU, but the CPU. The postprocess_variants.py is consuming a ridiculous amount of memory: . This is how I'm runing: . `sudo docker run --memory=""100g"" --memory-swap=""101g"" --oom-kill-disable -v ""${DATA_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/postprocess_variants --ref ""/input/10consensus.fasta"" --infile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/output/M10.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/output/intermediate_results_dir/gvcf.tfrecord@25.gz"" --gvcf_outfile ""/output/M10.output.g.vcf.gz"". `. And the process is consuming all the designed memory the is autmatic killed. When I set the flag ` --oom-kill-disable ` get stucked on `D - uninterruptible sleep `. There is any argument I can use to manage this? I understand that the makeexamples should consume the most of the memory, but something else is happening. . This is how my deepvariant-run/output/intermediate_results_dir/ is looking . ```. -rw-r--r-- 1 root root 7.3G Oct 5 06:53 call_variants_output.tfrecord.gz. -rw-r--r-- 1 root root 577M Sep 29 23:42 gvcf.tfrecord-00004-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:42 make_examples.tfrecord-00004-of-00025.gz. -rw-r--r-- 1 root root 576M Sep 29 23:33 gvcf.tfrecord-00020-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:33 make_examples.tfrecord-00020-of-00025.gz. -rw-r--r-- 1 root root 576M Sep 29 23:33 gvcf.tfrecord-00017-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:33 make_examples.tfrecord-00017-of-00025.gz. -rw-r--r-- 1 root root 575M Sep 29 23:33 gvcf.tfrecord-00000-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:33 make_examples.tfrecord-00000-of-00025.gz. -rw-r--r-- 1 root root 577M Sep 29 23:32 gvcf.tfrecord-00009-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:32 make_examples.tfrecord-00009-of-00025.gz. -rw-r--r-- 1 root root 576M Sep 29 23:31 gvcf.tfrecord-00003-of-00025",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:9843,performance,cach,cache,9843,f-00030.gz. -rw-r--r-- 1 root root 52K Sep 24 15:47 make_examples.tfrecord-00000-of-00030.gz. -rw-r--r-- 1 root root 5.9K Sep 24 15:47 gvcf.tfrecord-00003-of-00030.gz. -rw-r--r-- 1 root root 5.7K Sep 24 15:47 gvcf.tfrecord-00004-of-00030.gz. -rw-r--r-- 1 root root 3.6K Sep 24 15:47 gvcf.tfrecord-00022-of-00030.gz. -rw-r--r-- 1 root root 4.5K Sep 24 15:47 gvcf.tfrecord-00025-of-00030.gz. -rw-r--r-- 1 root root 22K Sep 24 15:47 make_examples.tfrecord-00004-of-00030.gz. -rw-r--r-- 1 root root 31K Sep 24 15:47 make_examples.tfrecord-00003-of-00030.gz. -rw-r--r-- 1 root root 16K Sep 24 15:47 make_examples.tfrecord-00022-of-00030.gz. -rw-r--r-- 1 root root 6.2K Sep 24 15:47 gvcf.tfrecord-00008-of-00030.gz. -rw-r--r-- 1 root root 56K Sep 24 15:47 make_examples.tfrecord-00008-of-00030.gz. -rw-r--r-- 1 root root 7.5K Sep 24 15:47 make_examples.tfrecord-00025-of-00030.gz. -rw-r--r-- 1 root root 4.6K Sep 24 15:47 gvcf.tfrecord-00026-of-00030.gz. -rw-r--r-- 1 root root 38K Sep 24 15:47 make_examples.tfrecord-00026-of-00030.gz. -rw-r--r-- 1 root root 5.2K Sep 24 15:47 gvcf.tfrecord-00002-of-00030.gz. -rw-r--r-- 1 root root 32K Sep 24 15:47 make_examples.tfrecord-00002-of-00030.gz. -rw-r--r-- 1 root root 7.0K Sep 24 15:47 gvcf.tfrecord-00007-of-00030.gz. -rw-r--r-- 1 root root 3.0K Sep 24 15:47 gvcf.tfrecord-00024-of-00030.gz. -rw-r--r-- 1 root root 44K Sep 24 15:47 make_examples.tfrecord-00007-of-00030.gz. -rw-r--r-- 1 root root 11K Sep 24 15:47 make_examples.tfrecord-00024-of-00030.gz. -rw-r--r-- 1 root root 5.3K Sep 24 15:47 gvcf.tfrecord-00013-of-00030.gz. -rw-r--r-- 1 root root 27K Sep 24 15:47 make_examples.tfrecord-00013-of-00030.gz. -rw-r--r-- 1 root root 4.9K Sep 24 15:47 gvcf.tfrecord-00005-of-00030.gz. -rw-r--r-- 1 root root 20K Sep 24 15:47 make_examples.tfrecord-00005-of-00030.gz. ```. This is how my `free -h` is looking like right now:. ```. total used free shared buff/cache available. Mem: 125G 123G 776M 224M 933M 354M. Swap: 101G 100G 1.7G. ```. Thank you. Cheers.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:9849,reliability,availab,available,9849,f-00030.gz. -rw-r--r-- 1 root root 52K Sep 24 15:47 make_examples.tfrecord-00000-of-00030.gz. -rw-r--r-- 1 root root 5.9K Sep 24 15:47 gvcf.tfrecord-00003-of-00030.gz. -rw-r--r-- 1 root root 5.7K Sep 24 15:47 gvcf.tfrecord-00004-of-00030.gz. -rw-r--r-- 1 root root 3.6K Sep 24 15:47 gvcf.tfrecord-00022-of-00030.gz. -rw-r--r-- 1 root root 4.5K Sep 24 15:47 gvcf.tfrecord-00025-of-00030.gz. -rw-r--r-- 1 root root 22K Sep 24 15:47 make_examples.tfrecord-00004-of-00030.gz. -rw-r--r-- 1 root root 31K Sep 24 15:47 make_examples.tfrecord-00003-of-00030.gz. -rw-r--r-- 1 root root 16K Sep 24 15:47 make_examples.tfrecord-00022-of-00030.gz. -rw-r--r-- 1 root root 6.2K Sep 24 15:47 gvcf.tfrecord-00008-of-00030.gz. -rw-r--r-- 1 root root 56K Sep 24 15:47 make_examples.tfrecord-00008-of-00030.gz. -rw-r--r-- 1 root root 7.5K Sep 24 15:47 make_examples.tfrecord-00025-of-00030.gz. -rw-r--r-- 1 root root 4.6K Sep 24 15:47 gvcf.tfrecord-00026-of-00030.gz. -rw-r--r-- 1 root root 38K Sep 24 15:47 make_examples.tfrecord-00026-of-00030.gz. -rw-r--r-- 1 root root 5.2K Sep 24 15:47 gvcf.tfrecord-00002-of-00030.gz. -rw-r--r-- 1 root root 32K Sep 24 15:47 make_examples.tfrecord-00002-of-00030.gz. -rw-r--r-- 1 root root 7.0K Sep 24 15:47 gvcf.tfrecord-00007-of-00030.gz. -rw-r--r-- 1 root root 3.0K Sep 24 15:47 gvcf.tfrecord-00024-of-00030.gz. -rw-r--r-- 1 root root 44K Sep 24 15:47 make_examples.tfrecord-00007-of-00030.gz. -rw-r--r-- 1 root root 11K Sep 24 15:47 make_examples.tfrecord-00024-of-00030.gz. -rw-r--r-- 1 root root 5.3K Sep 24 15:47 gvcf.tfrecord-00013-of-00030.gz. -rw-r--r-- 1 root root 27K Sep 24 15:47 make_examples.tfrecord-00013-of-00030.gz. -rw-r--r-- 1 root root 4.9K Sep 24 15:47 gvcf.tfrecord-00005-of-00030.gz. -rw-r--r-- 1 root root 20K Sep 24 15:47 make_examples.tfrecord-00005-of-00030.gz. ```. This is how my `free -h` is looking like right now:. ```. total used free shared buff/cache available. Mem: 125G 123G 776M 224M 933M 354M. Swap: 101G 100G 1.7G. ```. Thank you. Cheers.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:272,safety,input,input,272,"Hi @gunjanbaid,. I came across anothe memmory problem, not to GPU, but the CPU. The postprocess_variants.py is consuming a ridiculous amount of memory: . This is how I'm runing: . `sudo docker run --memory=""100g"" --memory-swap=""101g"" --oom-kill-disable -v ""${DATA_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/postprocess_variants --ref ""/input/10consensus.fasta"" --infile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/output/M10.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/output/intermediate_results_dir/gvcf.tfrecord@25.gz"" --gvcf_outfile ""/output/M10.output.g.vcf.gz"". `. And the process is consuming all the designed memory the is autmatic killed. When I set the flag ` --oom-kill-disable ` get stucked on `D - uninterruptible sleep `. There is any argument I can use to manage this? I understand that the makeexamples should consume the most of the memory, but something else is happening. . This is how my deepvariant-run/output/intermediate_results_dir/ is looking . ```. -rw-r--r-- 1 root root 7.3G Oct 5 06:53 call_variants_output.tfrecord.gz. -rw-r--r-- 1 root root 577M Sep 29 23:42 gvcf.tfrecord-00004-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:42 make_examples.tfrecord-00004-of-00025.gz. -rw-r--r-- 1 root root 576M Sep 29 23:33 gvcf.tfrecord-00020-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:33 make_examples.tfrecord-00020-of-00025.gz. -rw-r--r-- 1 root root 576M Sep 29 23:33 gvcf.tfrecord-00017-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:33 make_examples.tfrecord-00017-of-00025.gz. -rw-r--r-- 1 root root 575M Sep 29 23:33 gvcf.tfrecord-00000-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:33 make_examples.tfrecord-00000-of-00025.gz. -rw-r--r-- 1 root root 577M Sep 29 23:32 gvcf.tfrecord-00009-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:32 make_examples.tfrecord-00009-of-00025.gz. -rw-r--r-- 1 root root 576M Sep 29 23:31 gvcf.tfrecord-00003-of-00025",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:392,safety,input,input,392,"Hi @gunjanbaid,. I came across anothe memmory problem, not to GPU, but the CPU. The postprocess_variants.py is consuming a ridiculous amount of memory: . This is how I'm runing: . `sudo docker run --memory=""100g"" --memory-swap=""101g"" --oom-kill-disable -v ""${DATA_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/postprocess_variants --ref ""/input/10consensus.fasta"" --infile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/output/M10.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/output/intermediate_results_dir/gvcf.tfrecord@25.gz"" --gvcf_outfile ""/output/M10.output.g.vcf.gz"". `. And the process is consuming all the designed memory the is autmatic killed. When I set the flag ` --oom-kill-disable ` get stucked on `D - uninterruptible sleep `. There is any argument I can use to manage this? I understand that the makeexamples should consume the most of the memory, but something else is happening. . This is how my deepvariant-run/output/intermediate_results_dir/ is looking . ```. -rw-r--r-- 1 root root 7.3G Oct 5 06:53 call_variants_output.tfrecord.gz. -rw-r--r-- 1 root root 577M Sep 29 23:42 gvcf.tfrecord-00004-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:42 make_examples.tfrecord-00004-of-00025.gz. -rw-r--r-- 1 root root 576M Sep 29 23:33 gvcf.tfrecord-00020-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:33 make_examples.tfrecord-00020-of-00025.gz. -rw-r--r-- 1 root root 576M Sep 29 23:33 gvcf.tfrecord-00017-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:33 make_examples.tfrecord-00017-of-00025.gz. -rw-r--r-- 1 root root 575M Sep 29 23:33 gvcf.tfrecord-00000-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:33 make_examples.tfrecord-00000-of-00025.gz. -rw-r--r-- 1 root root 577M Sep 29 23:32 gvcf.tfrecord-00009-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:32 make_examples.tfrecord-00009-of-00025.gz. -rw-r--r-- 1 root root 576M Sep 29 23:31 gvcf.tfrecord-00003-of-00025",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:868,safety,manag,manage,868,"Hi @gunjanbaid,. I came across anothe memmory problem, not to GPU, but the CPU. The postprocess_variants.py is consuming a ridiculous amount of memory: . This is how I'm runing: . `sudo docker run --memory=""100g"" --memory-swap=""101g"" --oom-kill-disable -v ""${DATA_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/postprocess_variants --ref ""/input/10consensus.fasta"" --infile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/output/M10.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/output/intermediate_results_dir/gvcf.tfrecord@25.gz"" --gvcf_outfile ""/output/M10.output.g.vcf.gz"". `. And the process is consuming all the designed memory the is autmatic killed. When I set the flag ` --oom-kill-disable ` get stucked on `D - uninterruptible sleep `. There is any argument I can use to manage this? I understand that the makeexamples should consume the most of the memory, but something else is happening. . This is how my deepvariant-run/output/intermediate_results_dir/ is looking . ```. -rw-r--r-- 1 root root 7.3G Oct 5 06:53 call_variants_output.tfrecord.gz. -rw-r--r-- 1 root root 577M Sep 29 23:42 gvcf.tfrecord-00004-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:42 make_examples.tfrecord-00004-of-00025.gz. -rw-r--r-- 1 root root 576M Sep 29 23:33 gvcf.tfrecord-00020-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:33 make_examples.tfrecord-00020-of-00025.gz. -rw-r--r-- 1 root root 576M Sep 29 23:33 gvcf.tfrecord-00017-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:33 make_examples.tfrecord-00017-of-00025.gz. -rw-r--r-- 1 root root 575M Sep 29 23:33 gvcf.tfrecord-00000-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:33 make_examples.tfrecord-00000-of-00025.gz. -rw-r--r-- 1 root root 577M Sep 29 23:32 gvcf.tfrecord-00009-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:32 make_examples.tfrecord-00009-of-00025.gz. -rw-r--r-- 1 root root 576M Sep 29 23:31 gvcf.tfrecord-00003-of-00025",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:9849,safety,avail,available,9849,f-00030.gz. -rw-r--r-- 1 root root 52K Sep 24 15:47 make_examples.tfrecord-00000-of-00030.gz. -rw-r--r-- 1 root root 5.9K Sep 24 15:47 gvcf.tfrecord-00003-of-00030.gz. -rw-r--r-- 1 root root 5.7K Sep 24 15:47 gvcf.tfrecord-00004-of-00030.gz. -rw-r--r-- 1 root root 3.6K Sep 24 15:47 gvcf.tfrecord-00022-of-00030.gz. -rw-r--r-- 1 root root 4.5K Sep 24 15:47 gvcf.tfrecord-00025-of-00030.gz. -rw-r--r-- 1 root root 22K Sep 24 15:47 make_examples.tfrecord-00004-of-00030.gz. -rw-r--r-- 1 root root 31K Sep 24 15:47 make_examples.tfrecord-00003-of-00030.gz. -rw-r--r-- 1 root root 16K Sep 24 15:47 make_examples.tfrecord-00022-of-00030.gz. -rw-r--r-- 1 root root 6.2K Sep 24 15:47 gvcf.tfrecord-00008-of-00030.gz. -rw-r--r-- 1 root root 56K Sep 24 15:47 make_examples.tfrecord-00008-of-00030.gz. -rw-r--r-- 1 root root 7.5K Sep 24 15:47 make_examples.tfrecord-00025-of-00030.gz. -rw-r--r-- 1 root root 4.6K Sep 24 15:47 gvcf.tfrecord-00026-of-00030.gz. -rw-r--r-- 1 root root 38K Sep 24 15:47 make_examples.tfrecord-00026-of-00030.gz. -rw-r--r-- 1 root root 5.2K Sep 24 15:47 gvcf.tfrecord-00002-of-00030.gz. -rw-r--r-- 1 root root 32K Sep 24 15:47 make_examples.tfrecord-00002-of-00030.gz. -rw-r--r-- 1 root root 7.0K Sep 24 15:47 gvcf.tfrecord-00007-of-00030.gz. -rw-r--r-- 1 root root 3.0K Sep 24 15:47 gvcf.tfrecord-00024-of-00030.gz. -rw-r--r-- 1 root root 44K Sep 24 15:47 make_examples.tfrecord-00007-of-00030.gz. -rw-r--r-- 1 root root 11K Sep 24 15:47 make_examples.tfrecord-00024-of-00030.gz. -rw-r--r-- 1 root root 5.3K Sep 24 15:47 gvcf.tfrecord-00013-of-00030.gz. -rw-r--r-- 1 root root 27K Sep 24 15:47 make_examples.tfrecord-00013-of-00030.gz. -rw-r--r-- 1 root root 4.9K Sep 24 15:47 gvcf.tfrecord-00005-of-00030.gz. -rw-r--r-- 1 root root 20K Sep 24 15:47 make_examples.tfrecord-00005-of-00030.gz. ```. This is how my `free -h` is looking like right now:. ```. total used free shared buff/cache available. Mem: 125G 123G 776M 224M 933M 354M. Swap: 101G 100G 1.7G. ```. Thank you. Cheers.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:9849,security,availab,available,9849,f-00030.gz. -rw-r--r-- 1 root root 52K Sep 24 15:47 make_examples.tfrecord-00000-of-00030.gz. -rw-r--r-- 1 root root 5.9K Sep 24 15:47 gvcf.tfrecord-00003-of-00030.gz. -rw-r--r-- 1 root root 5.7K Sep 24 15:47 gvcf.tfrecord-00004-of-00030.gz. -rw-r--r-- 1 root root 3.6K Sep 24 15:47 gvcf.tfrecord-00022-of-00030.gz. -rw-r--r-- 1 root root 4.5K Sep 24 15:47 gvcf.tfrecord-00025-of-00030.gz. -rw-r--r-- 1 root root 22K Sep 24 15:47 make_examples.tfrecord-00004-of-00030.gz. -rw-r--r-- 1 root root 31K Sep 24 15:47 make_examples.tfrecord-00003-of-00030.gz. -rw-r--r-- 1 root root 16K Sep 24 15:47 make_examples.tfrecord-00022-of-00030.gz. -rw-r--r-- 1 root root 6.2K Sep 24 15:47 gvcf.tfrecord-00008-of-00030.gz. -rw-r--r-- 1 root root 56K Sep 24 15:47 make_examples.tfrecord-00008-of-00030.gz. -rw-r--r-- 1 root root 7.5K Sep 24 15:47 make_examples.tfrecord-00025-of-00030.gz. -rw-r--r-- 1 root root 4.6K Sep 24 15:47 gvcf.tfrecord-00026-of-00030.gz. -rw-r--r-- 1 root root 38K Sep 24 15:47 make_examples.tfrecord-00026-of-00030.gz. -rw-r--r-- 1 root root 5.2K Sep 24 15:47 gvcf.tfrecord-00002-of-00030.gz. -rw-r--r-- 1 root root 32K Sep 24 15:47 make_examples.tfrecord-00002-of-00030.gz. -rw-r--r-- 1 root root 7.0K Sep 24 15:47 gvcf.tfrecord-00007-of-00030.gz. -rw-r--r-- 1 root root 3.0K Sep 24 15:47 gvcf.tfrecord-00024-of-00030.gz. -rw-r--r-- 1 root root 44K Sep 24 15:47 make_examples.tfrecord-00007-of-00030.gz. -rw-r--r-- 1 root root 11K Sep 24 15:47 make_examples.tfrecord-00024-of-00030.gz. -rw-r--r-- 1 root root 5.3K Sep 24 15:47 gvcf.tfrecord-00013-of-00030.gz. -rw-r--r-- 1 root root 27K Sep 24 15:47 make_examples.tfrecord-00013-of-00030.gz. -rw-r--r-- 1 root root 4.9K Sep 24 15:47 gvcf.tfrecord-00005-of-00030.gz. -rw-r--r-- 1 root root 20K Sep 24 15:47 make_examples.tfrecord-00005-of-00030.gz. ```. This is how my `free -h` is looking like right now:. ```. total used free shared buff/cache available. Mem: 125G 123G 776M 224M 933M 354M. Swap: 101G 100G 1.7G. ```. Thank you. Cheers.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:883,testability,understand,understand,883,"Hi @gunjanbaid,. I came across anothe memmory problem, not to GPU, but the CPU. The postprocess_variants.py is consuming a ridiculous amount of memory: . This is how I'm runing: . `sudo docker run --memory=""100g"" --memory-swap=""101g"" --oom-kill-disable -v ""${DATA_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/postprocess_variants --ref ""/input/10consensus.fasta"" --infile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/output/M10.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/output/intermediate_results_dir/gvcf.tfrecord@25.gz"" --gvcf_outfile ""/output/M10.output.g.vcf.gz"". `. And the process is consuming all the designed memory the is autmatic killed. When I set the flag ` --oom-kill-disable ` get stucked on `D - uninterruptible sleep `. There is any argument I can use to manage this? I understand that the makeexamples should consume the most of the memory, but something else is happening. . This is how my deepvariant-run/output/intermediate_results_dir/ is looking . ```. -rw-r--r-- 1 root root 7.3G Oct 5 06:53 call_variants_output.tfrecord.gz. -rw-r--r-- 1 root root 577M Sep 29 23:42 gvcf.tfrecord-00004-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:42 make_examples.tfrecord-00004-of-00025.gz. -rw-r--r-- 1 root root 576M Sep 29 23:33 gvcf.tfrecord-00020-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:33 make_examples.tfrecord-00020-of-00025.gz. -rw-r--r-- 1 root root 576M Sep 29 23:33 gvcf.tfrecord-00017-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:33 make_examples.tfrecord-00017-of-00025.gz. -rw-r--r-- 1 root root 575M Sep 29 23:33 gvcf.tfrecord-00000-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:33 make_examples.tfrecord-00000-of-00025.gz. -rw-r--r-- 1 root root 577M Sep 29 23:32 gvcf.tfrecord-00009-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:32 make_examples.tfrecord-00009-of-00025.gz. -rw-r--r-- 1 root root 576M Sep 29 23:31 gvcf.tfrecord-00003-of-00025",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:144,usability,memor,memory,144,"Hi @gunjanbaid,. I came across anothe memmory problem, not to GPU, but the CPU. The postprocess_variants.py is consuming a ridiculous amount of memory: . This is how I'm runing: . `sudo docker run --memory=""100g"" --memory-swap=""101g"" --oom-kill-disable -v ""${DATA_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/postprocess_variants --ref ""/input/10consensus.fasta"" --infile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/output/M10.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/output/intermediate_results_dir/gvcf.tfrecord@25.gz"" --gvcf_outfile ""/output/M10.output.g.vcf.gz"". `. And the process is consuming all the designed memory the is autmatic killed. When I set the flag ` --oom-kill-disable ` get stucked on `D - uninterruptible sleep `. There is any argument I can use to manage this? I understand that the makeexamples should consume the most of the memory, but something else is happening. . This is how my deepvariant-run/output/intermediate_results_dir/ is looking . ```. -rw-r--r-- 1 root root 7.3G Oct 5 06:53 call_variants_output.tfrecord.gz. -rw-r--r-- 1 root root 577M Sep 29 23:42 gvcf.tfrecord-00004-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:42 make_examples.tfrecord-00004-of-00025.gz. -rw-r--r-- 1 root root 576M Sep 29 23:33 gvcf.tfrecord-00020-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:33 make_examples.tfrecord-00020-of-00025.gz. -rw-r--r-- 1 root root 576M Sep 29 23:33 gvcf.tfrecord-00017-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:33 make_examples.tfrecord-00017-of-00025.gz. -rw-r--r-- 1 root root 575M Sep 29 23:33 gvcf.tfrecord-00000-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:33 make_examples.tfrecord-00000-of-00025.gz. -rw-r--r-- 1 root root 577M Sep 29 23:32 gvcf.tfrecord-00009-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:32 make_examples.tfrecord-00009-of-00025.gz. -rw-r--r-- 1 root root 576M Sep 29 23:31 gvcf.tfrecord-00003-of-00025",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:199,usability,memor,memory,199,"Hi @gunjanbaid,. I came across anothe memmory problem, not to GPU, but the CPU. The postprocess_variants.py is consuming a ridiculous amount of memory: . This is how I'm runing: . `sudo docker run --memory=""100g"" --memory-swap=""101g"" --oom-kill-disable -v ""${DATA_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/postprocess_variants --ref ""/input/10consensus.fasta"" --infile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/output/M10.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/output/intermediate_results_dir/gvcf.tfrecord@25.gz"" --gvcf_outfile ""/output/M10.output.g.vcf.gz"". `. And the process is consuming all the designed memory the is autmatic killed. When I set the flag ` --oom-kill-disable ` get stucked on `D - uninterruptible sleep `. There is any argument I can use to manage this? I understand that the makeexamples should consume the most of the memory, but something else is happening. . This is how my deepvariant-run/output/intermediate_results_dir/ is looking . ```. -rw-r--r-- 1 root root 7.3G Oct 5 06:53 call_variants_output.tfrecord.gz. -rw-r--r-- 1 root root 577M Sep 29 23:42 gvcf.tfrecord-00004-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:42 make_examples.tfrecord-00004-of-00025.gz. -rw-r--r-- 1 root root 576M Sep 29 23:33 gvcf.tfrecord-00020-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:33 make_examples.tfrecord-00020-of-00025.gz. -rw-r--r-- 1 root root 576M Sep 29 23:33 gvcf.tfrecord-00017-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:33 make_examples.tfrecord-00017-of-00025.gz. -rw-r--r-- 1 root root 575M Sep 29 23:33 gvcf.tfrecord-00000-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:33 make_examples.tfrecord-00000-of-00025.gz. -rw-r--r-- 1 root root 577M Sep 29 23:32 gvcf.tfrecord-00009-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:32 make_examples.tfrecord-00009-of-00025.gz. -rw-r--r-- 1 root root 576M Sep 29 23:31 gvcf.tfrecord-00003-of-00025",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:215,usability,memor,memory-swap,215,"Hi @gunjanbaid,. I came across anothe memmory problem, not to GPU, but the CPU. The postprocess_variants.py is consuming a ridiculous amount of memory: . This is how I'm runing: . `sudo docker run --memory=""100g"" --memory-swap=""101g"" --oom-kill-disable -v ""${DATA_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/postprocess_variants --ref ""/input/10consensus.fasta"" --infile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/output/M10.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/output/intermediate_results_dir/gvcf.tfrecord@25.gz"" --gvcf_outfile ""/output/M10.output.g.vcf.gz"". `. And the process is consuming all the designed memory the is autmatic killed. When I set the flag ` --oom-kill-disable ` get stucked on `D - uninterruptible sleep `. There is any argument I can use to manage this? I understand that the makeexamples should consume the most of the memory, but something else is happening. . This is how my deepvariant-run/output/intermediate_results_dir/ is looking . ```. -rw-r--r-- 1 root root 7.3G Oct 5 06:53 call_variants_output.tfrecord.gz. -rw-r--r-- 1 root root 577M Sep 29 23:42 gvcf.tfrecord-00004-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:42 make_examples.tfrecord-00004-of-00025.gz. -rw-r--r-- 1 root root 576M Sep 29 23:33 gvcf.tfrecord-00020-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:33 make_examples.tfrecord-00020-of-00025.gz. -rw-r--r-- 1 root root 576M Sep 29 23:33 gvcf.tfrecord-00017-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:33 make_examples.tfrecord-00017-of-00025.gz. -rw-r--r-- 1 root root 575M Sep 29 23:33 gvcf.tfrecord-00000-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:33 make_examples.tfrecord-00000-of-00025.gz. -rw-r--r-- 1 root root 577M Sep 29 23:32 gvcf.tfrecord-00009-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:32 make_examples.tfrecord-00009-of-00025.gz. -rw-r--r-- 1 root root 576M Sep 29 23:31 gvcf.tfrecord-00003-of-00025",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:272,usability,input,input,272,"Hi @gunjanbaid,. I came across anothe memmory problem, not to GPU, but the CPU. The postprocess_variants.py is consuming a ridiculous amount of memory: . This is how I'm runing: . `sudo docker run --memory=""100g"" --memory-swap=""101g"" --oom-kill-disable -v ""${DATA_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/postprocess_variants --ref ""/input/10consensus.fasta"" --infile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/output/M10.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/output/intermediate_results_dir/gvcf.tfrecord@25.gz"" --gvcf_outfile ""/output/M10.output.g.vcf.gz"". `. And the process is consuming all the designed memory the is autmatic killed. When I set the flag ` --oom-kill-disable ` get stucked on `D - uninterruptible sleep `. There is any argument I can use to manage this? I understand that the makeexamples should consume the most of the memory, but something else is happening. . This is how my deepvariant-run/output/intermediate_results_dir/ is looking . ```. -rw-r--r-- 1 root root 7.3G Oct 5 06:53 call_variants_output.tfrecord.gz. -rw-r--r-- 1 root root 577M Sep 29 23:42 gvcf.tfrecord-00004-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:42 make_examples.tfrecord-00004-of-00025.gz. -rw-r--r-- 1 root root 576M Sep 29 23:33 gvcf.tfrecord-00020-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:33 make_examples.tfrecord-00020-of-00025.gz. -rw-r--r-- 1 root root 576M Sep 29 23:33 gvcf.tfrecord-00017-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:33 make_examples.tfrecord-00017-of-00025.gz. -rw-r--r-- 1 root root 575M Sep 29 23:33 gvcf.tfrecord-00000-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:33 make_examples.tfrecord-00000-of-00025.gz. -rw-r--r-- 1 root root 577M Sep 29 23:32 gvcf.tfrecord-00009-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:32 make_examples.tfrecord-00009-of-00025.gz. -rw-r--r-- 1 root root 576M Sep 29 23:31 gvcf.tfrecord-00003-of-00025",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:392,usability,input,input,392,"Hi @gunjanbaid,. I came across anothe memmory problem, not to GPU, but the CPU. The postprocess_variants.py is consuming a ridiculous amount of memory: . This is how I'm runing: . `sudo docker run --memory=""100g"" --memory-swap=""101g"" --oom-kill-disable -v ""${DATA_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/postprocess_variants --ref ""/input/10consensus.fasta"" --infile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/output/M10.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/output/intermediate_results_dir/gvcf.tfrecord@25.gz"" --gvcf_outfile ""/output/M10.output.g.vcf.gz"". `. And the process is consuming all the designed memory the is autmatic killed. When I set the flag ` --oom-kill-disable ` get stucked on `D - uninterruptible sleep `. There is any argument I can use to manage this? I understand that the makeexamples should consume the most of the memory, but something else is happening. . This is how my deepvariant-run/output/intermediate_results_dir/ is looking . ```. -rw-r--r-- 1 root root 7.3G Oct 5 06:53 call_variants_output.tfrecord.gz. -rw-r--r-- 1 root root 577M Sep 29 23:42 gvcf.tfrecord-00004-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:42 make_examples.tfrecord-00004-of-00025.gz. -rw-r--r-- 1 root root 576M Sep 29 23:33 gvcf.tfrecord-00020-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:33 make_examples.tfrecord-00020-of-00025.gz. -rw-r--r-- 1 root root 576M Sep 29 23:33 gvcf.tfrecord-00017-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:33 make_examples.tfrecord-00017-of-00025.gz. -rw-r--r-- 1 root root 575M Sep 29 23:33 gvcf.tfrecord-00000-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:33 make_examples.tfrecord-00000-of-00025.gz. -rw-r--r-- 1 root root 577M Sep 29 23:32 gvcf.tfrecord-00009-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:32 make_examples.tfrecord-00009-of-00025.gz. -rw-r--r-- 1 root root 576M Sep 29 23:31 gvcf.tfrecord-00003-of-00025",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:714,usability,memor,memory,714,"Hi @gunjanbaid,. I came across anothe memmory problem, not to GPU, but the CPU. The postprocess_variants.py is consuming a ridiculous amount of memory: . This is how I'm runing: . `sudo docker run --memory=""100g"" --memory-swap=""101g"" --oom-kill-disable -v ""${DATA_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/postprocess_variants --ref ""/input/10consensus.fasta"" --infile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/output/M10.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/output/intermediate_results_dir/gvcf.tfrecord@25.gz"" --gvcf_outfile ""/output/M10.output.g.vcf.gz"". `. And the process is consuming all the designed memory the is autmatic killed. When I set the flag ` --oom-kill-disable ` get stucked on `D - uninterruptible sleep `. There is any argument I can use to manage this? I understand that the makeexamples should consume the most of the memory, but something else is happening. . This is how my deepvariant-run/output/intermediate_results_dir/ is looking . ```. -rw-r--r-- 1 root root 7.3G Oct 5 06:53 call_variants_output.tfrecord.gz. -rw-r--r-- 1 root root 577M Sep 29 23:42 gvcf.tfrecord-00004-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:42 make_examples.tfrecord-00004-of-00025.gz. -rw-r--r-- 1 root root 576M Sep 29 23:33 gvcf.tfrecord-00020-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:33 make_examples.tfrecord-00020-of-00025.gz. -rw-r--r-- 1 root root 576M Sep 29 23:33 gvcf.tfrecord-00017-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:33 make_examples.tfrecord-00017-of-00025.gz. -rw-r--r-- 1 root root 575M Sep 29 23:33 gvcf.tfrecord-00000-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:33 make_examples.tfrecord-00000-of-00025.gz. -rw-r--r-- 1 root root 577M Sep 29 23:32 gvcf.tfrecord-00009-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:32 make_examples.tfrecord-00009-of-00025.gz. -rw-r--r-- 1 root root 576M Sep 29 23:31 gvcf.tfrecord-00003-of-00025",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:947,usability,memor,memory,947,"Hi @gunjanbaid,. I came across anothe memmory problem, not to GPU, but the CPU. The postprocess_variants.py is consuming a ridiculous amount of memory: . This is how I'm runing: . `sudo docker run --memory=""100g"" --memory-swap=""101g"" --oom-kill-disable -v ""${DATA_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/postprocess_variants --ref ""/input/10consensus.fasta"" --infile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/output/M10.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/output/intermediate_results_dir/gvcf.tfrecord@25.gz"" --gvcf_outfile ""/output/M10.output.g.vcf.gz"". `. And the process is consuming all the designed memory the is autmatic killed. When I set the flag ` --oom-kill-disable ` get stucked on `D - uninterruptible sleep `. There is any argument I can use to manage this? I understand that the makeexamples should consume the most of the memory, but something else is happening. . This is how my deepvariant-run/output/intermediate_results_dir/ is looking . ```. -rw-r--r-- 1 root root 7.3G Oct 5 06:53 call_variants_output.tfrecord.gz. -rw-r--r-- 1 root root 577M Sep 29 23:42 gvcf.tfrecord-00004-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:42 make_examples.tfrecord-00004-of-00025.gz. -rw-r--r-- 1 root root 576M Sep 29 23:33 gvcf.tfrecord-00020-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:33 make_examples.tfrecord-00020-of-00025.gz. -rw-r--r-- 1 root root 576M Sep 29 23:33 gvcf.tfrecord-00017-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:33 make_examples.tfrecord-00017-of-00025.gz. -rw-r--r-- 1 root root 575M Sep 29 23:33 gvcf.tfrecord-00000-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:33 make_examples.tfrecord-00000-of-00025.gz. -rw-r--r-- 1 root root 577M Sep 29 23:32 gvcf.tfrecord-00009-of-00025.gz. -rw-r--r-- 1 root root 8.8G Sep 29 23:32 make_examples.tfrecord-00009-of-00025.gz. -rw-r--r-- 1 root root 576M Sep 29 23:31 gvcf.tfrecord-00003-of-00025",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:469,energy efficiency,core,core,469,"@leorippel here is some information that might be relevant from [the docs](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-details.md):. > Because postprocess_variants combines and sorts the output of call_variants, it needs to see all of the outputs from call_variants for a single sample to merge into a final VCF. postprocess_variants is single-threaded and needs a non-trivial amount of memory to run (20-30 GB), so it is best run on a single/dual core machine with sufficient memory. It seems like you don't have much memory left on your machine, so that could be the issue. If you are able to free up some memory, you can run a sanity check by trying one of our [case studies](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-exome-case-study.md), which use smaller files and should consume much less memory.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:408,performance,memor,memory,408,"@leorippel here is some information that might be relevant from [the docs](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-details.md):. > Because postprocess_variants combines and sorts the output of call_variants, it needs to see all of the outputs from call_variants for a single sample to merge into a final VCF. postprocess_variants is single-threaded and needs a non-trivial amount of memory to run (20-30 GB), so it is best run on a single/dual core machine with sufficient memory. It seems like you don't have much memory left on your machine, so that could be the issue. If you are able to free up some memory, you can run a sanity check by trying one of our [case studies](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-exome-case-study.md), which use smaller files and should consume much less memory.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:498,performance,memor,memory,498,"@leorippel here is some information that might be relevant from [the docs](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-details.md):. > Because postprocess_variants combines and sorts the output of call_variants, it needs to see all of the outputs from call_variants for a single sample to merge into a final VCF. postprocess_variants is single-threaded and needs a non-trivial amount of memory to run (20-30 GB), so it is best run on a single/dual core machine with sufficient memory. It seems like you don't have much memory left on your machine, so that could be the issue. If you are able to free up some memory, you can run a sanity check by trying one of our [case studies](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-exome-case-study.md), which use smaller files and should consume much less memory.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:540,performance,memor,memory,540,"@leorippel here is some information that might be relevant from [the docs](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-details.md):. > Because postprocess_variants combines and sorts the output of call_variants, it needs to see all of the outputs from call_variants for a single sample to merge into a final VCF. postprocess_variants is single-threaded and needs a non-trivial amount of memory to run (20-30 GB), so it is best run on a single/dual core machine with sufficient memory. It seems like you don't have much memory left on your machine, so that could be the issue. If you are able to free up some memory, you can run a sanity check by trying one of our [case studies](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-exome-case-study.md), which use smaller files and should consume much less memory.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:629,performance,memor,memory,629,"@leorippel here is some information that might be relevant from [the docs](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-details.md):. > Because postprocess_variants combines and sorts the output of call_variants, it needs to see all of the outputs from call_variants for a single sample to merge into a final VCF. postprocess_variants is single-threaded and needs a non-trivial amount of memory to run (20-30 GB), so it is best run on a single/dual core machine with sufficient memory. It seems like you don't have much memory left on your machine, so that could be the issue. If you are able to free up some memory, you can run a sanity check by trying one of our [case studies](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-exome-case-study.md), which use smaller files and should consume much less memory.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:840,performance,memor,memory,840,"@leorippel here is some information that might be relevant from [the docs](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-details.md):. > Because postprocess_variants combines and sorts the output of call_variants, it needs to see all of the outputs from call_variants for a single sample to merge into a final VCF. postprocess_variants is single-threaded and needs a non-trivial amount of memory to run (20-30 GB), so it is best run on a single/dual core machine with sufficient memory. It seems like you don't have much memory left on your machine, so that could be the issue. If you are able to free up some memory, you can run a sanity check by trying one of our [case studies](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-exome-case-study.md), which use smaller files and should consume much less memory.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:651,safety,sanit,sanity,651,"@leorippel here is some information that might be relevant from [the docs](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-details.md):. > Because postprocess_variants combines and sorts the output of call_variants, it needs to see all of the outputs from call_variants for a single sample to merge into a final VCF. postprocess_variants is single-threaded and needs a non-trivial amount of memory to run (20-30 GB), so it is best run on a single/dual core machine with sufficient memory. It seems like you don't have much memory left on your machine, so that could be the issue. If you are able to free up some memory, you can run a sanity check by trying one of our [case studies](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-exome-case-study.md), which use smaller files and should consume much less memory.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:651,security,sanit,sanity,651,"@leorippel here is some information that might be relevant from [the docs](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-details.md):. > Because postprocess_variants combines and sorts the output of call_variants, it needs to see all of the outputs from call_variants for a single sample to merge into a final VCF. postprocess_variants is single-threaded and needs a non-trivial amount of memory to run (20-30 GB), so it is best run on a single/dual core machine with sufficient memory. It seems like you don't have much memory left on your machine, so that could be the issue. If you are able to free up some memory, you can run a sanity check by trying one of our [case studies](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-exome-case-study.md), which use smaller files and should consume much less memory.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:408,usability,memor,memory,408,"@leorippel here is some information that might be relevant from [the docs](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-details.md):. > Because postprocess_variants combines and sorts the output of call_variants, it needs to see all of the outputs from call_variants for a single sample to merge into a final VCF. postprocess_variants is single-threaded and needs a non-trivial amount of memory to run (20-30 GB), so it is best run on a single/dual core machine with sufficient memory. It seems like you don't have much memory left on your machine, so that could be the issue. If you are able to free up some memory, you can run a sanity check by trying one of our [case studies](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-exome-case-study.md), which use smaller files and should consume much less memory.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:498,usability,memor,memory,498,"@leorippel here is some information that might be relevant from [the docs](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-details.md):. > Because postprocess_variants combines and sorts the output of call_variants, it needs to see all of the outputs from call_variants for a single sample to merge into a final VCF. postprocess_variants is single-threaded and needs a non-trivial amount of memory to run (20-30 GB), so it is best run on a single/dual core machine with sufficient memory. It seems like you don't have much memory left on your machine, so that could be the issue. If you are able to free up some memory, you can run a sanity check by trying one of our [case studies](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-exome-case-study.md), which use smaller files and should consume much less memory.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:540,usability,memor,memory,540,"@leorippel here is some information that might be relevant from [the docs](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-details.md):. > Because postprocess_variants combines and sorts the output of call_variants, it needs to see all of the outputs from call_variants for a single sample to merge into a final VCF. postprocess_variants is single-threaded and needs a non-trivial amount of memory to run (20-30 GB), so it is best run on a single/dual core machine with sufficient memory. It seems like you don't have much memory left on your machine, so that could be the issue. If you are able to free up some memory, you can run a sanity check by trying one of our [case studies](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-exome-case-study.md), which use smaller files and should consume much less memory.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:629,usability,memor,memory,629,"@leorippel here is some information that might be relevant from [the docs](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-details.md):. > Because postprocess_variants combines and sorts the output of call_variants, it needs to see all of the outputs from call_variants for a single sample to merge into a final VCF. postprocess_variants is single-threaded and needs a non-trivial amount of memory to run (20-30 GB), so it is best run on a single/dual core machine with sufficient memory. It seems like you don't have much memory left on your machine, so that could be the issue. If you are able to free up some memory, you can run a sanity check by trying one of our [case studies](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-exome-case-study.md), which use smaller files and should consume much less memory.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:840,usability,memor,memory,840,"@leorippel here is some information that might be relevant from [the docs](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-details.md):. > Because postprocess_variants combines and sorts the output of call_variants, it needs to see all of the outputs from call_variants for a single sample to merge into a final VCF. postprocess_variants is single-threaded and needs a non-trivial amount of memory to run (20-30 GB), so it is best run on a single/dual core machine with sufficient memory. It seems like you don't have much memory left on your machine, so that could be the issue. If you are able to free up some memory, you can run a sanity check by trying one of our [case studies](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-exome-case-study.md), which use smaller files and should consume much less memory.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:173,energy efficiency,core,cores,173,"@gunjanbaid I forgot to mention, this is the` free -h` meanwhile is running the postprocess_variants. Before running the job is with the whole 126gb free and none of the 36 cores are in use. By that I want to say the whole machine is dedicated just to the postprocess. This is what I'm finding odd. 125 gb of ram plus 100gb of swap should be more than enough for the task. With a very small subset of the same dataset it works fine.. but with the whole deal, it crashes for OOM.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:391,integrability,sub,subset,391,"@gunjanbaid I forgot to mention, this is the` free -h` meanwhile is running the postprocess_variants. Before running the job is with the whole 126gb free and none of the 36 cores are in use. By that I want to say the whole machine is dedicated just to the postprocess. This is what I'm finding odd. 125 gb of ram plus 100gb of swap should be more than enough for the task. With a very small subset of the same dataset it works fine.. but with the whole deal, it crashes for OOM.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:141,performance,memor,memory,141,"@leorippel got it, thanks for the clarification! I'm not sure what the issue is in this case, but if you are able to use a machine with more memory, that would be the easiest option. Another option is to shard your data by chromosome and run DeepVariant separately for each chromosome (or groups of chromosomes).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:141,usability,memor,memory,141,"@leorippel got it, thanks for the clarification! I'm not sure what the issue is in this case, but if you are able to use a machine with more memory, that would be the easiest option. Another option is to shard your data by chromosome and run DeepVariant separately for each chromosome (or groups of chromosomes).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:29,usability,close,close,29,"Hi @leorippel , I'm going to close this issue now. Feel free to let us know if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:132,energy efficiency,gpu,gpus,132,"> Hi @leorippel, you can pass the desired argument to `run_deepvariant.py` using the below command:. > . > ```. > sudo docker run --gpus 1 \. > -v ""${DATA_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}:/output"" \. > google/deepvariant:""${BIN_VERSION}-gpu"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=""/input/${REF2}"" \. > --reads=""/input/${BAM2}"" \. > --output_vcf=/output/${OUTPUT_VCF} \. > --output_gvcf=/output/${OUTPUT_GVCF} \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=30 \. > --call_variants_extra_args=""config_string='gpu_options: {allow_growth: True}'"". > ```. > . > Specifically, I added: ` --call_variants_extra_args=""config_string='gpu_options: {allow_growth: True}'""`. > . > Does adding this fix the memory issue? Yes, it fixed the issue. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:242,energy efficiency,gpu,gpu,242,"> Hi @leorippel, you can pass the desired argument to `run_deepvariant.py` using the below command:. > . > ```. > sudo docker run --gpus 1 \. > -v ""${DATA_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}:/output"" \. > google/deepvariant:""${BIN_VERSION}-gpu"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=""/input/${REF2}"" \. > --reads=""/input/${BAM2}"" \. > --output_vcf=/output/${OUTPUT_VCF} \. > --output_gvcf=/output/${OUTPUT_GVCF} \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=30 \. > --call_variants_extra_args=""config_string='gpu_options: {allow_growth: True}'"". > ```. > . > Specifically, I added: ` --call_variants_extra_args=""config_string='gpu_options: {allow_growth: True}'""`. > . > Does adding this fix the memory issue? Yes, it fixed the issue. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:635,interoperability,Specif,Specifically,635,"> Hi @leorippel, you can pass the desired argument to `run_deepvariant.py` using the below command:. > . > ```. > sudo docker run --gpus 1 \. > -v ""${DATA_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}:/output"" \. > google/deepvariant:""${BIN_VERSION}-gpu"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=""/input/${REF2}"" \. > --reads=""/input/${BAM2}"" \. > --output_vcf=/output/${OUTPUT_VCF} \. > --output_gvcf=/output/${OUTPUT_GVCF} \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=30 \. > --call_variants_extra_args=""config_string='gpu_options: {allow_growth: True}'"". > ```. > . > Specifically, I added: ` --call_variants_extra_args=""config_string='gpu_options: {allow_growth: True}'""`. > . > Does adding this fix the memory issue? Yes, it fixed the issue. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:132,performance,gpu,gpus,132,"> Hi @leorippel, you can pass the desired argument to `run_deepvariant.py` using the below command:. > . > ```. > sudo docker run --gpus 1 \. > -v ""${DATA_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}:/output"" \. > google/deepvariant:""${BIN_VERSION}-gpu"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=""/input/${REF2}"" \. > --reads=""/input/${BAM2}"" \. > --output_vcf=/output/${OUTPUT_VCF} \. > --output_gvcf=/output/${OUTPUT_GVCF} \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=30 \. > --call_variants_extra_args=""config_string='gpu_options: {allow_growth: True}'"". > ```. > . > Specifically, I added: ` --call_variants_extra_args=""config_string='gpu_options: {allow_growth: True}'""`. > . > Does adding this fix the memory issue? Yes, it fixed the issue. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:242,performance,gpu,gpu,242,"> Hi @leorippel, you can pass the desired argument to `run_deepvariant.py` using the below command:. > . > ```. > sudo docker run --gpus 1 \. > -v ""${DATA_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}:/output"" \. > google/deepvariant:""${BIN_VERSION}-gpu"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=""/input/${REF2}"" \. > --reads=""/input/${BAM2}"" \. > --output_vcf=/output/${OUTPUT_VCF} \. > --output_gvcf=/output/${OUTPUT_GVCF} \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=30 \. > --call_variants_extra_args=""config_string='gpu_options: {allow_growth: True}'"". > ```. > . > Specifically, I added: ` --call_variants_extra_args=""config_string='gpu_options: {allow_growth: True}'""`. > . > Does adding this fix the memory issue? Yes, it fixed the issue. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:772,performance,memor,memory,772,"> Hi @leorippel, you can pass the desired argument to `run_deepvariant.py` using the below command:. > . > ```. > sudo docker run --gpus 1 \. > -v ""${DATA_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}:/output"" \. > google/deepvariant:""${BIN_VERSION}-gpu"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=""/input/${REF2}"" \. > --reads=""/input/${BAM2}"" \. > --output_vcf=/output/${OUTPUT_VCF} \. > --output_gvcf=/output/${OUTPUT_GVCF} \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=30 \. > --call_variants_extra_args=""config_string='gpu_options: {allow_growth: True}'"". > ```. > . > Specifically, I added: ` --call_variants_extra_args=""config_string='gpu_options: {allow_growth: True}'""`. > . > Does adding this fix the memory issue? Yes, it fixed the issue. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:747,reliability,Doe,Does,747,"> Hi @leorippel, you can pass the desired argument to `run_deepvariant.py` using the below command:. > . > ```. > sudo docker run --gpus 1 \. > -v ""${DATA_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}:/output"" \. > google/deepvariant:""${BIN_VERSION}-gpu"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=""/input/${REF2}"" \. > --reads=""/input/${BAM2}"" \. > --output_vcf=/output/${OUTPUT_VCF} \. > --output_gvcf=/output/${OUTPUT_GVCF} \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=30 \. > --call_variants_extra_args=""config_string='gpu_options: {allow_growth: True}'"". > ```. > . > Specifically, I added: ` --call_variants_extra_args=""config_string='gpu_options: {allow_growth: True}'""`. > . > Does adding this fix the memory issue? Yes, it fixed the issue. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:163,safety,input,input,163,"> Hi @leorippel, you can pass the desired argument to `run_deepvariant.py` using the below command:. > . > ```. > sudo docker run --gpus 1 \. > -v ""${DATA_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}:/output"" \. > google/deepvariant:""${BIN_VERSION}-gpu"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=""/input/${REF2}"" \. > --reads=""/input/${BAM2}"" \. > --output_vcf=/output/${OUTPUT_VCF} \. > --output_gvcf=/output/${OUTPUT_GVCF} \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=30 \. > --call_variants_extra_args=""config_string='gpu_options: {allow_growth: True}'"". > ```. > . > Specifically, I added: ` --call_variants_extra_args=""config_string='gpu_options: {allow_growth: True}'""`. > . > Does adding this fix the memory issue? Yes, it fixed the issue. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:324,safety,input,input,324,"> Hi @leorippel, you can pass the desired argument to `run_deepvariant.py` using the below command:. > . > ```. > sudo docker run --gpus 1 \. > -v ""${DATA_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}:/output"" \. > google/deepvariant:""${BIN_VERSION}-gpu"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=""/input/${REF2}"" \. > --reads=""/input/${BAM2}"" \. > --output_vcf=/output/${OUTPUT_VCF} \. > --output_gvcf=/output/${OUTPUT_GVCF} \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=30 \. > --call_variants_extra_args=""config_string='gpu_options: {allow_growth: True}'"". > ```. > . > Specifically, I added: ` --call_variants_extra_args=""config_string='gpu_options: {allow_growth: True}'""`. > . > Does adding this fix the memory issue? Yes, it fixed the issue. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:354,safety,input,input,354,"> Hi @leorippel, you can pass the desired argument to `run_deepvariant.py` using the below command:. > . > ```. > sudo docker run --gpus 1 \. > -v ""${DATA_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}:/output"" \. > google/deepvariant:""${BIN_VERSION}-gpu"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=""/input/${REF2}"" \. > --reads=""/input/${BAM2}"" \. > --output_vcf=/output/${OUTPUT_VCF} \. > --output_gvcf=/output/${OUTPUT_GVCF} \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=30 \. > --call_variants_extra_args=""config_string='gpu_options: {allow_growth: True}'"". > ```. > . > Specifically, I added: ` --call_variants_extra_args=""config_string='gpu_options: {allow_growth: True}'""`. > . > Does adding this fix the memory issue? Yes, it fixed the issue. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:91,usability,command,command,91,"> Hi @leorippel, you can pass the desired argument to `run_deepvariant.py` using the below command:. > . > ```. > sudo docker run --gpus 1 \. > -v ""${DATA_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}:/output"" \. > google/deepvariant:""${BIN_VERSION}-gpu"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=""/input/${REF2}"" \. > --reads=""/input/${BAM2}"" \. > --output_vcf=/output/${OUTPUT_VCF} \. > --output_gvcf=/output/${OUTPUT_GVCF} \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=30 \. > --call_variants_extra_args=""config_string='gpu_options: {allow_growth: True}'"". > ```. > . > Specifically, I added: ` --call_variants_extra_args=""config_string='gpu_options: {allow_growth: True}'""`. > . > Does adding this fix the memory issue? Yes, it fixed the issue. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:163,usability,input,input,163,"> Hi @leorippel, you can pass the desired argument to `run_deepvariant.py` using the below command:. > . > ```. > sudo docker run --gpus 1 \. > -v ""${DATA_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}:/output"" \. > google/deepvariant:""${BIN_VERSION}-gpu"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=""/input/${REF2}"" \. > --reads=""/input/${BAM2}"" \. > --output_vcf=/output/${OUTPUT_VCF} \. > --output_gvcf=/output/${OUTPUT_GVCF} \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=30 \. > --call_variants_extra_args=""config_string='gpu_options: {allow_growth: True}'"". > ```. > . > Specifically, I added: ` --call_variants_extra_args=""config_string='gpu_options: {allow_growth: True}'""`. > . > Does adding this fix the memory issue? Yes, it fixed the issue. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:324,usability,input,input,324,"> Hi @leorippel, you can pass the desired argument to `run_deepvariant.py` using the below command:. > . > ```. > sudo docker run --gpus 1 \. > -v ""${DATA_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}:/output"" \. > google/deepvariant:""${BIN_VERSION}-gpu"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=""/input/${REF2}"" \. > --reads=""/input/${BAM2}"" \. > --output_vcf=/output/${OUTPUT_VCF} \. > --output_gvcf=/output/${OUTPUT_GVCF} \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=30 \. > --call_variants_extra_args=""config_string='gpu_options: {allow_growth: True}'"". > ```. > . > Specifically, I added: ` --call_variants_extra_args=""config_string='gpu_options: {allow_growth: True}'""`. > . > Does adding this fix the memory issue? Yes, it fixed the issue. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:354,usability,input,input,354,"> Hi @leorippel, you can pass the desired argument to `run_deepvariant.py` using the below command:. > . > ```. > sudo docker run --gpus 1 \. > -v ""${DATA_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}:/output"" \. > google/deepvariant:""${BIN_VERSION}-gpu"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=""/input/${REF2}"" \. > --reads=""/input/${BAM2}"" \. > --output_vcf=/output/${OUTPUT_VCF} \. > --output_gvcf=/output/${OUTPUT_GVCF} \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=30 \. > --call_variants_extra_args=""config_string='gpu_options: {allow_growth: True}'"". > ```. > . > Specifically, I added: ` --call_variants_extra_args=""config_string='gpu_options: {allow_growth: True}'""`. > . > Does adding this fix the memory issue? Yes, it fixed the issue. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:772,usability,memor,memory,772,"> Hi @leorippel, you can pass the desired argument to `run_deepvariant.py` using the below command:. > . > ```. > sudo docker run --gpus 1 \. > -v ""${DATA_DIR}"":""/input"" \. > -v ""${OUTPUT_DIR}:/output"" \. > google/deepvariant:""${BIN_VERSION}-gpu"" \. > /opt/deepvariant/bin/run_deepvariant \. > --model_type=WGS \. > --ref=""/input/${REF2}"" \. > --reads=""/input/${BAM2}"" \. > --output_vcf=/output/${OUTPUT_VCF} \. > --output_gvcf=/output/${OUTPUT_GVCF} \. > --intermediate_results_dir /output/intermediate_results_dir \. > --num_shards=30 \. > --call_variants_extra_args=""config_string='gpu_options: {allow_growth: True}'"". > ```. > . > Specifically, I added: ` --call_variants_extra_args=""config_string='gpu_options: {allow_growth: True}'""`. > . > Does adding this fix the memory issue? Yes, it fixed the issue. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:326,modifiability,interm,intermediate,326,"> @leorippel got it, thanks for the clarification! I'm not sure what the issue is in this case, but if you are able to use a machine with more memory, that would be the easiest option. Another option is to shard your data by chromosome and run DeepVariant separately for each chromosome (or groups of chromosomes). I took the intermediate results and ran into a bigger machine. It worked. The memory consumption rised to 435G !! by far the hungry step. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:143,performance,memor,memory,143,"> @leorippel got it, thanks for the clarification! I'm not sure what the issue is in this case, but if you are able to use a machine with more memory, that would be the easiest option. Another option is to shard your data by chromosome and run DeepVariant separately for each chromosome (or groups of chromosomes). I took the intermediate results and ran into a bigger machine. It worked. The memory consumption rised to 435G !! by far the hungry step. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:393,performance,memor,memory,393,"> @leorippel got it, thanks for the clarification! I'm not sure what the issue is in this case, but if you are able to use a machine with more memory, that would be the easiest option. Another option is to shard your data by chromosome and run DeepVariant separately for each chromosome (or groups of chromosomes). I took the intermediate results and ran into a bigger machine. It worked. The memory consumption rised to 435G !! by far the hungry step. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:143,usability,memor,memory,143,"> @leorippel got it, thanks for the clarification! I'm not sure what the issue is in this case, but if you are able to use a machine with more memory, that would be the easiest option. Another option is to shard your data by chromosome and run DeepVariant separately for each chromosome (or groups of chromosomes). I took the intermediate results and ran into a bigger machine. It worked. The memory consumption rised to 435G !! by far the hungry step. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/358:393,usability,memor,memory,393,"> @leorippel got it, thanks for the clarification! I'm not sure what the issue is in this case, but if you are able to use a machine with more memory, that would be the easiest option. Another option is to shard your data by chromosome and run DeepVariant separately for each chromosome (or groups of chromosomes). I took the intermediate results and ran into a bigger machine. It worked. The memory consumption rised to 435G !! by far the hungry step. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/358
https://github.com/google/deepvariant/issues/359:156,deployability,contain,container,156,"Hi @yonatansc97 . Those imports you listed have something in common: they are in C++ (or protos) and therefore need to be compiled first. Inside the docker container, we have these already compiled into the binaries like `/opt/deepvariant/bin/make_examples`, while `make_examples.py` is the source, but it won't work without compiling its C++ dependencies. I don't know how to make this work with your special setup, but perhaps the documentation on building from source will help get you started: https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md. I hope that helps at least point you in the right direction! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/359
https://github.com/google/deepvariant/issues/359:343,deployability,depend,dependencies,343,"Hi @yonatansc97 . Those imports you listed have something in common: they are in C++ (or protos) and therefore need to be compiled first. Inside the docker container, we have these already compiled into the binaries like `/opt/deepvariant/bin/make_examples`, while `make_examples.py` is the source, but it won't work without compiling its C++ dependencies. I don't know how to make this work with your special setup, but perhaps the documentation on building from source will help get you started: https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md. I hope that helps at least point you in the right direction! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/359
https://github.com/google/deepvariant/issues/359:450,deployability,build,building,450,"Hi @yonatansc97 . Those imports you listed have something in common: they are in C++ (or protos) and therefore need to be compiled first. Inside the docker container, we have these already compiled into the binaries like `/opt/deepvariant/bin/make_examples`, while `make_examples.py` is the source, but it won't work without compiling its C++ dependencies. I don't know how to make this work with your special setup, but perhaps the documentation on building from source will help get you started: https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md. I hope that helps at least point you in the right direction! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/359
https://github.com/google/deepvariant/issues/359:563,deployability,build,build-test,563,"Hi @yonatansc97 . Those imports you listed have something in common: they are in C++ (or protos) and therefore need to be compiled first. Inside the docker container, we have these already compiled into the binaries like `/opt/deepvariant/bin/make_examples`, while `make_examples.py` is the source, but it won't work without compiling its C++ dependencies. I don't know how to make this work with your special setup, but perhaps the documentation on building from source will help get you started: https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md. I hope that helps at least point you in the right direction! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/359
https://github.com/google/deepvariant/issues/359:343,integrability,depend,dependencies,343,"Hi @yonatansc97 . Those imports you listed have something in common: they are in C++ (or protos) and therefore need to be compiled first. Inside the docker container, we have these already compiled into the binaries like `/opt/deepvariant/bin/make_examples`, while `make_examples.py` is the source, but it won't work without compiling its C++ dependencies. I don't know how to make this work with your special setup, but perhaps the documentation on building from source will help get you started: https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md. I hope that helps at least point you in the right direction! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/359
https://github.com/google/deepvariant/issues/359:343,modifiability,depend,dependencies,343,"Hi @yonatansc97 . Those imports you listed have something in common: they are in C++ (or protos) and therefore need to be compiled first. Inside the docker container, we have these already compiled into the binaries like `/opt/deepvariant/bin/make_examples`, while `make_examples.py` is the source, but it won't work without compiling its C++ dependencies. I don't know how to make this work with your special setup, but perhaps the documentation on building from source will help get you started: https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md. I hope that helps at least point you in the right direction! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/359
https://github.com/google/deepvariant/issues/359:343,safety,depend,dependencies,343,"Hi @yonatansc97 . Those imports you listed have something in common: they are in C++ (or protos) and therefore need to be compiled first. Inside the docker container, we have these already compiled into the binaries like `/opt/deepvariant/bin/make_examples`, while `make_examples.py` is the source, but it won't work without compiling its C++ dependencies. I don't know how to make this work with your special setup, but perhaps the documentation on building from source will help get you started: https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md. I hope that helps at least point you in the right direction! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/359
https://github.com/google/deepvariant/issues/359:569,safety,test,test,569,"Hi @yonatansc97 . Those imports you listed have something in common: they are in C++ (or protos) and therefore need to be compiled first. Inside the docker container, we have these already compiled into the binaries like `/opt/deepvariant/bin/make_examples`, while `make_examples.py` is the source, but it won't work without compiling its C++ dependencies. I don't know how to make this work with your special setup, but perhaps the documentation on building from source will help get you started: https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md. I hope that helps at least point you in the right direction! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/359
https://github.com/google/deepvariant/issues/359:343,testability,depend,dependencies,343,"Hi @yonatansc97 . Those imports you listed have something in common: they are in C++ (or protos) and therefore need to be compiled first. Inside the docker container, we have these already compiled into the binaries like `/opt/deepvariant/bin/make_examples`, while `make_examples.py` is the source, but it won't work without compiling its C++ dependencies. I don't know how to make this work with your special setup, but perhaps the documentation on building from source will help get you started: https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md. I hope that helps at least point you in the right direction! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/359
https://github.com/google/deepvariant/issues/359:569,testability,test,test,569,"Hi @yonatansc97 . Those imports you listed have something in common: they are in C++ (or protos) and therefore need to be compiled first. Inside the docker container, we have these already compiled into the binaries like `/opt/deepvariant/bin/make_examples`, while `make_examples.py` is the source, but it won't work without compiling its C++ dependencies. I don't know how to make this work with your special setup, but perhaps the documentation on building from source will help get you started: https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md. I hope that helps at least point you in the right direction! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/359
https://github.com/google/deepvariant/issues/359:433,usability,document,documentation,433,"Hi @yonatansc97 . Those imports you listed have something in common: they are in C++ (or protos) and therefore need to be compiled first. Inside the docker container, we have these already compiled into the binaries like `/opt/deepvariant/bin/make_examples`, while `make_examples.py` is the source, but it won't work without compiling its C++ dependencies. I don't know how to make this work with your special setup, but perhaps the documentation on building from source will help get you started: https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md. I hope that helps at least point you in the right direction! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/359
https://github.com/google/deepvariant/issues/359:476,usability,help,help,476,"Hi @yonatansc97 . Those imports you listed have something in common: they are in C++ (or protos) and therefore need to be compiled first. Inside the docker container, we have these already compiled into the binaries like `/opt/deepvariant/bin/make_examples`, while `make_examples.py` is the source, but it won't work without compiling its C++ dependencies. I don't know how to make this work with your special setup, but perhaps the documentation on building from source will help get you started: https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md. I hope that helps at least point you in the right direction! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/359
https://github.com/google/deepvariant/issues/359:590,usability,help,helps,590,"Hi @yonatansc97 . Those imports you listed have something in common: they are in C++ (or protos) and therefore need to be compiled first. Inside the docker container, we have these already compiled into the binaries like `/opt/deepvariant/bin/make_examples`, while `make_examples.py` is the source, but it won't work without compiling its C++ dependencies. I don't know how to make this work with your special setup, but perhaps the documentation on building from source will help get you started: https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md. I hope that helps at least point you in the right direction! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/359
https://github.com/google/deepvariant/issues/359:33,deployability,build,build-prereq,33,"Hi again :). I tried running the build-prereq / run-prereq. Right now they fail on my machine. However, I feel like building it from scratch on kind of makes the docker useless (which I would like to make use of). . Is it possible that in order to have the entire code files (without the python packages to run it), I can perhaps run only certain lines in the build-prereq?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/359
https://github.com/google/deepvariant/issues/359:75,deployability,fail,fail,75,"Hi again :). I tried running the build-prereq / run-prereq. Right now they fail on my machine. However, I feel like building it from scratch on kind of makes the docker useless (which I would like to make use of). . Is it possible that in order to have the entire code files (without the python packages to run it), I can perhaps run only certain lines in the build-prereq?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/359
https://github.com/google/deepvariant/issues/359:116,deployability,build,building,116,"Hi again :). I tried running the build-prereq / run-prereq. Right now they fail on my machine. However, I feel like building it from scratch on kind of makes the docker useless (which I would like to make use of). . Is it possible that in order to have the entire code files (without the python packages to run it), I can perhaps run only certain lines in the build-prereq?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/359
https://github.com/google/deepvariant/issues/359:360,deployability,build,build-prereq,360,"Hi again :). I tried running the build-prereq / run-prereq. Right now they fail on my machine. However, I feel like building it from scratch on kind of makes the docker useless (which I would like to make use of). . Is it possible that in order to have the entire code files (without the python packages to run it), I can perhaps run only certain lines in the build-prereq?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/359
https://github.com/google/deepvariant/issues/359:295,modifiability,pac,packages,295,"Hi again :). I tried running the build-prereq / run-prereq. Right now they fail on my machine. However, I feel like building it from scratch on kind of makes the docker useless (which I would like to make use of). . Is it possible that in order to have the entire code files (without the python packages to run it), I can perhaps run only certain lines in the build-prereq?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/359
https://github.com/google/deepvariant/issues/359:75,reliability,fail,fail,75,"Hi again :). I tried running the build-prereq / run-prereq. Right now they fail on my machine. However, I feel like building it from scratch on kind of makes the docker useless (which I would like to make use of). . Is it possible that in order to have the entire code files (without the python packages to run it), I can perhaps run only certain lines in the build-prereq?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/359
https://github.com/google/deepvariant/issues/359:129,deployability,build,build,129,"Docker is very useful for running DeepVariant, but usually people who want to experiment with making changes in DeepVariant will build from source, and then they can always package that up into a Docker image afterwards. We don't really have much ability to support you in running DeepVariant line-by-line, since that's not how it's meant to be used. But you are of course welcome to experiment with it anyway, it just means we don't have bandwidth to help you get it working. I'll leave you with one resource you may find interesting: this blog post by Jason Chin at DNAnexus: https://blog.dnanexus.com/2019-02-19-deep-dive-into-deepvariant/. Good luck with your explorations! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/359
https://github.com/google/deepvariant/issues/359:501,deployability,resourc,resource,501,"Docker is very useful for running DeepVariant, but usually people who want to experiment with making changes in DeepVariant will build from source, and then they can always package that up into a Docker image afterwards. We don't really have much ability to support you in running DeepVariant line-by-line, since that's not how it's meant to be used. But you are of course welcome to experiment with it anyway, it just means we don't have bandwidth to help you get it working. I'll leave you with one resource you may find interesting: this blog post by Jason Chin at DNAnexus: https://blog.dnanexus.com/2019-02-19-deep-dive-into-deepvariant/. Good luck with your explorations! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/359
https://github.com/google/deepvariant/issues/359:501,energy efficiency,resourc,resource,501,"Docker is very useful for running DeepVariant, but usually people who want to experiment with making changes in DeepVariant will build from source, and then they can always package that up into a Docker image afterwards. We don't really have much ability to support you in running DeepVariant line-by-line, since that's not how it's meant to be used. But you are of course welcome to experiment with it anyway, it just means we don't have bandwidth to help you get it working. I'll leave you with one resource you may find interesting: this blog post by Jason Chin at DNAnexus: https://blog.dnanexus.com/2019-02-19-deep-dive-into-deepvariant/. Good luck with your explorations! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/359
https://github.com/google/deepvariant/issues/359:173,modifiability,pac,package,173,"Docker is very useful for running DeepVariant, but usually people who want to experiment with making changes in DeepVariant will build from source, and then they can always package that up into a Docker image afterwards. We don't really have much ability to support you in running DeepVariant line-by-line, since that's not how it's meant to be used. But you are of course welcome to experiment with it anyway, it just means we don't have bandwidth to help you get it working. I'll leave you with one resource you may find interesting: this blog post by Jason Chin at DNAnexus: https://blog.dnanexus.com/2019-02-19-deep-dive-into-deepvariant/. Good luck with your explorations! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/359
https://github.com/google/deepvariant/issues/359:501,performance,resourc,resource,501,"Docker is very useful for running DeepVariant, but usually people who want to experiment with making changes in DeepVariant will build from source, and then they can always package that up into a Docker image afterwards. We don't really have much ability to support you in running DeepVariant line-by-line, since that's not how it's meant to be used. But you are of course welcome to experiment with it anyway, it just means we don't have bandwidth to help you get it working. I'll leave you with one resource you may find interesting: this blog post by Jason Chin at DNAnexus: https://blog.dnanexus.com/2019-02-19-deep-dive-into-deepvariant/. Good luck with your explorations! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/359
https://github.com/google/deepvariant/issues/359:501,safety,resourc,resource,501,"Docker is very useful for running DeepVariant, but usually people who want to experiment with making changes in DeepVariant will build from source, and then they can always package that up into a Docker image afterwards. We don't really have much ability to support you in running DeepVariant line-by-line, since that's not how it's meant to be used. But you are of course welcome to experiment with it anyway, it just means we don't have bandwidth to help you get it working. I'll leave you with one resource you may find interesting: this blog post by Jason Chin at DNAnexus: https://blog.dnanexus.com/2019-02-19-deep-dive-into-deepvariant/. Good luck with your explorations! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/359
https://github.com/google/deepvariant/issues/359:501,testability,resourc,resource,501,"Docker is very useful for running DeepVariant, but usually people who want to experiment with making changes in DeepVariant will build from source, and then they can always package that up into a Docker image afterwards. We don't really have much ability to support you in running DeepVariant line-by-line, since that's not how it's meant to be used. But you are of course welcome to experiment with it anyway, it just means we don't have bandwidth to help you get it working. I'll leave you with one resource you may find interesting: this blog post by Jason Chin at DNAnexus: https://blog.dnanexus.com/2019-02-19-deep-dive-into-deepvariant/. Good luck with your explorations! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/359
https://github.com/google/deepvariant/issues/359:258,usability,support,support,258,"Docker is very useful for running DeepVariant, but usually people who want to experiment with making changes in DeepVariant will build from source, and then they can always package that up into a Docker image afterwards. We don't really have much ability to support you in running DeepVariant line-by-line, since that's not how it's meant to be used. But you are of course welcome to experiment with it anyway, it just means we don't have bandwidth to help you get it working. I'll leave you with one resource you may find interesting: this blog post by Jason Chin at DNAnexus: https://blog.dnanexus.com/2019-02-19-deep-dive-into-deepvariant/. Good luck with your explorations! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/359
https://github.com/google/deepvariant/issues/359:452,usability,help,help,452,"Docker is very useful for running DeepVariant, but usually people who want to experiment with making changes in DeepVariant will build from source, and then they can always package that up into a Docker image afterwards. We don't really have much ability to support you in running DeepVariant line-by-line, since that's not how it's meant to be used. But you are of course welcome to experiment with it anyway, it just means we don't have bandwidth to help you get it working. I'll leave you with one resource you may find interesting: this blog post by Jason Chin at DNAnexus: https://blog.dnanexus.com/2019-02-19-deep-dive-into-deepvariant/. Good luck with your explorations! Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/359
https://github.com/google/deepvariant/issues/360:46,performance,memor,memory,46,"Hi @anands-repo , when you say it runs out of memory, are you using `DataflowRunner` like in our documentation?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:46,usability,memor,memory,46,"Hi @anands-repo , when you say it runs out of memory, are you using `DataflowRunner` like in our documentation?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:97,usability,document,documentation,97,"Hi @anands-repo , when you say it runs out of memory, are you using `DataflowRunner` like in our documentation?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:206,availability,error,errors,206,"Hi @pichuan . I am not running on Google Cloud, but on a local machine. So I went with the default runner. When I use DataflowRunner the shuffle script requests arguments relevant to GCS. For example I get errors such as:. ```Invalid GCS path (<PATH>), given for the option: temp_location```. <PATH> here is actually a valid path in my machine. Kindly advise. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:41,energy efficiency,Cloud,Cloud,41,"Hi @pichuan . I am not running on Google Cloud, but on a local machine. So I went with the default runner. When I use DataflowRunner the shuffle script requests arguments relevant to GCS. For example I get errors such as:. ```Invalid GCS path (<PATH>), given for the option: temp_location```. <PATH> here is actually a valid path in my machine. Kindly advise. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:206,performance,error,errors,206,"Hi @pichuan . I am not running on Google Cloud, but on a local machine. So I went with the default runner. When I use DataflowRunner the shuffle script requests arguments relevant to GCS. For example I get errors such as:. ```Invalid GCS path (<PATH>), given for the option: temp_location```. <PATH> here is actually a valid path in my machine. Kindly advise. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:206,safety,error,errors,206,"Hi @pichuan . I am not running on Google Cloud, but on a local machine. So I went with the default runner. When I use DataflowRunner the shuffle script requests arguments relevant to GCS. For example I get errors such as:. ```Invalid GCS path (<PATH>), given for the option: temp_location```. <PATH> here is actually a valid path in my machine. Kindly advise. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:319,safety,valid,valid,319,"Hi @pichuan . I am not running on Google Cloud, but on a local machine. So I went with the default runner. When I use DataflowRunner the shuffle script requests arguments relevant to GCS. For example I get errors such as:. ```Invalid GCS path (<PATH>), given for the option: temp_location```. <PATH> here is actually a valid path in my machine. Kindly advise. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:206,usability,error,errors,206,"Hi @pichuan . I am not running on Google Cloud, but on a local machine. So I went with the default runner. When I use DataflowRunner the shuffle script requests arguments relevant to GCS. For example I get errors such as:. ```Invalid GCS path (<PATH>), given for the option: temp_location```. <PATH> here is actually a valid path in my machine. Kindly advise. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:333,availability,error,error,333,"Command that works, but runs out of memory is this:. ```. python $SCRIPTPATH/shuffle_tfrecords_beam.py \. --input_pattern_list=$INPUT_PATTERN \. --output_pattern_prefix=$OUTPUT_PREFIX \. --output_dataset_config_pbtxt=$OUTPUT_DATASET_CONFIG_PBTXT \. --output_dataset_name=$OUTPUT_DATASET_NAME. ```. Command that doesn't run and gives error:. ```. python $SCRIPTPATH/shuffle_tfrecords_beam.py \. --input_pattern_list=$INPUT_PATTERN \. --output_pattern_prefix=$OUTPUT_PREFIX \. --output_dataset_config_pbtxt=$OUTPUT_DATASET_CONFIG_PBTXT \. --output_dataset_name=$OUTPUT_DATASET_NAME \. --job_name=$JOBNAME \. --project=$PROJECT_NAME \. --temp_location=$TEMPLOCATION \. --save_main_session \. --region us-east1. ```. Obtained error: ```Invalid GCS path (<PATH>), given for the option: temp_location```. I also tried the SparkRunner which works, but which runs into the same issue of memory. It seems DirectRunner and SparkRunner try to shuffle everything in memory (RAM) and do not use local storage. May be DataflowRunner uses local storage (it accepts a --temp_location argument)? However, this is not available to me on my local machine since the DataflowRunner seems to require the code to be run on Google Cloud.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:722,availability,error,error,722,"Command that works, but runs out of memory is this:. ```. python $SCRIPTPATH/shuffle_tfrecords_beam.py \. --input_pattern_list=$INPUT_PATTERN \. --output_pattern_prefix=$OUTPUT_PREFIX \. --output_dataset_config_pbtxt=$OUTPUT_DATASET_CONFIG_PBTXT \. --output_dataset_name=$OUTPUT_DATASET_NAME. ```. Command that doesn't run and gives error:. ```. python $SCRIPTPATH/shuffle_tfrecords_beam.py \. --input_pattern_list=$INPUT_PATTERN \. --output_pattern_prefix=$OUTPUT_PREFIX \. --output_dataset_config_pbtxt=$OUTPUT_DATASET_CONFIG_PBTXT \. --output_dataset_name=$OUTPUT_DATASET_NAME \. --job_name=$JOBNAME \. --project=$PROJECT_NAME \. --temp_location=$TEMPLOCATION \. --save_main_session \. --region us-east1. ```. Obtained error: ```Invalid GCS path (<PATH>), given for the option: temp_location```. I also tried the SparkRunner which works, but which runs into the same issue of memory. It seems DirectRunner and SparkRunner try to shuffle everything in memory (RAM) and do not use local storage. May be DataflowRunner uses local storage (it accepts a --temp_location argument)? However, this is not available to me on my local machine since the DataflowRunner seems to require the code to be run on Google Cloud.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:1100,availability,avail,available,1100,"Command that works, but runs out of memory is this:. ```. python $SCRIPTPATH/shuffle_tfrecords_beam.py \. --input_pattern_list=$INPUT_PATTERN \. --output_pattern_prefix=$OUTPUT_PREFIX \. --output_dataset_config_pbtxt=$OUTPUT_DATASET_CONFIG_PBTXT \. --output_dataset_name=$OUTPUT_DATASET_NAME. ```. Command that doesn't run and gives error:. ```. python $SCRIPTPATH/shuffle_tfrecords_beam.py \. --input_pattern_list=$INPUT_PATTERN \. --output_pattern_prefix=$OUTPUT_PREFIX \. --output_dataset_config_pbtxt=$OUTPUT_DATASET_CONFIG_PBTXT \. --output_dataset_name=$OUTPUT_DATASET_NAME \. --job_name=$JOBNAME \. --project=$PROJECT_NAME \. --temp_location=$TEMPLOCATION \. --save_main_session \. --region us-east1. ```. Obtained error: ```Invalid GCS path (<PATH>), given for the option: temp_location```. I also tried the SparkRunner which works, but which runs into the same issue of memory. It seems DirectRunner and SparkRunner try to shuffle everything in memory (RAM) and do not use local storage. May be DataflowRunner uses local storage (it accepts a --temp_location argument)? However, this is not available to me on my local machine since the DataflowRunner seems to require the code to be run on Google Cloud.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:1207,energy efficiency,Cloud,Cloud,1207,"Command that works, but runs out of memory is this:. ```. python $SCRIPTPATH/shuffle_tfrecords_beam.py \. --input_pattern_list=$INPUT_PATTERN \. --output_pattern_prefix=$OUTPUT_PREFIX \. --output_dataset_config_pbtxt=$OUTPUT_DATASET_CONFIG_PBTXT \. --output_dataset_name=$OUTPUT_DATASET_NAME. ```. Command that doesn't run and gives error:. ```. python $SCRIPTPATH/shuffle_tfrecords_beam.py \. --input_pattern_list=$INPUT_PATTERN \. --output_pattern_prefix=$OUTPUT_PREFIX \. --output_dataset_config_pbtxt=$OUTPUT_DATASET_CONFIG_PBTXT \. --output_dataset_name=$OUTPUT_DATASET_NAME \. --job_name=$JOBNAME \. --project=$PROJECT_NAME \. --temp_location=$TEMPLOCATION \. --save_main_session \. --region us-east1. ```. Obtained error: ```Invalid GCS path (<PATH>), given for the option: temp_location```. I also tried the SparkRunner which works, but which runs into the same issue of memory. It seems DirectRunner and SparkRunner try to shuffle everything in memory (RAM) and do not use local storage. May be DataflowRunner uses local storage (it accepts a --temp_location argument)? However, this is not available to me on my local machine since the DataflowRunner seems to require the code to be run on Google Cloud.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:36,performance,memor,memory,36,"Command that works, but runs out of memory is this:. ```. python $SCRIPTPATH/shuffle_tfrecords_beam.py \. --input_pattern_list=$INPUT_PATTERN \. --output_pattern_prefix=$OUTPUT_PREFIX \. --output_dataset_config_pbtxt=$OUTPUT_DATASET_CONFIG_PBTXT \. --output_dataset_name=$OUTPUT_DATASET_NAME. ```. Command that doesn't run and gives error:. ```. python $SCRIPTPATH/shuffle_tfrecords_beam.py \. --input_pattern_list=$INPUT_PATTERN \. --output_pattern_prefix=$OUTPUT_PREFIX \. --output_dataset_config_pbtxt=$OUTPUT_DATASET_CONFIG_PBTXT \. --output_dataset_name=$OUTPUT_DATASET_NAME \. --job_name=$JOBNAME \. --project=$PROJECT_NAME \. --temp_location=$TEMPLOCATION \. --save_main_session \. --region us-east1. ```. Obtained error: ```Invalid GCS path (<PATH>), given for the option: temp_location```. I also tried the SparkRunner which works, but which runs into the same issue of memory. It seems DirectRunner and SparkRunner try to shuffle everything in memory (RAM) and do not use local storage. May be DataflowRunner uses local storage (it accepts a --temp_location argument)? However, this is not available to me on my local machine since the DataflowRunner seems to require the code to be run on Google Cloud.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:333,performance,error,error,333,"Command that works, but runs out of memory is this:. ```. python $SCRIPTPATH/shuffle_tfrecords_beam.py \. --input_pattern_list=$INPUT_PATTERN \. --output_pattern_prefix=$OUTPUT_PREFIX \. --output_dataset_config_pbtxt=$OUTPUT_DATASET_CONFIG_PBTXT \. --output_dataset_name=$OUTPUT_DATASET_NAME. ```. Command that doesn't run and gives error:. ```. python $SCRIPTPATH/shuffle_tfrecords_beam.py \. --input_pattern_list=$INPUT_PATTERN \. --output_pattern_prefix=$OUTPUT_PREFIX \. --output_dataset_config_pbtxt=$OUTPUT_DATASET_CONFIG_PBTXT \. --output_dataset_name=$OUTPUT_DATASET_NAME \. --job_name=$JOBNAME \. --project=$PROJECT_NAME \. --temp_location=$TEMPLOCATION \. --save_main_session \. --region us-east1. ```. Obtained error: ```Invalid GCS path (<PATH>), given for the option: temp_location```. I also tried the SparkRunner which works, but which runs into the same issue of memory. It seems DirectRunner and SparkRunner try to shuffle everything in memory (RAM) and do not use local storage. May be DataflowRunner uses local storage (it accepts a --temp_location argument)? However, this is not available to me on my local machine since the DataflowRunner seems to require the code to be run on Google Cloud.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:722,performance,error,error,722,"Command that works, but runs out of memory is this:. ```. python $SCRIPTPATH/shuffle_tfrecords_beam.py \. --input_pattern_list=$INPUT_PATTERN \. --output_pattern_prefix=$OUTPUT_PREFIX \. --output_dataset_config_pbtxt=$OUTPUT_DATASET_CONFIG_PBTXT \. --output_dataset_name=$OUTPUT_DATASET_NAME. ```. Command that doesn't run and gives error:. ```. python $SCRIPTPATH/shuffle_tfrecords_beam.py \. --input_pattern_list=$INPUT_PATTERN \. --output_pattern_prefix=$OUTPUT_PREFIX \. --output_dataset_config_pbtxt=$OUTPUT_DATASET_CONFIG_PBTXT \. --output_dataset_name=$OUTPUT_DATASET_NAME \. --job_name=$JOBNAME \. --project=$PROJECT_NAME \. --temp_location=$TEMPLOCATION \. --save_main_session \. --region us-east1. ```. Obtained error: ```Invalid GCS path (<PATH>), given for the option: temp_location```. I also tried the SparkRunner which works, but which runs into the same issue of memory. It seems DirectRunner and SparkRunner try to shuffle everything in memory (RAM) and do not use local storage. May be DataflowRunner uses local storage (it accepts a --temp_location argument)? However, this is not available to me on my local machine since the DataflowRunner seems to require the code to be run on Google Cloud.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:879,performance,memor,memory,879,"Command that works, but runs out of memory is this:. ```. python $SCRIPTPATH/shuffle_tfrecords_beam.py \. --input_pattern_list=$INPUT_PATTERN \. --output_pattern_prefix=$OUTPUT_PREFIX \. --output_dataset_config_pbtxt=$OUTPUT_DATASET_CONFIG_PBTXT \. --output_dataset_name=$OUTPUT_DATASET_NAME. ```. Command that doesn't run and gives error:. ```. python $SCRIPTPATH/shuffle_tfrecords_beam.py \. --input_pattern_list=$INPUT_PATTERN \. --output_pattern_prefix=$OUTPUT_PREFIX \. --output_dataset_config_pbtxt=$OUTPUT_DATASET_CONFIG_PBTXT \. --output_dataset_name=$OUTPUT_DATASET_NAME \. --job_name=$JOBNAME \. --project=$PROJECT_NAME \. --temp_location=$TEMPLOCATION \. --save_main_session \. --region us-east1. ```. Obtained error: ```Invalid GCS path (<PATH>), given for the option: temp_location```. I also tried the SparkRunner which works, but which runs into the same issue of memory. It seems DirectRunner and SparkRunner try to shuffle everything in memory (RAM) and do not use local storage. May be DataflowRunner uses local storage (it accepts a --temp_location argument)? However, this is not available to me on my local machine since the DataflowRunner seems to require the code to be run on Google Cloud.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:954,performance,memor,memory,954,"Command that works, but runs out of memory is this:. ```. python $SCRIPTPATH/shuffle_tfrecords_beam.py \. --input_pattern_list=$INPUT_PATTERN \. --output_pattern_prefix=$OUTPUT_PREFIX \. --output_dataset_config_pbtxt=$OUTPUT_DATASET_CONFIG_PBTXT \. --output_dataset_name=$OUTPUT_DATASET_NAME. ```. Command that doesn't run and gives error:. ```. python $SCRIPTPATH/shuffle_tfrecords_beam.py \. --input_pattern_list=$INPUT_PATTERN \. --output_pattern_prefix=$OUTPUT_PREFIX \. --output_dataset_config_pbtxt=$OUTPUT_DATASET_CONFIG_PBTXT \. --output_dataset_name=$OUTPUT_DATASET_NAME \. --job_name=$JOBNAME \. --project=$PROJECT_NAME \. --temp_location=$TEMPLOCATION \. --save_main_session \. --region us-east1. ```. Obtained error: ```Invalid GCS path (<PATH>), given for the option: temp_location```. I also tried the SparkRunner which works, but which runs into the same issue of memory. It seems DirectRunner and SparkRunner try to shuffle everything in memory (RAM) and do not use local storage. May be DataflowRunner uses local storage (it accepts a --temp_location argument)? However, this is not available to me on my local machine since the DataflowRunner seems to require the code to be run on Google Cloud.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:311,reliability,doe,doesn,311,"Command that works, but runs out of memory is this:. ```. python $SCRIPTPATH/shuffle_tfrecords_beam.py \. --input_pattern_list=$INPUT_PATTERN \. --output_pattern_prefix=$OUTPUT_PREFIX \. --output_dataset_config_pbtxt=$OUTPUT_DATASET_CONFIG_PBTXT \. --output_dataset_name=$OUTPUT_DATASET_NAME. ```. Command that doesn't run and gives error:. ```. python $SCRIPTPATH/shuffle_tfrecords_beam.py \. --input_pattern_list=$INPUT_PATTERN \. --output_pattern_prefix=$OUTPUT_PREFIX \. --output_dataset_config_pbtxt=$OUTPUT_DATASET_CONFIG_PBTXT \. --output_dataset_name=$OUTPUT_DATASET_NAME \. --job_name=$JOBNAME \. --project=$PROJECT_NAME \. --temp_location=$TEMPLOCATION \. --save_main_session \. --region us-east1. ```. Obtained error: ```Invalid GCS path (<PATH>), given for the option: temp_location```. I also tried the SparkRunner which works, but which runs into the same issue of memory. It seems DirectRunner and SparkRunner try to shuffle everything in memory (RAM) and do not use local storage. May be DataflowRunner uses local storage (it accepts a --temp_location argument)? However, this is not available to me on my local machine since the DataflowRunner seems to require the code to be run on Google Cloud.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:1100,reliability,availab,available,1100,"Command that works, but runs out of memory is this:. ```. python $SCRIPTPATH/shuffle_tfrecords_beam.py \. --input_pattern_list=$INPUT_PATTERN \. --output_pattern_prefix=$OUTPUT_PREFIX \. --output_dataset_config_pbtxt=$OUTPUT_DATASET_CONFIG_PBTXT \. --output_dataset_name=$OUTPUT_DATASET_NAME. ```. Command that doesn't run and gives error:. ```. python $SCRIPTPATH/shuffle_tfrecords_beam.py \. --input_pattern_list=$INPUT_PATTERN \. --output_pattern_prefix=$OUTPUT_PREFIX \. --output_dataset_config_pbtxt=$OUTPUT_DATASET_CONFIG_PBTXT \. --output_dataset_name=$OUTPUT_DATASET_NAME \. --job_name=$JOBNAME \. --project=$PROJECT_NAME \. --temp_location=$TEMPLOCATION \. --save_main_session \. --region us-east1. ```. Obtained error: ```Invalid GCS path (<PATH>), given for the option: temp_location```. I also tried the SparkRunner which works, but which runs into the same issue of memory. It seems DirectRunner and SparkRunner try to shuffle everything in memory (RAM) and do not use local storage. May be DataflowRunner uses local storage (it accepts a --temp_location argument)? However, this is not available to me on my local machine since the DataflowRunner seems to require the code to be run on Google Cloud.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:333,safety,error,error,333,"Command that works, but runs out of memory is this:. ```. python $SCRIPTPATH/shuffle_tfrecords_beam.py \. --input_pattern_list=$INPUT_PATTERN \. --output_pattern_prefix=$OUTPUT_PREFIX \. --output_dataset_config_pbtxt=$OUTPUT_DATASET_CONFIG_PBTXT \. --output_dataset_name=$OUTPUT_DATASET_NAME. ```. Command that doesn't run and gives error:. ```. python $SCRIPTPATH/shuffle_tfrecords_beam.py \. --input_pattern_list=$INPUT_PATTERN \. --output_pattern_prefix=$OUTPUT_PREFIX \. --output_dataset_config_pbtxt=$OUTPUT_DATASET_CONFIG_PBTXT \. --output_dataset_name=$OUTPUT_DATASET_NAME \. --job_name=$JOBNAME \. --project=$PROJECT_NAME \. --temp_location=$TEMPLOCATION \. --save_main_session \. --region us-east1. ```. Obtained error: ```Invalid GCS path (<PATH>), given for the option: temp_location```. I also tried the SparkRunner which works, but which runs into the same issue of memory. It seems DirectRunner and SparkRunner try to shuffle everything in memory (RAM) and do not use local storage. May be DataflowRunner uses local storage (it accepts a --temp_location argument)? However, this is not available to me on my local machine since the DataflowRunner seems to require the code to be run on Google Cloud.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:722,safety,error,error,722,"Command that works, but runs out of memory is this:. ```. python $SCRIPTPATH/shuffle_tfrecords_beam.py \. --input_pattern_list=$INPUT_PATTERN \. --output_pattern_prefix=$OUTPUT_PREFIX \. --output_dataset_config_pbtxt=$OUTPUT_DATASET_CONFIG_PBTXT \. --output_dataset_name=$OUTPUT_DATASET_NAME. ```. Command that doesn't run and gives error:. ```. python $SCRIPTPATH/shuffle_tfrecords_beam.py \. --input_pattern_list=$INPUT_PATTERN \. --output_pattern_prefix=$OUTPUT_PREFIX \. --output_dataset_config_pbtxt=$OUTPUT_DATASET_CONFIG_PBTXT \. --output_dataset_name=$OUTPUT_DATASET_NAME \. --job_name=$JOBNAME \. --project=$PROJECT_NAME \. --temp_location=$TEMPLOCATION \. --save_main_session \. --region us-east1. ```. Obtained error: ```Invalid GCS path (<PATH>), given for the option: temp_location```. I also tried the SparkRunner which works, but which runs into the same issue of memory. It seems DirectRunner and SparkRunner try to shuffle everything in memory (RAM) and do not use local storage. May be DataflowRunner uses local storage (it accepts a --temp_location argument)? However, this is not available to me on my local machine since the DataflowRunner seems to require the code to be run on Google Cloud.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:1100,safety,avail,available,1100,"Command that works, but runs out of memory is this:. ```. python $SCRIPTPATH/shuffle_tfrecords_beam.py \. --input_pattern_list=$INPUT_PATTERN \. --output_pattern_prefix=$OUTPUT_PREFIX \. --output_dataset_config_pbtxt=$OUTPUT_DATASET_CONFIG_PBTXT \. --output_dataset_name=$OUTPUT_DATASET_NAME. ```. Command that doesn't run and gives error:. ```. python $SCRIPTPATH/shuffle_tfrecords_beam.py \. --input_pattern_list=$INPUT_PATTERN \. --output_pattern_prefix=$OUTPUT_PREFIX \. --output_dataset_config_pbtxt=$OUTPUT_DATASET_CONFIG_PBTXT \. --output_dataset_name=$OUTPUT_DATASET_NAME \. --job_name=$JOBNAME \. --project=$PROJECT_NAME \. --temp_location=$TEMPLOCATION \. --save_main_session \. --region us-east1. ```. Obtained error: ```Invalid GCS path (<PATH>), given for the option: temp_location```. I also tried the SparkRunner which works, but which runs into the same issue of memory. It seems DirectRunner and SparkRunner try to shuffle everything in memory (RAM) and do not use local storage. May be DataflowRunner uses local storage (it accepts a --temp_location argument)? However, this is not available to me on my local machine since the DataflowRunner seems to require the code to be run on Google Cloud.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:1100,security,availab,available,1100,"Command that works, but runs out of memory is this:. ```. python $SCRIPTPATH/shuffle_tfrecords_beam.py \. --input_pattern_list=$INPUT_PATTERN \. --output_pattern_prefix=$OUTPUT_PREFIX \. --output_dataset_config_pbtxt=$OUTPUT_DATASET_CONFIG_PBTXT \. --output_dataset_name=$OUTPUT_DATASET_NAME. ```. Command that doesn't run and gives error:. ```. python $SCRIPTPATH/shuffle_tfrecords_beam.py \. --input_pattern_list=$INPUT_PATTERN \. --output_pattern_prefix=$OUTPUT_PREFIX \. --output_dataset_config_pbtxt=$OUTPUT_DATASET_CONFIG_PBTXT \. --output_dataset_name=$OUTPUT_DATASET_NAME \. --job_name=$JOBNAME \. --project=$PROJECT_NAME \. --temp_location=$TEMPLOCATION \. --save_main_session \. --region us-east1. ```. Obtained error: ```Invalid GCS path (<PATH>), given for the option: temp_location```. I also tried the SparkRunner which works, but which runs into the same issue of memory. It seems DirectRunner and SparkRunner try to shuffle everything in memory (RAM) and do not use local storage. May be DataflowRunner uses local storage (it accepts a --temp_location argument)? However, this is not available to me on my local machine since the DataflowRunner seems to require the code to be run on Google Cloud.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:0,usability,Command,Command,0,"Command that works, but runs out of memory is this:. ```. python $SCRIPTPATH/shuffle_tfrecords_beam.py \. --input_pattern_list=$INPUT_PATTERN \. --output_pattern_prefix=$OUTPUT_PREFIX \. --output_dataset_config_pbtxt=$OUTPUT_DATASET_CONFIG_PBTXT \. --output_dataset_name=$OUTPUT_DATASET_NAME. ```. Command that doesn't run and gives error:. ```. python $SCRIPTPATH/shuffle_tfrecords_beam.py \. --input_pattern_list=$INPUT_PATTERN \. --output_pattern_prefix=$OUTPUT_PREFIX \. --output_dataset_config_pbtxt=$OUTPUT_DATASET_CONFIG_PBTXT \. --output_dataset_name=$OUTPUT_DATASET_NAME \. --job_name=$JOBNAME \. --project=$PROJECT_NAME \. --temp_location=$TEMPLOCATION \. --save_main_session \. --region us-east1. ```. Obtained error: ```Invalid GCS path (<PATH>), given for the option: temp_location```. I also tried the SparkRunner which works, but which runs into the same issue of memory. It seems DirectRunner and SparkRunner try to shuffle everything in memory (RAM) and do not use local storage. May be DataflowRunner uses local storage (it accepts a --temp_location argument)? However, this is not available to me on my local machine since the DataflowRunner seems to require the code to be run on Google Cloud.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:36,usability,memor,memory,36,"Command that works, but runs out of memory is this:. ```. python $SCRIPTPATH/shuffle_tfrecords_beam.py \. --input_pattern_list=$INPUT_PATTERN \. --output_pattern_prefix=$OUTPUT_PREFIX \. --output_dataset_config_pbtxt=$OUTPUT_DATASET_CONFIG_PBTXT \. --output_dataset_name=$OUTPUT_DATASET_NAME. ```. Command that doesn't run and gives error:. ```. python $SCRIPTPATH/shuffle_tfrecords_beam.py \. --input_pattern_list=$INPUT_PATTERN \. --output_pattern_prefix=$OUTPUT_PREFIX \. --output_dataset_config_pbtxt=$OUTPUT_DATASET_CONFIG_PBTXT \. --output_dataset_name=$OUTPUT_DATASET_NAME \. --job_name=$JOBNAME \. --project=$PROJECT_NAME \. --temp_location=$TEMPLOCATION \. --save_main_session \. --region us-east1. ```. Obtained error: ```Invalid GCS path (<PATH>), given for the option: temp_location```. I also tried the SparkRunner which works, but which runs into the same issue of memory. It seems DirectRunner and SparkRunner try to shuffle everything in memory (RAM) and do not use local storage. May be DataflowRunner uses local storage (it accepts a --temp_location argument)? However, this is not available to me on my local machine since the DataflowRunner seems to require the code to be run on Google Cloud.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:298,usability,Command,Command,298,"Command that works, but runs out of memory is this:. ```. python $SCRIPTPATH/shuffle_tfrecords_beam.py \. --input_pattern_list=$INPUT_PATTERN \. --output_pattern_prefix=$OUTPUT_PREFIX \. --output_dataset_config_pbtxt=$OUTPUT_DATASET_CONFIG_PBTXT \. --output_dataset_name=$OUTPUT_DATASET_NAME. ```. Command that doesn't run and gives error:. ```. python $SCRIPTPATH/shuffle_tfrecords_beam.py \. --input_pattern_list=$INPUT_PATTERN \. --output_pattern_prefix=$OUTPUT_PREFIX \. --output_dataset_config_pbtxt=$OUTPUT_DATASET_CONFIG_PBTXT \. --output_dataset_name=$OUTPUT_DATASET_NAME \. --job_name=$JOBNAME \. --project=$PROJECT_NAME \. --temp_location=$TEMPLOCATION \. --save_main_session \. --region us-east1. ```. Obtained error: ```Invalid GCS path (<PATH>), given for the option: temp_location```. I also tried the SparkRunner which works, but which runs into the same issue of memory. It seems DirectRunner and SparkRunner try to shuffle everything in memory (RAM) and do not use local storage. May be DataflowRunner uses local storage (it accepts a --temp_location argument)? However, this is not available to me on my local machine since the DataflowRunner seems to require the code to be run on Google Cloud.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:333,usability,error,error,333,"Command that works, but runs out of memory is this:. ```. python $SCRIPTPATH/shuffle_tfrecords_beam.py \. --input_pattern_list=$INPUT_PATTERN \. --output_pattern_prefix=$OUTPUT_PREFIX \. --output_dataset_config_pbtxt=$OUTPUT_DATASET_CONFIG_PBTXT \. --output_dataset_name=$OUTPUT_DATASET_NAME. ```. Command that doesn't run and gives error:. ```. python $SCRIPTPATH/shuffle_tfrecords_beam.py \. --input_pattern_list=$INPUT_PATTERN \. --output_pattern_prefix=$OUTPUT_PREFIX \. --output_dataset_config_pbtxt=$OUTPUT_DATASET_CONFIG_PBTXT \. --output_dataset_name=$OUTPUT_DATASET_NAME \. --job_name=$JOBNAME \. --project=$PROJECT_NAME \. --temp_location=$TEMPLOCATION \. --save_main_session \. --region us-east1. ```. Obtained error: ```Invalid GCS path (<PATH>), given for the option: temp_location```. I also tried the SparkRunner which works, but which runs into the same issue of memory. It seems DirectRunner and SparkRunner try to shuffle everything in memory (RAM) and do not use local storage. May be DataflowRunner uses local storage (it accepts a --temp_location argument)? However, this is not available to me on my local machine since the DataflowRunner seems to require the code to be run on Google Cloud.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:722,usability,error,error,722,"Command that works, but runs out of memory is this:. ```. python $SCRIPTPATH/shuffle_tfrecords_beam.py \. --input_pattern_list=$INPUT_PATTERN \. --output_pattern_prefix=$OUTPUT_PREFIX \. --output_dataset_config_pbtxt=$OUTPUT_DATASET_CONFIG_PBTXT \. --output_dataset_name=$OUTPUT_DATASET_NAME. ```. Command that doesn't run and gives error:. ```. python $SCRIPTPATH/shuffle_tfrecords_beam.py \. --input_pattern_list=$INPUT_PATTERN \. --output_pattern_prefix=$OUTPUT_PREFIX \. --output_dataset_config_pbtxt=$OUTPUT_DATASET_CONFIG_PBTXT \. --output_dataset_name=$OUTPUT_DATASET_NAME \. --job_name=$JOBNAME \. --project=$PROJECT_NAME \. --temp_location=$TEMPLOCATION \. --save_main_session \. --region us-east1. ```. Obtained error: ```Invalid GCS path (<PATH>), given for the option: temp_location```. I also tried the SparkRunner which works, but which runs into the same issue of memory. It seems DirectRunner and SparkRunner try to shuffle everything in memory (RAM) and do not use local storage. May be DataflowRunner uses local storage (it accepts a --temp_location argument)? However, this is not available to me on my local machine since the DataflowRunner seems to require the code to be run on Google Cloud.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:879,usability,memor,memory,879,"Command that works, but runs out of memory is this:. ```. python $SCRIPTPATH/shuffle_tfrecords_beam.py \. --input_pattern_list=$INPUT_PATTERN \. --output_pattern_prefix=$OUTPUT_PREFIX \. --output_dataset_config_pbtxt=$OUTPUT_DATASET_CONFIG_PBTXT \. --output_dataset_name=$OUTPUT_DATASET_NAME. ```. Command that doesn't run and gives error:. ```. python $SCRIPTPATH/shuffle_tfrecords_beam.py \. --input_pattern_list=$INPUT_PATTERN \. --output_pattern_prefix=$OUTPUT_PREFIX \. --output_dataset_config_pbtxt=$OUTPUT_DATASET_CONFIG_PBTXT \. --output_dataset_name=$OUTPUT_DATASET_NAME \. --job_name=$JOBNAME \. --project=$PROJECT_NAME \. --temp_location=$TEMPLOCATION \. --save_main_session \. --region us-east1. ```. Obtained error: ```Invalid GCS path (<PATH>), given for the option: temp_location```. I also tried the SparkRunner which works, but which runs into the same issue of memory. It seems DirectRunner and SparkRunner try to shuffle everything in memory (RAM) and do not use local storage. May be DataflowRunner uses local storage (it accepts a --temp_location argument)? However, this is not available to me on my local machine since the DataflowRunner seems to require the code to be run on Google Cloud.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:954,usability,memor,memory,954,"Command that works, but runs out of memory is this:. ```. python $SCRIPTPATH/shuffle_tfrecords_beam.py \. --input_pattern_list=$INPUT_PATTERN \. --output_pattern_prefix=$OUTPUT_PREFIX \. --output_dataset_config_pbtxt=$OUTPUT_DATASET_CONFIG_PBTXT \. --output_dataset_name=$OUTPUT_DATASET_NAME. ```. Command that doesn't run and gives error:. ```. python $SCRIPTPATH/shuffle_tfrecords_beam.py \. --input_pattern_list=$INPUT_PATTERN \. --output_pattern_prefix=$OUTPUT_PREFIX \. --output_dataset_config_pbtxt=$OUTPUT_DATASET_CONFIG_PBTXT \. --output_dataset_name=$OUTPUT_DATASET_NAME \. --job_name=$JOBNAME \. --project=$PROJECT_NAME \. --temp_location=$TEMPLOCATION \. --save_main_session \. --region us-east1. ```. Obtained error: ```Invalid GCS path (<PATH>), given for the option: temp_location```. I also tried the SparkRunner which works, but which runs into the same issue of memory. It seems DirectRunner and SparkRunner try to shuffle everything in memory (RAM) and do not use local storage. May be DataflowRunner uses local storage (it accepts a --temp_location argument)? However, this is not available to me on my local machine since the DataflowRunner seems to require the code to be run on Google Cloud.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:68,interoperability,distribut,distributed,68,"Hi @anands-repo . The point of using Dataflow is to run things in a distributed fashion, which means it shouldn't be running on your local machine. I assume that Spark runner should allow you to run with distributed workers as well if you set it up correctly, but I have never used it myself. Our team doesn't support different distributed setup for Beam. Please refer to https://beam.apache.org/documentation/runners/spark/ to see if you can set up Sparker runner so that it doesn't use your local memory. If you do figure out a good setup to run on Spark, please feel free to share some tips here so people can use it in the future! If you can't use the shuffle script, you can consider a less fine-grained shuffle ""hack"" in this older document http://bit.ly/train-deepvariant (Note that this doc is a one-off document, and is not maintained by our team. Please consider it as a possible example that you'll probably need to tweak for your own use case).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:204,interoperability,distribut,distributed,204,"Hi @anands-repo . The point of using Dataflow is to run things in a distributed fashion, which means it shouldn't be running on your local machine. I assume that Spark runner should allow you to run with distributed workers as well if you set it up correctly, but I have never used it myself. Our team doesn't support different distributed setup for Beam. Please refer to https://beam.apache.org/documentation/runners/spark/ to see if you can set up Sparker runner so that it doesn't use your local memory. If you do figure out a good setup to run on Spark, please feel free to share some tips here so people can use it in the future! If you can't use the shuffle script, you can consider a less fine-grained shuffle ""hack"" in this older document http://bit.ly/train-deepvariant (Note that this doc is a one-off document, and is not maintained by our team. Please consider it as a possible example that you'll probably need to tweak for your own use case).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:328,interoperability,distribut,distributed,328,"Hi @anands-repo . The point of using Dataflow is to run things in a distributed fashion, which means it shouldn't be running on your local machine. I assume that Spark runner should allow you to run with distributed workers as well if you set it up correctly, but I have never used it myself. Our team doesn't support different distributed setup for Beam. Please refer to https://beam.apache.org/documentation/runners/spark/ to see if you can set up Sparker runner so that it doesn't use your local memory. If you do figure out a good setup to run on Spark, please feel free to share some tips here so people can use it in the future! If you can't use the shuffle script, you can consider a less fine-grained shuffle ""hack"" in this older document http://bit.ly/train-deepvariant (Note that this doc is a one-off document, and is not maintained by our team. Please consider it as a possible example that you'll probably need to tweak for your own use case).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:578,interoperability,share,share,578,"Hi @anands-repo . The point of using Dataflow is to run things in a distributed fashion, which means it shouldn't be running on your local machine. I assume that Spark runner should allow you to run with distributed workers as well if you set it up correctly, but I have never used it myself. Our team doesn't support different distributed setup for Beam. Please refer to https://beam.apache.org/documentation/runners/spark/ to see if you can set up Sparker runner so that it doesn't use your local memory. If you do figure out a good setup to run on Spark, please feel free to share some tips here so people can use it in the future! If you can't use the shuffle script, you can consider a less fine-grained shuffle ""hack"" in this older document http://bit.ly/train-deepvariant (Note that this doc is a one-off document, and is not maintained by our team. Please consider it as a possible example that you'll probably need to tweak for your own use case).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:833,modifiability,maintain,maintained,833,"Hi @anands-repo . The point of using Dataflow is to run things in a distributed fashion, which means it shouldn't be running on your local machine. I assume that Spark runner should allow you to run with distributed workers as well if you set it up correctly, but I have never used it myself. Our team doesn't support different distributed setup for Beam. Please refer to https://beam.apache.org/documentation/runners/spark/ to see if you can set up Sparker runner so that it doesn't use your local memory. If you do figure out a good setup to run on Spark, please feel free to share some tips here so people can use it in the future! If you can't use the shuffle script, you can consider a less fine-grained shuffle ""hack"" in this older document http://bit.ly/train-deepvariant (Note that this doc is a one-off document, and is not maintained by our team. Please consider it as a possible example that you'll probably need to tweak for your own use case).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:499,performance,memor,memory,499,"Hi @anands-repo . The point of using Dataflow is to run things in a distributed fashion, which means it shouldn't be running on your local machine. I assume that Spark runner should allow you to run with distributed workers as well if you set it up correctly, but I have never used it myself. Our team doesn't support different distributed setup for Beam. Please refer to https://beam.apache.org/documentation/runners/spark/ to see if you can set up Sparker runner so that it doesn't use your local memory. If you do figure out a good setup to run on Spark, please feel free to share some tips here so people can use it in the future! If you can't use the shuffle script, you can consider a less fine-grained shuffle ""hack"" in this older document http://bit.ly/train-deepvariant (Note that this doc is a one-off document, and is not maintained by our team. Please consider it as a possible example that you'll probably need to tweak for your own use case).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:302,reliability,doe,doesn,302,"Hi @anands-repo . The point of using Dataflow is to run things in a distributed fashion, which means it shouldn't be running on your local machine. I assume that Spark runner should allow you to run with distributed workers as well if you set it up correctly, but I have never used it myself. Our team doesn't support different distributed setup for Beam. Please refer to https://beam.apache.org/documentation/runners/spark/ to see if you can set up Sparker runner so that it doesn't use your local memory. If you do figure out a good setup to run on Spark, please feel free to share some tips here so people can use it in the future! If you can't use the shuffle script, you can consider a less fine-grained shuffle ""hack"" in this older document http://bit.ly/train-deepvariant (Note that this doc is a one-off document, and is not maintained by our team. Please consider it as a possible example that you'll probably need to tweak for your own use case).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:476,reliability,doe,doesn,476,"Hi @anands-repo . The point of using Dataflow is to run things in a distributed fashion, which means it shouldn't be running on your local machine. I assume that Spark runner should allow you to run with distributed workers as well if you set it up correctly, but I have never used it myself. Our team doesn't support different distributed setup for Beam. Please refer to https://beam.apache.org/documentation/runners/spark/ to see if you can set up Sparker runner so that it doesn't use your local memory. If you do figure out a good setup to run on Spark, please feel free to share some tips here so people can use it in the future! If you can't use the shuffle script, you can consider a less fine-grained shuffle ""hack"" in this older document http://bit.ly/train-deepvariant (Note that this doc is a one-off document, and is not maintained by our team. Please consider it as a possible example that you'll probably need to tweak for your own use case).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:833,safety,maintain,maintained,833,"Hi @anands-repo . The point of using Dataflow is to run things in a distributed fashion, which means it shouldn't be running on your local machine. I assume that Spark runner should allow you to run with distributed workers as well if you set it up correctly, but I have never used it myself. Our team doesn't support different distributed setup for Beam. Please refer to https://beam.apache.org/documentation/runners/spark/ to see if you can set up Sparker runner so that it doesn't use your local memory. If you do figure out a good setup to run on Spark, please feel free to share some tips here so people can use it in the future! If you can't use the shuffle script, you can consider a less fine-grained shuffle ""hack"" in this older document http://bit.ly/train-deepvariant (Note that this doc is a one-off document, and is not maintained by our team. Please consider it as a possible example that you'll probably need to tweak for your own use case).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:297,security,team,team,297,"Hi @anands-repo . The point of using Dataflow is to run things in a distributed fashion, which means it shouldn't be running on your local machine. I assume that Spark runner should allow you to run with distributed workers as well if you set it up correctly, but I have never used it myself. Our team doesn't support different distributed setup for Beam. Please refer to https://beam.apache.org/documentation/runners/spark/ to see if you can set up Sparker runner so that it doesn't use your local memory. If you do figure out a good setup to run on Spark, please feel free to share some tips here so people can use it in the future! If you can't use the shuffle script, you can consider a less fine-grained shuffle ""hack"" in this older document http://bit.ly/train-deepvariant (Note that this doc is a one-off document, and is not maintained by our team. Please consider it as a possible example that you'll probably need to tweak for your own use case).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:718,security,hack,hack,718,"Hi @anands-repo . The point of using Dataflow is to run things in a distributed fashion, which means it shouldn't be running on your local machine. I assume that Spark runner should allow you to run with distributed workers as well if you set it up correctly, but I have never used it myself. Our team doesn't support different distributed setup for Beam. Please refer to https://beam.apache.org/documentation/runners/spark/ to see if you can set up Sparker runner so that it doesn't use your local memory. If you do figure out a good setup to run on Spark, please feel free to share some tips here so people can use it in the future! If you can't use the shuffle script, you can consider a less fine-grained shuffle ""hack"" in this older document http://bit.ly/train-deepvariant (Note that this doc is a one-off document, and is not maintained by our team. Please consider it as a possible example that you'll probably need to tweak for your own use case).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:851,security,team,team,851,"Hi @anands-repo . The point of using Dataflow is to run things in a distributed fashion, which means it shouldn't be running on your local machine. I assume that Spark runner should allow you to run with distributed workers as well if you set it up correctly, but I have never used it myself. Our team doesn't support different distributed setup for Beam. Please refer to https://beam.apache.org/documentation/runners/spark/ to see if you can set up Sparker runner so that it doesn't use your local memory. If you do figure out a good setup to run on Spark, please feel free to share some tips here so people can use it in the future! If you can't use the shuffle script, you can consider a less fine-grained shuffle ""hack"" in this older document http://bit.ly/train-deepvariant (Note that this doc is a one-off document, and is not maintained by our team. Please consider it as a possible example that you'll probably need to tweak for your own use case).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:310,usability,support,support,310,"Hi @anands-repo . The point of using Dataflow is to run things in a distributed fashion, which means it shouldn't be running on your local machine. I assume that Spark runner should allow you to run with distributed workers as well if you set it up correctly, but I have never used it myself. Our team doesn't support different distributed setup for Beam. Please refer to https://beam.apache.org/documentation/runners/spark/ to see if you can set up Sparker runner so that it doesn't use your local memory. If you do figure out a good setup to run on Spark, please feel free to share some tips here so people can use it in the future! If you can't use the shuffle script, you can consider a less fine-grained shuffle ""hack"" in this older document http://bit.ly/train-deepvariant (Note that this doc is a one-off document, and is not maintained by our team. Please consider it as a possible example that you'll probably need to tweak for your own use case).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:396,usability,document,documentation,396,"Hi @anands-repo . The point of using Dataflow is to run things in a distributed fashion, which means it shouldn't be running on your local machine. I assume that Spark runner should allow you to run with distributed workers as well if you set it up correctly, but I have never used it myself. Our team doesn't support different distributed setup for Beam. Please refer to https://beam.apache.org/documentation/runners/spark/ to see if you can set up Sparker runner so that it doesn't use your local memory. If you do figure out a good setup to run on Spark, please feel free to share some tips here so people can use it in the future! If you can't use the shuffle script, you can consider a less fine-grained shuffle ""hack"" in this older document http://bit.ly/train-deepvariant (Note that this doc is a one-off document, and is not maintained by our team. Please consider it as a possible example that you'll probably need to tweak for your own use case).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:499,usability,memor,memory,499,"Hi @anands-repo . The point of using Dataflow is to run things in a distributed fashion, which means it shouldn't be running on your local machine. I assume that Spark runner should allow you to run with distributed workers as well if you set it up correctly, but I have never used it myself. Our team doesn't support different distributed setup for Beam. Please refer to https://beam.apache.org/documentation/runners/spark/ to see if you can set up Sparker runner so that it doesn't use your local memory. If you do figure out a good setup to run on Spark, please feel free to share some tips here so people can use it in the future! If you can't use the shuffle script, you can consider a less fine-grained shuffle ""hack"" in this older document http://bit.ly/train-deepvariant (Note that this doc is a one-off document, and is not maintained by our team. Please consider it as a possible example that you'll probably need to tweak for your own use case).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:589,usability,tip,tips,589,"Hi @anands-repo . The point of using Dataflow is to run things in a distributed fashion, which means it shouldn't be running on your local machine. I assume that Spark runner should allow you to run with distributed workers as well if you set it up correctly, but I have never used it myself. Our team doesn't support different distributed setup for Beam. Please refer to https://beam.apache.org/documentation/runners/spark/ to see if you can set up Sparker runner so that it doesn't use your local memory. If you do figure out a good setup to run on Spark, please feel free to share some tips here so people can use it in the future! If you can't use the shuffle script, you can consider a less fine-grained shuffle ""hack"" in this older document http://bit.ly/train-deepvariant (Note that this doc is a one-off document, and is not maintained by our team. Please consider it as a possible example that you'll probably need to tweak for your own use case).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:738,usability,document,document,738,"Hi @anands-repo . The point of using Dataflow is to run things in a distributed fashion, which means it shouldn't be running on your local machine. I assume that Spark runner should allow you to run with distributed workers as well if you set it up correctly, but I have never used it myself. Our team doesn't support different distributed setup for Beam. Please refer to https://beam.apache.org/documentation/runners/spark/ to see if you can set up Sparker runner so that it doesn't use your local memory. If you do figure out a good setup to run on Spark, please feel free to share some tips here so people can use it in the future! If you can't use the shuffle script, you can consider a less fine-grained shuffle ""hack"" in this older document http://bit.ly/train-deepvariant (Note that this doc is a one-off document, and is not maintained by our team. Please consider it as a possible example that you'll probably need to tweak for your own use case).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:812,usability,document,document,812,"Hi @anands-repo . The point of using Dataflow is to run things in a distributed fashion, which means it shouldn't be running on your local machine. I assume that Spark runner should allow you to run with distributed workers as well if you set it up correctly, but I have never used it myself. Our team doesn't support different distributed setup for Beam. Please refer to https://beam.apache.org/documentation/runners/spark/ to see if you can set up Sparker runner so that it doesn't use your local memory. If you do figure out a good setup to run on Spark, please feel free to share some tips here so people can use it in the future! If you can't use the shuffle script, you can consider a less fine-grained shuffle ""hack"" in this older document http://bit.ly/train-deepvariant (Note that this doc is a one-off document, and is not maintained by our team. Please consider it as a possible example that you'll probably need to tweak for your own use case).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:457,availability,cluster,clusters,457,"Hi @pichuan Thanks for the advise. Will look into these possibilities. The coarse-grained shuffling would be easiest, however it is mentioned that shuffling is an important step in the document you mentioned as well as the training page. Technically stochastic/batch gradient descent does depend on random batches. I will look into spark, as well as other options like dask or torque (which would need a script hack). If I have a setup that works for local clusters, I will share it. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:289,deployability,depend,depend,289,"Hi @pichuan Thanks for the advise. Will look into these possibilities. The coarse-grained shuffling would be easiest, however it is mentioned that shuffling is an important step in the document you mentioned as well as the training page. Technically stochastic/batch gradient descent does depend on random batches. I will look into spark, as well as other options like dask or torque (which would need a script hack). If I have a setup that works for local clusters, I will share it. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:457,deployability,cluster,clusters,457,"Hi @pichuan Thanks for the advise. Will look into these possibilities. The coarse-grained shuffling would be easiest, however it is mentioned that shuffling is an important step in the document you mentioned as well as the training page. Technically stochastic/batch gradient descent does depend on random batches. I will look into spark, as well as other options like dask or torque (which would need a script hack). If I have a setup that works for local clusters, I will share it. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:261,integrability,batch,batch,261,"Hi @pichuan Thanks for the advise. Will look into these possibilities. The coarse-grained shuffling would be easiest, however it is mentioned that shuffling is an important step in the document you mentioned as well as the training page. Technically stochastic/batch gradient descent does depend on random batches. I will look into spark, as well as other options like dask or torque (which would need a script hack). If I have a setup that works for local clusters, I will share it. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:289,integrability,depend,depend,289,"Hi @pichuan Thanks for the advise. Will look into these possibilities. The coarse-grained shuffling would be easiest, however it is mentioned that shuffling is an important step in the document you mentioned as well as the training page. Technically stochastic/batch gradient descent does depend on random batches. I will look into spark, as well as other options like dask or torque (which would need a script hack). If I have a setup that works for local clusters, I will share it. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:306,integrability,batch,batches,306,"Hi @pichuan Thanks for the advise. Will look into these possibilities. The coarse-grained shuffling would be easiest, however it is mentioned that shuffling is an important step in the document you mentioned as well as the training page. Technically stochastic/batch gradient descent does depend on random batches. I will look into spark, as well as other options like dask or torque (which would need a script hack). If I have a setup that works for local clusters, I will share it. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:474,interoperability,share,share,474,"Hi @pichuan Thanks for the advise. Will look into these possibilities. The coarse-grained shuffling would be easiest, however it is mentioned that shuffling is an important step in the document you mentioned as well as the training page. Technically stochastic/batch gradient descent does depend on random batches. I will look into spark, as well as other options like dask or torque (which would need a script hack). If I have a setup that works for local clusters, I will share it. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:289,modifiability,depend,depend,289,"Hi @pichuan Thanks for the advise. Will look into these possibilities. The coarse-grained shuffling would be easiest, however it is mentioned that shuffling is an important step in the document you mentioned as well as the training page. Technically stochastic/batch gradient descent does depend on random batches. I will look into spark, as well as other options like dask or torque (which would need a script hack). If I have a setup that works for local clusters, I will share it. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:261,performance,batch,batch,261,"Hi @pichuan Thanks for the advise. Will look into these possibilities. The coarse-grained shuffling would be easiest, however it is mentioned that shuffling is an important step in the document you mentioned as well as the training page. Technically stochastic/batch gradient descent does depend on random batches. I will look into spark, as well as other options like dask or torque (which would need a script hack). If I have a setup that works for local clusters, I will share it. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:306,performance,batch,batches,306,"Hi @pichuan Thanks for the advise. Will look into these possibilities. The coarse-grained shuffling would be easiest, however it is mentioned that shuffling is an important step in the document you mentioned as well as the training page. Technically stochastic/batch gradient descent does depend on random batches. I will look into spark, as well as other options like dask or torque (which would need a script hack). If I have a setup that works for local clusters, I will share it. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:284,reliability,doe,does,284,"Hi @pichuan Thanks for the advise. Will look into these possibilities. The coarse-grained shuffling would be easiest, however it is mentioned that shuffling is an important step in the document you mentioned as well as the training page. Technically stochastic/batch gradient descent does depend on random batches. I will look into spark, as well as other options like dask or torque (which would need a script hack). If I have a setup that works for local clusters, I will share it. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:289,safety,depend,depend,289,"Hi @pichuan Thanks for the advise. Will look into these possibilities. The coarse-grained shuffling would be easiest, however it is mentioned that shuffling is an important step in the document you mentioned as well as the training page. Technically stochastic/batch gradient descent does depend on random batches. I will look into spark, as well as other options like dask or torque (which would need a script hack). If I have a setup that works for local clusters, I will share it. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:411,security,hack,hack,411,"Hi @pichuan Thanks for the advise. Will look into these possibilities. The coarse-grained shuffling would be easiest, however it is mentioned that shuffling is an important step in the document you mentioned as well as the training page. Technically stochastic/batch gradient descent does depend on random batches. I will look into spark, as well as other options like dask or torque (which would need a script hack). If I have a setup that works for local clusters, I will share it. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:289,testability,depend,depend,289,"Hi @pichuan Thanks for the advise. Will look into these possibilities. The coarse-grained shuffling would be easiest, however it is mentioned that shuffling is an important step in the document you mentioned as well as the training page. Technically stochastic/batch gradient descent does depend on random batches. I will look into spark, as well as other options like dask or torque (which would need a script hack). If I have a setup that works for local clusters, I will share it. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:185,usability,document,document,185,"Hi @pichuan Thanks for the advise. Will look into these possibilities. The coarse-grained shuffling would be easiest, however it is mentioned that shuffling is an important step in the document you mentioned as well as the training page. Technically stochastic/batch gradient descent does depend on random batches. I will look into spark, as well as other options like dask or torque (which would need a script hack). If I have a setup that works for local clusters, I will share it. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:392,deployability,stack,stackoverflow,392,"@pichuan . As you know, I am running the shuffle script using Spark. I am wondering how many output files are expected from running the script. When I use DirectRunner, I get a single output file. When I use the SparkRunner I get as many output files as there are input files fitting the pattern (I have noticed this mismatch between spark/direct runner in another situation as well: https://stackoverflow.com/questions/64450391/apache-beam-beam-flatten-doesnt-flatten-files-with-sparkrunner-but-does-so-wi). Is this the expected result when using Dataflow runner as well? Basically, I am simply trying to do a sanity check to make sure that the shuffler isn't simply reading in the data and copying it without shuffling, or simply shuffling within each shard. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:317,interoperability,mismatch,mismatch,317,"@pichuan . As you know, I am running the shuffle script using Spark. I am wondering how many output files are expected from running the script. When I use DirectRunner, I get a single output file. When I use the SparkRunner I get as many output files as there are input files fitting the pattern (I have noticed this mismatch between spark/direct runner in another situation as well: https://stackoverflow.com/questions/64450391/apache-beam-beam-flatten-doesnt-flatten-files-with-sparkrunner-but-does-so-wi). Is this the expected result when using Dataflow runner as well? Basically, I am simply trying to do a sanity check to make sure that the shuffler isn't simply reading in the data and copying it without shuffling, or simply shuffling within each shard. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:454,reliability,doe,doesnt-flatten-files-with-sparkrunner-but-does-so-wi,454,"@pichuan . As you know, I am running the shuffle script using Spark. I am wondering how many output files are expected from running the script. When I use DirectRunner, I get a single output file. When I use the SparkRunner I get as many output files as there are input files fitting the pattern (I have noticed this mismatch between spark/direct runner in another situation as well: https://stackoverflow.com/questions/64450391/apache-beam-beam-flatten-doesnt-flatten-files-with-sparkrunner-but-does-so-wi). Is this the expected result when using Dataflow runner as well? Basically, I am simply trying to do a sanity check to make sure that the shuffler isn't simply reading in the data and copying it without shuffling, or simply shuffling within each shard. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:264,safety,input,input,264,"@pichuan . As you know, I am running the shuffle script using Spark. I am wondering how many output files are expected from running the script. When I use DirectRunner, I get a single output file. When I use the SparkRunner I get as many output files as there are input files fitting the pattern (I have noticed this mismatch between spark/direct runner in another situation as well: https://stackoverflow.com/questions/64450391/apache-beam-beam-flatten-doesnt-flatten-files-with-sparkrunner-but-does-so-wi). Is this the expected result when using Dataflow runner as well? Basically, I am simply trying to do a sanity check to make sure that the shuffler isn't simply reading in the data and copying it without shuffling, or simply shuffling within each shard. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:611,safety,sanit,sanity,611,"@pichuan . As you know, I am running the shuffle script using Spark. I am wondering how many output files are expected from running the script. When I use DirectRunner, I get a single output file. When I use the SparkRunner I get as many output files as there are input files fitting the pattern (I have noticed this mismatch between spark/direct runner in another situation as well: https://stackoverflow.com/questions/64450391/apache-beam-beam-flatten-doesnt-flatten-files-with-sparkrunner-but-does-so-wi). Is this the expected result when using Dataflow runner as well? Basically, I am simply trying to do a sanity check to make sure that the shuffler isn't simply reading in the data and copying it without shuffling, or simply shuffling within each shard. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:611,security,sanit,sanity,611,"@pichuan . As you know, I am running the shuffle script using Spark. I am wondering how many output files are expected from running the script. When I use DirectRunner, I get a single output file. When I use the SparkRunner I get as many output files as there are input files fitting the pattern (I have noticed this mismatch between spark/direct runner in another situation as well: https://stackoverflow.com/questions/64450391/apache-beam-beam-flatten-doesnt-flatten-files-with-sparkrunner-but-does-so-wi). Is this the expected result when using Dataflow runner as well? Basically, I am simply trying to do a sanity check to make sure that the shuffler isn't simply reading in the data and copying it without shuffling, or simply shuffling within each shard. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:589,testability,simpl,simply,589,"@pichuan . As you know, I am running the shuffle script using Spark. I am wondering how many output files are expected from running the script. When I use DirectRunner, I get a single output file. When I use the SparkRunner I get as many output files as there are input files fitting the pattern (I have noticed this mismatch between spark/direct runner in another situation as well: https://stackoverflow.com/questions/64450391/apache-beam-beam-flatten-doesnt-flatten-files-with-sparkrunner-but-does-so-wi). Is this the expected result when using Dataflow runner as well? Basically, I am simply trying to do a sanity check to make sure that the shuffler isn't simply reading in the data and copying it without shuffling, or simply shuffling within each shard. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:661,testability,simpl,simply,661,"@pichuan . As you know, I am running the shuffle script using Spark. I am wondering how many output files are expected from running the script. When I use DirectRunner, I get a single output file. When I use the SparkRunner I get as many output files as there are input files fitting the pattern (I have noticed this mismatch between spark/direct runner in another situation as well: https://stackoverflow.com/questions/64450391/apache-beam-beam-flatten-doesnt-flatten-files-with-sparkrunner-but-does-so-wi). Is this the expected result when using Dataflow runner as well? Basically, I am simply trying to do a sanity check to make sure that the shuffler isn't simply reading in the data and copying it without shuffling, or simply shuffling within each shard. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:725,testability,simpl,simply,725,"@pichuan . As you know, I am running the shuffle script using Spark. I am wondering how many output files are expected from running the script. When I use DirectRunner, I get a single output file. When I use the SparkRunner I get as many output files as there are input files fitting the pattern (I have noticed this mismatch between spark/direct runner in another situation as well: https://stackoverflow.com/questions/64450391/apache-beam-beam-flatten-doesnt-flatten-files-with-sparkrunner-but-does-so-wi). Is this the expected result when using Dataflow runner as well? Basically, I am simply trying to do a sanity check to make sure that the shuffler isn't simply reading in the data and copying it without shuffling, or simply shuffling within each shard. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:264,usability,input,input,264,"@pichuan . As you know, I am running the shuffle script using Spark. I am wondering how many output files are expected from running the script. When I use DirectRunner, I get a single output file. When I use the SparkRunner I get as many output files as there are input files fitting the pattern (I have noticed this mismatch between spark/direct runner in another situation as well: https://stackoverflow.com/questions/64450391/apache-beam-beam-flatten-doesnt-flatten-files-with-sparkrunner-but-does-so-wi). Is this the expected result when using Dataflow runner as well? Basically, I am simply trying to do a sanity check to make sure that the shuffler isn't simply reading in the data and copying it without shuffling, or simply shuffling within each shard. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:589,usability,simpl,simply,589,"@pichuan . As you know, I am running the shuffle script using Spark. I am wondering how many output files are expected from running the script. When I use DirectRunner, I get a single output file. When I use the SparkRunner I get as many output files as there are input files fitting the pattern (I have noticed this mismatch between spark/direct runner in another situation as well: https://stackoverflow.com/questions/64450391/apache-beam-beam-flatten-doesnt-flatten-files-with-sparkrunner-but-does-so-wi). Is this the expected result when using Dataflow runner as well? Basically, I am simply trying to do a sanity check to make sure that the shuffler isn't simply reading in the data and copying it without shuffling, or simply shuffling within each shard. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:661,usability,simpl,simply,661,"@pichuan . As you know, I am running the shuffle script using Spark. I am wondering how many output files are expected from running the script. When I use DirectRunner, I get a single output file. When I use the SparkRunner I get as many output files as there are input files fitting the pattern (I have noticed this mismatch between spark/direct runner in another situation as well: https://stackoverflow.com/questions/64450391/apache-beam-beam-flatten-doesnt-flatten-files-with-sparkrunner-but-does-so-wi). Is this the expected result when using Dataflow runner as well? Basically, I am simply trying to do a sanity check to make sure that the shuffler isn't simply reading in the data and copying it without shuffling, or simply shuffling within each shard. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:725,usability,simpl,simply,725,"@pichuan . As you know, I am running the shuffle script using Spark. I am wondering how many output files are expected from running the script. When I use DirectRunner, I get a single output file. When I use the SparkRunner I get as many output files as there are input files fitting the pattern (I have noticed this mismatch between spark/direct runner in another situation as well: https://stackoverflow.com/questions/64450391/apache-beam-beam-flatten-doesnt-flatten-files-with-sparkrunner-but-does-so-wi). Is this the expected result when using Dataflow runner as well? Basically, I am simply trying to do a sanity check to make sure that the shuffler isn't simply reading in the data and copying it without shuffling, or simply shuffling within each shard. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:263,availability,Down,Downside,263,"Just for the record, I wrote a script which shuffles the records locally using as little memory as possible: [TFrecordShuffler](https://github.com/GuillaumeHolley/TFrecordShuffler). It uses about as much RAM as the total size of the input (record) files on disk. Downside is obviously the time it takes which is much longer than with a distributed google cloud or spark system I imagine. As an example, shuffling ~30 million records totaling 125 GB of files took 46h (wall-clock and CPU) and 150 GB of RAM.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:355,energy efficiency,cloud,cloud,355,"Just for the record, I wrote a script which shuffles the records locally using as little memory as possible: [TFrecordShuffler](https://github.com/GuillaumeHolley/TFrecordShuffler). It uses about as much RAM as the total size of the input (record) files on disk. Downside is obviously the time it takes which is much longer than with a distributed google cloud or spark system I imagine. As an example, shuffling ~30 million records totaling 125 GB of files took 46h (wall-clock and CPU) and 150 GB of RAM.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:473,energy efficiency,clock,clock,473,"Just for the record, I wrote a script which shuffles the records locally using as little memory as possible: [TFrecordShuffler](https://github.com/GuillaumeHolley/TFrecordShuffler). It uses about as much RAM as the total size of the input (record) files on disk. Downside is obviously the time it takes which is much longer than with a distributed google cloud or spark system I imagine. As an example, shuffling ~30 million records totaling 125 GB of files took 46h (wall-clock and CPU) and 150 GB of RAM.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:483,energy efficiency,CPU,CPU,483,"Just for the record, I wrote a script which shuffles the records locally using as little memory as possible: [TFrecordShuffler](https://github.com/GuillaumeHolley/TFrecordShuffler). It uses about as much RAM as the total size of the input (record) files on disk. Downside is obviously the time it takes which is much longer than with a distributed google cloud or spark system I imagine. As an example, shuffling ~30 million records totaling 125 GB of files took 46h (wall-clock and CPU) and 150 GB of RAM.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:336,interoperability,distribut,distributed,336,"Just for the record, I wrote a script which shuffles the records locally using as little memory as possible: [TFrecordShuffler](https://github.com/GuillaumeHolley/TFrecordShuffler). It uses about as much RAM as the total size of the input (record) files on disk. Downside is obviously the time it takes which is much longer than with a distributed google cloud or spark system I imagine. As an example, shuffling ~30 million records totaling 125 GB of files took 46h (wall-clock and CPU) and 150 GB of RAM.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:89,performance,memor,memory,89,"Just for the record, I wrote a script which shuffles the records locally using as little memory as possible: [TFrecordShuffler](https://github.com/GuillaumeHolley/TFrecordShuffler). It uses about as much RAM as the total size of the input (record) files on disk. Downside is obviously the time it takes which is much longer than with a distributed google cloud or spark system I imagine. As an example, shuffling ~30 million records totaling 125 GB of files took 46h (wall-clock and CPU) and 150 GB of RAM.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:257,performance,disk,disk,257,"Just for the record, I wrote a script which shuffles the records locally using as little memory as possible: [TFrecordShuffler](https://github.com/GuillaumeHolley/TFrecordShuffler). It uses about as much RAM as the total size of the input (record) files on disk. Downside is obviously the time it takes which is much longer than with a distributed google cloud or spark system I imagine. As an example, shuffling ~30 million records totaling 125 GB of files took 46h (wall-clock and CPU) and 150 GB of RAM.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:289,performance,time,time,289,"Just for the record, I wrote a script which shuffles the records locally using as little memory as possible: [TFrecordShuffler](https://github.com/GuillaumeHolley/TFrecordShuffler). It uses about as much RAM as the total size of the input (record) files on disk. Downside is obviously the time it takes which is much longer than with a distributed google cloud or spark system I imagine. As an example, shuffling ~30 million records totaling 125 GB of files took 46h (wall-clock and CPU) and 150 GB of RAM.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:483,performance,CPU,CPU,483,"Just for the record, I wrote a script which shuffles the records locally using as little memory as possible: [TFrecordShuffler](https://github.com/GuillaumeHolley/TFrecordShuffler). It uses about as much RAM as the total size of the input (record) files on disk. Downside is obviously the time it takes which is much longer than with a distributed google cloud or spark system I imagine. As an example, shuffling ~30 million records totaling 125 GB of files took 46h (wall-clock and CPU) and 150 GB of RAM.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:233,safety,input,input,233,"Just for the record, I wrote a script which shuffles the records locally using as little memory as possible: [TFrecordShuffler](https://github.com/GuillaumeHolley/TFrecordShuffler). It uses about as much RAM as the total size of the input (record) files on disk. Downside is obviously the time it takes which is much longer than with a distributed google cloud or spark system I imagine. As an example, shuffling ~30 million records totaling 125 GB of files took 46h (wall-clock and CPU) and 150 GB of RAM.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:89,usability,memor,memory,89,"Just for the record, I wrote a script which shuffles the records locally using as little memory as possible: [TFrecordShuffler](https://github.com/GuillaumeHolley/TFrecordShuffler). It uses about as much RAM as the total size of the input (record) files on disk. Downside is obviously the time it takes which is much longer than with a distributed google cloud or spark system I imagine. As an example, shuffling ~30 million records totaling 125 GB of files took 46h (wall-clock and CPU) and 150 GB of RAM.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:233,usability,input,input,233,"Just for the record, I wrote a script which shuffles the records locally using as little memory as possible: [TFrecordShuffler](https://github.com/GuillaumeHolley/TFrecordShuffler). It uses about as much RAM as the total size of the input (record) files on disk. Downside is obviously the time it takes which is much longer than with a distributed google cloud or spark system I imagine. As an example, shuffling ~30 million records totaling 125 GB of files took 46h (wall-clock and CPU) and 150 GB of RAM.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:37,deployability,updat,updating,37,"@GuillaumeHolley FYI, I'm working on updating the tutorial. I will add this sentence:. NOTE: If you prefer shuffling locally, please take a look at this user-provided. shuffler option: https://github.com/google/deepvariant/issues/360#issuecomment-1019990366. If you want to suggest a different sentence in the tutorial, please let me know!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:37,safety,updat,updating,37,"@GuillaumeHolley FYI, I'm working on updating the tutorial. I will add this sentence:. NOTE: If you prefer shuffling locally, please take a look at this user-provided. shuffler option: https://github.com/google/deepvariant/issues/360#issuecomment-1019990366. If you want to suggest a different sentence in the tutorial, please let me know!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:37,security,updat,updating,37,"@GuillaumeHolley FYI, I'm working on updating the tutorial. I will add this sentence:. NOTE: If you prefer shuffling locally, please take a look at this user-provided. shuffler option: https://github.com/google/deepvariant/issues/360#issuecomment-1019990366. If you want to suggest a different sentence in the tutorial, please let me know!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:100,usability,prefer,prefer,100,"@GuillaumeHolley FYI, I'm working on updating the tutorial. I will add this sentence:. NOTE: If you prefer shuffling locally, please take a look at this user-provided. shuffler option: https://github.com/google/deepvariant/issues/360#issuecomment-1019990366. If you want to suggest a different sentence in the tutorial, please let me know!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:153,usability,user,user-provided,153,"@GuillaumeHolley FYI, I'm working on updating the tutorial. I will add this sentence:. NOTE: If you prefer shuffling locally, please take a look at this user-provided. shuffler option: https://github.com/google/deepvariant/issues/360#issuecomment-1019990366. If you want to suggest a different sentence in the tutorial, please let me know!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:115,energy efficiency,CPU,CPUs,115,"@pichuan when shuffle the datasets using local runner, `direct_num_workers` is set to 0, it will use all the local CPUs. I got this warning that make me thinking. ```. WARNING:apache_beam.runners.portability.fn_api_runner.fn_runner:If direct_num_workers is not equal to 1, direct_running_mode should be `multi_processing` or `multi_threading` instead of `in_memory` in order for it to have the desired worker parallelism effect. direct_num_workers: 8 ; running_mode: in_memory. ```. Is there a reason we use `in_memory` rather than other modes? Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:196,modifiability,portab,portability,196,"@pichuan when shuffle the datasets using local runner, `direct_num_workers` is set to 0, it will use all the local CPUs. I got this warning that make me thinking. ```. WARNING:apache_beam.runners.portability.fn_api_runner.fn_runner:If direct_num_workers is not equal to 1, direct_running_mode should be `multi_processing` or `multi_threading` instead of `in_memory` in order for it to have the desired worker parallelism effect. direct_num_workers: 8 ; running_mode: in_memory. ```. Is there a reason we use `in_memory` rather than other modes? Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:115,performance,CPU,CPUs,115,"@pichuan when shuffle the datasets using local runner, `direct_num_workers` is set to 0, it will use all the local CPUs. I got this warning that make me thinking. ```. WARNING:apache_beam.runners.portability.fn_api_runner.fn_runner:If direct_num_workers is not equal to 1, direct_running_mode should be `multi_processing` or `multi_threading` instead of `in_memory` in order for it to have the desired worker parallelism effect. direct_num_workers: 8 ; running_mode: in_memory. ```. Is there a reason we use `in_memory` rather than other modes? Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:409,performance,parallel,parallelism,409,"@pichuan when shuffle the datasets using local runner, `direct_num_workers` is set to 0, it will use all the local CPUs. I got this warning that make me thinking. ```. WARNING:apache_beam.runners.portability.fn_api_runner.fn_runner:If direct_num_workers is not equal to 1, direct_running_mode should be `multi_processing` or `multi_threading` instead of `in_memory` in order for it to have the desired worker parallelism effect. direct_num_workers: 8 ; running_mode: in_memory. ```. Is there a reason we use `in_memory` rather than other modes? Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:112,security,team,team,112,"Hi @yinshiyi , Hello :D. First, this is a pretty old bug. It might be easier to open a new issue. Otherwise our team member on rotation might not notice it. To your question, are you asking about https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md ? In that documentation, we provided two examples of using the shuffle script. One is with:. ```. --runner=DirectRunner \. ```. the other one is with:. ```. --runner=DataflowRunner \. ```. If you intend to use Dataflow, please refer to the command that uses `DataflowRunner`. @yinshiyi , if you want to discuss further, please open a new issue with a few more details on which command you were using. That will be helpful for our team member to provide more support. Thank you :).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:127,security,rotat,rotation,127,"Hi @yinshiyi , Hello :D. First, this is a pretty old bug. It might be easier to open a new issue. Otherwise our team member on rotation might not notice it. To your question, are you asking about https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md ? In that documentation, we provided two examples of using the shuffle script. One is with:. ```. --runner=DirectRunner \. ```. the other one is with:. ```. --runner=DataflowRunner \. ```. If you intend to use Dataflow, please refer to the command that uses `DataflowRunner`. @yinshiyi , if you want to discuss further, please open a new issue with a few more details on which command you were using. That will be helpful for our team member to provide more support. Thank you :).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:716,security,team,team,716,"Hi @yinshiyi , Hello :D. First, this is a pretty old bug. It might be easier to open a new issue. Otherwise our team member on rotation might not notice it. To your question, are you asking about https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md ? In that documentation, we provided two examples of using the shuffle script. One is with:. ```. --runner=DirectRunner \. ```. the other one is with:. ```. --runner=DataflowRunner \. ```. If you intend to use Dataflow, please refer to the command that uses `DataflowRunner`. @yinshiyi , if you want to discuss further, please open a new issue with a few more details on which command you were using. That will be helpful for our team member to provide more support. Thank you :).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:296,usability,document,documentation,296,"Hi @yinshiyi , Hello :D. First, this is a pretty old bug. It might be easier to open a new issue. Otherwise our team member on rotation might not notice it. To your question, are you asking about https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md ? In that documentation, we provided two examples of using the shuffle script. One is with:. ```. --runner=DirectRunner \. ```. the other one is with:. ```. --runner=DataflowRunner \. ```. If you intend to use Dataflow, please refer to the command that uses `DataflowRunner`. @yinshiyi , if you want to discuss further, please open a new issue with a few more details on which command you were using. That will be helpful for our team member to provide more support. Thank you :).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:526,usability,command,command,526,"Hi @yinshiyi , Hello :D. First, this is a pretty old bug. It might be easier to open a new issue. Otherwise our team member on rotation might not notice it. To your question, are you asking about https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md ? In that documentation, we provided two examples of using the shuffle script. One is with:. ```. --runner=DirectRunner \. ```. the other one is with:. ```. --runner=DataflowRunner \. ```. If you intend to use Dataflow, please refer to the command that uses `DataflowRunner`. @yinshiyi , if you want to discuss further, please open a new issue with a few more details on which command you were using. That will be helpful for our team member to provide more support. Thank you :).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:663,usability,command,command,663,"Hi @yinshiyi , Hello :D. First, this is a pretty old bug. It might be easier to open a new issue. Otherwise our team member on rotation might not notice it. To your question, are you asking about https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md ? In that documentation, we provided two examples of using the shuffle script. One is with:. ```. --runner=DirectRunner \. ```. the other one is with:. ```. --runner=DataflowRunner \. ```. If you intend to use Dataflow, please refer to the command that uses `DataflowRunner`. @yinshiyi , if you want to discuss further, please open a new issue with a few more details on which command you were using. That will be helpful for our team member to provide more support. Thank you :).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:700,usability,help,helpful,700,"Hi @yinshiyi , Hello :D. First, this is a pretty old bug. It might be easier to open a new issue. Otherwise our team member on rotation might not notice it. To your question, are you asking about https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md ? In that documentation, we provided two examples of using the shuffle script. One is with:. ```. --runner=DirectRunner \. ```. the other one is with:. ```. --runner=DataflowRunner \. ```. If you intend to use Dataflow, please refer to the command that uses `DataflowRunner`. @yinshiyi , if you want to discuss further, please open a new issue with a few more details on which command you were using. That will be helpful for our team member to provide more support. Thank you :).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/360:744,usability,support,support,744,"Hi @yinshiyi , Hello :D. First, this is a pretty old bug. It might be easier to open a new issue. Otherwise our team member on rotation might not notice it. To your question, are you asking about https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md ? In that documentation, we provided two examples of using the shuffle script. One is with:. ```. --runner=DirectRunner \. ```. the other one is with:. ```. --runner=DataflowRunner \. ```. If you intend to use Dataflow, please refer to the command that uses `DataflowRunner`. @yinshiyi , if you want to discuss further, please open a new issue with a few more details on which command you were using. That will be helpful for our team member to provide more support. Thank you :).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/360
https://github.com/google/deepvariant/issues/361:23,testability,plan,planning,23,"Hi @aderzelle ,. I was planning to look into this today and reply. I think it is possible that there are some options in tensorflow. . I noticed that you just closed this issue. If you already found an answer, can you reply here for future reference? Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/361
https://github.com/google/deepvariant/issues/361:159,usability,close,closed,159,"Hi @aderzelle ,. I was planning to look into this today and reply. I think it is possible that there are some options in tensorflow. . I noticed that you just closed this issue. If you already found an answer, can you reply here for future reference? Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/361
https://github.com/google/deepvariant/issues/361:289,energy efficiency,cool,cool,289,"Hi @pichuan . oh no I haven't found a way ... I closed because rereading my phrasing I was afraid it sounded like a non-constructive complain. . Difficult for me to express it in English ... basically I was afraid opening this issue the way I did was not polite. . (but yes, it would be a cool option)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/361
https://github.com/google/deepvariant/issues/361:133,safety,compl,complain,133,"Hi @pichuan . oh no I haven't found a way ... I closed because rereading my phrasing I was afraid it sounded like a non-constructive complain. . Difficult for me to express it in English ... basically I was afraid opening this issue the way I did was not polite. . (but yes, it would be a cool option)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/361
https://github.com/google/deepvariant/issues/361:133,security,compl,complain,133,"Hi @pichuan . oh no I haven't found a way ... I closed because rereading my phrasing I was afraid it sounded like a non-constructive complain. . Difficult for me to express it in English ... basically I was afraid opening this issue the way I did was not polite. . (but yes, it would be a cool option)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/361
https://github.com/google/deepvariant/issues/361:48,usability,close,closed,48,"Hi @pichuan . oh no I haven't found a way ... I closed because rereading my phrasing I was afraid it sounded like a non-constructive complain. . Difficult for me to express it in English ... basically I was afraid opening this issue the way I did was not polite. . (but yes, it would be a cool option)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/361
https://github.com/google/deepvariant/issues/361:787,availability,checkpoint,checkpoint,787,"Hi @aderzelle ,. I think you can use the TensorFlow config string to constraint the CPU use of the call_variants step. For example, this discussion talked about how to run TF on one core:. https://medium.com/@liyin2015/tensorflow-cpus-and-gpus-configuration-9c223436d4ef. If you're running call_variants directly, you can add a flag like `--config_string ""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1""`. For example:. ```. sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --config_string ""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1"". ```. With this extra arg, I do see that:. ```. I1015 19:57:25.812590 140110048159488 estimator.py:212] Using config: {'_model_dir': '/tmp/tmpt8af___k', '_tf_random_seed': None, '_save_summary_steps': 10. 0, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': device_count {. key: ""cpu"" . value: 1. }. intra_op_parallelism_threads: 1. inter_op_parallelism_threads: 1. ```. But I'm not 100% sure that TensorFlow actually fully follow the instructions here or not. If you're using the one-step script (`run_deepvariant`), you can add:. `--call_variants_extra_args=""config_string=\""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1\""""`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/361
https://github.com/google/deepvariant/issues/361:244,deployability,configurat,configuration-,244,"Hi @aderzelle ,. I think you can use the TensorFlow config string to constraint the CPU use of the call_variants step. For example, this discussion talked about how to run TF on one core:. https://medium.com/@liyin2015/tensorflow-cpus-and-gpus-configuration-9c223436d4ef. If you're running call_variants directly, you can add a flag like `--config_string ""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1""`. For example:. ```. sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --config_string ""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1"". ```. With this extra arg, I do see that:. ```. I1015 19:57:25.812590 140110048159488 estimator.py:212] Using config: {'_model_dir': '/tmp/tmpt8af___k', '_tf_random_seed': None, '_save_summary_steps': 10. 0, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': device_count {. key: ""cpu"" . value: 1. }. intra_op_parallelism_threads: 1. inter_op_parallelism_threads: 1. ```. But I'm not 100% sure that TensorFlow actually fully follow the instructions here or not. If you're using the one-step script (`run_deepvariant`), you can add:. `--call_variants_extra_args=""config_string=\""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1\""""`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/361
https://github.com/google/deepvariant/issues/361:84,energy efficiency,CPU,CPU,84,"Hi @aderzelle ,. I think you can use the TensorFlow config string to constraint the CPU use of the call_variants step. For example, this discussion talked about how to run TF on one core:. https://medium.com/@liyin2015/tensorflow-cpus-and-gpus-configuration-9c223436d4ef. If you're running call_variants directly, you can add a flag like `--config_string ""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1""`. For example:. ```. sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --config_string ""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1"". ```. With this extra arg, I do see that:. ```. I1015 19:57:25.812590 140110048159488 estimator.py:212] Using config: {'_model_dir': '/tmp/tmpt8af___k', '_tf_random_seed': None, '_save_summary_steps': 10. 0, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': device_count {. key: ""cpu"" . value: 1. }. intra_op_parallelism_threads: 1. inter_op_parallelism_threads: 1. ```. But I'm not 100% sure that TensorFlow actually fully follow the instructions here or not. If you're using the one-step script (`run_deepvariant`), you can add:. `--call_variants_extra_args=""config_string=\""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1\""""`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/361
https://github.com/google/deepvariant/issues/361:182,energy efficiency,core,core,182,"Hi @aderzelle ,. I think you can use the TensorFlow config string to constraint the CPU use of the call_variants step. For example, this discussion talked about how to run TF on one core:. https://medium.com/@liyin2015/tensorflow-cpus-and-gpus-configuration-9c223436d4ef. If you're running call_variants directly, you can add a flag like `--config_string ""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1""`. For example:. ```. sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --config_string ""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1"". ```. With this extra arg, I do see that:. ```. I1015 19:57:25.812590 140110048159488 estimator.py:212] Using config: {'_model_dir': '/tmp/tmpt8af___k', '_tf_random_seed': None, '_save_summary_steps': 10. 0, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': device_count {. key: ""cpu"" . value: 1. }. intra_op_parallelism_threads: 1. inter_op_parallelism_threads: 1. ```. But I'm not 100% sure that TensorFlow actually fully follow the instructions here or not. If you're using the one-step script (`run_deepvariant`), you can add:. `--call_variants_extra_args=""config_string=\""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1\""""`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/361
https://github.com/google/deepvariant/issues/361:230,energy efficiency,cpu,cpus-and-gpus-configuration-,230,"Hi @aderzelle ,. I think you can use the TensorFlow config string to constraint the CPU use of the call_variants step. For example, this discussion talked about how to run TF on one core:. https://medium.com/@liyin2015/tensorflow-cpus-and-gpus-configuration-9c223436d4ef. If you're running call_variants directly, you can add a flag like `--config_string ""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1""`. For example:. ```. sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --config_string ""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1"". ```. With this extra arg, I do see that:. ```. I1015 19:57:25.812590 140110048159488 estimator.py:212] Using config: {'_model_dir': '/tmp/tmpt8af___k', '_tf_random_seed': None, '_save_summary_steps': 10. 0, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': device_count {. key: ""cpu"" . value: 1. }. intra_op_parallelism_threads: 1. inter_op_parallelism_threads: 1. ```. But I'm not 100% sure that TensorFlow actually fully follow the instructions here or not. If you're using the one-step script (`run_deepvariant`), you can add:. `--call_variants_extra_args=""config_string=\""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1\""""`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/361
https://github.com/google/deepvariant/issues/361:376,energy efficiency,cpu,cpu,376,"Hi @aderzelle ,. I think you can use the TensorFlow config string to constraint the CPU use of the call_variants step. For example, this discussion talked about how to run TF on one core:. https://medium.com/@liyin2015/tensorflow-cpus-and-gpus-configuration-9c223436d4ef. If you're running call_variants directly, you can add a flag like `--config_string ""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1""`. For example:. ```. sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --config_string ""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1"". ```. With this extra arg, I do see that:. ```. I1015 19:57:25.812590 140110048159488 estimator.py:212] Using config: {'_model_dir': '/tmp/tmpt8af___k', '_tf_random_seed': None, '_save_summary_steps': 10. 0, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': device_count {. key: ""cpu"" . value: 1. }. intra_op_parallelism_threads: 1. inter_op_parallelism_threads: 1. ```. But I'm not 100% sure that TensorFlow actually fully follow the instructions here or not. If you're using the one-step script (`run_deepvariant`), you can add:. `--call_variants_extra_args=""config_string=\""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1\""""`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/361
https://github.com/google/deepvariant/issues/361:804,energy efficiency,model,models,804,"Hi @aderzelle ,. I think you can use the TensorFlow config string to constraint the CPU use of the call_variants step. For example, this discussion talked about how to run TF on one core:. https://medium.com/@liyin2015/tensorflow-cpus-and-gpus-configuration-9c223436d4ef. If you're running call_variants directly, you can add a flag like `--config_string ""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1""`. For example:. ```. sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --config_string ""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1"". ```. With this extra arg, I do see that:. ```. I1015 19:57:25.812590 140110048159488 estimator.py:212] Using config: {'_model_dir': '/tmp/tmpt8af___k', '_tf_random_seed': None, '_save_summary_steps': 10. 0, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': device_count {. key: ""cpu"" . value: 1. }. intra_op_parallelism_threads: 1. inter_op_parallelism_threads: 1. ```. But I'm not 100% sure that TensorFlow actually fully follow the instructions here or not. If you're using the one-step script (`run_deepvariant`), you can add:. `--call_variants_extra_args=""config_string=\""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1\""""`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/361
https://github.com/google/deepvariant/issues/361:815,energy efficiency,model,model,815,"Hi @aderzelle ,. I think you can use the TensorFlow config string to constraint the CPU use of the call_variants step. For example, this discussion talked about how to run TF on one core:. https://medium.com/@liyin2015/tensorflow-cpus-and-gpus-configuration-9c223436d4ef. If you're running call_variants directly, you can add a flag like `--config_string ""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1""`. For example:. ```. sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --config_string ""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1"". ```. With this extra arg, I do see that:. ```. I1015 19:57:25.812590 140110048159488 estimator.py:212] Using config: {'_model_dir': '/tmp/tmpt8af___k', '_tf_random_seed': None, '_save_summary_steps': 10. 0, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': device_count {. key: ""cpu"" . value: 1. }. intra_op_parallelism_threads: 1. inter_op_parallelism_threads: 1. ```. But I'm not 100% sure that TensorFlow actually fully follow the instructions here or not. If you're using the one-step script (`run_deepvariant`), you can add:. `--call_variants_extra_args=""config_string=\""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1\""""`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/361
https://github.com/google/deepvariant/issues/361:867,energy efficiency,cpu,cpu,867,"Hi @aderzelle ,. I think you can use the TensorFlow config string to constraint the CPU use of the call_variants step. For example, this discussion talked about how to run TF on one core:. https://medium.com/@liyin2015/tensorflow-cpus-and-gpus-configuration-9c223436d4ef. If you're running call_variants directly, you can add a flag like `--config_string ""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1""`. For example:. ```. sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --config_string ""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1"". ```. With this extra arg, I do see that:. ```. I1015 19:57:25.812590 140110048159488 estimator.py:212] Using config: {'_model_dir': '/tmp/tmpt8af___k', '_tf_random_seed': None, '_save_summary_steps': 10. 0, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': device_count {. key: ""cpu"" . value: 1. }. intra_op_parallelism_threads: 1. inter_op_parallelism_threads: 1. ```. But I'm not 100% sure that TensorFlow actually fully follow the instructions here or not. If you're using the one-step script (`run_deepvariant`), you can add:. `--call_variants_extra_args=""config_string=\""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1\""""`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/361
https://github.com/google/deepvariant/issues/361:1031,energy efficiency,estimat,estimator,1031,"Hi @aderzelle ,. I think you can use the TensorFlow config string to constraint the CPU use of the call_variants step. For example, this discussion talked about how to run TF on one core:. https://medium.com/@liyin2015/tensorflow-cpus-and-gpus-configuration-9c223436d4ef. If you're running call_variants directly, you can add a flag like `--config_string ""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1""`. For example:. ```. sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --config_string ""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1"". ```. With this extra arg, I do see that:. ```. I1015 19:57:25.812590 140110048159488 estimator.py:212] Using config: {'_model_dir': '/tmp/tmpt8af___k', '_tf_random_seed': None, '_save_summary_steps': 10. 0, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': device_count {. key: ""cpu"" . value: 1. }. intra_op_parallelism_threads: 1. inter_op_parallelism_threads: 1. ```. But I'm not 100% sure that TensorFlow actually fully follow the instructions here or not. If you're using the one-step script (`run_deepvariant`), you can add:. `--call_variants_extra_args=""config_string=\""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1\""""`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/361
https://github.com/google/deepvariant/issues/361:1258,energy efficiency,cpu,cpu,1258,"Hi @aderzelle ,. I think you can use the TensorFlow config string to constraint the CPU use of the call_variants step. For example, this discussion talked about how to run TF on one core:. https://medium.com/@liyin2015/tensorflow-cpus-and-gpus-configuration-9c223436d4ef. If you're running call_variants directly, you can add a flag like `--config_string ""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1""`. For example:. ```. sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --config_string ""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1"". ```. With this extra arg, I do see that:. ```. I1015 19:57:25.812590 140110048159488 estimator.py:212] Using config: {'_model_dir': '/tmp/tmpt8af___k', '_tf_random_seed': None, '_save_summary_steps': 10. 0, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': device_count {. key: ""cpu"" . value: 1. }. intra_op_parallelism_threads: 1. inter_op_parallelism_threads: 1. ```. But I'm not 100% sure that TensorFlow actually fully follow the instructions here or not. If you're using the one-step script (`run_deepvariant`), you can add:. `--call_variants_extra_args=""config_string=\""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1\""""`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/361
https://github.com/google/deepvariant/issues/361:1575,energy efficiency,cpu,cpu,1575,"Hi @aderzelle ,. I think you can use the TensorFlow config string to constraint the CPU use of the call_variants step. For example, this discussion talked about how to run TF on one core:. https://medium.com/@liyin2015/tensorflow-cpus-and-gpus-configuration-9c223436d4ef. If you're running call_variants directly, you can add a flag like `--config_string ""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1""`. For example:. ```. sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" \. google/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \. --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \. --checkpoint ""/opt/models/wgs/model.ckpt"" \. --config_string ""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1"". ```. With this extra arg, I do see that:. ```. I1015 19:57:25.812590 140110048159488 estimator.py:212] Using config: {'_model_dir': '/tmp/tmpt8af___k', '_tf_random_seed': None, '_save_summary_steps': 10. 0, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': device_count {. key: ""cpu"" . value: 1. }. intra_op_parallelism_threads: 1. inter_op_parallelism_threads: 1. ```. But I'm not 100% sure that TensorFlow actually fully follow the instructions here or not. If you're using the one-step script (`run_deepvariant`), you can add:. `--call_variants_extra_args=""config_string=\""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1\""""`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/361
