id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/scverse/scanpy/issues/599:213,integrability,Buffer,Buffer,213,"Is there a way to filter for a set of genes, where if any one of the genes in a list are expressed, those cells will be plotted? I've tried switching Xparx's solution to a list, but receive the error ""ValueError: Buffer has wrong number of dimensions (expected 1, got 0)"". I've also tired chansigit's method, but find that flatten returns as not found?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/599
https://github.com/scverse/scanpy/issues/599:194,performance,error,error,194,"Is there a way to filter for a set of genes, where if any one of the genes in a list are expressed, those cells will be plotted? I've tried switching Xparx's solution to a list, but receive the error ""ValueError: Buffer has wrong number of dimensions (expected 1, got 0)"". I've also tired chansigit's method, but find that flatten returns as not found?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/599
https://github.com/scverse/scanpy/issues/599:194,safety,error,error,194,"Is there a way to filter for a set of genes, where if any one of the genes in a list are expressed, those cells will be plotted? I've tried switching Xparx's solution to a list, but receive the error ""ValueError: Buffer has wrong number of dimensions (expected 1, got 0)"". I've also tired chansigit's method, but find that flatten returns as not found?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/599
https://github.com/scverse/scanpy/issues/599:194,usability,error,error,194,"Is there a way to filter for a set of genes, where if any one of the genes in a list are expressed, those cells will be plotted? I've tried switching Xparx's solution to a list, but receive the error ""ValueError: Buffer has wrong number of dimensions (expected 1, got 0)"". I've also tired chansigit's method, but find that flatten returns as not found?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/599
https://github.com/scverse/scanpy/issues/601:20,availability,error,error,20,"Looks like the same error hit in #585, as well as https://github.com/theislab/scanpy/pull/493#issuecomment-477674448. @flying-sheep I haven't been able to reproduce, but maybe we should just throw an `__init__.py` in there, since it fixes this for @fbrundu, before the `v1.4.1` release?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/601
https://github.com/scverse/scanpy/issues/601:278,deployability,releas,release,278,"Looks like the same error hit in #585, as well as https://github.com/theislab/scanpy/pull/493#issuecomment-477674448. @flying-sheep I haven't been able to reproduce, but maybe we should just throw an `__init__.py` in there, since it fixes this for @fbrundu, before the `v1.4.1` release?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/601
https://github.com/scverse/scanpy/issues/601:20,performance,error,error,20,"Looks like the same error hit in #585, as well as https://github.com/theislab/scanpy/pull/493#issuecomment-477674448. @flying-sheep I haven't been able to reproduce, but maybe we should just throw an `__init__.py` in there, since it fixes this for @fbrundu, before the `v1.4.1` release?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/601
https://github.com/scverse/scanpy/issues/601:20,safety,error,error,20,"Looks like the same error hit in #585, as well as https://github.com/theislab/scanpy/pull/493#issuecomment-477674448. @flying-sheep I haven't been able to reproduce, but maybe we should just throw an `__init__.py` in there, since it fixes this for @fbrundu, before the `v1.4.1` release?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/601
https://github.com/scverse/scanpy/issues/601:20,usability,error,error,20,"Looks like the same error hit in #585, as well as https://github.com/theislab/scanpy/pull/493#issuecomment-477674448. @flying-sheep I haven't been able to reproduce, but maybe we should just throw an `__init__.py` in there, since it fixes this for @fbrundu, before the `v1.4.1` release?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/601
https://github.com/scverse/scanpy/issues/601:243,availability,fault,fault,243,"@vladie0, would you mind pulling again and checking if it works now? @flying-sheep if at least four people can't install the package (including in a clean conda environment on a lab mate's machine), what do you call it? I don't think it's our fault, but I think there's a bug somewhere.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/601
https://github.com/scverse/scanpy/issues/601:113,deployability,instal,install,113,"@vladie0, would you mind pulling again and checking if it works now? @flying-sheep if at least four people can't install the package (including in a clean conda environment on a lab mate's machine), what do you call it? I don't think it's our fault, but I think there's a bug somewhere.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/601
https://github.com/scverse/scanpy/issues/601:243,energy efficiency,fault,fault,243,"@vladie0, would you mind pulling again and checking if it works now? @flying-sheep if at least four people can't install the package (including in a clean conda environment on a lab mate's machine), what do you call it? I don't think it's our fault, but I think there's a bug somewhere.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/601
https://github.com/scverse/scanpy/issues/601:125,modifiability,pac,package,125,"@vladie0, would you mind pulling again and checking if it works now? @flying-sheep if at least four people can't install the package (including in a clean conda environment on a lab mate's machine), what do you call it? I don't think it's our fault, but I think there's a bug somewhere.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/601
https://github.com/scverse/scanpy/issues/601:243,performance,fault,fault,243,"@vladie0, would you mind pulling again and checking if it works now? @flying-sheep if at least four people can't install the package (including in a clean conda environment on a lab mate's machine), what do you call it? I don't think it's our fault, but I think there's a bug somewhere.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/601
https://github.com/scverse/scanpy/issues/601:243,reliability,fault,fault,243,"@vladie0, would you mind pulling again and checking if it works now? @flying-sheep if at least four people can't install the package (including in a clean conda environment on a lab mate's machine), what do you call it? I don't think it's our fault, but I think there's a bug somewhere.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/601
https://github.com/scverse/scanpy/issues/601:243,safety,fault,fault,243,"@vladie0, would you mind pulling again and checking if it works now? @flying-sheep if at least four people can't install the package (including in a clean conda environment on a lab mate's machine), what do you call it? I don't think it's our fault, but I think there's a bug somewhere.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/601
https://github.com/scverse/scanpy/pull/603:36,deployability,version,version,36,"Hmm, Maybe they have another scanpy version installed or some other weird broken setup. why not?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/603
https://github.com/scverse/scanpy/pull/603:44,deployability,instal,installed,44,"Hmm, Maybe they have another scanpy version installed or some other weird broken setup. why not?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/603
https://github.com/scverse/scanpy/pull/603:36,integrability,version,version,36,"Hmm, Maybe they have another scanpy version installed or some other weird broken setup. why not?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/603
https://github.com/scverse/scanpy/pull/603:36,modifiability,version,version,36,"Hmm, Maybe they have another scanpy version installed or some other weird broken setup. why not?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/603
https://github.com/scverse/scanpy/issues/606:109,usability,visual,visualizing-marker-genes,109,Do you also get the problem when trying the examples here: https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html? Can you make a small example to reproduce the problem?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/606
https://github.com/scverse/scanpy/issues/606:40,deployability,updat,updating,40,"Hi, . I tried several things, including updating matplotlib. The problem is gone for both with the example data and my own. Thanks for your help! Eva",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/606
https://github.com/scverse/scanpy/issues/606:40,safety,updat,updating,40,"Hi, . I tried several things, including updating matplotlib. The problem is gone for both with the example data and my own. Thanks for your help! Eva",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/606
https://github.com/scverse/scanpy/issues/606:40,security,updat,updating,40,"Hi, . I tried several things, including updating matplotlib. The problem is gone for both with the example data and my own. Thanks for your help! Eva",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/606
https://github.com/scverse/scanpy/issues/606:140,usability,help,help,140,"Hi, . I tried several things, including updating matplotlib. The problem is gone for both with the example data and my own. Thanks for your help! Eva",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/606
https://github.com/scverse/scanpy/issues/607:207,energy efficiency,adapt,adapted,207,"Why would a separate package be necessary? If you use it for transcriptome analysis, the only difference should be that the counts are (in theory) more accurate and thereâ€™s e.g. no need for methods that are adapted for zero-inflation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/607
https://github.com/scverse/scanpy/issues/607:207,integrability,adapt,adapted,207,"Why would a separate package be necessary? If you use it for transcriptome analysis, the only difference should be that the counts are (in theory) more accurate and thereâ€™s e.g. no need for methods that are adapted for zero-inflation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/607
https://github.com/scverse/scanpy/issues/607:207,interoperability,adapt,adapted,207,"Why would a separate package be necessary? If you use it for transcriptome analysis, the only difference should be that the counts are (in theory) more accurate and thereâ€™s e.g. no need for methods that are adapted for zero-inflation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/607
https://github.com/scverse/scanpy/issues/607:21,modifiability,pac,package,21,"Why would a separate package be necessary? If you use it for transcriptome analysis, the only difference should be that the counts are (in theory) more accurate and thereâ€™s e.g. no need for methods that are adapted for zero-inflation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/607
https://github.com/scverse/scanpy/issues/607:207,modifiability,adapt,adapted,207,"Why would a separate package be necessary? If you use it for transcriptome analysis, the only difference should be that the counts are (in theory) more accurate and thereâ€™s e.g. no need for methods that are adapted for zero-inflation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/607
https://github.com/scverse/scanpy/pull/608:228,interoperability,format,formatted,228,"The central commit is: https://github.com/theislab/scanpy/pull/608/commits/08111b87ca1274138e6c25963c412b44f7ab57c1. Unfortunately, this is not a great solution... But at least it's one that doesn't mess up the `numpydoc`-style-formatted return sections and re-enables return sections that have been written using prose, bulleted lists etc. (those went broke almost half a year ago after changing from `numpydoc` to `sphinx.ext.napoleon`).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/608
https://github.com/scverse/scanpy/pull/608:191,reliability,doe,doesn,191,"The central commit is: https://github.com/theislab/scanpy/pull/608/commits/08111b87ca1274138e6c25963c412b44f7ab57c1. Unfortunately, this is not a great solution... But at least it's one that doesn't mess up the `numpydoc`-style-formatted return sections and re-enables return sections that have been written using prose, bulleted lists etc. (those went broke almost half a year ago after changing from `numpydoc` to `sphinx.ext.napoleon`).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/608
https://github.com/scverse/scanpy/issues/609:33,performance,time,time,33,"I opened one, #611, its my first time (scary), hope I didn't do anything majorly stupid ;)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/609
https://github.com/scverse/scanpy/issues/609:21,usability,Close,Closed,21,"Looks great, thanks! Closed via #611.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/609
https://github.com/scverse/scanpy/pull/610:90,availability,Sla,Slack,90,"No, they aren't. Most of them start with `""""""my title`. But we can happily migrate... see Slack...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:90,reliability,Sla,Slack,90,"No, they aren't. Most of them start with `""""""my title`. But we can happily migrate... see Slack...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:45,deployability,contain,contained,45,"Ah, the problem was that the string actually contained that return type! Theoretically that should have parsed as a 1-element definition list, but I donâ€™t think itâ€™s a problem that it doesnâ€™t.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:184,reliability,doe,doesn,184,"Ah, the problem was that the string actually contained that return type! Theoretically that should have parsed as a 1-element definition list, but I donâ€™t think itâ€™s a problem that it doesnâ€™t.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:47,deployability,contain,contained,47,"> Ah, the problem was that the string actually contained that return type! This is the standard way of writing a numpydoc returns section. Also see https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.pp.filter_cells.html, for instance. This solution is dropping support for them. Also, do you by chance have another simple solution for having the styling of the return sections similar to the parameters section (what `numpydoc` did ðŸ™‚)? Bold font and spacings around colons? ![image](https://user-images.githubusercontent.com/16916678/56280862-67789100-610b-11e9-8c43-405072ce6fbd.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:199,deployability,api,api,199,"> Ah, the problem was that the string actually contained that return type! This is the standard way of writing a numpydoc returns section. Also see https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.pp.filter_cells.html, for instance. This solution is dropping support for them. Also, do you by chance have another simple solution for having the styling of the return sections similar to the parameters section (what `numpydoc` did ðŸ™‚)? Bold font and spacings around colons? ![image](https://user-images.githubusercontent.com/16916678/56280862-67789100-610b-11e9-8c43-405072ce6fbd.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:199,integrability,api,api,199,"> Ah, the problem was that the string actually contained that return type! This is the standard way of writing a numpydoc returns section. Also see https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.pp.filter_cells.html, for instance. This solution is dropping support for them. Also, do you by chance have another simple solution for having the styling of the return sections similar to the parameters section (what `numpydoc` did ðŸ™‚)? Bold font and spacings around colons? ![image](https://user-images.githubusercontent.com/16916678/56280862-67789100-610b-11e9-8c43-405072ce6fbd.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:87,interoperability,standard,standard,87,"> Ah, the problem was that the string actually contained that return type! This is the standard way of writing a numpydoc returns section. Also see https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.pp.filter_cells.html, for instance. This solution is dropping support for them. Also, do you by chance have another simple solution for having the styling of the return sections similar to the parameters section (what `numpydoc` did ðŸ™‚)? Bold font and spacings around colons? ![image](https://user-images.githubusercontent.com/16916678/56280862-67789100-610b-11e9-8c43-405072ce6fbd.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:188,interoperability,format,formatting,188,"> Ah, the problem was that the string actually contained that return type! This is the standard way of writing a numpydoc returns section. Also see https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.pp.filter_cells.html, for instance. This solution is dropping support for them. Also, do you by chance have another simple solution for having the styling of the return sections similar to the parameters section (what `numpydoc` did ðŸ™‚)? Bold font and spacings around colons? ![image](https://user-images.githubusercontent.com/16916678/56280862-67789100-610b-11e9-8c43-405072ce6fbd.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:199,interoperability,api,api,199,"> Ah, the problem was that the string actually contained that return type! This is the standard way of writing a numpydoc returns section. Also see https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.pp.filter_cells.html, for instance. This solution is dropping support for them. Also, do you by chance have another simple solution for having the styling of the return sections similar to the parameters section (what `numpydoc` did ðŸ™‚)? Bold font and spacings around colons? ![image](https://user-images.githubusercontent.com/16916678/56280862-67789100-610b-11e9-8c43-405072ce6fbd.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:403,modifiability,paramet,parameters,403,"> Ah, the problem was that the string actually contained that return type! This is the standard way of writing a numpydoc returns section. Also see https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.pp.filter_cells.html, for instance. This solution is dropping support for them. Also, do you by chance have another simple solution for having the styling of the return sections similar to the parameters section (what `numpydoc` did ðŸ™‚)? Bold font and spacings around colons? ![image](https://user-images.githubusercontent.com/16916678/56280862-67789100-610b-11e9-8c43-405072ce6fbd.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:326,testability,simpl,simple,326,"> Ah, the problem was that the string actually contained that return type! This is the standard way of writing a numpydoc returns section. Also see https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.pp.filter_cells.html, for instance. This solution is dropping support for them. Also, do you by chance have another simple solution for having the styling of the return sections similar to the parameters section (what `numpydoc` did ðŸ™‚)? Bold font and spacings around colons? ![image](https://user-images.githubusercontent.com/16916678/56280862-67789100-610b-11e9-8c43-405072ce6fbd.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:272,usability,support,support,272,"> Ah, the problem was that the string actually contained that return type! This is the standard way of writing a numpydoc returns section. Also see https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.pp.filter_cells.html, for instance. This solution is dropping support for them. Also, do you by chance have another simple solution for having the styling of the return sections similar to the parameters section (what `numpydoc` did ðŸ™‚)? Bold font and spacings around colons? ![image](https://user-images.githubusercontent.com/16916678/56280862-67789100-610b-11e9-8c43-405072ce6fbd.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:326,usability,simpl,simple,326,"> Ah, the problem was that the string actually contained that return type! This is the standard way of writing a numpydoc returns section. Also see https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.pp.filter_cells.html, for instance. This solution is dropping support for them. Also, do you by chance have another simple solution for having the styling of the return sections similar to the parameters section (what `numpydoc` did ðŸ™‚)? Bold font and spacings around colons? ![image](https://user-images.githubusercontent.com/16916678/56280862-67789100-610b-11e9-8c43-405072ce6fbd.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:502,usability,user,user-images,502,"> Ah, the problem was that the string actually contained that return type! This is the standard way of writing a numpydoc returns section. Also see https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.pp.filter_cells.html, for instance. This solution is dropping support for them. Also, do you by chance have another simple solution for having the styling of the return sections similar to the parameters section (what `numpydoc` did ðŸ™‚)? Bold font and spacings around colons? ![image](https://user-images.githubusercontent.com/16916678/56280862-67789100-610b-11e9-8c43-405072ce6fbd.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:194,deployability,api,api,194,"Unfortunately, changing. ```. .rst-content dl:not(.docutils) dl dt {. font-weight: normal;. ```. to `bold` changes a whole lot of other stuff. https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.tl.dpt.html. I just see that this how scikit-learn styles it, though. ![image](https://user-images.githubusercontent.com/16916678/56281210-567c4f80-610c-11e9-9e51-89af17c67664.png). Hence, only the spacing after the colon is the remaining inconsistency.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:194,integrability,api,api,194,"Unfortunately, changing. ```. .rst-content dl:not(.docutils) dl dt {. font-weight: normal;. ```. to `bold` changes a whole lot of other stuff. https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.tl.dpt.html. I just see that this how scikit-learn styles it, though. ![image](https://user-images.githubusercontent.com/16916678/56281210-567c4f80-610c-11e9-9e51-89af17c67664.png). Hence, only the spacing after the colon is the remaining inconsistency.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:183,interoperability,format,formatting,183,"Unfortunately, changing. ```. .rst-content dl:not(.docutils) dl dt {. font-weight: normal;. ```. to `bold` changes a whole lot of other stuff. https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.tl.dpt.html. I just see that this how scikit-learn styles it, though. ![image](https://user-images.githubusercontent.com/16916678/56281210-567c4f80-610c-11e9-9e51-89af17c67664.png). Hence, only the spacing after the colon is the remaining inconsistency.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:194,interoperability,api,api,194,"Unfortunately, changing. ```. .rst-content dl:not(.docutils) dl dt {. font-weight: normal;. ```. to `bold` changes a whole lot of other stuff. https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.tl.dpt.html. I just see that this how scikit-learn styles it, though. ![image](https://user-images.githubusercontent.com/16916678/56281210-567c4f80-610c-11e9-9e51-89af17c67664.png). Hence, only the spacing after the colon is the remaining inconsistency.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:35,performance,content,content,35,"Unfortunately, changing. ```. .rst-content dl:not(.docutils) dl dt {. font-weight: normal;. ```. to `bold` changes a whole lot of other stuff. https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.tl.dpt.html. I just see that this how scikit-learn styles it, though. ![image](https://user-images.githubusercontent.com/16916678/56281210-567c4f80-610c-11e9-9e51-89af17c67664.png). Hence, only the spacing after the colon is the remaining inconsistency.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:250,usability,learn,learn,250,"Unfortunately, changing. ```. .rst-content dl:not(.docutils) dl dt {. font-weight: normal;. ```. to `bold` changes a whole lot of other stuff. https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.tl.dpt.html. I just see that this how scikit-learn styles it, though. ![image](https://user-images.githubusercontent.com/16916678/56281210-567c4f80-610c-11e9-9e51-89af17c67664.png). Hence, only the spacing after the colon is the remaining inconsistency.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:292,usability,user,user-images,292,"Unfortunately, changing. ```. .rst-content dl:not(.docutils) dl dt {. font-weight: normal;. ```. to `bold` changes a whole lot of other stuff. https://scanpy.readthedocs.io/en/return-formatting/api/scanpy.tl.dpt.html. I just see that this how scikit-learn styles it, though. ![image](https://user-images.githubusercontent.com/16916678/56281210-567c4f80-610c-11e9-9e51-89af17c67664.png). Hence, only the spacing after the colon is the remaining inconsistency.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:56,availability,state,statement,56,"@ivirshup @flying-sheep I would remove the `, optional` statement from the docstrings, as, what we mean with this is ""a parameter has a default value"". Hence, it's redundant. However, it's consistently used in all of numpy, scipy, sklearn, pandas, etc. ![image](https://user-images.githubusercontent.com/16916678/56281364-b3780580-610c-11e9-8e40-2f44d3007a19.png). In the auto-generated type annotations, the default values miss completely, and I don't think we'll ever restore the `, optional` descriptor, there. ![image](https://user-images.githubusercontent.com/16916678/56281431-e15d4a00-610c-11e9-990c-6a2477540535.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:164,availability,redund,redundant,164,"@ivirshup @flying-sheep I would remove the `, optional` statement from the docstrings, as, what we mean with this is ""a parameter has a default value"". Hence, it's redundant. However, it's consistently used in all of numpy, scipy, sklearn, pandas, etc. ![image](https://user-images.githubusercontent.com/16916678/56281364-b3780580-610c-11e9-8e40-2f44d3007a19.png). In the auto-generated type annotations, the default values miss completely, and I don't think we'll ever restore the `, optional` descriptor, there. ![image](https://user-images.githubusercontent.com/16916678/56281431-e15d4a00-610c-11e9-990c-6a2477540535.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:189,availability,consist,consistently,189,"@ivirshup @flying-sheep I would remove the `, optional` statement from the docstrings, as, what we mean with this is ""a parameter has a default value"". Hence, it's redundant. However, it's consistently used in all of numpy, scipy, sklearn, pandas, etc. ![image](https://user-images.githubusercontent.com/16916678/56281364-b3780580-610c-11e9-8e40-2f44d3007a19.png). In the auto-generated type annotations, the default values miss completely, and I don't think we'll ever restore the `, optional` descriptor, there. ![image](https://user-images.githubusercontent.com/16916678/56281431-e15d4a00-610c-11e9-990c-6a2477540535.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:470,availability,restor,restore,470,"@ivirshup @flying-sheep I would remove the `, optional` statement from the docstrings, as, what we mean with this is ""a parameter has a default value"". Hence, it's redundant. However, it's consistently used in all of numpy, scipy, sklearn, pandas, etc. ![image](https://user-images.githubusercontent.com/16916678/56281364-b3780580-610c-11e9-8e40-2f44d3007a19.png). In the auto-generated type annotations, the default values miss completely, and I don't think we'll ever restore the `, optional` descriptor, there. ![image](https://user-images.githubusercontent.com/16916678/56281431-e15d4a00-610c-11e9-990c-6a2477540535.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:164,deployability,redundan,redundant,164,"@ivirshup @flying-sheep I would remove the `, optional` statement from the docstrings, as, what we mean with this is ""a parameter has a default value"". Hence, it's redundant. However, it's consistently used in all of numpy, scipy, sklearn, pandas, etc. ![image](https://user-images.githubusercontent.com/16916678/56281364-b3780580-610c-11e9-8e40-2f44d3007a19.png). In the auto-generated type annotations, the default values miss completely, and I don't think we'll ever restore the `, optional` descriptor, there. ![image](https://user-images.githubusercontent.com/16916678/56281431-e15d4a00-610c-11e9-990c-6a2477540535.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:56,integrability,state,statement,56,"@ivirshup @flying-sheep I would remove the `, optional` statement from the docstrings, as, what we mean with this is ""a parameter has a default value"". Hence, it's redundant. However, it's consistently used in all of numpy, scipy, sklearn, pandas, etc. ![image](https://user-images.githubusercontent.com/16916678/56281364-b3780580-610c-11e9-8e40-2f44d3007a19.png). In the auto-generated type annotations, the default values miss completely, and I don't think we'll ever restore the `, optional` descriptor, there. ![image](https://user-images.githubusercontent.com/16916678/56281431-e15d4a00-610c-11e9-990c-6a2477540535.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:120,modifiability,paramet,parameter,120,"@ivirshup @flying-sheep I would remove the `, optional` statement from the docstrings, as, what we mean with this is ""a parameter has a default value"". Hence, it's redundant. However, it's consistently used in all of numpy, scipy, sklearn, pandas, etc. ![image](https://user-images.githubusercontent.com/16916678/56281364-b3780580-610c-11e9-8e40-2f44d3007a19.png). In the auto-generated type annotations, the default values miss completely, and I don't think we'll ever restore the `, optional` descriptor, there. ![image](https://user-images.githubusercontent.com/16916678/56281431-e15d4a00-610c-11e9-990c-6a2477540535.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:164,reliability,redundan,redundant,164,"@ivirshup @flying-sheep I would remove the `, optional` statement from the docstrings, as, what we mean with this is ""a parameter has a default value"". Hence, it's redundant. However, it's consistently used in all of numpy, scipy, sklearn, pandas, etc. ![image](https://user-images.githubusercontent.com/16916678/56281364-b3780580-610c-11e9-8e40-2f44d3007a19.png). In the auto-generated type annotations, the default values miss completely, and I don't think we'll ever restore the `, optional` descriptor, there. ![image](https://user-images.githubusercontent.com/16916678/56281431-e15d4a00-610c-11e9-990c-6a2477540535.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:470,reliability,restor,restore,470,"@ivirshup @flying-sheep I would remove the `, optional` statement from the docstrings, as, what we mean with this is ""a parameter has a default value"". Hence, it's redundant. However, it's consistently used in all of numpy, scipy, sklearn, pandas, etc. ![image](https://user-images.githubusercontent.com/16916678/56281364-b3780580-610c-11e9-8e40-2f44d3007a19.png). In the auto-generated type annotations, the default values miss completely, and I don't think we'll ever restore the `, optional` descriptor, there. ![image](https://user-images.githubusercontent.com/16916678/56281431-e15d4a00-610c-11e9-990c-6a2477540535.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:164,safety,redund,redundant,164,"@ivirshup @flying-sheep I would remove the `, optional` statement from the docstrings, as, what we mean with this is ""a parameter has a default value"". Hence, it's redundant. However, it's consistently used in all of numpy, scipy, sklearn, pandas, etc. ![image](https://user-images.githubusercontent.com/16916678/56281364-b3780580-610c-11e9-8e40-2f44d3007a19.png). In the auto-generated type annotations, the default values miss completely, and I don't think we'll ever restore the `, optional` descriptor, there. ![image](https://user-images.githubusercontent.com/16916678/56281431-e15d4a00-610c-11e9-990c-6a2477540535.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:429,safety,compl,completely,429,"@ivirshup @flying-sheep I would remove the `, optional` statement from the docstrings, as, what we mean with this is ""a parameter has a default value"". Hence, it's redundant. However, it's consistently used in all of numpy, scipy, sklearn, pandas, etc. ![image](https://user-images.githubusercontent.com/16916678/56281364-b3780580-610c-11e9-8e40-2f44d3007a19.png). In the auto-generated type annotations, the default values miss completely, and I don't think we'll ever restore the `, optional` descriptor, there. ![image](https://user-images.githubusercontent.com/16916678/56281431-e15d4a00-610c-11e9-990c-6a2477540535.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:429,security,compl,completely,429,"@ivirshup @flying-sheep I would remove the `, optional` statement from the docstrings, as, what we mean with this is ""a parameter has a default value"". Hence, it's redundant. However, it's consistently used in all of numpy, scipy, sklearn, pandas, etc. ![image](https://user-images.githubusercontent.com/16916678/56281364-b3780580-610c-11e9-8e40-2f44d3007a19.png). In the auto-generated type annotations, the default values miss completely, and I don't think we'll ever restore the `, optional` descriptor, there. ![image](https://user-images.githubusercontent.com/16916678/56281431-e15d4a00-610c-11e9-990c-6a2477540535.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:189,usability,consist,consistently,189,"@ivirshup @flying-sheep I would remove the `, optional` statement from the docstrings, as, what we mean with this is ""a parameter has a default value"". Hence, it's redundant. However, it's consistently used in all of numpy, scipy, sklearn, pandas, etc. ![image](https://user-images.githubusercontent.com/16916678/56281364-b3780580-610c-11e9-8e40-2f44d3007a19.png). In the auto-generated type annotations, the default values miss completely, and I don't think we'll ever restore the `, optional` descriptor, there. ![image](https://user-images.githubusercontent.com/16916678/56281431-e15d4a00-610c-11e9-990c-6a2477540535.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:270,usability,user,user-images,270,"@ivirshup @flying-sheep I would remove the `, optional` statement from the docstrings, as, what we mean with this is ""a parameter has a default value"". Hence, it's redundant. However, it's consistently used in all of numpy, scipy, sklearn, pandas, etc. ![image](https://user-images.githubusercontent.com/16916678/56281364-b3780580-610c-11e9-8e40-2f44d3007a19.png). In the auto-generated type annotations, the default values miss completely, and I don't think we'll ever restore the `, optional` descriptor, there. ![image](https://user-images.githubusercontent.com/16916678/56281431-e15d4a00-610c-11e9-990c-6a2477540535.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:531,usability,user,user-images,531,"@ivirshup @flying-sheep I would remove the `, optional` statement from the docstrings, as, what we mean with this is ""a parameter has a default value"". Hence, it's redundant. However, it's consistently used in all of numpy, scipy, sklearn, pandas, etc. ![image](https://user-images.githubusercontent.com/16916678/56281364-b3780580-610c-11e9-8e40-2f44d3007a19.png). In the auto-generated type annotations, the default values miss completely, and I don't think we'll ever restore the `, optional` descriptor, there. ![image](https://user-images.githubusercontent.com/16916678/56281431-e15d4a00-610c-11e9-990c-6a2477540535.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:720,availability,state,statement,720,"> This is the standard way of writing a numpydoc returns section. [â€¦] This solution is dropping support for them. It certainly shouldnâ€™t, since the definition lists *are* rendered in other cases. IDK why not here, this should render as a definition list with one item. However, I donâ€™t like indenting the whole section except for the first line, so in case it always works once there are multiple definition list items, I donâ€™t worry too much here. > Also, do you by chance have another simple solution for having the styling of the return sections similar to the parameters section (what numpydoc did :slightly_smiling_face:)? Bold font and spacings around colons? Iâ€™ll figure it out. > I would remove the `, optional` statement from the docstrings, as, what we mean with this is ""a parameter has a default value"". Hence, it's redundant. However, it's consistently used in all of numpy, scipy, sklearn, pandas, etc. We should definitely put the defaults inline, and I also think the â€œoptionalâ€ is redundant. What would it even mean to have â€œa parameter that isnâ€™t optional but has a default valueâ€? Iâ€™m pretty sure people will understand it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:828,availability,redund,redundant,828,"> This is the standard way of writing a numpydoc returns section. [â€¦] This solution is dropping support for them. It certainly shouldnâ€™t, since the definition lists *are* rendered in other cases. IDK why not here, this should render as a definition list with one item. However, I donâ€™t like indenting the whole section except for the first line, so in case it always works once there are multiple definition list items, I donâ€™t worry too much here. > Also, do you by chance have another simple solution for having the styling of the return sections similar to the parameters section (what numpydoc did :slightly_smiling_face:)? Bold font and spacings around colons? Iâ€™ll figure it out. > I would remove the `, optional` statement from the docstrings, as, what we mean with this is ""a parameter has a default value"". Hence, it's redundant. However, it's consistently used in all of numpy, scipy, sklearn, pandas, etc. We should definitely put the defaults inline, and I also think the â€œoptionalâ€ is redundant. What would it even mean to have â€œa parameter that isnâ€™t optional but has a default valueâ€? Iâ€™m pretty sure people will understand it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:853,availability,consist,consistently,853,"> This is the standard way of writing a numpydoc returns section. [â€¦] This solution is dropping support for them. It certainly shouldnâ€™t, since the definition lists *are* rendered in other cases. IDK why not here, this should render as a definition list with one item. However, I donâ€™t like indenting the whole section except for the first line, so in case it always works once there are multiple definition list items, I donâ€™t worry too much here. > Also, do you by chance have another simple solution for having the styling of the return sections similar to the parameters section (what numpydoc did :slightly_smiling_face:)? Bold font and spacings around colons? Iâ€™ll figure it out. > I would remove the `, optional` statement from the docstrings, as, what we mean with this is ""a parameter has a default value"". Hence, it's redundant. However, it's consistently used in all of numpy, scipy, sklearn, pandas, etc. We should definitely put the defaults inline, and I also think the â€œoptionalâ€ is redundant. What would it even mean to have â€œa parameter that isnâ€™t optional but has a default valueâ€? Iâ€™m pretty sure people will understand it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:998,availability,redund,redundant,998,"> This is the standard way of writing a numpydoc returns section. [â€¦] This solution is dropping support for them. It certainly shouldnâ€™t, since the definition lists *are* rendered in other cases. IDK why not here, this should render as a definition list with one item. However, I donâ€™t like indenting the whole section except for the first line, so in case it always works once there are multiple definition list items, I donâ€™t worry too much here. > Also, do you by chance have another simple solution for having the styling of the return sections similar to the parameters section (what numpydoc did :slightly_smiling_face:)? Bold font and spacings around colons? Iâ€™ll figure it out. > I would remove the `, optional` statement from the docstrings, as, what we mean with this is ""a parameter has a default value"". Hence, it's redundant. However, it's consistently used in all of numpy, scipy, sklearn, pandas, etc. We should definitely put the defaults inline, and I also think the â€œoptionalâ€ is redundant. What would it even mean to have â€œa parameter that isnâ€™t optional but has a default valueâ€? Iâ€™m pretty sure people will understand it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:828,deployability,redundan,redundant,828,"> This is the standard way of writing a numpydoc returns section. [â€¦] This solution is dropping support for them. It certainly shouldnâ€™t, since the definition lists *are* rendered in other cases. IDK why not here, this should render as a definition list with one item. However, I donâ€™t like indenting the whole section except for the first line, so in case it always works once there are multiple definition list items, I donâ€™t worry too much here. > Also, do you by chance have another simple solution for having the styling of the return sections similar to the parameters section (what numpydoc did :slightly_smiling_face:)? Bold font and spacings around colons? Iâ€™ll figure it out. > I would remove the `, optional` statement from the docstrings, as, what we mean with this is ""a parameter has a default value"". Hence, it's redundant. However, it's consistently used in all of numpy, scipy, sklearn, pandas, etc. We should definitely put the defaults inline, and I also think the â€œoptionalâ€ is redundant. What would it even mean to have â€œa parameter that isnâ€™t optional but has a default valueâ€? Iâ€™m pretty sure people will understand it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:998,deployability,redundan,redundant,998,"> This is the standard way of writing a numpydoc returns section. [â€¦] This solution is dropping support for them. It certainly shouldnâ€™t, since the definition lists *are* rendered in other cases. IDK why not here, this should render as a definition list with one item. However, I donâ€™t like indenting the whole section except for the first line, so in case it always works once there are multiple definition list items, I donâ€™t worry too much here. > Also, do you by chance have another simple solution for having the styling of the return sections similar to the parameters section (what numpydoc did :slightly_smiling_face:)? Bold font and spacings around colons? Iâ€™ll figure it out. > I would remove the `, optional` statement from the docstrings, as, what we mean with this is ""a parameter has a default value"". Hence, it's redundant. However, it's consistently used in all of numpy, scipy, sklearn, pandas, etc. We should definitely put the defaults inline, and I also think the â€œoptionalâ€ is redundant. What would it even mean to have â€œa parameter that isnâ€™t optional but has a default valueâ€? Iâ€™m pretty sure people will understand it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:720,integrability,state,statement,720,"> This is the standard way of writing a numpydoc returns section. [â€¦] This solution is dropping support for them. It certainly shouldnâ€™t, since the definition lists *are* rendered in other cases. IDK why not here, this should render as a definition list with one item. However, I donâ€™t like indenting the whole section except for the first line, so in case it always works once there are multiple definition list items, I donâ€™t worry too much here. > Also, do you by chance have another simple solution for having the styling of the return sections similar to the parameters section (what numpydoc did :slightly_smiling_face:)? Bold font and spacings around colons? Iâ€™ll figure it out. > I would remove the `, optional` statement from the docstrings, as, what we mean with this is ""a parameter has a default value"". Hence, it's redundant. However, it's consistently used in all of numpy, scipy, sklearn, pandas, etc. We should definitely put the defaults inline, and I also think the â€œoptionalâ€ is redundant. What would it even mean to have â€œa parameter that isnâ€™t optional but has a default valueâ€? Iâ€™m pretty sure people will understand it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:14,interoperability,standard,standard,14,"> This is the standard way of writing a numpydoc returns section. [â€¦] This solution is dropping support for them. It certainly shouldnâ€™t, since the definition lists *are* rendered in other cases. IDK why not here, this should render as a definition list with one item. However, I donâ€™t like indenting the whole section except for the first line, so in case it always works once there are multiple definition list items, I donâ€™t worry too much here. > Also, do you by chance have another simple solution for having the styling of the return sections similar to the parameters section (what numpydoc did :slightly_smiling_face:)? Bold font and spacings around colons? Iâ€™ll figure it out. > I would remove the `, optional` statement from the docstrings, as, what we mean with this is ""a parameter has a default value"". Hence, it's redundant. However, it's consistently used in all of numpy, scipy, sklearn, pandas, etc. We should definitely put the defaults inline, and I also think the â€œoptionalâ€ is redundant. What would it even mean to have â€œa parameter that isnâ€™t optional but has a default valueâ€? Iâ€™m pretty sure people will understand it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:564,modifiability,paramet,parameters,564,"> This is the standard way of writing a numpydoc returns section. [â€¦] This solution is dropping support for them. It certainly shouldnâ€™t, since the definition lists *are* rendered in other cases. IDK why not here, this should render as a definition list with one item. However, I donâ€™t like indenting the whole section except for the first line, so in case it always works once there are multiple definition list items, I donâ€™t worry too much here. > Also, do you by chance have another simple solution for having the styling of the return sections similar to the parameters section (what numpydoc did :slightly_smiling_face:)? Bold font and spacings around colons? Iâ€™ll figure it out. > I would remove the `, optional` statement from the docstrings, as, what we mean with this is ""a parameter has a default value"". Hence, it's redundant. However, it's consistently used in all of numpy, scipy, sklearn, pandas, etc. We should definitely put the defaults inline, and I also think the â€œoptionalâ€ is redundant. What would it even mean to have â€œa parameter that isnâ€™t optional but has a default valueâ€? Iâ€™m pretty sure people will understand it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:784,modifiability,paramet,parameter,784,"> This is the standard way of writing a numpydoc returns section. [â€¦] This solution is dropping support for them. It certainly shouldnâ€™t, since the definition lists *are* rendered in other cases. IDK why not here, this should render as a definition list with one item. However, I donâ€™t like indenting the whole section except for the first line, so in case it always works once there are multiple definition list items, I donâ€™t worry too much here. > Also, do you by chance have another simple solution for having the styling of the return sections similar to the parameters section (what numpydoc did :slightly_smiling_face:)? Bold font and spacings around colons? Iâ€™ll figure it out. > I would remove the `, optional` statement from the docstrings, as, what we mean with this is ""a parameter has a default value"". Hence, it's redundant. However, it's consistently used in all of numpy, scipy, sklearn, pandas, etc. We should definitely put the defaults inline, and I also think the â€œoptionalâ€ is redundant. What would it even mean to have â€œa parameter that isnâ€™t optional but has a default valueâ€? Iâ€™m pretty sure people will understand it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:1044,modifiability,paramet,parameter,1044,"> This is the standard way of writing a numpydoc returns section. [â€¦] This solution is dropping support for them. It certainly shouldnâ€™t, since the definition lists *are* rendered in other cases. IDK why not here, this should render as a definition list with one item. However, I donâ€™t like indenting the whole section except for the first line, so in case it always works once there are multiple definition list items, I donâ€™t worry too much here. > Also, do you by chance have another simple solution for having the styling of the return sections similar to the parameters section (what numpydoc did :slightly_smiling_face:)? Bold font and spacings around colons? Iâ€™ll figure it out. > I would remove the `, optional` statement from the docstrings, as, what we mean with this is ""a parameter has a default value"". Hence, it's redundant. However, it's consistently used in all of numpy, scipy, sklearn, pandas, etc. We should definitely put the defaults inline, and I also think the â€œoptionalâ€ is redundant. What would it even mean to have â€œa parameter that isnâ€™t optional but has a default valueâ€? Iâ€™m pretty sure people will understand it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:828,reliability,redundan,redundant,828,"> This is the standard way of writing a numpydoc returns section. [â€¦] This solution is dropping support for them. It certainly shouldnâ€™t, since the definition lists *are* rendered in other cases. IDK why not here, this should render as a definition list with one item. However, I donâ€™t like indenting the whole section except for the first line, so in case it always works once there are multiple definition list items, I donâ€™t worry too much here. > Also, do you by chance have another simple solution for having the styling of the return sections similar to the parameters section (what numpydoc did :slightly_smiling_face:)? Bold font and spacings around colons? Iâ€™ll figure it out. > I would remove the `, optional` statement from the docstrings, as, what we mean with this is ""a parameter has a default value"". Hence, it's redundant. However, it's consistently used in all of numpy, scipy, sklearn, pandas, etc. We should definitely put the defaults inline, and I also think the â€œoptionalâ€ is redundant. What would it even mean to have â€œa parameter that isnâ€™t optional but has a default valueâ€? Iâ€™m pretty sure people will understand it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:998,reliability,redundan,redundant,998,"> This is the standard way of writing a numpydoc returns section. [â€¦] This solution is dropping support for them. It certainly shouldnâ€™t, since the definition lists *are* rendered in other cases. IDK why not here, this should render as a definition list with one item. However, I donâ€™t like indenting the whole section except for the first line, so in case it always works once there are multiple definition list items, I donâ€™t worry too much here. > Also, do you by chance have another simple solution for having the styling of the return sections similar to the parameters section (what numpydoc did :slightly_smiling_face:)? Bold font and spacings around colons? Iâ€™ll figure it out. > I would remove the `, optional` statement from the docstrings, as, what we mean with this is ""a parameter has a default value"". Hence, it's redundant. However, it's consistently used in all of numpy, scipy, sklearn, pandas, etc. We should definitely put the defaults inline, and I also think the â€œoptionalâ€ is redundant. What would it even mean to have â€œa parameter that isnâ€™t optional but has a default valueâ€? Iâ€™m pretty sure people will understand it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:319,safety,except,except,319,"> This is the standard way of writing a numpydoc returns section. [â€¦] This solution is dropping support for them. It certainly shouldnâ€™t, since the definition lists *are* rendered in other cases. IDK why not here, this should render as a definition list with one item. However, I donâ€™t like indenting the whole section except for the first line, so in case it always works once there are multiple definition list items, I donâ€™t worry too much here. > Also, do you by chance have another simple solution for having the styling of the return sections similar to the parameters section (what numpydoc did :slightly_smiling_face:)? Bold font and spacings around colons? Iâ€™ll figure it out. > I would remove the `, optional` statement from the docstrings, as, what we mean with this is ""a parameter has a default value"". Hence, it's redundant. However, it's consistently used in all of numpy, scipy, sklearn, pandas, etc. We should definitely put the defaults inline, and I also think the â€œoptionalâ€ is redundant. What would it even mean to have â€œa parameter that isnâ€™t optional but has a default valueâ€? Iâ€™m pretty sure people will understand it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:828,safety,redund,redundant,828,"> This is the standard way of writing a numpydoc returns section. [â€¦] This solution is dropping support for them. It certainly shouldnâ€™t, since the definition lists *are* rendered in other cases. IDK why not here, this should render as a definition list with one item. However, I donâ€™t like indenting the whole section except for the first line, so in case it always works once there are multiple definition list items, I donâ€™t worry too much here. > Also, do you by chance have another simple solution for having the styling of the return sections similar to the parameters section (what numpydoc did :slightly_smiling_face:)? Bold font and spacings around colons? Iâ€™ll figure it out. > I would remove the `, optional` statement from the docstrings, as, what we mean with this is ""a parameter has a default value"". Hence, it's redundant. However, it's consistently used in all of numpy, scipy, sklearn, pandas, etc. We should definitely put the defaults inline, and I also think the â€œoptionalâ€ is redundant. What would it even mean to have â€œa parameter that isnâ€™t optional but has a default valueâ€? Iâ€™m pretty sure people will understand it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:998,safety,redund,redundant,998,"> This is the standard way of writing a numpydoc returns section. [â€¦] This solution is dropping support for them. It certainly shouldnâ€™t, since the definition lists *are* rendered in other cases. IDK why not here, this should render as a definition list with one item. However, I donâ€™t like indenting the whole section except for the first line, so in case it always works once there are multiple definition list items, I donâ€™t worry too much here. > Also, do you by chance have another simple solution for having the styling of the return sections similar to the parameters section (what numpydoc did :slightly_smiling_face:)? Bold font and spacings around colons? Iâ€™ll figure it out. > I would remove the `, optional` statement from the docstrings, as, what we mean with this is ""a parameter has a default value"". Hence, it's redundant. However, it's consistently used in all of numpy, scipy, sklearn, pandas, etc. We should definitely put the defaults inline, and I also think the â€œoptionalâ€ is redundant. What would it even mean to have â€œa parameter that isnâ€™t optional but has a default valueâ€? Iâ€™m pretty sure people will understand it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:487,testability,simpl,simple,487,"> This is the standard way of writing a numpydoc returns section. [â€¦] This solution is dropping support for them. It certainly shouldnâ€™t, since the definition lists *are* rendered in other cases. IDK why not here, this should render as a definition list with one item. However, I donâ€™t like indenting the whole section except for the first line, so in case it always works once there are multiple definition list items, I donâ€™t worry too much here. > Also, do you by chance have another simple solution for having the styling of the return sections similar to the parameters section (what numpydoc did :slightly_smiling_face:)? Bold font and spacings around colons? Iâ€™ll figure it out. > I would remove the `, optional` statement from the docstrings, as, what we mean with this is ""a parameter has a default value"". Hence, it's redundant. However, it's consistently used in all of numpy, scipy, sklearn, pandas, etc. We should definitely put the defaults inline, and I also think the â€œoptionalâ€ is redundant. What would it even mean to have â€œa parameter that isnâ€™t optional but has a default valueâ€? Iâ€™m pretty sure people will understand it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:1128,testability,understand,understand,1128,"> This is the standard way of writing a numpydoc returns section. [â€¦] This solution is dropping support for them. It certainly shouldnâ€™t, since the definition lists *are* rendered in other cases. IDK why not here, this should render as a definition list with one item. However, I donâ€™t like indenting the whole section except for the first line, so in case it always works once there are multiple definition list items, I donâ€™t worry too much here. > Also, do you by chance have another simple solution for having the styling of the return sections similar to the parameters section (what numpydoc did :slightly_smiling_face:)? Bold font and spacings around colons? Iâ€™ll figure it out. > I would remove the `, optional` statement from the docstrings, as, what we mean with this is ""a parameter has a default value"". Hence, it's redundant. However, it's consistently used in all of numpy, scipy, sklearn, pandas, etc. We should definitely put the defaults inline, and I also think the â€œoptionalâ€ is redundant. What would it even mean to have â€œa parameter that isnâ€™t optional but has a default valueâ€? Iâ€™m pretty sure people will understand it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:96,usability,support,support,96,"> This is the standard way of writing a numpydoc returns section. [â€¦] This solution is dropping support for them. It certainly shouldnâ€™t, since the definition lists *are* rendered in other cases. IDK why not here, this should render as a definition list with one item. However, I donâ€™t like indenting the whole section except for the first line, so in case it always works once there are multiple definition list items, I donâ€™t worry too much here. > Also, do you by chance have another simple solution for having the styling of the return sections similar to the parameters section (what numpydoc did :slightly_smiling_face:)? Bold font and spacings around colons? Iâ€™ll figure it out. > I would remove the `, optional` statement from the docstrings, as, what we mean with this is ""a parameter has a default value"". Hence, it's redundant. However, it's consistently used in all of numpy, scipy, sklearn, pandas, etc. We should definitely put the defaults inline, and I also think the â€œoptionalâ€ is redundant. What would it even mean to have â€œa parameter that isnâ€™t optional but has a default valueâ€? Iâ€™m pretty sure people will understand it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:487,usability,simpl,simple,487,"> This is the standard way of writing a numpydoc returns section. [â€¦] This solution is dropping support for them. It certainly shouldnâ€™t, since the definition lists *are* rendered in other cases. IDK why not here, this should render as a definition list with one item. However, I donâ€™t like indenting the whole section except for the first line, so in case it always works once there are multiple definition list items, I donâ€™t worry too much here. > Also, do you by chance have another simple solution for having the styling of the return sections similar to the parameters section (what numpydoc did :slightly_smiling_face:)? Bold font and spacings around colons? Iâ€™ll figure it out. > I would remove the `, optional` statement from the docstrings, as, what we mean with this is ""a parameter has a default value"". Hence, it's redundant. However, it's consistently used in all of numpy, scipy, sklearn, pandas, etc. We should definitely put the defaults inline, and I also think the â€œoptionalâ€ is redundant. What would it even mean to have â€œa parameter that isnâ€™t optional but has a default valueâ€? Iâ€™m pretty sure people will understand it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:853,usability,consist,consistently,853,"> This is the standard way of writing a numpydoc returns section. [â€¦] This solution is dropping support for them. It certainly shouldnâ€™t, since the definition lists *are* rendered in other cases. IDK why not here, this should render as a definition list with one item. However, I donâ€™t like indenting the whole section except for the first line, so in case it always works once there are multiple definition list items, I donâ€™t worry too much here. > Also, do you by chance have another simple solution for having the styling of the return sections similar to the parameters section (what numpydoc did :slightly_smiling_face:)? Bold font and spacings around colons? Iâ€™ll figure it out. > I would remove the `, optional` statement from the docstrings, as, what we mean with this is ""a parameter has a default value"". Hence, it's redundant. However, it's consistently used in all of numpy, scipy, sklearn, pandas, etc. We should definitely put the defaults inline, and I also think the â€œoptionalâ€ is redundant. What would it even mean to have â€œa parameter that isnâ€™t optional but has a default valueâ€? Iâ€™m pretty sure people will understand it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:218,safety,except,except,218,"> IDK why not here, this should render as a definition list with one item. Hm, maybe because a new line is missing before the start of the list. I don't know now.... > However, I donâ€™t like indenting the whole section except for the first line, so in case it always works once there are multiple definition list items, I donâ€™t worry too much here. Absolutely agreed! So, let's just forget about the one-item definition list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:234,deployability,Depend,Depending,234,"I figured the one-item-thing out: The emitted code is:. ```rst. :param copy: If an :class:`~anndata.AnnData` is passed, determines whether a copy. is returned. :type copy: `bool`, optional (default: `False`). :returns: AnnData, None. Depending on `copy` returns or updates `adata` with the corrected data matrix. ```. And since `:returns:` is part of a field list, and field lists are defined by the indentation of the *block starting in the second line*, the additional indentation of the second line is ignored. So yes, only numpydoc-style sections with one item are affected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:265,deployability,updat,updates,265,"I figured the one-item-thing out: The emitted code is:. ```rst. :param copy: If an :class:`~anndata.AnnData` is passed, determines whether a copy. is returned. :type copy: `bool`, optional (default: `False`). :returns: AnnData, None. Depending on `copy` returns or updates `adata` with the corrected data matrix. ```. And since `:returns:` is part of a field list, and field lists are defined by the indentation of the *block starting in the second line*, the additional indentation of the second line is ignored. So yes, only numpydoc-style sections with one item are affected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:234,integrability,Depend,Depending,234,"I figured the one-item-thing out: The emitted code is:. ```rst. :param copy: If an :class:`~anndata.AnnData` is passed, determines whether a copy. is returned. :type copy: `bool`, optional (default: `False`). :returns: AnnData, None. Depending on `copy` returns or updates `adata` with the corrected data matrix. ```. And since `:returns:` is part of a field list, and field lists are defined by the indentation of the *block starting in the second line*, the additional indentation of the second line is ignored. So yes, only numpydoc-style sections with one item are affected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:234,modifiability,Depend,Depending,234,"I figured the one-item-thing out: The emitted code is:. ```rst. :param copy: If an :class:`~anndata.AnnData` is passed, determines whether a copy. is returned. :type copy: `bool`, optional (default: `False`). :returns: AnnData, None. Depending on `copy` returns or updates `adata` with the corrected data matrix. ```. And since `:returns:` is part of a field list, and field lists are defined by the indentation of the *block starting in the second line*, the additional indentation of the second line is ignored. So yes, only numpydoc-style sections with one item are affected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:234,safety,Depend,Depending,234,"I figured the one-item-thing out: The emitted code is:. ```rst. :param copy: If an :class:`~anndata.AnnData` is passed, determines whether a copy. is returned. :type copy: `bool`, optional (default: `False`). :returns: AnnData, None. Depending on `copy` returns or updates `adata` with the corrected data matrix. ```. And since `:returns:` is part of a field list, and field lists are defined by the indentation of the *block starting in the second line*, the additional indentation of the second line is ignored. So yes, only numpydoc-style sections with one item are affected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:265,safety,updat,updates,265,"I figured the one-item-thing out: The emitted code is:. ```rst. :param copy: If an :class:`~anndata.AnnData` is passed, determines whether a copy. is returned. :type copy: `bool`, optional (default: `False`). :returns: AnnData, None. Depending on `copy` returns or updates `adata` with the corrected data matrix. ```. And since `:returns:` is part of a field list, and field lists are defined by the indentation of the *block starting in the second line*, the additional indentation of the second line is ignored. So yes, only numpydoc-style sections with one item are affected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:265,security,updat,updates,265,"I figured the one-item-thing out: The emitted code is:. ```rst. :param copy: If an :class:`~anndata.AnnData` is passed, determines whether a copy. is returned. :type copy: `bool`, optional (default: `False`). :returns: AnnData, None. Depending on `copy` returns or updates `adata` with the corrected data matrix. ```. And since `:returns:` is part of a field list, and field lists are defined by the indentation of the *block starting in the second line*, the additional indentation of the second line is ignored. So yes, only numpydoc-style sections with one item are affected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:234,testability,Depend,Depending,234,"I figured the one-item-thing out: The emitted code is:. ```rst. :param copy: If an :class:`~anndata.AnnData` is passed, determines whether a copy. is returned. :type copy: `bool`, optional (default: `False`). :returns: AnnData, None. Depending on `copy` returns or updates `adata` with the corrected data matrix. ```. And since `:returns:` is part of a field list, and field lists are defined by the indentation of the *block starting in the second line*, the additional indentation of the second line is ignored. So yes, only numpydoc-style sections with one item are affected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:212,modifiability,paramet,parameter,212,"This should work for now. The problem is that this really needs to live in scanpydoc, where we have access to the fancy parsing. My plan is to merge this now, which has a solution for the simple case of. ```rst. parameter : some.type. Description. ```. Later Iâ€™ll introduce the same behavior as what scanpydoc does with the parameters:. If the return type is `...) -> Tuple[foo.bar, baz.zab]:`, then Iâ€™ll check if thereâ€™s a section like. ```rst. one_identifier. Desc. second_identifier. Desc. ```. and replace them with the same code as parameters. ----. This leaves 3 styles:. 1. Prose for a single return value. 2. The above for returning a tuple. 3. â€œthis function adds some AnnData.obs/var fieldsâ€. For 3., we have like 3 styles floating around and need to fix one:. 1. ``**dpt_pseudotime** : :class:`pandas.Series` (`adata.obs`, dtype `float`)``. 2. ``` ``adata.obs['louvain']`` (:class:`pandas.Series`, dtype ``category``) ```. 3. `` `adata.uns['leiden']['params']` : `dict` ``",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:324,modifiability,paramet,parameters,324,"This should work for now. The problem is that this really needs to live in scanpydoc, where we have access to the fancy parsing. My plan is to merge this now, which has a solution for the simple case of. ```rst. parameter : some.type. Description. ```. Later Iâ€™ll introduce the same behavior as what scanpydoc does with the parameters:. If the return type is `...) -> Tuple[foo.bar, baz.zab]:`, then Iâ€™ll check if thereâ€™s a section like. ```rst. one_identifier. Desc. second_identifier. Desc. ```. and replace them with the same code as parameters. ----. This leaves 3 styles:. 1. Prose for a single return value. 2. The above for returning a tuple. 3. â€œthis function adds some AnnData.obs/var fieldsâ€. For 3., we have like 3 styles floating around and need to fix one:. 1. ``**dpt_pseudotime** : :class:`pandas.Series` (`adata.obs`, dtype `float`)``. 2. ``` ``adata.obs['louvain']`` (:class:`pandas.Series`, dtype ``category``) ```. 3. `` `adata.uns['leiden']['params']` : `dict` ``",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:537,modifiability,paramet,parameters,537,"This should work for now. The problem is that this really needs to live in scanpydoc, where we have access to the fancy parsing. My plan is to merge this now, which has a solution for the simple case of. ```rst. parameter : some.type. Description. ```. Later Iâ€™ll introduce the same behavior as what scanpydoc does with the parameters:. If the return type is `...) -> Tuple[foo.bar, baz.zab]:`, then Iâ€™ll check if thereâ€™s a section like. ```rst. one_identifier. Desc. second_identifier. Desc. ```. and replace them with the same code as parameters. ----. This leaves 3 styles:. 1. Prose for a single return value. 2. The above for returning a tuple. 3. â€œthis function adds some AnnData.obs/var fieldsâ€. For 3., we have like 3 styles floating around and need to fix one:. 1. ``**dpt_pseudotime** : :class:`pandas.Series` (`adata.obs`, dtype `float`)``. 2. ``` ``adata.obs['louvain']`` (:class:`pandas.Series`, dtype ``category``) ```. 3. `` `adata.uns['leiden']['params']` : `dict` ``",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:310,reliability,doe,does,310,"This should work for now. The problem is that this really needs to live in scanpydoc, where we have access to the fancy parsing. My plan is to merge this now, which has a solution for the simple case of. ```rst. parameter : some.type. Description. ```. Later Iâ€™ll introduce the same behavior as what scanpydoc does with the parameters:. If the return type is `...) -> Tuple[foo.bar, baz.zab]:`, then Iâ€™ll check if thereâ€™s a section like. ```rst. one_identifier. Desc. second_identifier. Desc. ```. and replace them with the same code as parameters. ----. This leaves 3 styles:. 1. Prose for a single return value. 2. The above for returning a tuple. 3. â€œthis function adds some AnnData.obs/var fieldsâ€. For 3., we have like 3 styles floating around and need to fix one:. 1. ``**dpt_pseudotime** : :class:`pandas.Series` (`adata.obs`, dtype `float`)``. 2. ``` ``adata.obs['louvain']`` (:class:`pandas.Series`, dtype ``category``) ```. 3. `` `adata.uns['leiden']['params']` : `dict` ``",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:100,security,access,access,100,"This should work for now. The problem is that this really needs to live in scanpydoc, where we have access to the fancy parsing. My plan is to merge this now, which has a solution for the simple case of. ```rst. parameter : some.type. Description. ```. Later Iâ€™ll introduce the same behavior as what scanpydoc does with the parameters:. If the return type is `...) -> Tuple[foo.bar, baz.zab]:`, then Iâ€™ll check if thereâ€™s a section like. ```rst. one_identifier. Desc. second_identifier. Desc. ```. and replace them with the same code as parameters. ----. This leaves 3 styles:. 1. Prose for a single return value. 2. The above for returning a tuple. 3. â€œthis function adds some AnnData.obs/var fieldsâ€. For 3., we have like 3 styles floating around and need to fix one:. 1. ``**dpt_pseudotime** : :class:`pandas.Series` (`adata.obs`, dtype `float`)``. 2. ``` ``adata.obs['louvain']`` (:class:`pandas.Series`, dtype ``category``) ```. 3. `` `adata.uns['leiden']['params']` : `dict` ``",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:132,testability,plan,plan,132,"This should work for now. The problem is that this really needs to live in scanpydoc, where we have access to the fancy parsing. My plan is to merge this now, which has a solution for the simple case of. ```rst. parameter : some.type. Description. ```. Later Iâ€™ll introduce the same behavior as what scanpydoc does with the parameters:. If the return type is `...) -> Tuple[foo.bar, baz.zab]:`, then Iâ€™ll check if thereâ€™s a section like. ```rst. one_identifier. Desc. second_identifier. Desc. ```. and replace them with the same code as parameters. ----. This leaves 3 styles:. 1. Prose for a single return value. 2. The above for returning a tuple. 3. â€œthis function adds some AnnData.obs/var fieldsâ€. For 3., we have like 3 styles floating around and need to fix one:. 1. ``**dpt_pseudotime** : :class:`pandas.Series` (`adata.obs`, dtype `float`)``. 2. ``` ``adata.obs['louvain']`` (:class:`pandas.Series`, dtype ``category``) ```. 3. `` `adata.uns['leiden']['params']` : `dict` ``",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:188,testability,simpl,simple,188,"This should work for now. The problem is that this really needs to live in scanpydoc, where we have access to the fancy parsing. My plan is to merge this now, which has a solution for the simple case of. ```rst. parameter : some.type. Description. ```. Later Iâ€™ll introduce the same behavior as what scanpydoc does with the parameters:. If the return type is `...) -> Tuple[foo.bar, baz.zab]:`, then Iâ€™ll check if thereâ€™s a section like. ```rst. one_identifier. Desc. second_identifier. Desc. ```. and replace them with the same code as parameters. ----. This leaves 3 styles:. 1. Prose for a single return value. 2. The above for returning a tuple. 3. â€œthis function adds some AnnData.obs/var fieldsâ€. For 3., we have like 3 styles floating around and need to fix one:. 1. ``**dpt_pseudotime** : :class:`pandas.Series` (`adata.obs`, dtype `float`)``. 2. ``` ``adata.obs['louvain']`` (:class:`pandas.Series`, dtype ``category``) ```. 3. `` `adata.uns['leiden']['params']` : `dict` ``",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:188,usability,simpl,simple,188,"This should work for now. The problem is that this really needs to live in scanpydoc, where we have access to the fancy parsing. My plan is to merge this now, which has a solution for the simple case of. ```rst. parameter : some.type. Description. ```. Later Iâ€™ll introduce the same behavior as what scanpydoc does with the parameters:. If the return type is `...) -> Tuple[foo.bar, baz.zab]:`, then Iâ€™ll check if thereâ€™s a section like. ```rst. one_identifier. Desc. second_identifier. Desc. ```. and replace them with the same code as parameters. ----. This leaves 3 styles:. 1. Prose for a single return value. 2. The above for returning a tuple. 3. â€œthis function adds some AnnData.obs/var fieldsâ€. For 3., we have like 3 styles floating around and need to fix one:. 1. ``**dpt_pseudotime** : :class:`pandas.Series` (`adata.obs`, dtype `float`)``. 2. ``` ``adata.obs['louvain']`` (:class:`pandas.Series`, dtype ``category``) ```. 3. `` `adata.uns['leiden']['params']` : `dict` ``",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:283,usability,behavi,behavior,283,"This should work for now. The problem is that this really needs to live in scanpydoc, where we have access to the fancy parsing. My plan is to merge this now, which has a solution for the simple case of. ```rst. parameter : some.type. Description. ```. Later Iâ€™ll introduce the same behavior as what scanpydoc does with the parameters:. If the return type is `...) -> Tuple[foo.bar, baz.zab]:`, then Iâ€™ll check if thereâ€™s a section like. ```rst. one_identifier. Desc. second_identifier. Desc. ```. and replace them with the same code as parameters. ----. This leaves 3 styles:. 1. Prose for a single return value. 2. The above for returning a tuple. 3. â€œthis function adds some AnnData.obs/var fieldsâ€. For 3., we have like 3 styles floating around and need to fix one:. 1. ``**dpt_pseudotime** : :class:`pandas.Series` (`adata.obs`, dtype `float`)``. 2. ``` ``adata.obs['louvain']`` (:class:`pandas.Series`, dtype ``category``) ```. 3. `` `adata.uns['leiden']['params']` : `dict` ``",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:481,testability,simpl,simple,481,"As mentioned in the commit, fantastic! :smile:. Which style mentioned in https://github.com/theislab/scanpy/pull/610#issuecomment-484124854 did you decide for? Most manual corrections I saw in your PR pointed to solution (1) [`**dpt_pseudotime** : :class:`pandas.Series` (`adata.obs`, dtype `float`)`]. That's fine, but we should mention it in `CONTRIBUTING.md`. ðŸ™‚What do you think, @flying-sheep? ----. Why did you decide against the sklearn-style solution, which would have been simple to get without manual fixes (adding `**name**`)? https://github.com/theislab/scanpy/pull/610#issuecomment-484027492.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:481,usability,simpl,simple,481,"As mentioned in the commit, fantastic! :smile:. Which style mentioned in https://github.com/theislab/scanpy/pull/610#issuecomment-484124854 did you decide for? Most manual corrections I saw in your PR pointed to solution (1) [`**dpt_pseudotime** : :class:`pandas.Series` (`adata.obs`, dtype `float`)`]. That's fine, but we should mention it in `CONTRIBUTING.md`. ðŸ™‚What do you think, @flying-sheep? ----. Why did you decide against the sklearn-style solution, which would have been simple to get without manual fixes (adding `**name**`)? https://github.com/theislab/scanpy/pull/610#issuecomment-484027492.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:208,deployability,automat,automatic,208,"I think you misunderstood a bit. 1. Thereâ€™s three *types* of return sections â€“ prose, tuple, added anndata.obs/var fields. 2. Thereâ€™s *one style* for tuple return sections (like parameters). I implemented an automatic way to handle simple `name : type` cases of those and manually formatted the rest. Iâ€™ll automate the other cases in scanpydoc, then we can remove the manually formatted ones. 3. Thereâ€™s *3 styles* for anndata.obs/var field return sections. I left them as they are for now, since we have to decide one. I only reformatted tuple return sections, I neither formatted nor decided on anything about anndata.obs/var field return sections. I hope I was more clear this time :smile: .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:306,deployability,automat,automate,306,"I think you misunderstood a bit. 1. Thereâ€™s three *types* of return sections â€“ prose, tuple, added anndata.obs/var fields. 2. Thereâ€™s *one style* for tuple return sections (like parameters). I implemented an automatic way to handle simple `name : type` cases of those and manually formatted the rest. Iâ€™ll automate the other cases in scanpydoc, then we can remove the manually formatted ones. 3. Thereâ€™s *3 styles* for anndata.obs/var field return sections. I left them as they are for now, since we have to decide one. I only reformatted tuple return sections, I neither formatted nor decided on anything about anndata.obs/var field return sections. I hope I was more clear this time :smile: .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:281,interoperability,format,formatted,281,"I think you misunderstood a bit. 1. Thereâ€™s three *types* of return sections â€“ prose, tuple, added anndata.obs/var fields. 2. Thereâ€™s *one style* for tuple return sections (like parameters). I implemented an automatic way to handle simple `name : type` cases of those and manually formatted the rest. Iâ€™ll automate the other cases in scanpydoc, then we can remove the manually formatted ones. 3. Thereâ€™s *3 styles* for anndata.obs/var field return sections. I left them as they are for now, since we have to decide one. I only reformatted tuple return sections, I neither formatted nor decided on anything about anndata.obs/var field return sections. I hope I was more clear this time :smile: .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:377,interoperability,format,formatted,377,"I think you misunderstood a bit. 1. Thereâ€™s three *types* of return sections â€“ prose, tuple, added anndata.obs/var fields. 2. Thereâ€™s *one style* for tuple return sections (like parameters). I implemented an automatic way to handle simple `name : type` cases of those and manually formatted the rest. Iâ€™ll automate the other cases in scanpydoc, then we can remove the manually formatted ones. 3. Thereâ€™s *3 styles* for anndata.obs/var field return sections. I left them as they are for now, since we have to decide one. I only reformatted tuple return sections, I neither formatted nor decided on anything about anndata.obs/var field return sections. I hope I was more clear this time :smile: .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:572,interoperability,format,formatted,572,"I think you misunderstood a bit. 1. Thereâ€™s three *types* of return sections â€“ prose, tuple, added anndata.obs/var fields. 2. Thereâ€™s *one style* for tuple return sections (like parameters). I implemented an automatic way to handle simple `name : type` cases of those and manually formatted the rest. Iâ€™ll automate the other cases in scanpydoc, then we can remove the manually formatted ones. 3. Thereâ€™s *3 styles* for anndata.obs/var field return sections. I left them as they are for now, since we have to decide one. I only reformatted tuple return sections, I neither formatted nor decided on anything about anndata.obs/var field return sections. I hope I was more clear this time :smile: .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:178,modifiability,paramet,parameters,178,"I think you misunderstood a bit. 1. Thereâ€™s three *types* of return sections â€“ prose, tuple, added anndata.obs/var fields. 2. Thereâ€™s *one style* for tuple return sections (like parameters). I implemented an automatic way to handle simple `name : type` cases of those and manually formatted the rest. Iâ€™ll automate the other cases in scanpydoc, then we can remove the manually formatted ones. 3. Thereâ€™s *3 styles* for anndata.obs/var field return sections. I left them as they are for now, since we have to decide one. I only reformatted tuple return sections, I neither formatted nor decided on anything about anndata.obs/var field return sections. I hope I was more clear this time :smile: .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:680,performance,time,time,680,"I think you misunderstood a bit. 1. Thereâ€™s three *types* of return sections â€“ prose, tuple, added anndata.obs/var fields. 2. Thereâ€™s *one style* for tuple return sections (like parameters). I implemented an automatic way to handle simple `name : type` cases of those and manually formatted the rest. Iâ€™ll automate the other cases in scanpydoc, then we can remove the manually formatted ones. 3. Thereâ€™s *3 styles* for anndata.obs/var field return sections. I left them as they are for now, since we have to decide one. I only reformatted tuple return sections, I neither formatted nor decided on anything about anndata.obs/var field return sections. I hope I was more clear this time :smile: .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:208,testability,automat,automatic,208,"I think you misunderstood a bit. 1. Thereâ€™s three *types* of return sections â€“ prose, tuple, added anndata.obs/var fields. 2. Thereâ€™s *one style* for tuple return sections (like parameters). I implemented an automatic way to handle simple `name : type` cases of those and manually formatted the rest. Iâ€™ll automate the other cases in scanpydoc, then we can remove the manually formatted ones. 3. Thereâ€™s *3 styles* for anndata.obs/var field return sections. I left them as they are for now, since we have to decide one. I only reformatted tuple return sections, I neither formatted nor decided on anything about anndata.obs/var field return sections. I hope I was more clear this time :smile: .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:232,testability,simpl,simple,232,"I think you misunderstood a bit. 1. Thereâ€™s three *types* of return sections â€“ prose, tuple, added anndata.obs/var fields. 2. Thereâ€™s *one style* for tuple return sections (like parameters). I implemented an automatic way to handle simple `name : type` cases of those and manually formatted the rest. Iâ€™ll automate the other cases in scanpydoc, then we can remove the manually formatted ones. 3. Thereâ€™s *3 styles* for anndata.obs/var field return sections. I left them as they are for now, since we have to decide one. I only reformatted tuple return sections, I neither formatted nor decided on anything about anndata.obs/var field return sections. I hope I was more clear this time :smile: .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:306,testability,automat,automate,306,"I think you misunderstood a bit. 1. Thereâ€™s three *types* of return sections â€“ prose, tuple, added anndata.obs/var fields. 2. Thereâ€™s *one style* for tuple return sections (like parameters). I implemented an automatic way to handle simple `name : type` cases of those and manually formatted the rest. Iâ€™ll automate the other cases in scanpydoc, then we can remove the manually formatted ones. 3. Thereâ€™s *3 styles* for anndata.obs/var field return sections. I left them as they are for now, since we have to decide one. I only reformatted tuple return sections, I neither formatted nor decided on anything about anndata.obs/var field return sections. I hope I was more clear this time :smile: .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:232,usability,simpl,simple,232,"I think you misunderstood a bit. 1. Thereâ€™s three *types* of return sections â€“ prose, tuple, added anndata.obs/var fields. 2. Thereâ€™s *one style* for tuple return sections (like parameters). I implemented an automatic way to handle simple `name : type` cases of those and manually formatted the rest. Iâ€™ll automate the other cases in scanpydoc, then we can remove the manually formatted ones. 3. Thereâ€™s *3 styles* for anndata.obs/var field return sections. I left them as they are for now, since we have to decide one. I only reformatted tuple return sections, I neither formatted nor decided on anything about anndata.obs/var field return sections. I hope I was more clear this time :smile: .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:669,usability,clear,clear,669,"I think you misunderstood a bit. 1. Thereâ€™s three *types* of return sections â€“ prose, tuple, added anndata.obs/var fields. 2. Thereâ€™s *one style* for tuple return sections (like parameters). I implemented an automatic way to handle simple `name : type` cases of those and manually formatted the rest. Iâ€™ll automate the other cases in scanpydoc, then we can remove the manually formatted ones. 3. Thereâ€™s *3 styles* for anndata.obs/var field return sections. I left them as they are for now, since we have to decide one. I only reformatted tuple return sections, I neither formatted nor decided on anything about anndata.obs/var field return sections. I hope I was more clear this time :smile: .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:512,availability,consist,consistent,512,"This is unrelated to your post just now. :smile:. Also, `Parameters` etc. don't render as a heading (`rubric`) like the other sections (`Notes`, `Examples`, `See also` etc.) anymore. They simply render as . ```. <dl class=""field-list simple"">. <dt class=""field-odd"">Parameters</dt>. ```. whereas. ```. <p class=""rubric"">Notes</p>. ```. This is the new styling decision around numpydoc, also visible here: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html. In order to have it look consistent (as in sklearn), I think we do need. ```. .rst-content dl:not(.docutils) dl dt {. font-weight: bold;. font-size: 16px;. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:437,deployability,modul,modules,437,"This is unrelated to your post just now. :smile:. Also, `Parameters` etc. don't render as a heading (`rubric`) like the other sections (`Notes`, `Examples`, `See also` etc.) anymore. They simply render as . ```. <dl class=""field-list simple"">. <dt class=""field-odd"">Parameters</dt>. ```. whereas. ```. <p class=""rubric"">Notes</p>. ```. This is the new styling decision around numpydoc, also visible here: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html. In order to have it look consistent (as in sklearn), I think we do need. ```. .rst-content dl:not(.docutils) dl dt {. font-weight: bold;. font-size: 16px;. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:57,modifiability,Paramet,Parameters,57,"This is unrelated to your post just now. :smile:. Also, `Parameters` etc. don't render as a heading (`rubric`) like the other sections (`Notes`, `Examples`, `See also` etc.) anymore. They simply render as . ```. <dl class=""field-list simple"">. <dt class=""field-odd"">Parameters</dt>. ```. whereas. ```. <p class=""rubric"">Notes</p>. ```. This is the new styling decision around numpydoc, also visible here: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html. In order to have it look consistent (as in sklearn), I think we do need. ```. .rst-content dl:not(.docutils) dl dt {. font-weight: bold;. font-size: 16px;. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:266,modifiability,Paramet,Parameters,266,"This is unrelated to your post just now. :smile:. Also, `Parameters` etc. don't render as a heading (`rubric`) like the other sections (`Notes`, `Examples`, `See also` etc.) anymore. They simply render as . ```. <dl class=""field-list simple"">. <dt class=""field-odd"">Parameters</dt>. ```. whereas. ```. <p class=""rubric"">Notes</p>. ```. This is the new styling decision around numpydoc, also visible here: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html. In order to have it look consistent (as in sklearn), I think we do need. ```. .rst-content dl:not(.docutils) dl dt {. font-weight: bold;. font-size: 16px;. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:437,modifiability,modul,modules,437,"This is unrelated to your post just now. :smile:. Also, `Parameters` etc. don't render as a heading (`rubric`) like the other sections (`Notes`, `Examples`, `See also` etc.) anymore. They simply render as . ```. <dl class=""field-list simple"">. <dt class=""field-odd"">Parameters</dt>. ```. whereas. ```. <p class=""rubric"">Notes</p>. ```. This is the new styling decision around numpydoc, also visible here: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html. In order to have it look consistent (as in sklearn), I think we do need. ```. .rst-content dl:not(.docutils) dl dt {. font-weight: bold;. font-size: 16px;. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:463,modifiability,deco,decomposition,463,"This is unrelated to your post just now. :smile:. Also, `Parameters` etc. don't render as a heading (`rubric`) like the other sections (`Notes`, `Examples`, `See also` etc.) anymore. They simply render as . ```. <dl class=""field-list simple"">. <dt class=""field-odd"">Parameters</dt>. ```. whereas. ```. <p class=""rubric"">Notes</p>. ```. This is the new styling decision around numpydoc, also visible here: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html. In order to have it look consistent (as in sklearn), I think we do need. ```. .rst-content dl:not(.docutils) dl dt {. font-weight: bold;. font-size: 16px;. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:570,performance,content,content,570,"This is unrelated to your post just now. :smile:. Also, `Parameters` etc. don't render as a heading (`rubric`) like the other sections (`Notes`, `Examples`, `See also` etc.) anymore. They simply render as . ```. <dl class=""field-list simple"">. <dt class=""field-odd"">Parameters</dt>. ```. whereas. ```. <p class=""rubric"">Notes</p>. ```. This is the new styling decision around numpydoc, also visible here: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html. In order to have it look consistent (as in sklearn), I think we do need. ```. .rst-content dl:not(.docutils) dl dt {. font-weight: bold;. font-size: 16px;. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:437,safety,modul,modules,437,"This is unrelated to your post just now. :smile:. Also, `Parameters` etc. don't render as a heading (`rubric`) like the other sections (`Notes`, `Examples`, `See also` etc.) anymore. They simply render as . ```. <dl class=""field-list simple"">. <dt class=""field-odd"">Parameters</dt>. ```. whereas. ```. <p class=""rubric"">Notes</p>. ```. This is the new styling decision around numpydoc, also visible here: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html. In order to have it look consistent (as in sklearn), I think we do need. ```. .rst-content dl:not(.docutils) dl dt {. font-weight: bold;. font-size: 16px;. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:188,testability,simpl,simply,188,"This is unrelated to your post just now. :smile:. Also, `Parameters` etc. don't render as a heading (`rubric`) like the other sections (`Notes`, `Examples`, `See also` etc.) anymore. They simply render as . ```. <dl class=""field-list simple"">. <dt class=""field-odd"">Parameters</dt>. ```. whereas. ```. <p class=""rubric"">Notes</p>. ```. This is the new styling decision around numpydoc, also visible here: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html. In order to have it look consistent (as in sklearn), I think we do need. ```. .rst-content dl:not(.docutils) dl dt {. font-weight: bold;. font-size: 16px;. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:234,testability,simpl,simple,234,"This is unrelated to your post just now. :smile:. Also, `Parameters` etc. don't render as a heading (`rubric`) like the other sections (`Notes`, `Examples`, `See also` etc.) anymore. They simply render as . ```. <dl class=""field-list simple"">. <dt class=""field-odd"">Parameters</dt>. ```. whereas. ```. <p class=""rubric"">Notes</p>. ```. This is the new styling decision around numpydoc, also visible here: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html. In order to have it look consistent (as in sklearn), I think we do need. ```. .rst-content dl:not(.docutils) dl dt {. font-weight: bold;. font-size: 16px;. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:188,usability,simpl,simply,188,"This is unrelated to your post just now. :smile:. Also, `Parameters` etc. don't render as a heading (`rubric`) like the other sections (`Notes`, `Examples`, `See also` etc.) anymore. They simply render as . ```. <dl class=""field-list simple"">. <dt class=""field-odd"">Parameters</dt>. ```. whereas. ```. <p class=""rubric"">Notes</p>. ```. This is the new styling decision around numpydoc, also visible here: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html. In order to have it look consistent (as in sklearn), I think we do need. ```. .rst-content dl:not(.docutils) dl dt {. font-weight: bold;. font-size: 16px;. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:234,usability,simpl,simple,234,"This is unrelated to your post just now. :smile:. Also, `Parameters` etc. don't render as a heading (`rubric`) like the other sections (`Notes`, `Examples`, `See also` etc.) anymore. They simply render as . ```. <dl class=""field-list simple"">. <dt class=""field-odd"">Parameters</dt>. ```. whereas. ```. <p class=""rubric"">Notes</p>. ```. This is the new styling decision around numpydoc, also visible here: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html. In order to have it look consistent (as in sklearn), I think we do need. ```. .rst-content dl:not(.docutils) dl dt {. font-weight: bold;. font-size: 16px;. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:420,usability,learn,learn,420,"This is unrelated to your post just now. :smile:. Also, `Parameters` etc. don't render as a heading (`rubric`) like the other sections (`Notes`, `Examples`, `See also` etc.) anymore. They simply render as . ```. <dl class=""field-list simple"">. <dt class=""field-odd"">Parameters</dt>. ```. whereas. ```. <p class=""rubric"">Notes</p>. ```. This is the new styling decision around numpydoc, also visible here: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html. In order to have it look consistent (as in sklearn), I think we do need. ```. .rst-content dl:not(.docutils) dl dt {. font-weight: bold;. font-size: 16px;. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:512,usability,consist,consistent,512,"This is unrelated to your post just now. :smile:. Also, `Parameters` etc. don't render as a heading (`rubric`) like the other sections (`Notes`, `Examples`, `See also` etc.) anymore. They simply render as . ```. <dl class=""field-list simple"">. <dt class=""field-odd"">Parameters</dt>. ```. whereas. ```. <p class=""rubric"">Notes</p>. ```. This is the new styling decision around numpydoc, also visible here: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html. In order to have it look consistent (as in sklearn), I think we do need. ```. .rst-content dl:not(.docutils) dl dt {. font-weight: bold;. font-size: 16px;. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:140,availability,consist,consistency,140,https://github.com/theislab/scanpy/pull/610#issuecomment-484427693: Great! Let's leave the remaining ones as a task for moving towards more consistency in the future. :smile:,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:140,usability,consist,consistency,140,https://github.com/theislab/scanpy/pull/610#issuecomment-484427693: Great! Let's leave the remaining ones as a task for moving towards more consistency in the future. :smile:,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:31,availability,consist,consistent,31,Adapted the css to enforce the consistent styling of headings on each page: https://github.com/theislab/scanpy/commit/1f579f6f745d01c599a39f38abadb8a32f95c12b,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:0,energy efficiency,Adapt,Adapted,0,Adapted the css to enforce the consistent styling of headings on each page: https://github.com/theislab/scanpy/commit/1f579f6f745d01c599a39f38abadb8a32f95c12b,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:0,integrability,Adapt,Adapted,0,Adapted the css to enforce the consistent styling of headings on each page: https://github.com/theislab/scanpy/commit/1f579f6f745d01c599a39f38abadb8a32f95c12b,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:0,interoperability,Adapt,Adapted,0,Adapted the css to enforce the consistent styling of headings on each page: https://github.com/theislab/scanpy/commit/1f579f6f745d01c599a39f38abadb8a32f95c12b,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:0,modifiability,Adapt,Adapted,0,Adapted the css to enforce the consistent styling of headings on each page: https://github.com/theislab/scanpy/commit/1f579f6f745d01c599a39f38abadb8a32f95c12b,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:31,usability,consist,consistent,31,Adapted the css to enforce the consistent styling of headings on each page: https://github.com/theislab/scanpy/commit/1f579f6f745d01c599a39f38abadb8a32f95c12b,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/pull/610:58,interoperability,Specif,Specifically,58,"Good! I fixed some typos and improved the examples a bit. Specifically when thereâ€™s something to link, they should do it! `` :class:`pandas.Series` `` instead of ``` ``pd.Series`` ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/610
https://github.com/scverse/scanpy/issues/612:73,availability,cluster,cluster,73,"Hey! Thank you for using Scanpy! ```. # First solution: assign subset to cluster 1 -- does not work. adata[gene1_obs,:].obs[""my_cluster""] = 1. ```. If you print `adata[gene1_obs,:]`, you'll see that it generates a view on `adata`. If you try the annotations of the view, however, this view becomes a copy. This is a safety measure as most people treat subsets of the data matrix as if it was a copy. Views are necessary, however, to be able to index in a data matrix (also for efficiency reasons). But I realize that we should print warnings when someone tries to do this, similar to pandas, which also prints a warning if you do `df['col'].loc['a':'b'] = value`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/612
https://github.com/scverse/scanpy/issues/612:73,deployability,cluster,cluster,73,"Hey! Thank you for using Scanpy! ```. # First solution: assign subset to cluster 1 -- does not work. adata[gene1_obs,:].obs[""my_cluster""] = 1. ```. If you print `adata[gene1_obs,:]`, you'll see that it generates a view on `adata`. If you try the annotations of the view, however, this view becomes a copy. This is a safety measure as most people treat subsets of the data matrix as if it was a copy. Views are necessary, however, to be able to index in a data matrix (also for efficiency reasons). But I realize that we should print warnings when someone tries to do this, similar to pandas, which also prints a warning if you do `df['col'].loc['a':'b'] = value`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/612
https://github.com/scverse/scanpy/issues/612:323,energy efficiency,measur,measure,323,"Hey! Thank you for using Scanpy! ```. # First solution: assign subset to cluster 1 -- does not work. adata[gene1_obs,:].obs[""my_cluster""] = 1. ```. If you print `adata[gene1_obs,:]`, you'll see that it generates a view on `adata`. If you try the annotations of the view, however, this view becomes a copy. This is a safety measure as most people treat subsets of the data matrix as if it was a copy. Views are necessary, however, to be able to index in a data matrix (also for efficiency reasons). But I realize that we should print warnings when someone tries to do this, similar to pandas, which also prints a warning if you do `df['col'].loc['a':'b'] = value`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/612
https://github.com/scverse/scanpy/issues/612:63,integrability,sub,subset,63,"Hey! Thank you for using Scanpy! ```. # First solution: assign subset to cluster 1 -- does not work. adata[gene1_obs,:].obs[""my_cluster""] = 1. ```. If you print `adata[gene1_obs,:]`, you'll see that it generates a view on `adata`. If you try the annotations of the view, however, this view becomes a copy. This is a safety measure as most people treat subsets of the data matrix as if it was a copy. Views are necessary, however, to be able to index in a data matrix (also for efficiency reasons). But I realize that we should print warnings when someone tries to do this, similar to pandas, which also prints a warning if you do `df['col'].loc['a':'b'] = value`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/612
https://github.com/scverse/scanpy/issues/612:352,integrability,sub,subsets,352,"Hey! Thank you for using Scanpy! ```. # First solution: assign subset to cluster 1 -- does not work. adata[gene1_obs,:].obs[""my_cluster""] = 1. ```. If you print `adata[gene1_obs,:]`, you'll see that it generates a view on `adata`. If you try the annotations of the view, however, this view becomes a copy. This is a safety measure as most people treat subsets of the data matrix as if it was a copy. Views are necessary, however, to be able to index in a data matrix (also for efficiency reasons). But I realize that we should print warnings when someone tries to do this, similar to pandas, which also prints a warning if you do `df['col'].loc['a':'b'] = value`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/612
https://github.com/scverse/scanpy/issues/612:86,reliability,doe,does,86,"Hey! Thank you for using Scanpy! ```. # First solution: assign subset to cluster 1 -- does not work. adata[gene1_obs,:].obs[""my_cluster""] = 1. ```. If you print `adata[gene1_obs,:]`, you'll see that it generates a view on `adata`. If you try the annotations of the view, however, this view becomes a copy. This is a safety measure as most people treat subsets of the data matrix as if it was a copy. Views are necessary, however, to be able to index in a data matrix (also for efficiency reasons). But I realize that we should print warnings when someone tries to do this, similar to pandas, which also prints a warning if you do `df['col'].loc['a':'b'] = value`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/612
https://github.com/scverse/scanpy/issues/612:316,safety,safe,safety,316,"Hey! Thank you for using Scanpy! ```. # First solution: assign subset to cluster 1 -- does not work. adata[gene1_obs,:].obs[""my_cluster""] = 1. ```. If you print `adata[gene1_obs,:]`, you'll see that it generates a view on `adata`. If you try the annotations of the view, however, this view becomes a copy. This is a safety measure as most people treat subsets of the data matrix as if it was a copy. Views are necessary, however, to be able to index in a data matrix (also for efficiency reasons). But I realize that we should print warnings when someone tries to do this, similar to pandas, which also prints a warning if you do `df['col'].loc['a':'b'] = value`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/612
https://github.com/scverse/scanpy/issues/612:477,usability,efficien,efficiency,477,"Hey! Thank you for using Scanpy! ```. # First solution: assign subset to cluster 1 -- does not work. adata[gene1_obs,:].obs[""my_cluster""] = 1. ```. If you print `adata[gene1_obs,:]`, you'll see that it generates a view on `adata`. If you try the annotations of the view, however, this view becomes a copy. This is a safety measure as most people treat subsets of the data matrix as if it was a copy. Views are necessary, however, to be able to index in a data matrix (also for efficiency reasons). But I realize that we should print warnings when someone tries to do this, similar to pandas, which also prints a warning if you do `df['col'].loc['a':'b'] = value`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/612
https://github.com/scverse/scanpy/issues/612:112,availability,consist,consistently,112,"We now print a warning. It might sometimes lead to some cluttered output, but it forces people to use `.copy()` consistently and should prevent issues like the above. ![image](https://user-images.githubusercontent.com/16916678/56283523-52533080-6112-11e9-9263-75bca8a262e6.png). Commit is here: https://github.com/theislab/anndata/commit/97ad1e42486a49f06619340099ba7dce5049a393.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/612
https://github.com/scverse/scanpy/issues/612:136,safety,prevent,prevent,136,"We now print a warning. It might sometimes lead to some cluttered output, but it forces people to use `.copy()` consistently and should prevent issues like the above. ![image](https://user-images.githubusercontent.com/16916678/56283523-52533080-6112-11e9-9263-75bca8a262e6.png). Commit is here: https://github.com/theislab/anndata/commit/97ad1e42486a49f06619340099ba7dce5049a393.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/612
https://github.com/scverse/scanpy/issues/612:136,security,preven,prevent,136,"We now print a warning. It might sometimes lead to some cluttered output, but it forces people to use `.copy()` consistently and should prevent issues like the above. ![image](https://user-images.githubusercontent.com/16916678/56283523-52533080-6112-11e9-9263-75bca8a262e6.png). Commit is here: https://github.com/theislab/anndata/commit/97ad1e42486a49f06619340099ba7dce5049a393.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/612
https://github.com/scverse/scanpy/issues/612:112,usability,consist,consistently,112,"We now print a warning. It might sometimes lead to some cluttered output, but it forces people to use `.copy()` consistently and should prevent issues like the above. ![image](https://user-images.githubusercontent.com/16916678/56283523-52533080-6112-11e9-9263-75bca8a262e6.png). Commit is here: https://github.com/theislab/anndata/commit/97ad1e42486a49f06619340099ba7dce5049a393.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/612
https://github.com/scverse/scanpy/issues/612:184,usability,user,user-images,184,"We now print a warning. It might sometimes lead to some cluttered output, but it forces people to use `.copy()` consistently and should prevent issues like the above. ![image](https://user-images.githubusercontent.com/16916678/56283523-52533080-6112-11e9-9263-75bca8a262e6.png). Commit is here: https://github.com/theislab/anndata/commit/97ad1e42486a49f06619340099ba7dce5049a393.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/612
https://github.com/scverse/scanpy/issues/613:22,energy efficiency,cool,cool,22,"I think it'll be very cool to make this lookup dynamic somehow e.g. `sc.pl.umap(adata, color='X_PCA:1')`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/613
https://github.com/scverse/scanpy/issues/613:222,usability,person,personal,222,"@gokceneraslan, how about a tuple like `({obsm_key}, {obsm_value_column})`? It's more verbose, but I think more flexible, particularly if `obsm` starts to allow more kinds of values. Here's an example (using some stuff on personal branches):. <img width=""703"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/56583661-6c62a680-661d-11e9-8314-ca0b0bf8a309.png"">.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/613
https://github.com/scverse/scanpy/issues/613:285,usability,user,user-images,285,"@gokceneraslan, how about a tuple like `({obsm_key}, {obsm_value_column})`? It's more verbose, but I think more flexible, particularly if `obsm` starts to allow more kinds of values. Here's an example (using some stuff on personal branches):. <img width=""703"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/56583661-6c62a680-661d-11e9-8314-ca0b0bf8a309.png"">.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/613
https://github.com/scverse/scanpy/pull/614:38,deployability,log,log,38,"I just found out that raw was already log transformed. Now I fixed it and nanmin seems a bit too conservative, any ideas? ![image](https://user-images.githubusercontent.com/1140359/56463105-ea177f80-639b-11e9-80d3-f96463734634.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:42,integrability,transform,transformed,42,"I just found out that raw was already log transformed. Now I fixed it and nanmin seems a bit too conservative, any ideas? ![image](https://user-images.githubusercontent.com/1140359/56463105-ea177f80-639b-11e9-80d3-f96463734634.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:42,interoperability,transform,transformed,42,"I just found out that raw was already log transformed. Now I fixed it and nanmin seems a bit too conservative, any ideas? ![image](https://user-images.githubusercontent.com/1140359/56463105-ea177f80-639b-11e9-80d3-f96463734634.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:38,safety,log,log,38,"I just found out that raw was already log transformed. Now I fixed it and nanmin seems a bit too conservative, any ideas? ![image](https://user-images.githubusercontent.com/1140359/56463105-ea177f80-639b-11e9-80d3-f96463734634.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:38,security,log,log,38,"I just found out that raw was already log transformed. Now I fixed it and nanmin seems a bit too conservative, any ideas? ![image](https://user-images.githubusercontent.com/1140359/56463105-ea177f80-639b-11e9-80d3-f96463734634.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:38,testability,log,log,38,"I just found out that raw was already log transformed. Now I fixed it and nanmin seems a bit too conservative, any ideas? ![image](https://user-images.githubusercontent.com/1140359/56463105-ea177f80-639b-11e9-80d3-f96463734634.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:139,usability,user,user-images,139,"I just found out that raw was already log transformed. Now I fixed it and nanmin seems a bit too conservative, any ideas? ![image](https://user-images.githubusercontent.com/1140359/56463105-ea177f80-639b-11e9-80d3-f96463734634.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:61,integrability,batch,batches,61,"Hey! I think it's a great idea to put HVG calculation within batches as a pull request. This is also an important pre-processing step for `sce.mnn_correct`, which I usually run on the intersection of HVGs from each batch. Do you think you could output the intersection as well? Or would it just be a matter of subsetting HVGs that are detected in all batches from the resulting dataframe? I'm a bit more skeptical for advertising this as a batch-correction approach though. Maybe for time-series data where you can't use other batch correction methods?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:215,integrability,batch,batch,215,"Hey! I think it's a great idea to put HVG calculation within batches as a pull request. This is also an important pre-processing step for `sce.mnn_correct`, which I usually run on the intersection of HVGs from each batch. Do you think you could output the intersection as well? Or would it just be a matter of subsetting HVGs that are detected in all batches from the resulting dataframe? I'm a bit more skeptical for advertising this as a batch-correction approach though. Maybe for time-series data where you can't use other batch correction methods?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:310,integrability,sub,subsetting,310,"Hey! I think it's a great idea to put HVG calculation within batches as a pull request. This is also an important pre-processing step for `sce.mnn_correct`, which I usually run on the intersection of HVGs from each batch. Do you think you could output the intersection as well? Or would it just be a matter of subsetting HVGs that are detected in all batches from the resulting dataframe? I'm a bit more skeptical for advertising this as a batch-correction approach though. Maybe for time-series data where you can't use other batch correction methods?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:351,integrability,batch,batches,351,"Hey! I think it's a great idea to put HVG calculation within batches as a pull request. This is also an important pre-processing step for `sce.mnn_correct`, which I usually run on the intersection of HVGs from each batch. Do you think you could output the intersection as well? Or would it just be a matter of subsetting HVGs that are detected in all batches from the resulting dataframe? I'm a bit more skeptical for advertising this as a batch-correction approach though. Maybe for time-series data where you can't use other batch correction methods?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:440,integrability,batch,batch-correction,440,"Hey! I think it's a great idea to put HVG calculation within batches as a pull request. This is also an important pre-processing step for `sce.mnn_correct`, which I usually run on the intersection of HVGs from each batch. Do you think you could output the intersection as well? Or would it just be a matter of subsetting HVGs that are detected in all batches from the resulting dataframe? I'm a bit more skeptical for advertising this as a batch-correction approach though. Maybe for time-series data where you can't use other batch correction methods?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:527,integrability,batch,batch,527,"Hey! I think it's a great idea to put HVG calculation within batches as a pull request. This is also an important pre-processing step for `sce.mnn_correct`, which I usually run on the intersection of HVGs from each batch. Do you think you could output the intersection as well? Or would it just be a matter of subsetting HVGs that are detected in all batches from the resulting dataframe? I'm a bit more skeptical for advertising this as a batch-correction approach though. Maybe for time-series data where you can't use other batch correction methods?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:61,performance,batch,batches,61,"Hey! I think it's a great idea to put HVG calculation within batches as a pull request. This is also an important pre-processing step for `sce.mnn_correct`, which I usually run on the intersection of HVGs from each batch. Do you think you could output the intersection as well? Or would it just be a matter of subsetting HVGs that are detected in all batches from the resulting dataframe? I'm a bit more skeptical for advertising this as a batch-correction approach though. Maybe for time-series data where you can't use other batch correction methods?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:215,performance,batch,batch,215,"Hey! I think it's a great idea to put HVG calculation within batches as a pull request. This is also an important pre-processing step for `sce.mnn_correct`, which I usually run on the intersection of HVGs from each batch. Do you think you could output the intersection as well? Or would it just be a matter of subsetting HVGs that are detected in all batches from the resulting dataframe? I'm a bit more skeptical for advertising this as a batch-correction approach though. Maybe for time-series data where you can't use other batch correction methods?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:351,performance,batch,batches,351,"Hey! I think it's a great idea to put HVG calculation within batches as a pull request. This is also an important pre-processing step for `sce.mnn_correct`, which I usually run on the intersection of HVGs from each batch. Do you think you could output the intersection as well? Or would it just be a matter of subsetting HVGs that are detected in all batches from the resulting dataframe? I'm a bit more skeptical for advertising this as a batch-correction approach though. Maybe for time-series data where you can't use other batch correction methods?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:440,performance,batch,batch-correction,440,"Hey! I think it's a great idea to put HVG calculation within batches as a pull request. This is also an important pre-processing step for `sce.mnn_correct`, which I usually run on the intersection of HVGs from each batch. Do you think you could output the intersection as well? Or would it just be a matter of subsetting HVGs that are detected in all batches from the resulting dataframe? I'm a bit more skeptical for advertising this as a batch-correction approach though. Maybe for time-series data where you can't use other batch correction methods?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:484,performance,time,time-series,484,"Hey! I think it's a great idea to put HVG calculation within batches as a pull request. This is also an important pre-processing step for `sce.mnn_correct`, which I usually run on the intersection of HVGs from each batch. Do you think you could output the intersection as well? Or would it just be a matter of subsetting HVGs that are detected in all batches from the resulting dataframe? I'm a bit more skeptical for advertising this as a batch-correction approach though. Maybe for time-series data where you can't use other batch correction methods?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:527,performance,batch,batch,527,"Hey! I think it's a great idea to put HVG calculation within batches as a pull request. This is also an important pre-processing step for `sce.mnn_correct`, which I usually run on the intersection of HVGs from each batch. Do you think you could output the intersection as well? Or would it just be a matter of subsetting HVGs that are detected in all batches from the resulting dataframe? I'm a bit more skeptical for advertising this as a batch-correction approach though. Maybe for time-series data where you can't use other batch correction methods?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:335,safety,detect,detected,335,"Hey! I think it's a great idea to put HVG calculation within batches as a pull request. This is also an important pre-processing step for `sce.mnn_correct`, which I usually run on the intersection of HVGs from each batch. Do you think you could output the intersection as well? Or would it just be a matter of subsetting HVGs that are detected in all batches from the resulting dataframe? I'm a bit more skeptical for advertising this as a batch-correction approach though. Maybe for time-series data where you can't use other batch correction methods?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:335,security,detect,detected,335,"Hey! I think it's a great idea to put HVG calculation within batches as a pull request. This is also an important pre-processing step for `sce.mnn_correct`, which I usually run on the intersection of HVGs from each batch. Do you think you could output the intersection as well? Or would it just be a matter of subsetting HVGs that are detected in all batches from the resulting dataframe? I'm a bit more skeptical for advertising this as a batch-correction approach though. Maybe for time-series data where you can't use other batch correction methods?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:457,availability,consist,consistent,457,"Hey. I also thought about the intersection but didn't implement it as the default output for two reasons. . 1) it can be too harsh, especially if there is some biological variation between batches. When we sort the genes based on in how many batches they're detected as HVG and on mean normalized dispersion, there is still a chance for the user to catch such ""biological"" genes with a high n_top_genes value. . 2) Output of highly_variable_genes should be consistent regardless of batch_key option. So n_top_genes and mean/dispersion cutoff flavors should still work the same way. I feel like using the intersection directly as the output violates that. However, making `'highly_variable': np.nansum` part available in adata.var is a good idea. Then users can manually make the selection more stringent by selecting genes where this value == nbatches.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:707,availability,avail,available,707,"Hey. I also thought about the intersection but didn't implement it as the default output for two reasons. . 1) it can be too harsh, especially if there is some biological variation between batches. When we sort the genes based on in how many batches they're detected as HVG and on mean normalized dispersion, there is still a chance for the user to catch such ""biological"" genes with a high n_top_genes value. . 2) Output of highly_variable_genes should be consistent regardless of batch_key option. So n_top_genes and mean/dispersion cutoff flavors should still work the same way. I feel like using the intersection directly as the output violates that. However, making `'highly_variable': np.nansum` part available in adata.var is a good idea. Then users can manually make the selection more stringent by selecting genes where this value == nbatches.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:189,integrability,batch,batches,189,"Hey. I also thought about the intersection but didn't implement it as the default output for two reasons. . 1) it can be too harsh, especially if there is some biological variation between batches. When we sort the genes based on in how many batches they're detected as HVG and on mean normalized dispersion, there is still a chance for the user to catch such ""biological"" genes with a high n_top_genes value. . 2) Output of highly_variable_genes should be consistent regardless of batch_key option. So n_top_genes and mean/dispersion cutoff flavors should still work the same way. I feel like using the intersection directly as the output violates that. However, making `'highly_variable': np.nansum` part available in adata.var is a good idea. Then users can manually make the selection more stringent by selecting genes where this value == nbatches.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:242,integrability,batch,batches,242,"Hey. I also thought about the intersection but didn't implement it as the default output for two reasons. . 1) it can be too harsh, especially if there is some biological variation between batches. When we sort the genes based on in how many batches they're detected as HVG and on mean normalized dispersion, there is still a chance for the user to catch such ""biological"" genes with a high n_top_genes value. . 2) Output of highly_variable_genes should be consistent regardless of batch_key option. So n_top_genes and mean/dispersion cutoff flavors should still work the same way. I feel like using the intersection directly as the output violates that. However, making `'highly_variable': np.nansum` part available in adata.var is a good idea. Then users can manually make the selection more stringent by selecting genes where this value == nbatches.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:189,performance,batch,batches,189,"Hey. I also thought about the intersection but didn't implement it as the default output for two reasons. . 1) it can be too harsh, especially if there is some biological variation between batches. When we sort the genes based on in how many batches they're detected as HVG and on mean normalized dispersion, there is still a chance for the user to catch such ""biological"" genes with a high n_top_genes value. . 2) Output of highly_variable_genes should be consistent regardless of batch_key option. So n_top_genes and mean/dispersion cutoff flavors should still work the same way. I feel like using the intersection directly as the output violates that. However, making `'highly_variable': np.nansum` part available in adata.var is a good idea. Then users can manually make the selection more stringent by selecting genes where this value == nbatches.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:242,performance,batch,batches,242,"Hey. I also thought about the intersection but didn't implement it as the default output for two reasons. . 1) it can be too harsh, especially if there is some biological variation between batches. When we sort the genes based on in how many batches they're detected as HVG and on mean normalized dispersion, there is still a chance for the user to catch such ""biological"" genes with a high n_top_genes value. . 2) Output of highly_variable_genes should be consistent regardless of batch_key option. So n_top_genes and mean/dispersion cutoff flavors should still work the same way. I feel like using the intersection directly as the output violates that. However, making `'highly_variable': np.nansum` part available in adata.var is a good idea. Then users can manually make the selection more stringent by selecting genes where this value == nbatches.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:707,reliability,availab,available,707,"Hey. I also thought about the intersection but didn't implement it as the default output for two reasons. . 1) it can be too harsh, especially if there is some biological variation between batches. When we sort the genes based on in how many batches they're detected as HVG and on mean normalized dispersion, there is still a chance for the user to catch such ""biological"" genes with a high n_top_genes value. . 2) Output of highly_variable_genes should be consistent regardless of batch_key option. So n_top_genes and mean/dispersion cutoff flavors should still work the same way. I feel like using the intersection directly as the output violates that. However, making `'highly_variable': np.nansum` part available in adata.var is a good idea. Then users can manually make the selection more stringent by selecting genes where this value == nbatches.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:258,safety,detect,detected,258,"Hey. I also thought about the intersection but didn't implement it as the default output for two reasons. . 1) it can be too harsh, especially if there is some biological variation between batches. When we sort the genes based on in how many batches they're detected as HVG and on mean normalized dispersion, there is still a chance for the user to catch such ""biological"" genes with a high n_top_genes value. . 2) Output of highly_variable_genes should be consistent regardless of batch_key option. So n_top_genes and mean/dispersion cutoff flavors should still work the same way. I feel like using the intersection directly as the output violates that. However, making `'highly_variable': np.nansum` part available in adata.var is a good idea. Then users can manually make the selection more stringent by selecting genes where this value == nbatches.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:707,safety,avail,available,707,"Hey. I also thought about the intersection but didn't implement it as the default output for two reasons. . 1) it can be too harsh, especially if there is some biological variation between batches. When we sort the genes based on in how many batches they're detected as HVG and on mean normalized dispersion, there is still a chance for the user to catch such ""biological"" genes with a high n_top_genes value. . 2) Output of highly_variable_genes should be consistent regardless of batch_key option. So n_top_genes and mean/dispersion cutoff flavors should still work the same way. I feel like using the intersection directly as the output violates that. However, making `'highly_variable': np.nansum` part available in adata.var is a good idea. Then users can manually make the selection more stringent by selecting genes where this value == nbatches.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:258,security,detect,detected,258,"Hey. I also thought about the intersection but didn't implement it as the default output for two reasons. . 1) it can be too harsh, especially if there is some biological variation between batches. When we sort the genes based on in how many batches they're detected as HVG and on mean normalized dispersion, there is still a chance for the user to catch such ""biological"" genes with a high n_top_genes value. . 2) Output of highly_variable_genes should be consistent regardless of batch_key option. So n_top_genes and mean/dispersion cutoff flavors should still work the same way. I feel like using the intersection directly as the output violates that. However, making `'highly_variable': np.nansum` part available in adata.var is a good idea. Then users can manually make the selection more stringent by selecting genes where this value == nbatches.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:707,security,availab,available,707,"Hey. I also thought about the intersection but didn't implement it as the default output for two reasons. . 1) it can be too harsh, especially if there is some biological variation between batches. When we sort the genes based on in how many batches they're detected as HVG and on mean normalized dispersion, there is still a chance for the user to catch such ""biological"" genes with a high n_top_genes value. . 2) Output of highly_variable_genes should be consistent regardless of batch_key option. So n_top_genes and mean/dispersion cutoff flavors should still work the same way. I feel like using the intersection directly as the output violates that. However, making `'highly_variable': np.nansum` part available in adata.var is a good idea. Then users can manually make the selection more stringent by selecting genes where this value == nbatches.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:341,usability,user,user,341,"Hey. I also thought about the intersection but didn't implement it as the default output for two reasons. . 1) it can be too harsh, especially if there is some biological variation between batches. When we sort the genes based on in how many batches they're detected as HVG and on mean normalized dispersion, there is still a chance for the user to catch such ""biological"" genes with a high n_top_genes value. . 2) Output of highly_variable_genes should be consistent regardless of batch_key option. So n_top_genes and mean/dispersion cutoff flavors should still work the same way. I feel like using the intersection directly as the output violates that. However, making `'highly_variable': np.nansum` part available in adata.var is a good idea. Then users can manually make the selection more stringent by selecting genes where this value == nbatches.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:457,usability,consist,consistent,457,"Hey. I also thought about the intersection but didn't implement it as the default output for two reasons. . 1) it can be too harsh, especially if there is some biological variation between batches. When we sort the genes based on in how many batches they're detected as HVG and on mean normalized dispersion, there is still a chance for the user to catch such ""biological"" genes with a high n_top_genes value. . 2) Output of highly_variable_genes should be consistent regardless of batch_key option. So n_top_genes and mean/dispersion cutoff flavors should still work the same way. I feel like using the intersection directly as the output violates that. However, making `'highly_variable': np.nansum` part available in adata.var is a good idea. Then users can manually make the selection more stringent by selecting genes where this value == nbatches.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:751,usability,user,users,751,"Hey. I also thought about the intersection but didn't implement it as the default output for two reasons. . 1) it can be too harsh, especially if there is some biological variation between batches. When we sort the genes based on in how many batches they're detected as HVG and on mean normalized dispersion, there is still a chance for the user to catch such ""biological"" genes with a high n_top_genes value. . 2) Output of highly_variable_genes should be consistent regardless of batch_key option. So n_top_genes and mean/dispersion cutoff flavors should still work the same way. I feel like using the intersection directly as the output violates that. However, making `'highly_variable': np.nansum` part available in adata.var is a good idea. Then users can manually make the selection more stringent by selecting genes where this value == nbatches.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:66,deployability,integr,integration,66,"I agree that using the intersection is too harsh if the only data integration/batch correction you do is this HVG filtering, but for `mnn_correct` for example you only use these HVGs to calculate the technical batch vector. In that case you ideally don't want to capture the biological variation between batch samples. For that I reckon having an option of getting the intersection would be good. And for point 2... definitely agreed... but again, a different use case for me. The same approach (intersection of HVGs) is suggested for the new CCA in Seurat v3 btw.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:66,integrability,integr,integration,66,"I agree that using the intersection is too harsh if the only data integration/batch correction you do is this HVG filtering, but for `mnn_correct` for example you only use these HVGs to calculate the technical batch vector. In that case you ideally don't want to capture the biological variation between batch samples. For that I reckon having an option of getting the intersection would be good. And for point 2... definitely agreed... but again, a different use case for me. The same approach (intersection of HVGs) is suggested for the new CCA in Seurat v3 btw.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:78,integrability,batch,batch,78,"I agree that using the intersection is too harsh if the only data integration/batch correction you do is this HVG filtering, but for `mnn_correct` for example you only use these HVGs to calculate the technical batch vector. In that case you ideally don't want to capture the biological variation between batch samples. For that I reckon having an option of getting the intersection would be good. And for point 2... definitely agreed... but again, a different use case for me. The same approach (intersection of HVGs) is suggested for the new CCA in Seurat v3 btw.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:114,integrability,filter,filtering,114,"I agree that using the intersection is too harsh if the only data integration/batch correction you do is this HVG filtering, but for `mnn_correct` for example you only use these HVGs to calculate the technical batch vector. In that case you ideally don't want to capture the biological variation between batch samples. For that I reckon having an option of getting the intersection would be good. And for point 2... definitely agreed... but again, a different use case for me. The same approach (intersection of HVGs) is suggested for the new CCA in Seurat v3 btw.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:210,integrability,batch,batch,210,"I agree that using the intersection is too harsh if the only data integration/batch correction you do is this HVG filtering, but for `mnn_correct` for example you only use these HVGs to calculate the technical batch vector. In that case you ideally don't want to capture the biological variation between batch samples. For that I reckon having an option of getting the intersection would be good. And for point 2... definitely agreed... but again, a different use case for me. The same approach (intersection of HVGs) is suggested for the new CCA in Seurat v3 btw.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:304,integrability,batch,batch,304,"I agree that using the intersection is too harsh if the only data integration/batch correction you do is this HVG filtering, but for `mnn_correct` for example you only use these HVGs to calculate the technical batch vector. In that case you ideally don't want to capture the biological variation between batch samples. For that I reckon having an option of getting the intersection would be good. And for point 2... definitely agreed... but again, a different use case for me. The same approach (intersection of HVGs) is suggested for the new CCA in Seurat v3 btw.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:66,interoperability,integr,integration,66,"I agree that using the intersection is too harsh if the only data integration/batch correction you do is this HVG filtering, but for `mnn_correct` for example you only use these HVGs to calculate the technical batch vector. In that case you ideally don't want to capture the biological variation between batch samples. For that I reckon having an option of getting the intersection would be good. And for point 2... definitely agreed... but again, a different use case for me. The same approach (intersection of HVGs) is suggested for the new CCA in Seurat v3 btw.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:66,modifiability,integr,integration,66,"I agree that using the intersection is too harsh if the only data integration/batch correction you do is this HVG filtering, but for `mnn_correct` for example you only use these HVGs to calculate the technical batch vector. In that case you ideally don't want to capture the biological variation between batch samples. For that I reckon having an option of getting the intersection would be good. And for point 2... definitely agreed... but again, a different use case for me. The same approach (intersection of HVGs) is suggested for the new CCA in Seurat v3 btw.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:78,performance,batch,batch,78,"I agree that using the intersection is too harsh if the only data integration/batch correction you do is this HVG filtering, but for `mnn_correct` for example you only use these HVGs to calculate the technical batch vector. In that case you ideally don't want to capture the biological variation between batch samples. For that I reckon having an option of getting the intersection would be good. And for point 2... definitely agreed... but again, a different use case for me. The same approach (intersection of HVGs) is suggested for the new CCA in Seurat v3 btw.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:210,performance,batch,batch,210,"I agree that using the intersection is too harsh if the only data integration/batch correction you do is this HVG filtering, but for `mnn_correct` for example you only use these HVGs to calculate the technical batch vector. In that case you ideally don't want to capture the biological variation between batch samples. For that I reckon having an option of getting the intersection would be good. And for point 2... definitely agreed... but again, a different use case for me. The same approach (intersection of HVGs) is suggested for the new CCA in Seurat v3 btw.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:304,performance,batch,batch,304,"I agree that using the intersection is too harsh if the only data integration/batch correction you do is this HVG filtering, but for `mnn_correct` for example you only use these HVGs to calculate the technical batch vector. In that case you ideally don't want to capture the biological variation between batch samples. For that I reckon having an option of getting the intersection would be good. And for point 2... definitely agreed... but again, a different use case for me. The same approach (intersection of HVGs) is suggested for the new CCA in Seurat v3 btw.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:66,reliability,integr,integration,66,"I agree that using the intersection is too harsh if the only data integration/batch correction you do is this HVG filtering, but for `mnn_correct` for example you only use these HVGs to calculate the technical batch vector. In that case you ideally don't want to capture the biological variation between batch samples. For that I reckon having an option of getting the intersection would be good. And for point 2... definitely agreed... but again, a different use case for me. The same approach (intersection of HVGs) is suggested for the new CCA in Seurat v3 btw.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:66,security,integr,integration,66,"I agree that using the intersection is too harsh if the only data integration/batch correction you do is this HVG filtering, but for `mnn_correct` for example you only use these HVGs to calculate the technical batch vector. In that case you ideally don't want to capture the biological variation between batch samples. For that I reckon having an option of getting the intersection would be good. And for point 2... definitely agreed... but again, a different use case for me. The same approach (intersection of HVGs) is suggested for the new CCA in Seurat v3 btw.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:66,testability,integr,integration,66,"I agree that using the intersection is too harsh if the only data integration/batch correction you do is this HVG filtering, but for `mnn_correct` for example you only use these HVGs to calculate the technical batch vector. In that case you ideally don't want to capture the biological variation between batch samples. For that I reckon having an option of getting the intersection would be good. And for point 2... definitely agreed... but again, a different use case for me. The same approach (intersection of HVGs) is suggested for the new CCA in Seurat v3 btw.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:221,integrability,batch,batch,221,@LuckyMD Can you maybe check if the new `highly_variable_nbatches` variable in `adata.var` is useful for finding the intersections? . `adata.var['highly_variable'] = adata.var['highly_variable_nbatches'] == len(adata.obs.batch.cat.categories)`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:67,modifiability,variab,variable,67,@LuckyMD Can you maybe check if the new `highly_variable_nbatches` variable in `adata.var` is useful for finding the intersections? . `adata.var['highly_variable'] = adata.var['highly_variable_nbatches'] == len(adata.obs.batch.cat.categories)`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:221,performance,batch,batch,221,@LuckyMD Can you maybe check if the new `highly_variable_nbatches` variable in `adata.var` is useful for finding the intersections? . `adata.var['highly_variable'] = adata.var['highly_variable_nbatches'] == len(adata.obs.batch.cat.categories)`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:80,availability,slo,slow,80,"I'm sure it will be... Just having network issues atm, so server storage is too slow to do anything. Can't really test this atm I'm afraid :/.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:35,performance,network,network,35,"I'm sure it will be... Just having network issues atm, so server storage is too slow to do anything. Can't really test this atm I'm afraid :/.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:80,reliability,slo,slow,80,"I'm sure it will be... Just having network issues atm, so server storage is too slow to do anything. Can't really test this atm I'm afraid :/.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:114,safety,test,test,114,"I'm sure it will be... Just having network issues atm, so server storage is too slow to do anything. Can't really test this atm I'm afraid :/.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:35,security,network,network,35,"I'm sure it will be... Just having network issues atm, so server storage is too slow to do anything. Can't really test this atm I'm afraid :/.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/614:114,testability,test,test,114,"I'm sure it will be... Just having network issues atm, so server storage is too slow to do anything. Can't really test this atm I'm afraid :/.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/614
https://github.com/scverse/scanpy/pull/615:249,deployability,Updat,Updating,249,"Cool! :smile:. But is it possible to stay within the naming scheme? Or maybe even better, to avoid cluttering the namespace with many similar functions, add a parameter to `calculate_qc_metrics` that allows to restrict to `obs` and `var` if wanted? Updating `docs/release_notes.rst` would also be great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:0,energy efficiency,Cool,Cool,0,"Cool! :smile:. But is it possible to stay within the naming scheme? Or maybe even better, to avoid cluttering the namespace with many similar functions, add a parameter to `calculate_qc_metrics` that allows to restrict to `obs` and `var` if wanted? Updating `docs/release_notes.rst` would also be great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:159,modifiability,paramet,parameter,159,"Cool! :smile:. But is it possible to stay within the naming scheme? Or maybe even better, to avoid cluttering the namespace with many similar functions, add a parameter to `calculate_qc_metrics` that allows to restrict to `obs` and `var` if wanted? Updating `docs/release_notes.rst` would also be great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:93,safety,avoid,avoid,93,"Cool! :smile:. But is it possible to stay within the naming scheme? Or maybe even better, to avoid cluttering the namespace with many similar functions, add a parameter to `calculate_qc_metrics` that allows to restrict to `obs` and `var` if wanted? Updating `docs/release_notes.rst` would also be great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:249,safety,Updat,Updating,249,"Cool! :smile:. But is it possible to stay within the naming scheme? Or maybe even better, to avoid cluttering the namespace with many similar functions, add a parameter to `calculate_qc_metrics` that allows to restrict to `obs` and `var` if wanted? Updating `docs/release_notes.rst` would also be great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:249,security,Updat,Updating,249,"Cool! :smile:. But is it possible to stay within the naming scheme? Or maybe even better, to avoid cluttering the namespace with many similar functions, add a parameter to `calculate_qc_metrics` that allows to restrict to `obs` and `var` if wanted? Updating `docs/release_notes.rst` would also be great!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:613,energy efficiency,current,current,613,"I'd have to think about the naming scheme, since its something I've been going back and forth on recently. I'm playing around with these semantic a bit over in my [mantis](https://github.com/ivirshup/mantis) repo, but I don't think I'm ready to make a call on which one I like best. Right now I'm split between following [plyexperiment](https://github.com/sa-lee/plyexperiment) and `xarray`. I think I'd like to keep them as separate functions, since the `calculate_qc_metrics` function already has about as complicated a relationship as I'd want with parameter values and return type. How about these keep their current name for now, but we don't export these functions beyond `_qc.py`? This will let me play around with their naming scheme and the usefulness a bit more, while scanpy gets the faster, more thorough tests and improved `calculate_qc_metrics`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:137,interoperability,semant,semantic,137,"I'd have to think about the naming scheme, since its something I've been going back and forth on recently. I'm playing around with these semantic a bit over in my [mantis](https://github.com/ivirshup/mantis) repo, but I don't think I'm ready to make a call on which one I like best. Right now I'm split between following [plyexperiment](https://github.com/sa-lee/plyexperiment) and `xarray`. I think I'd like to keep them as separate functions, since the `calculate_qc_metrics` function already has about as complicated a relationship as I'd want with parameter values and return type. How about these keep their current name for now, but we don't export these functions beyond `_qc.py`? This will let me play around with their naming scheme and the usefulness a bit more, while scanpy gets the faster, more thorough tests and improved `calculate_qc_metrics`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:552,modifiability,paramet,parameter,552,"I'd have to think about the naming scheme, since its something I've been going back and forth on recently. I'm playing around with these semantic a bit over in my [mantis](https://github.com/ivirshup/mantis) repo, but I don't think I'm ready to make a call on which one I like best. Right now I'm split between following [plyexperiment](https://github.com/sa-lee/plyexperiment) and `xarray`. I think I'd like to keep them as separate functions, since the `calculate_qc_metrics` function already has about as complicated a relationship as I'd want with parameter values and return type. How about these keep their current name for now, but we don't export these functions beyond `_qc.py`? This will let me play around with their naming scheme and the usefulness a bit more, while scanpy gets the faster, more thorough tests and improved `calculate_qc_metrics`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:508,safety,compl,complicated,508,"I'd have to think about the naming scheme, since its something I've been going back and forth on recently. I'm playing around with these semantic a bit over in my [mantis](https://github.com/ivirshup/mantis) repo, but I don't think I'm ready to make a call on which one I like best. Right now I'm split between following [plyexperiment](https://github.com/sa-lee/plyexperiment) and `xarray`. I think I'd like to keep them as separate functions, since the `calculate_qc_metrics` function already has about as complicated a relationship as I'd want with parameter values and return type. How about these keep their current name for now, but we don't export these functions beyond `_qc.py`? This will let me play around with their naming scheme and the usefulness a bit more, while scanpy gets the faster, more thorough tests and improved `calculate_qc_metrics`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:817,safety,test,tests,817,"I'd have to think about the naming scheme, since its something I've been going back and forth on recently. I'm playing around with these semantic a bit over in my [mantis](https://github.com/ivirshup/mantis) repo, but I don't think I'm ready to make a call on which one I like best. Right now I'm split between following [plyexperiment](https://github.com/sa-lee/plyexperiment) and `xarray`. I think I'd like to keep them as separate functions, since the `calculate_qc_metrics` function already has about as complicated a relationship as I'd want with parameter values and return type. How about these keep their current name for now, but we don't export these functions beyond `_qc.py`? This will let me play around with their naming scheme and the usefulness a bit more, while scanpy gets the faster, more thorough tests and improved `calculate_qc_metrics`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:508,security,compl,complicated,508,"I'd have to think about the naming scheme, since its something I've been going back and forth on recently. I'm playing around with these semantic a bit over in my [mantis](https://github.com/ivirshup/mantis) repo, but I don't think I'm ready to make a call on which one I like best. Right now I'm split between following [plyexperiment](https://github.com/sa-lee/plyexperiment) and `xarray`. I think I'd like to keep them as separate functions, since the `calculate_qc_metrics` function already has about as complicated a relationship as I'd want with parameter values and return type. How about these keep their current name for now, but we don't export these functions beyond `_qc.py`? This will let me play around with their naming scheme and the usefulness a bit more, while scanpy gets the faster, more thorough tests and improved `calculate_qc_metrics`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:817,testability,test,tests,817,"I'd have to think about the naming scheme, since its something I've been going back and forth on recently. I'm playing around with these semantic a bit over in my [mantis](https://github.com/ivirshup/mantis) repo, but I don't think I'm ready to make a call on which one I like best. Right now I'm split between following [plyexperiment](https://github.com/sa-lee/plyexperiment) and `xarray`. I think I'd like to keep them as separate functions, since the `calculate_qc_metrics` function already has about as complicated a relationship as I'd want with parameter values and return type. How about these keep their current name for now, but we don't export these functions beyond `_qc.py`? This will let me play around with their naming scheme and the usefulness a bit more, while scanpy gets the faster, more thorough tests and improved `calculate_qc_metrics`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:29,energy efficiency,current,current,29,"> How about these keep their current name for now, but we don't export these functions beyond _qc.py? That's OK for me! I thought about naming these `calculate_qc_metrics_obs` and `..._var`, but that's not great, either... `describe_obs` and `describe_var` are probably better names compared to `calculate_qc_metrics`... So let's just not add a new bunch of functions for now and think about a lasting solution for Scanpy 2, where we can also break backwards compat. The `mantis` repo looks fantastic!!!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:49,availability,error,error,49,"@flying-sheep and idea what's up with this build error? Docstrings are failing tests, but look fine to me. Seems related to https://github.com/theislab/scanpy/commit/3cacdc87ab47bae70b415e93f2fea74a018c39e2.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:43,deployability,build,build,43,"@flying-sheep and idea what's up with this build error? Docstrings are failing tests, but look fine to me. Seems related to https://github.com/theislab/scanpy/commit/3cacdc87ab47bae70b415e93f2fea74a018c39e2.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:71,deployability,fail,failing,71,"@flying-sheep and idea what's up with this build error? Docstrings are failing tests, but look fine to me. Seems related to https://github.com/theislab/scanpy/commit/3cacdc87ab47bae70b415e93f2fea74a018c39e2.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:49,performance,error,error,49,"@flying-sheep and idea what's up with this build error? Docstrings are failing tests, but look fine to me. Seems related to https://github.com/theislab/scanpy/commit/3cacdc87ab47bae70b415e93f2fea74a018c39e2.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:71,reliability,fail,failing,71,"@flying-sheep and idea what's up with this build error? Docstrings are failing tests, but look fine to me. Seems related to https://github.com/theislab/scanpy/commit/3cacdc87ab47bae70b415e93f2fea74a018c39e2.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:49,safety,error,error,49,"@flying-sheep and idea what's up with this build error? Docstrings are failing tests, but look fine to me. Seems related to https://github.com/theislab/scanpy/commit/3cacdc87ab47bae70b415e93f2fea74a018c39e2.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:79,safety,test,tests,79,"@flying-sheep and idea what's up with this build error? Docstrings are failing tests, but look fine to me. Seems related to https://github.com/theislab/scanpy/commit/3cacdc87ab47bae70b415e93f2fea74a018c39e2.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:79,testability,test,tests,79,"@flying-sheep and idea what's up with this build error? Docstrings are failing tests, but look fine to me. Seems related to https://github.com/theislab/scanpy/commit/3cacdc87ab47bae70b415e93f2fea74a018c39e2.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:49,usability,error,error,49,"@flying-sheep and idea what's up with this build error? Docstrings are failing tests, but look fine to me. Seems related to https://github.com/theislab/scanpy/commit/3cacdc87ab47bae70b415e93f2fea74a018c39e2.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:62,deployability,releas,release,62,"Yes, it's good to go! Sorry about the trivial conflict in the release notes: I just made 1.4.2 based on the changes of the last two weeks on master, which I had the chance to test in the past week...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:46,interoperability,conflict,conflict,46,"Yes, it's good to go! Sorry about the trivial conflict in the release notes: I just made 1.4.2 based on the changes of the last two weeks on master, which I had the chance to test in the past week...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:175,safety,test,test,175,"Yes, it's good to go! Sorry about the trivial conflict in the release notes: I just made 1.4.2 based on the changes of the last two weeks on master, which I had the chance to test in the past week...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:175,testability,test,test,175,"Yes, it's good to go! Sorry about the trivial conflict in the release notes: I just made 1.4.2 based on the changes of the last two weeks on master, which I had the chance to test in the past week...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:41,deployability,releas,release,41,"No problem, thanks! I think conflicts in release note are going to be common. Maybe those commits could be separate from the PR?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/pull/615:28,interoperability,conflict,conflicts,28,"No problem, thanks! I think conflicts in release note are going to be common. Maybe those commits could be separate from the PR?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/615
https://github.com/scverse/scanpy/issues/617:97,reliability,doe,does,97,"I agree that `plot_scatter` should be exported, and with a different name. Since `sc.pl.scatter` does different things, maybe it could be exported as something like `sc.pl.dimred` or `sc.pl.obsm`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:90,energy efficiency,current,current,90,"The question is whether we want two scatter functions. Two will lead to simpler code. The current `pl.scatter` function is a relict from earlier times and could be simplified by dropping support for `basis`. On the other hand, it might not be so much work to add support for `x` and `y` in `plot_scatter`, in which case both functions should do the same thing. If we decide we want two functions, the second one should be `pl.scatter_embedding`, I'd say, as it's a method to visualize results of the embedding algorithms. I'm also ok with `pl.scatter_obsm` (`dimred` is something that is basically not used in the language of the docs, and I think it's a pretty misguided name of a class of functions anyway). The first one can remain `pl.scatter` and is similar to `plt.scatter` or `sns.scatter`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:145,performance,time,times,145,"The question is whether we want two scatter functions. Two will lead to simpler code. The current `pl.scatter` function is a relict from earlier times and could be simplified by dropping support for `basis`. On the other hand, it might not be so much work to add support for `x` and `y` in `plot_scatter`, in which case both functions should do the same thing. If we decide we want two functions, the second one should be `pl.scatter_embedding`, I'd say, as it's a method to visualize results of the embedding algorithms. I'm also ok with `pl.scatter_obsm` (`dimred` is something that is basically not used in the language of the docs, and I think it's a pretty misguided name of a class of functions anyway). The first one can remain `pl.scatter` and is similar to `plt.scatter` or `sns.scatter`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:72,testability,simpl,simpler,72,"The question is whether we want two scatter functions. Two will lead to simpler code. The current `pl.scatter` function is a relict from earlier times and could be simplified by dropping support for `basis`. On the other hand, it might not be so much work to add support for `x` and `y` in `plot_scatter`, in which case both functions should do the same thing. If we decide we want two functions, the second one should be `pl.scatter_embedding`, I'd say, as it's a method to visualize results of the embedding algorithms. I'm also ok with `pl.scatter_obsm` (`dimred` is something that is basically not used in the language of the docs, and I think it's a pretty misguided name of a class of functions anyway). The first one can remain `pl.scatter` and is similar to `plt.scatter` or `sns.scatter`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:164,testability,simpl,simplified,164,"The question is whether we want two scatter functions. Two will lead to simpler code. The current `pl.scatter` function is a relict from earlier times and could be simplified by dropping support for `basis`. On the other hand, it might not be so much work to add support for `x` and `y` in `plot_scatter`, in which case both functions should do the same thing. If we decide we want two functions, the second one should be `pl.scatter_embedding`, I'd say, as it's a method to visualize results of the embedding algorithms. I'm also ok with `pl.scatter_obsm` (`dimred` is something that is basically not used in the language of the docs, and I think it's a pretty misguided name of a class of functions anyway). The first one can remain `pl.scatter` and is similar to `plt.scatter` or `sns.scatter`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:72,usability,simpl,simpler,72,"The question is whether we want two scatter functions. Two will lead to simpler code. The current `pl.scatter` function is a relict from earlier times and could be simplified by dropping support for `basis`. On the other hand, it might not be so much work to add support for `x` and `y` in `plot_scatter`, in which case both functions should do the same thing. If we decide we want two functions, the second one should be `pl.scatter_embedding`, I'd say, as it's a method to visualize results of the embedding algorithms. I'm also ok with `pl.scatter_obsm` (`dimred` is something that is basically not used in the language of the docs, and I think it's a pretty misguided name of a class of functions anyway). The first one can remain `pl.scatter` and is similar to `plt.scatter` or `sns.scatter`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:164,usability,simpl,simplified,164,"The question is whether we want two scatter functions. Two will lead to simpler code. The current `pl.scatter` function is a relict from earlier times and could be simplified by dropping support for `basis`. On the other hand, it might not be so much work to add support for `x` and `y` in `plot_scatter`, in which case both functions should do the same thing. If we decide we want two functions, the second one should be `pl.scatter_embedding`, I'd say, as it's a method to visualize results of the embedding algorithms. I'm also ok with `pl.scatter_obsm` (`dimred` is something that is basically not used in the language of the docs, and I think it's a pretty misguided name of a class of functions anyway). The first one can remain `pl.scatter` and is similar to `plt.scatter` or `sns.scatter`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:187,usability,support,support,187,"The question is whether we want two scatter functions. Two will lead to simpler code. The current `pl.scatter` function is a relict from earlier times and could be simplified by dropping support for `basis`. On the other hand, it might not be so much work to add support for `x` and `y` in `plot_scatter`, in which case both functions should do the same thing. If we decide we want two functions, the second one should be `pl.scatter_embedding`, I'd say, as it's a method to visualize results of the embedding algorithms. I'm also ok with `pl.scatter_obsm` (`dimred` is something that is basically not used in the language of the docs, and I think it's a pretty misguided name of a class of functions anyway). The first one can remain `pl.scatter` and is similar to `plt.scatter` or `sns.scatter`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:263,usability,support,support,263,"The question is whether we want two scatter functions. Two will lead to simpler code. The current `pl.scatter` function is a relict from earlier times and could be simplified by dropping support for `basis`. On the other hand, it might not be so much work to add support for `x` and `y` in `plot_scatter`, in which case both functions should do the same thing. If we decide we want two functions, the second one should be `pl.scatter_embedding`, I'd say, as it's a method to visualize results of the embedding algorithms. I'm also ok with `pl.scatter_obsm` (`dimred` is something that is basically not used in the language of the docs, and I think it's a pretty misguided name of a class of functions anyway). The first one can remain `pl.scatter` and is similar to `plt.scatter` or `sns.scatter`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:475,usability,visual,visualize,475,"The question is whether we want two scatter functions. Two will lead to simpler code. The current `pl.scatter` function is a relict from earlier times and could be simplified by dropping support for `basis`. On the other hand, it might not be so much work to add support for `x` and `y` in `plot_scatter`, in which case both functions should do the same thing. If we decide we want two functions, the second one should be `pl.scatter_embedding`, I'd say, as it's a method to visualize results of the embedding algorithms. I'm also ok with `pl.scatter_obsm` (`dimred` is something that is basically not used in the language of the docs, and I think it's a pretty misguided name of a class of functions anyway). The first one can remain `pl.scatter` and is similar to `plt.scatter` or `sns.scatter`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:138,testability,simpl,simple,138,Sorry to be unresponsive for a while. I think exporting `plot_scatter` as `pl.scatter_embedding` and keeping `pl.scatter` sounds a like a simple and good solution!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:138,usability,simpl,simple,138,Sorry to be unresponsive for a while. I think exporting `plot_scatter` as `pl.scatter_embedding` and keeping `pl.scatter` sounds a like a simple and good solution!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:77,deployability,modul,module,77,"In scvelo I have decided to handle everything within one single `pl.scatter` module, where you can pass anything to `basis`, `x`, `y` and `color` from obs/var keys to arrays to lists of such. The implementation is rather simple and condensed, and @flying-sheep I'm happy to support you if (partly) merging into scanpy sounds reasonable to you. You find some exemplary use cases in this notebook: https://scvelo-notebooks.readthedocs.io/Pancreas.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:77,modifiability,modul,module,77,"In scvelo I have decided to handle everything within one single `pl.scatter` module, where you can pass anything to `basis`, `x`, `y` and `color` from obs/var keys to arrays to lists of such. The implementation is rather simple and condensed, and @flying-sheep I'm happy to support you if (partly) merging into scanpy sounds reasonable to you. You find some exemplary use cases in this notebook: https://scvelo-notebooks.readthedocs.io/Pancreas.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:77,safety,modul,module,77,"In scvelo I have decided to handle everything within one single `pl.scatter` module, where you can pass anything to `basis`, `x`, `y` and `color` from obs/var keys to arrays to lists of such. The implementation is rather simple and condensed, and @flying-sheep I'm happy to support you if (partly) merging into scanpy sounds reasonable to you. You find some exemplary use cases in this notebook: https://scvelo-notebooks.readthedocs.io/Pancreas.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:221,testability,simpl,simple,221,"In scvelo I have decided to handle everything within one single `pl.scatter` module, where you can pass anything to `basis`, `x`, `y` and `color` from obs/var keys to arrays to lists of such. The implementation is rather simple and condensed, and @flying-sheep I'm happy to support you if (partly) merging into scanpy sounds reasonable to you. You find some exemplary use cases in this notebook: https://scvelo-notebooks.readthedocs.io/Pancreas.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:221,usability,simpl,simple,221,"In scvelo I have decided to handle everything within one single `pl.scatter` module, where you can pass anything to `basis`, `x`, `y` and `color` from obs/var keys to arrays to lists of such. The implementation is rather simple and condensed, and @flying-sheep I'm happy to support you if (partly) merging into scanpy sounds reasonable to you. You find some exemplary use cases in this notebook: https://scvelo-notebooks.readthedocs.io/Pancreas.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:274,usability,support,support,274,"In scvelo I have decided to handle everything within one single `pl.scatter` module, where you can pass anything to `basis`, `x`, `y` and `color` from obs/var keys to arrays to lists of such. The implementation is rather simple and condensed, and @flying-sheep I'm happy to support you if (partly) merging into scanpy sounds reasonable to you. You find some exemplary use cases in this notebook: https://scvelo-notebooks.readthedocs.io/Pancreas.html",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:214,usability,close,closed,214,"@flying-sheep, didn't @fidelram implement this as `sc.pl.embedding` in #794? I think it got discussed in a different issue (https://github.com/theislab/scanpy/issues/762#issuecomment-517978906), so this didn't get closed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:106,interoperability,share,share,106,"Yes, but Iâ€™m not happy about the spaghetti code in pl.scatter. We should make pl.embedding and pl.scatter share most of their code, and make that code as simple as @VolkerBergenâ€™s.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:154,testability,simpl,simple,154,"Yes, but Iâ€™m not happy about the spaghetti code in pl.scatter. We should make pl.embedding and pl.scatter share most of their code, and make that code as simple as @VolkerBergenâ€™s.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:154,usability,simpl,simple,154,"Yes, but Iâ€™m not happy about the spaghetti code in pl.scatter. We should make pl.embedding and pl.scatter share most of their code, and make that code as simple as @VolkerBergenâ€™s.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:1298,availability,cluster,clusters,1298,"Happy to discuss what can be integrated from scvelo's `pl.scatter` into scanpy or how scvelo's codebase can be used.. just to mention some of the features that may also be interesting for scanpy:. - (`x`, `y`) is `str` key of (var_names, var_names), (var, var), (obs, obs), (array, array), (obs, var_names), where I find particularly passing arrays to be very convenient. - `basis` from obsm (what is the reason for having an additional `pl.embedding`?) or var_names (on layer1 vs layer2, e.g. spliced vs. unspliced). - `color` is `str` key of obs, var, layers or directly pass an array (which I find very convenient). while each of these can also be a list/tuple of `str` or arrays. . Further, we 'beautified' the colorbar, ticks etc. and added some functionality such as plotting a lin.reg line or polynomial fit of any degree directly on top of the scatterplot, show histogram/density along x and y axes, added `dpi` and `figsize` attributes and **kwargs for all other matplotlib-specific attributes such as `vmin`/`vmax`. . Apart from these it entails all functionality of scanpy's `pl.scatter`. It turned out to be very convenient to have pretty much everything within one single `pl.scatter` module, not matter whether you want to visualize an embedding, any user-specified arrays colored by clusters, or visualize a gene trend along a pseudotime. I'd start of with the general question of whether incorporating some of these functionalities into scanpy's `pl.scatter` that may be useful, or whether re-implementing it based on scvelo's `pl.scatter` codebase makes more sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:29,deployability,integr,integrated,29,"Happy to discuss what can be integrated from scvelo's `pl.scatter` into scanpy or how scvelo's codebase can be used.. just to mention some of the features that may also be interesting for scanpy:. - (`x`, `y`) is `str` key of (var_names, var_names), (var, var), (obs, obs), (array, array), (obs, var_names), where I find particularly passing arrays to be very convenient. - `basis` from obsm (what is the reason for having an additional `pl.embedding`?) or var_names (on layer1 vs layer2, e.g. spliced vs. unspliced). - `color` is `str` key of obs, var, layers or directly pass an array (which I find very convenient). while each of these can also be a list/tuple of `str` or arrays. . Further, we 'beautified' the colorbar, ticks etc. and added some functionality such as plotting a lin.reg line or polynomial fit of any degree directly on top of the scatterplot, show histogram/density along x and y axes, added `dpi` and `figsize` attributes and **kwargs for all other matplotlib-specific attributes such as `vmin`/`vmax`. . Apart from these it entails all functionality of scanpy's `pl.scatter`. It turned out to be very convenient to have pretty much everything within one single `pl.scatter` module, not matter whether you want to visualize an embedding, any user-specified arrays colored by clusters, or visualize a gene trend along a pseudotime. I'd start of with the general question of whether incorporating some of these functionalities into scanpy's `pl.scatter` that may be useful, or whether re-implementing it based on scvelo's `pl.scatter` codebase makes more sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:1198,deployability,modul,module,1198,"Happy to discuss what can be integrated from scvelo's `pl.scatter` into scanpy or how scvelo's codebase can be used.. just to mention some of the features that may also be interesting for scanpy:. - (`x`, `y`) is `str` key of (var_names, var_names), (var, var), (obs, obs), (array, array), (obs, var_names), where I find particularly passing arrays to be very convenient. - `basis` from obsm (what is the reason for having an additional `pl.embedding`?) or var_names (on layer1 vs layer2, e.g. spliced vs. unspliced). - `color` is `str` key of obs, var, layers or directly pass an array (which I find very convenient). while each of these can also be a list/tuple of `str` or arrays. . Further, we 'beautified' the colorbar, ticks etc. and added some functionality such as plotting a lin.reg line or polynomial fit of any degree directly on top of the scatterplot, show histogram/density along x and y axes, added `dpi` and `figsize` attributes and **kwargs for all other matplotlib-specific attributes such as `vmin`/`vmax`. . Apart from these it entails all functionality of scanpy's `pl.scatter`. It turned out to be very convenient to have pretty much everything within one single `pl.scatter` module, not matter whether you want to visualize an embedding, any user-specified arrays colored by clusters, or visualize a gene trend along a pseudotime. I'd start of with the general question of whether incorporating some of these functionalities into scanpy's `pl.scatter` that may be useful, or whether re-implementing it based on scvelo's `pl.scatter` codebase makes more sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:1298,deployability,cluster,clusters,1298,"Happy to discuss what can be integrated from scvelo's `pl.scatter` into scanpy or how scvelo's codebase can be used.. just to mention some of the features that may also be interesting for scanpy:. - (`x`, `y`) is `str` key of (var_names, var_names), (var, var), (obs, obs), (array, array), (obs, var_names), where I find particularly passing arrays to be very convenient. - `basis` from obsm (what is the reason for having an additional `pl.embedding`?) or var_names (on layer1 vs layer2, e.g. spliced vs. unspliced). - `color` is `str` key of obs, var, layers or directly pass an array (which I find very convenient). while each of these can also be a list/tuple of `str` or arrays. . Further, we 'beautified' the colorbar, ticks etc. and added some functionality such as plotting a lin.reg line or polynomial fit of any degree directly on top of the scatterplot, show histogram/density along x and y axes, added `dpi` and `figsize` attributes and **kwargs for all other matplotlib-specific attributes such as `vmin`/`vmax`. . Apart from these it entails all functionality of scanpy's `pl.scatter`. It turned out to be very convenient to have pretty much everything within one single `pl.scatter` module, not matter whether you want to visualize an embedding, any user-specified arrays colored by clusters, or visualize a gene trend along a pseudotime. I'd start of with the general question of whether incorporating some of these functionalities into scanpy's `pl.scatter` that may be useful, or whether re-implementing it based on scvelo's `pl.scatter` codebase makes more sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:29,integrability,integr,integrated,29,"Happy to discuss what can be integrated from scvelo's `pl.scatter` into scanpy or how scvelo's codebase can be used.. just to mention some of the features that may also be interesting for scanpy:. - (`x`, `y`) is `str` key of (var_names, var_names), (var, var), (obs, obs), (array, array), (obs, var_names), where I find particularly passing arrays to be very convenient. - `basis` from obsm (what is the reason for having an additional `pl.embedding`?) or var_names (on layer1 vs layer2, e.g. spliced vs. unspliced). - `color` is `str` key of obs, var, layers or directly pass an array (which I find very convenient). while each of these can also be a list/tuple of `str` or arrays. . Further, we 'beautified' the colorbar, ticks etc. and added some functionality such as plotting a lin.reg line or polynomial fit of any degree directly on top of the scatterplot, show histogram/density along x and y axes, added `dpi` and `figsize` attributes and **kwargs for all other matplotlib-specific attributes such as `vmin`/`vmax`. . Apart from these it entails all functionality of scanpy's `pl.scatter`. It turned out to be very convenient to have pretty much everything within one single `pl.scatter` module, not matter whether you want to visualize an embedding, any user-specified arrays colored by clusters, or visualize a gene trend along a pseudotime. I'd start of with the general question of whether incorporating some of these functionalities into scanpy's `pl.scatter` that may be useful, or whether re-implementing it based on scvelo's `pl.scatter` codebase makes more sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:29,interoperability,integr,integrated,29,"Happy to discuss what can be integrated from scvelo's `pl.scatter` into scanpy or how scvelo's codebase can be used.. just to mention some of the features that may also be interesting for scanpy:. - (`x`, `y`) is `str` key of (var_names, var_names), (var, var), (obs, obs), (array, array), (obs, var_names), where I find particularly passing arrays to be very convenient. - `basis` from obsm (what is the reason for having an additional `pl.embedding`?) or var_names (on layer1 vs layer2, e.g. spliced vs. unspliced). - `color` is `str` key of obs, var, layers or directly pass an array (which I find very convenient). while each of these can also be a list/tuple of `str` or arrays. . Further, we 'beautified' the colorbar, ticks etc. and added some functionality such as plotting a lin.reg line or polynomial fit of any degree directly on top of the scatterplot, show histogram/density along x and y axes, added `dpi` and `figsize` attributes and **kwargs for all other matplotlib-specific attributes such as `vmin`/`vmax`. . Apart from these it entails all functionality of scanpy's `pl.scatter`. It turned out to be very convenient to have pretty much everything within one single `pl.scatter` module, not matter whether you want to visualize an embedding, any user-specified arrays colored by clusters, or visualize a gene trend along a pseudotime. I'd start of with the general question of whether incorporating some of these functionalities into scanpy's `pl.scatter` that may be useful, or whether re-implementing it based on scvelo's `pl.scatter` codebase makes more sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:983,interoperability,specif,specific,983,"Happy to discuss what can be integrated from scvelo's `pl.scatter` into scanpy or how scvelo's codebase can be used.. just to mention some of the features that may also be interesting for scanpy:. - (`x`, `y`) is `str` key of (var_names, var_names), (var, var), (obs, obs), (array, array), (obs, var_names), where I find particularly passing arrays to be very convenient. - `basis` from obsm (what is the reason for having an additional `pl.embedding`?) or var_names (on layer1 vs layer2, e.g. spliced vs. unspliced). - `color` is `str` key of obs, var, layers or directly pass an array (which I find very convenient). while each of these can also be a list/tuple of `str` or arrays. . Further, we 'beautified' the colorbar, ticks etc. and added some functionality such as plotting a lin.reg line or polynomial fit of any degree directly on top of the scatterplot, show histogram/density along x and y axes, added `dpi` and `figsize` attributes and **kwargs for all other matplotlib-specific attributes such as `vmin`/`vmax`. . Apart from these it entails all functionality of scanpy's `pl.scatter`. It turned out to be very convenient to have pretty much everything within one single `pl.scatter` module, not matter whether you want to visualize an embedding, any user-specified arrays colored by clusters, or visualize a gene trend along a pseudotime. I'd start of with the general question of whether incorporating some of these functionalities into scanpy's `pl.scatter` that may be useful, or whether re-implementing it based on scvelo's `pl.scatter` codebase makes more sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:1270,interoperability,specif,specified,1270,"Happy to discuss what can be integrated from scvelo's `pl.scatter` into scanpy or how scvelo's codebase can be used.. just to mention some of the features that may also be interesting for scanpy:. - (`x`, `y`) is `str` key of (var_names, var_names), (var, var), (obs, obs), (array, array), (obs, var_names), where I find particularly passing arrays to be very convenient. - `basis` from obsm (what is the reason for having an additional `pl.embedding`?) or var_names (on layer1 vs layer2, e.g. spliced vs. unspliced). - `color` is `str` key of obs, var, layers or directly pass an array (which I find very convenient). while each of these can also be a list/tuple of `str` or arrays. . Further, we 'beautified' the colorbar, ticks etc. and added some functionality such as plotting a lin.reg line or polynomial fit of any degree directly on top of the scatterplot, show histogram/density along x and y axes, added `dpi` and `figsize` attributes and **kwargs for all other matplotlib-specific attributes such as `vmin`/`vmax`. . Apart from these it entails all functionality of scanpy's `pl.scatter`. It turned out to be very convenient to have pretty much everything within one single `pl.scatter` module, not matter whether you want to visualize an embedding, any user-specified arrays colored by clusters, or visualize a gene trend along a pseudotime. I'd start of with the general question of whether incorporating some of these functionalities into scanpy's `pl.scatter` that may be useful, or whether re-implementing it based on scvelo's `pl.scatter` codebase makes more sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:29,modifiability,integr,integrated,29,"Happy to discuss what can be integrated from scvelo's `pl.scatter` into scanpy or how scvelo's codebase can be used.. just to mention some of the features that may also be interesting for scanpy:. - (`x`, `y`) is `str` key of (var_names, var_names), (var, var), (obs, obs), (array, array), (obs, var_names), where I find particularly passing arrays to be very convenient. - `basis` from obsm (what is the reason for having an additional `pl.embedding`?) or var_names (on layer1 vs layer2, e.g. spliced vs. unspliced). - `color` is `str` key of obs, var, layers or directly pass an array (which I find very convenient). while each of these can also be a list/tuple of `str` or arrays. . Further, we 'beautified' the colorbar, ticks etc. and added some functionality such as plotting a lin.reg line or polynomial fit of any degree directly on top of the scatterplot, show histogram/density along x and y axes, added `dpi` and `figsize` attributes and **kwargs for all other matplotlib-specific attributes such as `vmin`/`vmax`. . Apart from these it entails all functionality of scanpy's `pl.scatter`. It turned out to be very convenient to have pretty much everything within one single `pl.scatter` module, not matter whether you want to visualize an embedding, any user-specified arrays colored by clusters, or visualize a gene trend along a pseudotime. I'd start of with the general question of whether incorporating some of these functionalities into scanpy's `pl.scatter` that may be useful, or whether re-implementing it based on scvelo's `pl.scatter` codebase makes more sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:554,modifiability,layer,layers,554,"Happy to discuss what can be integrated from scvelo's `pl.scatter` into scanpy or how scvelo's codebase can be used.. just to mention some of the features that may also be interesting for scanpy:. - (`x`, `y`) is `str` key of (var_names, var_names), (var, var), (obs, obs), (array, array), (obs, var_names), where I find particularly passing arrays to be very convenient. - `basis` from obsm (what is the reason for having an additional `pl.embedding`?) or var_names (on layer1 vs layer2, e.g. spliced vs. unspliced). - `color` is `str` key of obs, var, layers or directly pass an array (which I find very convenient). while each of these can also be a list/tuple of `str` or arrays. . Further, we 'beautified' the colorbar, ticks etc. and added some functionality such as plotting a lin.reg line or polynomial fit of any degree directly on top of the scatterplot, show histogram/density along x and y axes, added `dpi` and `figsize` attributes and **kwargs for all other matplotlib-specific attributes such as `vmin`/`vmax`. . Apart from these it entails all functionality of scanpy's `pl.scatter`. It turned out to be very convenient to have pretty much everything within one single `pl.scatter` module, not matter whether you want to visualize an embedding, any user-specified arrays colored by clusters, or visualize a gene trend along a pseudotime. I'd start of with the general question of whether incorporating some of these functionalities into scanpy's `pl.scatter` that may be useful, or whether re-implementing it based on scvelo's `pl.scatter` codebase makes more sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:1198,modifiability,modul,module,1198,"Happy to discuss what can be integrated from scvelo's `pl.scatter` into scanpy or how scvelo's codebase can be used.. just to mention some of the features that may also be interesting for scanpy:. - (`x`, `y`) is `str` key of (var_names, var_names), (var, var), (obs, obs), (array, array), (obs, var_names), where I find particularly passing arrays to be very convenient. - `basis` from obsm (what is the reason for having an additional `pl.embedding`?) or var_names (on layer1 vs layer2, e.g. spliced vs. unspliced). - `color` is `str` key of obs, var, layers or directly pass an array (which I find very convenient). while each of these can also be a list/tuple of `str` or arrays. . Further, we 'beautified' the colorbar, ticks etc. and added some functionality such as plotting a lin.reg line or polynomial fit of any degree directly on top of the scatterplot, show histogram/density along x and y axes, added `dpi` and `figsize` attributes and **kwargs for all other matplotlib-specific attributes such as `vmin`/`vmax`. . Apart from these it entails all functionality of scanpy's `pl.scatter`. It turned out to be very convenient to have pretty much everything within one single `pl.scatter` module, not matter whether you want to visualize an embedding, any user-specified arrays colored by clusters, or visualize a gene trend along a pseudotime. I'd start of with the general question of whether incorporating some of these functionalities into scanpy's `pl.scatter` that may be useful, or whether re-implementing it based on scvelo's `pl.scatter` codebase makes more sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:29,reliability,integr,integrated,29,"Happy to discuss what can be integrated from scvelo's `pl.scatter` into scanpy or how scvelo's codebase can be used.. just to mention some of the features that may also be interesting for scanpy:. - (`x`, `y`) is `str` key of (var_names, var_names), (var, var), (obs, obs), (array, array), (obs, var_names), where I find particularly passing arrays to be very convenient. - `basis` from obsm (what is the reason for having an additional `pl.embedding`?) or var_names (on layer1 vs layer2, e.g. spliced vs. unspliced). - `color` is `str` key of obs, var, layers or directly pass an array (which I find very convenient). while each of these can also be a list/tuple of `str` or arrays. . Further, we 'beautified' the colorbar, ticks etc. and added some functionality such as plotting a lin.reg line or polynomial fit of any degree directly on top of the scatterplot, show histogram/density along x and y axes, added `dpi` and `figsize` attributes and **kwargs for all other matplotlib-specific attributes such as `vmin`/`vmax`. . Apart from these it entails all functionality of scanpy's `pl.scatter`. It turned out to be very convenient to have pretty much everything within one single `pl.scatter` module, not matter whether you want to visualize an embedding, any user-specified arrays colored by clusters, or visualize a gene trend along a pseudotime. I'd start of with the general question of whether incorporating some of these functionalities into scanpy's `pl.scatter` that may be useful, or whether re-implementing it based on scvelo's `pl.scatter` codebase makes more sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:1198,safety,modul,module,1198,"Happy to discuss what can be integrated from scvelo's `pl.scatter` into scanpy or how scvelo's codebase can be used.. just to mention some of the features that may also be interesting for scanpy:. - (`x`, `y`) is `str` key of (var_names, var_names), (var, var), (obs, obs), (array, array), (obs, var_names), where I find particularly passing arrays to be very convenient. - `basis` from obsm (what is the reason for having an additional `pl.embedding`?) or var_names (on layer1 vs layer2, e.g. spliced vs. unspliced). - `color` is `str` key of obs, var, layers or directly pass an array (which I find very convenient). while each of these can also be a list/tuple of `str` or arrays. . Further, we 'beautified' the colorbar, ticks etc. and added some functionality such as plotting a lin.reg line or polynomial fit of any degree directly on top of the scatterplot, show histogram/density along x and y axes, added `dpi` and `figsize` attributes and **kwargs for all other matplotlib-specific attributes such as `vmin`/`vmax`. . Apart from these it entails all functionality of scanpy's `pl.scatter`. It turned out to be very convenient to have pretty much everything within one single `pl.scatter` module, not matter whether you want to visualize an embedding, any user-specified arrays colored by clusters, or visualize a gene trend along a pseudotime. I'd start of with the general question of whether incorporating some of these functionalities into scanpy's `pl.scatter` that may be useful, or whether re-implementing it based on scvelo's `pl.scatter` codebase makes more sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:29,security,integr,integrated,29,"Happy to discuss what can be integrated from scvelo's `pl.scatter` into scanpy or how scvelo's codebase can be used.. just to mention some of the features that may also be interesting for scanpy:. - (`x`, `y`) is `str` key of (var_names, var_names), (var, var), (obs, obs), (array, array), (obs, var_names), where I find particularly passing arrays to be very convenient. - `basis` from obsm (what is the reason for having an additional `pl.embedding`?) or var_names (on layer1 vs layer2, e.g. spliced vs. unspliced). - `color` is `str` key of obs, var, layers or directly pass an array (which I find very convenient). while each of these can also be a list/tuple of `str` or arrays. . Further, we 'beautified' the colorbar, ticks etc. and added some functionality such as plotting a lin.reg line or polynomial fit of any degree directly on top of the scatterplot, show histogram/density along x and y axes, added `dpi` and `figsize` attributes and **kwargs for all other matplotlib-specific attributes such as `vmin`/`vmax`. . Apart from these it entails all functionality of scanpy's `pl.scatter`. It turned out to be very convenient to have pretty much everything within one single `pl.scatter` module, not matter whether you want to visualize an embedding, any user-specified arrays colored by clusters, or visualize a gene trend along a pseudotime. I'd start of with the general question of whether incorporating some of these functionalities into scanpy's `pl.scatter` that may be useful, or whether re-implementing it based on scvelo's `pl.scatter` codebase makes more sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:29,testability,integr,integrated,29,"Happy to discuss what can be integrated from scvelo's `pl.scatter` into scanpy or how scvelo's codebase can be used.. just to mention some of the features that may also be interesting for scanpy:. - (`x`, `y`) is `str` key of (var_names, var_names), (var, var), (obs, obs), (array, array), (obs, var_names), where I find particularly passing arrays to be very convenient. - `basis` from obsm (what is the reason for having an additional `pl.embedding`?) or var_names (on layer1 vs layer2, e.g. spliced vs. unspliced). - `color` is `str` key of obs, var, layers or directly pass an array (which I find very convenient). while each of these can also be a list/tuple of `str` or arrays. . Further, we 'beautified' the colorbar, ticks etc. and added some functionality such as plotting a lin.reg line or polynomial fit of any degree directly on top of the scatterplot, show histogram/density along x and y axes, added `dpi` and `figsize` attributes and **kwargs for all other matplotlib-specific attributes such as `vmin`/`vmax`. . Apart from these it entails all functionality of scanpy's `pl.scatter`. It turned out to be very convenient to have pretty much everything within one single `pl.scatter` module, not matter whether you want to visualize an embedding, any user-specified arrays colored by clusters, or visualize a gene trend along a pseudotime. I'd start of with the general question of whether incorporating some of these functionalities into scanpy's `pl.scatter` that may be useful, or whether re-implementing it based on scvelo's `pl.scatter` codebase makes more sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:1237,usability,visual,visualize,1237,"Happy to discuss what can be integrated from scvelo's `pl.scatter` into scanpy or how scvelo's codebase can be used.. just to mention some of the features that may also be interesting for scanpy:. - (`x`, `y`) is `str` key of (var_names, var_names), (var, var), (obs, obs), (array, array), (obs, var_names), where I find particularly passing arrays to be very convenient. - `basis` from obsm (what is the reason for having an additional `pl.embedding`?) or var_names (on layer1 vs layer2, e.g. spliced vs. unspliced). - `color` is `str` key of obs, var, layers or directly pass an array (which I find very convenient). while each of these can also be a list/tuple of `str` or arrays. . Further, we 'beautified' the colorbar, ticks etc. and added some functionality such as plotting a lin.reg line or polynomial fit of any degree directly on top of the scatterplot, show histogram/density along x and y axes, added `dpi` and `figsize` attributes and **kwargs for all other matplotlib-specific attributes such as `vmin`/`vmax`. . Apart from these it entails all functionality of scanpy's `pl.scatter`. It turned out to be very convenient to have pretty much everything within one single `pl.scatter` module, not matter whether you want to visualize an embedding, any user-specified arrays colored by clusters, or visualize a gene trend along a pseudotime. I'd start of with the general question of whether incorporating some of these functionalities into scanpy's `pl.scatter` that may be useful, or whether re-implementing it based on scvelo's `pl.scatter` codebase makes more sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:1265,usability,user,user-specified,1265,"Happy to discuss what can be integrated from scvelo's `pl.scatter` into scanpy or how scvelo's codebase can be used.. just to mention some of the features that may also be interesting for scanpy:. - (`x`, `y`) is `str` key of (var_names, var_names), (var, var), (obs, obs), (array, array), (obs, var_names), where I find particularly passing arrays to be very convenient. - `basis` from obsm (what is the reason for having an additional `pl.embedding`?) or var_names (on layer1 vs layer2, e.g. spliced vs. unspliced). - `color` is `str` key of obs, var, layers or directly pass an array (which I find very convenient). while each of these can also be a list/tuple of `str` or arrays. . Further, we 'beautified' the colorbar, ticks etc. and added some functionality such as plotting a lin.reg line or polynomial fit of any degree directly on top of the scatterplot, show histogram/density along x and y axes, added `dpi` and `figsize` attributes and **kwargs for all other matplotlib-specific attributes such as `vmin`/`vmax`. . Apart from these it entails all functionality of scanpy's `pl.scatter`. It turned out to be very convenient to have pretty much everything within one single `pl.scatter` module, not matter whether you want to visualize an embedding, any user-specified arrays colored by clusters, or visualize a gene trend along a pseudotime. I'd start of with the general question of whether incorporating some of these functionalities into scanpy's `pl.scatter` that may be useful, or whether re-implementing it based on scvelo's `pl.scatter` codebase makes more sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:1311,usability,visual,visualize,1311,"Happy to discuss what can be integrated from scvelo's `pl.scatter` into scanpy or how scvelo's codebase can be used.. just to mention some of the features that may also be interesting for scanpy:. - (`x`, `y`) is `str` key of (var_names, var_names), (var, var), (obs, obs), (array, array), (obs, var_names), where I find particularly passing arrays to be very convenient. - `basis` from obsm (what is the reason for having an additional `pl.embedding`?) or var_names (on layer1 vs layer2, e.g. spliced vs. unspliced). - `color` is `str` key of obs, var, layers or directly pass an array (which I find very convenient). while each of these can also be a list/tuple of `str` or arrays. . Further, we 'beautified' the colorbar, ticks etc. and added some functionality such as plotting a lin.reg line or polynomial fit of any degree directly on top of the scatterplot, show histogram/density along x and y axes, added `dpi` and `figsize` attributes and **kwargs for all other matplotlib-specific attributes such as `vmin`/`vmax`. . Apart from these it entails all functionality of scanpy's `pl.scatter`. It turned out to be very convenient to have pretty much everything within one single `pl.scatter` module, not matter whether you want to visualize an embedding, any user-specified arrays colored by clusters, or visualize a gene trend along a pseudotime. I'd start of with the general question of whether incorporating some of these functionalities into scanpy's `pl.scatter` that may be useful, or whether re-implementing it based on scvelo's `pl.scatter` codebase makes more sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:73,deployability,version,version,73,"I welcome @VolkerBergen ideas about plot scatter. I have used the scvelo version of scatter and works quite well and always thought that we could integrate this. Our comprehensive collection of tests related to embeddings should facilitate the recreation of the current functionality using a scatter module. As @flying-sheep points out we have a mess with respect to `pl.scatter` and `pl.embeddings` and would be great to unify the code. Currently, `pl.scatter` is used to plot two genes or any two variables like in `sc.pl.highly_variable_genes`. `pl.embedding` takes x,y (and z if 3D) from `.obsm` while adjusting color and size depending on given parameters. When I started working on the plotting functions I didn't touch `pl.scatter` which remains quite convoluted and hard to follow. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:146,deployability,integr,integrate,146,"I welcome @VolkerBergen ideas about plot scatter. I have used the scvelo version of scatter and works quite well and always thought that we could integrate this. Our comprehensive collection of tests related to embeddings should facilitate the recreation of the current functionality using a scatter module. As @flying-sheep points out we have a mess with respect to `pl.scatter` and `pl.embeddings` and would be great to unify the code. Currently, `pl.scatter` is used to plot two genes or any two variables like in `sc.pl.highly_variable_genes`. `pl.embedding` takes x,y (and z if 3D) from `.obsm` while adjusting color and size depending on given parameters. When I started working on the plotting functions I didn't touch `pl.scatter` which remains quite convoluted and hard to follow. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:300,deployability,modul,module,300,"I welcome @VolkerBergen ideas about plot scatter. I have used the scvelo version of scatter and works quite well and always thought that we could integrate this. Our comprehensive collection of tests related to embeddings should facilitate the recreation of the current functionality using a scatter module. As @flying-sheep points out we have a mess with respect to `pl.scatter` and `pl.embeddings` and would be great to unify the code. Currently, `pl.scatter` is used to plot two genes or any two variables like in `sc.pl.highly_variable_genes`. `pl.embedding` takes x,y (and z if 3D) from `.obsm` while adjusting color and size depending on given parameters. When I started working on the plotting functions I didn't touch `pl.scatter` which remains quite convoluted and hard to follow. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:631,deployability,depend,depending,631,"I welcome @VolkerBergen ideas about plot scatter. I have used the scvelo version of scatter and works quite well and always thought that we could integrate this. Our comprehensive collection of tests related to embeddings should facilitate the recreation of the current functionality using a scatter module. As @flying-sheep points out we have a mess with respect to `pl.scatter` and `pl.embeddings` and would be great to unify the code. Currently, `pl.scatter` is used to plot two genes or any two variables like in `sc.pl.highly_variable_genes`. `pl.embedding` takes x,y (and z if 3D) from `.obsm` while adjusting color and size depending on given parameters. When I started working on the plotting functions I didn't touch `pl.scatter` which remains quite convoluted and hard to follow. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:262,energy efficiency,current,current,262,"I welcome @VolkerBergen ideas about plot scatter. I have used the scvelo version of scatter and works quite well and always thought that we could integrate this. Our comprehensive collection of tests related to embeddings should facilitate the recreation of the current functionality using a scatter module. As @flying-sheep points out we have a mess with respect to `pl.scatter` and `pl.embeddings` and would be great to unify the code. Currently, `pl.scatter` is used to plot two genes or any two variables like in `sc.pl.highly_variable_genes`. `pl.embedding` takes x,y (and z if 3D) from `.obsm` while adjusting color and size depending on given parameters. When I started working on the plotting functions I didn't touch `pl.scatter` which remains quite convoluted and hard to follow. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:438,energy efficiency,Current,Currently,438,"I welcome @VolkerBergen ideas about plot scatter. I have used the scvelo version of scatter and works quite well and always thought that we could integrate this. Our comprehensive collection of tests related to embeddings should facilitate the recreation of the current functionality using a scatter module. As @flying-sheep points out we have a mess with respect to `pl.scatter` and `pl.embeddings` and would be great to unify the code. Currently, `pl.scatter` is used to plot two genes or any two variables like in `sc.pl.highly_variable_genes`. `pl.embedding` takes x,y (and z if 3D) from `.obsm` while adjusting color and size depending on given parameters. When I started working on the plotting functions I didn't touch `pl.scatter` which remains quite convoluted and hard to follow. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:73,integrability,version,version,73,"I welcome @VolkerBergen ideas about plot scatter. I have used the scvelo version of scatter and works quite well and always thought that we could integrate this. Our comprehensive collection of tests related to embeddings should facilitate the recreation of the current functionality using a scatter module. As @flying-sheep points out we have a mess with respect to `pl.scatter` and `pl.embeddings` and would be great to unify the code. Currently, `pl.scatter` is used to plot two genes or any two variables like in `sc.pl.highly_variable_genes`. `pl.embedding` takes x,y (and z if 3D) from `.obsm` while adjusting color and size depending on given parameters. When I started working on the plotting functions I didn't touch `pl.scatter` which remains quite convoluted and hard to follow. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:146,integrability,integr,integrate,146,"I welcome @VolkerBergen ideas about plot scatter. I have used the scvelo version of scatter and works quite well and always thought that we could integrate this. Our comprehensive collection of tests related to embeddings should facilitate the recreation of the current functionality using a scatter module. As @flying-sheep points out we have a mess with respect to `pl.scatter` and `pl.embeddings` and would be great to unify the code. Currently, `pl.scatter` is used to plot two genes or any two variables like in `sc.pl.highly_variable_genes`. `pl.embedding` takes x,y (and z if 3D) from `.obsm` while adjusting color and size depending on given parameters. When I started working on the plotting functions I didn't touch `pl.scatter` which remains quite convoluted and hard to follow. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:631,integrability,depend,depending,631,"I welcome @VolkerBergen ideas about plot scatter. I have used the scvelo version of scatter and works quite well and always thought that we could integrate this. Our comprehensive collection of tests related to embeddings should facilitate the recreation of the current functionality using a scatter module. As @flying-sheep points out we have a mess with respect to `pl.scatter` and `pl.embeddings` and would be great to unify the code. Currently, `pl.scatter` is used to plot two genes or any two variables like in `sc.pl.highly_variable_genes`. `pl.embedding` takes x,y (and z if 3D) from `.obsm` while adjusting color and size depending on given parameters. When I started working on the plotting functions I didn't touch `pl.scatter` which remains quite convoluted and hard to follow. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:146,interoperability,integr,integrate,146,"I welcome @VolkerBergen ideas about plot scatter. I have used the scvelo version of scatter and works quite well and always thought that we could integrate this. Our comprehensive collection of tests related to embeddings should facilitate the recreation of the current functionality using a scatter module. As @flying-sheep points out we have a mess with respect to `pl.scatter` and `pl.embeddings` and would be great to unify the code. Currently, `pl.scatter` is used to plot two genes or any two variables like in `sc.pl.highly_variable_genes`. `pl.embedding` takes x,y (and z if 3D) from `.obsm` while adjusting color and size depending on given parameters. When I started working on the plotting functions I didn't touch `pl.scatter` which remains quite convoluted and hard to follow. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:73,modifiability,version,version,73,"I welcome @VolkerBergen ideas about plot scatter. I have used the scvelo version of scatter and works quite well and always thought that we could integrate this. Our comprehensive collection of tests related to embeddings should facilitate the recreation of the current functionality using a scatter module. As @flying-sheep points out we have a mess with respect to `pl.scatter` and `pl.embeddings` and would be great to unify the code. Currently, `pl.scatter` is used to plot two genes or any two variables like in `sc.pl.highly_variable_genes`. `pl.embedding` takes x,y (and z if 3D) from `.obsm` while adjusting color and size depending on given parameters. When I started working on the plotting functions I didn't touch `pl.scatter` which remains quite convoluted and hard to follow. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:146,modifiability,integr,integrate,146,"I welcome @VolkerBergen ideas about plot scatter. I have used the scvelo version of scatter and works quite well and always thought that we could integrate this. Our comprehensive collection of tests related to embeddings should facilitate the recreation of the current functionality using a scatter module. As @flying-sheep points out we have a mess with respect to `pl.scatter` and `pl.embeddings` and would be great to unify the code. Currently, `pl.scatter` is used to plot two genes or any two variables like in `sc.pl.highly_variable_genes`. `pl.embedding` takes x,y (and z if 3D) from `.obsm` while adjusting color and size depending on given parameters. When I started working on the plotting functions I didn't touch `pl.scatter` which remains quite convoluted and hard to follow. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:300,modifiability,modul,module,300,"I welcome @VolkerBergen ideas about plot scatter. I have used the scvelo version of scatter and works quite well and always thought that we could integrate this. Our comprehensive collection of tests related to embeddings should facilitate the recreation of the current functionality using a scatter module. As @flying-sheep points out we have a mess with respect to `pl.scatter` and `pl.embeddings` and would be great to unify the code. Currently, `pl.scatter` is used to plot two genes or any two variables like in `sc.pl.highly_variable_genes`. `pl.embedding` takes x,y (and z if 3D) from `.obsm` while adjusting color and size depending on given parameters. When I started working on the plotting functions I didn't touch `pl.scatter` which remains quite convoluted and hard to follow. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:499,modifiability,variab,variables,499,"I welcome @VolkerBergen ideas about plot scatter. I have used the scvelo version of scatter and works quite well and always thought that we could integrate this. Our comprehensive collection of tests related to embeddings should facilitate the recreation of the current functionality using a scatter module. As @flying-sheep points out we have a mess with respect to `pl.scatter` and `pl.embeddings` and would be great to unify the code. Currently, `pl.scatter` is used to plot two genes or any two variables like in `sc.pl.highly_variable_genes`. `pl.embedding` takes x,y (and z if 3D) from `.obsm` while adjusting color and size depending on given parameters. When I started working on the plotting functions I didn't touch `pl.scatter` which remains quite convoluted and hard to follow. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:631,modifiability,depend,depending,631,"I welcome @VolkerBergen ideas about plot scatter. I have used the scvelo version of scatter and works quite well and always thought that we could integrate this. Our comprehensive collection of tests related to embeddings should facilitate the recreation of the current functionality using a scatter module. As @flying-sheep points out we have a mess with respect to `pl.scatter` and `pl.embeddings` and would be great to unify the code. Currently, `pl.scatter` is used to plot two genes or any two variables like in `sc.pl.highly_variable_genes`. `pl.embedding` takes x,y (and z if 3D) from `.obsm` while adjusting color and size depending on given parameters. When I started working on the plotting functions I didn't touch `pl.scatter` which remains quite convoluted and hard to follow. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:650,modifiability,paramet,parameters,650,"I welcome @VolkerBergen ideas about plot scatter. I have used the scvelo version of scatter and works quite well and always thought that we could integrate this. Our comprehensive collection of tests related to embeddings should facilitate the recreation of the current functionality using a scatter module. As @flying-sheep points out we have a mess with respect to `pl.scatter` and `pl.embeddings` and would be great to unify the code. Currently, `pl.scatter` is used to plot two genes or any two variables like in `sc.pl.highly_variable_genes`. `pl.embedding` takes x,y (and z if 3D) from `.obsm` while adjusting color and size depending on given parameters. When I started working on the plotting functions I didn't touch `pl.scatter` which remains quite convoluted and hard to follow. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:146,reliability,integr,integrate,146,"I welcome @VolkerBergen ideas about plot scatter. I have used the scvelo version of scatter and works quite well and always thought that we could integrate this. Our comprehensive collection of tests related to embeddings should facilitate the recreation of the current functionality using a scatter module. As @flying-sheep points out we have a mess with respect to `pl.scatter` and `pl.embeddings` and would be great to unify the code. Currently, `pl.scatter` is used to plot two genes or any two variables like in `sc.pl.highly_variable_genes`. `pl.embedding` takes x,y (and z if 3D) from `.obsm` while adjusting color and size depending on given parameters. When I started working on the plotting functions I didn't touch `pl.scatter` which remains quite convoluted and hard to follow. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:194,safety,test,tests,194,"I welcome @VolkerBergen ideas about plot scatter. I have used the scvelo version of scatter and works quite well and always thought that we could integrate this. Our comprehensive collection of tests related to embeddings should facilitate the recreation of the current functionality using a scatter module. As @flying-sheep points out we have a mess with respect to `pl.scatter` and `pl.embeddings` and would be great to unify the code. Currently, `pl.scatter` is used to plot two genes or any two variables like in `sc.pl.highly_variable_genes`. `pl.embedding` takes x,y (and z if 3D) from `.obsm` while adjusting color and size depending on given parameters. When I started working on the plotting functions I didn't touch `pl.scatter` which remains quite convoluted and hard to follow. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:300,safety,modul,module,300,"I welcome @VolkerBergen ideas about plot scatter. I have used the scvelo version of scatter and works quite well and always thought that we could integrate this. Our comprehensive collection of tests related to embeddings should facilitate the recreation of the current functionality using a scatter module. As @flying-sheep points out we have a mess with respect to `pl.scatter` and `pl.embeddings` and would be great to unify the code. Currently, `pl.scatter` is used to plot two genes or any two variables like in `sc.pl.highly_variable_genes`. `pl.embedding` takes x,y (and z if 3D) from `.obsm` while adjusting color and size depending on given parameters. When I started working on the plotting functions I didn't touch `pl.scatter` which remains quite convoluted and hard to follow. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:631,safety,depend,depending,631,"I welcome @VolkerBergen ideas about plot scatter. I have used the scvelo version of scatter and works quite well and always thought that we could integrate this. Our comprehensive collection of tests related to embeddings should facilitate the recreation of the current functionality using a scatter module. As @flying-sheep points out we have a mess with respect to `pl.scatter` and `pl.embeddings` and would be great to unify the code. Currently, `pl.scatter` is used to plot two genes or any two variables like in `sc.pl.highly_variable_genes`. `pl.embedding` takes x,y (and z if 3D) from `.obsm` while adjusting color and size depending on given parameters. When I started working on the plotting functions I didn't touch `pl.scatter` which remains quite convoluted and hard to follow. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:146,security,integr,integrate,146,"I welcome @VolkerBergen ideas about plot scatter. I have used the scvelo version of scatter and works quite well and always thought that we could integrate this. Our comprehensive collection of tests related to embeddings should facilitate the recreation of the current functionality using a scatter module. As @flying-sheep points out we have a mess with respect to `pl.scatter` and `pl.embeddings` and would be great to unify the code. Currently, `pl.scatter` is used to plot two genes or any two variables like in `sc.pl.highly_variable_genes`. `pl.embedding` takes x,y (and z if 3D) from `.obsm` while adjusting color and size depending on given parameters. When I started working on the plotting functions I didn't touch `pl.scatter` which remains quite convoluted and hard to follow. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:146,testability,integr,integrate,146,"I welcome @VolkerBergen ideas about plot scatter. I have used the scvelo version of scatter and works quite well and always thought that we could integrate this. Our comprehensive collection of tests related to embeddings should facilitate the recreation of the current functionality using a scatter module. As @flying-sheep points out we have a mess with respect to `pl.scatter` and `pl.embeddings` and would be great to unify the code. Currently, `pl.scatter` is used to plot two genes or any two variables like in `sc.pl.highly_variable_genes`. `pl.embedding` takes x,y (and z if 3D) from `.obsm` while adjusting color and size depending on given parameters. When I started working on the plotting functions I didn't touch `pl.scatter` which remains quite convoluted and hard to follow. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:194,testability,test,tests,194,"I welcome @VolkerBergen ideas about plot scatter. I have used the scvelo version of scatter and works quite well and always thought that we could integrate this. Our comprehensive collection of tests related to embeddings should facilitate the recreation of the current functionality using a scatter module. As @flying-sheep points out we have a mess with respect to `pl.scatter` and `pl.embeddings` and would be great to unify the code. Currently, `pl.scatter` is used to plot two genes or any two variables like in `sc.pl.highly_variable_genes`. `pl.embedding` takes x,y (and z if 3D) from `.obsm` while adjusting color and size depending on given parameters. When I started working on the plotting functions I didn't touch `pl.scatter` which remains quite convoluted and hard to follow. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:631,testability,depend,depending,631,"I welcome @VolkerBergen ideas about plot scatter. I have used the scvelo version of scatter and works quite well and always thought that we could integrate this. Our comprehensive collection of tests related to embeddings should facilitate the recreation of the current functionality using a scatter module. As @flying-sheep points out we have a mess with respect to `pl.scatter` and `pl.embeddings` and would be great to unify the code. Currently, `pl.scatter` is used to plot two genes or any two variables like in `sc.pl.highly_variable_genes`. `pl.embedding` takes x,y (and z if 3D) from `.obsm` while adjusting color and size depending on given parameters. When I started working on the plotting functions I didn't touch `pl.scatter` which remains quite convoluted and hard to follow. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:17,energy efficiency,current,currently,17,@stefanpeidli is currently testing whether scVelo `pl.scatter` entails all scanpy functionality. @fidelram Can you give me a very brief outline of what functionality you added in `pl.embedding` so that I can account for these as well?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:27,safety,test,testing,27,@stefanpeidli is currently testing whether scVelo `pl.scatter` entails all scanpy functionality. @fidelram Can you give me a very brief outline of what functionality you added in `pl.embedding` so that I can account for these as well?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:27,testability,test,testing,27,@stefanpeidli is currently testing whether scVelo `pl.scatter` entails all scanpy functionality. @fidelram Can you give me a very brief outline of what functionality you added in `pl.embedding` so that I can account for these as well?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:383,availability,sli,slightly,383,"Tested scvelo's vs scanpy's `pl.scatter` (will share test modules + notebooks later). scvelo entails scanpy's functionality (and adds a couple extensions as explained above), except for the following:. - we don't have `left_margin` and `right_margin` attributes (don't think they're necessary). - point `size` convention acc. to scanpy (if settings._rcParams_style == 'scanpy') else slightly adjusted. - when `basis` is in `.var_names`, then an unspliced/spliced phase portrait is plotted (I guess that's not needed in scanpy?). Further we have some more defaults:. - if `basis` is None, then use a default basis in the given order if available: umap, tsne, pca. - if `color` is None, then use a default color in the given order if available: clusters, louvain. - if `frameon` is None, then only set frame if it is not embedding and axes values do matter. - if `color_map` is None, then use 'viridis_r' if vals in [0,1], else matplotlib's default. Further, these can be used interchangeably: `size` and `s`, `color` and `c`, `color_map` and `cmap`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:635,availability,avail,available,635,"Tested scvelo's vs scanpy's `pl.scatter` (will share test modules + notebooks later). scvelo entails scanpy's functionality (and adds a couple extensions as explained above), except for the following:. - we don't have `left_margin` and `right_margin` attributes (don't think they're necessary). - point `size` convention acc. to scanpy (if settings._rcParams_style == 'scanpy') else slightly adjusted. - when `basis` is in `.var_names`, then an unspliced/spliced phase portrait is plotted (I guess that's not needed in scanpy?). Further we have some more defaults:. - if `basis` is None, then use a default basis in the given order if available: umap, tsne, pca. - if `color` is None, then use a default color in the given order if available: clusters, louvain. - if `frameon` is None, then only set frame if it is not embedding and axes values do matter. - if `color_map` is None, then use 'viridis_r' if vals in [0,1], else matplotlib's default. Further, these can be used interchangeably: `size` and `s`, `color` and `c`, `color_map` and `cmap`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:732,availability,avail,available,732,"Tested scvelo's vs scanpy's `pl.scatter` (will share test modules + notebooks later). scvelo entails scanpy's functionality (and adds a couple extensions as explained above), except for the following:. - we don't have `left_margin` and `right_margin` attributes (don't think they're necessary). - point `size` convention acc. to scanpy (if settings._rcParams_style == 'scanpy') else slightly adjusted. - when `basis` is in `.var_names`, then an unspliced/spliced phase portrait is plotted (I guess that's not needed in scanpy?). Further we have some more defaults:. - if `basis` is None, then use a default basis in the given order if available: umap, tsne, pca. - if `color` is None, then use a default color in the given order if available: clusters, louvain. - if `frameon` is None, then only set frame if it is not embedding and axes values do matter. - if `color_map` is None, then use 'viridis_r' if vals in [0,1], else matplotlib's default. Further, these can be used interchangeably: `size` and `s`, `color` and `c`, `color_map` and `cmap`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:743,availability,cluster,clusters,743,"Tested scvelo's vs scanpy's `pl.scatter` (will share test modules + notebooks later). scvelo entails scanpy's functionality (and adds a couple extensions as explained above), except for the following:. - we don't have `left_margin` and `right_margin` attributes (don't think they're necessary). - point `size` convention acc. to scanpy (if settings._rcParams_style == 'scanpy') else slightly adjusted. - when `basis` is in `.var_names`, then an unspliced/spliced phase portrait is plotted (I guess that's not needed in scanpy?). Further we have some more defaults:. - if `basis` is None, then use a default basis in the given order if available: umap, tsne, pca. - if `color` is None, then use a default color in the given order if available: clusters, louvain. - if `frameon` is None, then only set frame if it is not embedding and axes values do matter. - if `color_map` is None, then use 'viridis_r' if vals in [0,1], else matplotlib's default. Further, these can be used interchangeably: `size` and `s`, `color` and `c`, `color_map` and `cmap`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:58,deployability,modul,modules,58,"Tested scvelo's vs scanpy's `pl.scatter` (will share test modules + notebooks later). scvelo entails scanpy's functionality (and adds a couple extensions as explained above), except for the following:. - we don't have `left_margin` and `right_margin` attributes (don't think they're necessary). - point `size` convention acc. to scanpy (if settings._rcParams_style == 'scanpy') else slightly adjusted. - when `basis` is in `.var_names`, then an unspliced/spliced phase portrait is plotted (I guess that's not needed in scanpy?). Further we have some more defaults:. - if `basis` is None, then use a default basis in the given order if available: umap, tsne, pca. - if `color` is None, then use a default color in the given order if available: clusters, louvain. - if `frameon` is None, then only set frame if it is not embedding and axes values do matter. - if `color_map` is None, then use 'viridis_r' if vals in [0,1], else matplotlib's default. Further, these can be used interchangeably: `size` and `s`, `color` and `c`, `color_map` and `cmap`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:743,deployability,cluster,clusters,743,"Tested scvelo's vs scanpy's `pl.scatter` (will share test modules + notebooks later). scvelo entails scanpy's functionality (and adds a couple extensions as explained above), except for the following:. - we don't have `left_margin` and `right_margin` attributes (don't think they're necessary). - point `size` convention acc. to scanpy (if settings._rcParams_style == 'scanpy') else slightly adjusted. - when `basis` is in `.var_names`, then an unspliced/spliced phase portrait is plotted (I guess that's not needed in scanpy?). Further we have some more defaults:. - if `basis` is None, then use a default basis in the given order if available: umap, tsne, pca. - if `color` is None, then use a default color in the given order if available: clusters, louvain. - if `frameon` is None, then only set frame if it is not embedding and axes values do matter. - if `color_map` is None, then use 'viridis_r' if vals in [0,1], else matplotlib's default. Further, these can be used interchangeably: `size` and `s`, `color` and `c`, `color_map` and `cmap`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:136,integrability,coupl,couple,136,"Tested scvelo's vs scanpy's `pl.scatter` (will share test modules + notebooks later). scvelo entails scanpy's functionality (and adds a couple extensions as explained above), except for the following:. - we don't have `left_margin` and `right_margin` attributes (don't think they're necessary). - point `size` convention acc. to scanpy (if settings._rcParams_style == 'scanpy') else slightly adjusted. - when `basis` is in `.var_names`, then an unspliced/spliced phase portrait is plotted (I guess that's not needed in scanpy?). Further we have some more defaults:. - if `basis` is None, then use a default basis in the given order if available: umap, tsne, pca. - if `color` is None, then use a default color in the given order if available: clusters, louvain. - if `frameon` is None, then only set frame if it is not embedding and axes values do matter. - if `color_map` is None, then use 'viridis_r' if vals in [0,1], else matplotlib's default. Further, these can be used interchangeably: `size` and `s`, `color` and `c`, `color_map` and `cmap`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:47,interoperability,share,share,47,"Tested scvelo's vs scanpy's `pl.scatter` (will share test modules + notebooks later). scvelo entails scanpy's functionality (and adds a couple extensions as explained above), except for the following:. - we don't have `left_margin` and `right_margin` attributes (don't think they're necessary). - point `size` convention acc. to scanpy (if settings._rcParams_style == 'scanpy') else slightly adjusted. - when `basis` is in `.var_names`, then an unspliced/spliced phase portrait is plotted (I guess that's not needed in scanpy?). Further we have some more defaults:. - if `basis` is None, then use a default basis in the given order if available: umap, tsne, pca. - if `color` is None, then use a default color in the given order if available: clusters, louvain. - if `frameon` is None, then only set frame if it is not embedding and axes values do matter. - if `color_map` is None, then use 'viridis_r' if vals in [0,1], else matplotlib's default. Further, these can be used interchangeably: `size` and `s`, `color` and `c`, `color_map` and `cmap`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:58,modifiability,modul,modules,58,"Tested scvelo's vs scanpy's `pl.scatter` (will share test modules + notebooks later). scvelo entails scanpy's functionality (and adds a couple extensions as explained above), except for the following:. - we don't have `left_margin` and `right_margin` attributes (don't think they're necessary). - point `size` convention acc. to scanpy (if settings._rcParams_style == 'scanpy') else slightly adjusted. - when `basis` is in `.var_names`, then an unspliced/spliced phase portrait is plotted (I guess that's not needed in scanpy?). Further we have some more defaults:. - if `basis` is None, then use a default basis in the given order if available: umap, tsne, pca. - if `color` is None, then use a default color in the given order if available: clusters, louvain. - if `frameon` is None, then only set frame if it is not embedding and axes values do matter. - if `color_map` is None, then use 'viridis_r' if vals in [0,1], else matplotlib's default. Further, these can be used interchangeably: `size` and `s`, `color` and `c`, `color_map` and `cmap`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:136,modifiability,coupl,couple,136,"Tested scvelo's vs scanpy's `pl.scatter` (will share test modules + notebooks later). scvelo entails scanpy's functionality (and adds a couple extensions as explained above), except for the following:. - we don't have `left_margin` and `right_margin` attributes (don't think they're necessary). - point `size` convention acc. to scanpy (if settings._rcParams_style == 'scanpy') else slightly adjusted. - when `basis` is in `.var_names`, then an unspliced/spliced phase portrait is plotted (I guess that's not needed in scanpy?). Further we have some more defaults:. - if `basis` is None, then use a default basis in the given order if available: umap, tsne, pca. - if `color` is None, then use a default color in the given order if available: clusters, louvain. - if `frameon` is None, then only set frame if it is not embedding and axes values do matter. - if `color_map` is None, then use 'viridis_r' if vals in [0,1], else matplotlib's default. Further, these can be used interchangeably: `size` and `s`, `color` and `c`, `color_map` and `cmap`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:143,modifiability,extens,extensions,143,"Tested scvelo's vs scanpy's `pl.scatter` (will share test modules + notebooks later). scvelo entails scanpy's functionality (and adds a couple extensions as explained above), except for the following:. - we don't have `left_margin` and `right_margin` attributes (don't think they're necessary). - point `size` convention acc. to scanpy (if settings._rcParams_style == 'scanpy') else slightly adjusted. - when `basis` is in `.var_names`, then an unspliced/spliced phase portrait is plotted (I guess that's not needed in scanpy?). Further we have some more defaults:. - if `basis` is None, then use a default basis in the given order if available: umap, tsne, pca. - if `color` is None, then use a default color in the given order if available: clusters, louvain. - if `frameon` is None, then only set frame if it is not embedding and axes values do matter. - if `color_map` is None, then use 'viridis_r' if vals in [0,1], else matplotlib's default. Further, these can be used interchangeably: `size` and `s`, `color` and `c`, `color_map` and `cmap`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:383,reliability,sli,slightly,383,"Tested scvelo's vs scanpy's `pl.scatter` (will share test modules + notebooks later). scvelo entails scanpy's functionality (and adds a couple extensions as explained above), except for the following:. - we don't have `left_margin` and `right_margin` attributes (don't think they're necessary). - point `size` convention acc. to scanpy (if settings._rcParams_style == 'scanpy') else slightly adjusted. - when `basis` is in `.var_names`, then an unspliced/spliced phase portrait is plotted (I guess that's not needed in scanpy?). Further we have some more defaults:. - if `basis` is None, then use a default basis in the given order if available: umap, tsne, pca. - if `color` is None, then use a default color in the given order if available: clusters, louvain. - if `frameon` is None, then only set frame if it is not embedding and axes values do matter. - if `color_map` is None, then use 'viridis_r' if vals in [0,1], else matplotlib's default. Further, these can be used interchangeably: `size` and `s`, `color` and `c`, `color_map` and `cmap`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:635,reliability,availab,available,635,"Tested scvelo's vs scanpy's `pl.scatter` (will share test modules + notebooks later). scvelo entails scanpy's functionality (and adds a couple extensions as explained above), except for the following:. - we don't have `left_margin` and `right_margin` attributes (don't think they're necessary). - point `size` convention acc. to scanpy (if settings._rcParams_style == 'scanpy') else slightly adjusted. - when `basis` is in `.var_names`, then an unspliced/spliced phase portrait is plotted (I guess that's not needed in scanpy?). Further we have some more defaults:. - if `basis` is None, then use a default basis in the given order if available: umap, tsne, pca. - if `color` is None, then use a default color in the given order if available: clusters, louvain. - if `frameon` is None, then only set frame if it is not embedding and axes values do matter. - if `color_map` is None, then use 'viridis_r' if vals in [0,1], else matplotlib's default. Further, these can be used interchangeably: `size` and `s`, `color` and `c`, `color_map` and `cmap`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:732,reliability,availab,available,732,"Tested scvelo's vs scanpy's `pl.scatter` (will share test modules + notebooks later). scvelo entails scanpy's functionality (and adds a couple extensions as explained above), except for the following:. - we don't have `left_margin` and `right_margin` attributes (don't think they're necessary). - point `size` convention acc. to scanpy (if settings._rcParams_style == 'scanpy') else slightly adjusted. - when `basis` is in `.var_names`, then an unspliced/spliced phase portrait is plotted (I guess that's not needed in scanpy?). Further we have some more defaults:. - if `basis` is None, then use a default basis in the given order if available: umap, tsne, pca. - if `color` is None, then use a default color in the given order if available: clusters, louvain. - if `frameon` is None, then only set frame if it is not embedding and axes values do matter. - if `color_map` is None, then use 'viridis_r' if vals in [0,1], else matplotlib's default. Further, these can be used interchangeably: `size` and `s`, `color` and `c`, `color_map` and `cmap`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:0,safety,Test,Tested,0,"Tested scvelo's vs scanpy's `pl.scatter` (will share test modules + notebooks later). scvelo entails scanpy's functionality (and adds a couple extensions as explained above), except for the following:. - we don't have `left_margin` and `right_margin` attributes (don't think they're necessary). - point `size` convention acc. to scanpy (if settings._rcParams_style == 'scanpy') else slightly adjusted. - when `basis` is in `.var_names`, then an unspliced/spliced phase portrait is plotted (I guess that's not needed in scanpy?). Further we have some more defaults:. - if `basis` is None, then use a default basis in the given order if available: umap, tsne, pca. - if `color` is None, then use a default color in the given order if available: clusters, louvain. - if `frameon` is None, then only set frame if it is not embedding and axes values do matter. - if `color_map` is None, then use 'viridis_r' if vals in [0,1], else matplotlib's default. Further, these can be used interchangeably: `size` and `s`, `color` and `c`, `color_map` and `cmap`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:53,safety,test,test,53,"Tested scvelo's vs scanpy's `pl.scatter` (will share test modules + notebooks later). scvelo entails scanpy's functionality (and adds a couple extensions as explained above), except for the following:. - we don't have `left_margin` and `right_margin` attributes (don't think they're necessary). - point `size` convention acc. to scanpy (if settings._rcParams_style == 'scanpy') else slightly adjusted. - when `basis` is in `.var_names`, then an unspliced/spliced phase portrait is plotted (I guess that's not needed in scanpy?). Further we have some more defaults:. - if `basis` is None, then use a default basis in the given order if available: umap, tsne, pca. - if `color` is None, then use a default color in the given order if available: clusters, louvain. - if `frameon` is None, then only set frame if it is not embedding and axes values do matter. - if `color_map` is None, then use 'viridis_r' if vals in [0,1], else matplotlib's default. Further, these can be used interchangeably: `size` and `s`, `color` and `c`, `color_map` and `cmap`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:58,safety,modul,modules,58,"Tested scvelo's vs scanpy's `pl.scatter` (will share test modules + notebooks later). scvelo entails scanpy's functionality (and adds a couple extensions as explained above), except for the following:. - we don't have `left_margin` and `right_margin` attributes (don't think they're necessary). - point `size` convention acc. to scanpy (if settings._rcParams_style == 'scanpy') else slightly adjusted. - when `basis` is in `.var_names`, then an unspliced/spliced phase portrait is plotted (I guess that's not needed in scanpy?). Further we have some more defaults:. - if `basis` is None, then use a default basis in the given order if available: umap, tsne, pca. - if `color` is None, then use a default color in the given order if available: clusters, louvain. - if `frameon` is None, then only set frame if it is not embedding and axes values do matter. - if `color_map` is None, then use 'viridis_r' if vals in [0,1], else matplotlib's default. Further, these can be used interchangeably: `size` and `s`, `color` and `c`, `color_map` and `cmap`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:175,safety,except,except,175,"Tested scvelo's vs scanpy's `pl.scatter` (will share test modules + notebooks later). scvelo entails scanpy's functionality (and adds a couple extensions as explained above), except for the following:. - we don't have `left_margin` and `right_margin` attributes (don't think they're necessary). - point `size` convention acc. to scanpy (if settings._rcParams_style == 'scanpy') else slightly adjusted. - when `basis` is in `.var_names`, then an unspliced/spliced phase portrait is plotted (I guess that's not needed in scanpy?). Further we have some more defaults:. - if `basis` is None, then use a default basis in the given order if available: umap, tsne, pca. - if `color` is None, then use a default color in the given order if available: clusters, louvain. - if `frameon` is None, then only set frame if it is not embedding and axes values do matter. - if `color_map` is None, then use 'viridis_r' if vals in [0,1], else matplotlib's default. Further, these can be used interchangeably: `size` and `s`, `color` and `c`, `color_map` and `cmap`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:635,safety,avail,available,635,"Tested scvelo's vs scanpy's `pl.scatter` (will share test modules + notebooks later). scvelo entails scanpy's functionality (and adds a couple extensions as explained above), except for the following:. - we don't have `left_margin` and `right_margin` attributes (don't think they're necessary). - point `size` convention acc. to scanpy (if settings._rcParams_style == 'scanpy') else slightly adjusted. - when `basis` is in `.var_names`, then an unspliced/spliced phase portrait is plotted (I guess that's not needed in scanpy?). Further we have some more defaults:. - if `basis` is None, then use a default basis in the given order if available: umap, tsne, pca. - if `color` is None, then use a default color in the given order if available: clusters, louvain. - if `frameon` is None, then only set frame if it is not embedding and axes values do matter. - if `color_map` is None, then use 'viridis_r' if vals in [0,1], else matplotlib's default. Further, these can be used interchangeably: `size` and `s`, `color` and `c`, `color_map` and `cmap`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:732,safety,avail,available,732,"Tested scvelo's vs scanpy's `pl.scatter` (will share test modules + notebooks later). scvelo entails scanpy's functionality (and adds a couple extensions as explained above), except for the following:. - we don't have `left_margin` and `right_margin` attributes (don't think they're necessary). - point `size` convention acc. to scanpy (if settings._rcParams_style == 'scanpy') else slightly adjusted. - when `basis` is in `.var_names`, then an unspliced/spliced phase portrait is plotted (I guess that's not needed in scanpy?). Further we have some more defaults:. - if `basis` is None, then use a default basis in the given order if available: umap, tsne, pca. - if `color` is None, then use a default color in the given order if available: clusters, louvain. - if `frameon` is None, then only set frame if it is not embedding and axes values do matter. - if `color_map` is None, then use 'viridis_r' if vals in [0,1], else matplotlib's default. Further, these can be used interchangeably: `size` and `s`, `color` and `c`, `color_map` and `cmap`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:635,security,availab,available,635,"Tested scvelo's vs scanpy's `pl.scatter` (will share test modules + notebooks later). scvelo entails scanpy's functionality (and adds a couple extensions as explained above), except for the following:. - we don't have `left_margin` and `right_margin` attributes (don't think they're necessary). - point `size` convention acc. to scanpy (if settings._rcParams_style == 'scanpy') else slightly adjusted. - when `basis` is in `.var_names`, then an unspliced/spliced phase portrait is plotted (I guess that's not needed in scanpy?). Further we have some more defaults:. - if `basis` is None, then use a default basis in the given order if available: umap, tsne, pca. - if `color` is None, then use a default color in the given order if available: clusters, louvain. - if `frameon` is None, then only set frame if it is not embedding and axes values do matter. - if `color_map` is None, then use 'viridis_r' if vals in [0,1], else matplotlib's default. Further, these can be used interchangeably: `size` and `s`, `color` and `c`, `color_map` and `cmap`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:732,security,availab,available,732,"Tested scvelo's vs scanpy's `pl.scatter` (will share test modules + notebooks later). scvelo entails scanpy's functionality (and adds a couple extensions as explained above), except for the following:. - we don't have `left_margin` and `right_margin` attributes (don't think they're necessary). - point `size` convention acc. to scanpy (if settings._rcParams_style == 'scanpy') else slightly adjusted. - when `basis` is in `.var_names`, then an unspliced/spliced phase portrait is plotted (I guess that's not needed in scanpy?). Further we have some more defaults:. - if `basis` is None, then use a default basis in the given order if available: umap, tsne, pca. - if `color` is None, then use a default color in the given order if available: clusters, louvain. - if `frameon` is None, then only set frame if it is not embedding and axes values do matter. - if `color_map` is None, then use 'viridis_r' if vals in [0,1], else matplotlib's default. Further, these can be used interchangeably: `size` and `s`, `color` and `c`, `color_map` and `cmap`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:0,testability,Test,Tested,0,"Tested scvelo's vs scanpy's `pl.scatter` (will share test modules + notebooks later). scvelo entails scanpy's functionality (and adds a couple extensions as explained above), except for the following:. - we don't have `left_margin` and `right_margin` attributes (don't think they're necessary). - point `size` convention acc. to scanpy (if settings._rcParams_style == 'scanpy') else slightly adjusted. - when `basis` is in `.var_names`, then an unspliced/spliced phase portrait is plotted (I guess that's not needed in scanpy?). Further we have some more defaults:. - if `basis` is None, then use a default basis in the given order if available: umap, tsne, pca. - if `color` is None, then use a default color in the given order if available: clusters, louvain. - if `frameon` is None, then only set frame if it is not embedding and axes values do matter. - if `color_map` is None, then use 'viridis_r' if vals in [0,1], else matplotlib's default. Further, these can be used interchangeably: `size` and `s`, `color` and `c`, `color_map` and `cmap`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:53,testability,test,test,53,"Tested scvelo's vs scanpy's `pl.scatter` (will share test modules + notebooks later). scvelo entails scanpy's functionality (and adds a couple extensions as explained above), except for the following:. - we don't have `left_margin` and `right_margin` attributes (don't think they're necessary). - point `size` convention acc. to scanpy (if settings._rcParams_style == 'scanpy') else slightly adjusted. - when `basis` is in `.var_names`, then an unspliced/spliced phase portrait is plotted (I guess that's not needed in scanpy?). Further we have some more defaults:. - if `basis` is None, then use a default basis in the given order if available: umap, tsne, pca. - if `color` is None, then use a default color in the given order if available: clusters, louvain. - if `frameon` is None, then only set frame if it is not embedding and axes values do matter. - if `color_map` is None, then use 'viridis_r' if vals in [0,1], else matplotlib's default. Further, these can be used interchangeably: `size` and `s`, `color` and `c`, `color_map` and `cmap`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:136,testability,coupl,couple,136,"Tested scvelo's vs scanpy's `pl.scatter` (will share test modules + notebooks later). scvelo entails scanpy's functionality (and adds a couple extensions as explained above), except for the following:. - we don't have `left_margin` and `right_margin` attributes (don't think they're necessary). - point `size` convention acc. to scanpy (if settings._rcParams_style == 'scanpy') else slightly adjusted. - when `basis` is in `.var_names`, then an unspliced/spliced phase portrait is plotted (I guess that's not needed in scanpy?). Further we have some more defaults:. - if `basis` is None, then use a default basis in the given order if available: umap, tsne, pca. - if `color` is None, then use a default color in the given order if available: clusters, louvain. - if `frameon` is None, then only set frame if it is not embedding and axes values do matter. - if `color_map` is None, then use 'viridis_r' if vals in [0,1], else matplotlib's default. Further, these can be used interchangeably: `size` and `s`, `color` and `c`, `color_map` and `cmap`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:297,integrability,compon,components,297,"@VolkerBergen on the top of my head here are some features that were added recently:. * adjustment of multiple plots distance using `wspace` and `hspace` (indeed I have never used `left_margin` and `right_margin` which I think are not necessary). * multiple plots by a combination of `color` and `components`. E.g. This will produce four plots: `sc.pl.pca(adata, color=['CD14', 'CD8'], components=['1,2', '2,3']` . * support for layers. * when the marker size given by the user is a vector, this is also subjected to any sorting of the cells (eg. `sort_order=True`, `groups=['<group name']` ). * added the option to outline the plot using the parameter `add_outline` #794 . * added vmin and vmax as percentiles #794 . * when using the `groups` parameter, plot those groups on top #891 .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:386,integrability,compon,components,386,"@VolkerBergen on the top of my head here are some features that were added recently:. * adjustment of multiple plots distance using `wspace` and `hspace` (indeed I have never used `left_margin` and `right_margin` which I think are not necessary). * multiple plots by a combination of `color` and `components`. E.g. This will produce four plots: `sc.pl.pca(adata, color=['CD14', 'CD8'], components=['1,2', '2,3']` . * support for layers. * when the marker size given by the user is a vector, this is also subjected to any sorting of the cells (eg. `sort_order=True`, `groups=['<group name']` ). * added the option to outline the plot using the parameter `add_outline` #794 . * added vmin and vmax as percentiles #794 . * when using the `groups` parameter, plot those groups on top #891 .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:504,integrability,sub,subjected,504,"@VolkerBergen on the top of my head here are some features that were added recently:. * adjustment of multiple plots distance using `wspace` and `hspace` (indeed I have never used `left_margin` and `right_margin` which I think are not necessary). * multiple plots by a combination of `color` and `components`. E.g. This will produce four plots: `sc.pl.pca(adata, color=['CD14', 'CD8'], components=['1,2', '2,3']` . * support for layers. * when the marker size given by the user is a vector, this is also subjected to any sorting of the cells (eg. `sort_order=True`, `groups=['<group name']` ). * added the option to outline the plot using the parameter `add_outline` #794 . * added vmin and vmax as percentiles #794 . * when using the `groups` parameter, plot those groups on top #891 .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:297,interoperability,compon,components,297,"@VolkerBergen on the top of my head here are some features that were added recently:. * adjustment of multiple plots distance using `wspace` and `hspace` (indeed I have never used `left_margin` and `right_margin` which I think are not necessary). * multiple plots by a combination of `color` and `components`. E.g. This will produce four plots: `sc.pl.pca(adata, color=['CD14', 'CD8'], components=['1,2', '2,3']` . * support for layers. * when the marker size given by the user is a vector, this is also subjected to any sorting of the cells (eg. `sort_order=True`, `groups=['<group name']` ). * added the option to outline the plot using the parameter `add_outline` #794 . * added vmin and vmax as percentiles #794 . * when using the `groups` parameter, plot those groups on top #891 .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:386,interoperability,compon,components,386,"@VolkerBergen on the top of my head here are some features that were added recently:. * adjustment of multiple plots distance using `wspace` and `hspace` (indeed I have never used `left_margin` and `right_margin` which I think are not necessary). * multiple plots by a combination of `color` and `components`. E.g. This will produce four plots: `sc.pl.pca(adata, color=['CD14', 'CD8'], components=['1,2', '2,3']` . * support for layers. * when the marker size given by the user is a vector, this is also subjected to any sorting of the cells (eg. `sort_order=True`, `groups=['<group name']` ). * added the option to outline the plot using the parameter `add_outline` #794 . * added vmin and vmax as percentiles #794 . * when using the `groups` parameter, plot those groups on top #891 .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:297,modifiability,compon,components,297,"@VolkerBergen on the top of my head here are some features that were added recently:. * adjustment of multiple plots distance using `wspace` and `hspace` (indeed I have never used `left_margin` and `right_margin` which I think are not necessary). * multiple plots by a combination of `color` and `components`. E.g. This will produce four plots: `sc.pl.pca(adata, color=['CD14', 'CD8'], components=['1,2', '2,3']` . * support for layers. * when the marker size given by the user is a vector, this is also subjected to any sorting of the cells (eg. `sort_order=True`, `groups=['<group name']` ). * added the option to outline the plot using the parameter `add_outline` #794 . * added vmin and vmax as percentiles #794 . * when using the `groups` parameter, plot those groups on top #891 .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:386,modifiability,compon,components,386,"@VolkerBergen on the top of my head here are some features that were added recently:. * adjustment of multiple plots distance using `wspace` and `hspace` (indeed I have never used `left_margin` and `right_margin` which I think are not necessary). * multiple plots by a combination of `color` and `components`. E.g. This will produce four plots: `sc.pl.pca(adata, color=['CD14', 'CD8'], components=['1,2', '2,3']` . * support for layers. * when the marker size given by the user is a vector, this is also subjected to any sorting of the cells (eg. `sort_order=True`, `groups=['<group name']` ). * added the option to outline the plot using the parameter `add_outline` #794 . * added vmin and vmax as percentiles #794 . * when using the `groups` parameter, plot those groups on top #891 .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:429,modifiability,layer,layers,429,"@VolkerBergen on the top of my head here are some features that were added recently:. * adjustment of multiple plots distance using `wspace` and `hspace` (indeed I have never used `left_margin` and `right_margin` which I think are not necessary). * multiple plots by a combination of `color` and `components`. E.g. This will produce four plots: `sc.pl.pca(adata, color=['CD14', 'CD8'], components=['1,2', '2,3']` . * support for layers. * when the marker size given by the user is a vector, this is also subjected to any sorting of the cells (eg. `sort_order=True`, `groups=['<group name']` ). * added the option to outline the plot using the parameter `add_outline` #794 . * added vmin and vmax as percentiles #794 . * when using the `groups` parameter, plot those groups on top #891 .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:643,modifiability,paramet,parameter,643,"@VolkerBergen on the top of my head here are some features that were added recently:. * adjustment of multiple plots distance using `wspace` and `hspace` (indeed I have never used `left_margin` and `right_margin` which I think are not necessary). * multiple plots by a combination of `color` and `components`. E.g. This will produce four plots: `sc.pl.pca(adata, color=['CD14', 'CD8'], components=['1,2', '2,3']` . * support for layers. * when the marker size given by the user is a vector, this is also subjected to any sorting of the cells (eg. `sort_order=True`, `groups=['<group name']` ). * added the option to outline the plot using the parameter `add_outline` #794 . * added vmin and vmax as percentiles #794 . * when using the `groups` parameter, plot those groups on top #891 .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:744,modifiability,paramet,parameter,744,"@VolkerBergen on the top of my head here are some features that were added recently:. * adjustment of multiple plots distance using `wspace` and `hspace` (indeed I have never used `left_margin` and `right_margin` which I think are not necessary). * multiple plots by a combination of `color` and `components`. E.g. This will produce four plots: `sc.pl.pca(adata, color=['CD14', 'CD8'], components=['1,2', '2,3']` . * support for layers. * when the marker size given by the user is a vector, this is also subjected to any sorting of the cells (eg. `sort_order=True`, `groups=['<group name']` ). * added the option to outline the plot using the parameter `add_outline` #794 . * added vmin and vmax as percentiles #794 . * when using the `groups` parameter, plot those groups on top #891 .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:417,usability,support,support,417,"@VolkerBergen on the top of my head here are some features that were added recently:. * adjustment of multiple plots distance using `wspace` and `hspace` (indeed I have never used `left_margin` and `right_margin` which I think are not necessary). * multiple plots by a combination of `color` and `components`. E.g. This will produce four plots: `sc.pl.pca(adata, color=['CD14', 'CD8'], components=['1,2', '2,3']` . * support for layers. * when the marker size given by the user is a vector, this is also subjected to any sorting of the cells (eg. `sort_order=True`, `groups=['<group name']` ). * added the option to outline the plot using the parameter `add_outline` #794 . * added vmin and vmax as percentiles #794 . * when using the `groups` parameter, plot those groups on top #891 .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/issues/617:473,usability,user,user,473,"@VolkerBergen on the top of my head here are some features that were added recently:. * adjustment of multiple plots distance using `wspace` and `hspace` (indeed I have never used `left_margin` and `right_margin` which I think are not necessary). * multiple plots by a combination of `color` and `components`. E.g. This will produce four plots: `sc.pl.pca(adata, color=['CD14', 'CD8'], components=['1,2', '2,3']` . * support for layers. * when the marker size given by the user is a vector, this is also subjected to any sorting of the cells (eg. `sort_order=True`, `groups=['<group name']` ). * added the option to outline the plot using the parameter `add_outline` #794 . * added vmin and vmax as percentiles #794 . * when using the `groups` parameter, plot those groups on top #891 .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/617
https://github.com/scverse/scanpy/pull/618:32,deployability,updat,update,32,This is great! ðŸ‘ . Can you also update `docs/release_notes.rst` and then simply merge?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/618
https://github.com/scverse/scanpy/pull/618:32,safety,updat,update,32,This is great! ðŸ‘ . Can you also update `docs/release_notes.rst` and then simply merge?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/618
https://github.com/scverse/scanpy/pull/618:32,security,updat,update,32,This is great! ðŸ‘ . Can you also update `docs/release_notes.rst` and then simply merge?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/618
https://github.com/scverse/scanpy/pull/618:73,testability,simpl,simply,73,This is great! ðŸ‘ . Can you also update `docs/release_notes.rst` and then simply merge?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/618
https://github.com/scverse/scanpy/pull/618:73,usability,simpl,simply,73,This is great! ðŸ‘ . Can you also update `docs/release_notes.rst` and then simply merge?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/618
https://github.com/scverse/scanpy/pull/619:13,deployability,updat,updated,13,"Thanks! I've updated the docs, but it turned out not much was actually shared. Where should I put these in the api docs? A new `utils` section, or something under `Further modules`? I'm thinking I'd just include `rank_genes_groups_df` and `obs_values_df` on the site.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/pull/619:111,deployability,api,api,111,"Thanks! I've updated the docs, but it turned out not much was actually shared. Where should I put these in the api docs? A new `utils` section, or something under `Further modules`? I'm thinking I'd just include `rank_genes_groups_df` and `obs_values_df` on the site.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/pull/619:172,deployability,modul,modules,172,"Thanks! I've updated the docs, but it turned out not much was actually shared. Where should I put these in the api docs? A new `utils` section, or something under `Further modules`? I'm thinking I'd just include `rank_genes_groups_df` and `obs_values_df` on the site.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/pull/619:111,integrability,api,api,111,"Thanks! I've updated the docs, but it turned out not much was actually shared. Where should I put these in the api docs? A new `utils` section, or something under `Further modules`? I'm thinking I'd just include `rank_genes_groups_df` and `obs_values_df` on the site.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/pull/619:71,interoperability,share,shared,71,"Thanks! I've updated the docs, but it turned out not much was actually shared. Where should I put these in the api docs? A new `utils` section, or something under `Further modules`? I'm thinking I'd just include `rank_genes_groups_df` and `obs_values_df` on the site.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/pull/619:111,interoperability,api,api,111,"Thanks! I've updated the docs, but it turned out not much was actually shared. Where should I put these in the api docs? A new `utils` section, or something under `Further modules`? I'm thinking I'd just include `rank_genes_groups_df` and `obs_values_df` on the site.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/pull/619:172,modifiability,modul,modules,172,"Thanks! I've updated the docs, but it turned out not much was actually shared. Where should I put these in the api docs? A new `utils` section, or something under `Further modules`? I'm thinking I'd just include `rank_genes_groups_df` and `obs_values_df` on the site.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/pull/619:13,safety,updat,updated,13,"Thanks! I've updated the docs, but it turned out not much was actually shared. Where should I put these in the api docs? A new `utils` section, or something under `Further modules`? I'm thinking I'd just include `rank_genes_groups_df` and `obs_values_df` on the site.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/pull/619:172,safety,modul,modules,172,"Thanks! I've updated the docs, but it turned out not much was actually shared. Where should I put these in the api docs? A new `utils` section, or something under `Further modules`? I'm thinking I'd just include `rank_genes_groups_df` and `obs_values_df` on the site.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/pull/619:13,security,updat,updated,13,"Thanks! I've updated the docs, but it turned out not much was actually shared. Where should I put these in the api docs? A new `utils` section, or something under `Further modules`? I'm thinking I'd just include `rank_genes_groups_df` and `obs_values_df` on the site.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/pull/619:36,deployability,depend,depend,36,"Since the `obs_values_df ` will now depend on an updated version of AnnData, I'm thinking I'll move this version of `rank_genes_groups_df` over to #467 so that can get merged. Edit: Actually, this isn't the case since we'll need backwards compatibility anyways, nvm",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/pull/619:49,deployability,updat,updated,49,"Since the `obs_values_df ` will now depend on an updated version of AnnData, I'm thinking I'll move this version of `rank_genes_groups_df` over to #467 so that can get merged. Edit: Actually, this isn't the case since we'll need backwards compatibility anyways, nvm",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/pull/619:57,deployability,version,version,57,"Since the `obs_values_df ` will now depend on an updated version of AnnData, I'm thinking I'll move this version of `rank_genes_groups_df` over to #467 so that can get merged. Edit: Actually, this isn't the case since we'll need backwards compatibility anyways, nvm",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/pull/619:105,deployability,version,version,105,"Since the `obs_values_df ` will now depend on an updated version of AnnData, I'm thinking I'll move this version of `rank_genes_groups_df` over to #467 so that can get merged. Edit: Actually, this isn't the case since we'll need backwards compatibility anyways, nvm",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/pull/619:36,integrability,depend,depend,36,"Since the `obs_values_df ` will now depend on an updated version of AnnData, I'm thinking I'll move this version of `rank_genes_groups_df` over to #467 so that can get merged. Edit: Actually, this isn't the case since we'll need backwards compatibility anyways, nvm",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/pull/619:57,integrability,version,version,57,"Since the `obs_values_df ` will now depend on an updated version of AnnData, I'm thinking I'll move this version of `rank_genes_groups_df` over to #467 so that can get merged. Edit: Actually, this isn't the case since we'll need backwards compatibility anyways, nvm",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/pull/619:105,integrability,version,version,105,"Since the `obs_values_df ` will now depend on an updated version of AnnData, I'm thinking I'll move this version of `rank_genes_groups_df` over to #467 so that can get merged. Edit: Actually, this isn't the case since we'll need backwards compatibility anyways, nvm",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/pull/619:239,interoperability,compatib,compatibility,239,"Since the `obs_values_df ` will now depend on an updated version of AnnData, I'm thinking I'll move this version of `rank_genes_groups_df` over to #467 so that can get merged. Edit: Actually, this isn't the case since we'll need backwards compatibility anyways, nvm",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/pull/619:36,modifiability,depend,depend,36,"Since the `obs_values_df ` will now depend on an updated version of AnnData, I'm thinking I'll move this version of `rank_genes_groups_df` over to #467 so that can get merged. Edit: Actually, this isn't the case since we'll need backwards compatibility anyways, nvm",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/pull/619:57,modifiability,version,version,57,"Since the `obs_values_df ` will now depend on an updated version of AnnData, I'm thinking I'll move this version of `rank_genes_groups_df` over to #467 so that can get merged. Edit: Actually, this isn't the case since we'll need backwards compatibility anyways, nvm",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/pull/619:105,modifiability,version,version,105,"Since the `obs_values_df ` will now depend on an updated version of AnnData, I'm thinking I'll move this version of `rank_genes_groups_df` over to #467 so that can get merged. Edit: Actually, this isn't the case since we'll need backwards compatibility anyways, nvm",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/pull/619:36,safety,depend,depend,36,"Since the `obs_values_df ` will now depend on an updated version of AnnData, I'm thinking I'll move this version of `rank_genes_groups_df` over to #467 so that can get merged. Edit: Actually, this isn't the case since we'll need backwards compatibility anyways, nvm",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/pull/619:49,safety,updat,updated,49,"Since the `obs_values_df ` will now depend on an updated version of AnnData, I'm thinking I'll move this version of `rank_genes_groups_df` over to #467 so that can get merged. Edit: Actually, this isn't the case since we'll need backwards compatibility anyways, nvm",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/pull/619:49,security,updat,updated,49,"Since the `obs_values_df ` will now depend on an updated version of AnnData, I'm thinking I'll move this version of `rank_genes_groups_df` over to #467 so that can get merged. Edit: Actually, this isn't the case since we'll need backwards compatibility anyways, nvm",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/pull/619:36,testability,depend,depend,36,"Since the `obs_values_df ` will now depend on an updated version of AnnData, I'm thinking I'll move this version of `rank_genes_groups_df` over to #467 so that can get merged. Edit: Actually, this isn't the case since we'll need backwards compatibility anyways, nvm",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/pull/619:0,reliability,Doe,Does,0,"Does `obs_values_df` need the `_df`, or could it be `obs_values`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/pull/619:132,deployability,observ,observations,132,"I think `obs_values` is fine. But maybe, `aggregate_obs` is even better, as this describes what it does (aggregating annotations of observations with partial (projections of) observations). It's no problem at all to make the next Scanpy release depend on the current AnnData release, both in the requirements and the minimal version check upon importing Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/pull/619:175,deployability,observ,observations,175,"I think `obs_values` is fine. But maybe, `aggregate_obs` is even better, as this describes what it does (aggregating annotations of observations with partial (projections of) observations). It's no problem at all to make the next Scanpy release depend on the current AnnData release, both in the requirements and the minimal version check upon importing Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/pull/619:237,deployability,releas,release,237,"I think `obs_values` is fine. But maybe, `aggregate_obs` is even better, as this describes what it does (aggregating annotations of observations with partial (projections of) observations). It's no problem at all to make the next Scanpy release depend on the current AnnData release, both in the requirements and the minimal version check upon importing Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/pull/619:245,deployability,depend,depend,245,"I think `obs_values` is fine. But maybe, `aggregate_obs` is even better, as this describes what it does (aggregating annotations of observations with partial (projections of) observations). It's no problem at all to make the next Scanpy release depend on the current AnnData release, both in the requirements and the minimal version check upon importing Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/pull/619:275,deployability,releas,release,275,"I think `obs_values` is fine. But maybe, `aggregate_obs` is even better, as this describes what it does (aggregating annotations of observations with partial (projections of) observations). It's no problem at all to make the next Scanpy release depend on the current AnnData release, both in the requirements and the minimal version check upon importing Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/pull/619:325,deployability,version,version,325,"I think `obs_values` is fine. But maybe, `aggregate_obs` is even better, as this describes what it does (aggregating annotations of observations with partial (projections of) observations). It's no problem at all to make the next Scanpy release depend on the current AnnData release, both in the requirements and the minimal version check upon importing Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/pull/619:259,energy efficiency,current,current,259,"I think `obs_values` is fine. But maybe, `aggregate_obs` is even better, as this describes what it does (aggregating annotations of observations with partial (projections of) observations). It's no problem at all to make the next Scanpy release depend on the current AnnData release, both in the requirements and the minimal version check upon importing Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/pull/619:245,integrability,depend,depend,245,"I think `obs_values` is fine. But maybe, `aggregate_obs` is even better, as this describes what it does (aggregating annotations of observations with partial (projections of) observations). It's no problem at all to make the next Scanpy release depend on the current AnnData release, both in the requirements and the minimal version check upon importing Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/pull/619:325,integrability,version,version,325,"I think `obs_values` is fine. But maybe, `aggregate_obs` is even better, as this describes what it does (aggregating annotations of observations with partial (projections of) observations). It's no problem at all to make the next Scanpy release depend on the current AnnData release, both in the requirements and the minimal version check upon importing Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/pull/619:245,modifiability,depend,depend,245,"I think `obs_values` is fine. But maybe, `aggregate_obs` is even better, as this describes what it does (aggregating annotations of observations with partial (projections of) observations). It's no problem at all to make the next Scanpy release depend on the current AnnData release, both in the requirements and the minimal version check upon importing Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/pull/619:325,modifiability,version,version,325,"I think `obs_values` is fine. But maybe, `aggregate_obs` is even better, as this describes what it does (aggregating annotations of observations with partial (projections of) observations). It's no problem at all to make the next Scanpy release depend on the current AnnData release, both in the requirements and the minimal version check upon importing Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/pull/619:99,reliability,doe,does,99,"I think `obs_values` is fine. But maybe, `aggregate_obs` is even better, as this describes what it does (aggregating annotations of observations with partial (projections of) observations). It's no problem at all to make the next Scanpy release depend on the current AnnData release, both in the requirements and the minimal version check upon importing Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/pull/619:245,safety,depend,depend,245,"I think `obs_values` is fine. But maybe, `aggregate_obs` is even better, as this describes what it does (aggregating annotations of observations with partial (projections of) observations). It's no problem at all to make the next Scanpy release depend on the current AnnData release, both in the requirements and the minimal version check upon importing Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/pull/619:132,testability,observ,observations,132,"I think `obs_values` is fine. But maybe, `aggregate_obs` is even better, as this describes what it does (aggregating annotations of observations with partial (projections of) observations). It's no problem at all to make the next Scanpy release depend on the current AnnData release, both in the requirements and the minimal version check upon importing Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/pull/619:175,testability,observ,observations,175,"I think `obs_values` is fine. But maybe, `aggregate_obs` is even better, as this describes what it does (aggregating annotations of observations with partial (projections of) observations). It's no problem at all to make the next Scanpy release depend on the current AnnData release, both in the requirements and the minimal version check upon importing Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/pull/619:245,testability,depend,depend,245,"I think `obs_values` is fine. But maybe, `aggregate_obs` is even better, as this describes what it does (aggregating annotations of observations with partial (projections of) observations). It's no problem at all to make the next Scanpy release depend on the current AnnData release, both in the requirements and the minimal version check upon importing Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/pull/619:317,usability,minim,minimal,317,"I think `obs_values` is fine. But maybe, `aggregate_obs` is even better, as this describes what it does (aggregating annotations of observations with partial (projections of) observations). It's no problem at all to make the next Scanpy release depend on the current AnnData release, both in the requirements and the minimal version check upon importing Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/pull/619:33,energy efficiency,reduc,reduction,33,"To me, `aggregate_obs` implies a reduction, but that might mostly be because `pd.DataFrame.aggregate`. I could go for `collect_obs`, but if this goes into `sc.get` there's already a verb. I think `sc.get.obs_df` would work (though kind of ambiguous with `adata.obs`) as would `sc.get.obs_values`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/pull/619:101,deployability,releas,release,101,Kinda? This is waiting on https://github.com/theislab/anndata/pull/144 and a following AnnData point release. But I think that PR is ready to go.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/pull/619:49,usability,close,close,49,"Getting back around to this, I think it's pretty close to ready. Two last things to consider:. * Name change of `obs_values_df`. I like`obs_df` since it fits with `obs_vector` and has a nice short name. * Support for `.raw`, I've dropped it at the moment, but maybe should add it back in.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/pull/619:205,usability,Support,Support,205,"Getting back around to this, I think it's pretty close to ready. Two last things to consider:. * Name change of `obs_values_df`. I like`obs_df` since it fits with `obs_vector` and has a nice short name. * Support for `.raw`, I've dropped it at the moment, but maybe should add it back in.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/pull/619:59,deployability,releas,release,59,"The docs look great! I just wonder about the above: In the release notes, we refer to everything as `scanpy.*`, not `sc.*`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/pull/619:57,deployability,modul,module,57,Do we have some kind of tutorial around the new `sc.get` module?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/pull/619:57,modifiability,modul,module,57,Do we have some kind of tutorial around the new `sc.get` module?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/pull/619:57,safety,modul,module,57,Do we have some kind of tutorial around the new `sc.get` module?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/619
https://github.com/scverse/scanpy/issues/620:376,availability,error,error,376,"This seems to have something to do with not being able to estimate the variance within a group. If you add thes lines:. ```. adata.X[0,:] = np.array([1.,1.,1.]). adata.X[11,:] = np.array([1.,1.,1.]). ```. to your data, then it works as expected. I assume this is due to `NaN` being set to 1 in the p-value calculation. The t-test isn't defined in this case. Not sure what the error for `""logreg""` is though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/620
https://github.com/scverse/scanpy/issues/620:388,deployability,log,logreg,388,"This seems to have something to do with not being able to estimate the variance within a group. If you add thes lines:. ```. adata.X[0,:] = np.array([1.,1.,1.]). adata.X[11,:] = np.array([1.,1.,1.]). ```. to your data, then it works as expected. I assume this is due to `NaN` being set to 1 in the p-value calculation. The t-test isn't defined in this case. Not sure what the error for `""logreg""` is though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/620
https://github.com/scverse/scanpy/issues/620:58,energy efficiency,estimat,estimate,58,"This seems to have something to do with not being able to estimate the variance within a group. If you add thes lines:. ```. adata.X[0,:] = np.array([1.,1.,1.]). adata.X[11,:] = np.array([1.,1.,1.]). ```. to your data, then it works as expected. I assume this is due to `NaN` being set to 1 in the p-value calculation. The t-test isn't defined in this case. Not sure what the error for `""logreg""` is though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/620
https://github.com/scverse/scanpy/issues/620:376,performance,error,error,376,"This seems to have something to do with not being able to estimate the variance within a group. If you add thes lines:. ```. adata.X[0,:] = np.array([1.,1.,1.]). adata.X[11,:] = np.array([1.,1.,1.]). ```. to your data, then it works as expected. I assume this is due to `NaN` being set to 1 in the p-value calculation. The t-test isn't defined in this case. Not sure what the error for `""logreg""` is though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/620
https://github.com/scverse/scanpy/issues/620:325,safety,test,test,325,"This seems to have something to do with not being able to estimate the variance within a group. If you add thes lines:. ```. adata.X[0,:] = np.array([1.,1.,1.]). adata.X[11,:] = np.array([1.,1.,1.]). ```. to your data, then it works as expected. I assume this is due to `NaN` being set to 1 in the p-value calculation. The t-test isn't defined in this case. Not sure what the error for `""logreg""` is though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/620
https://github.com/scverse/scanpy/issues/620:376,safety,error,error,376,"This seems to have something to do with not being able to estimate the variance within a group. If you add thes lines:. ```. adata.X[0,:] = np.array([1.,1.,1.]). adata.X[11,:] = np.array([1.,1.,1.]). ```. to your data, then it works as expected. I assume this is due to `NaN` being set to 1 in the p-value calculation. The t-test isn't defined in this case. Not sure what the error for `""logreg""` is though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/620
https://github.com/scverse/scanpy/issues/620:388,safety,log,logreg,388,"This seems to have something to do with not being able to estimate the variance within a group. If you add thes lines:. ```. adata.X[0,:] = np.array([1.,1.,1.]). adata.X[11,:] = np.array([1.,1.,1.]). ```. to your data, then it works as expected. I assume this is due to `NaN` being set to 1 in the p-value calculation. The t-test isn't defined in this case. Not sure what the error for `""logreg""` is though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/620
https://github.com/scverse/scanpy/issues/620:388,security,log,logreg,388,"This seems to have something to do with not being able to estimate the variance within a group. If you add thes lines:. ```. adata.X[0,:] = np.array([1.,1.,1.]). adata.X[11,:] = np.array([1.,1.,1.]). ```. to your data, then it works as expected. I assume this is due to `NaN` being set to 1 in the p-value calculation. The t-test isn't defined in this case. Not sure what the error for `""logreg""` is though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/620
https://github.com/scverse/scanpy/issues/620:325,testability,test,test,325,"This seems to have something to do with not being able to estimate the variance within a group. If you add thes lines:. ```. adata.X[0,:] = np.array([1.,1.,1.]). adata.X[11,:] = np.array([1.,1.,1.]). ```. to your data, then it works as expected. I assume this is due to `NaN` being set to 1 in the p-value calculation. The t-test isn't defined in this case. Not sure what the error for `""logreg""` is though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/620
https://github.com/scverse/scanpy/issues/620:388,testability,log,logreg,388,"This seems to have something to do with not being able to estimate the variance within a group. If you add thes lines:. ```. adata.X[0,:] = np.array([1.,1.,1.]). adata.X[11,:] = np.array([1.,1.,1.]). ```. to your data, then it works as expected. I assume this is due to `NaN` being set to 1 in the p-value calculation. The t-test isn't defined in this case. Not sure what the error for `""logreg""` is though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/620
https://github.com/scverse/scanpy/issues/620:376,usability,error,error,376,"This seems to have something to do with not being able to estimate the variance within a group. If you add thes lines:. ```. adata.X[0,:] = np.array([1.,1.,1.]). adata.X[11,:] = np.array([1.,1.,1.]). ```. to your data, then it works as expected. I assume this is due to `NaN` being set to 1 in the p-value calculation. The t-test isn't defined in this case. Not sure what the error for `""logreg""` is though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/620
https://github.com/scverse/scanpy/issues/620:52,interoperability,standard,standard,52,Btw. I think scanpy uses Welch's t-test and not the standard t-test. So the comparison with `stats.ttest_ind` is not entirely correct. I guess `stats.ttest_ind` calculates the asymptote of the statistic (`-inf`) and uses that to give a p-value of 0.0.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/620
https://github.com/scverse/scanpy/issues/620:35,safety,test,test,35,Btw. I think scanpy uses Welch's t-test and not the standard t-test. So the comparison with `stats.ttest_ind` is not entirely correct. I guess `stats.ttest_ind` calculates the asymptote of the statistic (`-inf`) and uses that to give a p-value of 0.0.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/620
https://github.com/scverse/scanpy/issues/620:63,safety,test,test,63,Btw. I think scanpy uses Welch's t-test and not the standard t-test. So the comparison with `stats.ttest_ind` is not entirely correct. I guess `stats.ttest_ind` calculates the asymptote of the statistic (`-inf`) and uses that to give a p-value of 0.0.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/620
https://github.com/scverse/scanpy/issues/620:35,testability,test,test,35,Btw. I think scanpy uses Welch's t-test and not the standard t-test. So the comparison with `stats.ttest_ind` is not entirely correct. I guess `stats.ttest_ind` calculates the asymptote of the statistic (`-inf`) and uses that to give a p-value of 0.0.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/620
https://github.com/scverse/scanpy/issues/620:63,testability,test,test,63,Btw. I think scanpy uses Welch's t-test and not the standard t-test. So the comparison with `stats.ttest_ind` is not entirely correct. I guess `stats.ttest_ind` calculates the asymptote of the statistic (`-inf`) and uses that to give a p-value of 0.0.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/620
https://github.com/scverse/scanpy/issues/620:299,energy efficiency,current,currently,299,"I don't think it should matter whether it's Welch's in this case, since the population variances are both zero, right? Anyways, scipy's Welch's gives the same result:. ```python. print(stats.ttest_ind([0,0,0,0,0], [5,5,5,5,5], equal_var=False)) . Ttest_indResult(statistic=-inf, pvalue=0.0). ```. I currently suspect this bit of code for the bug:. https://github.com/theislab/scanpy/blob/00471636b050bf42ef41cc9ba0d45783de6b32e6/scanpy/tools/_rank_genes_groups.py#L198-L201",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/620
https://github.com/scverse/scanpy/issues/620:436,usability,tool,tools,436,"I don't think it should matter whether it's Welch's in this case, since the population variances are both zero, right? Anyways, scipy's Welch's gives the same result:. ```python. print(stats.ttest_ind([0,0,0,0,0], [5,5,5,5,5], equal_var=False)) . Ttest_indResult(statistic=-inf, pvalue=0.0). ```. I currently suspect this bit of code for the bug:. https://github.com/theislab/scanpy/blob/00471636b050bf42ef41cc9ba0d45783de6b32e6/scanpy/tools/_rank_genes_groups.py#L198-L201",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/620
https://github.com/scverse/scanpy/issues/620:26,integrability,topic,topic,26,"It doesn't matter for the topic of this discussion indeed. I reckon line 201 is the cause. If you want to solve this as scipy does, you will probably have to test for 0 variance and then assign a `-Inf` score, which defaults to a p-value of 0. I wonder if you get spurious marker gene results then though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/620
https://github.com/scverse/scanpy/issues/620:3,reliability,doe,doesn,3,"It doesn't matter for the topic of this discussion indeed. I reckon line 201 is the cause. If you want to solve this as scipy does, you will probably have to test for 0 variance and then assign a `-Inf` score, which defaults to a p-value of 0. I wonder if you get spurious marker gene results then though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/620
https://github.com/scverse/scanpy/issues/620:126,reliability,doe,does,126,"It doesn't matter for the topic of this discussion indeed. I reckon line 201 is the cause. If you want to solve this as scipy does, you will probably have to test for 0 variance and then assign a `-Inf` score, which defaults to a p-value of 0. I wonder if you get spurious marker gene results then though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/620
https://github.com/scverse/scanpy/issues/620:158,safety,test,test,158,"It doesn't matter for the topic of this discussion indeed. I reckon line 201 is the cause. If you want to solve this as scipy does, you will probably have to test for 0 variance and then assign a `-Inf` score, which defaults to a p-value of 0. I wonder if you get spurious marker gene results then though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/620
https://github.com/scverse/scanpy/issues/620:158,testability,test,test,158,"It doesn't matter for the topic of this discussion indeed. I reckon line 201 is the cause. If you want to solve this as scipy does, you will probably have to test for 0 variance and then assign a `-Inf` score, which defaults to a p-value of 0. I wonder if you get spurious marker gene results then though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/620
https://github.com/scverse/scanpy/issues/620:22,reliability,doe,does,22,Turns out `np.divide` does the reasonable thing. But also this appears to only be the start of the problem.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/620
https://github.com/scverse/scanpy/pull/621:15,safety,review,review,15,"Someone should review this before it's merged. I think this won't cause any problems, but I'm also not too familiar with the DE code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/621
https://github.com/scverse/scanpy/pull/621:15,testability,review,review,15,"Someone should review this before it's merged. I think this won't cause any problems, but I'm also not too familiar with the DE code.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/621
https://github.com/scverse/scanpy/pull/621:106,availability,slo,slower,106,I wonder why this wasn't done in the first place. Is scipy not already a dependency of scanpy? Or is this slower than the initial implementation?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/621
https://github.com/scverse/scanpy/pull/621:73,deployability,depend,dependency,73,I wonder why this wasn't done in the first place. Is scipy not already a dependency of scanpy? Or is this slower than the initial implementation?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/621
https://github.com/scverse/scanpy/pull/621:73,integrability,depend,dependency,73,I wonder why this wasn't done in the first place. Is scipy not already a dependency of scanpy? Or is this slower than the initial implementation?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/621
https://github.com/scverse/scanpy/pull/621:73,modifiability,depend,dependency,73,I wonder why this wasn't done in the first place. Is scipy not already a dependency of scanpy? Or is this slower than the initial implementation?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/621
https://github.com/scverse/scanpy/pull/621:106,reliability,slo,slower,106,I wonder why this wasn't done in the first place. Is scipy not already a dependency of scanpy? Or is this slower than the initial implementation?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/621
https://github.com/scverse/scanpy/pull/621:73,safety,depend,dependency,73,I wonder why this wasn't done in the first place. Is scipy not already a dependency of scanpy? Or is this slower than the initial implementation?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/621
https://github.com/scverse/scanpy/pull/621:73,testability,depend,dependency,73,I wonder why this wasn't done in the first place. Is scipy not already a dependency of scanpy? Or is this slower than the initial implementation?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/621
https://github.com/scverse/scanpy/pull/621:53,deployability,version,version,53,"Looks great! I wasn't aware of this high-dimensional version of a t-test in Scipy, which seems to be as efficient as the current implementation. I only investigated thoroughly for Wilcoxon rank and found that Scipy doesn't have a scalable version to offer. But yes, this will get merged after 1.4.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/621
https://github.com/scverse/scanpy/pull/621:239,deployability,version,version,239,"Looks great! I wasn't aware of this high-dimensional version of a t-test in Scipy, which seems to be as efficient as the current implementation. I only investigated thoroughly for Wilcoxon rank and found that Scipy doesn't have a scalable version to offer. But yes, this will get merged after 1.4.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/621
https://github.com/scverse/scanpy/pull/621:121,energy efficiency,current,current,121,"Looks great! I wasn't aware of this high-dimensional version of a t-test in Scipy, which seems to be as efficient as the current implementation. I only investigated thoroughly for Wilcoxon rank and found that Scipy doesn't have a scalable version to offer. But yes, this will get merged after 1.4.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/621
https://github.com/scverse/scanpy/pull/621:53,integrability,version,version,53,"Looks great! I wasn't aware of this high-dimensional version of a t-test in Scipy, which seems to be as efficient as the current implementation. I only investigated thoroughly for Wilcoxon rank and found that Scipy doesn't have a scalable version to offer. But yes, this will get merged after 1.4.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/621
https://github.com/scverse/scanpy/pull/621:239,integrability,version,version,239,"Looks great! I wasn't aware of this high-dimensional version of a t-test in Scipy, which seems to be as efficient as the current implementation. I only investigated thoroughly for Wilcoxon rank and found that Scipy doesn't have a scalable version to offer. But yes, this will get merged after 1.4.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/621
https://github.com/scverse/scanpy/pull/621:53,modifiability,version,version,53,"Looks great! I wasn't aware of this high-dimensional version of a t-test in Scipy, which seems to be as efficient as the current implementation. I only investigated thoroughly for Wilcoxon rank and found that Scipy doesn't have a scalable version to offer. But yes, this will get merged after 1.4.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/621
https://github.com/scverse/scanpy/pull/621:230,modifiability,scal,scalable,230,"Looks great! I wasn't aware of this high-dimensional version of a t-test in Scipy, which seems to be as efficient as the current implementation. I only investigated thoroughly for Wilcoxon rank and found that Scipy doesn't have a scalable version to offer. But yes, this will get merged after 1.4.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/621
https://github.com/scverse/scanpy/pull/621:239,modifiability,version,version,239,"Looks great! I wasn't aware of this high-dimensional version of a t-test in Scipy, which seems to be as efficient as the current implementation. I only investigated thoroughly for Wilcoxon rank and found that Scipy doesn't have a scalable version to offer. But yes, this will get merged after 1.4.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/621
https://github.com/scverse/scanpy/pull/621:230,performance,scalab,scalable,230,"Looks great! I wasn't aware of this high-dimensional version of a t-test in Scipy, which seems to be as efficient as the current implementation. I only investigated thoroughly for Wilcoxon rank and found that Scipy doesn't have a scalable version to offer. But yes, this will get merged after 1.4.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/621
https://github.com/scverse/scanpy/pull/621:215,reliability,doe,doesn,215,"Looks great! I wasn't aware of this high-dimensional version of a t-test in Scipy, which seems to be as efficient as the current implementation. I only investigated thoroughly for Wilcoxon rank and found that Scipy doesn't have a scalable version to offer. But yes, this will get merged after 1.4.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/621
https://github.com/scverse/scanpy/pull/621:68,safety,test,test,68,"Looks great! I wasn't aware of this high-dimensional version of a t-test in Scipy, which seems to be as efficient as the current implementation. I only investigated thoroughly for Wilcoxon rank and found that Scipy doesn't have a scalable version to offer. But yes, this will get merged after 1.4.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/621
https://github.com/scverse/scanpy/pull/621:68,testability,test,test,68,"Looks great! I wasn't aware of this high-dimensional version of a t-test in Scipy, which seems to be as efficient as the current implementation. I only investigated thoroughly for Wilcoxon rank and found that Scipy doesn't have a scalable version to offer. But yes, this will get merged after 1.4.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/621
https://github.com/scverse/scanpy/pull/621:104,usability,efficien,efficient,104,"Looks great! I wasn't aware of this high-dimensional version of a t-test in Scipy, which seems to be as efficient as the current implementation. I only investigated thoroughly for Wilcoxon rank and found that Scipy doesn't have a scalable version to offer. But yes, this will get merged after 1.4.1.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/621
https://github.com/scverse/scanpy/pull/621:37,modifiability,scal,scalable,37,"1.4.1 is out, are we sure this is as scalable as it was before and not a backwards breaking change. If yes, we can merge immediately.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/621
https://github.com/scverse/scanpy/pull/621:37,performance,scalab,scalable,37,"1.4.1 is out, are we sure this is as scalable as it was before and not a backwards breaking change. If yes, we can merge immediately.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/621
https://github.com/scverse/scanpy/pull/621:193,availability,sli,slightly,193,"I'm not completely sure this doesn't break anything, but the regression tests pass. The internal code is very similar, so I'm not too worried about these changes. It does look like it's (very) slightly slower. Running this a thousand times for pbmc68k dataset took ~2.3% longer (about 1.4 ms per run) than the previous version. That said, we're very inefficient about mean and variance calculation, so I think that's a better place to optimize. Edit: I've force pushed to fix some minor formatting issues (trailing white space, blank line, typo) that I didn't think deserved it's own commit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/621
https://github.com/scverse/scanpy/pull/621:202,availability,slo,slower,202,"I'm not completely sure this doesn't break anything, but the regression tests pass. The internal code is very similar, so I'm not too worried about these changes. It does look like it's (very) slightly slower. Running this a thousand times for pbmc68k dataset took ~2.3% longer (about 1.4 ms per run) than the previous version. That said, we're very inefficient about mean and variance calculation, so I think that's a better place to optimize. Edit: I've force pushed to fix some minor formatting issues (trailing white space, blank line, typo) that I didn't think deserved it's own commit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/621
https://github.com/scverse/scanpy/pull/621:319,deployability,version,version,319,"I'm not completely sure this doesn't break anything, but the regression tests pass. The internal code is very similar, so I'm not too worried about these changes. It does look like it's (very) slightly slower. Running this a thousand times for pbmc68k dataset took ~2.3% longer (about 1.4 ms per run) than the previous version. That said, we're very inefficient about mean and variance calculation, so I think that's a better place to optimize. Edit: I've force pushed to fix some minor formatting issues (trailing white space, blank line, typo) that I didn't think deserved it's own commit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/621
https://github.com/scverse/scanpy/pull/621:435,energy efficiency,optim,optimize,435,"I'm not completely sure this doesn't break anything, but the regression tests pass. The internal code is very similar, so I'm not too worried about these changes. It does look like it's (very) slightly slower. Running this a thousand times for pbmc68k dataset took ~2.3% longer (about 1.4 ms per run) than the previous version. That said, we're very inefficient about mean and variance calculation, so I think that's a better place to optimize. Edit: I've force pushed to fix some minor formatting issues (trailing white space, blank line, typo) that I didn't think deserved it's own commit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/621
https://github.com/scverse/scanpy/pull/621:319,integrability,version,version,319,"I'm not completely sure this doesn't break anything, but the regression tests pass. The internal code is very similar, so I'm not too worried about these changes. It does look like it's (very) slightly slower. Running this a thousand times for pbmc68k dataset took ~2.3% longer (about 1.4 ms per run) than the previous version. That said, we're very inefficient about mean and variance calculation, so I think that's a better place to optimize. Edit: I've force pushed to fix some minor formatting issues (trailing white space, blank line, typo) that I didn't think deserved it's own commit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/621
https://github.com/scverse/scanpy/pull/621:487,interoperability,format,formatting,487,"I'm not completely sure this doesn't break anything, but the regression tests pass. The internal code is very similar, so I'm not too worried about these changes. It does look like it's (very) slightly slower. Running this a thousand times for pbmc68k dataset took ~2.3% longer (about 1.4 ms per run) than the previous version. That said, we're very inefficient about mean and variance calculation, so I think that's a better place to optimize. Edit: I've force pushed to fix some minor formatting issues (trailing white space, blank line, typo) that I didn't think deserved it's own commit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/621
https://github.com/scverse/scanpy/pull/621:319,modifiability,version,version,319,"I'm not completely sure this doesn't break anything, but the regression tests pass. The internal code is very similar, so I'm not too worried about these changes. It does look like it's (very) slightly slower. Running this a thousand times for pbmc68k dataset took ~2.3% longer (about 1.4 ms per run) than the previous version. That said, we're very inefficient about mean and variance calculation, so I think that's a better place to optimize. Edit: I've force pushed to fix some minor formatting issues (trailing white space, blank line, typo) that I didn't think deserved it's own commit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/621
https://github.com/scverse/scanpy/pull/621:234,performance,time,times,234,"I'm not completely sure this doesn't break anything, but the regression tests pass. The internal code is very similar, so I'm not too worried about these changes. It does look like it's (very) slightly slower. Running this a thousand times for pbmc68k dataset took ~2.3% longer (about 1.4 ms per run) than the previous version. That said, we're very inefficient about mean and variance calculation, so I think that's a better place to optimize. Edit: I've force pushed to fix some minor formatting issues (trailing white space, blank line, typo) that I didn't think deserved it's own commit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/621
https://github.com/scverse/scanpy/pull/621:435,performance,optimiz,optimize,435,"I'm not completely sure this doesn't break anything, but the regression tests pass. The internal code is very similar, so I'm not too worried about these changes. It does look like it's (very) slightly slower. Running this a thousand times for pbmc68k dataset took ~2.3% longer (about 1.4 ms per run) than the previous version. That said, we're very inefficient about mean and variance calculation, so I think that's a better place to optimize. Edit: I've force pushed to fix some minor formatting issues (trailing white space, blank line, typo) that I didn't think deserved it's own commit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/621
https://github.com/scverse/scanpy/pull/621:29,reliability,doe,doesn,29,"I'm not completely sure this doesn't break anything, but the regression tests pass. The internal code is very similar, so I'm not too worried about these changes. It does look like it's (very) slightly slower. Running this a thousand times for pbmc68k dataset took ~2.3% longer (about 1.4 ms per run) than the previous version. That said, we're very inefficient about mean and variance calculation, so I think that's a better place to optimize. Edit: I've force pushed to fix some minor formatting issues (trailing white space, blank line, typo) that I didn't think deserved it's own commit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/621
https://github.com/scverse/scanpy/pull/621:166,reliability,doe,does,166,"I'm not completely sure this doesn't break anything, but the regression tests pass. The internal code is very similar, so I'm not too worried about these changes. It does look like it's (very) slightly slower. Running this a thousand times for pbmc68k dataset took ~2.3% longer (about 1.4 ms per run) than the previous version. That said, we're very inefficient about mean and variance calculation, so I think that's a better place to optimize. Edit: I've force pushed to fix some minor formatting issues (trailing white space, blank line, typo) that I didn't think deserved it's own commit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/621
https://github.com/scverse/scanpy/pull/621:193,reliability,sli,slightly,193,"I'm not completely sure this doesn't break anything, but the regression tests pass. The internal code is very similar, so I'm not too worried about these changes. It does look like it's (very) slightly slower. Running this a thousand times for pbmc68k dataset took ~2.3% longer (about 1.4 ms per run) than the previous version. That said, we're very inefficient about mean and variance calculation, so I think that's a better place to optimize. Edit: I've force pushed to fix some minor formatting issues (trailing white space, blank line, typo) that I didn't think deserved it's own commit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/621
https://github.com/scverse/scanpy/pull/621:202,reliability,slo,slower,202,"I'm not completely sure this doesn't break anything, but the regression tests pass. The internal code is very similar, so I'm not too worried about these changes. It does look like it's (very) slightly slower. Running this a thousand times for pbmc68k dataset took ~2.3% longer (about 1.4 ms per run) than the previous version. That said, we're very inefficient about mean and variance calculation, so I think that's a better place to optimize. Edit: I've force pushed to fix some minor formatting issues (trailing white space, blank line, typo) that I didn't think deserved it's own commit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/621
https://github.com/scverse/scanpy/pull/621:8,safety,compl,completely,8,"I'm not completely sure this doesn't break anything, but the regression tests pass. The internal code is very similar, so I'm not too worried about these changes. It does look like it's (very) slightly slower. Running this a thousand times for pbmc68k dataset took ~2.3% longer (about 1.4 ms per run) than the previous version. That said, we're very inefficient about mean and variance calculation, so I think that's a better place to optimize. Edit: I've force pushed to fix some minor formatting issues (trailing white space, blank line, typo) that I didn't think deserved it's own commit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/621
https://github.com/scverse/scanpy/pull/621:72,safety,test,tests,72,"I'm not completely sure this doesn't break anything, but the regression tests pass. The internal code is very similar, so I'm not too worried about these changes. It does look like it's (very) slightly slower. Running this a thousand times for pbmc68k dataset took ~2.3% longer (about 1.4 ms per run) than the previous version. That said, we're very inefficient about mean and variance calculation, so I think that's a better place to optimize. Edit: I've force pushed to fix some minor formatting issues (trailing white space, blank line, typo) that I didn't think deserved it's own commit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/621
https://github.com/scverse/scanpy/pull/621:8,security,compl,completely,8,"I'm not completely sure this doesn't break anything, but the regression tests pass. The internal code is very similar, so I'm not too worried about these changes. It does look like it's (very) slightly slower. Running this a thousand times for pbmc68k dataset took ~2.3% longer (about 1.4 ms per run) than the previous version. That said, we're very inefficient about mean and variance calculation, so I think that's a better place to optimize. Edit: I've force pushed to fix some minor formatting issues (trailing white space, blank line, typo) that I didn't think deserved it's own commit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/621
https://github.com/scverse/scanpy/pull/621:61,testability,regress,regression,61,"I'm not completely sure this doesn't break anything, but the regression tests pass. The internal code is very similar, so I'm not too worried about these changes. It does look like it's (very) slightly slower. Running this a thousand times for pbmc68k dataset took ~2.3% longer (about 1.4 ms per run) than the previous version. That said, we're very inefficient about mean and variance calculation, so I think that's a better place to optimize. Edit: I've force pushed to fix some minor formatting issues (trailing white space, blank line, typo) that I didn't think deserved it's own commit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/621
https://github.com/scverse/scanpy/pull/621:72,testability,test,tests,72,"I'm not completely sure this doesn't break anything, but the regression tests pass. The internal code is very similar, so I'm not too worried about these changes. It does look like it's (very) slightly slower. Running this a thousand times for pbmc68k dataset took ~2.3% longer (about 1.4 ms per run) than the previous version. That said, we're very inefficient about mean and variance calculation, so I think that's a better place to optimize. Edit: I've force pushed to fix some minor formatting issues (trailing white space, blank line, typo) that I didn't think deserved it's own commit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/621
https://github.com/scverse/scanpy/pull/622:463,deployability,updat,update,463,"This is very helpful! Great! :smile:. But, can have a non-recursive formulation of this? Others and I worked to get rid of many of the initial recursive formulations as they were hard to read. And here, it's the same thing. It's already a very long function and should not get longer. Can you just rename the old `highly_variable_genes` to `_highly_variable_genes_single_batch` and remove the recursion? It's a very simple change, I'd be grateful... Can you also update `docs/release_notes.rst` with a link to this PR?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/622
https://github.com/scverse/scanpy/pull/622:463,safety,updat,update,463,"This is very helpful! Great! :smile:. But, can have a non-recursive formulation of this? Others and I worked to get rid of many of the initial recursive formulations as they were hard to read. And here, it's the same thing. It's already a very long function and should not get longer. Can you just rename the old `highly_variable_genes` to `_highly_variable_genes_single_batch` and remove the recursion? It's a very simple change, I'd be grateful... Can you also update `docs/release_notes.rst` with a link to this PR?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/622
https://github.com/scverse/scanpy/pull/622:463,security,updat,update,463,"This is very helpful! Great! :smile:. But, can have a non-recursive formulation of this? Others and I worked to get rid of many of the initial recursive formulations as they were hard to read. And here, it's the same thing. It's already a very long function and should not get longer. Can you just rename the old `highly_variable_genes` to `_highly_variable_genes_single_batch` and remove the recursion? It's a very simple change, I'd be grateful... Can you also update `docs/release_notes.rst` with a link to this PR?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/622
https://github.com/scverse/scanpy/pull/622:416,testability,simpl,simple,416,"This is very helpful! Great! :smile:. But, can have a non-recursive formulation of this? Others and I worked to get rid of many of the initial recursive formulations as they were hard to read. And here, it's the same thing. It's already a very long function and should not get longer. Can you just rename the old `highly_variable_genes` to `_highly_variable_genes_single_batch` and remove the recursion? It's a very simple change, I'd be grateful... Can you also update `docs/release_notes.rst` with a link to this PR?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/622
https://github.com/scverse/scanpy/pull/622:13,usability,help,helpful,13,"This is very helpful! Great! :smile:. But, can have a non-recursive formulation of this? Others and I worked to get rid of many of the initial recursive formulations as they were hard to read. And here, it's the same thing. It's already a very long function and should not get longer. Can you just rename the old `highly_variable_genes` to `_highly_variable_genes_single_batch` and remove the recursion? It's a very simple change, I'd be grateful... Can you also update `docs/release_notes.rst` with a link to this PR?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/622
https://github.com/scverse/scanpy/pull/622:416,usability,simpl,simple,416,"This is very helpful! Great! :smile:. But, can have a non-recursive formulation of this? Others and I worked to get rid of many of the initial recursive formulations as they were hard to read. And here, it's the same thing. It's already a very long function and should not get longer. Can you just rename the old `highly_variable_genes` to `_highly_variable_genes_single_batch` and remove the recursion? It's a very simple change, I'd be grateful... Can you also update `docs/release_notes.rst` with a link to this PR?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/622
https://github.com/scverse/scanpy/pull/622:72,deployability,releas,release,72,"Actually, I added two commits to my master branch and one was about the release notes. But then instead of pushing to my fork, I pushed these to the master branch of scanpy repo by mistake. Fortunately, there were just these two commits that I wanted to add to this PR, so everything should be all right. Sorry for the confusion.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/622
https://github.com/scverse/scanpy/pull/623:63,deployability,log,logging,63,OK. I initially thought that PCA is such a fast step that much logging is not needed. But you're right. :),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/623
https://github.com/scverse/scanpy/pull/623:63,safety,log,logging,63,OK. I initially thought that PCA is such a fast step that much logging is not needed. But you're right. :),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/623
https://github.com/scverse/scanpy/pull/623:63,security,log,logging,63,OK. I initially thought that PCA is such a fast step that much logging is not needed. But you're right. :),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/623
https://github.com/scverse/scanpy/pull/623:63,testability,log,logging,63,OK. I initially thought that PCA is such a fast step that much logging is not needed. But you're right. :),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/623
https://github.com/scverse/scanpy/issues/625:24,availability,cluster,cluster,24,"You can interchange the cluster order (reference with the cluster your intersted in) and get only-upregulated ones for the inverted comparison (hence down-regulated genes) or set `rankby_abs` to `True`, which will then give you down-regulated genes in addition to up-regulated ones.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/625
https://github.com/scverse/scanpy/issues/625:58,availability,cluster,cluster,58,"You can interchange the cluster order (reference with the cluster your intersted in) and get only-upregulated ones for the inverted comparison (hence down-regulated genes) or set `rankby_abs` to `True`, which will then give you down-regulated genes in addition to up-regulated ones.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/625
https://github.com/scverse/scanpy/issues/625:150,availability,down,down-regulated,150,"You can interchange the cluster order (reference with the cluster your intersted in) and get only-upregulated ones for the inverted comparison (hence down-regulated genes) or set `rankby_abs` to `True`, which will then give you down-regulated genes in addition to up-regulated ones.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/625
https://github.com/scverse/scanpy/issues/625:228,availability,down,down-regulated,228,"You can interchange the cluster order (reference with the cluster your intersted in) and get only-upregulated ones for the inverted comparison (hence down-regulated genes) or set `rankby_abs` to `True`, which will then give you down-regulated genes in addition to up-regulated ones.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/625
https://github.com/scverse/scanpy/issues/625:24,deployability,cluster,cluster,24,"You can interchange the cluster order (reference with the cluster your intersted in) and get only-upregulated ones for the inverted comparison (hence down-regulated genes) or set `rankby_abs` to `True`, which will then give you down-regulated genes in addition to up-regulated ones.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/625
https://github.com/scverse/scanpy/issues/625:58,deployability,cluster,cluster,58,"You can interchange the cluster order (reference with the cluster your intersted in) and get only-upregulated ones for the inverted comparison (hence down-regulated genes) or set `rankby_abs` to `True`, which will then give you down-regulated genes in addition to up-regulated ones.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/625
https://github.com/scverse/scanpy/issues/625:44,deployability,integr,integrate,44,In case you're still looking to use MAST or integrate other `R` tools into a scanpy pipeline. That works quite well via [anndata2ri](www.github.com/flying-sheep/anndata2ri). An example of how you can do this can be found in the case study notebook [here](https://github.com/theislab/single-cell-tutorial/blob/master/latest_notebook/Case-study_Mouse-intestinal-epithelium_1904.ipynb).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/625
https://github.com/scverse/scanpy/issues/625:84,deployability,pipelin,pipeline,84,In case you're still looking to use MAST or integrate other `R` tools into a scanpy pipeline. That works quite well via [anndata2ri](www.github.com/flying-sheep/anndata2ri). An example of how you can do this can be found in the case study notebook [here](https://github.com/theislab/single-cell-tutorial/blob/master/latest_notebook/Case-study_Mouse-intestinal-epithelium_1904.ipynb).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/625
https://github.com/scverse/scanpy/issues/625:44,integrability,integr,integrate,44,In case you're still looking to use MAST or integrate other `R` tools into a scanpy pipeline. That works quite well via [anndata2ri](www.github.com/flying-sheep/anndata2ri). An example of how you can do this can be found in the case study notebook [here](https://github.com/theislab/single-cell-tutorial/blob/master/latest_notebook/Case-study_Mouse-intestinal-epithelium_1904.ipynb).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/625
https://github.com/scverse/scanpy/issues/625:84,integrability,pipelin,pipeline,84,In case you're still looking to use MAST or integrate other `R` tools into a scanpy pipeline. That works quite well via [anndata2ri](www.github.com/flying-sheep/anndata2ri). An example of how you can do this can be found in the case study notebook [here](https://github.com/theislab/single-cell-tutorial/blob/master/latest_notebook/Case-study_Mouse-intestinal-epithelium_1904.ipynb).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/625
https://github.com/scverse/scanpy/issues/625:44,interoperability,integr,integrate,44,In case you're still looking to use MAST or integrate other `R` tools into a scanpy pipeline. That works quite well via [anndata2ri](www.github.com/flying-sheep/anndata2ri). An example of how you can do this can be found in the case study notebook [here](https://github.com/theislab/single-cell-tutorial/blob/master/latest_notebook/Case-study_Mouse-intestinal-epithelium_1904.ipynb).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/625
https://github.com/scverse/scanpy/issues/625:44,modifiability,integr,integrate,44,In case you're still looking to use MAST or integrate other `R` tools into a scanpy pipeline. That works quite well via [anndata2ri](www.github.com/flying-sheep/anndata2ri). An example of how you can do this can be found in the case study notebook [here](https://github.com/theislab/single-cell-tutorial/blob/master/latest_notebook/Case-study_Mouse-intestinal-epithelium_1904.ipynb).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/625
https://github.com/scverse/scanpy/issues/625:44,reliability,integr,integrate,44,In case you're still looking to use MAST or integrate other `R` tools into a scanpy pipeline. That works quite well via [anndata2ri](www.github.com/flying-sheep/anndata2ri). An example of how you can do this can be found in the case study notebook [here](https://github.com/theislab/single-cell-tutorial/blob/master/latest_notebook/Case-study_Mouse-intestinal-epithelium_1904.ipynb).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/625
https://github.com/scverse/scanpy/issues/625:44,security,integr,integrate,44,In case you're still looking to use MAST or integrate other `R` tools into a scanpy pipeline. That works quite well via [anndata2ri](www.github.com/flying-sheep/anndata2ri). An example of how you can do this can be found in the case study notebook [here](https://github.com/theislab/single-cell-tutorial/blob/master/latest_notebook/Case-study_Mouse-intestinal-epithelium_1904.ipynb).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/625
https://github.com/scverse/scanpy/issues/625:44,testability,integr,integrate,44,In case you're still looking to use MAST or integrate other `R` tools into a scanpy pipeline. That works quite well via [anndata2ri](www.github.com/flying-sheep/anndata2ri). An example of how you can do this can be found in the case study notebook [here](https://github.com/theislab/single-cell-tutorial/blob/master/latest_notebook/Case-study_Mouse-intestinal-epithelium_1904.ipynb).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/625
https://github.com/scverse/scanpy/issues/625:64,usability,tool,tools,64,In case you're still looking to use MAST or integrate other `R` tools into a scanpy pipeline. That works quite well via [anndata2ri](www.github.com/flying-sheep/anndata2ri). An example of how you can do this can be found in the case study notebook [here](https://github.com/theislab/single-cell-tutorial/blob/master/latest_notebook/Case-study_Mouse-intestinal-epithelium_1904.ipynb).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/625
https://github.com/scverse/scanpy/issues/626:5,reliability,doe,doesn,5,That doesn't seem to be a Scanpy or AnnData issue but an issue with a corrupted HDF5 file as such.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/626
https://github.com/scverse/scanpy/issues/626:131,availability,error,error,131,> That doesn't seem to be a Scanpy or AnnData issue but an issue with a corrupted HDF5 file as such. Thanks! I struggled with this error for a long time yet it turns out the file is corrupted.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/626
https://github.com/scverse/scanpy/issues/626:131,performance,error,error,131,> That doesn't seem to be a Scanpy or AnnData issue but an issue with a corrupted HDF5 file as such. Thanks! I struggled with this error for a long time yet it turns out the file is corrupted.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/626
https://github.com/scverse/scanpy/issues/626:148,performance,time,time,148,> That doesn't seem to be a Scanpy or AnnData issue but an issue with a corrupted HDF5 file as such. Thanks! I struggled with this error for a long time yet it turns out the file is corrupted.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/626
https://github.com/scverse/scanpy/issues/626:7,reliability,doe,doesn,7,> That doesn't seem to be a Scanpy or AnnData issue but an issue with a corrupted HDF5 file as such. Thanks! I struggled with this error for a long time yet it turns out the file is corrupted.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/626
https://github.com/scverse/scanpy/issues/626:131,safety,error,error,131,> That doesn't seem to be a Scanpy or AnnData issue but an issue with a corrupted HDF5 file as such. Thanks! I struggled with this error for a long time yet it turns out the file is corrupted.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/626
https://github.com/scverse/scanpy/issues/626:131,usability,error,error,131,> That doesn't seem to be a Scanpy or AnnData issue but an issue with a corrupted HDF5 file as such. Thanks! I struggled with this error for a long time yet it turns out the file is corrupted.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/626
https://github.com/scverse/scanpy/issues/627:37,modifiability,Maintain,Maintaining,37,"Hi! Thank you for your contribution! Maintaining scanpy is a lot of work, so we like to rely on GitHub to help us here. Please read up on how to use github so you can [create a pull request](https://help.github.com/en/articles/creating-a-pull-request) with your changes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/627
https://github.com/scverse/scanpy/issues/627:37,safety,Maintain,Maintaining,37,"Hi! Thank you for your contribution! Maintaining scanpy is a lot of work, so we like to rely on GitHub to help us here. Please read up on how to use github so you can [create a pull request](https://help.github.com/en/articles/creating-a-pull-request) with your changes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/627
https://github.com/scverse/scanpy/issues/627:106,usability,help,help,106,"Hi! Thank you for your contribution! Maintaining scanpy is a lot of work, so we like to rely on GitHub to help us here. Please read up on how to use github so you can [create a pull request](https://help.github.com/en/articles/creating-a-pull-request) with your changes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/627
https://github.com/scverse/scanpy/issues/627:199,usability,help,help,199,"Hi! Thank you for your contribution! Maintaining scanpy is a lot of work, so we like to rely on GitHub to help us here. Please read up on how to use github so you can [create a pull request](https://help.github.com/en/articles/creating-a-pull-request) with your changes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/627
https://github.com/scverse/scanpy/issues/627:96,deployability,fail,failed,96,"Thanks for your reply. I have modified code and sent a pull request, but it seems that test is. failed, but i don't know why test is failed. I have double check my code,. its fine. Thanks ,. Waiting for your reply. Regards,. Khalid. On Fri, May 3, 2019 at 5:17 PM Philipp A. <notifications@github.com> wrote:. > Hi! Thank you for your contribution! >. > Maintaining scanpy is a lot of work, so we like to rely on GitHub to help. > us here. Please read up on how to use github so you can create a pull. > request <https://help.github.com/en/articles/creating-a-pull-request>. > with your changes. >. > â€”. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/627#issuecomment-489024928>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOHDRLVEO2QTVBGAQ2TPTP7J7ANCNFSM4HJSV4RQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/627
https://github.com/scverse/scanpy/issues/627:133,deployability,fail,failed,133,"Thanks for your reply. I have modified code and sent a pull request, but it seems that test is. failed, but i don't know why test is failed. I have double check my code,. its fine. Thanks ,. Waiting for your reply. Regards,. Khalid. On Fri, May 3, 2019 at 5:17 PM Philipp A. <notifications@github.com> wrote:. > Hi! Thank you for your contribution! >. > Maintaining scanpy is a lot of work, so we like to rely on GitHub to help. > us here. Please read up on how to use github so you can create a pull. > request <https://help.github.com/en/articles/creating-a-pull-request>. > with your changes. >. > â€”. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/627#issuecomment-489024928>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOHDRLVEO2QTVBGAQ2TPTP7J7ANCNFSM4HJSV4RQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/627
https://github.com/scverse/scanpy/issues/627:354,modifiability,Maintain,Maintaining,354,"Thanks for your reply. I have modified code and sent a pull request, but it seems that test is. failed, but i don't know why test is failed. I have double check my code,. its fine. Thanks ,. Waiting for your reply. Regards,. Khalid. On Fri, May 3, 2019 at 5:17 PM Philipp A. <notifications@github.com> wrote:. > Hi! Thank you for your contribution! >. > Maintaining scanpy is a lot of work, so we like to rely on GitHub to help. > us here. Please read up on how to use github so you can create a pull. > request <https://help.github.com/en/articles/creating-a-pull-request>. > with your changes. >. > â€”. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/627#issuecomment-489024928>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOHDRLVEO2QTVBGAQ2TPTP7J7ANCNFSM4HJSV4RQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/627
https://github.com/scverse/scanpy/issues/627:96,reliability,fail,failed,96,"Thanks for your reply. I have modified code and sent a pull request, but it seems that test is. failed, but i don't know why test is failed. I have double check my code,. its fine. Thanks ,. Waiting for your reply. Regards,. Khalid. On Fri, May 3, 2019 at 5:17 PM Philipp A. <notifications@github.com> wrote:. > Hi! Thank you for your contribution! >. > Maintaining scanpy is a lot of work, so we like to rely on GitHub to help. > us here. Please read up on how to use github so you can create a pull. > request <https://help.github.com/en/articles/creating-a-pull-request>. > with your changes. >. > â€”. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/627#issuecomment-489024928>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOHDRLVEO2QTVBGAQ2TPTP7J7ANCNFSM4HJSV4RQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/627
https://github.com/scverse/scanpy/issues/627:133,reliability,fail,failed,133,"Thanks for your reply. I have modified code and sent a pull request, but it seems that test is. failed, but i don't know why test is failed. I have double check my code,. its fine. Thanks ,. Waiting for your reply. Regards,. Khalid. On Fri, May 3, 2019 at 5:17 PM Philipp A. <notifications@github.com> wrote:. > Hi! Thank you for your contribution! >. > Maintaining scanpy is a lot of work, so we like to rely on GitHub to help. > us here. Please read up on how to use github so you can create a pull. > request <https://help.github.com/en/articles/creating-a-pull-request>. > with your changes. >. > â€”. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/627#issuecomment-489024928>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOHDRLVEO2QTVBGAQ2TPTP7J7ANCNFSM4HJSV4RQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/627
https://github.com/scverse/scanpy/issues/627:87,safety,test,test,87,"Thanks for your reply. I have modified code and sent a pull request, but it seems that test is. failed, but i don't know why test is failed. I have double check my code,. its fine. Thanks ,. Waiting for your reply. Regards,. Khalid. On Fri, May 3, 2019 at 5:17 PM Philipp A. <notifications@github.com> wrote:. > Hi! Thank you for your contribution! >. > Maintaining scanpy is a lot of work, so we like to rely on GitHub to help. > us here. Please read up on how to use github so you can create a pull. > request <https://help.github.com/en/articles/creating-a-pull-request>. > with your changes. >. > â€”. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/627#issuecomment-489024928>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOHDRLVEO2QTVBGAQ2TPTP7J7ANCNFSM4HJSV4RQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/627
https://github.com/scverse/scanpy/issues/627:125,safety,test,test,125,"Thanks for your reply. I have modified code and sent a pull request, but it seems that test is. failed, but i don't know why test is failed. I have double check my code,. its fine. Thanks ,. Waiting for your reply. Regards,. Khalid. On Fri, May 3, 2019 at 5:17 PM Philipp A. <notifications@github.com> wrote:. > Hi! Thank you for your contribution! >. > Maintaining scanpy is a lot of work, so we like to rely on GitHub to help. > us here. Please read up on how to use github so you can create a pull. > request <https://help.github.com/en/articles/creating-a-pull-request>. > with your changes. >. > â€”. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/627#issuecomment-489024928>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOHDRLVEO2QTVBGAQ2TPTP7J7ANCNFSM4HJSV4RQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/627
https://github.com/scverse/scanpy/issues/627:354,safety,Maintain,Maintaining,354,"Thanks for your reply. I have modified code and sent a pull request, but it seems that test is. failed, but i don't know why test is failed. I have double check my code,. its fine. Thanks ,. Waiting for your reply. Regards,. Khalid. On Fri, May 3, 2019 at 5:17 PM Philipp A. <notifications@github.com> wrote:. > Hi! Thank you for your contribution! >. > Maintaining scanpy is a lot of work, so we like to rely on GitHub to help. > us here. Please read up on how to use github so you can create a pull. > request <https://help.github.com/en/articles/creating-a-pull-request>. > with your changes. >. > â€”. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/627#issuecomment-489024928>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOHDRLVEO2QTVBGAQ2TPTP7J7ANCNFSM4HJSV4RQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/627
https://github.com/scverse/scanpy/issues/627:30,security,modif,modified,30,"Thanks for your reply. I have modified code and sent a pull request, but it seems that test is. failed, but i don't know why test is failed. I have double check my code,. its fine. Thanks ,. Waiting for your reply. Regards,. Khalid. On Fri, May 3, 2019 at 5:17 PM Philipp A. <notifications@github.com> wrote:. > Hi! Thank you for your contribution! >. > Maintaining scanpy is a lot of work, so we like to rely on GitHub to help. > us here. Please read up on how to use github so you can create a pull. > request <https://help.github.com/en/articles/creating-a-pull-request>. > with your changes. >. > â€”. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/627#issuecomment-489024928>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOHDRLVEO2QTVBGAQ2TPTP7J7ANCNFSM4HJSV4RQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/627
https://github.com/scverse/scanpy/issues/627:641,security,auth,authored,641,"Thanks for your reply. I have modified code and sent a pull request, but it seems that test is. failed, but i don't know why test is failed. I have double check my code,. its fine. Thanks ,. Waiting for your reply. Regards,. Khalid. On Fri, May 3, 2019 at 5:17 PM Philipp A. <notifications@github.com> wrote:. > Hi! Thank you for your contribution! >. > Maintaining scanpy is a lot of work, so we like to rely on GitHub to help. > us here. Please read up on how to use github so you can create a pull. > request <https://help.github.com/en/articles/creating-a-pull-request>. > with your changes. >. > â€”. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/627#issuecomment-489024928>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOHDRLVEO2QTVBGAQ2TPTP7J7ANCNFSM4HJSV4RQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/627
https://github.com/scverse/scanpy/issues/627:858,security,auth,auth,858,"Thanks for your reply. I have modified code and sent a pull request, but it seems that test is. failed, but i don't know why test is failed. I have double check my code,. its fine. Thanks ,. Waiting for your reply. Regards,. Khalid. On Fri, May 3, 2019 at 5:17 PM Philipp A. <notifications@github.com> wrote:. > Hi! Thank you for your contribution! >. > Maintaining scanpy is a lot of work, so we like to rely on GitHub to help. > us here. Please read up on how to use github so you can create a pull. > request <https://help.github.com/en/articles/creating-a-pull-request>. > with your changes. >. > â€”. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/627#issuecomment-489024928>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOHDRLVEO2QTVBGAQ2TPTP7J7ANCNFSM4HJSV4RQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/627
https://github.com/scverse/scanpy/issues/627:87,testability,test,test,87,"Thanks for your reply. I have modified code and sent a pull request, but it seems that test is. failed, but i don't know why test is failed. I have double check my code,. its fine. Thanks ,. Waiting for your reply. Regards,. Khalid. On Fri, May 3, 2019 at 5:17 PM Philipp A. <notifications@github.com> wrote:. > Hi! Thank you for your contribution! >. > Maintaining scanpy is a lot of work, so we like to rely on GitHub to help. > us here. Please read up on how to use github so you can create a pull. > request <https://help.github.com/en/articles/creating-a-pull-request>. > with your changes. >. > â€”. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/627#issuecomment-489024928>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOHDRLVEO2QTVBGAQ2TPTP7J7ANCNFSM4HJSV4RQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/627
https://github.com/scverse/scanpy/issues/627:125,testability,test,test,125,"Thanks for your reply. I have modified code and sent a pull request, but it seems that test is. failed, but i don't know why test is failed. I have double check my code,. its fine. Thanks ,. Waiting for your reply. Regards,. Khalid. On Fri, May 3, 2019 at 5:17 PM Philipp A. <notifications@github.com> wrote:. > Hi! Thank you for your contribution! >. > Maintaining scanpy is a lot of work, so we like to rely on GitHub to help. > us here. Please read up on how to use github so you can create a pull. > request <https://help.github.com/en/articles/creating-a-pull-request>. > with your changes. >. > â€”. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/627#issuecomment-489024928>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOHDRLVEO2QTVBGAQ2TPTP7J7ANCNFSM4HJSV4RQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/627
https://github.com/scverse/scanpy/issues/627:423,usability,help,help,423,"Thanks for your reply. I have modified code and sent a pull request, but it seems that test is. failed, but i don't know why test is failed. I have double check my code,. its fine. Thanks ,. Waiting for your reply. Regards,. Khalid. On Fri, May 3, 2019 at 5:17 PM Philipp A. <notifications@github.com> wrote:. > Hi! Thank you for your contribution! >. > Maintaining scanpy is a lot of work, so we like to rely on GitHub to help. > us here. Please read up on how to use github so you can create a pull. > request <https://help.github.com/en/articles/creating-a-pull-request>. > with your changes. >. > â€”. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/627#issuecomment-489024928>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOHDRLVEO2QTVBGAQ2TPTP7J7ANCNFSM4HJSV4RQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/627
https://github.com/scverse/scanpy/issues/627:521,usability,help,help,521,"Thanks for your reply. I have modified code and sent a pull request, but it seems that test is. failed, but i don't know why test is failed. I have double check my code,. its fine. Thanks ,. Waiting for your reply. Regards,. Khalid. On Fri, May 3, 2019 at 5:17 PM Philipp A. <notifications@github.com> wrote:. > Hi! Thank you for your contribution! >. > Maintaining scanpy is a lot of work, so we like to rely on GitHub to help. > us here. Please read up on how to use github so you can create a pull. > request <https://help.github.com/en/articles/creating-a-pull-request>. > with your changes. >. > â€”. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/627#issuecomment-489024928>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOHDRLVEO2QTVBGAQ2TPTP7J7ANCNFSM4HJSV4RQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/627
https://github.com/scverse/scanpy/pull/628:4,deployability,fail,failing,4,The failing test is un-related to the PR. @LuckyMD can you take a look at the changes?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/628
https://github.com/scverse/scanpy/pull/628:4,reliability,fail,failing,4,The failing test is un-related to the PR. @LuckyMD can you take a look at the changes?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/628
https://github.com/scverse/scanpy/pull/628:12,safety,test,test,12,The failing test is un-related to the PR. @LuckyMD can you take a look at the changes?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/628
https://github.com/scverse/scanpy/pull/628:12,testability,test,test,12,The failing test is un-related to the PR. @LuckyMD can you take a look at the changes?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/628
https://github.com/scverse/scanpy/pull/628:80,energy efficiency,current,current,80,"I really like this! I was trying to figure out how to easily do this within the current `plot_scatter` framework, but I couldn't figure out how. One thing though... You could add an option `group='all'`, which should be the default. That would be much more user-friendly than the current setup.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/628
https://github.com/scverse/scanpy/pull/628:280,energy efficiency,current,current,280,"I really like this! I was trying to figure out how to easily do this within the current `plot_scatter` framework, but I couldn't figure out how. One thing though... You could add an option `group='all'`, which should be the default. That would be much more user-friendly than the current setup.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/628
https://github.com/scverse/scanpy/pull/628:257,usability,user,user-friendly,257,"I really like this! I was trying to figure out how to easily do this within the current `plot_scatter` framework, but I couldn't figure out how. One thing though... You could add an option `group='all'`, which should be the default. That would be much more user-friendly than the current setup.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/628
https://github.com/scverse/scanpy/pull/628:42,usability,user,user-images,42,Now it looks like this:. ![image](https://user-images.githubusercontent.com/4964309/57133589-8df12a00-6da3-11e9-907a-7ec3bcdbb007.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/628
https://github.com/scverse/scanpy/pull/628:42,usability,tool,tool,42,"It feels like I tried to give the density tool wings by putting together lego bricks which just about kept it in the air, and then you just crossed it with a pegasus ðŸ˜†.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/628
https://github.com/scverse/scanpy/pull/628:5,energy efficiency,cool,cool,5,Very cool!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/628
https://github.com/scverse/scanpy/issues/629:824,availability,Mask,Mask,824,"It looks like those warning are being raised from `scipy.stats.distributions.t.sf`. . This was also happening in the tests, but there's already a bunch of warnings in the tests so we didn't see it. ~~I believe we didn't get this warning from the older code because of these lines:~~. ```python. dof[np.isnan(dof)] = 0		. pvals = stats.t.sf(abs(scores), dof)*2 # *2 because of two-tailed t-test. ```. I don't think it's the above lines anymore, since the replacing the `ttest_ind_from_stats` call with the following still throws the warning:. ```python. df, denom = stats.stats._unequal_var_ttest_denom(. v1=var_group, n1=ns_group, v2=var_rest, n2=ns_rest. ). df[np.isnan(df)] = 0. scores, pvals = stats.stats._ttest_ind_from_stats(. mean_group, mean_rest, denom, df. ). ```. Other than that, potential solutions include:. * Mask out genes which aren't expressed in the compared groups (since there's not too much point in getting and correcting a pvalue for them). * Revert change (would bring back issue of genes with variance of 0). * Wrap the t-test with something like `np.errstate` to hide the warning",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/629
https://github.com/scverse/scanpy/issues/629:1037,integrability,Wrap,Wrap,1037,"It looks like those warning are being raised from `scipy.stats.distributions.t.sf`. . This was also happening in the tests, but there's already a bunch of warnings in the tests so we didn't see it. ~~I believe we didn't get this warning from the older code because of these lines:~~. ```python. dof[np.isnan(dof)] = 0		. pvals = stats.t.sf(abs(scores), dof)*2 # *2 because of two-tailed t-test. ```. I don't think it's the above lines anymore, since the replacing the `ttest_ind_from_stats` call with the following still throws the warning:. ```python. df, denom = stats.stats._unequal_var_ttest_denom(. v1=var_group, n1=ns_group, v2=var_rest, n2=ns_rest. ). df[np.isnan(df)] = 0. scores, pvals = stats.stats._ttest_ind_from_stats(. mean_group, mean_rest, denom, df. ). ```. Other than that, potential solutions include:. * Mask out genes which aren't expressed in the compared groups (since there's not too much point in getting and correcting a pvalue for them). * Revert change (would bring back issue of genes with variance of 0). * Wrap the t-test with something like `np.errstate` to hide the warning",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/629
https://github.com/scverse/scanpy/issues/629:63,interoperability,distribut,distributions,63,"It looks like those warning are being raised from `scipy.stats.distributions.t.sf`. . This was also happening in the tests, but there's already a bunch of warnings in the tests so we didn't see it. ~~I believe we didn't get this warning from the older code because of these lines:~~. ```python. dof[np.isnan(dof)] = 0		. pvals = stats.t.sf(abs(scores), dof)*2 # *2 because of two-tailed t-test. ```. I don't think it's the above lines anymore, since the replacing the `ttest_ind_from_stats` call with the following still throws the warning:. ```python. df, denom = stats.stats._unequal_var_ttest_denom(. v1=var_group, n1=ns_group, v2=var_rest, n2=ns_rest. ). df[np.isnan(df)] = 0. scores, pvals = stats.stats._ttest_ind_from_stats(. mean_group, mean_rest, denom, df. ). ```. Other than that, potential solutions include:. * Mask out genes which aren't expressed in the compared groups (since there's not too much point in getting and correcting a pvalue for them). * Revert change (would bring back issue of genes with variance of 0). * Wrap the t-test with something like `np.errstate` to hide the warning",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/629
https://github.com/scverse/scanpy/issues/629:117,safety,test,tests,117,"It looks like those warning are being raised from `scipy.stats.distributions.t.sf`. . This was also happening in the tests, but there's already a bunch of warnings in the tests so we didn't see it. ~~I believe we didn't get this warning from the older code because of these lines:~~. ```python. dof[np.isnan(dof)] = 0		. pvals = stats.t.sf(abs(scores), dof)*2 # *2 because of two-tailed t-test. ```. I don't think it's the above lines anymore, since the replacing the `ttest_ind_from_stats` call with the following still throws the warning:. ```python. df, denom = stats.stats._unequal_var_ttest_denom(. v1=var_group, n1=ns_group, v2=var_rest, n2=ns_rest. ). df[np.isnan(df)] = 0. scores, pvals = stats.stats._ttest_ind_from_stats(. mean_group, mean_rest, denom, df. ). ```. Other than that, potential solutions include:. * Mask out genes which aren't expressed in the compared groups (since there's not too much point in getting and correcting a pvalue for them). * Revert change (would bring back issue of genes with variance of 0). * Wrap the t-test with something like `np.errstate` to hide the warning",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/629
https://github.com/scverse/scanpy/issues/629:171,safety,test,tests,171,"It looks like those warning are being raised from `scipy.stats.distributions.t.sf`. . This was also happening in the tests, but there's already a bunch of warnings in the tests so we didn't see it. ~~I believe we didn't get this warning from the older code because of these lines:~~. ```python. dof[np.isnan(dof)] = 0		. pvals = stats.t.sf(abs(scores), dof)*2 # *2 because of two-tailed t-test. ```. I don't think it's the above lines anymore, since the replacing the `ttest_ind_from_stats` call with the following still throws the warning:. ```python. df, denom = stats.stats._unequal_var_ttest_denom(. v1=var_group, n1=ns_group, v2=var_rest, n2=ns_rest. ). df[np.isnan(df)] = 0. scores, pvals = stats.stats._ttest_ind_from_stats(. mean_group, mean_rest, denom, df. ). ```. Other than that, potential solutions include:. * Mask out genes which aren't expressed in the compared groups (since there's not too much point in getting and correcting a pvalue for them). * Revert change (would bring back issue of genes with variance of 0). * Wrap the t-test with something like `np.errstate` to hide the warning",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/629
https://github.com/scverse/scanpy/issues/629:389,safety,test,test,389,"It looks like those warning are being raised from `scipy.stats.distributions.t.sf`. . This was also happening in the tests, but there's already a bunch of warnings in the tests so we didn't see it. ~~I believe we didn't get this warning from the older code because of these lines:~~. ```python. dof[np.isnan(dof)] = 0		. pvals = stats.t.sf(abs(scores), dof)*2 # *2 because of two-tailed t-test. ```. I don't think it's the above lines anymore, since the replacing the `ttest_ind_from_stats` call with the following still throws the warning:. ```python. df, denom = stats.stats._unequal_var_ttest_denom(. v1=var_group, n1=ns_group, v2=var_rest, n2=ns_rest. ). df[np.isnan(df)] = 0. scores, pvals = stats.stats._ttest_ind_from_stats(. mean_group, mean_rest, denom, df. ). ```. Other than that, potential solutions include:. * Mask out genes which aren't expressed in the compared groups (since there's not too much point in getting and correcting a pvalue for them). * Revert change (would bring back issue of genes with variance of 0). * Wrap the t-test with something like `np.errstate` to hide the warning",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/629
https://github.com/scverse/scanpy/issues/629:1048,safety,test,test,1048,"It looks like those warning are being raised from `scipy.stats.distributions.t.sf`. . This was also happening in the tests, but there's already a bunch of warnings in the tests so we didn't see it. ~~I believe we didn't get this warning from the older code because of these lines:~~. ```python. dof[np.isnan(dof)] = 0		. pvals = stats.t.sf(abs(scores), dof)*2 # *2 because of two-tailed t-test. ```. I don't think it's the above lines anymore, since the replacing the `ttest_ind_from_stats` call with the following still throws the warning:. ```python. df, denom = stats.stats._unequal_var_ttest_denom(. v1=var_group, n1=ns_group, v2=var_rest, n2=ns_rest. ). df[np.isnan(df)] = 0. scores, pvals = stats.stats._ttest_ind_from_stats(. mean_group, mean_rest, denom, df. ). ```. Other than that, potential solutions include:. * Mask out genes which aren't expressed in the compared groups (since there's not too much point in getting and correcting a pvalue for them). * Revert change (would bring back issue of genes with variance of 0). * Wrap the t-test with something like `np.errstate` to hide the warning",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/629
https://github.com/scverse/scanpy/issues/629:117,testability,test,tests,117,"It looks like those warning are being raised from `scipy.stats.distributions.t.sf`. . This was also happening in the tests, but there's already a bunch of warnings in the tests so we didn't see it. ~~I believe we didn't get this warning from the older code because of these lines:~~. ```python. dof[np.isnan(dof)] = 0		. pvals = stats.t.sf(abs(scores), dof)*2 # *2 because of two-tailed t-test. ```. I don't think it's the above lines anymore, since the replacing the `ttest_ind_from_stats` call with the following still throws the warning:. ```python. df, denom = stats.stats._unequal_var_ttest_denom(. v1=var_group, n1=ns_group, v2=var_rest, n2=ns_rest. ). df[np.isnan(df)] = 0. scores, pvals = stats.stats._ttest_ind_from_stats(. mean_group, mean_rest, denom, df. ). ```. Other than that, potential solutions include:. * Mask out genes which aren't expressed in the compared groups (since there's not too much point in getting and correcting a pvalue for them). * Revert change (would bring back issue of genes with variance of 0). * Wrap the t-test with something like `np.errstate` to hide the warning",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/629
https://github.com/scverse/scanpy/issues/629:171,testability,test,tests,171,"It looks like those warning are being raised from `scipy.stats.distributions.t.sf`. . This was also happening in the tests, but there's already a bunch of warnings in the tests so we didn't see it. ~~I believe we didn't get this warning from the older code because of these lines:~~. ```python. dof[np.isnan(dof)] = 0		. pvals = stats.t.sf(abs(scores), dof)*2 # *2 because of two-tailed t-test. ```. I don't think it's the above lines anymore, since the replacing the `ttest_ind_from_stats` call with the following still throws the warning:. ```python. df, denom = stats.stats._unequal_var_ttest_denom(. v1=var_group, n1=ns_group, v2=var_rest, n2=ns_rest. ). df[np.isnan(df)] = 0. scores, pvals = stats.stats._ttest_ind_from_stats(. mean_group, mean_rest, denom, df. ). ```. Other than that, potential solutions include:. * Mask out genes which aren't expressed in the compared groups (since there's not too much point in getting and correcting a pvalue for them). * Revert change (would bring back issue of genes with variance of 0). * Wrap the t-test with something like `np.errstate` to hide the warning",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/629
https://github.com/scverse/scanpy/issues/629:389,testability,test,test,389,"It looks like those warning are being raised from `scipy.stats.distributions.t.sf`. . This was also happening in the tests, but there's already a bunch of warnings in the tests so we didn't see it. ~~I believe we didn't get this warning from the older code because of these lines:~~. ```python. dof[np.isnan(dof)] = 0		. pvals = stats.t.sf(abs(scores), dof)*2 # *2 because of two-tailed t-test. ```. I don't think it's the above lines anymore, since the replacing the `ttest_ind_from_stats` call with the following still throws the warning:. ```python. df, denom = stats.stats._unequal_var_ttest_denom(. v1=var_group, n1=ns_group, v2=var_rest, n2=ns_rest. ). df[np.isnan(df)] = 0. scores, pvals = stats.stats._ttest_ind_from_stats(. mean_group, mean_rest, denom, df. ). ```. Other than that, potential solutions include:. * Mask out genes which aren't expressed in the compared groups (since there's not too much point in getting and correcting a pvalue for them). * Revert change (would bring back issue of genes with variance of 0). * Wrap the t-test with something like `np.errstate` to hide the warning",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/629
https://github.com/scverse/scanpy/issues/629:1048,testability,test,test,1048,"It looks like those warning are being raised from `scipy.stats.distributions.t.sf`. . This was also happening in the tests, but there's already a bunch of warnings in the tests so we didn't see it. ~~I believe we didn't get this warning from the older code because of these lines:~~. ```python. dof[np.isnan(dof)] = 0		. pvals = stats.t.sf(abs(scores), dof)*2 # *2 because of two-tailed t-test. ```. I don't think it's the above lines anymore, since the replacing the `ttest_ind_from_stats` call with the following still throws the warning:. ```python. df, denom = stats.stats._unequal_var_ttest_denom(. v1=var_group, n1=ns_group, v2=var_rest, n2=ns_rest. ). df[np.isnan(df)] = 0. scores, pvals = stats.stats._ttest_ind_from_stats(. mean_group, mean_rest, denom, df. ). ```. Other than that, potential solutions include:. * Mask out genes which aren't expressed in the compared groups (since there's not too much point in getting and correcting a pvalue for them). * Revert change (would bring back issue of genes with variance of 0). * Wrap the t-test with something like `np.errstate` to hide the warning",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/629
https://github.com/scverse/scanpy/issues/629:12,availability,Mask,Mask,12,"Cool! . > * Mask out genes which aren't expressed in the compared groups (since there's not too much point in getting and correcting a pvalue for them). I think masking out might be problematic because, `n_genes=adata.n_vars` should return all genes in any case. . > * Revert change (would bring back issue of genes with variance of 0). I feel like using scipy function will slightly increase the maintainability (and simplicity) of the code, so I'm fine with keeping the scipy switch. > * Wrap the t-test with something like `np.errstate` to hide the warning. This sounds good. Replacing weird scipy warning with a proper scanpy warning would also make sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/629
https://github.com/scverse/scanpy/issues/629:161,availability,mask,masking,161,"Cool! . > * Mask out genes which aren't expressed in the compared groups (since there's not too much point in getting and correcting a pvalue for them). I think masking out might be problematic because, `n_genes=adata.n_vars` should return all genes in any case. . > * Revert change (would bring back issue of genes with variance of 0). I feel like using scipy function will slightly increase the maintainability (and simplicity) of the code, so I'm fine with keeping the scipy switch. > * Wrap the t-test with something like `np.errstate` to hide the warning. This sounds good. Replacing weird scipy warning with a proper scanpy warning would also make sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/629
https://github.com/scverse/scanpy/issues/629:375,availability,sli,slightly,375,"Cool! . > * Mask out genes which aren't expressed in the compared groups (since there's not too much point in getting and correcting a pvalue for them). I think masking out might be problematic because, `n_genes=adata.n_vars` should return all genes in any case. . > * Revert change (would bring back issue of genes with variance of 0). I feel like using scipy function will slightly increase the maintainability (and simplicity) of the code, so I'm fine with keeping the scipy switch. > * Wrap the t-test with something like `np.errstate` to hide the warning. This sounds good. Replacing weird scipy warning with a proper scanpy warning would also make sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/629
https://github.com/scverse/scanpy/issues/629:0,energy efficiency,Cool,Cool,0,"Cool! . > * Mask out genes which aren't expressed in the compared groups (since there's not too much point in getting and correcting a pvalue for them). I think masking out might be problematic because, `n_genes=adata.n_vars` should return all genes in any case. . > * Revert change (would bring back issue of genes with variance of 0). I feel like using scipy function will slightly increase the maintainability (and simplicity) of the code, so I'm fine with keeping the scipy switch. > * Wrap the t-test with something like `np.errstate` to hide the warning. This sounds good. Replacing weird scipy warning with a proper scanpy warning would also make sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/629
https://github.com/scverse/scanpy/issues/629:490,integrability,Wrap,Wrap,490,"Cool! . > * Mask out genes which aren't expressed in the compared groups (since there's not too much point in getting and correcting a pvalue for them). I think masking out might be problematic because, `n_genes=adata.n_vars` should return all genes in any case. . > * Revert change (would bring back issue of genes with variance of 0). I feel like using scipy function will slightly increase the maintainability (and simplicity) of the code, so I'm fine with keeping the scipy switch. > * Wrap the t-test with something like `np.errstate` to hide the warning. This sounds good. Replacing weird scipy warning with a proper scanpy warning would also make sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/629
https://github.com/scverse/scanpy/issues/629:397,modifiability,maintain,maintainability,397,"Cool! . > * Mask out genes which aren't expressed in the compared groups (since there's not too much point in getting and correcting a pvalue for them). I think masking out might be problematic because, `n_genes=adata.n_vars` should return all genes in any case. . > * Revert change (would bring back issue of genes with variance of 0). I feel like using scipy function will slightly increase the maintainability (and simplicity) of the code, so I'm fine with keeping the scipy switch. > * Wrap the t-test with something like `np.errstate` to hide the warning. This sounds good. Replacing weird scipy warning with a proper scanpy warning would also make sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/629
https://github.com/scverse/scanpy/issues/629:375,reliability,sli,slightly,375,"Cool! . > * Mask out genes which aren't expressed in the compared groups (since there's not too much point in getting and correcting a pvalue for them). I think masking out might be problematic because, `n_genes=adata.n_vars` should return all genes in any case. . > * Revert change (would bring back issue of genes with variance of 0). I feel like using scipy function will slightly increase the maintainability (and simplicity) of the code, so I'm fine with keeping the scipy switch. > * Wrap the t-test with something like `np.errstate` to hide the warning. This sounds good. Replacing weird scipy warning with a proper scanpy warning would also make sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/629
https://github.com/scverse/scanpy/issues/629:397,safety,maintain,maintainability,397,"Cool! . > * Mask out genes which aren't expressed in the compared groups (since there's not too much point in getting and correcting a pvalue for them). I think masking out might be problematic because, `n_genes=adata.n_vars` should return all genes in any case. . > * Revert change (would bring back issue of genes with variance of 0). I feel like using scipy function will slightly increase the maintainability (and simplicity) of the code, so I'm fine with keeping the scipy switch. > * Wrap the t-test with something like `np.errstate` to hide the warning. This sounds good. Replacing weird scipy warning with a proper scanpy warning would also make sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/629
https://github.com/scverse/scanpy/issues/629:501,safety,test,test,501,"Cool! . > * Mask out genes which aren't expressed in the compared groups (since there's not too much point in getting and correcting a pvalue for them). I think masking out might be problematic because, `n_genes=adata.n_vars` should return all genes in any case. . > * Revert change (would bring back issue of genes with variance of 0). I feel like using scipy function will slightly increase the maintainability (and simplicity) of the code, so I'm fine with keeping the scipy switch. > * Wrap the t-test with something like `np.errstate` to hide the warning. This sounds good. Replacing weird scipy warning with a proper scanpy warning would also make sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/629
https://github.com/scverse/scanpy/issues/629:418,testability,simpl,simplicity,418,"Cool! . > * Mask out genes which aren't expressed in the compared groups (since there's not too much point in getting and correcting a pvalue for them). I think masking out might be problematic because, `n_genes=adata.n_vars` should return all genes in any case. . > * Revert change (would bring back issue of genes with variance of 0). I feel like using scipy function will slightly increase the maintainability (and simplicity) of the code, so I'm fine with keeping the scipy switch. > * Wrap the t-test with something like `np.errstate` to hide the warning. This sounds good. Replacing weird scipy warning with a proper scanpy warning would also make sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/629
https://github.com/scverse/scanpy/issues/629:501,testability,test,test,501,"Cool! . > * Mask out genes which aren't expressed in the compared groups (since there's not too much point in getting and correcting a pvalue for them). I think masking out might be problematic because, `n_genes=adata.n_vars` should return all genes in any case. . > * Revert change (would bring back issue of genes with variance of 0). I feel like using scipy function will slightly increase the maintainability (and simplicity) of the code, so I'm fine with keeping the scipy switch. > * Wrap the t-test with something like `np.errstate` to hide the warning. This sounds good. Replacing weird scipy warning with a proper scanpy warning would also make sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/629
https://github.com/scverse/scanpy/issues/629:418,usability,simpl,simplicity,418,"Cool! . > * Mask out genes which aren't expressed in the compared groups (since there's not too much point in getting and correcting a pvalue for them). I think masking out might be problematic because, `n_genes=adata.n_vars` should return all genes in any case. . > * Revert change (would bring back issue of genes with variance of 0). I feel like using scipy function will slightly increase the maintainability (and simplicity) of the code, so I'm fine with keeping the scipy switch. > * Wrap the t-test with something like `np.errstate` to hide the warning. This sounds good. Replacing weird scipy warning with a proper scanpy warning would also make sense.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/629
https://github.com/scverse/scanpy/issues/629:52,availability,mask,masked,52,You could just output `NaN` for all genes that were masked? That would be accurate as the test would fail in that case anyway. That should solve `n_genes == adata.n_vars`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/629
https://github.com/scverse/scanpy/issues/629:101,deployability,fail,fail,101,You could just output `NaN` for all genes that were masked? That would be accurate as the test would fail in that case anyway. That should solve `n_genes == adata.n_vars`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/629
https://github.com/scverse/scanpy/issues/629:101,reliability,fail,fail,101,You could just output `NaN` for all genes that were masked? That would be accurate as the test would fail in that case anyway. That should solve `n_genes == adata.n_vars`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/629
https://github.com/scverse/scanpy/issues/629:90,safety,test,test,90,You could just output `NaN` for all genes that were masked? That would be accurate as the test would fail in that case anyway. That should solve `n_genes == adata.n_vars`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/629
https://github.com/scverse/scanpy/issues/629:90,testability,test,test,90,You could just output `NaN` for all genes that were masked? That would be accurate as the test would fail in that case anyway. That should solve `n_genes == adata.n_vars`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/629
https://github.com/scverse/scanpy/issues/629:0,usability,Close,Closed,0,Closed via #683,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/629
https://github.com/scverse/scanpy/pull/630:337,availability,down,downloaded,337,"seems like you did something wrong. the commit you added (74540cc133ca9cfe0744ca9d3b250454a76a9c4d) reverts a lot of changes we made since. i assume you just copied all your code over the current master branch, and not the version of the master branch as it was when you made the changes. you need to find the version of scanpy that you downloaded before you made your changes and modify that one to have just the changes you want to commit. otherwise we have no idea what your actual changes are.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:223,deployability,version,version,223,"seems like you did something wrong. the commit you added (74540cc133ca9cfe0744ca9d3b250454a76a9c4d) reverts a lot of changes we made since. i assume you just copied all your code over the current master branch, and not the version of the master branch as it was when you made the changes. you need to find the version of scanpy that you downloaded before you made your changes and modify that one to have just the changes you want to commit. otherwise we have no idea what your actual changes are.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:310,deployability,version,version,310,"seems like you did something wrong. the commit you added (74540cc133ca9cfe0744ca9d3b250454a76a9c4d) reverts a lot of changes we made since. i assume you just copied all your code over the current master branch, and not the version of the master branch as it was when you made the changes. you need to find the version of scanpy that you downloaded before you made your changes and modify that one to have just the changes you want to commit. otherwise we have no idea what your actual changes are.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:188,energy efficiency,current,current,188,"seems like you did something wrong. the commit you added (74540cc133ca9cfe0744ca9d3b250454a76a9c4d) reverts a lot of changes we made since. i assume you just copied all your code over the current master branch, and not the version of the master branch as it was when you made the changes. you need to find the version of scanpy that you downloaded before you made your changes and modify that one to have just the changes you want to commit. otherwise we have no idea what your actual changes are.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:223,integrability,version,version,223,"seems like you did something wrong. the commit you added (74540cc133ca9cfe0744ca9d3b250454a76a9c4d) reverts a lot of changes we made since. i assume you just copied all your code over the current master branch, and not the version of the master branch as it was when you made the changes. you need to find the version of scanpy that you downloaded before you made your changes and modify that one to have just the changes you want to commit. otherwise we have no idea what your actual changes are.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:310,integrability,version,version,310,"seems like you did something wrong. the commit you added (74540cc133ca9cfe0744ca9d3b250454a76a9c4d) reverts a lot of changes we made since. i assume you just copied all your code over the current master branch, and not the version of the master branch as it was when you made the changes. you need to find the version of scanpy that you downloaded before you made your changes and modify that one to have just the changes you want to commit. otherwise we have no idea what your actual changes are.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:223,modifiability,version,version,223,"seems like you did something wrong. the commit you added (74540cc133ca9cfe0744ca9d3b250454a76a9c4d) reverts a lot of changes we made since. i assume you just copied all your code over the current master branch, and not the version of the master branch as it was when you made the changes. you need to find the version of scanpy that you downloaded before you made your changes and modify that one to have just the changes you want to commit. otherwise we have no idea what your actual changes are.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:310,modifiability,version,version,310,"seems like you did something wrong. the commit you added (74540cc133ca9cfe0744ca9d3b250454a76a9c4d) reverts a lot of changes we made since. i assume you just copied all your code over the current master branch, and not the version of the master branch as it was when you made the changes. you need to find the version of scanpy that you downloaded before you made your changes and modify that one to have just the changes you want to commit. otherwise we have no idea what your actual changes are.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:381,security,modif,modify,381,"seems like you did something wrong. the commit you added (74540cc133ca9cfe0744ca9d3b250454a76a9c4d) reverts a lot of changes we made since. i assume you just copied all your code over the current master branch, and not the version of the master branch as it was when you made the changes. you need to find the version of scanpy that you downloaded before you made your changes and modify that one to have just the changes you want to commit. otherwise we have no idea what your actual changes are.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:674,availability,down,downloaded,674,"Hi,. I have modified version 1.4, but i think git only allow latest version to. fork. Is there any other simple way, so that i can share my code for Scanpy. version 1.4. Thanks,. Khalid. On Sat, May 4, 2019 at 2:26 AM Philipp A. <notifications@github.com> wrote:. > seems like you did something wrong. the commit you added (74540cc. > <https://github.com/theislab/scanpy/commit/74540cc133ca9cfe0744ca9d3b250454a76a9c4d>). > reverts a lot of changes we made since. >. > i assume you just copied all your code over the current master branch, and. > not the version of the master branch as it was when you made the changes. >. > you need to find the version of scanpy that you downloaded before you made. > your changes and modify that one to have just the changes you want to. > commit. otherwise we have no idea what your actual changes are. >. > â€”. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/630#issuecomment-489194292>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOBLBVFOZLO23ZCULELPTR7U3ANCNFSM4HKUCBXA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:21,deployability,version,version,21,"Hi,. I have modified version 1.4, but i think git only allow latest version to. fork. Is there any other simple way, so that i can share my code for Scanpy. version 1.4. Thanks,. Khalid. On Sat, May 4, 2019 at 2:26 AM Philipp A. <notifications@github.com> wrote:. > seems like you did something wrong. the commit you added (74540cc. > <https://github.com/theislab/scanpy/commit/74540cc133ca9cfe0744ca9d3b250454a76a9c4d>). > reverts a lot of changes we made since. >. > i assume you just copied all your code over the current master branch, and. > not the version of the master branch as it was when you made the changes. >. > you need to find the version of scanpy that you downloaded before you made. > your changes and modify that one to have just the changes you want to. > commit. otherwise we have no idea what your actual changes are. >. > â€”. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/630#issuecomment-489194292>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOBLBVFOZLO23ZCULELPTR7U3ANCNFSM4HKUCBXA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:68,deployability,version,version,68,"Hi,. I have modified version 1.4, but i think git only allow latest version to. fork. Is there any other simple way, so that i can share my code for Scanpy. version 1.4. Thanks,. Khalid. On Sat, May 4, 2019 at 2:26 AM Philipp A. <notifications@github.com> wrote:. > seems like you did something wrong. the commit you added (74540cc. > <https://github.com/theislab/scanpy/commit/74540cc133ca9cfe0744ca9d3b250454a76a9c4d>). > reverts a lot of changes we made since. >. > i assume you just copied all your code over the current master branch, and. > not the version of the master branch as it was when you made the changes. >. > you need to find the version of scanpy that you downloaded before you made. > your changes and modify that one to have just the changes you want to. > commit. otherwise we have no idea what your actual changes are. >. > â€”. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/630#issuecomment-489194292>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOBLBVFOZLO23ZCULELPTR7U3ANCNFSM4HKUCBXA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:157,deployability,version,version,157,"Hi,. I have modified version 1.4, but i think git only allow latest version to. fork. Is there any other simple way, so that i can share my code for Scanpy. version 1.4. Thanks,. Khalid. On Sat, May 4, 2019 at 2:26 AM Philipp A. <notifications@github.com> wrote:. > seems like you did something wrong. the commit you added (74540cc. > <https://github.com/theislab/scanpy/commit/74540cc133ca9cfe0744ca9d3b250454a76a9c4d>). > reverts a lot of changes we made since. >. > i assume you just copied all your code over the current master branch, and. > not the version of the master branch as it was when you made the changes. >. > you need to find the version of scanpy that you downloaded before you made. > your changes and modify that one to have just the changes you want to. > commit. otherwise we have no idea what your actual changes are. >. > â€”. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/630#issuecomment-489194292>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOBLBVFOZLO23ZCULELPTR7U3ANCNFSM4HKUCBXA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:555,deployability,version,version,555,"Hi,. I have modified version 1.4, but i think git only allow latest version to. fork. Is there any other simple way, so that i can share my code for Scanpy. version 1.4. Thanks,. Khalid. On Sat, May 4, 2019 at 2:26 AM Philipp A. <notifications@github.com> wrote:. > seems like you did something wrong. the commit you added (74540cc. > <https://github.com/theislab/scanpy/commit/74540cc133ca9cfe0744ca9d3b250454a76a9c4d>). > reverts a lot of changes we made since. >. > i assume you just copied all your code over the current master branch, and. > not the version of the master branch as it was when you made the changes. >. > you need to find the version of scanpy that you downloaded before you made. > your changes and modify that one to have just the changes you want to. > commit. otherwise we have no idea what your actual changes are. >. > â€”. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/630#issuecomment-489194292>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOBLBVFOZLO23ZCULELPTR7U3ANCNFSM4HKUCBXA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:647,deployability,version,version,647,"Hi,. I have modified version 1.4, but i think git only allow latest version to. fork. Is there any other simple way, so that i can share my code for Scanpy. version 1.4. Thanks,. Khalid. On Sat, May 4, 2019 at 2:26 AM Philipp A. <notifications@github.com> wrote:. > seems like you did something wrong. the commit you added (74540cc. > <https://github.com/theislab/scanpy/commit/74540cc133ca9cfe0744ca9d3b250454a76a9c4d>). > reverts a lot of changes we made since. >. > i assume you just copied all your code over the current master branch, and. > not the version of the master branch as it was when you made the changes. >. > you need to find the version of scanpy that you downloaded before you made. > your changes and modify that one to have just the changes you want to. > commit. otherwise we have no idea what your actual changes are. >. > â€”. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/630#issuecomment-489194292>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOBLBVFOZLO23ZCULELPTR7U3ANCNFSM4HKUCBXA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:517,energy efficiency,current,current,517,"Hi,. I have modified version 1.4, but i think git only allow latest version to. fork. Is there any other simple way, so that i can share my code for Scanpy. version 1.4. Thanks,. Khalid. On Sat, May 4, 2019 at 2:26 AM Philipp A. <notifications@github.com> wrote:. > seems like you did something wrong. the commit you added (74540cc. > <https://github.com/theislab/scanpy/commit/74540cc133ca9cfe0744ca9d3b250454a76a9c4d>). > reverts a lot of changes we made since. >. > i assume you just copied all your code over the current master branch, and. > not the version of the master branch as it was when you made the changes. >. > you need to find the version of scanpy that you downloaded before you made. > your changes and modify that one to have just the changes you want to. > commit. otherwise we have no idea what your actual changes are. >. > â€”. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/630#issuecomment-489194292>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOBLBVFOZLO23ZCULELPTR7U3ANCNFSM4HKUCBXA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:21,integrability,version,version,21,"Hi,. I have modified version 1.4, but i think git only allow latest version to. fork. Is there any other simple way, so that i can share my code for Scanpy. version 1.4. Thanks,. Khalid. On Sat, May 4, 2019 at 2:26 AM Philipp A. <notifications@github.com> wrote:. > seems like you did something wrong. the commit you added (74540cc. > <https://github.com/theislab/scanpy/commit/74540cc133ca9cfe0744ca9d3b250454a76a9c4d>). > reverts a lot of changes we made since. >. > i assume you just copied all your code over the current master branch, and. > not the version of the master branch as it was when you made the changes. >. > you need to find the version of scanpy that you downloaded before you made. > your changes and modify that one to have just the changes you want to. > commit. otherwise we have no idea what your actual changes are. >. > â€”. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/630#issuecomment-489194292>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOBLBVFOZLO23ZCULELPTR7U3ANCNFSM4HKUCBXA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:68,integrability,version,version,68,"Hi,. I have modified version 1.4, but i think git only allow latest version to. fork. Is there any other simple way, so that i can share my code for Scanpy. version 1.4. Thanks,. Khalid. On Sat, May 4, 2019 at 2:26 AM Philipp A. <notifications@github.com> wrote:. > seems like you did something wrong. the commit you added (74540cc. > <https://github.com/theislab/scanpy/commit/74540cc133ca9cfe0744ca9d3b250454a76a9c4d>). > reverts a lot of changes we made since. >. > i assume you just copied all your code over the current master branch, and. > not the version of the master branch as it was when you made the changes. >. > you need to find the version of scanpy that you downloaded before you made. > your changes and modify that one to have just the changes you want to. > commit. otherwise we have no idea what your actual changes are. >. > â€”. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/630#issuecomment-489194292>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOBLBVFOZLO23ZCULELPTR7U3ANCNFSM4HKUCBXA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:157,integrability,version,version,157,"Hi,. I have modified version 1.4, but i think git only allow latest version to. fork. Is there any other simple way, so that i can share my code for Scanpy. version 1.4. Thanks,. Khalid. On Sat, May 4, 2019 at 2:26 AM Philipp A. <notifications@github.com> wrote:. > seems like you did something wrong. the commit you added (74540cc. > <https://github.com/theislab/scanpy/commit/74540cc133ca9cfe0744ca9d3b250454a76a9c4d>). > reverts a lot of changes we made since. >. > i assume you just copied all your code over the current master branch, and. > not the version of the master branch as it was when you made the changes. >. > you need to find the version of scanpy that you downloaded before you made. > your changes and modify that one to have just the changes you want to. > commit. otherwise we have no idea what your actual changes are. >. > â€”. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/630#issuecomment-489194292>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOBLBVFOZLO23ZCULELPTR7U3ANCNFSM4HKUCBXA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:555,integrability,version,version,555,"Hi,. I have modified version 1.4, but i think git only allow latest version to. fork. Is there any other simple way, so that i can share my code for Scanpy. version 1.4. Thanks,. Khalid. On Sat, May 4, 2019 at 2:26 AM Philipp A. <notifications@github.com> wrote:. > seems like you did something wrong. the commit you added (74540cc. > <https://github.com/theislab/scanpy/commit/74540cc133ca9cfe0744ca9d3b250454a76a9c4d>). > reverts a lot of changes we made since. >. > i assume you just copied all your code over the current master branch, and. > not the version of the master branch as it was when you made the changes. >. > you need to find the version of scanpy that you downloaded before you made. > your changes and modify that one to have just the changes you want to. > commit. otherwise we have no idea what your actual changes are. >. > â€”. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/630#issuecomment-489194292>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOBLBVFOZLO23ZCULELPTR7U3ANCNFSM4HKUCBXA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:647,integrability,version,version,647,"Hi,. I have modified version 1.4, but i think git only allow latest version to. fork. Is there any other simple way, so that i can share my code for Scanpy. version 1.4. Thanks,. Khalid. On Sat, May 4, 2019 at 2:26 AM Philipp A. <notifications@github.com> wrote:. > seems like you did something wrong. the commit you added (74540cc. > <https://github.com/theislab/scanpy/commit/74540cc133ca9cfe0744ca9d3b250454a76a9c4d>). > reverts a lot of changes we made since. >. > i assume you just copied all your code over the current master branch, and. > not the version of the master branch as it was when you made the changes. >. > you need to find the version of scanpy that you downloaded before you made. > your changes and modify that one to have just the changes you want to. > commit. otherwise we have no idea what your actual changes are. >. > â€”. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/630#issuecomment-489194292>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOBLBVFOZLO23ZCULELPTR7U3ANCNFSM4HKUCBXA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:131,interoperability,share,share,131,"Hi,. I have modified version 1.4, but i think git only allow latest version to. fork. Is there any other simple way, so that i can share my code for Scanpy. version 1.4. Thanks,. Khalid. On Sat, May 4, 2019 at 2:26 AM Philipp A. <notifications@github.com> wrote:. > seems like you did something wrong. the commit you added (74540cc. > <https://github.com/theislab/scanpy/commit/74540cc133ca9cfe0744ca9d3b250454a76a9c4d>). > reverts a lot of changes we made since. >. > i assume you just copied all your code over the current master branch, and. > not the version of the master branch as it was when you made the changes. >. > you need to find the version of scanpy that you downloaded before you made. > your changes and modify that one to have just the changes you want to. > commit. otherwise we have no idea what your actual changes are. >. > â€”. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/630#issuecomment-489194292>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOBLBVFOZLO23ZCULELPTR7U3ANCNFSM4HKUCBXA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:21,modifiability,version,version,21,"Hi,. I have modified version 1.4, but i think git only allow latest version to. fork. Is there any other simple way, so that i can share my code for Scanpy. version 1.4. Thanks,. Khalid. On Sat, May 4, 2019 at 2:26 AM Philipp A. <notifications@github.com> wrote:. > seems like you did something wrong. the commit you added (74540cc. > <https://github.com/theislab/scanpy/commit/74540cc133ca9cfe0744ca9d3b250454a76a9c4d>). > reverts a lot of changes we made since. >. > i assume you just copied all your code over the current master branch, and. > not the version of the master branch as it was when you made the changes. >. > you need to find the version of scanpy that you downloaded before you made. > your changes and modify that one to have just the changes you want to. > commit. otherwise we have no idea what your actual changes are. >. > â€”. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/630#issuecomment-489194292>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOBLBVFOZLO23ZCULELPTR7U3ANCNFSM4HKUCBXA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:68,modifiability,version,version,68,"Hi,. I have modified version 1.4, but i think git only allow latest version to. fork. Is there any other simple way, so that i can share my code for Scanpy. version 1.4. Thanks,. Khalid. On Sat, May 4, 2019 at 2:26 AM Philipp A. <notifications@github.com> wrote:. > seems like you did something wrong. the commit you added (74540cc. > <https://github.com/theislab/scanpy/commit/74540cc133ca9cfe0744ca9d3b250454a76a9c4d>). > reverts a lot of changes we made since. >. > i assume you just copied all your code over the current master branch, and. > not the version of the master branch as it was when you made the changes. >. > you need to find the version of scanpy that you downloaded before you made. > your changes and modify that one to have just the changes you want to. > commit. otherwise we have no idea what your actual changes are. >. > â€”. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/630#issuecomment-489194292>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOBLBVFOZLO23ZCULELPTR7U3ANCNFSM4HKUCBXA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:157,modifiability,version,version,157,"Hi,. I have modified version 1.4, but i think git only allow latest version to. fork. Is there any other simple way, so that i can share my code for Scanpy. version 1.4. Thanks,. Khalid. On Sat, May 4, 2019 at 2:26 AM Philipp A. <notifications@github.com> wrote:. > seems like you did something wrong. the commit you added (74540cc. > <https://github.com/theislab/scanpy/commit/74540cc133ca9cfe0744ca9d3b250454a76a9c4d>). > reverts a lot of changes we made since. >. > i assume you just copied all your code over the current master branch, and. > not the version of the master branch as it was when you made the changes. >. > you need to find the version of scanpy that you downloaded before you made. > your changes and modify that one to have just the changes you want to. > commit. otherwise we have no idea what your actual changes are. >. > â€”. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/630#issuecomment-489194292>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOBLBVFOZLO23ZCULELPTR7U3ANCNFSM4HKUCBXA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:555,modifiability,version,version,555,"Hi,. I have modified version 1.4, but i think git only allow latest version to. fork. Is there any other simple way, so that i can share my code for Scanpy. version 1.4. Thanks,. Khalid. On Sat, May 4, 2019 at 2:26 AM Philipp A. <notifications@github.com> wrote:. > seems like you did something wrong. the commit you added (74540cc. > <https://github.com/theislab/scanpy/commit/74540cc133ca9cfe0744ca9d3b250454a76a9c4d>). > reverts a lot of changes we made since. >. > i assume you just copied all your code over the current master branch, and. > not the version of the master branch as it was when you made the changes. >. > you need to find the version of scanpy that you downloaded before you made. > your changes and modify that one to have just the changes you want to. > commit. otherwise we have no idea what your actual changes are. >. > â€”. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/630#issuecomment-489194292>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOBLBVFOZLO23ZCULELPTR7U3ANCNFSM4HKUCBXA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:647,modifiability,version,version,647,"Hi,. I have modified version 1.4, but i think git only allow latest version to. fork. Is there any other simple way, so that i can share my code for Scanpy. version 1.4. Thanks,. Khalid. On Sat, May 4, 2019 at 2:26 AM Philipp A. <notifications@github.com> wrote:. > seems like you did something wrong. the commit you added (74540cc. > <https://github.com/theislab/scanpy/commit/74540cc133ca9cfe0744ca9d3b250454a76a9c4d>). > reverts a lot of changes we made since. >. > i assume you just copied all your code over the current master branch, and. > not the version of the master branch as it was when you made the changes. >. > you need to find the version of scanpy that you downloaded before you made. > your changes and modify that one to have just the changes you want to. > commit. otherwise we have no idea what your actual changes are. >. > â€”. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/630#issuecomment-489194292>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOBLBVFOZLO23ZCULELPTR7U3ANCNFSM4HKUCBXA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:12,security,modif,modified,12,"Hi,. I have modified version 1.4, but i think git only allow latest version to. fork. Is there any other simple way, so that i can share my code for Scanpy. version 1.4. Thanks,. Khalid. On Sat, May 4, 2019 at 2:26 AM Philipp A. <notifications@github.com> wrote:. > seems like you did something wrong. the commit you added (74540cc. > <https://github.com/theislab/scanpy/commit/74540cc133ca9cfe0744ca9d3b250454a76a9c4d>). > reverts a lot of changes we made since. >. > i assume you just copied all your code over the current master branch, and. > not the version of the master branch as it was when you made the changes. >. > you need to find the version of scanpy that you downloaded before you made. > your changes and modify that one to have just the changes you want to. > commit. otherwise we have no idea what your actual changes are. >. > â€”. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/630#issuecomment-489194292>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOBLBVFOZLO23ZCULELPTR7U3ANCNFSM4HKUCBXA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:721,security,modif,modify,721,"Hi,. I have modified version 1.4, but i think git only allow latest version to. fork. Is there any other simple way, so that i can share my code for Scanpy. version 1.4. Thanks,. Khalid. On Sat, May 4, 2019 at 2:26 AM Philipp A. <notifications@github.com> wrote:. > seems like you did something wrong. the commit you added (74540cc. > <https://github.com/theislab/scanpy/commit/74540cc133ca9cfe0744ca9d3b250454a76a9c4d>). > reverts a lot of changes we made since. >. > i assume you just copied all your code over the current master branch, and. > not the version of the master branch as it was when you made the changes. >. > you need to find the version of scanpy that you downloaded before you made. > your changes and modify that one to have just the changes you want to. > commit. otherwise we have no idea what your actual changes are. >. > â€”. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/630#issuecomment-489194292>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOBLBVFOZLO23ZCULELPTR7U3ANCNFSM4HKUCBXA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:886,security,auth,authored,886,"Hi,. I have modified version 1.4, but i think git only allow latest version to. fork. Is there any other simple way, so that i can share my code for Scanpy. version 1.4. Thanks,. Khalid. On Sat, May 4, 2019 at 2:26 AM Philipp A. <notifications@github.com> wrote:. > seems like you did something wrong. the commit you added (74540cc. > <https://github.com/theislab/scanpy/commit/74540cc133ca9cfe0744ca9d3b250454a76a9c4d>). > reverts a lot of changes we made since. >. > i assume you just copied all your code over the current master branch, and. > not the version of the master branch as it was when you made the changes. >. > you need to find the version of scanpy that you downloaded before you made. > your changes and modify that one to have just the changes you want to. > commit. otherwise we have no idea what your actual changes are. >. > â€”. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/630#issuecomment-489194292>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOBLBVFOZLO23ZCULELPTR7U3ANCNFSM4HKUCBXA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:1101,security,auth,auth,1101,"Hi,. I have modified version 1.4, but i think git only allow latest version to. fork. Is there any other simple way, so that i can share my code for Scanpy. version 1.4. Thanks,. Khalid. On Sat, May 4, 2019 at 2:26 AM Philipp A. <notifications@github.com> wrote:. > seems like you did something wrong. the commit you added (74540cc. > <https://github.com/theislab/scanpy/commit/74540cc133ca9cfe0744ca9d3b250454a76a9c4d>). > reverts a lot of changes we made since. >. > i assume you just copied all your code over the current master branch, and. > not the version of the master branch as it was when you made the changes. >. > you need to find the version of scanpy that you downloaded before you made. > your changes and modify that one to have just the changes you want to. > commit. otherwise we have no idea what your actual changes are. >. > â€”. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/630#issuecomment-489194292>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOBLBVFOZLO23ZCULELPTR7U3ANCNFSM4HKUCBXA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:105,testability,simpl,simple,105,"Hi,. I have modified version 1.4, but i think git only allow latest version to. fork. Is there any other simple way, so that i can share my code for Scanpy. version 1.4. Thanks,. Khalid. On Sat, May 4, 2019 at 2:26 AM Philipp A. <notifications@github.com> wrote:. > seems like you did something wrong. the commit you added (74540cc. > <https://github.com/theislab/scanpy/commit/74540cc133ca9cfe0744ca9d3b250454a76a9c4d>). > reverts a lot of changes we made since. >. > i assume you just copied all your code over the current master branch, and. > not the version of the master branch as it was when you made the changes. >. > you need to find the version of scanpy that you downloaded before you made. > your changes and modify that one to have just the changes you want to. > commit. otherwise we have no idea what your actual changes are. >. > â€”. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/630#issuecomment-489194292>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOBLBVFOZLO23ZCULELPTR7U3ANCNFSM4HKUCBXA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:105,usability,simpl,simple,105,"Hi,. I have modified version 1.4, but i think git only allow latest version to. fork. Is there any other simple way, so that i can share my code for Scanpy. version 1.4. Thanks,. Khalid. On Sat, May 4, 2019 at 2:26 AM Philipp A. <notifications@github.com> wrote:. > seems like you did something wrong. the commit you added (74540cc. > <https://github.com/theislab/scanpy/commit/74540cc133ca9cfe0744ca9d3b250454a76a9c4d>). > reverts a lot of changes we made since. >. > i assume you just copied all your code over the current master branch, and. > not the version of the master branch as it was when you made the changes. >. > you need to find the version of scanpy that you downloaded before you made. > your changes and modify that one to have just the changes you want to. > commit. otherwise we have no idea what your actual changes are. >. > â€”. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/630#issuecomment-489194292>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOBLBVFOZLO23ZCULELPTR7U3ANCNFSM4HKUCBXA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:325,availability,cluster,clustering,325,"yes! 1. make sure you have all of your modified copy somewhere outside of the cloned directory! weâ€™re going to destroy all changes inside of that directory! 2. Destroy all changes and go back to 1.4: `git reset --hard 1.4` (if itâ€™s really 1.4 and not e.g. 1.4.1). 3. Make a new branch based on 1.4: `git checkout -b weighted-clustering`. 4. Copy your changes over again. 5. Check if all is right: `git diff` should only tell you about your modified files, not other junk. 6. Add all files you changed individually (not `git add .` or `git add -A`, but `git add scanpy/file1.py scanpy/file2.py ...`). 7. `git diff` should now say that thereâ€™s a few staged files (your modifications), and no other added or modified files. 8. Commit your changes `git commit -m 'your commit message'` and push them `git push`. Now you can make a new PR with just your changes in it: https://github.com/theislab/scanpy/compare/master...Khalid-Usman:weighted-clustering. Make sure that you only see your changes on the lower part of that page before hitting the â€œCreate Pull Requestâ€ button",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:938,availability,cluster,clustering,938,"yes! 1. make sure you have all of your modified copy somewhere outside of the cloned directory! weâ€™re going to destroy all changes inside of that directory! 2. Destroy all changes and go back to 1.4: `git reset --hard 1.4` (if itâ€™s really 1.4 and not e.g. 1.4.1). 3. Make a new branch based on 1.4: `git checkout -b weighted-clustering`. 4. Copy your changes over again. 5. Check if all is right: `git diff` should only tell you about your modified files, not other junk. 6. Add all files you changed individually (not `git add .` or `git add -A`, but `git add scanpy/file1.py scanpy/file2.py ...`). 7. `git diff` should now say that thereâ€™s a few staged files (your modifications), and no other added or modified files. 8. Commit your changes `git commit -m 'your commit message'` and push them `git push`. Now you can make a new PR with just your changes in it: https://github.com/theislab/scanpy/compare/master...Khalid-Usman:weighted-clustering. Make sure that you only see your changes on the lower part of that page before hitting the â€œCreate Pull Requestâ€ button",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:325,deployability,cluster,clustering,325,"yes! 1. make sure you have all of your modified copy somewhere outside of the cloned directory! weâ€™re going to destroy all changes inside of that directory! 2. Destroy all changes and go back to 1.4: `git reset --hard 1.4` (if itâ€™s really 1.4 and not e.g. 1.4.1). 3. Make a new branch based on 1.4: `git checkout -b weighted-clustering`. 4. Copy your changes over again. 5. Check if all is right: `git diff` should only tell you about your modified files, not other junk. 6. Add all files you changed individually (not `git add .` or `git add -A`, but `git add scanpy/file1.py scanpy/file2.py ...`). 7. `git diff` should now say that thereâ€™s a few staged files (your modifications), and no other added or modified files. 8. Commit your changes `git commit -m 'your commit message'` and push them `git push`. Now you can make a new PR with just your changes in it: https://github.com/theislab/scanpy/compare/master...Khalid-Usman:weighted-clustering. Make sure that you only see your changes on the lower part of that page before hitting the â€œCreate Pull Requestâ€ button",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:648,deployability,stage,staged,648,"yes! 1. make sure you have all of your modified copy somewhere outside of the cloned directory! weâ€™re going to destroy all changes inside of that directory! 2. Destroy all changes and go back to 1.4: `git reset --hard 1.4` (if itâ€™s really 1.4 and not e.g. 1.4.1). 3. Make a new branch based on 1.4: `git checkout -b weighted-clustering`. 4. Copy your changes over again. 5. Check if all is right: `git diff` should only tell you about your modified files, not other junk. 6. Add all files you changed individually (not `git add .` or `git add -A`, but `git add scanpy/file1.py scanpy/file2.py ...`). 7. `git diff` should now say that thereâ€™s a few staged files (your modifications), and no other added or modified files. 8. Commit your changes `git commit -m 'your commit message'` and push them `git push`. Now you can make a new PR with just your changes in it: https://github.com/theislab/scanpy/compare/master...Khalid-Usman:weighted-clustering. Make sure that you only see your changes on the lower part of that page before hitting the â€œCreate Pull Requestâ€ button",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:938,deployability,cluster,clustering,938,"yes! 1. make sure you have all of your modified copy somewhere outside of the cloned directory! weâ€™re going to destroy all changes inside of that directory! 2. Destroy all changes and go back to 1.4: `git reset --hard 1.4` (if itâ€™s really 1.4 and not e.g. 1.4.1). 3. Make a new branch based on 1.4: `git checkout -b weighted-clustering`. 4. Copy your changes over again. 5. Check if all is right: `git diff` should only tell you about your modified files, not other junk. 6. Add all files you changed individually (not `git add .` or `git add -A`, but `git add scanpy/file1.py scanpy/file2.py ...`). 7. `git diff` should now say that thereâ€™s a few staged files (your modifications), and no other added or modified files. 8. Commit your changes `git commit -m 'your commit message'` and push them `git push`. Now you can make a new PR with just your changes in it: https://github.com/theislab/scanpy/compare/master...Khalid-Usman:weighted-clustering. Make sure that you only see your changes on the lower part of that page before hitting the â€œCreate Pull Requestâ€ button",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:772,integrability,messag,message,772,"yes! 1. make sure you have all of your modified copy somewhere outside of the cloned directory! weâ€™re going to destroy all changes inside of that directory! 2. Destroy all changes and go back to 1.4: `git reset --hard 1.4` (if itâ€™s really 1.4 and not e.g. 1.4.1). 3. Make a new branch based on 1.4: `git checkout -b weighted-clustering`. 4. Copy your changes over again. 5. Check if all is right: `git diff` should only tell you about your modified files, not other junk. 6. Add all files you changed individually (not `git add .` or `git add -A`, but `git add scanpy/file1.py scanpy/file2.py ...`). 7. `git diff` should now say that thereâ€™s a few staged files (your modifications), and no other added or modified files. 8. Commit your changes `git commit -m 'your commit message'` and push them `git push`. Now you can make a new PR with just your changes in it: https://github.com/theislab/scanpy/compare/master...Khalid-Usman:weighted-clustering. Make sure that you only see your changes on the lower part of that page before hitting the â€œCreate Pull Requestâ€ button",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:772,interoperability,messag,message,772,"yes! 1. make sure you have all of your modified copy somewhere outside of the cloned directory! weâ€™re going to destroy all changes inside of that directory! 2. Destroy all changes and go back to 1.4: `git reset --hard 1.4` (if itâ€™s really 1.4 and not e.g. 1.4.1). 3. Make a new branch based on 1.4: `git checkout -b weighted-clustering`. 4. Copy your changes over again. 5. Check if all is right: `git diff` should only tell you about your modified files, not other junk. 6. Add all files you changed individually (not `git add .` or `git add -A`, but `git add scanpy/file1.py scanpy/file2.py ...`). 7. `git diff` should now say that thereâ€™s a few staged files (your modifications), and no other added or modified files. 8. Commit your changes `git commit -m 'your commit message'` and push them `git push`. Now you can make a new PR with just your changes in it: https://github.com/theislab/scanpy/compare/master...Khalid-Usman:weighted-clustering. Make sure that you only see your changes on the lower part of that page before hitting the â€œCreate Pull Requestâ€ button",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:39,security,modif,modified,39,"yes! 1. make sure you have all of your modified copy somewhere outside of the cloned directory! weâ€™re going to destroy all changes inside of that directory! 2. Destroy all changes and go back to 1.4: `git reset --hard 1.4` (if itâ€™s really 1.4 and not e.g. 1.4.1). 3. Make a new branch based on 1.4: `git checkout -b weighted-clustering`. 4. Copy your changes over again. 5. Check if all is right: `git diff` should only tell you about your modified files, not other junk. 6. Add all files you changed individually (not `git add .` or `git add -A`, but `git add scanpy/file1.py scanpy/file2.py ...`). 7. `git diff` should now say that thereâ€™s a few staged files (your modifications), and no other added or modified files. 8. Commit your changes `git commit -m 'your commit message'` and push them `git push`. Now you can make a new PR with just your changes in it: https://github.com/theislab/scanpy/compare/master...Khalid-Usman:weighted-clustering. Make sure that you only see your changes on the lower part of that page before hitting the â€œCreate Pull Requestâ€ button",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:440,security,modif,modified,440,"yes! 1. make sure you have all of your modified copy somewhere outside of the cloned directory! weâ€™re going to destroy all changes inside of that directory! 2. Destroy all changes and go back to 1.4: `git reset --hard 1.4` (if itâ€™s really 1.4 and not e.g. 1.4.1). 3. Make a new branch based on 1.4: `git checkout -b weighted-clustering`. 4. Copy your changes over again. 5. Check if all is right: `git diff` should only tell you about your modified files, not other junk. 6. Add all files you changed individually (not `git add .` or `git add -A`, but `git add scanpy/file1.py scanpy/file2.py ...`). 7. `git diff` should now say that thereâ€™s a few staged files (your modifications), and no other added or modified files. 8. Commit your changes `git commit -m 'your commit message'` and push them `git push`. Now you can make a new PR with just your changes in it: https://github.com/theislab/scanpy/compare/master...Khalid-Usman:weighted-clustering. Make sure that you only see your changes on the lower part of that page before hitting the â€œCreate Pull Requestâ€ button",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:667,security,modif,modifications,667,"yes! 1. make sure you have all of your modified copy somewhere outside of the cloned directory! weâ€™re going to destroy all changes inside of that directory! 2. Destroy all changes and go back to 1.4: `git reset --hard 1.4` (if itâ€™s really 1.4 and not e.g. 1.4.1). 3. Make a new branch based on 1.4: `git checkout -b weighted-clustering`. 4. Copy your changes over again. 5. Check if all is right: `git diff` should only tell you about your modified files, not other junk. 6. Add all files you changed individually (not `git add .` or `git add -A`, but `git add scanpy/file1.py scanpy/file2.py ...`). 7. `git diff` should now say that thereâ€™s a few staged files (your modifications), and no other added or modified files. 8. Commit your changes `git commit -m 'your commit message'` and push them `git push`. Now you can make a new PR with just your changes in it: https://github.com/theislab/scanpy/compare/master...Khalid-Usman:weighted-clustering. Make sure that you only see your changes on the lower part of that page before hitting the â€œCreate Pull Requestâ€ button",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:705,security,modif,modified,705,"yes! 1. make sure you have all of your modified copy somewhere outside of the cloned directory! weâ€™re going to destroy all changes inside of that directory! 2. Destroy all changes and go back to 1.4: `git reset --hard 1.4` (if itâ€™s really 1.4 and not e.g. 1.4.1). 3. Make a new branch based on 1.4: `git checkout -b weighted-clustering`. 4. Copy your changes over again. 5. Check if all is right: `git diff` should only tell you about your modified files, not other junk. 6. Add all files you changed individually (not `git add .` or `git add -A`, but `git add scanpy/file1.py scanpy/file2.py ...`). 7. `git diff` should now say that thereâ€™s a few staged files (your modifications), and no other added or modified files. 8. Commit your changes `git commit -m 'your commit message'` and push them `git push`. Now you can make a new PR with just your changes in it: https://github.com/theislab/scanpy/compare/master...Khalid-Usman:weighted-clustering. Make sure that you only see your changes on the lower part of that page before hitting the â€œCreate Pull Requestâ€ button",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:493,availability,cluster,clustering,493,"Is it fine or need to do anything else before make a pull request ? Thanks. On Tue, May 14, 2019 at 4:05 PM Philipp A. <notifications@github.com> wrote:. > yes! >. > 1. make sure you have all your modified copy somewhere outside of the. > cloned directory! weâ€™re going to destroy all changes inside of that. > directory! > 2. Destroy all changes and go back to 1.4: git reset --hard 1.4 (if. > itâ€™s really 1.4 and not e.g. 1.4.1). > 3. Make a new branch based on 1.4: git checkout -b weighted-clustering. > 4. Copy your changes over again. > 5. Add all files you changed individually (not git add . or git add -A,. > but git add scanpy/file1.py scanpy/file2.py ...). > 6. git commit -m 'your commit message'. >. > Now you can make a new PR with just your changes in it:. > master...Khalid-Usman:weighted-clustering. > <https://github.com/theislab/scanpy/compare/master...Khalid-Usman:weighted-clustering>. >. > Make sure that you only see your changes on the lower part of that page. > before hitting the â€œCreate Pull Requestâ€ button. >. > â€”. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOA2G7HECFLNHTNNZ33PVJXFZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVKTS6Y#issuecomment-492124539>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOHOOABYXSQL6HLZUN3PVJXFZANCNFSM4HKUCBXA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:804,availability,cluster,clustering,804,"Is it fine or need to do anything else before make a pull request ? Thanks. On Tue, May 14, 2019 at 4:05 PM Philipp A. <notifications@github.com> wrote:. > yes! >. > 1. make sure you have all your modified copy somewhere outside of the. > cloned directory! weâ€™re going to destroy all changes inside of that. > directory! > 2. Destroy all changes and go back to 1.4: git reset --hard 1.4 (if. > itâ€™s really 1.4 and not e.g. 1.4.1). > 3. Make a new branch based on 1.4: git checkout -b weighted-clustering. > 4. Copy your changes over again. > 5. Add all files you changed individually (not git add . or git add -A,. > but git add scanpy/file1.py scanpy/file2.py ...). > 6. git commit -m 'your commit message'. >. > Now you can make a new PR with just your changes in it:. > master...Khalid-Usman:weighted-clustering. > <https://github.com/theislab/scanpy/compare/master...Khalid-Usman:weighted-clustering>. >. > Make sure that you only see your changes on the lower part of that page. > before hitting the â€œCreate Pull Requestâ€ button. >. > â€”. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOA2G7HECFLNHTNNZ33PVJXFZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVKTS6Y#issuecomment-492124539>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOHOOABYXSQL6HLZUN3PVJXFZANCNFSM4HKUCBXA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:893,availability,cluster,clustering,893,"Is it fine or need to do anything else before make a pull request ? Thanks. On Tue, May 14, 2019 at 4:05 PM Philipp A. <notifications@github.com> wrote:. > yes! >. > 1. make sure you have all your modified copy somewhere outside of the. > cloned directory! weâ€™re going to destroy all changes inside of that. > directory! > 2. Destroy all changes and go back to 1.4: git reset --hard 1.4 (if. > itâ€™s really 1.4 and not e.g. 1.4.1). > 3. Make a new branch based on 1.4: git checkout -b weighted-clustering. > 4. Copy your changes over again. > 5. Add all files you changed individually (not git add . or git add -A,. > but git add scanpy/file1.py scanpy/file2.py ...). > 6. git commit -m 'your commit message'. >. > Now you can make a new PR with just your changes in it:. > master...Khalid-Usman:weighted-clustering. > <https://github.com/theislab/scanpy/compare/master...Khalid-Usman:weighted-clustering>. >. > Make sure that you only see your changes on the lower part of that page. > before hitting the â€œCreate Pull Requestâ€ button. >. > â€”. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOA2G7HECFLNHTNNZ33PVJXFZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVKTS6Y#issuecomment-492124539>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOHOOABYXSQL6HLZUN3PVJXFZANCNFSM4HKUCBXA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:493,deployability,cluster,clustering,493,"Is it fine or need to do anything else before make a pull request ? Thanks. On Tue, May 14, 2019 at 4:05 PM Philipp A. <notifications@github.com> wrote:. > yes! >. > 1. make sure you have all your modified copy somewhere outside of the. > cloned directory! weâ€™re going to destroy all changes inside of that. > directory! > 2. Destroy all changes and go back to 1.4: git reset --hard 1.4 (if. > itâ€™s really 1.4 and not e.g. 1.4.1). > 3. Make a new branch based on 1.4: git checkout -b weighted-clustering. > 4. Copy your changes over again. > 5. Add all files you changed individually (not git add . or git add -A,. > but git add scanpy/file1.py scanpy/file2.py ...). > 6. git commit -m 'your commit message'. >. > Now you can make a new PR with just your changes in it:. > master...Khalid-Usman:weighted-clustering. > <https://github.com/theislab/scanpy/compare/master...Khalid-Usman:weighted-clustering>. >. > Make sure that you only see your changes on the lower part of that page. > before hitting the â€œCreate Pull Requestâ€ button. >. > â€”. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOA2G7HECFLNHTNNZ33PVJXFZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVKTS6Y#issuecomment-492124539>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOHOOABYXSQL6HLZUN3PVJXFZANCNFSM4HKUCBXA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:804,deployability,cluster,clustering,804,"Is it fine or need to do anything else before make a pull request ? Thanks. On Tue, May 14, 2019 at 4:05 PM Philipp A. <notifications@github.com> wrote:. > yes! >. > 1. make sure you have all your modified copy somewhere outside of the. > cloned directory! weâ€™re going to destroy all changes inside of that. > directory! > 2. Destroy all changes and go back to 1.4: git reset --hard 1.4 (if. > itâ€™s really 1.4 and not e.g. 1.4.1). > 3. Make a new branch based on 1.4: git checkout -b weighted-clustering. > 4. Copy your changes over again. > 5. Add all files you changed individually (not git add . or git add -A,. > but git add scanpy/file1.py scanpy/file2.py ...). > 6. git commit -m 'your commit message'. >. > Now you can make a new PR with just your changes in it:. > master...Khalid-Usman:weighted-clustering. > <https://github.com/theislab/scanpy/compare/master...Khalid-Usman:weighted-clustering>. >. > Make sure that you only see your changes on the lower part of that page. > before hitting the â€œCreate Pull Requestâ€ button. >. > â€”. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOA2G7HECFLNHTNNZ33PVJXFZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVKTS6Y#issuecomment-492124539>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOHOOABYXSQL6HLZUN3PVJXFZANCNFSM4HKUCBXA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:893,deployability,cluster,clustering,893,"Is it fine or need to do anything else before make a pull request ? Thanks. On Tue, May 14, 2019 at 4:05 PM Philipp A. <notifications@github.com> wrote:. > yes! >. > 1. make sure you have all your modified copy somewhere outside of the. > cloned directory! weâ€™re going to destroy all changes inside of that. > directory! > 2. Destroy all changes and go back to 1.4: git reset --hard 1.4 (if. > itâ€™s really 1.4 and not e.g. 1.4.1). > 3. Make a new branch based on 1.4: git checkout -b weighted-clustering. > 4. Copy your changes over again. > 5. Add all files you changed individually (not git add . or git add -A,. > but git add scanpy/file1.py scanpy/file2.py ...). > 6. git commit -m 'your commit message'. >. > Now you can make a new PR with just your changes in it:. > master...Khalid-Usman:weighted-clustering. > <https://github.com/theislab/scanpy/compare/master...Khalid-Usman:weighted-clustering>. >. > Make sure that you only see your changes on the lower part of that page. > before hitting the â€œCreate Pull Requestâ€ button. >. > â€”. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOA2G7HECFLNHTNNZ33PVJXFZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVKTS6Y#issuecomment-492124539>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOHOOABYXSQL6HLZUN3PVJXFZANCNFSM4HKUCBXA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:699,integrability,messag,message,699,"Is it fine or need to do anything else before make a pull request ? Thanks. On Tue, May 14, 2019 at 4:05 PM Philipp A. <notifications@github.com> wrote:. > yes! >. > 1. make sure you have all your modified copy somewhere outside of the. > cloned directory! weâ€™re going to destroy all changes inside of that. > directory! > 2. Destroy all changes and go back to 1.4: git reset --hard 1.4 (if. > itâ€™s really 1.4 and not e.g. 1.4.1). > 3. Make a new branch based on 1.4: git checkout -b weighted-clustering. > 4. Copy your changes over again. > 5. Add all files you changed individually (not git add . or git add -A,. > but git add scanpy/file1.py scanpy/file2.py ...). > 6. git commit -m 'your commit message'. >. > Now you can make a new PR with just your changes in it:. > master...Khalid-Usman:weighted-clustering. > <https://github.com/theislab/scanpy/compare/master...Khalid-Usman:weighted-clustering>. >. > Make sure that you only see your changes on the lower part of that page. > before hitting the â€œCreate Pull Requestâ€ button. >. > â€”. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOA2G7HECFLNHTNNZ33PVJXFZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVKTS6Y#issuecomment-492124539>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOHOOABYXSQL6HLZUN3PVJXFZANCNFSM4HKUCBXA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:699,interoperability,messag,message,699,"Is it fine or need to do anything else before make a pull request ? Thanks. On Tue, May 14, 2019 at 4:05 PM Philipp A. <notifications@github.com> wrote:. > yes! >. > 1. make sure you have all your modified copy somewhere outside of the. > cloned directory! weâ€™re going to destroy all changes inside of that. > directory! > 2. Destroy all changes and go back to 1.4: git reset --hard 1.4 (if. > itâ€™s really 1.4 and not e.g. 1.4.1). > 3. Make a new branch based on 1.4: git checkout -b weighted-clustering. > 4. Copy your changes over again. > 5. Add all files you changed individually (not git add . or git add -A,. > but git add scanpy/file1.py scanpy/file2.py ...). > 6. git commit -m 'your commit message'. >. > Now you can make a new PR with just your changes in it:. > master...Khalid-Usman:weighted-clustering. > <https://github.com/theislab/scanpy/compare/master...Khalid-Usman:weighted-clustering>. >. > Make sure that you only see your changes on the lower part of that page. > before hitting the â€œCreate Pull Requestâ€ button. >. > â€”. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOA2G7HECFLNHTNNZ33PVJXFZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVKTS6Y#issuecomment-492124539>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOHOOABYXSQL6HLZUN3PVJXFZANCNFSM4HKUCBXA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:197,security,modif,modified,197,"Is it fine or need to do anything else before make a pull request ? Thanks. On Tue, May 14, 2019 at 4:05 PM Philipp A. <notifications@github.com> wrote:. > yes! >. > 1. make sure you have all your modified copy somewhere outside of the. > cloned directory! weâ€™re going to destroy all changes inside of that. > directory! > 2. Destroy all changes and go back to 1.4: git reset --hard 1.4 (if. > itâ€™s really 1.4 and not e.g. 1.4.1). > 3. Make a new branch based on 1.4: git checkout -b weighted-clustering. > 4. Copy your changes over again. > 5. Add all files you changed individually (not git add . or git add -A,. > but git add scanpy/file1.py scanpy/file2.py ...). > 6. git commit -m 'your commit message'. >. > Now you can make a new PR with just your changes in it:. > master...Khalid-Usman:weighted-clustering. > <https://github.com/theislab/scanpy/compare/master...Khalid-Usman:weighted-clustering>. >. > Make sure that you only see your changes on the lower part of that page. > before hitting the â€œCreate Pull Requestâ€ button. >. > â€”. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOA2G7HECFLNHTNNZ33PVJXFZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVKTS6Y#issuecomment-492124539>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOHOOABYXSQL6HLZUN3PVJXFZANCNFSM4HKUCBXA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:1080,security,auth,authored,1080,"Is it fine or need to do anything else before make a pull request ? Thanks. On Tue, May 14, 2019 at 4:05 PM Philipp A. <notifications@github.com> wrote:. > yes! >. > 1. make sure you have all your modified copy somewhere outside of the. > cloned directory! weâ€™re going to destroy all changes inside of that. > directory! > 2. Destroy all changes and go back to 1.4: git reset --hard 1.4 (if. > itâ€™s really 1.4 and not e.g. 1.4.1). > 3. Make a new branch based on 1.4: git checkout -b weighted-clustering. > 4. Copy your changes over again. > 5. Add all files you changed individually (not git add . or git add -A,. > but git add scanpy/file1.py scanpy/file2.py ...). > 6. git commit -m 'your commit message'. >. > Now you can make a new PR with just your changes in it:. > master...Khalid-Usman:weighted-clustering. > <https://github.com/theislab/scanpy/compare/master...Khalid-Usman:weighted-clustering>. >. > Make sure that you only see your changes on the lower part of that page. > before hitting the â€œCreate Pull Requestâ€ button. >. > â€”. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOA2G7HECFLNHTNNZ33PVJXFZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVKTS6Y#issuecomment-492124539>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOHOOABYXSQL6HLZUN3PVJXFZANCNFSM4HKUCBXA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:1446,security,auth,auth,1446,"Is it fine or need to do anything else before make a pull request ? Thanks. On Tue, May 14, 2019 at 4:05 PM Philipp A. <notifications@github.com> wrote:. > yes! >. > 1. make sure you have all your modified copy somewhere outside of the. > cloned directory! weâ€™re going to destroy all changes inside of that. > directory! > 2. Destroy all changes and go back to 1.4: git reset --hard 1.4 (if. > itâ€™s really 1.4 and not e.g. 1.4.1). > 3. Make a new branch based on 1.4: git checkout -b weighted-clustering. > 4. Copy your changes over again. > 5. Add all files you changed individually (not git add . or git add -A,. > but git add scanpy/file1.py scanpy/file2.py ...). > 6. git commit -m 'your commit message'. >. > Now you can make a new PR with just your changes in it:. > master...Khalid-Usman:weighted-clustering. > <https://github.com/theislab/scanpy/compare/master...Khalid-Usman:weighted-clustering>. >. > Make sure that you only see your changes on the lower part of that page. > before hitting the â€œCreate Pull Requestâ€ button. >. > â€”. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOA2G7HECFLNHTNNZ33PVJXFZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVKTS6Y#issuecomment-492124539>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOHOOABYXSQL6HLZUN3PVJXFZANCNFSM4HKUCBXA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:34,usability,close,close,34,"You can just open a new one, Iâ€™ll close this one then :slightly_smiling_face:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:139,deployability,observ,observations,139,"Ok , thanks for letting me know. Please check the pull request. I have. verified my code by keeping weights 1 and it has same values when. observations has no weights or all weights equal to 1. I also suggest to update PCA for weighted sampled data. Thanks,. Khalid Usman. On Tue, May 14, 2019 at 7:53 PM Philipp A. <notifications@github.com> wrote:. > You can just open a new one, Iâ€™ll close this one then ðŸ™‚. >. > â€”. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOFHXLS2NZRCDSLJHE3PVKR2ZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVLHMMQ#issuecomment-492205618>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOBMZBMFMNA6FCEMFULPVKR2ZANCNFSM4HKUCBXA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:212,deployability,updat,update,212,"Ok , thanks for letting me know. Please check the pull request. I have. verified my code by keeping weights 1 and it has same values when. observations has no weights or all weights equal to 1. I also suggest to update PCA for weighted sampled data. Thanks,. Khalid Usman. On Tue, May 14, 2019 at 7:53 PM Philipp A. <notifications@github.com> wrote:. > You can just open a new one, Iâ€™ll close this one then ðŸ™‚. >. > â€”. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOFHXLS2NZRCDSLJHE3PVKR2ZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVLHMMQ#issuecomment-492205618>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOBMZBMFMNA6FCEMFULPVKR2ZANCNFSM4HKUCBXA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:212,safety,updat,update,212,"Ok , thanks for letting me know. Please check the pull request. I have. verified my code by keeping weights 1 and it has same values when. observations has no weights or all weights equal to 1. I also suggest to update PCA for weighted sampled data. Thanks,. Khalid Usman. On Tue, May 14, 2019 at 7:53 PM Philipp A. <notifications@github.com> wrote:. > You can just open a new one, Iâ€™ll close this one then ðŸ™‚. >. > â€”. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOFHXLS2NZRCDSLJHE3PVKR2ZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVLHMMQ#issuecomment-492205618>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOBMZBMFMNA6FCEMFULPVKR2ZANCNFSM4HKUCBXA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:212,security,updat,update,212,"Ok , thanks for letting me know. Please check the pull request. I have. verified my code by keeping weights 1 and it has same values when. observations has no weights or all weights equal to 1. I also suggest to update PCA for weighted sampled data. Thanks,. Khalid Usman. On Tue, May 14, 2019 at 7:53 PM Philipp A. <notifications@github.com> wrote:. > You can just open a new one, Iâ€™ll close this one then ðŸ™‚. >. > â€”. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOFHXLS2NZRCDSLJHE3PVKR2ZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVLHMMQ#issuecomment-492205618>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOBMZBMFMNA6FCEMFULPVKR2ZANCNFSM4HKUCBXA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:455,security,auth,authored,455,"Ok , thanks for letting me know. Please check the pull request. I have. verified my code by keeping weights 1 and it has same values when. observations has no weights or all weights equal to 1. I also suggest to update PCA for weighted sampled data. Thanks,. Khalid Usman. On Tue, May 14, 2019 at 7:53 PM Philipp A. <notifications@github.com> wrote:. > You can just open a new one, Iâ€™ll close this one then ðŸ™‚. >. > â€”. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOFHXLS2NZRCDSLJHE3PVKR2ZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVLHMMQ#issuecomment-492205618>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOBMZBMFMNA6FCEMFULPVKR2ZANCNFSM4HKUCBXA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:821,security,auth,auth,821,"Ok , thanks for letting me know. Please check the pull request. I have. verified my code by keeping weights 1 and it has same values when. observations has no weights or all weights equal to 1. I also suggest to update PCA for weighted sampled data. Thanks,. Khalid Usman. On Tue, May 14, 2019 at 7:53 PM Philipp A. <notifications@github.com> wrote:. > You can just open a new one, Iâ€™ll close this one then ðŸ™‚. >. > â€”. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOFHXLS2NZRCDSLJHE3PVKR2ZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVLHMMQ#issuecomment-492205618>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOBMZBMFMNA6FCEMFULPVKR2ZANCNFSM4HKUCBXA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:72,testability,verif,verified,72,"Ok , thanks for letting me know. Please check the pull request. I have. verified my code by keeping weights 1 and it has same values when. observations has no weights or all weights equal to 1. I also suggest to update PCA for weighted sampled data. Thanks,. Khalid Usman. On Tue, May 14, 2019 at 7:53 PM Philipp A. <notifications@github.com> wrote:. > You can just open a new one, Iâ€™ll close this one then ðŸ™‚. >. > â€”. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOFHXLS2NZRCDSLJHE3PVKR2ZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVLHMMQ#issuecomment-492205618>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOBMZBMFMNA6FCEMFULPVKR2ZANCNFSM4HKUCBXA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:139,testability,observ,observations,139,"Ok , thanks for letting me know. Please check the pull request. I have. verified my code by keeping weights 1 and it has same values when. observations has no weights or all weights equal to 1. I also suggest to update PCA for weighted sampled data. Thanks,. Khalid Usman. On Tue, May 14, 2019 at 7:53 PM Philipp A. <notifications@github.com> wrote:. > You can just open a new one, Iâ€™ll close this one then ðŸ™‚. >. > â€”. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOFHXLS2NZRCDSLJHE3PVKR2ZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVLHMMQ#issuecomment-492205618>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOBMZBMFMNA6FCEMFULPVKR2ZANCNFSM4HKUCBXA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:387,usability,close,close,387,"Ok , thanks for letting me know. Please check the pull request. I have. verified my code by keeping weights 1 and it has same values when. observations has no weights or all weights equal to 1. I also suggest to update PCA for weighted sampled data. Thanks,. Khalid Usman. On Tue, May 14, 2019 at 7:53 PM Philipp A. <notifications@github.com> wrote:. > You can just open a new one, Iâ€™ll close this one then ðŸ™‚. >. > â€”. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOFHXLS2NZRCDSLJHE3PVKR2ZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVLHMMQ#issuecomment-492205618>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOBMZBMFMNA6FCEMFULPVKR2ZANCNFSM4HKUCBXA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:498,deployability,observ,observations,498,"Hi Phillip,. I have removed issue from the pull request by the testing tool, now the. tools showed me duplications, which are mostly from other code and 1-2 from. my code. Please have a look into it. It's my first pull request and its. taking too much time :(. Thanks. Khalid. On Tue, May 14, 2019 at 9:28 PM khalid usman <khalid0491@gmail.com> wrote:. > Ok , thanks for letting me know. Please check the pull request. I have. > verified my code by keeping weights 1 and it has same values when. > observations has no weights or all weights equal to 1. >. > I also suggest to update PCA for weighted sampled data. >. > Thanks,. > Khalid Usman. >. > On Tue, May 14, 2019 at 7:53 PM Philipp A. <notifications@github.com>. > wrote:. >. >> You can just open a new one, Iâ€™ll close this one then ðŸ™‚. >>. >> â€”. >> You are receiving this because you authored the thread. >> Reply to this email directly, view it on GitHub. >> <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOFHXLS2NZRCDSLJHE3PVKR2ZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVLHMMQ#issuecomment-492205618>,. >> or mute the thread. >> <https://github.com/notifications/unsubscribe-auth/ABREGOBMZBMFMNA6FCEMFULPVKR2ZANCNFSM4HKUCBXA>. >> . >>. >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:576,deployability,updat,update,576,"Hi Phillip,. I have removed issue from the pull request by the testing tool, now the. tools showed me duplications, which are mostly from other code and 1-2 from. my code. Please have a look into it. It's my first pull request and its. taking too much time :(. Thanks. Khalid. On Tue, May 14, 2019 at 9:28 PM khalid usman <khalid0491@gmail.com> wrote:. > Ok , thanks for letting me know. Please check the pull request. I have. > verified my code by keeping weights 1 and it has same values when. > observations has no weights or all weights equal to 1. >. > I also suggest to update PCA for weighted sampled data. >. > Thanks,. > Khalid Usman. >. > On Tue, May 14, 2019 at 7:53 PM Philipp A. <notifications@github.com>. > wrote:. >. >> You can just open a new one, Iâ€™ll close this one then ðŸ™‚. >>. >> â€”. >> You are receiving this because you authored the thread. >> Reply to this email directly, view it on GitHub. >> <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOFHXLS2NZRCDSLJHE3PVKR2ZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVLHMMQ#issuecomment-492205618>,. >> or mute the thread. >> <https://github.com/notifications/unsubscribe-auth/ABREGOBMZBMFMNA6FCEMFULPVKR2ZANCNFSM4HKUCBXA>. >> . >>. >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:252,performance,time,time,252,"Hi Phillip,. I have removed issue from the pull request by the testing tool, now the. tools showed me duplications, which are mostly from other code and 1-2 from. my code. Please have a look into it. It's my first pull request and its. taking too much time :(. Thanks. Khalid. On Tue, May 14, 2019 at 9:28 PM khalid usman <khalid0491@gmail.com> wrote:. > Ok , thanks for letting me know. Please check the pull request. I have. > verified my code by keeping weights 1 and it has same values when. > observations has no weights or all weights equal to 1. >. > I also suggest to update PCA for weighted sampled data. >. > Thanks,. > Khalid Usman. >. > On Tue, May 14, 2019 at 7:53 PM Philipp A. <notifications@github.com>. > wrote:. >. >> You can just open a new one, Iâ€™ll close this one then ðŸ™‚. >>. >> â€”. >> You are receiving this because you authored the thread. >> Reply to this email directly, view it on GitHub. >> <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOFHXLS2NZRCDSLJHE3PVKR2ZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVLHMMQ#issuecomment-492205618>,. >> or mute the thread. >> <https://github.com/notifications/unsubscribe-auth/ABREGOBMZBMFMNA6FCEMFULPVKR2ZANCNFSM4HKUCBXA>. >> . >>. >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:63,safety,test,testing,63,"Hi Phillip,. I have removed issue from the pull request by the testing tool, now the. tools showed me duplications, which are mostly from other code and 1-2 from. my code. Please have a look into it. It's my first pull request and its. taking too much time :(. Thanks. Khalid. On Tue, May 14, 2019 at 9:28 PM khalid usman <khalid0491@gmail.com> wrote:. > Ok , thanks for letting me know. Please check the pull request. I have. > verified my code by keeping weights 1 and it has same values when. > observations has no weights or all weights equal to 1. >. > I also suggest to update PCA for weighted sampled data. >. > Thanks,. > Khalid Usman. >. > On Tue, May 14, 2019 at 7:53 PM Philipp A. <notifications@github.com>. > wrote:. >. >> You can just open a new one, Iâ€™ll close this one then ðŸ™‚. >>. >> â€”. >> You are receiving this because you authored the thread. >> Reply to this email directly, view it on GitHub. >> <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOFHXLS2NZRCDSLJHE3PVKR2ZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVLHMMQ#issuecomment-492205618>,. >> or mute the thread. >> <https://github.com/notifications/unsubscribe-auth/ABREGOBMZBMFMNA6FCEMFULPVKR2ZANCNFSM4HKUCBXA>. >> . >>. >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:576,safety,updat,update,576,"Hi Phillip,. I have removed issue from the pull request by the testing tool, now the. tools showed me duplications, which are mostly from other code and 1-2 from. my code. Please have a look into it. It's my first pull request and its. taking too much time :(. Thanks. Khalid. On Tue, May 14, 2019 at 9:28 PM khalid usman <khalid0491@gmail.com> wrote:. > Ok , thanks for letting me know. Please check the pull request. I have. > verified my code by keeping weights 1 and it has same values when. > observations has no weights or all weights equal to 1. >. > I also suggest to update PCA for weighted sampled data. >. > Thanks,. > Khalid Usman. >. > On Tue, May 14, 2019 at 7:53 PM Philipp A. <notifications@github.com>. > wrote:. >. >> You can just open a new one, Iâ€™ll close this one then ðŸ™‚. >>. >> â€”. >> You are receiving this because you authored the thread. >> Reply to this email directly, view it on GitHub. >> <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOFHXLS2NZRCDSLJHE3PVKR2ZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVLHMMQ#issuecomment-492205618>,. >> or mute the thread. >> <https://github.com/notifications/unsubscribe-auth/ABREGOBMZBMFMNA6FCEMFULPVKR2ZANCNFSM4HKUCBXA>. >> . >>. >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:576,security,updat,update,576,"Hi Phillip,. I have removed issue from the pull request by the testing tool, now the. tools showed me duplications, which are mostly from other code and 1-2 from. my code. Please have a look into it. It's my first pull request and its. taking too much time :(. Thanks. Khalid. On Tue, May 14, 2019 at 9:28 PM khalid usman <khalid0491@gmail.com> wrote:. > Ok , thanks for letting me know. Please check the pull request. I have. > verified my code by keeping weights 1 and it has same values when. > observations has no weights or all weights equal to 1. >. > I also suggest to update PCA for weighted sampled data. >. > Thanks,. > Khalid Usman. >. > On Tue, May 14, 2019 at 7:53 PM Philipp A. <notifications@github.com>. > wrote:. >. >> You can just open a new one, Iâ€™ll close this one then ðŸ™‚. >>. >> â€”. >> You are receiving this because you authored the thread. >> Reply to this email directly, view it on GitHub. >> <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOFHXLS2NZRCDSLJHE3PVKR2ZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVLHMMQ#issuecomment-492205618>,. >> or mute the thread. >> <https://github.com/notifications/unsubscribe-auth/ABREGOBMZBMFMNA6FCEMFULPVKR2ZANCNFSM4HKUCBXA>. >> . >>. >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:841,security,auth,authored,841,"Hi Phillip,. I have removed issue from the pull request by the testing tool, now the. tools showed me duplications, which are mostly from other code and 1-2 from. my code. Please have a look into it. It's my first pull request and its. taking too much time :(. Thanks. Khalid. On Tue, May 14, 2019 at 9:28 PM khalid usman <khalid0491@gmail.com> wrote:. > Ok , thanks for letting me know. Please check the pull request. I have. > verified my code by keeping weights 1 and it has same values when. > observations has no weights or all weights equal to 1. >. > I also suggest to update PCA for weighted sampled data. >. > Thanks,. > Khalid Usman. >. > On Tue, May 14, 2019 at 7:53 PM Philipp A. <notifications@github.com>. > wrote:. >. >> You can just open a new one, Iâ€™ll close this one then ðŸ™‚. >>. >> â€”. >> You are receiving this because you authored the thread. >> Reply to this email directly, view it on GitHub. >> <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOFHXLS2NZRCDSLJHE3PVKR2ZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVLHMMQ#issuecomment-492205618>,. >> or mute the thread. >> <https://github.com/notifications/unsubscribe-auth/ABREGOBMZBMFMNA6FCEMFULPVKR2ZANCNFSM4HKUCBXA>. >> . >>. >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:1211,security,auth,auth,1211,"Hi Phillip,. I have removed issue from the pull request by the testing tool, now the. tools showed me duplications, which are mostly from other code and 1-2 from. my code. Please have a look into it. It's my first pull request and its. taking too much time :(. Thanks. Khalid. On Tue, May 14, 2019 at 9:28 PM khalid usman <khalid0491@gmail.com> wrote:. > Ok , thanks for letting me know. Please check the pull request. I have. > verified my code by keeping weights 1 and it has same values when. > observations has no weights or all weights equal to 1. >. > I also suggest to update PCA for weighted sampled data. >. > Thanks,. > Khalid Usman. >. > On Tue, May 14, 2019 at 7:53 PM Philipp A. <notifications@github.com>. > wrote:. >. >> You can just open a new one, Iâ€™ll close this one then ðŸ™‚. >>. >> â€”. >> You are receiving this because you authored the thread. >> Reply to this email directly, view it on GitHub. >> <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOFHXLS2NZRCDSLJHE3PVKR2ZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVLHMMQ#issuecomment-492205618>,. >> or mute the thread. >> <https://github.com/notifications/unsubscribe-auth/ABREGOBMZBMFMNA6FCEMFULPVKR2ZANCNFSM4HKUCBXA>. >> . >>. >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:63,testability,test,testing,63,"Hi Phillip,. I have removed issue from the pull request by the testing tool, now the. tools showed me duplications, which are mostly from other code and 1-2 from. my code. Please have a look into it. It's my first pull request and its. taking too much time :(. Thanks. Khalid. On Tue, May 14, 2019 at 9:28 PM khalid usman <khalid0491@gmail.com> wrote:. > Ok , thanks for letting me know. Please check the pull request. I have. > verified my code by keeping weights 1 and it has same values when. > observations has no weights or all weights equal to 1. >. > I also suggest to update PCA for weighted sampled data. >. > Thanks,. > Khalid Usman. >. > On Tue, May 14, 2019 at 7:53 PM Philipp A. <notifications@github.com>. > wrote:. >. >> You can just open a new one, Iâ€™ll close this one then ðŸ™‚. >>. >> â€”. >> You are receiving this because you authored the thread. >> Reply to this email directly, view it on GitHub. >> <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOFHXLS2NZRCDSLJHE3PVKR2ZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVLHMMQ#issuecomment-492205618>,. >> or mute the thread. >> <https://github.com/notifications/unsubscribe-auth/ABREGOBMZBMFMNA6FCEMFULPVKR2ZANCNFSM4HKUCBXA>. >> . >>. >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:429,testability,verif,verified,429,"Hi Phillip,. I have removed issue from the pull request by the testing tool, now the. tools showed me duplications, which are mostly from other code and 1-2 from. my code. Please have a look into it. It's my first pull request and its. taking too much time :(. Thanks. Khalid. On Tue, May 14, 2019 at 9:28 PM khalid usman <khalid0491@gmail.com> wrote:. > Ok , thanks for letting me know. Please check the pull request. I have. > verified my code by keeping weights 1 and it has same values when. > observations has no weights or all weights equal to 1. >. > I also suggest to update PCA for weighted sampled data. >. > Thanks,. > Khalid Usman. >. > On Tue, May 14, 2019 at 7:53 PM Philipp A. <notifications@github.com>. > wrote:. >. >> You can just open a new one, Iâ€™ll close this one then ðŸ™‚. >>. >> â€”. >> You are receiving this because you authored the thread. >> Reply to this email directly, view it on GitHub. >> <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOFHXLS2NZRCDSLJHE3PVKR2ZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVLHMMQ#issuecomment-492205618>,. >> or mute the thread. >> <https://github.com/notifications/unsubscribe-auth/ABREGOBMZBMFMNA6FCEMFULPVKR2ZANCNFSM4HKUCBXA>. >> . >>. >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:498,testability,observ,observations,498,"Hi Phillip,. I have removed issue from the pull request by the testing tool, now the. tools showed me duplications, which are mostly from other code and 1-2 from. my code. Please have a look into it. It's my first pull request and its. taking too much time :(. Thanks. Khalid. On Tue, May 14, 2019 at 9:28 PM khalid usman <khalid0491@gmail.com> wrote:. > Ok , thanks for letting me know. Please check the pull request. I have. > verified my code by keeping weights 1 and it has same values when. > observations has no weights or all weights equal to 1. >. > I also suggest to update PCA for weighted sampled data. >. > Thanks,. > Khalid Usman. >. > On Tue, May 14, 2019 at 7:53 PM Philipp A. <notifications@github.com>. > wrote:. >. >> You can just open a new one, Iâ€™ll close this one then ðŸ™‚. >>. >> â€”. >> You are receiving this because you authored the thread. >> Reply to this email directly, view it on GitHub. >> <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOFHXLS2NZRCDSLJHE3PVKR2ZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVLHMMQ#issuecomment-492205618>,. >> or mute the thread. >> <https://github.com/notifications/unsubscribe-auth/ABREGOBMZBMFMNA6FCEMFULPVKR2ZANCNFSM4HKUCBXA>. >> . >>. >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:71,usability,tool,tool,71,"Hi Phillip,. I have removed issue from the pull request by the testing tool, now the. tools showed me duplications, which are mostly from other code and 1-2 from. my code. Please have a look into it. It's my first pull request and its. taking too much time :(. Thanks. Khalid. On Tue, May 14, 2019 at 9:28 PM khalid usman <khalid0491@gmail.com> wrote:. > Ok , thanks for letting me know. Please check the pull request. I have. > verified my code by keeping weights 1 and it has same values when. > observations has no weights or all weights equal to 1. >. > I also suggest to update PCA for weighted sampled data. >. > Thanks,. > Khalid Usman. >. > On Tue, May 14, 2019 at 7:53 PM Philipp A. <notifications@github.com>. > wrote:. >. >> You can just open a new one, Iâ€™ll close this one then ðŸ™‚. >>. >> â€”. >> You are receiving this because you authored the thread. >> Reply to this email directly, view it on GitHub. >> <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOFHXLS2NZRCDSLJHE3PVKR2ZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVLHMMQ#issuecomment-492205618>,. >> or mute the thread. >> <https://github.com/notifications/unsubscribe-auth/ABREGOBMZBMFMNA6FCEMFULPVKR2ZANCNFSM4HKUCBXA>. >> . >>. >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:86,usability,tool,tools,86,"Hi Phillip,. I have removed issue from the pull request by the testing tool, now the. tools showed me duplications, which are mostly from other code and 1-2 from. my code. Please have a look into it. It's my first pull request and its. taking too much time :(. Thanks. Khalid. On Tue, May 14, 2019 at 9:28 PM khalid usman <khalid0491@gmail.com> wrote:. > Ok , thanks for letting me know. Please check the pull request. I have. > verified my code by keeping weights 1 and it has same values when. > observations has no weights or all weights equal to 1. >. > I also suggest to update PCA for weighted sampled data. >. > Thanks,. > Khalid Usman. >. > On Tue, May 14, 2019 at 7:53 PM Philipp A. <notifications@github.com>. > wrote:. >. >> You can just open a new one, Iâ€™ll close this one then ðŸ™‚. >>. >> â€”. >> You are receiving this because you authored the thread. >> Reply to this email directly, view it on GitHub. >> <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOFHXLS2NZRCDSLJHE3PVKR2ZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVLHMMQ#issuecomment-492205618>,. >> or mute the thread. >> <https://github.com/notifications/unsubscribe-auth/ABREGOBMZBMFMNA6FCEMFULPVKR2ZANCNFSM4HKUCBXA>. >> . >>. >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:770,usability,close,close,770,"Hi Phillip,. I have removed issue from the pull request by the testing tool, now the. tools showed me duplications, which are mostly from other code and 1-2 from. my code. Please have a look into it. It's my first pull request and its. taking too much time :(. Thanks. Khalid. On Tue, May 14, 2019 at 9:28 PM khalid usman <khalid0491@gmail.com> wrote:. > Ok , thanks for letting me know. Please check the pull request. I have. > verified my code by keeping weights 1 and it has same values when. > observations has no weights or all weights equal to 1. >. > I also suggest to update PCA for weighted sampled data. >. > Thanks,. > Khalid Usman. >. > On Tue, May 14, 2019 at 7:53 PM Philipp A. <notifications@github.com>. > wrote:. >. >> You can just open a new one, Iâ€™ll close this one then ðŸ™‚. >>. >> â€”. >> You are receiving this because you authored the thread. >> Reply to this email directly, view it on GitHub. >> <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOFHXLS2NZRCDSLJHE3PVKR2ZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVLHMMQ#issuecomment-492205618>,. >> or mute the thread. >> <https://github.com/notifications/unsubscribe-auth/ABREGOBMZBMFMNA6FCEMFULPVKR2ZANCNFSM4HKUCBXA>. >> . >>. >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:76,deployability,version,version,76,"Thanks alot Philip for your help, I just learned testing :). Hopefully this version will be accepted, as I tested now on my laptop and. push when it passed tests for all 5 plots. Thanks,. Khalid. On Mon, May 20, 2019 at 12:37 PM khalid usman <khalid0491@gmail.com> wrote:. > Hi Phillip,. >. > I have removed issue from the pull request by the testing tool, now the. > tools showed me duplications, which are mostly from other code and 1-2 from. > my code. Please have a look into it. It's my first pull request and its. > taking too much time :(. >. > Thanks. > Khalid. >. > On Tue, May 14, 2019 at 9:28 PM khalid usman <khalid0491@gmail.com> wrote:. >. >> Ok , thanks for letting me know. Please check the pull request. I have. >> verified my code by keeping weights 1 and it has same values when. >> observations has no weights or all weights equal to 1. >>. >> I also suggest to update PCA for weighted sampled data. >>. >> Thanks,. >> Khalid Usman. >>. >> On Tue, May 14, 2019 at 7:53 PM Philipp A. <notifications@github.com>. >> wrote:. >>. >>> You can just open a new one, Iâ€™ll close this one then ðŸ™‚. >>>. >>> â€”. >>> You are receiving this because you authored the thread. >>> Reply to this email directly, view it on GitHub. >>> <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOFHXLS2NZRCDSLJHE3PVKR2ZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVLHMMQ#issuecomment-492205618>,. >>> or mute the thread. >>> <https://github.com/notifications/unsubscribe-auth/ABREGOBMZBMFMNA6FCEMFULPVKR2ZANCNFSM4HKUCBXA>. >>> . >>>. >>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:802,deployability,observ,observations,802,"Thanks alot Philip for your help, I just learned testing :). Hopefully this version will be accepted, as I tested now on my laptop and. push when it passed tests for all 5 plots. Thanks,. Khalid. On Mon, May 20, 2019 at 12:37 PM khalid usman <khalid0491@gmail.com> wrote:. > Hi Phillip,. >. > I have removed issue from the pull request by the testing tool, now the. > tools showed me duplications, which are mostly from other code and 1-2 from. > my code. Please have a look into it. It's my first pull request and its. > taking too much time :(. >. > Thanks. > Khalid. >. > On Tue, May 14, 2019 at 9:28 PM khalid usman <khalid0491@gmail.com> wrote:. >. >> Ok , thanks for letting me know. Please check the pull request. I have. >> verified my code by keeping weights 1 and it has same values when. >> observations has no weights or all weights equal to 1. >>. >> I also suggest to update PCA for weighted sampled data. >>. >> Thanks,. >> Khalid Usman. >>. >> On Tue, May 14, 2019 at 7:53 PM Philipp A. <notifications@github.com>. >> wrote:. >>. >>> You can just open a new one, Iâ€™ll close this one then ðŸ™‚. >>>. >>> â€”. >>> You are receiving this because you authored the thread. >>> Reply to this email directly, view it on GitHub. >>> <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOFHXLS2NZRCDSLJHE3PVKR2ZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVLHMMQ#issuecomment-492205618>,. >>> or mute the thread. >>> <https://github.com/notifications/unsubscribe-auth/ABREGOBMZBMFMNA6FCEMFULPVKR2ZANCNFSM4HKUCBXA>. >>> . >>>. >>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:882,deployability,updat,update,882,"Thanks alot Philip for your help, I just learned testing :). Hopefully this version will be accepted, as I tested now on my laptop and. push when it passed tests for all 5 plots. Thanks,. Khalid. On Mon, May 20, 2019 at 12:37 PM khalid usman <khalid0491@gmail.com> wrote:. > Hi Phillip,. >. > I have removed issue from the pull request by the testing tool, now the. > tools showed me duplications, which are mostly from other code and 1-2 from. > my code. Please have a look into it. It's my first pull request and its. > taking too much time :(. >. > Thanks. > Khalid. >. > On Tue, May 14, 2019 at 9:28 PM khalid usman <khalid0491@gmail.com> wrote:. >. >> Ok , thanks for letting me know. Please check the pull request. I have. >> verified my code by keeping weights 1 and it has same values when. >> observations has no weights or all weights equal to 1. >>. >> I also suggest to update PCA for weighted sampled data. >>. >> Thanks,. >> Khalid Usman. >>. >> On Tue, May 14, 2019 at 7:53 PM Philipp A. <notifications@github.com>. >> wrote:. >>. >>> You can just open a new one, Iâ€™ll close this one then ðŸ™‚. >>>. >>> â€”. >>> You are receiving this because you authored the thread. >>> Reply to this email directly, view it on GitHub. >>> <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOFHXLS2NZRCDSLJHE3PVKR2ZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVLHMMQ#issuecomment-492205618>,. >>> or mute the thread. >>> <https://github.com/notifications/unsubscribe-auth/ABREGOBMZBMFMNA6FCEMFULPVKR2ZANCNFSM4HKUCBXA>. >>> . >>>. >>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:76,integrability,version,version,76,"Thanks alot Philip for your help, I just learned testing :). Hopefully this version will be accepted, as I tested now on my laptop and. push when it passed tests for all 5 plots. Thanks,. Khalid. On Mon, May 20, 2019 at 12:37 PM khalid usman <khalid0491@gmail.com> wrote:. > Hi Phillip,. >. > I have removed issue from the pull request by the testing tool, now the. > tools showed me duplications, which are mostly from other code and 1-2 from. > my code. Please have a look into it. It's my first pull request and its. > taking too much time :(. >. > Thanks. > Khalid. >. > On Tue, May 14, 2019 at 9:28 PM khalid usman <khalid0491@gmail.com> wrote:. >. >> Ok , thanks for letting me know. Please check the pull request. I have. >> verified my code by keeping weights 1 and it has same values when. >> observations has no weights or all weights equal to 1. >>. >> I also suggest to update PCA for weighted sampled data. >>. >> Thanks,. >> Khalid Usman. >>. >> On Tue, May 14, 2019 at 7:53 PM Philipp A. <notifications@github.com>. >> wrote:. >>. >>> You can just open a new one, Iâ€™ll close this one then ðŸ™‚. >>>. >>> â€”. >>> You are receiving this because you authored the thread. >>> Reply to this email directly, view it on GitHub. >>> <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOFHXLS2NZRCDSLJHE3PVKR2ZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVLHMMQ#issuecomment-492205618>,. >>> or mute the thread. >>> <https://github.com/notifications/unsubscribe-auth/ABREGOBMZBMFMNA6FCEMFULPVKR2ZANCNFSM4HKUCBXA>. >>> . >>>. >>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:76,modifiability,version,version,76,"Thanks alot Philip for your help, I just learned testing :). Hopefully this version will be accepted, as I tested now on my laptop and. push when it passed tests for all 5 plots. Thanks,. Khalid. On Mon, May 20, 2019 at 12:37 PM khalid usman <khalid0491@gmail.com> wrote:. > Hi Phillip,. >. > I have removed issue from the pull request by the testing tool, now the. > tools showed me duplications, which are mostly from other code and 1-2 from. > my code. Please have a look into it. It's my first pull request and its. > taking too much time :(. >. > Thanks. > Khalid. >. > On Tue, May 14, 2019 at 9:28 PM khalid usman <khalid0491@gmail.com> wrote:. >. >> Ok , thanks for letting me know. Please check the pull request. I have. >> verified my code by keeping weights 1 and it has same values when. >> observations has no weights or all weights equal to 1. >>. >> I also suggest to update PCA for weighted sampled data. >>. >> Thanks,. >> Khalid Usman. >>. >> On Tue, May 14, 2019 at 7:53 PM Philipp A. <notifications@github.com>. >> wrote:. >>. >>> You can just open a new one, Iâ€™ll close this one then ðŸ™‚. >>>. >>> â€”. >>> You are receiving this because you authored the thread. >>> Reply to this email directly, view it on GitHub. >>> <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOFHXLS2NZRCDSLJHE3PVKR2ZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVLHMMQ#issuecomment-492205618>,. >>> or mute the thread. >>> <https://github.com/notifications/unsubscribe-auth/ABREGOBMZBMFMNA6FCEMFULPVKR2ZANCNFSM4HKUCBXA>. >>> . >>>. >>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:538,performance,time,time,538,"Thanks alot Philip for your help, I just learned testing :). Hopefully this version will be accepted, as I tested now on my laptop and. push when it passed tests for all 5 plots. Thanks,. Khalid. On Mon, May 20, 2019 at 12:37 PM khalid usman <khalid0491@gmail.com> wrote:. > Hi Phillip,. >. > I have removed issue from the pull request by the testing tool, now the. > tools showed me duplications, which are mostly from other code and 1-2 from. > my code. Please have a look into it. It's my first pull request and its. > taking too much time :(. >. > Thanks. > Khalid. >. > On Tue, May 14, 2019 at 9:28 PM khalid usman <khalid0491@gmail.com> wrote:. >. >> Ok , thanks for letting me know. Please check the pull request. I have. >> verified my code by keeping weights 1 and it has same values when. >> observations has no weights or all weights equal to 1. >>. >> I also suggest to update PCA for weighted sampled data. >>. >> Thanks,. >> Khalid Usman. >>. >> On Tue, May 14, 2019 at 7:53 PM Philipp A. <notifications@github.com>. >> wrote:. >>. >>> You can just open a new one, Iâ€™ll close this one then ðŸ™‚. >>>. >>> â€”. >>> You are receiving this because you authored the thread. >>> Reply to this email directly, view it on GitHub. >>> <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOFHXLS2NZRCDSLJHE3PVKR2ZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVLHMMQ#issuecomment-492205618>,. >>> or mute the thread. >>> <https://github.com/notifications/unsubscribe-auth/ABREGOBMZBMFMNA6FCEMFULPVKR2ZANCNFSM4HKUCBXA>. >>> . >>>. >>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:49,safety,test,testing,49,"Thanks alot Philip for your help, I just learned testing :). Hopefully this version will be accepted, as I tested now on my laptop and. push when it passed tests for all 5 plots. Thanks,. Khalid. On Mon, May 20, 2019 at 12:37 PM khalid usman <khalid0491@gmail.com> wrote:. > Hi Phillip,. >. > I have removed issue from the pull request by the testing tool, now the. > tools showed me duplications, which are mostly from other code and 1-2 from. > my code. Please have a look into it. It's my first pull request and its. > taking too much time :(. >. > Thanks. > Khalid. >. > On Tue, May 14, 2019 at 9:28 PM khalid usman <khalid0491@gmail.com> wrote:. >. >> Ok , thanks for letting me know. Please check the pull request. I have. >> verified my code by keeping weights 1 and it has same values when. >> observations has no weights or all weights equal to 1. >>. >> I also suggest to update PCA for weighted sampled data. >>. >> Thanks,. >> Khalid Usman. >>. >> On Tue, May 14, 2019 at 7:53 PM Philipp A. <notifications@github.com>. >> wrote:. >>. >>> You can just open a new one, Iâ€™ll close this one then ðŸ™‚. >>>. >>> â€”. >>> You are receiving this because you authored the thread. >>> Reply to this email directly, view it on GitHub. >>> <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOFHXLS2NZRCDSLJHE3PVKR2ZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVLHMMQ#issuecomment-492205618>,. >>> or mute the thread. >>> <https://github.com/notifications/unsubscribe-auth/ABREGOBMZBMFMNA6FCEMFULPVKR2ZANCNFSM4HKUCBXA>. >>> . >>>. >>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:107,safety,test,tested,107,"Thanks alot Philip for your help, I just learned testing :). Hopefully this version will be accepted, as I tested now on my laptop and. push when it passed tests for all 5 plots. Thanks,. Khalid. On Mon, May 20, 2019 at 12:37 PM khalid usman <khalid0491@gmail.com> wrote:. > Hi Phillip,. >. > I have removed issue from the pull request by the testing tool, now the. > tools showed me duplications, which are mostly from other code and 1-2 from. > my code. Please have a look into it. It's my first pull request and its. > taking too much time :(. >. > Thanks. > Khalid. >. > On Tue, May 14, 2019 at 9:28 PM khalid usman <khalid0491@gmail.com> wrote:. >. >> Ok , thanks for letting me know. Please check the pull request. I have. >> verified my code by keeping weights 1 and it has same values when. >> observations has no weights or all weights equal to 1. >>. >> I also suggest to update PCA for weighted sampled data. >>. >> Thanks,. >> Khalid Usman. >>. >> On Tue, May 14, 2019 at 7:53 PM Philipp A. <notifications@github.com>. >> wrote:. >>. >>> You can just open a new one, Iâ€™ll close this one then ðŸ™‚. >>>. >>> â€”. >>> You are receiving this because you authored the thread. >>> Reply to this email directly, view it on GitHub. >>> <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOFHXLS2NZRCDSLJHE3PVKR2ZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVLHMMQ#issuecomment-492205618>,. >>> or mute the thread. >>> <https://github.com/notifications/unsubscribe-auth/ABREGOBMZBMFMNA6FCEMFULPVKR2ZANCNFSM4HKUCBXA>. >>> . >>>. >>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:156,safety,test,tests,156,"Thanks alot Philip for your help, I just learned testing :). Hopefully this version will be accepted, as I tested now on my laptop and. push when it passed tests for all 5 plots. Thanks,. Khalid. On Mon, May 20, 2019 at 12:37 PM khalid usman <khalid0491@gmail.com> wrote:. > Hi Phillip,. >. > I have removed issue from the pull request by the testing tool, now the. > tools showed me duplications, which are mostly from other code and 1-2 from. > my code. Please have a look into it. It's my first pull request and its. > taking too much time :(. >. > Thanks. > Khalid. >. > On Tue, May 14, 2019 at 9:28 PM khalid usman <khalid0491@gmail.com> wrote:. >. >> Ok , thanks for letting me know. Please check the pull request. I have. >> verified my code by keeping weights 1 and it has same values when. >> observations has no weights or all weights equal to 1. >>. >> I also suggest to update PCA for weighted sampled data. >>. >> Thanks,. >> Khalid Usman. >>. >> On Tue, May 14, 2019 at 7:53 PM Philipp A. <notifications@github.com>. >> wrote:. >>. >>> You can just open a new one, Iâ€™ll close this one then ðŸ™‚. >>>. >>> â€”. >>> You are receiving this because you authored the thread. >>> Reply to this email directly, view it on GitHub. >>> <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOFHXLS2NZRCDSLJHE3PVKR2ZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVLHMMQ#issuecomment-492205618>,. >>> or mute the thread. >>> <https://github.com/notifications/unsubscribe-auth/ABREGOBMZBMFMNA6FCEMFULPVKR2ZANCNFSM4HKUCBXA>. >>> . >>>. >>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:343,safety,test,testing,343,"Thanks alot Philip for your help, I just learned testing :). Hopefully this version will be accepted, as I tested now on my laptop and. push when it passed tests for all 5 plots. Thanks,. Khalid. On Mon, May 20, 2019 at 12:37 PM khalid usman <khalid0491@gmail.com> wrote:. > Hi Phillip,. >. > I have removed issue from the pull request by the testing tool, now the. > tools showed me duplications, which are mostly from other code and 1-2 from. > my code. Please have a look into it. It's my first pull request and its. > taking too much time :(. >. > Thanks. > Khalid. >. > On Tue, May 14, 2019 at 9:28 PM khalid usman <khalid0491@gmail.com> wrote:. >. >> Ok , thanks for letting me know. Please check the pull request. I have. >> verified my code by keeping weights 1 and it has same values when. >> observations has no weights or all weights equal to 1. >>. >> I also suggest to update PCA for weighted sampled data. >>. >> Thanks,. >> Khalid Usman. >>. >> On Tue, May 14, 2019 at 7:53 PM Philipp A. <notifications@github.com>. >> wrote:. >>. >>> You can just open a new one, Iâ€™ll close this one then ðŸ™‚. >>>. >>> â€”. >>> You are receiving this because you authored the thread. >>> Reply to this email directly, view it on GitHub. >>> <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOFHXLS2NZRCDSLJHE3PVKR2ZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVLHMMQ#issuecomment-492205618>,. >>> or mute the thread. >>> <https://github.com/notifications/unsubscribe-auth/ABREGOBMZBMFMNA6FCEMFULPVKR2ZANCNFSM4HKUCBXA>. >>> . >>>. >>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:882,safety,updat,update,882,"Thanks alot Philip for your help, I just learned testing :). Hopefully this version will be accepted, as I tested now on my laptop and. push when it passed tests for all 5 plots. Thanks,. Khalid. On Mon, May 20, 2019 at 12:37 PM khalid usman <khalid0491@gmail.com> wrote:. > Hi Phillip,. >. > I have removed issue from the pull request by the testing tool, now the. > tools showed me duplications, which are mostly from other code and 1-2 from. > my code. Please have a look into it. It's my first pull request and its. > taking too much time :(. >. > Thanks. > Khalid. >. > On Tue, May 14, 2019 at 9:28 PM khalid usman <khalid0491@gmail.com> wrote:. >. >> Ok , thanks for letting me know. Please check the pull request. I have. >> verified my code by keeping weights 1 and it has same values when. >> observations has no weights or all weights equal to 1. >>. >> I also suggest to update PCA for weighted sampled data. >>. >> Thanks,. >> Khalid Usman. >>. >> On Tue, May 14, 2019 at 7:53 PM Philipp A. <notifications@github.com>. >> wrote:. >>. >>> You can just open a new one, Iâ€™ll close this one then ðŸ™‚. >>>. >>> â€”. >>> You are receiving this because you authored the thread. >>> Reply to this email directly, view it on GitHub. >>> <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOFHXLS2NZRCDSLJHE3PVKR2ZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVLHMMQ#issuecomment-492205618>,. >>> or mute the thread. >>> <https://github.com/notifications/unsubscribe-auth/ABREGOBMZBMFMNA6FCEMFULPVKR2ZANCNFSM4HKUCBXA>. >>> . >>>. >>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:882,security,updat,update,882,"Thanks alot Philip for your help, I just learned testing :). Hopefully this version will be accepted, as I tested now on my laptop and. push when it passed tests for all 5 plots. Thanks,. Khalid. On Mon, May 20, 2019 at 12:37 PM khalid usman <khalid0491@gmail.com> wrote:. > Hi Phillip,. >. > I have removed issue from the pull request by the testing tool, now the. > tools showed me duplications, which are mostly from other code and 1-2 from. > my code. Please have a look into it. It's my first pull request and its. > taking too much time :(. >. > Thanks. > Khalid. >. > On Tue, May 14, 2019 at 9:28 PM khalid usman <khalid0491@gmail.com> wrote:. >. >> Ok , thanks for letting me know. Please check the pull request. I have. >> verified my code by keeping weights 1 and it has same values when. >> observations has no weights or all weights equal to 1. >>. >> I also suggest to update PCA for weighted sampled data. >>. >> Thanks,. >> Khalid Usman. >>. >> On Tue, May 14, 2019 at 7:53 PM Philipp A. <notifications@github.com>. >> wrote:. >>. >>> You can just open a new one, Iâ€™ll close this one then ðŸ™‚. >>>. >>> â€”. >>> You are receiving this because you authored the thread. >>> Reply to this email directly, view it on GitHub. >>> <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOFHXLS2NZRCDSLJHE3PVKR2ZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVLHMMQ#issuecomment-492205618>,. >>> or mute the thread. >>> <https://github.com/notifications/unsubscribe-auth/ABREGOBMZBMFMNA6FCEMFULPVKR2ZANCNFSM4HKUCBXA>. >>> . >>>. >>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:1158,security,auth,authored,1158,"Thanks alot Philip for your help, I just learned testing :). Hopefully this version will be accepted, as I tested now on my laptop and. push when it passed tests for all 5 plots. Thanks,. Khalid. On Mon, May 20, 2019 at 12:37 PM khalid usman <khalid0491@gmail.com> wrote:. > Hi Phillip,. >. > I have removed issue from the pull request by the testing tool, now the. > tools showed me duplications, which are mostly from other code and 1-2 from. > my code. Please have a look into it. It's my first pull request and its. > taking too much time :(. >. > Thanks. > Khalid. >. > On Tue, May 14, 2019 at 9:28 PM khalid usman <khalid0491@gmail.com> wrote:. >. >> Ok , thanks for letting me know. Please check the pull request. I have. >> verified my code by keeping weights 1 and it has same values when. >> observations has no weights or all weights equal to 1. >>. >> I also suggest to update PCA for weighted sampled data. >>. >> Thanks,. >> Khalid Usman. >>. >> On Tue, May 14, 2019 at 7:53 PM Philipp A. <notifications@github.com>. >> wrote:. >>. >>> You can just open a new one, Iâ€™ll close this one then ðŸ™‚. >>>. >>> â€”. >>> You are receiving this because you authored the thread. >>> Reply to this email directly, view it on GitHub. >>> <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOFHXLS2NZRCDSLJHE3PVKR2ZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVLHMMQ#issuecomment-492205618>,. >>> or mute the thread. >>> <https://github.com/notifications/unsubscribe-auth/ABREGOBMZBMFMNA6FCEMFULPVKR2ZANCNFSM4HKUCBXA>. >>> . >>>. >>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:1532,security,auth,auth,1532,"Thanks alot Philip for your help, I just learned testing :). Hopefully this version will be accepted, as I tested now on my laptop and. push when it passed tests for all 5 plots. Thanks,. Khalid. On Mon, May 20, 2019 at 12:37 PM khalid usman <khalid0491@gmail.com> wrote:. > Hi Phillip,. >. > I have removed issue from the pull request by the testing tool, now the. > tools showed me duplications, which are mostly from other code and 1-2 from. > my code. Please have a look into it. It's my first pull request and its. > taking too much time :(. >. > Thanks. > Khalid. >. > On Tue, May 14, 2019 at 9:28 PM khalid usman <khalid0491@gmail.com> wrote:. >. >> Ok , thanks for letting me know. Please check the pull request. I have. >> verified my code by keeping weights 1 and it has same values when. >> observations has no weights or all weights equal to 1. >>. >> I also suggest to update PCA for weighted sampled data. >>. >> Thanks,. >> Khalid Usman. >>. >> On Tue, May 14, 2019 at 7:53 PM Philipp A. <notifications@github.com>. >> wrote:. >>. >>> You can just open a new one, Iâ€™ll close this one then ðŸ™‚. >>>. >>> â€”. >>> You are receiving this because you authored the thread. >>> Reply to this email directly, view it on GitHub. >>> <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOFHXLS2NZRCDSLJHE3PVKR2ZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVLHMMQ#issuecomment-492205618>,. >>> or mute the thread. >>> <https://github.com/notifications/unsubscribe-auth/ABREGOBMZBMFMNA6FCEMFULPVKR2ZANCNFSM4HKUCBXA>. >>> . >>>. >>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:49,testability,test,testing,49,"Thanks alot Philip for your help, I just learned testing :). Hopefully this version will be accepted, as I tested now on my laptop and. push when it passed tests for all 5 plots. Thanks,. Khalid. On Mon, May 20, 2019 at 12:37 PM khalid usman <khalid0491@gmail.com> wrote:. > Hi Phillip,. >. > I have removed issue from the pull request by the testing tool, now the. > tools showed me duplications, which are mostly from other code and 1-2 from. > my code. Please have a look into it. It's my first pull request and its. > taking too much time :(. >. > Thanks. > Khalid. >. > On Tue, May 14, 2019 at 9:28 PM khalid usman <khalid0491@gmail.com> wrote:. >. >> Ok , thanks for letting me know. Please check the pull request. I have. >> verified my code by keeping weights 1 and it has same values when. >> observations has no weights or all weights equal to 1. >>. >> I also suggest to update PCA for weighted sampled data. >>. >> Thanks,. >> Khalid Usman. >>. >> On Tue, May 14, 2019 at 7:53 PM Philipp A. <notifications@github.com>. >> wrote:. >>. >>> You can just open a new one, Iâ€™ll close this one then ðŸ™‚. >>>. >>> â€”. >>> You are receiving this because you authored the thread. >>> Reply to this email directly, view it on GitHub. >>> <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOFHXLS2NZRCDSLJHE3PVKR2ZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVLHMMQ#issuecomment-492205618>,. >>> or mute the thread. >>> <https://github.com/notifications/unsubscribe-auth/ABREGOBMZBMFMNA6FCEMFULPVKR2ZANCNFSM4HKUCBXA>. >>> . >>>. >>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:107,testability,test,tested,107,"Thanks alot Philip for your help, I just learned testing :). Hopefully this version will be accepted, as I tested now on my laptop and. push when it passed tests for all 5 plots. Thanks,. Khalid. On Mon, May 20, 2019 at 12:37 PM khalid usman <khalid0491@gmail.com> wrote:. > Hi Phillip,. >. > I have removed issue from the pull request by the testing tool, now the. > tools showed me duplications, which are mostly from other code and 1-2 from. > my code. Please have a look into it. It's my first pull request and its. > taking too much time :(. >. > Thanks. > Khalid. >. > On Tue, May 14, 2019 at 9:28 PM khalid usman <khalid0491@gmail.com> wrote:. >. >> Ok , thanks for letting me know. Please check the pull request. I have. >> verified my code by keeping weights 1 and it has same values when. >> observations has no weights or all weights equal to 1. >>. >> I also suggest to update PCA for weighted sampled data. >>. >> Thanks,. >> Khalid Usman. >>. >> On Tue, May 14, 2019 at 7:53 PM Philipp A. <notifications@github.com>. >> wrote:. >>. >>> You can just open a new one, Iâ€™ll close this one then ðŸ™‚. >>>. >>> â€”. >>> You are receiving this because you authored the thread. >>> Reply to this email directly, view it on GitHub. >>> <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOFHXLS2NZRCDSLJHE3PVKR2ZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVLHMMQ#issuecomment-492205618>,. >>> or mute the thread. >>> <https://github.com/notifications/unsubscribe-auth/ABREGOBMZBMFMNA6FCEMFULPVKR2ZANCNFSM4HKUCBXA>. >>> . >>>. >>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:156,testability,test,tests,156,"Thanks alot Philip for your help, I just learned testing :). Hopefully this version will be accepted, as I tested now on my laptop and. push when it passed tests for all 5 plots. Thanks,. Khalid. On Mon, May 20, 2019 at 12:37 PM khalid usman <khalid0491@gmail.com> wrote:. > Hi Phillip,. >. > I have removed issue from the pull request by the testing tool, now the. > tools showed me duplications, which are mostly from other code and 1-2 from. > my code. Please have a look into it. It's my first pull request and its. > taking too much time :(. >. > Thanks. > Khalid. >. > On Tue, May 14, 2019 at 9:28 PM khalid usman <khalid0491@gmail.com> wrote:. >. >> Ok , thanks for letting me know. Please check the pull request. I have. >> verified my code by keeping weights 1 and it has same values when. >> observations has no weights or all weights equal to 1. >>. >> I also suggest to update PCA for weighted sampled data. >>. >> Thanks,. >> Khalid Usman. >>. >> On Tue, May 14, 2019 at 7:53 PM Philipp A. <notifications@github.com>. >> wrote:. >>. >>> You can just open a new one, Iâ€™ll close this one then ðŸ™‚. >>>. >>> â€”. >>> You are receiving this because you authored the thread. >>> Reply to this email directly, view it on GitHub. >>> <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOFHXLS2NZRCDSLJHE3PVKR2ZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVLHMMQ#issuecomment-492205618>,. >>> or mute the thread. >>> <https://github.com/notifications/unsubscribe-auth/ABREGOBMZBMFMNA6FCEMFULPVKR2ZANCNFSM4HKUCBXA>. >>> . >>>. >>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:343,testability,test,testing,343,"Thanks alot Philip for your help, I just learned testing :). Hopefully this version will be accepted, as I tested now on my laptop and. push when it passed tests for all 5 plots. Thanks,. Khalid. On Mon, May 20, 2019 at 12:37 PM khalid usman <khalid0491@gmail.com> wrote:. > Hi Phillip,. >. > I have removed issue from the pull request by the testing tool, now the. > tools showed me duplications, which are mostly from other code and 1-2 from. > my code. Please have a look into it. It's my first pull request and its. > taking too much time :(. >. > Thanks. > Khalid. >. > On Tue, May 14, 2019 at 9:28 PM khalid usman <khalid0491@gmail.com> wrote:. >. >> Ok , thanks for letting me know. Please check the pull request. I have. >> verified my code by keeping weights 1 and it has same values when. >> observations has no weights or all weights equal to 1. >>. >> I also suggest to update PCA for weighted sampled data. >>. >> Thanks,. >> Khalid Usman. >>. >> On Tue, May 14, 2019 at 7:53 PM Philipp A. <notifications@github.com>. >> wrote:. >>. >>> You can just open a new one, Iâ€™ll close this one then ðŸ™‚. >>>. >>> â€”. >>> You are receiving this because you authored the thread. >>> Reply to this email directly, view it on GitHub. >>> <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOFHXLS2NZRCDSLJHE3PVKR2ZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVLHMMQ#issuecomment-492205618>,. >>> or mute the thread. >>> <https://github.com/notifications/unsubscribe-auth/ABREGOBMZBMFMNA6FCEMFULPVKR2ZANCNFSM4HKUCBXA>. >>> . >>>. >>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:732,testability,verif,verified,732,"Thanks alot Philip for your help, I just learned testing :). Hopefully this version will be accepted, as I tested now on my laptop and. push when it passed tests for all 5 plots. Thanks,. Khalid. On Mon, May 20, 2019 at 12:37 PM khalid usman <khalid0491@gmail.com> wrote:. > Hi Phillip,. >. > I have removed issue from the pull request by the testing tool, now the. > tools showed me duplications, which are mostly from other code and 1-2 from. > my code. Please have a look into it. It's my first pull request and its. > taking too much time :(. >. > Thanks. > Khalid. >. > On Tue, May 14, 2019 at 9:28 PM khalid usman <khalid0491@gmail.com> wrote:. >. >> Ok , thanks for letting me know. Please check the pull request. I have. >> verified my code by keeping weights 1 and it has same values when. >> observations has no weights or all weights equal to 1. >>. >> I also suggest to update PCA for weighted sampled data. >>. >> Thanks,. >> Khalid Usman. >>. >> On Tue, May 14, 2019 at 7:53 PM Philipp A. <notifications@github.com>. >> wrote:. >>. >>> You can just open a new one, Iâ€™ll close this one then ðŸ™‚. >>>. >>> â€”. >>> You are receiving this because you authored the thread. >>> Reply to this email directly, view it on GitHub. >>> <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOFHXLS2NZRCDSLJHE3PVKR2ZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVLHMMQ#issuecomment-492205618>,. >>> or mute the thread. >>> <https://github.com/notifications/unsubscribe-auth/ABREGOBMZBMFMNA6FCEMFULPVKR2ZANCNFSM4HKUCBXA>. >>> . >>>. >>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:802,testability,observ,observations,802,"Thanks alot Philip for your help, I just learned testing :). Hopefully this version will be accepted, as I tested now on my laptop and. push when it passed tests for all 5 plots. Thanks,. Khalid. On Mon, May 20, 2019 at 12:37 PM khalid usman <khalid0491@gmail.com> wrote:. > Hi Phillip,. >. > I have removed issue from the pull request by the testing tool, now the. > tools showed me duplications, which are mostly from other code and 1-2 from. > my code. Please have a look into it. It's my first pull request and its. > taking too much time :(. >. > Thanks. > Khalid. >. > On Tue, May 14, 2019 at 9:28 PM khalid usman <khalid0491@gmail.com> wrote:. >. >> Ok , thanks for letting me know. Please check the pull request. I have. >> verified my code by keeping weights 1 and it has same values when. >> observations has no weights or all weights equal to 1. >>. >> I also suggest to update PCA for weighted sampled data. >>. >> Thanks,. >> Khalid Usman. >>. >> On Tue, May 14, 2019 at 7:53 PM Philipp A. <notifications@github.com>. >> wrote:. >>. >>> You can just open a new one, Iâ€™ll close this one then ðŸ™‚. >>>. >>> â€”. >>> You are receiving this because you authored the thread. >>> Reply to this email directly, view it on GitHub. >>> <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOFHXLS2NZRCDSLJHE3PVKR2ZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVLHMMQ#issuecomment-492205618>,. >>> or mute the thread. >>> <https://github.com/notifications/unsubscribe-auth/ABREGOBMZBMFMNA6FCEMFULPVKR2ZANCNFSM4HKUCBXA>. >>> . >>>. >>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:28,usability,help,help,28,"Thanks alot Philip for your help, I just learned testing :). Hopefully this version will be accepted, as I tested now on my laptop and. push when it passed tests for all 5 plots. Thanks,. Khalid. On Mon, May 20, 2019 at 12:37 PM khalid usman <khalid0491@gmail.com> wrote:. > Hi Phillip,. >. > I have removed issue from the pull request by the testing tool, now the. > tools showed me duplications, which are mostly from other code and 1-2 from. > my code. Please have a look into it. It's my first pull request and its. > taking too much time :(. >. > Thanks. > Khalid. >. > On Tue, May 14, 2019 at 9:28 PM khalid usman <khalid0491@gmail.com> wrote:. >. >> Ok , thanks for letting me know. Please check the pull request. I have. >> verified my code by keeping weights 1 and it has same values when. >> observations has no weights or all weights equal to 1. >>. >> I also suggest to update PCA for weighted sampled data. >>. >> Thanks,. >> Khalid Usman. >>. >> On Tue, May 14, 2019 at 7:53 PM Philipp A. <notifications@github.com>. >> wrote:. >>. >>> You can just open a new one, Iâ€™ll close this one then ðŸ™‚. >>>. >>> â€”. >>> You are receiving this because you authored the thread. >>> Reply to this email directly, view it on GitHub. >>> <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOFHXLS2NZRCDSLJHE3PVKR2ZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVLHMMQ#issuecomment-492205618>,. >>> or mute the thread. >>> <https://github.com/notifications/unsubscribe-auth/ABREGOBMZBMFMNA6FCEMFULPVKR2ZANCNFSM4HKUCBXA>. >>> . >>>. >>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:41,usability,learn,learned,41,"Thanks alot Philip for your help, I just learned testing :). Hopefully this version will be accepted, as I tested now on my laptop and. push when it passed tests for all 5 plots. Thanks,. Khalid. On Mon, May 20, 2019 at 12:37 PM khalid usman <khalid0491@gmail.com> wrote:. > Hi Phillip,. >. > I have removed issue from the pull request by the testing tool, now the. > tools showed me duplications, which are mostly from other code and 1-2 from. > my code. Please have a look into it. It's my first pull request and its. > taking too much time :(. >. > Thanks. > Khalid. >. > On Tue, May 14, 2019 at 9:28 PM khalid usman <khalid0491@gmail.com> wrote:. >. >> Ok , thanks for letting me know. Please check the pull request. I have. >> verified my code by keeping weights 1 and it has same values when. >> observations has no weights or all weights equal to 1. >>. >> I also suggest to update PCA for weighted sampled data. >>. >> Thanks,. >> Khalid Usman. >>. >> On Tue, May 14, 2019 at 7:53 PM Philipp A. <notifications@github.com>. >> wrote:. >>. >>> You can just open a new one, Iâ€™ll close this one then ðŸ™‚. >>>. >>> â€”. >>> You are receiving this because you authored the thread. >>> Reply to this email directly, view it on GitHub. >>> <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOFHXLS2NZRCDSLJHE3PVKR2ZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVLHMMQ#issuecomment-492205618>,. >>> or mute the thread. >>> <https://github.com/notifications/unsubscribe-auth/ABREGOBMZBMFMNA6FCEMFULPVKR2ZANCNFSM4HKUCBXA>. >>> . >>>. >>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:351,usability,tool,tool,351,"Thanks alot Philip for your help, I just learned testing :). Hopefully this version will be accepted, as I tested now on my laptop and. push when it passed tests for all 5 plots. Thanks,. Khalid. On Mon, May 20, 2019 at 12:37 PM khalid usman <khalid0491@gmail.com> wrote:. > Hi Phillip,. >. > I have removed issue from the pull request by the testing tool, now the. > tools showed me duplications, which are mostly from other code and 1-2 from. > my code. Please have a look into it. It's my first pull request and its. > taking too much time :(. >. > Thanks. > Khalid. >. > On Tue, May 14, 2019 at 9:28 PM khalid usman <khalid0491@gmail.com> wrote:. >. >> Ok , thanks for letting me know. Please check the pull request. I have. >> verified my code by keeping weights 1 and it has same values when. >> observations has no weights or all weights equal to 1. >>. >> I also suggest to update PCA for weighted sampled data. >>. >> Thanks,. >> Khalid Usman. >>. >> On Tue, May 14, 2019 at 7:53 PM Philipp A. <notifications@github.com>. >> wrote:. >>. >>> You can just open a new one, Iâ€™ll close this one then ðŸ™‚. >>>. >>> â€”. >>> You are receiving this because you authored the thread. >>> Reply to this email directly, view it on GitHub. >>> <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOFHXLS2NZRCDSLJHE3PVKR2ZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVLHMMQ#issuecomment-492205618>,. >>> or mute the thread. >>> <https://github.com/notifications/unsubscribe-auth/ABREGOBMZBMFMNA6FCEMFULPVKR2ZANCNFSM4HKUCBXA>. >>> . >>>. >>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:368,usability,tool,tools,368,"Thanks alot Philip for your help, I just learned testing :). Hopefully this version will be accepted, as I tested now on my laptop and. push when it passed tests for all 5 plots. Thanks,. Khalid. On Mon, May 20, 2019 at 12:37 PM khalid usman <khalid0491@gmail.com> wrote:. > Hi Phillip,. >. > I have removed issue from the pull request by the testing tool, now the. > tools showed me duplications, which are mostly from other code and 1-2 from. > my code. Please have a look into it. It's my first pull request and its. > taking too much time :(. >. > Thanks. > Khalid. >. > On Tue, May 14, 2019 at 9:28 PM khalid usman <khalid0491@gmail.com> wrote:. >. >> Ok , thanks for letting me know. Please check the pull request. I have. >> verified my code by keeping weights 1 and it has same values when. >> observations has no weights or all weights equal to 1. >>. >> I also suggest to update PCA for weighted sampled data. >>. >> Thanks,. >> Khalid Usman. >>. >> On Tue, May 14, 2019 at 7:53 PM Philipp A. <notifications@github.com>. >> wrote:. >>. >>> You can just open a new one, Iâ€™ll close this one then ðŸ™‚. >>>. >>> â€”. >>> You are receiving this because you authored the thread. >>> Reply to this email directly, view it on GitHub. >>> <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOFHXLS2NZRCDSLJHE3PVKR2ZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVLHMMQ#issuecomment-492205618>,. >>> or mute the thread. >>> <https://github.com/notifications/unsubscribe-auth/ABREGOBMZBMFMNA6FCEMFULPVKR2ZANCNFSM4HKUCBXA>. >>> . >>>. >>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/pull/630:1084,usability,close,close,1084,"Thanks alot Philip for your help, I just learned testing :). Hopefully this version will be accepted, as I tested now on my laptop and. push when it passed tests for all 5 plots. Thanks,. Khalid. On Mon, May 20, 2019 at 12:37 PM khalid usman <khalid0491@gmail.com> wrote:. > Hi Phillip,. >. > I have removed issue from the pull request by the testing tool, now the. > tools showed me duplications, which are mostly from other code and 1-2 from. > my code. Please have a look into it. It's my first pull request and its. > taking too much time :(. >. > Thanks. > Khalid. >. > On Tue, May 14, 2019 at 9:28 PM khalid usman <khalid0491@gmail.com> wrote:. >. >> Ok , thanks for letting me know. Please check the pull request. I have. >> verified my code by keeping weights 1 and it has same values when. >> observations has no weights or all weights equal to 1. >>. >> I also suggest to update PCA for weighted sampled data. >>. >> Thanks,. >> Khalid Usman. >>. >> On Tue, May 14, 2019 at 7:53 PM Philipp A. <notifications@github.com>. >> wrote:. >>. >>> You can just open a new one, Iâ€™ll close this one then ðŸ™‚. >>>. >>> â€”. >>> You are receiving this because you authored the thread. >>> Reply to this email directly, view it on GitHub. >>> <https://github.com/theislab/scanpy/pull/630?email_source=notifications&email_token=ABREGOFHXLS2NZRCDSLJHE3PVKR2ZA5CNFSM4HKUCBXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVLHMMQ#issuecomment-492205618>,. >>> or mute the thread. >>> <https://github.com/notifications/unsubscribe-auth/ABREGOBMZBMFMNA6FCEMFULPVKR2ZANCNFSM4HKUCBXA>. >>> . >>>. >>.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/630
https://github.com/scverse/scanpy/issues/631:58,deployability,version,version,58,"No problem, I think that the change is not present in the version 1.4.2, I tried the github code and also the pip version. Thank you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/631
https://github.com/scverse/scanpy/issues/631:114,deployability,version,version,114,"No problem, I think that the change is not present in the version 1.4.2, I tried the github code and also the pip version. Thank you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/631
https://github.com/scverse/scanpy/issues/631:58,integrability,version,version,58,"No problem, I think that the change is not present in the version 1.4.2, I tried the github code and also the pip version. Thank you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/631
https://github.com/scverse/scanpy/issues/631:114,integrability,version,version,114,"No problem, I think that the change is not present in the version 1.4.2, I tried the github code and also the pip version. Thank you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/631
https://github.com/scverse/scanpy/issues/631:58,modifiability,version,version,58,"No problem, I think that the change is not present in the version 1.4.2, I tried the github code and also the pip version. Thank you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/631
https://github.com/scverse/scanpy/issues/631:114,modifiability,version,version,114,"No problem, I think that the change is not present in the version 1.4.2, I tried the github code and also the pip version. Thank you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/631
https://github.com/scverse/scanpy/issues/631:22,deployability,instal,install,22,It should work if you install from github. https://github.com/theislab/scanpy/commit/fe2580cb58e2ad6312ea989b0c9a40351510051a,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/631
https://github.com/scverse/scanpy/issues/631:104,deployability,instal,install,104,"Thanks. On Mon, 6 May 2019 at 18:49, Koncopd <notifications@github.com> wrote:. > It should work if you install from github. > fe2580c. > <https://github.com/theislab/scanpy/commit/fe2580cb58e2ad6312ea989b0c9a40351510051a>. >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/631#issuecomment-489690557>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ACPDY4SWBVPGTIQHOG365L3PUBOS3ANCNFSM4HLAPBTA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/631
https://github.com/scverse/scanpy/issues/631:481,security,auth,auth,481,"Thanks. On Mon, 6 May 2019 at 18:49, Koncopd <notifications@github.com> wrote:. > It should work if you install from github. > fe2580c. > <https://github.com/theislab/scanpy/commit/fe2580cb58e2ad6312ea989b0c9a40351510051a>. >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/631#issuecomment-489690557>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ACPDY4SWBVPGTIQHOG365L3PUBOS3ANCNFSM4HLAPBTA>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/631
https://github.com/scverse/scanpy/issues/632:83,deployability,releas,release,83,"As per the referencing issue, it looks like this should be fixed by the next bbknn release in a day or two. Thanks for the bug report @jipeifeng!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/632
https://github.com/scverse/scanpy/issues/632:200,availability,replic,replicate,200,"BBKNN 1.3.2 is on pip and should fix this issue. Note that trimming is now ran by default, so if you previously ran BBKNN without providing the `trim` parameter you'll now have to provide `trim=0` to replicate old behaviour.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/632
https://github.com/scverse/scanpy/issues/632:151,modifiability,paramet,parameter,151,"BBKNN 1.3.2 is on pip and should fix this issue. Note that trimming is now ran by default, so if you previously ran BBKNN without providing the `trim` parameter you'll now have to provide `trim=0` to replicate old behaviour.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/632
https://github.com/scverse/scanpy/issues/632:214,usability,behavi,behaviour,214,"BBKNN 1.3.2 is on pip and should fix this issue. Note that trimming is now ran by default, so if you previously ran BBKNN without providing the `trim` parameter you'll now have to provide `trim=0` to replicate old behaviour.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/632
https://github.com/scverse/scanpy/issues/633:968,availability,Cluster,ClusterMap,968,"Thanks for your information. I am surprised that this step is taking too. long as is was supposed to reduce the plotting time. I would not wait for. more than 5 minutes to see a plot. How many genes were you planning to plot? The background is that when plotting a heatmap, the matplotlib. visualization will randomly drop genes because the resolution of the. screens is not high enough. Thus, when the number of genes is large, I was. trying to find a compromise by fitting a line before the plotting and then. only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you. find an example that can reproduce the problem? On Tue, May 7, 2019 at 10:19 AM brianpenghe <notifications@github.com>. wrote:. > I was trying to plot a heatmap using this command:. > ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident',. > use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0,. > dendrogram=True, save='ClusterMap.png'). >. > And it didn't finish running after an overnight, with the following. > warning message:. > WARNING: Gene labels are not shown when more than 50 genes are visualized. > To show gene labels set show_gene_labels=True. > /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227:. > UserWarning:. > The maximal number of iterations maxit (set to 20 by the program). > allowed for finding a smoothing spline with fp=s has been reached: s. > too small. > There is an approximation returned but the corresponding weighted sum. > of squared residuals does not satisfy the condition abs(fp-s)/s < tol. > warnings.warn(message). >. > I don't understand why this is taking this long because seaborn was able. > to finish plotting within 30 minutes. Do you know why? >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/633>, or mute the thread. > <https://github.com/notifications/unsub",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:968,deployability,Cluster,ClusterMap,968,"Thanks for your information. I am surprised that this step is taking too. long as is was supposed to reduce the plotting time. I would not wait for. more than 5 minutes to see a plot. How many genes were you planning to plot? The background is that when plotting a heatmap, the matplotlib. visualization will randomly drop genes because the resolution of the. screens is not high enough. Thus, when the number of genes is large, I was. trying to find a compromise by fitting a line before the plotting and then. only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you. find an example that can reproduce the problem? On Tue, May 7, 2019 at 10:19 AM brianpenghe <notifications@github.com>. wrote:. > I was trying to plot a heatmap using this command:. > ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident',. > use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0,. > dendrogram=True, save='ClusterMap.png'). >. > And it didn't finish running after an overnight, with the following. > warning message:. > WARNING: Gene labels are not shown when more than 50 genes are visualized. > To show gene labels set show_gene_labels=True. > /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227:. > UserWarning:. > The maximal number of iterations maxit (set to 20 by the program). > allowed for finding a smoothing spline with fp=s has been reached: s. > too small. > There is an approximation returned but the corresponding weighted sum. > of squared residuals does not satisfy the condition abs(fp-s)/s < tol. > warnings.warn(message). >. > I don't understand why this is taking this long because seaborn was able. > to finish plotting within 30 minutes. Do you know why? >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/633>, or mute the thread. > <https://github.com/notifications/unsub",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:101,energy efficiency,reduc,reduce,101,"Thanks for your information. I am surprised that this step is taking too. long as is was supposed to reduce the plotting time. I would not wait for. more than 5 minutes to see a plot. How many genes were you planning to plot? The background is that when plotting a heatmap, the matplotlib. visualization will randomly drop genes because the resolution of the. screens is not high enough. Thus, when the number of genes is large, I was. trying to find a compromise by fitting a line before the plotting and then. only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you. find an example that can reproduce the problem? On Tue, May 7, 2019 at 10:19 AM brianpenghe <notifications@github.com>. wrote:. > I was trying to plot a heatmap using this command:. > ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident',. > use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0,. > dendrogram=True, save='ClusterMap.png'). >. > And it didn't finish running after an overnight, with the following. > warning message:. > WARNING: Gene labels are not shown when more than 50 genes are visualized. > To show gene labels set show_gene_labels=True. > /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227:. > UserWarning:. > The maximal number of iterations maxit (set to 20 by the program). > allowed for finding a smoothing spline with fp=s has been reached: s. > too small. > There is an approximation returned but the corresponding weighted sum. > of squared residuals does not satisfy the condition abs(fp-s)/s < tol. > warnings.warn(message). >. > I don't understand why this is taking this long because seaborn was able. > to finish plotting within 30 minutes. Do you know why? >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/633>, or mute the thread. > <https://github.com/notifications/unsub",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:265,energy efficiency,heat,heatmap,265,"Thanks for your information. I am surprised that this step is taking too. long as is was supposed to reduce the plotting time. I would not wait for. more than 5 minutes to see a plot. How many genes were you planning to plot? The background is that when plotting a heatmap, the matplotlib. visualization will randomly drop genes because the resolution of the. screens is not high enough. Thus, when the number of genes is large, I was. trying to find a compromise by fitting a line before the plotting and then. only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you. find an example that can reproduce the problem? On Tue, May 7, 2019 at 10:19 AM brianpenghe <notifications@github.com>. wrote:. > I was trying to plot a heatmap using this command:. > ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident',. > use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0,. > dendrogram=True, save='ClusterMap.png'). >. > And it didn't finish running after an overnight, with the following. > warning message:. > WARNING: Gene labels are not shown when more than 50 genes are visualized. > To show gene labels set show_gene_labels=True. > /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227:. > UserWarning:. > The maximal number of iterations maxit (set to 20 by the program). > allowed for finding a smoothing spline with fp=s has been reached: s. > too small. > There is an approximation returned but the corresponding weighted sum. > of squared residuals does not satisfy the condition abs(fp-s)/s < tol. > warnings.warn(message). >. > I don't understand why this is taking this long because seaborn was able. > to finish plotting within 30 minutes. Do you know why? >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/633>, or mute the thread. > <https://github.com/notifications/unsub",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:762,energy efficiency,heat,heatmap,762,"Thanks for your information. I am surprised that this step is taking too. long as is was supposed to reduce the plotting time. I would not wait for. more than 5 minutes to see a plot. How many genes were you planning to plot? The background is that when plotting a heatmap, the matplotlib. visualization will randomly drop genes because the resolution of the. screens is not high enough. Thus, when the number of genes is large, I was. trying to find a compromise by fitting a line before the plotting and then. only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you. find an example that can reproduce the problem? On Tue, May 7, 2019 at 10:19 AM brianpenghe <notifications@github.com>. wrote:. > I was trying to plot a heatmap using this command:. > ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident',. > use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0,. > dendrogram=True, save='ClusterMap.png'). >. > And it didn't finish running after an overnight, with the following. > warning message:. > WARNING: Gene labels are not shown when more than 50 genes are visualized. > To show gene labels set show_gene_labels=True. > /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227:. > UserWarning:. > The maximal number of iterations maxit (set to 20 by the program). > allowed for finding a smoothing spline with fp=s has been reached: s. > too small. > There is an approximation returned but the corresponding weighted sum. > of squared residuals does not satisfy the condition abs(fp-s)/s < tol. > warnings.warn(message). >. > I don't understand why this is taking this long because seaborn was able. > to finish plotting within 30 minutes. Do you know why? >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/633>, or mute the thread. > <https://github.com/notifications/unsub",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:803,energy efficiency,heat,heatmap,803,"Thanks for your information. I am surprised that this step is taking too. long as is was supposed to reduce the plotting time. I would not wait for. more than 5 minutes to see a plot. How many genes were you planning to plot? The background is that when plotting a heatmap, the matplotlib. visualization will randomly drop genes because the resolution of the. screens is not high enough. Thus, when the number of genes is large, I was. trying to find a compromise by fitting a line before the plotting and then. only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you. find an example that can reproduce the problem? On Tue, May 7, 2019 at 10:19 AM brianpenghe <notifications@github.com>. wrote:. > I was trying to plot a heatmap using this command:. > ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident',. > use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0,. > dendrogram=True, save='ClusterMap.png'). >. > And it didn't finish running after an overnight, with the following. > warning message:. > WARNING: Gene labels are not shown when more than 50 genes are visualized. > To show gene labels set show_gene_labels=True. > /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227:. > UserWarning:. > The maximal number of iterations maxit (set to 20 by the program). > allowed for finding a smoothing spline with fp=s has been reached: s. > too small. > There is an approximation returned but the corresponding weighted sum. > of squared residuals does not satisfy the condition abs(fp-s)/s < tol. > warnings.warn(message). >. > I don't understand why this is taking this long because seaborn was able. > to finish plotting within 30 minutes. Do you know why? >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/633>, or mute the thread. > <https://github.com/notifications/unsub",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:1070,integrability,messag,message,1070,"long as is was supposed to reduce the plotting time. I would not wait for. more than 5 minutes to see a plot. How many genes were you planning to plot? The background is that when plotting a heatmap, the matplotlib. visualization will randomly drop genes because the resolution of the. screens is not high enough. Thus, when the number of genes is large, I was. trying to find a compromise by fitting a line before the plotting and then. only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you. find an example that can reproduce the problem? On Tue, May 7, 2019 at 10:19 AM brianpenghe <notifications@github.com>. wrote:. > I was trying to plot a heatmap using this command:. > ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident',. > use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0,. > dendrogram=True, save='ClusterMap.png'). >. > And it didn't finish running after an overnight, with the following. > warning message:. > WARNING: Gene labels are not shown when more than 50 genes are visualized. > To show gene labels set show_gene_labels=True. > /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227:. > UserWarning:. > The maximal number of iterations maxit (set to 20 by the program). > allowed for finding a smoothing spline with fp=s has been reached: s. > too small. > There is an approximation returned but the corresponding weighted sum. > of squared residuals does not satisfy the condition abs(fp-s)/s < tol. > warnings.warn(message). >. > I don't understand why this is taking this long because seaborn was able. > to finish plotting within 30 minutes. Do you know why? >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/633>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VNDX37RZL256MWKDM3PUE3RFANCNFSM4HLGOYGA>. > . >. -- . Fid",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:1615,integrability,messag,message,1615,"was supposed to reduce the plotting time. I would not wait for. more than 5 minutes to see a plot. How many genes were you planning to plot? The background is that when plotting a heatmap, the matplotlib. visualization will randomly drop genes because the resolution of the. screens is not high enough. Thus, when the number of genes is large, I was. trying to find a compromise by fitting a line before the plotting and then. only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you. find an example that can reproduce the problem? On Tue, May 7, 2019 at 10:19 AM brianpenghe <notifications@github.com>. wrote:. > I was trying to plot a heatmap using this command:. > ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident',. > use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0,. > dendrogram=True, save='ClusterMap.png'). >. > And it didn't finish running after an overnight, with the following. > warning message:. > WARNING: Gene labels are not shown when more than 50 genes are visualized. > To show gene labels set show_gene_labels=True. > /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227:. > UserWarning:. > The maximal number of iterations maxit (set to 20 by the program). > allowed for finding a smoothing spline with fp=s has been reached: s. > too small. > There is an approximation returned but the corresponding weighted sum. > of squared residuals does not satisfy the condition abs(fp-s)/s < tol. > warnings.warn(message). >. > I don't understand why this is taking this long because seaborn was able. > to finish plotting within 30 minutes. Do you know why? >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/633>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VNDX37RZL256MWKDM3PUE3RFANCNFSM4HLGOYGA>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:1810,integrability,sub,subscribed,1810,"was supposed to reduce the plotting time. I would not wait for. more than 5 minutes to see a plot. How many genes were you planning to plot? The background is that when plotting a heatmap, the matplotlib. visualization will randomly drop genes because the resolution of the. screens is not high enough. Thus, when the number of genes is large, I was. trying to find a compromise by fitting a line before the plotting and then. only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you. find an example that can reproduce the problem? On Tue, May 7, 2019 at 10:19 AM brianpenghe <notifications@github.com>. wrote:. > I was trying to plot a heatmap using this command:. > ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident',. > use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0,. > dendrogram=True, save='ClusterMap.png'). >. > And it didn't finish running after an overnight, with the following. > warning message:. > WARNING: Gene labels are not shown when more than 50 genes are visualized. > To show gene labels set show_gene_labels=True. > /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227:. > UserWarning:. > The maximal number of iterations maxit (set to 20 by the program). > allowed for finding a smoothing spline with fp=s has been reached: s. > too small. > There is an approximation returned but the corresponding weighted sum. > of squared residuals does not satisfy the condition abs(fp-s)/s < tol. > warnings.warn(message). >. > I don't understand why this is taking this long because seaborn was able. > to finish plotting within 30 minutes. Do you know why? >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/633>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VNDX37RZL256MWKDM3PUE3RFANCNFSM4HLGOYGA>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:1070,interoperability,messag,message,1070,"long as is was supposed to reduce the plotting time. I would not wait for. more than 5 minutes to see a plot. How many genes were you planning to plot? The background is that when plotting a heatmap, the matplotlib. visualization will randomly drop genes because the resolution of the. screens is not high enough. Thus, when the number of genes is large, I was. trying to find a compromise by fitting a line before the plotting and then. only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you. find an example that can reproduce the problem? On Tue, May 7, 2019 at 10:19 AM brianpenghe <notifications@github.com>. wrote:. > I was trying to plot a heatmap using this command:. > ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident',. > use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0,. > dendrogram=True, save='ClusterMap.png'). >. > And it didn't finish running after an overnight, with the following. > warning message:. > WARNING: Gene labels are not shown when more than 50 genes are visualized. > To show gene labels set show_gene_labels=True. > /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227:. > UserWarning:. > The maximal number of iterations maxit (set to 20 by the program). > allowed for finding a smoothing spline with fp=s has been reached: s. > too small. > There is an approximation returned but the corresponding weighted sum. > of squared residuals does not satisfy the condition abs(fp-s)/s < tol. > warnings.warn(message). >. > I don't understand why this is taking this long because seaborn was able. > to finish plotting within 30 minutes. Do you know why? >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/633>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VNDX37RZL256MWKDM3PUE3RFANCNFSM4HLGOYGA>. > . >. -- . Fid",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:1615,interoperability,messag,message,1615,"was supposed to reduce the plotting time. I would not wait for. more than 5 minutes to see a plot. How many genes were you planning to plot? The background is that when plotting a heatmap, the matplotlib. visualization will randomly drop genes because the resolution of the. screens is not high enough. Thus, when the number of genes is large, I was. trying to find a compromise by fitting a line before the plotting and then. only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you. find an example that can reproduce the problem? On Tue, May 7, 2019 at 10:19 AM brianpenghe <notifications@github.com>. wrote:. > I was trying to plot a heatmap using this command:. > ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident',. > use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0,. > dendrogram=True, save='ClusterMap.png'). >. > And it didn't finish running after an overnight, with the following. > warning message:. > WARNING: Gene labels are not shown when more than 50 genes are visualized. > To show gene labels set show_gene_labels=True. > /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227:. > UserWarning:. > The maximal number of iterations maxit (set to 20 by the program). > allowed for finding a smoothing spline with fp=s has been reached: s. > too small. > There is an approximation returned but the corresponding weighted sum. > of squared residuals does not satisfy the condition abs(fp-s)/s < tol. > warnings.warn(message). >. > I don't understand why this is taking this long because seaborn was able. > to finish plotting within 30 minutes. Do you know why? >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/633>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VNDX37RZL256MWKDM3PUE3RFANCNFSM4HLGOYGA>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:1238,modifiability,pac,packages,1238,"was supposed to reduce the plotting time. I would not wait for. more than 5 minutes to see a plot. How many genes were you planning to plot? The background is that when plotting a heatmap, the matplotlib. visualization will randomly drop genes because the resolution of the. screens is not high enough. Thus, when the number of genes is large, I was. trying to find a compromise by fitting a line before the plotting and then. only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you. find an example that can reproduce the problem? On Tue, May 7, 2019 at 10:19 AM brianpenghe <notifications@github.com>. wrote:. > I was trying to plot a heatmap using this command:. > ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident',. > use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0,. > dendrogram=True, save='ClusterMap.png'). >. > And it didn't finish running after an overnight, with the following. > warning message:. > WARNING: Gene labels are not shown when more than 50 genes are visualized. > To show gene labels set show_gene_labels=True. > /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227:. > UserWarning:. > The maximal number of iterations maxit (set to 20 by the program). > allowed for finding a smoothing spline with fp=s has been reached: s. > too small. > There is an approximation returned but the corresponding weighted sum. > of squared residuals does not satisfy the condition abs(fp-s)/s < tol. > warnings.warn(message). >. > I don't understand why this is taking this long because seaborn was able. > to finish plotting within 30 minutes. Do you know why? >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/633>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VNDX37RZL256MWKDM3PUE3RFANCNFSM4HLGOYGA>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:121,performance,time,time,121,"Thanks for your information. I am surprised that this step is taking too. long as is was supposed to reduce the plotting time. I would not wait for. more than 5 minutes to see a plot. How many genes were you planning to plot? The background is that when plotting a heatmap, the matplotlib. visualization will randomly drop genes because the resolution of the. screens is not high enough. Thus, when the number of genes is large, I was. trying to find a compromise by fitting a line before the plotting and then. only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you. find an example that can reproduce the problem? On Tue, May 7, 2019 at 10:19 AM brianpenghe <notifications@github.com>. wrote:. > I was trying to plot a heatmap using this command:. > ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident',. > use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0,. > dendrogram=True, save='ClusterMap.png'). >. > And it didn't finish running after an overnight, with the following. > warning message:. > WARNING: Gene labels are not shown when more than 50 genes are visualized. > To show gene labels set show_gene_labels=True. > /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227:. > UserWarning:. > The maximal number of iterations maxit (set to 20 by the program). > allowed for finding a smoothing spline with fp=s has been reached: s. > too small. > There is an approximation returned but the corresponding weighted sum. > of squared residuals does not satisfy the condition abs(fp-s)/s < tol. > warnings.warn(message). >. > I don't understand why this is taking this long because seaborn was able. > to finish plotting within 30 minutes. Do you know why? >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/633>, or mute the thread. > <https://github.com/notifications/unsub",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:1549,reliability,doe,does,1549,"was supposed to reduce the plotting time. I would not wait for. more than 5 minutes to see a plot. How many genes were you planning to plot? The background is that when plotting a heatmap, the matplotlib. visualization will randomly drop genes because the resolution of the. screens is not high enough. Thus, when the number of genes is large, I was. trying to find a compromise by fitting a line before the plotting and then. only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you. find an example that can reproduce the problem? On Tue, May 7, 2019 at 10:19 AM brianpenghe <notifications@github.com>. wrote:. > I was trying to plot a heatmap using this command:. > ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident',. > use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0,. > dendrogram=True, save='ClusterMap.png'). >. > And it didn't finish running after an overnight, with the following. > warning message:. > WARNING: Gene labels are not shown when more than 50 genes are visualized. > To show gene labels set show_gene_labels=True. > /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227:. > UserWarning:. > The maximal number of iterations maxit (set to 20 by the program). > allowed for finding a smoothing spline with fp=s has been reached: s. > too small. > There is an approximation returned but the corresponding weighted sum. > of squared residuals does not satisfy the condition abs(fp-s)/s < tol. > warnings.warn(message). >. > I don't understand why this is taking this long because seaborn was able. > to finish plotting within 30 minutes. Do you know why? >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/633>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VNDX37RZL256MWKDM3PUE3RFANCNFSM4HLGOYGA>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:855,security,ident,ident,855,"Thanks for your information. I am surprised that this step is taking too. long as is was supposed to reduce the plotting time. I would not wait for. more than 5 minutes to see a plot. How many genes were you planning to plot? The background is that when plotting a heatmap, the matplotlib. visualization will randomly drop genes because the resolution of the. screens is not high enough. Thus, when the number of genes is large, I was. trying to find a compromise by fitting a line before the plotting and then. only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you. find an example that can reproduce the problem? On Tue, May 7, 2019 at 10:19 AM brianpenghe <notifications@github.com>. wrote:. > I was trying to plot a heatmap using this command:. > ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident',. > use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0,. > dendrogram=True, save='ClusterMap.png'). >. > And it didn't finish running after an overnight, with the following. > warning message:. > WARNING: Gene labels are not shown when more than 50 genes are visualized. > To show gene labels set show_gene_labels=True. > /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227:. > UserWarning:. > The maximal number of iterations maxit (set to 20 by the program). > allowed for finding a smoothing spline with fp=s has been reached: s. > too small. > There is an approximation returned but the corresponding weighted sum. > of squared residuals does not satisfy the condition abs(fp-s)/s < tol. > warnings.warn(message). >. > I don't understand why this is taking this long because seaborn was able. > to finish plotting within 30 minutes. Do you know why? >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/633>, or mute the thread. > <https://github.com/notifications/unsub",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:2007,security,auth,auth,2007,"was supposed to reduce the plotting time. I would not wait for. more than 5 minutes to see a plot. How many genes were you planning to plot? The background is that when plotting a heatmap, the matplotlib. visualization will randomly drop genes because the resolution of the. screens is not high enough. Thus, when the number of genes is large, I was. trying to find a compromise by fitting a line before the plotting and then. only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you. find an example that can reproduce the problem? On Tue, May 7, 2019 at 10:19 AM brianpenghe <notifications@github.com>. wrote:. > I was trying to plot a heatmap using this command:. > ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident',. > use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0,. > dendrogram=True, save='ClusterMap.png'). >. > And it didn't finish running after an overnight, with the following. > warning message:. > WARNING: Gene labels are not shown when more than 50 genes are visualized. > To show gene labels set show_gene_labels=True. > /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227:. > UserWarning:. > The maximal number of iterations maxit (set to 20 by the program). > allowed for finding a smoothing spline with fp=s has been reached: s. > too small. > There is an approximation returned but the corresponding weighted sum. > of squared residuals does not satisfy the condition abs(fp-s)/s < tol. > warnings.warn(message). >. > I don't understand why this is taking this long because seaborn was able. > to finish plotting within 30 minutes. Do you know why? >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/633>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VNDX37RZL256MWKDM3PUE3RFANCNFSM4HLGOYGA>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:208,testability,plan,planning,208,"Thanks for your information. I am surprised that this step is taking too. long as is was supposed to reduce the plotting time. I would not wait for. more than 5 minutes to see a plot. How many genes were you planning to plot? The background is that when plotting a heatmap, the matplotlib. visualization will randomly drop genes because the resolution of the. screens is not high enough. Thus, when the number of genes is large, I was. trying to find a compromise by fitting a line before the plotting and then. only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you. find an example that can reproduce the problem? On Tue, May 7, 2019 at 10:19 AM brianpenghe <notifications@github.com>. wrote:. > I was trying to plot a heatmap using this command:. > ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident',. > use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0,. > dendrogram=True, save='ClusterMap.png'). >. > And it didn't finish running after an overnight, with the following. > warning message:. > WARNING: Gene labels are not shown when more than 50 genes are visualized. > To show gene labels set show_gene_labels=True. > /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227:. > UserWarning:. > The maximal number of iterations maxit (set to 20 by the program). > allowed for finding a smoothing spline with fp=s has been reached: s. > too small. > There is an approximation returned but the corresponding weighted sum. > of squared residuals does not satisfy the condition abs(fp-s)/s < tol. > warnings.warn(message). >. > I don't understand why this is taking this long because seaborn was able. > to finish plotting within 30 minutes. Do you know why? >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/633>, or mute the thread. > <https://github.com/notifications/unsub",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:1638,testability,understand,understand,1638,"was supposed to reduce the plotting time. I would not wait for. more than 5 minutes to see a plot. How many genes were you planning to plot? The background is that when plotting a heatmap, the matplotlib. visualization will randomly drop genes because the resolution of the. screens is not high enough. Thus, when the number of genes is large, I was. trying to find a compromise by fitting a line before the plotting and then. only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you. find an example that can reproduce the problem? On Tue, May 7, 2019 at 10:19 AM brianpenghe <notifications@github.com>. wrote:. > I was trying to plot a heatmap using this command:. > ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident',. > use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0,. > dendrogram=True, save='ClusterMap.png'). >. > And it didn't finish running after an overnight, with the following. > warning message:. > WARNING: Gene labels are not shown when more than 50 genes are visualized. > To show gene labels set show_gene_labels=True. > /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227:. > UserWarning:. > The maximal number of iterations maxit (set to 20 by the program). > allowed for finding a smoothing spline with fp=s has been reached: s. > too small. > There is an approximation returned but the corresponding weighted sum. > of squared residuals does not satisfy the condition abs(fp-s)/s < tol. > warnings.warn(message). >. > I don't understand why this is taking this long because seaborn was able. > to finish plotting within 30 minutes. Do you know why? >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/633>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VNDX37RZL256MWKDM3PUE3RFANCNFSM4HLGOYGA>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:290,usability,visual,visualization,290,"Thanks for your information. I am surprised that this step is taking too. long as is was supposed to reduce the plotting time. I would not wait for. more than 5 minutes to see a plot. How many genes were you planning to plot? The background is that when plotting a heatmap, the matplotlib. visualization will randomly drop genes because the resolution of the. screens is not high enough. Thus, when the number of genes is large, I was. trying to find a compromise by fitting a line before the plotting and then. only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you. find an example that can reproduce the problem? On Tue, May 7, 2019 at 10:19 AM brianpenghe <notifications@github.com>. wrote:. > I was trying to plot a heatmap using this command:. > ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident',. > use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0,. > dendrogram=True, save='ClusterMap.png'). >. > And it didn't finish running after an overnight, with the following. > warning message:. > WARNING: Gene labels are not shown when more than 50 genes are visualized. > To show gene labels set show_gene_labels=True. > /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227:. > UserWarning:. > The maximal number of iterations maxit (set to 20 by the program). > allowed for finding a smoothing spline with fp=s has been reached: s. > too small. > There is an approximation returned but the corresponding weighted sum. > of squared residuals does not satisfy the condition abs(fp-s)/s < tol. > warnings.warn(message). >. > I don't understand why this is taking this long because seaborn was able. > to finish plotting within 30 minutes. Do you know why? >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/633>, or mute the thread. > <https://github.com/notifications/unsub",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:543,usability,help,help,543,"Thanks for your information. I am surprised that this step is taking too. long as is was supposed to reduce the plotting time. I would not wait for. more than 5 minutes to see a plot. How many genes were you planning to plot? The background is that when plotting a heatmap, the matplotlib. visualization will randomly drop genes because the resolution of the. screens is not high enough. Thus, when the number of genes is large, I was. trying to find a compromise by fitting a line before the plotting and then. only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you. find an example that can reproduce the problem? On Tue, May 7, 2019 at 10:19 AM brianpenghe <notifications@github.com>. wrote:. > I was trying to plot a heatmap using this command:. > ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident',. > use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0,. > dendrogram=True, save='ClusterMap.png'). >. > And it didn't finish running after an overnight, with the following. > warning message:. > WARNING: Gene labels are not shown when more than 50 genes are visualized. > To show gene labels set show_gene_labels=True. > /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227:. > UserWarning:. > The maximal number of iterations maxit (set to 20 by the program). > allowed for finding a smoothing spline with fp=s has been reached: s. > too small. > There is an approximation returned but the corresponding weighted sum. > of squared residuals does not satisfy the condition abs(fp-s)/s < tol. > warnings.warn(message). >. > I don't understand why this is taking this long because seaborn was able. > to finish plotting within 30 minutes. Do you know why? >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/633>, or mute the thread. > <https://github.com/notifications/unsub",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:781,usability,command,command,781,"Thanks for your information. I am surprised that this step is taking too. long as is was supposed to reduce the plotting time. I would not wait for. more than 5 minutes to see a plot. How many genes were you planning to plot? The background is that when plotting a heatmap, the matplotlib. visualization will randomly drop genes because the resolution of the. screens is not high enough. Thus, when the number of genes is large, I was. trying to find a compromise by fitting a line before the plotting and then. only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you. find an example that can reproduce the problem? On Tue, May 7, 2019 at 10:19 AM brianpenghe <notifications@github.com>. wrote:. > I was trying to plot a heatmap using this command:. > ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident',. > use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0,. > dendrogram=True, save='ClusterMap.png'). >. > And it didn't finish running after an overnight, with the following. > warning message:. > WARNING: Gene labels are not shown when more than 50 genes are visualized. > To show gene labels set show_gene_labels=True. > /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227:. > UserWarning:. > The maximal number of iterations maxit (set to 20 by the program). > allowed for finding a smoothing spline with fp=s has been reached: s. > too small. > There is an approximation returned but the corresponding weighted sum. > of squared residuals does not satisfy the condition abs(fp-s)/s < tol. > warnings.warn(message). >. > I don't understand why this is taking this long because seaborn was able. > to finish plotting within 30 minutes. Do you know why? >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/633>, or mute the thread. > <https://github.com/notifications/unsub",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:1145,usability,visual,visualized,1145,"was supposed to reduce the plotting time. I would not wait for. more than 5 minutes to see a plot. How many genes were you planning to plot? The background is that when plotting a heatmap, the matplotlib. visualization will randomly drop genes because the resolution of the. screens is not high enough. Thus, when the number of genes is large, I was. trying to find a compromise by fitting a line before the plotting and then. only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you. find an example that can reproduce the problem? On Tue, May 7, 2019 at 10:19 AM brianpenghe <notifications@github.com>. wrote:. > I was trying to plot a heatmap using this command:. > ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident',. > use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0,. > dendrogram=True, save='ClusterMap.png'). >. > And it didn't finish running after an overnight, with the following. > warning message:. > WARNING: Gene labels are not shown when more than 50 genes are visualized. > To show gene labels set show_gene_labels=True. > /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227:. > UserWarning:. > The maximal number of iterations maxit (set to 20 by the program). > allowed for finding a smoothing spline with fp=s has been reached: s. > too small. > There is an approximation returned but the corresponding weighted sum. > of squared residuals does not satisfy the condition abs(fp-s)/s < tol. > warnings.warn(message). >. > I don't understand why this is taking this long because seaborn was able. > to finish plotting within 30 minutes. Do you know why? >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/633>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VNDX37RZL256MWKDM3PUE3RFANCNFSM4HLGOYGA>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:1285,usability,User,UserWarning,1285,"was supposed to reduce the plotting time. I would not wait for. more than 5 minutes to see a plot. How many genes were you planning to plot? The background is that when plotting a heatmap, the matplotlib. visualization will randomly drop genes because the resolution of the. screens is not high enough. Thus, when the number of genes is large, I was. trying to find a compromise by fitting a line before the plotting and then. only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you. find an example that can reproduce the problem? On Tue, May 7, 2019 at 10:19 AM brianpenghe <notifications@github.com>. wrote:. > I was trying to plot a heatmap using this command:. > ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident',. > use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0,. > dendrogram=True, save='ClusterMap.png'). >. > And it didn't finish running after an overnight, with the following. > warning message:. > WARNING: Gene labels are not shown when more than 50 genes are visualized. > To show gene labels set show_gene_labels=True. > /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227:. > UserWarning:. > The maximal number of iterations maxit (set to 20 by the program). > allowed for finding a smoothing spline with fp=s has been reached: s. > too small. > There is an approximation returned but the corresponding weighted sum. > of squared residuals does not satisfy the condition abs(fp-s)/s < tol. > warnings.warn(message). >. > I don't understand why this is taking this long because seaborn was able. > to finish plotting within 30 minutes. Do you know why? >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/633>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VNDX37RZL256MWKDM3PUE3RFANCNFSM4HLGOYGA>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:948,availability,Cluster,ClusterMap,948,"> Thanks for your information. I am surprised that this step is taking too long as is was supposed to reduce the plotting time. I would not wait for more than 5 minutes to see a plot. How many genes were you planning to plot? The background is that when plotting a heatmap, the matplotlib visualization will randomly drop genes because the resolution of the screens is not high enough. Thus, when the number of genes is large, I was trying to find a compromise by fitting a line before the plotting and then only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you find an example that can reproduce the problem? > [â€¦](#). > On Tue, May 7, 2019 at 10:19 AM brianpenghe ***@***.***> wrote: I was trying to plot a heatmap using this command: ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0, dendrogram=True, save='ClusterMap.png') And it didn't finish running after an overnight, with the following warning message: WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set show_gene_labels=True /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227: UserWarning: The maximal number of iterations maxit (set to 20 by the program) allowed for finding a smoothing spline with fp=s has been reached: s too small. There is an approximation returned but the corresponding weighted sum of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message) I don't understand why this is taking this long because seaborn was able to finish plotting within 30 minutes. Do you know why? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#633>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ABF37VNDX37RZL256MWKDM3PUE3RFANCNFSM4HLGOYGA> . > -- Fidel Ramirez. I was planning to plot a heatmap of 300 ge",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:2116,availability,cluster,clustering,2116,"ting and then only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you find an example that can reproduce the problem? > [â€¦](#). > On Tue, May 7, 2019 at 10:19 AM brianpenghe ***@***.***> wrote: I was trying to plot a heatmap using this command: ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0, dendrogram=True, save='ClusterMap.png') And it didn't finish running after an overnight, with the following warning message: WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set show_gene_labels=True /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227: UserWarning: The maximal number of iterations maxit (set to 20 by the program) allowed for finding a smoothing spline with fp=s has been reached: s too small. There is an approximation returned but the corresponding weighted sum of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message) I don't understand why this is taking this long because seaborn was able to finish plotting within 30 minutes. Do you know why? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#633>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ABF37VNDX37RZL256MWKDM3PUE3RFANCNFSM4HLGOYGA> . > -- Fidel Ramirez. I was planning to plot a heatmap of 300 genes. However, I have 90k cells. I guess the time-consuming part is the PCA because that's what's required to do the clustering by groups. I thought a naive way to do the clustering is just to construct the ""pseudobulks"" for each group by calculating the average and then simply clustering the ""pseudobulks"", instead of trying to look at individual cells. Another advantage of checking the pseudobulk is that the size of each group won't affect the landscape of principle components in that way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:2170,availability,cluster,clustering,2170,"ting and then only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you find an example that can reproduce the problem? > [â€¦](#). > On Tue, May 7, 2019 at 10:19 AM brianpenghe ***@***.***> wrote: I was trying to plot a heatmap using this command: ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0, dendrogram=True, save='ClusterMap.png') And it didn't finish running after an overnight, with the following warning message: WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set show_gene_labels=True /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227: UserWarning: The maximal number of iterations maxit (set to 20 by the program) allowed for finding a smoothing spline with fp=s has been reached: s too small. There is an approximation returned but the corresponding weighted sum of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message) I don't understand why this is taking this long because seaborn was able to finish plotting within 30 minutes. Do you know why? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#633>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ABF37VNDX37RZL256MWKDM3PUE3RFANCNFSM4HLGOYGA> . > -- Fidel Ramirez. I was planning to plot a heatmap of 300 genes. However, I have 90k cells. I guess the time-consuming part is the PCA because that's what's required to do the clustering by groups. I thought a naive way to do the clustering is just to construct the ""pseudobulks"" for each group by calculating the average and then simply clustering the ""pseudobulks"", instead of trying to look at individual cells. Another advantage of checking the pseudobulk is that the size of each group won't affect the landscape of principle components in that way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:2278,availability,cluster,clustering,2278,"ting and then only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you find an example that can reproduce the problem? > [â€¦](#). > On Tue, May 7, 2019 at 10:19 AM brianpenghe ***@***.***> wrote: I was trying to plot a heatmap using this command: ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0, dendrogram=True, save='ClusterMap.png') And it didn't finish running after an overnight, with the following warning message: WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set show_gene_labels=True /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227: UserWarning: The maximal number of iterations maxit (set to 20 by the program) allowed for finding a smoothing spline with fp=s has been reached: s too small. There is an approximation returned but the corresponding weighted sum of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message) I don't understand why this is taking this long because seaborn was able to finish plotting within 30 minutes. Do you know why? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#633>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ABF37VNDX37RZL256MWKDM3PUE3RFANCNFSM4HLGOYGA> . > -- Fidel Ramirez. I was planning to plot a heatmap of 300 genes. However, I have 90k cells. I guess the time-consuming part is the PCA because that's what's required to do the clustering by groups. I thought a naive way to do the clustering is just to construct the ""pseudobulks"" for each group by calculating the average and then simply clustering the ""pseudobulks"", instead of trying to look at individual cells. Another advantage of checking the pseudobulk is that the size of each group won't affect the landscape of principle components in that way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:948,deployability,Cluster,ClusterMap,948,"> Thanks for your information. I am surprised that this step is taking too long as is was supposed to reduce the plotting time. I would not wait for more than 5 minutes to see a plot. How many genes were you planning to plot? The background is that when plotting a heatmap, the matplotlib visualization will randomly drop genes because the resolution of the screens is not high enough. Thus, when the number of genes is large, I was trying to find a compromise by fitting a line before the plotting and then only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you find an example that can reproduce the problem? > [â€¦](#). > On Tue, May 7, 2019 at 10:19 AM brianpenghe ***@***.***> wrote: I was trying to plot a heatmap using this command: ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0, dendrogram=True, save='ClusterMap.png') And it didn't finish running after an overnight, with the following warning message: WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set show_gene_labels=True /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227: UserWarning: The maximal number of iterations maxit (set to 20 by the program) allowed for finding a smoothing spline with fp=s has been reached: s too small. There is an approximation returned but the corresponding weighted sum of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message) I don't understand why this is taking this long because seaborn was able to finish plotting within 30 minutes. Do you know why? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#633>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ABF37VNDX37RZL256MWKDM3PUE3RFANCNFSM4HLGOYGA> . > -- Fidel Ramirez. I was planning to plot a heatmap of 300 ge",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:2116,deployability,cluster,clustering,2116,"ting and then only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you find an example that can reproduce the problem? > [â€¦](#). > On Tue, May 7, 2019 at 10:19 AM brianpenghe ***@***.***> wrote: I was trying to plot a heatmap using this command: ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0, dendrogram=True, save='ClusterMap.png') And it didn't finish running after an overnight, with the following warning message: WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set show_gene_labels=True /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227: UserWarning: The maximal number of iterations maxit (set to 20 by the program) allowed for finding a smoothing spline with fp=s has been reached: s too small. There is an approximation returned but the corresponding weighted sum of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message) I don't understand why this is taking this long because seaborn was able to finish plotting within 30 minutes. Do you know why? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#633>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ABF37VNDX37RZL256MWKDM3PUE3RFANCNFSM4HLGOYGA> . > -- Fidel Ramirez. I was planning to plot a heatmap of 300 genes. However, I have 90k cells. I guess the time-consuming part is the PCA because that's what's required to do the clustering by groups. I thought a naive way to do the clustering is just to construct the ""pseudobulks"" for each group by calculating the average and then simply clustering the ""pseudobulks"", instead of trying to look at individual cells. Another advantage of checking the pseudobulk is that the size of each group won't affect the landscape of principle components in that way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:2170,deployability,cluster,clustering,2170,"ting and then only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you find an example that can reproduce the problem? > [â€¦](#). > On Tue, May 7, 2019 at 10:19 AM brianpenghe ***@***.***> wrote: I was trying to plot a heatmap using this command: ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0, dendrogram=True, save='ClusterMap.png') And it didn't finish running after an overnight, with the following warning message: WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set show_gene_labels=True /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227: UserWarning: The maximal number of iterations maxit (set to 20 by the program) allowed for finding a smoothing spline with fp=s has been reached: s too small. There is an approximation returned but the corresponding weighted sum of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message) I don't understand why this is taking this long because seaborn was able to finish plotting within 30 minutes. Do you know why? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#633>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ABF37VNDX37RZL256MWKDM3PUE3RFANCNFSM4HLGOYGA> . > -- Fidel Ramirez. I was planning to plot a heatmap of 300 genes. However, I have 90k cells. I guess the time-consuming part is the PCA because that's what's required to do the clustering by groups. I thought a naive way to do the clustering is just to construct the ""pseudobulks"" for each group by calculating the average and then simply clustering the ""pseudobulks"", instead of trying to look at individual cells. Another advantage of checking the pseudobulk is that the size of each group won't affect the landscape of principle components in that way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:2278,deployability,cluster,clustering,2278,"ting and then only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you find an example that can reproduce the problem? > [â€¦](#). > On Tue, May 7, 2019 at 10:19 AM brianpenghe ***@***.***> wrote: I was trying to plot a heatmap using this command: ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0, dendrogram=True, save='ClusterMap.png') And it didn't finish running after an overnight, with the following warning message: WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set show_gene_labels=True /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227: UserWarning: The maximal number of iterations maxit (set to 20 by the program) allowed for finding a smoothing spline with fp=s has been reached: s too small. There is an approximation returned but the corresponding weighted sum of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message) I don't understand why this is taking this long because seaborn was able to finish plotting within 30 minutes. Do you know why? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#633>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ABF37VNDX37RZL256MWKDM3PUE3RFANCNFSM4HLGOYGA> . > -- Fidel Ramirez. I was planning to plot a heatmap of 300 genes. However, I have 90k cells. I guess the time-consuming part is the PCA because that's what's required to do the clustering by groups. I thought a naive way to do the clustering is just to construct the ""pseudobulks"" for each group by calculating the average and then simply clustering the ""pseudobulks"", instead of trying to look at individual cells. Another advantage of checking the pseudobulk is that the size of each group won't affect the landscape of principle components in that way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:102,energy efficiency,reduc,reduce,102,"> Thanks for your information. I am surprised that this step is taking too long as is was supposed to reduce the plotting time. I would not wait for more than 5 minutes to see a plot. How many genes were you planning to plot? The background is that when plotting a heatmap, the matplotlib visualization will randomly drop genes because the resolution of the screens is not high enough. Thus, when the number of genes is large, I was trying to find a compromise by fitting a line before the plotting and then only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you find an example that can reproduce the problem? > [â€¦](#). > On Tue, May 7, 2019 at 10:19 AM brianpenghe ***@***.***> wrote: I was trying to plot a heatmap using this command: ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0, dendrogram=True, save='ClusterMap.png') And it didn't finish running after an overnight, with the following warning message: WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set show_gene_labels=True /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227: UserWarning: The maximal number of iterations maxit (set to 20 by the program) allowed for finding a smoothing spline with fp=s has been reached: s too small. There is an approximation returned but the corresponding weighted sum of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message) I don't understand why this is taking this long because seaborn was able to finish plotting within 30 minutes. Do you know why? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#633>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ABF37VNDX37RZL256MWKDM3PUE3RFANCNFSM4HLGOYGA> . > -- Fidel Ramirez. I was planning to plot a heatmap of 300 ge",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:265,energy efficiency,heat,heatmap,265,"> Thanks for your information. I am surprised that this step is taking too long as is was supposed to reduce the plotting time. I would not wait for more than 5 minutes to see a plot. How many genes were you planning to plot? The background is that when plotting a heatmap, the matplotlib visualization will randomly drop genes because the resolution of the screens is not high enough. Thus, when the number of genes is large, I was trying to find a compromise by fitting a line before the plotting and then only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you find an example that can reproduce the problem? > [â€¦](#). > On Tue, May 7, 2019 at 10:19 AM brianpenghe ***@***.***> wrote: I was trying to plot a heatmap using this command: ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0, dendrogram=True, save='ClusterMap.png') And it didn't finish running after an overnight, with the following warning message: WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set show_gene_labels=True /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227: UserWarning: The maximal number of iterations maxit (set to 20 by the program) allowed for finding a smoothing spline with fp=s has been reached: s too small. There is an approximation returned but the corresponding weighted sum of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message) I don't understand why this is taking this long because seaborn was able to finish plotting within 30 minutes. Do you know why? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#633>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ABF37VNDX37RZL256MWKDM3PUE3RFANCNFSM4HLGOYGA> . > -- Fidel Ramirez. I was planning to plot a heatmap of 300 ge",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:751,energy efficiency,heat,heatmap,751,"> Thanks for your information. I am surprised that this step is taking too long as is was supposed to reduce the plotting time. I would not wait for more than 5 minutes to see a plot. How many genes were you planning to plot? The background is that when plotting a heatmap, the matplotlib visualization will randomly drop genes because the resolution of the screens is not high enough. Thus, when the number of genes is large, I was trying to find a compromise by fitting a line before the plotting and then only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you find an example that can reproduce the problem? > [â€¦](#). > On Tue, May 7, 2019 at 10:19 AM brianpenghe ***@***.***> wrote: I was trying to plot a heatmap using this command: ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0, dendrogram=True, save='ClusterMap.png') And it didn't finish running after an overnight, with the following warning message: WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set show_gene_labels=True /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227: UserWarning: The maximal number of iterations maxit (set to 20 by the program) allowed for finding a smoothing spline with fp=s has been reached: s too small. There is an approximation returned but the corresponding weighted sum of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message) I don't understand why this is taking this long because seaborn was able to finish plotting within 30 minutes. Do you know why? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#633>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ABF37VNDX37RZL256MWKDM3PUE3RFANCNFSM4HLGOYGA> . > -- Fidel Ramirez. I was planning to plot a heatmap of 300 ge",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:789,energy efficiency,heat,heatmap,789,"> Thanks for your information. I am surprised that this step is taking too long as is was supposed to reduce the plotting time. I would not wait for more than 5 minutes to see a plot. How many genes were you planning to plot? The background is that when plotting a heatmap, the matplotlib visualization will randomly drop genes because the resolution of the screens is not high enough. Thus, when the number of genes is large, I was trying to find a compromise by fitting a line before the plotting and then only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you find an example that can reproduce the problem? > [â€¦](#). > On Tue, May 7, 2019 at 10:19 AM brianpenghe ***@***.***> wrote: I was trying to plot a heatmap using this command: ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0, dendrogram=True, save='ClusterMap.png') And it didn't finish running after an overnight, with the following warning message: WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set show_gene_labels=True /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227: UserWarning: The maximal number of iterations maxit (set to 20 by the program) allowed for finding a smoothing spline with fp=s has been reached: s too small. There is an approximation returned but the corresponding weighted sum of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message) I don't understand why this is taking this long because seaborn was able to finish plotting within 30 minutes. Do you know why? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#633>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ABF37VNDX37RZL256MWKDM3PUE3RFANCNFSM4HLGOYGA> . > -- Fidel Ramirez. I was planning to plot a heatmap of 300 ge",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:1983,energy efficiency,heat,heatmap,1983,"ting and then only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you find an example that can reproduce the problem? > [â€¦](#). > On Tue, May 7, 2019 at 10:19 AM brianpenghe ***@***.***> wrote: I was trying to plot a heatmap using this command: ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0, dendrogram=True, save='ClusterMap.png') And it didn't finish running after an overnight, with the following warning message: WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set show_gene_labels=True /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227: UserWarning: The maximal number of iterations maxit (set to 20 by the program) allowed for finding a smoothing spline with fp=s has been reached: s too small. There is an approximation returned but the corresponding weighted sum of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message) I don't understand why this is taking this long because seaborn was able to finish plotting within 30 minutes. Do you know why? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#633>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ABF37VNDX37RZL256MWKDM3PUE3RFANCNFSM4HLGOYGA> . > -- Fidel Ramirez. I was planning to plot a heatmap of 300 genes. However, I have 90k cells. I guess the time-consuming part is the PCA because that's what's required to do the clustering by groups. I thought a naive way to do the clustering is just to construct the ""pseudobulks"" for each group by calculating the average and then simply clustering the ""pseudobulks"", instead of trying to look at individual cells. Another advantage of checking the pseudobulk is that the size of each group won't affect the landscape of principle components in that way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:1041,integrability,messag,message,1041," that this step is taking too long as is was supposed to reduce the plotting time. I would not wait for more than 5 minutes to see a plot. How many genes were you planning to plot? The background is that when plotting a heatmap, the matplotlib visualization will randomly drop genes because the resolution of the screens is not high enough. Thus, when the number of genes is large, I was trying to find a compromise by fitting a line before the plotting and then only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you find an example that can reproduce the problem? > [â€¦](#). > On Tue, May 7, 2019 at 10:19 AM brianpenghe ***@***.***> wrote: I was trying to plot a heatmap using this command: ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0, dendrogram=True, save='ClusterMap.png') And it didn't finish running after an overnight, with the following warning message: WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set show_gene_labels=True /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227: UserWarning: The maximal number of iterations maxit (set to 20 by the program) allowed for finding a smoothing spline with fp=s has been reached: s too small. There is an approximation returned but the corresponding weighted sum of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message) I don't understand why this is taking this long because seaborn was able to finish plotting within 30 minutes. Do you know why? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#633>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ABF37VNDX37RZL256MWKDM3PUE3RFANCNFSM4HLGOYGA> . > -- Fidel Ramirez. I was planning to plot a heatmap of 300 genes. However, I have 90k cells. I guess the t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:1559,integrability,messag,message,1559,"ting and then only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you find an example that can reproduce the problem? > [â€¦](#). > On Tue, May 7, 2019 at 10:19 AM brianpenghe ***@***.***> wrote: I was trying to plot a heatmap using this command: ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0, dendrogram=True, save='ClusterMap.png') And it didn't finish running after an overnight, with the following warning message: WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set show_gene_labels=True /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227: UserWarning: The maximal number of iterations maxit (set to 20 by the program) allowed for finding a smoothing spline with fp=s has been reached: s too small. There is an approximation returned but the corresponding weighted sum of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message) I don't understand why this is taking this long because seaborn was able to finish plotting within 30 minutes. Do you know why? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#633>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ABF37VNDX37RZL256MWKDM3PUE3RFANCNFSM4HLGOYGA> . > -- Fidel Ramirez. I was planning to plot a heatmap of 300 genes. However, I have 90k cells. I guess the time-consuming part is the PCA because that's what's required to do the clustering by groups. I thought a naive way to do the clustering is just to construct the ""pseudobulks"" for each group by calculating the average and then simply clustering the ""pseudobulks"", instead of trying to look at individual cells. Another advantage of checking the pseudobulk is that the size of each group won't affect the landscape of principle components in that way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:1737,integrability,sub,subscribed,1737,"ting and then only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you find an example that can reproduce the problem? > [â€¦](#). > On Tue, May 7, 2019 at 10:19 AM brianpenghe ***@***.***> wrote: I was trying to plot a heatmap using this command: ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0, dendrogram=True, save='ClusterMap.png') And it didn't finish running after an overnight, with the following warning message: WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set show_gene_labels=True /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227: UserWarning: The maximal number of iterations maxit (set to 20 by the program) allowed for finding a smoothing spline with fp=s has been reached: s too small. There is an approximation returned but the corresponding weighted sum of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message) I don't understand why this is taking this long because seaborn was able to finish plotting within 30 minutes. Do you know why? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#633>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ABF37VNDX37RZL256MWKDM3PUE3RFANCNFSM4HLGOYGA> . > -- Fidel Ramirez. I was planning to plot a heatmap of 300 genes. However, I have 90k cells. I guess the time-consuming part is the PCA because that's what's required to do the clustering by groups. I thought a naive way to do the clustering is just to construct the ""pseudobulks"" for each group by calculating the average and then simply clustering the ""pseudobulks"", instead of trying to look at individual cells. Another advantage of checking the pseudobulk is that the size of each group won't affect the landscape of principle components in that way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:2471,integrability,compon,components,2471,"ting and then only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you find an example that can reproduce the problem? > [â€¦](#). > On Tue, May 7, 2019 at 10:19 AM brianpenghe ***@***.***> wrote: I was trying to plot a heatmap using this command: ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0, dendrogram=True, save='ClusterMap.png') And it didn't finish running after an overnight, with the following warning message: WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set show_gene_labels=True /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227: UserWarning: The maximal number of iterations maxit (set to 20 by the program) allowed for finding a smoothing spline with fp=s has been reached: s too small. There is an approximation returned but the corresponding weighted sum of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message) I don't understand why this is taking this long because seaborn was able to finish plotting within 30 minutes. Do you know why? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#633>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ABF37VNDX37RZL256MWKDM3PUE3RFANCNFSM4HLGOYGA> . > -- Fidel Ramirez. I was planning to plot a heatmap of 300 genes. However, I have 90k cells. I guess the time-consuming part is the PCA because that's what's required to do the clustering by groups. I thought a naive way to do the clustering is just to construct the ""pseudobulks"" for each group by calculating the average and then simply clustering the ""pseudobulks"", instead of trying to look at individual cells. Another advantage of checking the pseudobulk is that the size of each group won't affect the landscape of principle components in that way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:1041,interoperability,messag,message,1041," that this step is taking too long as is was supposed to reduce the plotting time. I would not wait for more than 5 minutes to see a plot. How many genes were you planning to plot? The background is that when plotting a heatmap, the matplotlib visualization will randomly drop genes because the resolution of the screens is not high enough. Thus, when the number of genes is large, I was trying to find a compromise by fitting a line before the plotting and then only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you find an example that can reproduce the problem? > [â€¦](#). > On Tue, May 7, 2019 at 10:19 AM brianpenghe ***@***.***> wrote: I was trying to plot a heatmap using this command: ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0, dendrogram=True, save='ClusterMap.png') And it didn't finish running after an overnight, with the following warning message: WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set show_gene_labels=True /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227: UserWarning: The maximal number of iterations maxit (set to 20 by the program) allowed for finding a smoothing spline with fp=s has been reached: s too small. There is an approximation returned but the corresponding weighted sum of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message) I don't understand why this is taking this long because seaborn was able to finish plotting within 30 minutes. Do you know why? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#633>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ABF37VNDX37RZL256MWKDM3PUE3RFANCNFSM4HLGOYGA> . > -- Fidel Ramirez. I was planning to plot a heatmap of 300 genes. However, I have 90k cells. I guess the t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:1559,interoperability,messag,message,1559,"ting and then only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you find an example that can reproduce the problem? > [â€¦](#). > On Tue, May 7, 2019 at 10:19 AM brianpenghe ***@***.***> wrote: I was trying to plot a heatmap using this command: ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0, dendrogram=True, save='ClusterMap.png') And it didn't finish running after an overnight, with the following warning message: WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set show_gene_labels=True /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227: UserWarning: The maximal number of iterations maxit (set to 20 by the program) allowed for finding a smoothing spline with fp=s has been reached: s too small. There is an approximation returned but the corresponding weighted sum of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message) I don't understand why this is taking this long because seaborn was able to finish plotting within 30 minutes. Do you know why? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#633>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ABF37VNDX37RZL256MWKDM3PUE3RFANCNFSM4HLGOYGA> . > -- Fidel Ramirez. I was planning to plot a heatmap of 300 genes. However, I have 90k cells. I guess the time-consuming part is the PCA because that's what's required to do the clustering by groups. I thought a naive way to do the clustering is just to construct the ""pseudobulks"" for each group by calculating the average and then simply clustering the ""pseudobulks"", instead of trying to look at individual cells. Another advantage of checking the pseudobulk is that the size of each group won't affect the landscape of principle components in that way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:2471,interoperability,compon,components,2471,"ting and then only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you find an example that can reproduce the problem? > [â€¦](#). > On Tue, May 7, 2019 at 10:19 AM brianpenghe ***@***.***> wrote: I was trying to plot a heatmap using this command: ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0, dendrogram=True, save='ClusterMap.png') And it didn't finish running after an overnight, with the following warning message: WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set show_gene_labels=True /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227: UserWarning: The maximal number of iterations maxit (set to 20 by the program) allowed for finding a smoothing spline with fp=s has been reached: s too small. There is an approximation returned but the corresponding weighted sum of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message) I don't understand why this is taking this long because seaborn was able to finish plotting within 30 minutes. Do you know why? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#633>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ABF37VNDX37RZL256MWKDM3PUE3RFANCNFSM4HLGOYGA> . > -- Fidel Ramirez. I was planning to plot a heatmap of 300 genes. However, I have 90k cells. I guess the time-consuming part is the PCA because that's what's required to do the clustering by groups. I thought a naive way to do the clustering is just to construct the ""pseudobulks"" for each group by calculating the average and then simply clustering the ""pseudobulks"", instead of trying to look at individual cells. Another advantage of checking the pseudobulk is that the size of each group won't affect the landscape of principle components in that way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:1201,modifiability,pac,packages,1201,"ou planning to plot? The background is that when plotting a heatmap, the matplotlib visualization will randomly drop genes because the resolution of the screens is not high enough. Thus, when the number of genes is large, I was trying to find a compromise by fitting a line before the plotting and then only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you find an example that can reproduce the problem? > [â€¦](#). > On Tue, May 7, 2019 at 10:19 AM brianpenghe ***@***.***> wrote: I was trying to plot a heatmap using this command: ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0, dendrogram=True, save='ClusterMap.png') And it didn't finish running after an overnight, with the following warning message: WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set show_gene_labels=True /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227: UserWarning: The maximal number of iterations maxit (set to 20 by the program) allowed for finding a smoothing spline with fp=s has been reached: s too small. There is an approximation returned but the corresponding weighted sum of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message) I don't understand why this is taking this long because seaborn was able to finish plotting within 30 minutes. Do you know why? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#633>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ABF37VNDX37RZL256MWKDM3PUE3RFANCNFSM4HLGOYGA> . > -- Fidel Ramirez. I was planning to plot a heatmap of 300 genes. However, I have 90k cells. I guess the time-consuming part is the PCA because that's what's required to do the clustering by groups. I thought a naive way to do the clustering is just to construct the",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:2471,modifiability,compon,components,2471,"ting and then only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you find an example that can reproduce the problem? > [â€¦](#). > On Tue, May 7, 2019 at 10:19 AM brianpenghe ***@***.***> wrote: I was trying to plot a heatmap using this command: ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0, dendrogram=True, save='ClusterMap.png') And it didn't finish running after an overnight, with the following warning message: WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set show_gene_labels=True /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227: UserWarning: The maximal number of iterations maxit (set to 20 by the program) allowed for finding a smoothing spline with fp=s has been reached: s too small. There is an approximation returned but the corresponding weighted sum of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message) I don't understand why this is taking this long because seaborn was able to finish plotting within 30 minutes. Do you know why? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#633>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ABF37VNDX37RZL256MWKDM3PUE3RFANCNFSM4HLGOYGA> . > -- Fidel Ramirez. I was planning to plot a heatmap of 300 genes. However, I have 90k cells. I guess the time-consuming part is the PCA because that's what's required to do the clustering by groups. I thought a naive way to do the clustering is just to construct the ""pseudobulks"" for each group by calculating the average and then simply clustering the ""pseudobulks"", instead of trying to look at individual cells. Another advantage of checking the pseudobulk is that the size of each group won't affect the landscape of principle components in that way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:122,performance,time,time,122,"> Thanks for your information. I am surprised that this step is taking too long as is was supposed to reduce the plotting time. I would not wait for more than 5 minutes to see a plot. How many genes were you planning to plot? The background is that when plotting a heatmap, the matplotlib visualization will randomly drop genes because the resolution of the screens is not high enough. Thus, when the number of genes is large, I was trying to find a compromise by fitting a line before the plotting and then only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you find an example that can reproduce the problem? > [â€¦](#). > On Tue, May 7, 2019 at 10:19 AM brianpenghe ***@***.***> wrote: I was trying to plot a heatmap using this command: ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0, dendrogram=True, save='ClusterMap.png') And it didn't finish running after an overnight, with the following warning message: WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set show_gene_labels=True /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227: UserWarning: The maximal number of iterations maxit (set to 20 by the program) allowed for finding a smoothing spline with fp=s has been reached: s too small. There is an approximation returned but the corresponding weighted sum of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message) I don't understand why this is taking this long because seaborn was able to finish plotting within 30 minutes. Do you know why? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#633>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ABF37VNDX37RZL256MWKDM3PUE3RFANCNFSM4HLGOYGA> . > -- Fidel Ramirez. I was planning to plot a heatmap of 300 ge",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:2044,performance,time,time-consuming,2044,"ting and then only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you find an example that can reproduce the problem? > [â€¦](#). > On Tue, May 7, 2019 at 10:19 AM brianpenghe ***@***.***> wrote: I was trying to plot a heatmap using this command: ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0, dendrogram=True, save='ClusterMap.png') And it didn't finish running after an overnight, with the following warning message: WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set show_gene_labels=True /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227: UserWarning: The maximal number of iterations maxit (set to 20 by the program) allowed for finding a smoothing spline with fp=s has been reached: s too small. There is an approximation returned but the corresponding weighted sum of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message) I don't understand why this is taking this long because seaborn was able to finish plotting within 30 minutes. Do you know why? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#633>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ABF37VNDX37RZL256MWKDM3PUE3RFANCNFSM4HLGOYGA> . > -- Fidel Ramirez. I was planning to plot a heatmap of 300 genes. However, I have 90k cells. I guess the time-consuming part is the PCA because that's what's required to do the clustering by groups. I thought a naive way to do the clustering is just to construct the ""pseudobulks"" for each group by calculating the average and then simply clustering the ""pseudobulks"", instead of trying to look at individual cells. Another advantage of checking the pseudobulk is that the size of each group won't affect the landscape of principle components in that way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:1495,reliability,doe,does,1495,"ting and then only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you find an example that can reproduce the problem? > [â€¦](#). > On Tue, May 7, 2019 at 10:19 AM brianpenghe ***@***.***> wrote: I was trying to plot a heatmap using this command: ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0, dendrogram=True, save='ClusterMap.png') And it didn't finish running after an overnight, with the following warning message: WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set show_gene_labels=True /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227: UserWarning: The maximal number of iterations maxit (set to 20 by the program) allowed for finding a smoothing spline with fp=s has been reached: s too small. There is an approximation returned but the corresponding weighted sum of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message) I don't understand why this is taking this long because seaborn was able to finish plotting within 30 minutes. Do you know why? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#633>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ABF37VNDX37RZL256MWKDM3PUE3RFANCNFSM4HLGOYGA> . > -- Fidel Ramirez. I was planning to plot a heatmap of 300 genes. However, I have 90k cells. I guess the time-consuming part is the PCA because that's what's required to do the clustering by groups. I thought a naive way to do the clustering is just to construct the ""pseudobulks"" for each group by calculating the average and then simply clustering the ""pseudobulks"", instead of trying to look at individual cells. Another advantage of checking the pseudobulk is that the size of each group won't affect the landscape of principle components in that way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:841,security,ident,ident,841,"> Thanks for your information. I am surprised that this step is taking too long as is was supposed to reduce the plotting time. I would not wait for more than 5 minutes to see a plot. How many genes were you planning to plot? The background is that when plotting a heatmap, the matplotlib visualization will randomly drop genes because the resolution of the screens is not high enough. Thus, when the number of genes is large, I was trying to find a compromise by fitting a line before the plotting and then only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you find an example that can reproduce the problem? > [â€¦](#). > On Tue, May 7, 2019 at 10:19 AM brianpenghe ***@***.***> wrote: I was trying to plot a heatmap using this command: ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0, dendrogram=True, save='ClusterMap.png') And it didn't finish running after an overnight, with the following warning message: WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set show_gene_labels=True /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227: UserWarning: The maximal number of iterations maxit (set to 20 by the program) allowed for finding a smoothing spline with fp=s has been reached: s too small. There is an approximation returned but the corresponding weighted sum of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message) I don't understand why this is taking this long because seaborn was able to finish plotting within 30 minutes. Do you know why? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#633>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ABF37VNDX37RZL256MWKDM3PUE3RFANCNFSM4HLGOYGA> . > -- Fidel Ramirez. I was planning to plot a heatmap of 300 ge",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:1885,security,auth,auth,1885,"ting and then only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you find an example that can reproduce the problem? > [â€¦](#). > On Tue, May 7, 2019 at 10:19 AM brianpenghe ***@***.***> wrote: I was trying to plot a heatmap using this command: ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0, dendrogram=True, save='ClusterMap.png') And it didn't finish running after an overnight, with the following warning message: WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set show_gene_labels=True /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227: UserWarning: The maximal number of iterations maxit (set to 20 by the program) allowed for finding a smoothing spline with fp=s has been reached: s too small. There is an approximation returned but the corresponding weighted sum of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message) I don't understand why this is taking this long because seaborn was able to finish plotting within 30 minutes. Do you know why? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#633>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ABF37VNDX37RZL256MWKDM3PUE3RFANCNFSM4HLGOYGA> . > -- Fidel Ramirez. I was planning to plot a heatmap of 300 genes. However, I have 90k cells. I guess the time-consuming part is the PCA because that's what's required to do the clustering by groups. I thought a naive way to do the clustering is just to construct the ""pseudobulks"" for each group by calculating the average and then simply clustering the ""pseudobulks"", instead of trying to look at individual cells. Another advantage of checking the pseudobulk is that the size of each group won't affect the landscape of principle components in that way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:208,testability,plan,planning,208,"> Thanks for your information. I am surprised that this step is taking too long as is was supposed to reduce the plotting time. I would not wait for more than 5 minutes to see a plot. How many genes were you planning to plot? The background is that when plotting a heatmap, the matplotlib visualization will randomly drop genes because the resolution of the screens is not high enough. Thus, when the number of genes is large, I was trying to find a compromise by fitting a line before the plotting and then only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you find an example that can reproduce the problem? > [â€¦](#). > On Tue, May 7, 2019 at 10:19 AM brianpenghe ***@***.***> wrote: I was trying to plot a heatmap using this command: ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0, dendrogram=True, save='ClusterMap.png') And it didn't finish running after an overnight, with the following warning message: WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set show_gene_labels=True /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227: UserWarning: The maximal number of iterations maxit (set to 20 by the program) allowed for finding a smoothing spline with fp=s has been reached: s too small. There is an approximation returned but the corresponding weighted sum of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message) I don't understand why this is taking this long because seaborn was able to finish plotting within 30 minutes. Do you know why? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#633>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ABF37VNDX37RZL256MWKDM3PUE3RFANCNFSM4HLGOYGA> . > -- Fidel Ramirez. I was planning to plot a heatmap of 300 ge",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:1576,testability,understand,understand,1576,"ting and then only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you find an example that can reproduce the problem? > [â€¦](#). > On Tue, May 7, 2019 at 10:19 AM brianpenghe ***@***.***> wrote: I was trying to plot a heatmap using this command: ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0, dendrogram=True, save='ClusterMap.png') And it didn't finish running after an overnight, with the following warning message: WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set show_gene_labels=True /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227: UserWarning: The maximal number of iterations maxit (set to 20 by the program) allowed for finding a smoothing spline with fp=s has been reached: s too small. There is an approximation returned but the corresponding weighted sum of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message) I don't understand why this is taking this long because seaborn was able to finish plotting within 30 minutes. Do you know why? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#633>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ABF37VNDX37RZL256MWKDM3PUE3RFANCNFSM4HLGOYGA> . > -- Fidel Ramirez. I was planning to plot a heatmap of 300 genes. However, I have 90k cells. I guess the time-consuming part is the PCA because that's what's required to do the clustering by groups. I thought a naive way to do the clustering is just to construct the ""pseudobulks"" for each group by calculating the average and then simply clustering the ""pseudobulks"", instead of trying to look at individual cells. Another advantage of checking the pseudobulk is that the size of each group won't affect the landscape of principle components in that way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:1964,testability,plan,planning,1964,"ting and then only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you find an example that can reproduce the problem? > [â€¦](#). > On Tue, May 7, 2019 at 10:19 AM brianpenghe ***@***.***> wrote: I was trying to plot a heatmap using this command: ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0, dendrogram=True, save='ClusterMap.png') And it didn't finish running after an overnight, with the following warning message: WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set show_gene_labels=True /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227: UserWarning: The maximal number of iterations maxit (set to 20 by the program) allowed for finding a smoothing spline with fp=s has been reached: s too small. There is an approximation returned but the corresponding weighted sum of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message) I don't understand why this is taking this long because seaborn was able to finish plotting within 30 minutes. Do you know why? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#633>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ABF37VNDX37RZL256MWKDM3PUE3RFANCNFSM4HLGOYGA> . > -- Fidel Ramirez. I was planning to plot a heatmap of 300 genes. However, I have 90k cells. I guess the time-consuming part is the PCA because that's what's required to do the clustering by groups. I thought a naive way to do the clustering is just to construct the ""pseudobulks"" for each group by calculating the average and then simply clustering the ""pseudobulks"", instead of trying to look at individual cells. Another advantage of checking the pseudobulk is that the size of each group won't affect the landscape of principle components in that way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:2271,testability,simpl,simply,2271,"ting and then only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you find an example that can reproduce the problem? > [â€¦](#). > On Tue, May 7, 2019 at 10:19 AM brianpenghe ***@***.***> wrote: I was trying to plot a heatmap using this command: ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0, dendrogram=True, save='ClusterMap.png') And it didn't finish running after an overnight, with the following warning message: WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set show_gene_labels=True /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227: UserWarning: The maximal number of iterations maxit (set to 20 by the program) allowed for finding a smoothing spline with fp=s has been reached: s too small. There is an approximation returned but the corresponding weighted sum of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message) I don't understand why this is taking this long because seaborn was able to finish plotting within 30 minutes. Do you know why? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#633>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ABF37VNDX37RZL256MWKDM3PUE3RFANCNFSM4HLGOYGA> . > -- Fidel Ramirez. I was planning to plot a heatmap of 300 genes. However, I have 90k cells. I guess the time-consuming part is the PCA because that's what's required to do the clustering by groups. I thought a naive way to do the clustering is just to construct the ""pseudobulks"" for each group by calculating the average and then simply clustering the ""pseudobulks"", instead of trying to look at individual cells. Another advantage of checking the pseudobulk is that the size of each group won't affect the landscape of principle components in that way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:289,usability,visual,visualization,289,"> Thanks for your information. I am surprised that this step is taking too long as is was supposed to reduce the plotting time. I would not wait for more than 5 minutes to see a plot. How many genes were you planning to plot? The background is that when plotting a heatmap, the matplotlib visualization will randomly drop genes because the resolution of the screens is not high enough. Thus, when the number of genes is large, I was trying to find a compromise by fitting a line before the plotting and then only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you find an example that can reproduce the problem? > [â€¦](#). > On Tue, May 7, 2019 at 10:19 AM brianpenghe ***@***.***> wrote: I was trying to plot a heatmap using this command: ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0, dendrogram=True, save='ClusterMap.png') And it didn't finish running after an overnight, with the following warning message: WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set show_gene_labels=True /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227: UserWarning: The maximal number of iterations maxit (set to 20 by the program) allowed for finding a smoothing spline with fp=s has been reached: s too small. There is an approximation returned but the corresponding weighted sum of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message) I don't understand why this is taking this long because seaborn was able to finish plotting within 30 minutes. Do you know why? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#633>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ABF37VNDX37RZL256MWKDM3PUE3RFANCNFSM4HLGOYGA> . > -- Fidel Ramirez. I was planning to plot a heatmap of 300 ge",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:539,usability,help,help,539,"> Thanks for your information. I am surprised that this step is taking too long as is was supposed to reduce the plotting time. I would not wait for more than 5 minutes to see a plot. How many genes were you planning to plot? The background is that when plotting a heatmap, the matplotlib visualization will randomly drop genes because the resolution of the screens is not high enough. Thus, when the number of genes is large, I was trying to find a compromise by fitting a line before the plotting and then only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you find an example that can reproduce the problem? > [â€¦](#). > On Tue, May 7, 2019 at 10:19 AM brianpenghe ***@***.***> wrote: I was trying to plot a heatmap using this command: ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0, dendrogram=True, save='ClusterMap.png') And it didn't finish running after an overnight, with the following warning message: WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set show_gene_labels=True /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227: UserWarning: The maximal number of iterations maxit (set to 20 by the program) allowed for finding a smoothing spline with fp=s has been reached: s too small. There is an approximation returned but the corresponding weighted sum of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message) I don't understand why this is taking this long because seaborn was able to finish plotting within 30 minutes. Do you know why? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#633>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ABF37VNDX37RZL256MWKDM3PUE3RFANCNFSM4HLGOYGA> . > -- Fidel Ramirez. I was planning to plot a heatmap of 300 ge",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:770,usability,command,command,770,"> Thanks for your information. I am surprised that this step is taking too long as is was supposed to reduce the plotting time. I would not wait for more than 5 minutes to see a plot. How many genes were you planning to plot? The background is that when plotting a heatmap, the matplotlib visualization will randomly drop genes because the resolution of the screens is not high enough. Thus, when the number of genes is large, I was trying to find a compromise by fitting a line before the plotting and then only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you find an example that can reproduce the problem? > [â€¦](#). > On Tue, May 7, 2019 at 10:19 AM brianpenghe ***@***.***> wrote: I was trying to plot a heatmap using this command: ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0, dendrogram=True, save='ClusterMap.png') And it didn't finish running after an overnight, with the following warning message: WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set show_gene_labels=True /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227: UserWarning: The maximal number of iterations maxit (set to 20 by the program) allowed for finding a smoothing spline with fp=s has been reached: s too small. There is an approximation returned but the corresponding weighted sum of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message) I don't understand why this is taking this long because seaborn was able to finish plotting within 30 minutes. Do you know why? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#633>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ABF37VNDX37RZL256MWKDM3PUE3RFANCNFSM4HLGOYGA> . > -- Fidel Ramirez. I was planning to plot a heatmap of 300 ge",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:1113,usability,visual,visualized,1113,"ing time. I would not wait for more than 5 minutes to see a plot. How many genes were you planning to plot? The background is that when plotting a heatmap, the matplotlib visualization will randomly drop genes because the resolution of the screens is not high enough. Thus, when the number of genes is large, I was trying to find a compromise by fitting a line before the plotting and then only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you find an example that can reproduce the problem? > [â€¦](#). > On Tue, May 7, 2019 at 10:19 AM brianpenghe ***@***.***> wrote: I was trying to plot a heatmap using this command: ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0, dendrogram=True, save='ClusterMap.png') And it didn't finish running after an overnight, with the following warning message: WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set show_gene_labels=True /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227: UserWarning: The maximal number of iterations maxit (set to 20 by the program) allowed for finding a smoothing spline with fp=s has been reached: s too small. There is an approximation returned but the corresponding weighted sum of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message) I don't understand why this is taking this long because seaborn was able to finish plotting within 30 minutes. Do you know why? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#633>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ABF37VNDX37RZL256MWKDM3PUE3RFANCNFSM4HLGOYGA> . > -- Fidel Ramirez. I was planning to plot a heatmap of 300 genes. However, I have 90k cells. I guess the time-consuming part is the PCA because that's what's required to do the cl",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:1245,usability,User,UserWarning,1245,"en plotting a heatmap, the matplotlib visualization will randomly drop genes because the resolution of the screens is not high enough. Thus, when the number of genes is large, I was trying to find a compromise by fitting a line before the plotting and then only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you find an example that can reproduce the problem? > [â€¦](#). > On Tue, May 7, 2019 at 10:19 AM brianpenghe ***@***.***> wrote: I was trying to plot a heatmap using this command: ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0, dendrogram=True, save='ClusterMap.png') And it didn't finish running after an overnight, with the following warning message: WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set show_gene_labels=True /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227: UserWarning: The maximal number of iterations maxit (set to 20 by the program) allowed for finding a smoothing spline with fp=s has been reached: s too small. There is an approximation returned but the corresponding weighted sum of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message) I don't understand why this is taking this long because seaborn was able to finish plotting within 30 minutes. Do you know why? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#633>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ABF37VNDX37RZL256MWKDM3PUE3RFANCNFSM4HLGOYGA> . > -- Fidel Ramirez. I was planning to plot a heatmap of 300 genes. However, I have 90k cells. I guess the time-consuming part is the PCA because that's what's required to do the clustering by groups. I thought a naive way to do the clustering is just to construct the ""pseudobulks"" for each group by calculating t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:2271,usability,simpl,simply,2271,"ting and then only plotting the fit. Can you help me solve the issue by sharing the data with me? Or, can you find an example that can reproduce the problem? > [â€¦](#). > On Tue, May 7, 2019 at 10:19 AM brianpenghe ***@***.***> wrote: I was trying to plot a heatmap using this command: ax=sc2.pl.heatmap(adata, sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0, dendrogram=True, save='ClusterMap.png') And it didn't finish running after an overnight, with the following warning message: WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set show_gene_labels=True /usr/local/lib/python3.6/dist-packages/scipy/interpolate/fitpack2.py:227: UserWarning: The maximal number of iterations maxit (set to 20 by the program) allowed for finding a smoothing spline with fp=s has been reached: s too small. There is an approximation returned but the corresponding weighted sum of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message) I don't understand why this is taking this long because seaborn was able to finish plotting within 30 minutes. Do you know why? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#633>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ABF37VNDX37RZL256MWKDM3PUE3RFANCNFSM4HLGOYGA> . > -- Fidel Ramirez. I was planning to plot a heatmap of 300 genes. However, I have 90k cells. I guess the time-consuming part is the PCA because that's what's required to do the clustering by groups. I thought a naive way to do the clustering is just to construct the ""pseudobulks"" for each group by calculating the average and then simply clustering the ""pseudobulks"", instead of trying to look at individual cells. Another advantage of checking the pseudobulk is that the size of each group won't affect the landscape of principle components in that way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:64,energy efficiency,heat,heatmap,64,"I have the same issue than @brianpenghe . I am trying to plot a heatmap using:. `sc.pl.heatmap(adata_10x_day13_MPCs_early, output_great_1090, groupby='condition', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0, dendrogram=True)`. n_cells = 1071. n_genes = 1121. I got this message as well:. WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set `show_gene_labels=True`. /home/grupos_srs_ap/Programs/anaconda3/lib/python3.7/site-packages/scipy/interpolate/fitpack2.py:227: UserWarning: . The maximal number of iterations maxit (set to 20 by the program). allowed for finding a smoothing spline with fp=s has been reached: s. too small. There is an approximation returned but the corresponding weighted sum. of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message). The heatmap was plotted after few minutes, though:. ![image](https://user-images.githubusercontent.com/63011975/78168487-675dc800-7426-11ea-9374-a6a4aab24506.png). I think the plot is not correct. I tried modifying the scanpy/plotting/_anndata.py, removing the smoothing function as described above, but got the same results. . Can you help me? Thanks a lot in advance",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:87,energy efficiency,heat,heatmap,87,"I have the same issue than @brianpenghe . I am trying to plot a heatmap using:. `sc.pl.heatmap(adata_10x_day13_MPCs_early, output_great_1090, groupby='condition', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0, dendrogram=True)`. n_cells = 1071. n_genes = 1121. I got this message as well:. WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set `show_gene_labels=True`. /home/grupos_srs_ap/Programs/anaconda3/lib/python3.7/site-packages/scipy/interpolate/fitpack2.py:227: UserWarning: . The maximal number of iterations maxit (set to 20 by the program). allowed for finding a smoothing spline with fp=s has been reached: s. too small. There is an approximation returned but the corresponding weighted sum. of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message). The heatmap was plotted after few minutes, though:. ![image](https://user-images.githubusercontent.com/63011975/78168487-675dc800-7426-11ea-9374-a6a4aab24506.png). I think the plot is not correct. I tried modifying the scanpy/plotting/_anndata.py, removing the smoothing function as described above, but got the same results. . Can you help me? Thanks a lot in advance",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:878,energy efficiency,heat,heatmap,878,"I have the same issue than @brianpenghe . I am trying to plot a heatmap using:. `sc.pl.heatmap(adata_10x_day13_MPCs_early, output_great_1090, groupby='condition', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0, dendrogram=True)`. n_cells = 1071. n_genes = 1121. I got this message as well:. WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set `show_gene_labels=True`. /home/grupos_srs_ap/Programs/anaconda3/lib/python3.7/site-packages/scipy/interpolate/fitpack2.py:227: UserWarning: . The maximal number of iterations maxit (set to 20 by the program). allowed for finding a smoothing spline with fp=s has been reached: s. too small. There is an approximation returned but the corresponding weighted sum. of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message). The heatmap was plotted after few minutes, though:. ![image](https://user-images.githubusercontent.com/63011975/78168487-675dc800-7426-11ea-9374-a6a4aab24506.png). I think the plot is not correct. I tried modifying the scanpy/plotting/_anndata.py, removing the smoothing function as described above, but got the same results. . Can you help me? Thanks a lot in advance",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:301,integrability,messag,message,301,"I have the same issue than @brianpenghe . I am trying to plot a heatmap using:. `sc.pl.heatmap(adata_10x_day13_MPCs_early, output_great_1090, groupby='condition', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0, dendrogram=True)`. n_cells = 1071. n_genes = 1121. I got this message as well:. WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set `show_gene_labels=True`. /home/grupos_srs_ap/Programs/anaconda3/lib/python3.7/site-packages/scipy/interpolate/fitpack2.py:227: UserWarning: . The maximal number of iterations maxit (set to 20 by the program). allowed for finding a smoothing spline with fp=s has been reached: s. too small. There is an approximation returned but the corresponding weighted sum. of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message). The heatmap was plotted after few minutes, though:. ![image](https://user-images.githubusercontent.com/63011975/78168487-675dc800-7426-11ea-9374-a6a4aab24506.png). I think the plot is not correct. I tried modifying the scanpy/plotting/_anndata.py, removing the smoothing function as described above, but got the same results. . Can you help me? Thanks a lot in advance",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:864,integrability,messag,message,864,"I have the same issue than @brianpenghe . I am trying to plot a heatmap using:. `sc.pl.heatmap(adata_10x_day13_MPCs_early, output_great_1090, groupby='condition', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0, dendrogram=True)`. n_cells = 1071. n_genes = 1121. I got this message as well:. WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set `show_gene_labels=True`. /home/grupos_srs_ap/Programs/anaconda3/lib/python3.7/site-packages/scipy/interpolate/fitpack2.py:227: UserWarning: . The maximal number of iterations maxit (set to 20 by the program). allowed for finding a smoothing spline with fp=s has been reached: s. too small. There is an approximation returned but the corresponding weighted sum. of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message). The heatmap was plotted after few minutes, though:. ![image](https://user-images.githubusercontent.com/63011975/78168487-675dc800-7426-11ea-9374-a6a4aab24506.png). I think the plot is not correct. I tried modifying the scanpy/plotting/_anndata.py, removing the smoothing function as described above, but got the same results. . Can you help me? Thanks a lot in advance",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:301,interoperability,messag,message,301,"I have the same issue than @brianpenghe . I am trying to plot a heatmap using:. `sc.pl.heatmap(adata_10x_day13_MPCs_early, output_great_1090, groupby='condition', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0, dendrogram=True)`. n_cells = 1071. n_genes = 1121. I got this message as well:. WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set `show_gene_labels=True`. /home/grupos_srs_ap/Programs/anaconda3/lib/python3.7/site-packages/scipy/interpolate/fitpack2.py:227: UserWarning: . The maximal number of iterations maxit (set to 20 by the program). allowed for finding a smoothing spline with fp=s has been reached: s. too small. There is an approximation returned but the corresponding weighted sum. of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message). The heatmap was plotted after few minutes, though:. ![image](https://user-images.githubusercontent.com/63011975/78168487-675dc800-7426-11ea-9374-a6a4aab24506.png). I think the plot is not correct. I tried modifying the scanpy/plotting/_anndata.py, removing the smoothing function as described above, but got the same results. . Can you help me? Thanks a lot in advance",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:864,interoperability,messag,message,864,"I have the same issue than @brianpenghe . I am trying to plot a heatmap using:. `sc.pl.heatmap(adata_10x_day13_MPCs_early, output_great_1090, groupby='condition', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0, dendrogram=True)`. n_cells = 1071. n_genes = 1121. I got this message as well:. WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set `show_gene_labels=True`. /home/grupos_srs_ap/Programs/anaconda3/lib/python3.7/site-packages/scipy/interpolate/fitpack2.py:227: UserWarning: . The maximal number of iterations maxit (set to 20 by the program). allowed for finding a smoothing spline with fp=s has been reached: s. too small. There is an approximation returned but the corresponding weighted sum. of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message). The heatmap was plotted after few minutes, though:. ![image](https://user-images.githubusercontent.com/63011975/78168487-675dc800-7426-11ea-9374-a6a4aab24506.png). I think the plot is not correct. I tried modifying the scanpy/plotting/_anndata.py, removing the smoothing function as described above, but got the same results. . Can you help me? Thanks a lot in advance",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:501,modifiability,pac,packages,501,"I have the same issue than @brianpenghe . I am trying to plot a heatmap using:. `sc.pl.heatmap(adata_10x_day13_MPCs_early, output_great_1090, groupby='condition', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0, dendrogram=True)`. n_cells = 1071. n_genes = 1121. I got this message as well:. WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set `show_gene_labels=True`. /home/grupos_srs_ap/Programs/anaconda3/lib/python3.7/site-packages/scipy/interpolate/fitpack2.py:227: UserWarning: . The maximal number of iterations maxit (set to 20 by the program). allowed for finding a smoothing spline with fp=s has been reached: s. too small. There is an approximation returned but the corresponding weighted sum. of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message). The heatmap was plotted after few minutes, though:. ![image](https://user-images.githubusercontent.com/63011975/78168487-675dc800-7426-11ea-9374-a6a4aab24506.png). I think the plot is not correct. I tried modifying the scanpy/plotting/_anndata.py, removing the smoothing function as described above, but got the same results. . Can you help me? Thanks a lot in advance",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:800,reliability,doe,does,800,"I have the same issue than @brianpenghe . I am trying to plot a heatmap using:. `sc.pl.heatmap(adata_10x_day13_MPCs_early, output_great_1090, groupby='condition', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0, dendrogram=True)`. n_cells = 1071. n_genes = 1121. I got this message as well:. WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set `show_gene_labels=True`. /home/grupos_srs_ap/Programs/anaconda3/lib/python3.7/site-packages/scipy/interpolate/fitpack2.py:227: UserWarning: . The maximal number of iterations maxit (set to 20 by the program). allowed for finding a smoothing spline with fp=s has been reached: s. too small. There is an approximation returned but the corresponding weighted sum. of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message). The heatmap was plotted after few minutes, though:. ![image](https://user-images.githubusercontent.com/63011975/78168487-675dc800-7426-11ea-9374-a6a4aab24506.png). I think the plot is not correct. I tried modifying the scanpy/plotting/_anndata.py, removing the smoothing function as described above, but got the same results. . Can you help me? Thanks a lot in advance",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:1079,security,modif,modifying,1079,"I have the same issue than @brianpenghe . I am trying to plot a heatmap using:. `sc.pl.heatmap(adata_10x_day13_MPCs_early, output_great_1090, groupby='condition', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0, dendrogram=True)`. n_cells = 1071. n_genes = 1121. I got this message as well:. WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set `show_gene_labels=True`. /home/grupos_srs_ap/Programs/anaconda3/lib/python3.7/site-packages/scipy/interpolate/fitpack2.py:227: UserWarning: . The maximal number of iterations maxit (set to 20 by the program). allowed for finding a smoothing spline with fp=s has been reached: s. too small. There is an approximation returned but the corresponding weighted sum. of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message). The heatmap was plotted after few minutes, though:. ![image](https://user-images.githubusercontent.com/63011975/78168487-675dc800-7426-11ea-9374-a6a4aab24506.png). I think the plot is not correct. I tried modifying the scanpy/plotting/_anndata.py, removing the smoothing function as described above, but got the same results. . Can you help me? Thanks a lot in advance",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:382,usability,visual,visualized,382,"I have the same issue than @brianpenghe . I am trying to plot a heatmap using:. `sc.pl.heatmap(adata_10x_day13_MPCs_early, output_great_1090, groupby='condition', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0, dendrogram=True)`. n_cells = 1071. n_genes = 1121. I got this message as well:. WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set `show_gene_labels=True`. /home/grupos_srs_ap/Programs/anaconda3/lib/python3.7/site-packages/scipy/interpolate/fitpack2.py:227: UserWarning: . The maximal number of iterations maxit (set to 20 by the program). allowed for finding a smoothing spline with fp=s has been reached: s. too small. There is an approximation returned but the corresponding weighted sum. of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message). The heatmap was plotted after few minutes, though:. ![image](https://user-images.githubusercontent.com/63011975/78168487-675dc800-7426-11ea-9374-a6a4aab24506.png). I think the plot is not correct. I tried modifying the scanpy/plotting/_anndata.py, removing the smoothing function as described above, but got the same results. . Can you help me? Thanks a lot in advance",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:545,usability,User,UserWarning,545,"I have the same issue than @brianpenghe . I am trying to plot a heatmap using:. `sc.pl.heatmap(adata_10x_day13_MPCs_early, output_great_1090, groupby='condition', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0, dendrogram=True)`. n_cells = 1071. n_genes = 1121. I got this message as well:. WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set `show_gene_labels=True`. /home/grupos_srs_ap/Programs/anaconda3/lib/python3.7/site-packages/scipy/interpolate/fitpack2.py:227: UserWarning: . The maximal number of iterations maxit (set to 20 by the program). allowed for finding a smoothing spline with fp=s has been reached: s. too small. There is an approximation returned but the corresponding weighted sum. of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message). The heatmap was plotted after few minutes, though:. ![image](https://user-images.githubusercontent.com/63011975/78168487-675dc800-7426-11ea-9374-a6a4aab24506.png). I think the plot is not correct. I tried modifying the scanpy/plotting/_anndata.py, removing the smoothing function as described above, but got the same results. . Can you help me? Thanks a lot in advance",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:943,usability,user,user-images,943,"I have the same issue than @brianpenghe . I am trying to plot a heatmap using:. `sc.pl.heatmap(adata_10x_day13_MPCs_early, output_great_1090, groupby='condition', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0, dendrogram=True)`. n_cells = 1071. n_genes = 1121. I got this message as well:. WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set `show_gene_labels=True`. /home/grupos_srs_ap/Programs/anaconda3/lib/python3.7/site-packages/scipy/interpolate/fitpack2.py:227: UserWarning: . The maximal number of iterations maxit (set to 20 by the program). allowed for finding a smoothing spline with fp=s has been reached: s. too small. There is an approximation returned but the corresponding weighted sum. of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message). The heatmap was plotted after few minutes, though:. ![image](https://user-images.githubusercontent.com/63011975/78168487-675dc800-7426-11ea-9374-a6a4aab24506.png). I think the plot is not correct. I tried modifying the scanpy/plotting/_anndata.py, removing the smoothing function as described above, but got the same results. . Can you help me? Thanks a lot in advance",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/633:1210,usability,help,help,1210,"I have the same issue than @brianpenghe . I am trying to plot a heatmap using:. `sc.pl.heatmap(adata_10x_day13_MPCs_early, output_great_1090, groupby='condition', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True, var_group_rotation=0, dendrogram=True)`. n_cells = 1071. n_genes = 1121. I got this message as well:. WARNING: Gene labels are not shown when more than 50 genes are visualized. To show gene labels set `show_gene_labels=True`. /home/grupos_srs_ap/Programs/anaconda3/lib/python3.7/site-packages/scipy/interpolate/fitpack2.py:227: UserWarning: . The maximal number of iterations maxit (set to 20 by the program). allowed for finding a smoothing spline with fp=s has been reached: s. too small. There is an approximation returned but the corresponding weighted sum. of squared residuals does not satisfy the condition abs(fp-s)/s < tol. warnings.warn(message). The heatmap was plotted after few minutes, though:. ![image](https://user-images.githubusercontent.com/63011975/78168487-675dc800-7426-11ea-9374-a6a4aab24506.png). I think the plot is not correct. I tried modifying the scanpy/plotting/_anndata.py, removing the smoothing function as described above, but got the same results. . Can you help me? Thanks a lot in advance",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/633
https://github.com/scverse/scanpy/issues/634:71,reliability,doe,does,71,"Sorry, are you referring to https://github.com/theislab/scvelo? Scanpy does not have any builtin function regarding RNA velocity.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/634
https://github.com/scverse/scanpy/issues/634:22,interoperability,specif,specific,22,Closing as this seems specific to velocyto,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/634
https://github.com/scverse/scanpy/issues/635:0,usability,Close,Closed,0,Closed by #636,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/635
https://github.com/scverse/scanpy/issues/637:517,availability,Cluster,ClusterMap,517,"There was a similar report not so long ago but I have not been able to reproduce the issue. . Seems to be related with the matplotlib version used. Maybe you can try to update to the latest version. . . > On 10 May 2019, at 01:38, brianpenghe <notifications@github.com> wrote:. > . > I ran this:. > ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7), var_group_rotation=0, dendrogram=True, save='ClusterMap.png'). > . > But the image has something weird. Here are the snapshot:. > . > The lines don't align well with the heatmap. > . > Additionally, the lines don't align well with the group ID colors, either. > . > . > And the ID colors seem to be not aligned well with the heatmap either. > . > Any thoughts? > . > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:134,deployability,version,version,134,"There was a similar report not so long ago but I have not been able to reproduce the issue. . Seems to be related with the matplotlib version used. Maybe you can try to update to the latest version. . . > On 10 May 2019, at 01:38, brianpenghe <notifications@github.com> wrote:. > . > I ran this:. > ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7), var_group_rotation=0, dendrogram=True, save='ClusterMap.png'). > . > But the image has something weird. Here are the snapshot:. > . > The lines don't align well with the heatmap. > . > Additionally, the lines don't align well with the group ID colors, either. > . > . > And the ID colors seem to be not aligned well with the heatmap either. > . > Any thoughts? > . > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:169,deployability,updat,update,169,"There was a similar report not so long ago but I have not been able to reproduce the issue. . Seems to be related with the matplotlib version used. Maybe you can try to update to the latest version. . . > On 10 May 2019, at 01:38, brianpenghe <notifications@github.com> wrote:. > . > I ran this:. > ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7), var_group_rotation=0, dendrogram=True, save='ClusterMap.png'). > . > But the image has something weird. Here are the snapshot:. > . > The lines don't align well with the heatmap. > . > Additionally, the lines don't align well with the group ID colors, either. > . > . > And the ID colors seem to be not aligned well with the heatmap either. > . > Any thoughts? > . > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:190,deployability,version,version,190,"There was a similar report not so long ago but I have not been able to reproduce the issue. . Seems to be related with the matplotlib version used. Maybe you can try to update to the latest version. . . > On 10 May 2019, at 01:38, brianpenghe <notifications@github.com> wrote:. > . > I ran this:. > ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7), var_group_rotation=0, dendrogram=True, save='ClusterMap.png'). > . > But the image has something weird. Here are the snapshot:. > . > The lines don't align well with the heatmap. > . > Additionally, the lines don't align well with the group ID colors, either. > . > . > And the ID colors seem to be not aligned well with the heatmap either. > . > Any thoughts? > . > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:517,deployability,Cluster,ClusterMap,517,"There was a similar report not so long ago but I have not been able to reproduce the issue. . Seems to be related with the matplotlib version used. Maybe you can try to update to the latest version. . . > On 10 May 2019, at 01:38, brianpenghe <notifications@github.com> wrote:. > . > I ran this:. > ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7), var_group_rotation=0, dendrogram=True, save='ClusterMap.png'). > . > But the image has something weird. Here are the snapshot:. > . > The lines don't align well with the heatmap. > . > Additionally, the lines don't align well with the group ID colors, either. > . > . > And the ID colors seem to be not aligned well with the heatmap either. > . > Any thoughts? > . > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:308,energy efficiency,heat,heatmap,308,"There was a similar report not so long ago but I have not been able to reproduce the issue. . Seems to be related with the matplotlib version used. Maybe you can try to update to the latest version. . . > On 10 May 2019, at 01:38, brianpenghe <notifications@github.com> wrote:. > . > I ran this:. > ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7), var_group_rotation=0, dendrogram=True, save='ClusterMap.png'). > . > But the image has something weird. Here are the snapshot:. > . > The lines don't align well with the heatmap. > . > Additionally, the lines don't align well with the group ID colors, either. > . > . > And the ID colors seem to be not aligned well with the heatmap either. > . > Any thoughts? > . > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:642,energy efficiency,heat,heatmap,642,"There was a similar report not so long ago but I have not been able to reproduce the issue. . Seems to be related with the matplotlib version used. Maybe you can try to update to the latest version. . . > On 10 May 2019, at 01:38, brianpenghe <notifications@github.com> wrote:. > . > I ran this:. > ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7), var_group_rotation=0, dendrogram=True, save='ClusterMap.png'). > . > But the image has something weird. Here are the snapshot:. > . > The lines don't align well with the heatmap. > . > Additionally, the lines don't align well with the group ID colors, either. > . > . > And the ID colors seem to be not aligned well with the heatmap either. > . > Any thoughts? > . > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:797,energy efficiency,heat,heatmap,797,"There was a similar report not so long ago but I have not been able to reproduce the issue. . Seems to be related with the matplotlib version used. Maybe you can try to update to the latest version. . . > On 10 May 2019, at 01:38, brianpenghe <notifications@github.com> wrote:. > . > I ran this:. > ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7), var_group_rotation=0, dendrogram=True, save='ClusterMap.png'). > . > But the image has something weird. Here are the snapshot:. > . > The lines don't align well with the heatmap. > . > Additionally, the lines don't align well with the group ID colors, either. > . > . > And the ID colors seem to be not aligned well with the heatmap either. > . > Any thoughts? > . > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:134,integrability,version,version,134,"There was a similar report not so long ago but I have not been able to reproduce the issue. . Seems to be related with the matplotlib version used. Maybe you can try to update to the latest version. . . > On 10 May 2019, at 01:38, brianpenghe <notifications@github.com> wrote:. > . > I ran this:. > ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7), var_group_rotation=0, dendrogram=True, save='ClusterMap.png'). > . > But the image has something weird. Here are the snapshot:. > . > The lines don't align well with the heatmap. > . > Additionally, the lines don't align well with the group ID colors, either. > . > . > And the ID colors seem to be not aligned well with the heatmap either. > . > Any thoughts? > . > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:190,integrability,version,version,190,"There was a similar report not so long ago but I have not been able to reproduce the issue. . Seems to be related with the matplotlib version used. Maybe you can try to update to the latest version. . . > On 10 May 2019, at 01:38, brianpenghe <notifications@github.com> wrote:. > . > I ran this:. > ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7), var_group_rotation=0, dendrogram=True, save='ClusterMap.png'). > . > But the image has something weird. Here are the snapshot:. > . > The lines don't align well with the heatmap. > . > Additionally, the lines don't align well with the group ID colors, either. > . > . > And the ID colors seem to be not aligned well with the heatmap either. > . > Any thoughts? > . > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:883,integrability,sub,subscribed,883,"There was a similar report not so long ago but I have not been able to reproduce the issue. . Seems to be related with the matplotlib version used. Maybe you can try to update to the latest version. . . > On 10 May 2019, at 01:38, brianpenghe <notifications@github.com> wrote:. > . > I ran this:. > ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7), var_group_rotation=0, dendrogram=True, save='ClusterMap.png'). > . > But the image has something weird. Here are the snapshot:. > . > The lines don't align well with the heatmap. > . > Additionally, the lines don't align well with the group ID colors, either. > . > . > And the ID colors seem to be not aligned well with the heatmap either. > . > Any thoughts? > . > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:134,modifiability,version,version,134,"There was a similar report not so long ago but I have not been able to reproduce the issue. . Seems to be related with the matplotlib version used. Maybe you can try to update to the latest version. . . > On 10 May 2019, at 01:38, brianpenghe <notifications@github.com> wrote:. > . > I ran this:. > ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7), var_group_rotation=0, dendrogram=True, save='ClusterMap.png'). > . > But the image has something weird. Here are the snapshot:. > . > The lines don't align well with the heatmap. > . > Additionally, the lines don't align well with the group ID colors, either. > . > . > And the ID colors seem to be not aligned well with the heatmap either. > . > Any thoughts? > . > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:190,modifiability,version,version,190,"There was a similar report not so long ago but I have not been able to reproduce the issue. . Seems to be related with the matplotlib version used. Maybe you can try to update to the latest version. . . > On 10 May 2019, at 01:38, brianpenghe <notifications@github.com> wrote:. > . > I ran this:. > ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7), var_group_rotation=0, dendrogram=True, save='ClusterMap.png'). > . > But the image has something weird. Here are the snapshot:. > . > The lines don't align well with the heatmap. > . > Additionally, the lines don't align well with the group ID colors, either. > . > . > And the ID colors seem to be not aligned well with the heatmap either. > . > Any thoughts? > . > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:169,safety,updat,update,169,"There was a similar report not so long ago but I have not been able to reproduce the issue. . Seems to be related with the matplotlib version used. Maybe you can try to update to the latest version. . . > On 10 May 2019, at 01:38, brianpenghe <notifications@github.com> wrote:. > . > I ran this:. > ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7), var_group_rotation=0, dendrogram=True, save='ClusterMap.png'). > . > But the image has something weird. Here are the snapshot:. > . > The lines don't align well with the heatmap. > . > Additionally, the lines don't align well with the group ID colors, either. > . > . > And the ID colors seem to be not aligned well with the heatmap either. > . > Any thoughts? > . > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:169,security,updat,update,169,"There was a similar report not so long ago but I have not been able to reproduce the issue. . Seems to be related with the matplotlib version used. Maybe you can try to update to the latest version. . . > On 10 May 2019, at 01:38, brianpenghe <notifications@github.com> wrote:. > . > I ran this:. > ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7), var_group_rotation=0, dendrogram=True, save='ClusterMap.png'). > . > But the image has something weird. Here are the snapshot:. > . > The lines don't align well with the heatmap. > . > Additionally, the lines don't align well with the group ID colors, either. > . > . > And the ID colors seem to be not aligned well with the heatmap either. > . > Any thoughts? > . > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:380,security,ident,ident,380,"There was a similar report not so long ago but I have not been able to reproduce the issue. . Seems to be related with the matplotlib version used. Maybe you can try to update to the latest version. . . > On 10 May 2019, at 01:38, brianpenghe <notifications@github.com> wrote:. > . > I ran this:. > ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7), var_group_rotation=0, dendrogram=True, save='ClusterMap.png'). > . > But the image has something weird. Here are the snapshot:. > . > The lines don't align well with the heatmap. > . > Additionally, the lines don't align well with the group ID colors, either. > . > . > And the ID colors seem to be not aligned well with the heatmap either. > . > Any thoughts? > . > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:499,availability,Cluster,ClusterMap,499,"> There was a similar report not so long ago but I have not been able to reproduce the issue. Seems to be related with the matplotlib version used. Maybe you can try to update to the latest version. > [â€¦](#). > On 10 May 2019, at 01:38, brianpenghe ***@***.***> wrote: I ran this: ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7), var_group_rotation=0, dendrogram=True, save='ClusterMap.png') But the image has something weird. Here are the snapshot: The lines don't align well with the heatmap. Additionally, the lines don't align well with the group ID colors, either And the ID colors seem to be not aligned well with the heatmap either. Any thoughts? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub, or mute the thread. I have been using matplotlib 3.0.3 and I believe it's the latest version already. My Scanpy version was 1.4.1. . Can it be an issue about duplicated gene names/make unique?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:134,deployability,version,version,134,"> There was a similar report not so long ago but I have not been able to reproduce the issue. Seems to be related with the matplotlib version used. Maybe you can try to update to the latest version. > [â€¦](#). > On 10 May 2019, at 01:38, brianpenghe ***@***.***> wrote: I ran this: ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7), var_group_rotation=0, dendrogram=True, save='ClusterMap.png') But the image has something weird. Here are the snapshot: The lines don't align well with the heatmap. Additionally, the lines don't align well with the group ID colors, either And the ID colors seem to be not aligned well with the heatmap either. Any thoughts? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub, or mute the thread. I have been using matplotlib 3.0.3 and I believe it's the latest version already. My Scanpy version was 1.4.1. . Can it be an issue about duplicated gene names/make unique?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:169,deployability,updat,update,169,"> There was a similar report not so long ago but I have not been able to reproduce the issue. Seems to be related with the matplotlib version used. Maybe you can try to update to the latest version. > [â€¦](#). > On 10 May 2019, at 01:38, brianpenghe ***@***.***> wrote: I ran this: ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7), var_group_rotation=0, dendrogram=True, save='ClusterMap.png') But the image has something weird. Here are the snapshot: The lines don't align well with the heatmap. Additionally, the lines don't align well with the group ID colors, either And the ID colors seem to be not aligned well with the heatmap either. Any thoughts? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub, or mute the thread. I have been using matplotlib 3.0.3 and I believe it's the latest version already. My Scanpy version was 1.4.1. . Can it be an issue about duplicated gene names/make unique?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:190,deployability,version,version,190,"> There was a similar report not so long ago but I have not been able to reproduce the issue. Seems to be related with the matplotlib version used. Maybe you can try to update to the latest version. > [â€¦](#). > On 10 May 2019, at 01:38, brianpenghe ***@***.***> wrote: I ran this: ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7), var_group_rotation=0, dendrogram=True, save='ClusterMap.png') But the image has something weird. Here are the snapshot: The lines don't align well with the heatmap. Additionally, the lines don't align well with the group ID colors, either And the ID colors seem to be not aligned well with the heatmap either. Any thoughts? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub, or mute the thread. I have been using matplotlib 3.0.3 and I believe it's the latest version already. My Scanpy version was 1.4.1. . Can it be an issue about duplicated gene names/make unique?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:499,deployability,Cluster,ClusterMap,499,"> There was a similar report not so long ago but I have not been able to reproduce the issue. Seems to be related with the matplotlib version used. Maybe you can try to update to the latest version. > [â€¦](#). > On 10 May 2019, at 01:38, brianpenghe ***@***.***> wrote: I ran this: ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7), var_group_rotation=0, dendrogram=True, save='ClusterMap.png') But the image has something weird. Here are the snapshot: The lines don't align well with the heatmap. Additionally, the lines don't align well with the group ID colors, either And the ID colors seem to be not aligned well with the heatmap either. Any thoughts? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub, or mute the thread. I have been using matplotlib 3.0.3 and I believe it's the latest version already. My Scanpy version was 1.4.1. . Can it be an issue about duplicated gene names/make unique?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:980,deployability,version,version,980,"> There was a similar report not so long ago but I have not been able to reproduce the issue. Seems to be related with the matplotlib version used. Maybe you can try to update to the latest version. > [â€¦](#). > On 10 May 2019, at 01:38, brianpenghe ***@***.***> wrote: I ran this: ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7), var_group_rotation=0, dendrogram=True, save='ClusterMap.png') But the image has something weird. Here are the snapshot: The lines don't align well with the heatmap. Additionally, the lines don't align well with the group ID colors, either And the ID colors seem to be not aligned well with the heatmap either. Any thoughts? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub, or mute the thread. I have been using matplotlib 3.0.3 and I believe it's the latest version already. My Scanpy version was 1.4.1. . Can it be an issue about duplicated gene names/make unique?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:1007,deployability,version,version,1007,"> There was a similar report not so long ago but I have not been able to reproduce the issue. Seems to be related with the matplotlib version used. Maybe you can try to update to the latest version. > [â€¦](#). > On 10 May 2019, at 01:38, brianpenghe ***@***.***> wrote: I ran this: ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7), var_group_rotation=0, dendrogram=True, save='ClusterMap.png') But the image has something weird. Here are the snapshot: The lines don't align well with the heatmap. Additionally, the lines don't align well with the group ID colors, either And the ID colors seem to be not aligned well with the heatmap either. Any thoughts? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub, or mute the thread. I have been using matplotlib 3.0.3 and I believe it's the latest version already. My Scanpy version was 1.4.1. . Can it be an issue about duplicated gene names/make unique?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:290,energy efficiency,heat,heatmap,290,"> There was a similar report not so long ago but I have not been able to reproduce the issue. Seems to be related with the matplotlib version used. Maybe you can try to update to the latest version. > [â€¦](#). > On 10 May 2019, at 01:38, brianpenghe ***@***.***> wrote: I ran this: ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7), var_group_rotation=0, dendrogram=True, save='ClusterMap.png') But the image has something weird. Here are the snapshot: The lines don't align well with the heatmap. Additionally, the lines don't align well with the group ID colors, either And the ID colors seem to be not aligned well with the heatmap either. Any thoughts? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub, or mute the thread. I have been using matplotlib 3.0.3 and I believe it's the latest version already. My Scanpy version was 1.4.1. . Can it be an issue about duplicated gene names/make unique?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:610,energy efficiency,heat,heatmap,610,"> There was a similar report not so long ago but I have not been able to reproduce the issue. Seems to be related with the matplotlib version used. Maybe you can try to update to the latest version. > [â€¦](#). > On 10 May 2019, at 01:38, brianpenghe ***@***.***> wrote: I ran this: ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7), var_group_rotation=0, dendrogram=True, save='ClusterMap.png') But the image has something weird. Here are the snapshot: The lines don't align well with the heatmap. Additionally, the lines don't align well with the group ID colors, either And the ID colors seem to be not aligned well with the heatmap either. Any thoughts? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub, or mute the thread. I have been using matplotlib 3.0.3 and I believe it's the latest version already. My Scanpy version was 1.4.1. . Can it be an issue about duplicated gene names/make unique?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:748,energy efficiency,heat,heatmap,748,"> There was a similar report not so long ago but I have not been able to reproduce the issue. Seems to be related with the matplotlib version used. Maybe you can try to update to the latest version. > [â€¦](#). > On 10 May 2019, at 01:38, brianpenghe ***@***.***> wrote: I ran this: ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7), var_group_rotation=0, dendrogram=True, save='ClusterMap.png') But the image has something weird. Here are the snapshot: The lines don't align well with the heatmap. Additionally, the lines don't align well with the group ID colors, either And the ID colors seem to be not aligned well with the heatmap either. Any thoughts? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub, or mute the thread. I have been using matplotlib 3.0.3 and I believe it's the latest version already. My Scanpy version was 1.4.1. . Can it be an issue about duplicated gene names/make unique?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:134,integrability,version,version,134,"> There was a similar report not so long ago but I have not been able to reproduce the issue. Seems to be related with the matplotlib version used. Maybe you can try to update to the latest version. > [â€¦](#). > On 10 May 2019, at 01:38, brianpenghe ***@***.***> wrote: I ran this: ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7), var_group_rotation=0, dendrogram=True, save='ClusterMap.png') But the image has something weird. Here are the snapshot: The lines don't align well with the heatmap. Additionally, the lines don't align well with the group ID colors, either And the ID colors seem to be not aligned well with the heatmap either. Any thoughts? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub, or mute the thread. I have been using matplotlib 3.0.3 and I believe it's the latest version already. My Scanpy version was 1.4.1. . Can it be an issue about duplicated gene names/make unique?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:190,integrability,version,version,190,"> There was a similar report not so long ago but I have not been able to reproduce the issue. Seems to be related with the matplotlib version used. Maybe you can try to update to the latest version. > [â€¦](#). > On 10 May 2019, at 01:38, brianpenghe ***@***.***> wrote: I ran this: ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7), var_group_rotation=0, dendrogram=True, save='ClusterMap.png') But the image has something weird. Here are the snapshot: The lines don't align well with the heatmap. Additionally, the lines don't align well with the group ID colors, either And the ID colors seem to be not aligned well with the heatmap either. Any thoughts? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub, or mute the thread. I have been using matplotlib 3.0.3 and I believe it's the latest version already. My Scanpy version was 1.4.1. . Can it be an issue about duplicated gene names/make unique?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:819,integrability,sub,subscribed,819,"> There was a similar report not so long ago but I have not been able to reproduce the issue. Seems to be related with the matplotlib version used. Maybe you can try to update to the latest version. > [â€¦](#). > On 10 May 2019, at 01:38, brianpenghe ***@***.***> wrote: I ran this: ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7), var_group_rotation=0, dendrogram=True, save='ClusterMap.png') But the image has something weird. Here are the snapshot: The lines don't align well with the heatmap. Additionally, the lines don't align well with the group ID colors, either And the ID colors seem to be not aligned well with the heatmap either. Any thoughts? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub, or mute the thread. I have been using matplotlib 3.0.3 and I believe it's the latest version already. My Scanpy version was 1.4.1. . Can it be an issue about duplicated gene names/make unique?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:980,integrability,version,version,980,"> There was a similar report not so long ago but I have not been able to reproduce the issue. Seems to be related with the matplotlib version used. Maybe you can try to update to the latest version. > [â€¦](#). > On 10 May 2019, at 01:38, brianpenghe ***@***.***> wrote: I ran this: ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7), var_group_rotation=0, dendrogram=True, save='ClusterMap.png') But the image has something weird. Here are the snapshot: The lines don't align well with the heatmap. Additionally, the lines don't align well with the group ID colors, either And the ID colors seem to be not aligned well with the heatmap either. Any thoughts? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub, or mute the thread. I have been using matplotlib 3.0.3 and I believe it's the latest version already. My Scanpy version was 1.4.1. . Can it be an issue about duplicated gene names/make unique?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:1007,integrability,version,version,1007,"> There was a similar report not so long ago but I have not been able to reproduce the issue. Seems to be related with the matplotlib version used. Maybe you can try to update to the latest version. > [â€¦](#). > On 10 May 2019, at 01:38, brianpenghe ***@***.***> wrote: I ran this: ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7), var_group_rotation=0, dendrogram=True, save='ClusterMap.png') But the image has something weird. Here are the snapshot: The lines don't align well with the heatmap. Additionally, the lines don't align well with the group ID colors, either And the ID colors seem to be not aligned well with the heatmap either. Any thoughts? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub, or mute the thread. I have been using matplotlib 3.0.3 and I believe it's the latest version already. My Scanpy version was 1.4.1. . Can it be an issue about duplicated gene names/make unique?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:134,modifiability,version,version,134,"> There was a similar report not so long ago but I have not been able to reproduce the issue. Seems to be related with the matplotlib version used. Maybe you can try to update to the latest version. > [â€¦](#). > On 10 May 2019, at 01:38, brianpenghe ***@***.***> wrote: I ran this: ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7), var_group_rotation=0, dendrogram=True, save='ClusterMap.png') But the image has something weird. Here are the snapshot: The lines don't align well with the heatmap. Additionally, the lines don't align well with the group ID colors, either And the ID colors seem to be not aligned well with the heatmap either. Any thoughts? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub, or mute the thread. I have been using matplotlib 3.0.3 and I believe it's the latest version already. My Scanpy version was 1.4.1. . Can it be an issue about duplicated gene names/make unique?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:190,modifiability,version,version,190,"> There was a similar report not so long ago but I have not been able to reproduce the issue. Seems to be related with the matplotlib version used. Maybe you can try to update to the latest version. > [â€¦](#). > On 10 May 2019, at 01:38, brianpenghe ***@***.***> wrote: I ran this: ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7), var_group_rotation=0, dendrogram=True, save='ClusterMap.png') But the image has something weird. Here are the snapshot: The lines don't align well with the heatmap. Additionally, the lines don't align well with the group ID colors, either And the ID colors seem to be not aligned well with the heatmap either. Any thoughts? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub, or mute the thread. I have been using matplotlib 3.0.3 and I believe it's the latest version already. My Scanpy version was 1.4.1. . Can it be an issue about duplicated gene names/make unique?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:980,modifiability,version,version,980,"> There was a similar report not so long ago but I have not been able to reproduce the issue. Seems to be related with the matplotlib version used. Maybe you can try to update to the latest version. > [â€¦](#). > On 10 May 2019, at 01:38, brianpenghe ***@***.***> wrote: I ran this: ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7), var_group_rotation=0, dendrogram=True, save='ClusterMap.png') But the image has something weird. Here are the snapshot: The lines don't align well with the heatmap. Additionally, the lines don't align well with the group ID colors, either And the ID colors seem to be not aligned well with the heatmap either. Any thoughts? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub, or mute the thread. I have been using matplotlib 3.0.3 and I believe it's the latest version already. My Scanpy version was 1.4.1. . Can it be an issue about duplicated gene names/make unique?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:1007,modifiability,version,version,1007,"> There was a similar report not so long ago but I have not been able to reproduce the issue. Seems to be related with the matplotlib version used. Maybe you can try to update to the latest version. > [â€¦](#). > On 10 May 2019, at 01:38, brianpenghe ***@***.***> wrote: I ran this: ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7), var_group_rotation=0, dendrogram=True, save='ClusterMap.png') But the image has something weird. Here are the snapshot: The lines don't align well with the heatmap. Additionally, the lines don't align well with the group ID colors, either And the ID colors seem to be not aligned well with the heatmap either. Any thoughts? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub, or mute the thread. I have been using matplotlib 3.0.3 and I believe it's the latest version already. My Scanpy version was 1.4.1. . Can it be an issue about duplicated gene names/make unique?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:169,safety,updat,update,169,"> There was a similar report not so long ago but I have not been able to reproduce the issue. Seems to be related with the matplotlib version used. Maybe you can try to update to the latest version. > [â€¦](#). > On 10 May 2019, at 01:38, brianpenghe ***@***.***> wrote: I ran this: ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7), var_group_rotation=0, dendrogram=True, save='ClusterMap.png') But the image has something weird. Here are the snapshot: The lines don't align well with the heatmap. Additionally, the lines don't align well with the group ID colors, either And the ID colors seem to be not aligned well with the heatmap either. Any thoughts? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub, or mute the thread. I have been using matplotlib 3.0.3 and I believe it's the latest version already. My Scanpy version was 1.4.1. . Can it be an issue about duplicated gene names/make unique?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:169,security,updat,update,169,"> There was a similar report not so long ago but I have not been able to reproduce the issue. Seems to be related with the matplotlib version used. Maybe you can try to update to the latest version. > [â€¦](#). > On 10 May 2019, at 01:38, brianpenghe ***@***.***> wrote: I ran this: ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7), var_group_rotation=0, dendrogram=True, save='ClusterMap.png') But the image has something weird. Here are the snapshot: The lines don't align well with the heatmap. Additionally, the lines don't align well with the group ID colors, either And the ID colors seem to be not aligned well with the heatmap either. Any thoughts? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub, or mute the thread. I have been using matplotlib 3.0.3 and I believe it's the latest version already. My Scanpy version was 1.4.1. . Can it be an issue about duplicated gene names/make unique?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:362,security,ident,ident,362,"> There was a similar report not so long ago but I have not been able to reproduce the issue. Seems to be related with the matplotlib version used. Maybe you can try to update to the latest version. > [â€¦](#). > On 10 May 2019, at 01:38, brianpenghe ***@***.***> wrote: I ran this: ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7), var_group_rotation=0, dendrogram=True, save='ClusterMap.png') But the image has something weird. Here are the snapshot: The lines don't align well with the heatmap. Additionally, the lines don't align well with the group ID colors, either And the ID colors seem to be not aligned well with the heatmap either. Any thoughts? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub, or mute the thread. I have been using matplotlib 3.0.3 and I believe it's the latest version already. My Scanpy version was 1.4.1. . Can it be an issue about duplicated gene names/make unique?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:958,availability,Cluster,ClusterMap,958,"In your heat map the lines are separating categorical groups right? In other words genes are rows. . In your command you are doing a selection over a larger dataset. Have you tried to make a copy of the adata object? adata = Mouse10Xdata[NewIndex3,:].copy(). I ask in case the categories reported in the selection that you are doing create a mismatch, which may be solved by doing a copy. . > On 15 May 2019, at 11:24, brianpenghe <notifications@github.com> wrote:. > . > There was a similar report not so long ago but I have not been able to reproduce the issue. Seems to be related with the matplotlib version used. Maybe you can try to update to the latest version. > â€¦. > On 10 May 2019, at 01:38, brianpenghe @.***> wrote: I ran this: ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7), var_group_rotation=0, dendrogram=True, save='ClusterMap.png') But the image has something weird. Here are the snapshot: The lines don't align well with the heatmap. Additionally, the lines don't align well with the group ID colors, either And the ID colors seem to be not aligned well with the heatmap either. Any thoughts? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub, or mute the thread. > . > I have been using matplotlib 3.0.3 and I believe it's the latest version already. My Scanpy version was 1.4.1. > Can it be an issue about duplicated gene names/make unique? > . > â€”. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:604,deployability,version,version,604,"In your heat map the lines are separating categorical groups right? In other words genes are rows. . In your command you are doing a selection over a larger dataset. Have you tried to make a copy of the adata object? adata = Mouse10Xdata[NewIndex3,:].copy(). I ask in case the categories reported in the selection that you are doing create a mismatch, which may be solved by doing a copy. . > On 15 May 2019, at 11:24, brianpenghe <notifications@github.com> wrote:. > . > There was a similar report not so long ago but I have not been able to reproduce the issue. Seems to be related with the matplotlib version used. Maybe you can try to update to the latest version. > â€¦. > On 10 May 2019, at 01:38, brianpenghe @.***> wrote: I ran this: ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7), var_group_rotation=0, dendrogram=True, save='ClusterMap.png') But the image has something weird. Here are the snapshot: The lines don't align well with the heatmap. Additionally, the lines don't align well with the group ID colors, either And the ID colors seem to be not aligned well with the heatmap either. Any thoughts? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub, or mute the thread. > . > I have been using matplotlib 3.0.3 and I believe it's the latest version already. My Scanpy version was 1.4.1. > Can it be an issue about duplicated gene names/make unique? > . > â€”. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:639,deployability,updat,update,639,"In your heat map the lines are separating categorical groups right? In other words genes are rows. . In your command you are doing a selection over a larger dataset. Have you tried to make a copy of the adata object? adata = Mouse10Xdata[NewIndex3,:].copy(). I ask in case the categories reported in the selection that you are doing create a mismatch, which may be solved by doing a copy. . > On 15 May 2019, at 11:24, brianpenghe <notifications@github.com> wrote:. > . > There was a similar report not so long ago but I have not been able to reproduce the issue. Seems to be related with the matplotlib version used. Maybe you can try to update to the latest version. > â€¦. > On 10 May 2019, at 01:38, brianpenghe @.***> wrote: I ran this: ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7), var_group_rotation=0, dendrogram=True, save='ClusterMap.png') But the image has something weird. Here are the snapshot: The lines don't align well with the heatmap. Additionally, the lines don't align well with the group ID colors, either And the ID colors seem to be not aligned well with the heatmap either. Any thoughts? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub, or mute the thread. > . > I have been using matplotlib 3.0.3 and I believe it's the latest version already. My Scanpy version was 1.4.1. > Can it be an issue about duplicated gene names/make unique? > . > â€”. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:660,deployability,version,version,660,"In your heat map the lines are separating categorical groups right? In other words genes are rows. . In your command you are doing a selection over a larger dataset. Have you tried to make a copy of the adata object? adata = Mouse10Xdata[NewIndex3,:].copy(). I ask in case the categories reported in the selection that you are doing create a mismatch, which may be solved by doing a copy. . > On 15 May 2019, at 11:24, brianpenghe <notifications@github.com> wrote:. > . > There was a similar report not so long ago but I have not been able to reproduce the issue. Seems to be related with the matplotlib version used. Maybe you can try to update to the latest version. > â€¦. > On 10 May 2019, at 01:38, brianpenghe @.***> wrote: I ran this: ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7), var_group_rotation=0, dendrogram=True, save='ClusterMap.png') But the image has something weird. Here are the snapshot: The lines don't align well with the heatmap. Additionally, the lines don't align well with the group ID colors, either And the ID colors seem to be not aligned well with the heatmap either. Any thoughts? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub, or mute the thread. > . > I have been using matplotlib 3.0.3 and I believe it's the latest version already. My Scanpy version was 1.4.1. > Can it be an issue about duplicated gene names/make unique? > . > â€”. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:958,deployability,Cluster,ClusterMap,958,"In your heat map the lines are separating categorical groups right? In other words genes are rows. . In your command you are doing a selection over a larger dataset. Have you tried to make a copy of the adata object? adata = Mouse10Xdata[NewIndex3,:].copy(). I ask in case the categories reported in the selection that you are doing create a mismatch, which may be solved by doing a copy. . > On 15 May 2019, at 11:24, brianpenghe <notifications@github.com> wrote:. > . > There was a similar report not so long ago but I have not been able to reproduce the issue. Seems to be related with the matplotlib version used. Maybe you can try to update to the latest version. > â€¦. > On 10 May 2019, at 01:38, brianpenghe @.***> wrote: I ran this: ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7), var_group_rotation=0, dendrogram=True, save='ClusterMap.png') But the image has something weird. Here are the snapshot: The lines don't align well with the heatmap. Additionally, the lines don't align well with the group ID colors, either And the ID colors seem to be not aligned well with the heatmap either. Any thoughts? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub, or mute the thread. > . > I have been using matplotlib 3.0.3 and I believe it's the latest version already. My Scanpy version was 1.4.1. > Can it be an issue about duplicated gene names/make unique? > . > â€”. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:1445,deployability,version,version,1445,"In your heat map the lines are separating categorical groups right? In other words genes are rows. . In your command you are doing a selection over a larger dataset. Have you tried to make a copy of the adata object? adata = Mouse10Xdata[NewIndex3,:].copy(). I ask in case the categories reported in the selection that you are doing create a mismatch, which may be solved by doing a copy. . > On 15 May 2019, at 11:24, brianpenghe <notifications@github.com> wrote:. > . > There was a similar report not so long ago but I have not been able to reproduce the issue. Seems to be related with the matplotlib version used. Maybe you can try to update to the latest version. > â€¦. > On 10 May 2019, at 01:38, brianpenghe @.***> wrote: I ran this: ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7), var_group_rotation=0, dendrogram=True, save='ClusterMap.png') But the image has something weird. Here are the snapshot: The lines don't align well with the heatmap. Additionally, the lines don't align well with the group ID colors, either And the ID colors seem to be not aligned well with the heatmap either. Any thoughts? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub, or mute the thread. > . > I have been using matplotlib 3.0.3 and I believe it's the latest version already. My Scanpy version was 1.4.1. > Can it be an issue about duplicated gene names/make unique? > . > â€”. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:1472,deployability,version,version,1472,"In your heat map the lines are separating categorical groups right? In other words genes are rows. . In your command you are doing a selection over a larger dataset. Have you tried to make a copy of the adata object? adata = Mouse10Xdata[NewIndex3,:].copy(). I ask in case the categories reported in the selection that you are doing create a mismatch, which may be solved by doing a copy. . > On 15 May 2019, at 11:24, brianpenghe <notifications@github.com> wrote:. > . > There was a similar report not so long ago but I have not been able to reproduce the issue. Seems to be related with the matplotlib version used. Maybe you can try to update to the latest version. > â€¦. > On 10 May 2019, at 01:38, brianpenghe @.***> wrote: I ran this: ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7), var_group_rotation=0, dendrogram=True, save='ClusterMap.png') But the image has something weird. Here are the snapshot: The lines don't align well with the heatmap. Additionally, the lines don't align well with the group ID colors, either And the ID colors seem to be not aligned well with the heatmap either. Any thoughts? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub, or mute the thread. > . > I have been using matplotlib 3.0.3 and I believe it's the latest version already. My Scanpy version was 1.4.1. > Can it be an issue about duplicated gene names/make unique? > . > â€”. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:8,energy efficiency,heat,heat,8,"In your heat map the lines are separating categorical groups right? In other words genes are rows. . In your command you are doing a selection over a larger dataset. Have you tried to make a copy of the adata object? adata = Mouse10Xdata[NewIndex3,:].copy(). I ask in case the categories reported in the selection that you are doing create a mismatch, which may be solved by doing a copy. . > On 15 May 2019, at 11:24, brianpenghe <notifications@github.com> wrote:. > . > There was a similar report not so long ago but I have not been able to reproduce the issue. Seems to be related with the matplotlib version used. Maybe you can try to update to the latest version. > â€¦. > On 10 May 2019, at 01:38, brianpenghe @.***> wrote: I ran this: ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7), var_group_rotation=0, dendrogram=True, save='ClusterMap.png') But the image has something weird. Here are the snapshot: The lines don't align well with the heatmap. Additionally, the lines don't align well with the group ID colors, either And the ID colors seem to be not aligned well with the heatmap either. Any thoughts? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub, or mute the thread. > . > I have been using matplotlib 3.0.3 and I believe it's the latest version already. My Scanpy version was 1.4.1. > Can it be an issue about duplicated gene names/make unique? > . > â€”. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:749,energy efficiency,heat,heatmap,749,"In your heat map the lines are separating categorical groups right? In other words genes are rows. . In your command you are doing a selection over a larger dataset. Have you tried to make a copy of the adata object? adata = Mouse10Xdata[NewIndex3,:].copy(). I ask in case the categories reported in the selection that you are doing create a mismatch, which may be solved by doing a copy. . > On 15 May 2019, at 11:24, brianpenghe <notifications@github.com> wrote:. > . > There was a similar report not so long ago but I have not been able to reproduce the issue. Seems to be related with the matplotlib version used. Maybe you can try to update to the latest version. > â€¦. > On 10 May 2019, at 01:38, brianpenghe @.***> wrote: I ran this: ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7), var_group_rotation=0, dendrogram=True, save='ClusterMap.png') But the image has something weird. Here are the snapshot: The lines don't align well with the heatmap. Additionally, the lines don't align well with the group ID colors, either And the ID colors seem to be not aligned well with the heatmap either. Any thoughts? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub, or mute the thread. > . > I have been using matplotlib 3.0.3 and I believe it's the latest version already. My Scanpy version was 1.4.1. > Can it be an issue about duplicated gene names/make unique? > . > â€”. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:1069,energy efficiency,heat,heatmap,1069,"In your heat map the lines are separating categorical groups right? In other words genes are rows. . In your command you are doing a selection over a larger dataset. Have you tried to make a copy of the adata object? adata = Mouse10Xdata[NewIndex3,:].copy(). I ask in case the categories reported in the selection that you are doing create a mismatch, which may be solved by doing a copy. . > On 15 May 2019, at 11:24, brianpenghe <notifications@github.com> wrote:. > . > There was a similar report not so long ago but I have not been able to reproduce the issue. Seems to be related with the matplotlib version used. Maybe you can try to update to the latest version. > â€¦. > On 10 May 2019, at 01:38, brianpenghe @.***> wrote: I ran this: ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7), var_group_rotation=0, dendrogram=True, save='ClusterMap.png') But the image has something weird. Here are the snapshot: The lines don't align well with the heatmap. Additionally, the lines don't align well with the group ID colors, either And the ID colors seem to be not aligned well with the heatmap either. Any thoughts? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub, or mute the thread. > . > I have been using matplotlib 3.0.3 and I believe it's the latest version already. My Scanpy version was 1.4.1. > Can it be an issue about duplicated gene names/make unique? > . > â€”. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:1207,energy efficiency,heat,heatmap,1207,"In your heat map the lines are separating categorical groups right? In other words genes are rows. . In your command you are doing a selection over a larger dataset. Have you tried to make a copy of the adata object? adata = Mouse10Xdata[NewIndex3,:].copy(). I ask in case the categories reported in the selection that you are doing create a mismatch, which may be solved by doing a copy. . > On 15 May 2019, at 11:24, brianpenghe <notifications@github.com> wrote:. > . > There was a similar report not so long ago but I have not been able to reproduce the issue. Seems to be related with the matplotlib version used. Maybe you can try to update to the latest version. > â€¦. > On 10 May 2019, at 01:38, brianpenghe @.***> wrote: I ran this: ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7), var_group_rotation=0, dendrogram=True, save='ClusterMap.png') But the image has something weird. Here are the snapshot: The lines don't align well with the heatmap. Additionally, the lines don't align well with the group ID colors, either And the ID colors seem to be not aligned well with the heatmap either. Any thoughts? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub, or mute the thread. > . > I have been using matplotlib 3.0.3 and I believe it's the latest version already. My Scanpy version was 1.4.1. > Can it be an issue about duplicated gene names/make unique? > . > â€”. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:604,integrability,version,version,604,"In your heat map the lines are separating categorical groups right? In other words genes are rows. . In your command you are doing a selection over a larger dataset. Have you tried to make a copy of the adata object? adata = Mouse10Xdata[NewIndex3,:].copy(). I ask in case the categories reported in the selection that you are doing create a mismatch, which may be solved by doing a copy. . > On 15 May 2019, at 11:24, brianpenghe <notifications@github.com> wrote:. > . > There was a similar report not so long ago but I have not been able to reproduce the issue. Seems to be related with the matplotlib version used. Maybe you can try to update to the latest version. > â€¦. > On 10 May 2019, at 01:38, brianpenghe @.***> wrote: I ran this: ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7), var_group_rotation=0, dendrogram=True, save='ClusterMap.png') But the image has something weird. Here are the snapshot: The lines don't align well with the heatmap. Additionally, the lines don't align well with the group ID colors, either And the ID colors seem to be not aligned well with the heatmap either. Any thoughts? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub, or mute the thread. > . > I have been using matplotlib 3.0.3 and I believe it's the latest version already. My Scanpy version was 1.4.1. > Can it be an issue about duplicated gene names/make unique? > . > â€”. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:660,integrability,version,version,660,"In your heat map the lines are separating categorical groups right? In other words genes are rows. . In your command you are doing a selection over a larger dataset. Have you tried to make a copy of the adata object? adata = Mouse10Xdata[NewIndex3,:].copy(). I ask in case the categories reported in the selection that you are doing create a mismatch, which may be solved by doing a copy. . > On 15 May 2019, at 11:24, brianpenghe <notifications@github.com> wrote:. > . > There was a similar report not so long ago but I have not been able to reproduce the issue. Seems to be related with the matplotlib version used. Maybe you can try to update to the latest version. > â€¦. > On 10 May 2019, at 01:38, brianpenghe @.***> wrote: I ran this: ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7), var_group_rotation=0, dendrogram=True, save='ClusterMap.png') But the image has something weird. Here are the snapshot: The lines don't align well with the heatmap. Additionally, the lines don't align well with the group ID colors, either And the ID colors seem to be not aligned well with the heatmap either. Any thoughts? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub, or mute the thread. > . > I have been using matplotlib 3.0.3 and I believe it's the latest version already. My Scanpy version was 1.4.1. > Can it be an issue about duplicated gene names/make unique? > . > â€”. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:1278,integrability,sub,subscribed,1278,"In your heat map the lines are separating categorical groups right? In other words genes are rows. . In your command you are doing a selection over a larger dataset. Have you tried to make a copy of the adata object? adata = Mouse10Xdata[NewIndex3,:].copy(). I ask in case the categories reported in the selection that you are doing create a mismatch, which may be solved by doing a copy. . > On 15 May 2019, at 11:24, brianpenghe <notifications@github.com> wrote:. > . > There was a similar report not so long ago but I have not been able to reproduce the issue. Seems to be related with the matplotlib version used. Maybe you can try to update to the latest version. > â€¦. > On 10 May 2019, at 01:38, brianpenghe @.***> wrote: I ran this: ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7), var_group_rotation=0, dendrogram=True, save='ClusterMap.png') But the image has something weird. Here are the snapshot: The lines don't align well with the heatmap. Additionally, the lines don't align well with the group ID colors, either And the ID colors seem to be not aligned well with the heatmap either. Any thoughts? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub, or mute the thread. > . > I have been using matplotlib 3.0.3 and I believe it's the latest version already. My Scanpy version was 1.4.1. > Can it be an issue about duplicated gene names/make unique? > . > â€”. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:1445,integrability,version,version,1445,"In your heat map the lines are separating categorical groups right? In other words genes are rows. . In your command you are doing a selection over a larger dataset. Have you tried to make a copy of the adata object? adata = Mouse10Xdata[NewIndex3,:].copy(). I ask in case the categories reported in the selection that you are doing create a mismatch, which may be solved by doing a copy. . > On 15 May 2019, at 11:24, brianpenghe <notifications@github.com> wrote:. > . > There was a similar report not so long ago but I have not been able to reproduce the issue. Seems to be related with the matplotlib version used. Maybe you can try to update to the latest version. > â€¦. > On 10 May 2019, at 01:38, brianpenghe @.***> wrote: I ran this: ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7), var_group_rotation=0, dendrogram=True, save='ClusterMap.png') But the image has something weird. Here are the snapshot: The lines don't align well with the heatmap. Additionally, the lines don't align well with the group ID colors, either And the ID colors seem to be not aligned well with the heatmap either. Any thoughts? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub, or mute the thread. > . > I have been using matplotlib 3.0.3 and I believe it's the latest version already. My Scanpy version was 1.4.1. > Can it be an issue about duplicated gene names/make unique? > . > â€”. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:1472,integrability,version,version,1472,"In your heat map the lines are separating categorical groups right? In other words genes are rows. . In your command you are doing a selection over a larger dataset. Have you tried to make a copy of the adata object? adata = Mouse10Xdata[NewIndex3,:].copy(). I ask in case the categories reported in the selection that you are doing create a mismatch, which may be solved by doing a copy. . > On 15 May 2019, at 11:24, brianpenghe <notifications@github.com> wrote:. > . > There was a similar report not so long ago but I have not been able to reproduce the issue. Seems to be related with the matplotlib version used. Maybe you can try to update to the latest version. > â€¦. > On 10 May 2019, at 01:38, brianpenghe @.***> wrote: I ran this: ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7), var_group_rotation=0, dendrogram=True, save='ClusterMap.png') But the image has something weird. Here are the snapshot: The lines don't align well with the heatmap. Additionally, the lines don't align well with the group ID colors, either And the ID colors seem to be not aligned well with the heatmap either. Any thoughts? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub, or mute the thread. > . > I have been using matplotlib 3.0.3 and I believe it's the latest version already. My Scanpy version was 1.4.1. > Can it be an issue about duplicated gene names/make unique? > . > â€”. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:342,interoperability,mismatch,mismatch,342,"In your heat map the lines are separating categorical groups right? In other words genes are rows. . In your command you are doing a selection over a larger dataset. Have you tried to make a copy of the adata object? adata = Mouse10Xdata[NewIndex3,:].copy(). I ask in case the categories reported in the selection that you are doing create a mismatch, which may be solved by doing a copy. . > On 15 May 2019, at 11:24, brianpenghe <notifications@github.com> wrote:. > . > There was a similar report not so long ago but I have not been able to reproduce the issue. Seems to be related with the matplotlib version used. Maybe you can try to update to the latest version. > â€¦. > On 10 May 2019, at 01:38, brianpenghe @.***> wrote: I ran this: ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7), var_group_rotation=0, dendrogram=True, save='ClusterMap.png') But the image has something weird. Here are the snapshot: The lines don't align well with the heatmap. Additionally, the lines don't align well with the group ID colors, either And the ID colors seem to be not aligned well with the heatmap either. Any thoughts? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub, or mute the thread. > . > I have been using matplotlib 3.0.3 and I believe it's the latest version already. My Scanpy version was 1.4.1. > Can it be an issue about duplicated gene names/make unique? > . > â€”. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:604,modifiability,version,version,604,"In your heat map the lines are separating categorical groups right? In other words genes are rows. . In your command you are doing a selection over a larger dataset. Have you tried to make a copy of the adata object? adata = Mouse10Xdata[NewIndex3,:].copy(). I ask in case the categories reported in the selection that you are doing create a mismatch, which may be solved by doing a copy. . > On 15 May 2019, at 11:24, brianpenghe <notifications@github.com> wrote:. > . > There was a similar report not so long ago but I have not been able to reproduce the issue. Seems to be related with the matplotlib version used. Maybe you can try to update to the latest version. > â€¦. > On 10 May 2019, at 01:38, brianpenghe @.***> wrote: I ran this: ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7), var_group_rotation=0, dendrogram=True, save='ClusterMap.png') But the image has something weird. Here are the snapshot: The lines don't align well with the heatmap. Additionally, the lines don't align well with the group ID colors, either And the ID colors seem to be not aligned well with the heatmap either. Any thoughts? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub, or mute the thread. > . > I have been using matplotlib 3.0.3 and I believe it's the latest version already. My Scanpy version was 1.4.1. > Can it be an issue about duplicated gene names/make unique? > . > â€”. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:660,modifiability,version,version,660,"In your heat map the lines are separating categorical groups right? In other words genes are rows. . In your command you are doing a selection over a larger dataset. Have you tried to make a copy of the adata object? adata = Mouse10Xdata[NewIndex3,:].copy(). I ask in case the categories reported in the selection that you are doing create a mismatch, which may be solved by doing a copy. . > On 15 May 2019, at 11:24, brianpenghe <notifications@github.com> wrote:. > . > There was a similar report not so long ago but I have not been able to reproduce the issue. Seems to be related with the matplotlib version used. Maybe you can try to update to the latest version. > â€¦. > On 10 May 2019, at 01:38, brianpenghe @.***> wrote: I ran this: ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7), var_group_rotation=0, dendrogram=True, save='ClusterMap.png') But the image has something weird. Here are the snapshot: The lines don't align well with the heatmap. Additionally, the lines don't align well with the group ID colors, either And the ID colors seem to be not aligned well with the heatmap either. Any thoughts? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub, or mute the thread. > . > I have been using matplotlib 3.0.3 and I believe it's the latest version already. My Scanpy version was 1.4.1. > Can it be an issue about duplicated gene names/make unique? > . > â€”. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:1445,modifiability,version,version,1445,"In your heat map the lines are separating categorical groups right? In other words genes are rows. . In your command you are doing a selection over a larger dataset. Have you tried to make a copy of the adata object? adata = Mouse10Xdata[NewIndex3,:].copy(). I ask in case the categories reported in the selection that you are doing create a mismatch, which may be solved by doing a copy. . > On 15 May 2019, at 11:24, brianpenghe <notifications@github.com> wrote:. > . > There was a similar report not so long ago but I have not been able to reproduce the issue. Seems to be related with the matplotlib version used. Maybe you can try to update to the latest version. > â€¦. > On 10 May 2019, at 01:38, brianpenghe @.***> wrote: I ran this: ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7), var_group_rotation=0, dendrogram=True, save='ClusterMap.png') But the image has something weird. Here are the snapshot: The lines don't align well with the heatmap. Additionally, the lines don't align well with the group ID colors, either And the ID colors seem to be not aligned well with the heatmap either. Any thoughts? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub, or mute the thread. > . > I have been using matplotlib 3.0.3 and I believe it's the latest version already. My Scanpy version was 1.4.1. > Can it be an issue about duplicated gene names/make unique? > . > â€”. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:1472,modifiability,version,version,1472,"In your heat map the lines are separating categorical groups right? In other words genes are rows. . In your command you are doing a selection over a larger dataset. Have you tried to make a copy of the adata object? adata = Mouse10Xdata[NewIndex3,:].copy(). I ask in case the categories reported in the selection that you are doing create a mismatch, which may be solved by doing a copy. . > On 15 May 2019, at 11:24, brianpenghe <notifications@github.com> wrote:. > . > There was a similar report not so long ago but I have not been able to reproduce the issue. Seems to be related with the matplotlib version used. Maybe you can try to update to the latest version. > â€¦. > On 10 May 2019, at 01:38, brianpenghe @.***> wrote: I ran this: ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7), var_group_rotation=0, dendrogram=True, save='ClusterMap.png') But the image has something weird. Here are the snapshot: The lines don't align well with the heatmap. Additionally, the lines don't align well with the group ID colors, either And the ID colors seem to be not aligned well with the heatmap either. Any thoughts? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub, or mute the thread. > . > I have been using matplotlib 3.0.3 and I believe it's the latest version already. My Scanpy version was 1.4.1. > Can it be an issue about duplicated gene names/make unique? > . > â€”. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:639,safety,updat,update,639,"In your heat map the lines are separating categorical groups right? In other words genes are rows. . In your command you are doing a selection over a larger dataset. Have you tried to make a copy of the adata object? adata = Mouse10Xdata[NewIndex3,:].copy(). I ask in case the categories reported in the selection that you are doing create a mismatch, which may be solved by doing a copy. . > On 15 May 2019, at 11:24, brianpenghe <notifications@github.com> wrote:. > . > There was a similar report not so long ago but I have not been able to reproduce the issue. Seems to be related with the matplotlib version used. Maybe you can try to update to the latest version. > â€¦. > On 10 May 2019, at 01:38, brianpenghe @.***> wrote: I ran this: ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7), var_group_rotation=0, dendrogram=True, save='ClusterMap.png') But the image has something weird. Here are the snapshot: The lines don't align well with the heatmap. Additionally, the lines don't align well with the group ID colors, either And the ID colors seem to be not aligned well with the heatmap either. Any thoughts? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub, or mute the thread. > . > I have been using matplotlib 3.0.3 and I believe it's the latest version already. My Scanpy version was 1.4.1. > Can it be an issue about duplicated gene names/make unique? > . > â€”. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:639,security,updat,update,639,"In your heat map the lines are separating categorical groups right? In other words genes are rows. . In your command you are doing a selection over a larger dataset. Have you tried to make a copy of the adata object? adata = Mouse10Xdata[NewIndex3,:].copy(). I ask in case the categories reported in the selection that you are doing create a mismatch, which may be solved by doing a copy. . > On 15 May 2019, at 11:24, brianpenghe <notifications@github.com> wrote:. > . > There was a similar report not so long ago but I have not been able to reproduce the issue. Seems to be related with the matplotlib version used. Maybe you can try to update to the latest version. > â€¦. > On 10 May 2019, at 01:38, brianpenghe @.***> wrote: I ran this: ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7), var_group_rotation=0, dendrogram=True, save='ClusterMap.png') But the image has something weird. Here are the snapshot: The lines don't align well with the heatmap. Additionally, the lines don't align well with the group ID colors, either And the ID colors seem to be not aligned well with the heatmap either. Any thoughts? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub, or mute the thread. > . > I have been using matplotlib 3.0.3 and I believe it's the latest version already. My Scanpy version was 1.4.1. > Can it be an issue about duplicated gene names/make unique? > . > â€”. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:821,security,ident,ident,821,"In your heat map the lines are separating categorical groups right? In other words genes are rows. . In your command you are doing a selection over a larger dataset. Have you tried to make a copy of the adata object? adata = Mouse10Xdata[NewIndex3,:].copy(). I ask in case the categories reported in the selection that you are doing create a mismatch, which may be solved by doing a copy. . > On 15 May 2019, at 11:24, brianpenghe <notifications@github.com> wrote:. > . > There was a similar report not so long ago but I have not been able to reproduce the issue. Seems to be related with the matplotlib version used. Maybe you can try to update to the latest version. > â€¦. > On 10 May 2019, at 01:38, brianpenghe @.***> wrote: I ran this: ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7), var_group_rotation=0, dendrogram=True, save='ClusterMap.png') But the image has something weird. Here are the snapshot: The lines don't align well with the heatmap. Additionally, the lines don't align well with the group ID colors, either And the ID colors seem to be not aligned well with the heatmap either. Any thoughts? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub, or mute the thread. > . > I have been using matplotlib 3.0.3 and I believe it's the latest version already. My Scanpy version was 1.4.1. > Can it be an issue about duplicated gene names/make unique? > . > â€”. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:109,usability,command,command,109,"In your heat map the lines are separating categorical groups right? In other words genes are rows. . In your command you are doing a selection over a larger dataset. Have you tried to make a copy of the adata object? adata = Mouse10Xdata[NewIndex3,:].copy(). I ask in case the categories reported in the selection that you are doing create a mismatch, which may be solved by doing a copy. . > On 15 May 2019, at 11:24, brianpenghe <notifications@github.com> wrote:. > . > There was a similar report not so long ago but I have not been able to reproduce the issue. Seems to be related with the matplotlib version used. Maybe you can try to update to the latest version. > â€¦. > On 10 May 2019, at 01:38, brianpenghe @.***> wrote: I ran this: ax=sc.pl.heatmap(Mouse10Xdata[NewIndex3,:], sorted_unique_marker_genes, groupby='ident', use_raw=False, vmin=-3, vmax=3, cmap='bwr',show=True,swap_axes=True,figsize=(10,7), var_group_rotation=0, dendrogram=True, save='ClusterMap.png') But the image has something weird. Here are the snapshot: The lines don't align well with the heatmap. Additionally, the lines don't align well with the group ID colors, either And the ID colors seem to be not aligned well with the heatmap either. Any thoughts? â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub, or mute the thread. > . > I have been using matplotlib 3.0.3 and I believe it's the latest version already. My Scanpy version was 1.4.1. > Can it be an issue about duplicated gene names/make unique? > . > â€”. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:4,deployability,updat,update,4,Any update?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:4,safety,updat,update,4,Any update?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:4,security,updat,update,4,Any update?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:6,deployability,updat,update,6,> Any update? I tried that but it's still giving me exactly the same tree : (,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:6,safety,updat,update,6,> Any update? I tried that but it's still giving me exactly the same tree : (,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:6,security,updat,update,6,> Any update? I tried that but it's still giving me exactly the same tree : (,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:29,energy efficiency,heat,heatmap,29,In the tests that I have the heatmap seems to be ok. See https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html#Visualize-marker-genes-using-heatmap. Do you think that the problem occurs when lot of cells/genes are plotted?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:166,energy efficiency,heat,heatmap,166,In the tests that I have the heatmap seems to be ok. See https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html#Visualize-marker-genes-using-heatmap. Do you think that the problem occurs when lot of cells/genes are plotted?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:7,safety,test,tests,7,In the tests that I have the heatmap seems to be ok. See https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html#Visualize-marker-genes-using-heatmap. Do you think that the problem occurs when lot of cells/genes are plotted?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:7,testability,test,tests,7,In the tests that I have the heatmap seems to be ok. See https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html#Visualize-marker-genes-using-heatmap. Do you think that the problem occurs when lot of cells/genes are plotted?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:107,usability,visual,visualizing-marker-genes,107,In the tests that I have the heatmap seems to be ok. See https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html#Visualize-marker-genes-using-heatmap. Do you think that the problem occurs when lot of cells/genes are plotted?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:137,usability,Visual,Visualize-marker-genes-using-heatmap,137,In the tests that I have the heatmap seems to be ok. See https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html#Visualize-marker-genes-using-heatmap. Do you think that the problem occurs when lot of cells/genes are plotted?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:31,energy efficiency,heat,heatmap,31,> In the tests that I have the heatmap seems to be ok. See https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html#Visualize-marker-genes-using-heatmap. > . > Do you think that the problem occurs when lot of cells/genes are plotted? It's possible.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:168,energy efficiency,heat,heatmap,168,> In the tests that I have the heatmap seems to be ok. See https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html#Visualize-marker-genes-using-heatmap. > . > Do you think that the problem occurs when lot of cells/genes are plotted? It's possible.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:9,safety,test,tests,9,> In the tests that I have the heatmap seems to be ok. See https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html#Visualize-marker-genes-using-heatmap. > . > Do you think that the problem occurs when lot of cells/genes are plotted? It's possible.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:9,testability,test,tests,9,> In the tests that I have the heatmap seems to be ok. See https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html#Visualize-marker-genes-using-heatmap. > . > Do you think that the problem occurs when lot of cells/genes are plotted? It's possible.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:109,usability,visual,visualizing-marker-genes,109,> In the tests that I have the heatmap seems to be ok. See https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html#Visualize-marker-genes-using-heatmap. > . > Do you think that the problem occurs when lot of cells/genes are plotted? It's possible.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:139,usability,Visual,Visualize-marker-genes-using-heatmap,139,> In the tests that I have the heatmap seems to be ok. See https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html#Visualize-marker-genes-using-heatmap. > . > Do you think that the problem occurs when lot of cells/genes are plotted? It's possible.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:59,energy efficiency,heat,heatmap,59,"The test is not good neither. If you look carefully at the heatmap generated with sc.pl.rank_genes_groups_heatmap(pbmc, n_genes=3, standard_scale='var'), you would find it is not aligned perfectly, especially for the local magnified heatmap. @fidelram @brianpenghe . ![Figure_1](https://user-images.githubusercontent.com/29703450/62337090-c4a32180-b505-11e9-8757-73bc248d7754.png). ![1](https://user-images.githubusercontent.com/29703450/62337095-c66ce500-b505-11e9-8391-74b7b9948ab2.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:233,energy efficiency,heat,heatmap,233,"The test is not good neither. If you look carefully at the heatmap generated with sc.pl.rank_genes_groups_heatmap(pbmc, n_genes=3, standard_scale='var'), you would find it is not aligned perfectly, especially for the local magnified heatmap. @fidelram @brianpenghe . ![Figure_1](https://user-images.githubusercontent.com/29703450/62337090-c4a32180-b505-11e9-8757-73bc248d7754.png). ![1](https://user-images.githubusercontent.com/29703450/62337095-c66ce500-b505-11e9-8391-74b7b9948ab2.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:4,safety,test,test,4,"The test is not good neither. If you look carefully at the heatmap generated with sc.pl.rank_genes_groups_heatmap(pbmc, n_genes=3, standard_scale='var'), you would find it is not aligned perfectly, especially for the local magnified heatmap. @fidelram @brianpenghe . ![Figure_1](https://user-images.githubusercontent.com/29703450/62337090-c4a32180-b505-11e9-8757-73bc248d7754.png). ![1](https://user-images.githubusercontent.com/29703450/62337095-c66ce500-b505-11e9-8391-74b7b9948ab2.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:4,testability,test,test,4,"The test is not good neither. If you look carefully at the heatmap generated with sc.pl.rank_genes_groups_heatmap(pbmc, n_genes=3, standard_scale='var'), you would find it is not aligned perfectly, especially for the local magnified heatmap. @fidelram @brianpenghe . ![Figure_1](https://user-images.githubusercontent.com/29703450/62337090-c4a32180-b505-11e9-8757-73bc248d7754.png). ![1](https://user-images.githubusercontent.com/29703450/62337095-c66ce500-b505-11e9-8391-74b7b9948ab2.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:287,usability,user,user-images,287,"The test is not good neither. If you look carefully at the heatmap generated with sc.pl.rank_genes_groups_heatmap(pbmc, n_genes=3, standard_scale='var'), you would find it is not aligned perfectly, especially for the local magnified heatmap. @fidelram @brianpenghe . ![Figure_1](https://user-images.githubusercontent.com/29703450/62337090-c4a32180-b505-11e9-8757-73bc248d7754.png). ![1](https://user-images.githubusercontent.com/29703450/62337095-c66ce500-b505-11e9-8391-74b7b9948ab2.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/637:395,usability,user,user-images,395,"The test is not good neither. If you look carefully at the heatmap generated with sc.pl.rank_genes_groups_heatmap(pbmc, n_genes=3, standard_scale='var'), you would find it is not aligned perfectly, especially for the local magnified heatmap. @fidelram @brianpenghe . ![Figure_1](https://user-images.githubusercontent.com/29703450/62337090-c4a32180-b505-11e9-8757-73bc248d7754.png). ![1](https://user-images.githubusercontent.com/29703450/62337095-c66ce500-b505-11e9-8391-74b7b9948ab2.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/637
https://github.com/scverse/scanpy/issues/638:371,energy efficiency,draw,drawing,371,"Hi @bioguy2018, if I understand you correctly a similar question was asked [here before](https://github.com/theislab/scanpy/issues/532). To sum it up you would need:. ````. # calculate score: option 1. sc.tl.score_genes(adata, gene_list, score_name='my_score') . # option 2. adata.obs['my_score'] = adata.X[:,my_genes].sum(1) # as a more common alternative use mean(). # drawing graph. sc.pl.umap(adata, color='my_score'). .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/638
https://github.com/scverse/scanpy/issues/638:21,testability,understand,understand,21,"Hi @bioguy2018, if I understand you correctly a similar question was asked [here before](https://github.com/theislab/scanpy/issues/532). To sum it up you would need:. ````. # calculate score: option 1. sc.tl.score_genes(adata, gene_list, score_name='my_score') . # option 2. adata.obs['my_score'] = adata.X[:,my_genes].sum(1) # as a more common alternative use mean(). # drawing graph. sc.pl.umap(adata, color='my_score'). .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/638
https://github.com/scverse/scanpy/issues/638:217,reliability,doe,does,217,"Hi, thanks for the feedback! Another interesting option is [AUCell](https://github.com/aertslab/pySCENIC/blob/master/src/pyscenic/aucell.py) from the [SCENIC workflow](https://www.nature.com/articles/nmeth.4463) that does the comparison based on ranked gene expression - haven't tried it myself though. Best wishes",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/638
https://github.com/scverse/scanpy/issues/638:19,usability,feedback,feedback,19,"Hi, thanks for the feedback! Another interesting option is [AUCell](https://github.com/aertslab/pySCENIC/blob/master/src/pyscenic/aucell.py) from the [SCENIC workflow](https://www.nature.com/articles/nmeth.4463) that does the comparison based on ranked gene expression - haven't tried it myself though. Best wishes",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/638
https://github.com/scverse/scanpy/issues/638:158,usability,workflow,workflow,158,"Hi, thanks for the feedback! Another interesting option is [AUCell](https://github.com/aertslab/pySCENIC/blob/master/src/pyscenic/aucell.py) from the [SCENIC workflow](https://www.nature.com/articles/nmeth.4463) that does the comparison based on ranked gene expression - haven't tried it myself though. Best wishes",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/638
https://github.com/scverse/scanpy/issues/638:64,usability,experien,experience,64,@tilofreiwald Thanks a lot! I will try it out and will write my experience here soon!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/638
https://github.com/scverse/scanpy/issues/639:112,availability,error,error,112,"Hi,. Can I ask if you are using the data from the tutorial? If not, then there are a a few potential sources of error here:. 1. You may have not aligned against the mitochondrial genome (Cell Ranger versions < 2.0 don't do this). 2. You may have mouse data, in which case it should be lower case `mt-` instead of `MT-`. 3. Your data may have been pre-processed to take out mitochondrial genes. I recall looking through quite a few datasets where there were really no mitochondrial genes. Scanpy doesn't automatically filter out mitochondrial genes. You could also check if you have any mitochondrial genes by just outputting this line: `adata.var_names.str.startswith('MT-')`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/639
https://github.com/scverse/scanpy/issues/639:199,deployability,version,versions,199,"Hi,. Can I ask if you are using the data from the tutorial? If not, then there are a a few potential sources of error here:. 1. You may have not aligned against the mitochondrial genome (Cell Ranger versions < 2.0 don't do this). 2. You may have mouse data, in which case it should be lower case `mt-` instead of `MT-`. 3. Your data may have been pre-processed to take out mitochondrial genes. I recall looking through quite a few datasets where there were really no mitochondrial genes. Scanpy doesn't automatically filter out mitochondrial genes. You could also check if you have any mitochondrial genes by just outputting this line: `adata.var_names.str.startswith('MT-')`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/639
https://github.com/scverse/scanpy/issues/639:503,deployability,automat,automatically,503,"Hi,. Can I ask if you are using the data from the tutorial? If not, then there are a a few potential sources of error here:. 1. You may have not aligned against the mitochondrial genome (Cell Ranger versions < 2.0 don't do this). 2. You may have mouse data, in which case it should be lower case `mt-` instead of `MT-`. 3. Your data may have been pre-processed to take out mitochondrial genes. I recall looking through quite a few datasets where there were really no mitochondrial genes. Scanpy doesn't automatically filter out mitochondrial genes. You could also check if you have any mitochondrial genes by just outputting this line: `adata.var_names.str.startswith('MT-')`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/639
https://github.com/scverse/scanpy/issues/639:199,integrability,version,versions,199,"Hi,. Can I ask if you are using the data from the tutorial? If not, then there are a a few potential sources of error here:. 1. You may have not aligned against the mitochondrial genome (Cell Ranger versions < 2.0 don't do this). 2. You may have mouse data, in which case it should be lower case `mt-` instead of `MT-`. 3. Your data may have been pre-processed to take out mitochondrial genes. I recall looking through quite a few datasets where there were really no mitochondrial genes. Scanpy doesn't automatically filter out mitochondrial genes. You could also check if you have any mitochondrial genes by just outputting this line: `adata.var_names.str.startswith('MT-')`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/639
https://github.com/scverse/scanpy/issues/639:517,integrability,filter,filter,517,"Hi,. Can I ask if you are using the data from the tutorial? If not, then there are a a few potential sources of error here:. 1. You may have not aligned against the mitochondrial genome (Cell Ranger versions < 2.0 don't do this). 2. You may have mouse data, in which case it should be lower case `mt-` instead of `MT-`. 3. Your data may have been pre-processed to take out mitochondrial genes. I recall looking through quite a few datasets where there were really no mitochondrial genes. Scanpy doesn't automatically filter out mitochondrial genes. You could also check if you have any mitochondrial genes by just outputting this line: `adata.var_names.str.startswith('MT-')`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/639
https://github.com/scverse/scanpy/issues/639:199,modifiability,version,versions,199,"Hi,. Can I ask if you are using the data from the tutorial? If not, then there are a a few potential sources of error here:. 1. You may have not aligned against the mitochondrial genome (Cell Ranger versions < 2.0 don't do this). 2. You may have mouse data, in which case it should be lower case `mt-` instead of `MT-`. 3. Your data may have been pre-processed to take out mitochondrial genes. I recall looking through quite a few datasets where there were really no mitochondrial genes. Scanpy doesn't automatically filter out mitochondrial genes. You could also check if you have any mitochondrial genes by just outputting this line: `adata.var_names.str.startswith('MT-')`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/639
https://github.com/scverse/scanpy/issues/639:112,performance,error,error,112,"Hi,. Can I ask if you are using the data from the tutorial? If not, then there are a a few potential sources of error here:. 1. You may have not aligned against the mitochondrial genome (Cell Ranger versions < 2.0 don't do this). 2. You may have mouse data, in which case it should be lower case `mt-` instead of `MT-`. 3. Your data may have been pre-processed to take out mitochondrial genes. I recall looking through quite a few datasets where there were really no mitochondrial genes. Scanpy doesn't automatically filter out mitochondrial genes. You could also check if you have any mitochondrial genes by just outputting this line: `adata.var_names.str.startswith('MT-')`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/639
https://github.com/scverse/scanpy/issues/639:495,reliability,doe,doesn,495,"Hi,. Can I ask if you are using the data from the tutorial? If not, then there are a a few potential sources of error here:. 1. You may have not aligned against the mitochondrial genome (Cell Ranger versions < 2.0 don't do this). 2. You may have mouse data, in which case it should be lower case `mt-` instead of `MT-`. 3. Your data may have been pre-processed to take out mitochondrial genes. I recall looking through quite a few datasets where there were really no mitochondrial genes. Scanpy doesn't automatically filter out mitochondrial genes. You could also check if you have any mitochondrial genes by just outputting this line: `adata.var_names.str.startswith('MT-')`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/639
https://github.com/scverse/scanpy/issues/639:112,safety,error,error,112,"Hi,. Can I ask if you are using the data from the tutorial? If not, then there are a a few potential sources of error here:. 1. You may have not aligned against the mitochondrial genome (Cell Ranger versions < 2.0 don't do this). 2. You may have mouse data, in which case it should be lower case `mt-` instead of `MT-`. 3. Your data may have been pre-processed to take out mitochondrial genes. I recall looking through quite a few datasets where there were really no mitochondrial genes. Scanpy doesn't automatically filter out mitochondrial genes. You could also check if you have any mitochondrial genes by just outputting this line: `adata.var_names.str.startswith('MT-')`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/639
https://github.com/scverse/scanpy/issues/639:503,testability,automat,automatically,503,"Hi,. Can I ask if you are using the data from the tutorial? If not, then there are a a few potential sources of error here:. 1. You may have not aligned against the mitochondrial genome (Cell Ranger versions < 2.0 don't do this). 2. You may have mouse data, in which case it should be lower case `mt-` instead of `MT-`. 3. Your data may have been pre-processed to take out mitochondrial genes. I recall looking through quite a few datasets where there were really no mitochondrial genes. Scanpy doesn't automatically filter out mitochondrial genes. You could also check if you have any mitochondrial genes by just outputting this line: `adata.var_names.str.startswith('MT-')`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/639
https://github.com/scverse/scanpy/issues/639:112,usability,error,error,112,"Hi,. Can I ask if you are using the data from the tutorial? If not, then there are a a few potential sources of error here:. 1. You may have not aligned against the mitochondrial genome (Cell Ranger versions < 2.0 don't do this). 2. You may have mouse data, in which case it should be lower case `mt-` instead of `MT-`. 3. Your data may have been pre-processed to take out mitochondrial genes. I recall looking through quite a few datasets where there were really no mitochondrial genes. Scanpy doesn't automatically filter out mitochondrial genes. You could also check if you have any mitochondrial genes by just outputting this line: `adata.var_names.str.startswith('MT-')`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/639
https://github.com/scverse/scanpy/issues/639:246,usability,mous,mouse,246,"Hi,. Can I ask if you are using the data from the tutorial? If not, then there are a a few potential sources of error here:. 1. You may have not aligned against the mitochondrial genome (Cell Ranger versions < 2.0 don't do this). 2. You may have mouse data, in which case it should be lower case `mt-` instead of `MT-`. 3. Your data may have been pre-processed to take out mitochondrial genes. I recall looking through quite a few datasets where there were really no mitochondrial genes. Scanpy doesn't automatically filter out mitochondrial genes. You could also check if you have any mitochondrial genes by just outputting this line: `adata.var_names.str.startswith('MT-')`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/639
https://github.com/scverse/scanpy/issues/639:120,availability,error,error,120,"> Hi,. > . > Can I ask if you are using the data from the tutorial? If not, then there are a a few potential sources of error here:. > . > 1. You may have not aligned against the mitochondrial genome (Cell Ranger versions < 2.0 don't do this). > 2. You may have mouse data, in which case it should be lower case `mt-` instead of `MT-`. > 3. Your data may have been pre-processed to take out mitochondrial genes. > . > I recall looking through quite a few datasets where there were really no mitochondrial genes. Scanpy doesn't automatically filter out mitochondrial genes. You could also check if you have any mitochondrial genes by just outputting this line: `adata.var_names.str.startswith('MT-')`. It really helps! Thanks very much!(by the way I am using zebrafish data and it still should be lower case, but I was just not aware of this)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/639
https://github.com/scverse/scanpy/issues/639:213,deployability,version,versions,213,"> Hi,. > . > Can I ask if you are using the data from the tutorial? If not, then there are a a few potential sources of error here:. > . > 1. You may have not aligned against the mitochondrial genome (Cell Ranger versions < 2.0 don't do this). > 2. You may have mouse data, in which case it should be lower case `mt-` instead of `MT-`. > 3. Your data may have been pre-processed to take out mitochondrial genes. > . > I recall looking through quite a few datasets where there were really no mitochondrial genes. Scanpy doesn't automatically filter out mitochondrial genes. You could also check if you have any mitochondrial genes by just outputting this line: `adata.var_names.str.startswith('MT-')`. It really helps! Thanks very much!(by the way I am using zebrafish data and it still should be lower case, but I was just not aware of this)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/639
https://github.com/scverse/scanpy/issues/639:527,deployability,automat,automatically,527,"> Hi,. > . > Can I ask if you are using the data from the tutorial? If not, then there are a a few potential sources of error here:. > . > 1. You may have not aligned against the mitochondrial genome (Cell Ranger versions < 2.0 don't do this). > 2. You may have mouse data, in which case it should be lower case `mt-` instead of `MT-`. > 3. Your data may have been pre-processed to take out mitochondrial genes. > . > I recall looking through quite a few datasets where there were really no mitochondrial genes. Scanpy doesn't automatically filter out mitochondrial genes. You could also check if you have any mitochondrial genes by just outputting this line: `adata.var_names.str.startswith('MT-')`. It really helps! Thanks very much!(by the way I am using zebrafish data and it still should be lower case, but I was just not aware of this)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/639
https://github.com/scverse/scanpy/issues/639:213,integrability,version,versions,213,"> Hi,. > . > Can I ask if you are using the data from the tutorial? If not, then there are a a few potential sources of error here:. > . > 1. You may have not aligned against the mitochondrial genome (Cell Ranger versions < 2.0 don't do this). > 2. You may have mouse data, in which case it should be lower case `mt-` instead of `MT-`. > 3. Your data may have been pre-processed to take out mitochondrial genes. > . > I recall looking through quite a few datasets where there were really no mitochondrial genes. Scanpy doesn't automatically filter out mitochondrial genes. You could also check if you have any mitochondrial genes by just outputting this line: `adata.var_names.str.startswith('MT-')`. It really helps! Thanks very much!(by the way I am using zebrafish data and it still should be lower case, but I was just not aware of this)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/639
https://github.com/scverse/scanpy/issues/639:541,integrability,filter,filter,541,"> Hi,. > . > Can I ask if you are using the data from the tutorial? If not, then there are a a few potential sources of error here:. > . > 1. You may have not aligned against the mitochondrial genome (Cell Ranger versions < 2.0 don't do this). > 2. You may have mouse data, in which case it should be lower case `mt-` instead of `MT-`. > 3. Your data may have been pre-processed to take out mitochondrial genes. > . > I recall looking through quite a few datasets where there were really no mitochondrial genes. Scanpy doesn't automatically filter out mitochondrial genes. You could also check if you have any mitochondrial genes by just outputting this line: `adata.var_names.str.startswith('MT-')`. It really helps! Thanks very much!(by the way I am using zebrafish data and it still should be lower case, but I was just not aware of this)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/639
https://github.com/scverse/scanpy/issues/639:213,modifiability,version,versions,213,"> Hi,. > . > Can I ask if you are using the data from the tutorial? If not, then there are a a few potential sources of error here:. > . > 1. You may have not aligned against the mitochondrial genome (Cell Ranger versions < 2.0 don't do this). > 2. You may have mouse data, in which case it should be lower case `mt-` instead of `MT-`. > 3. Your data may have been pre-processed to take out mitochondrial genes. > . > I recall looking through quite a few datasets where there were really no mitochondrial genes. Scanpy doesn't automatically filter out mitochondrial genes. You could also check if you have any mitochondrial genes by just outputting this line: `adata.var_names.str.startswith('MT-')`. It really helps! Thanks very much!(by the way I am using zebrafish data and it still should be lower case, but I was just not aware of this)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/639
https://github.com/scverse/scanpy/issues/639:120,performance,error,error,120,"> Hi,. > . > Can I ask if you are using the data from the tutorial? If not, then there are a a few potential sources of error here:. > . > 1. You may have not aligned against the mitochondrial genome (Cell Ranger versions < 2.0 don't do this). > 2. You may have mouse data, in which case it should be lower case `mt-` instead of `MT-`. > 3. Your data may have been pre-processed to take out mitochondrial genes. > . > I recall looking through quite a few datasets where there were really no mitochondrial genes. Scanpy doesn't automatically filter out mitochondrial genes. You could also check if you have any mitochondrial genes by just outputting this line: `adata.var_names.str.startswith('MT-')`. It really helps! Thanks very much!(by the way I am using zebrafish data and it still should be lower case, but I was just not aware of this)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/639
https://github.com/scverse/scanpy/issues/639:519,reliability,doe,doesn,519,"> Hi,. > . > Can I ask if you are using the data from the tutorial? If not, then there are a a few potential sources of error here:. > . > 1. You may have not aligned against the mitochondrial genome (Cell Ranger versions < 2.0 don't do this). > 2. You may have mouse data, in which case it should be lower case `mt-` instead of `MT-`. > 3. Your data may have been pre-processed to take out mitochondrial genes. > . > I recall looking through quite a few datasets where there were really no mitochondrial genes. Scanpy doesn't automatically filter out mitochondrial genes. You could also check if you have any mitochondrial genes by just outputting this line: `adata.var_names.str.startswith('MT-')`. It really helps! Thanks very much!(by the way I am using zebrafish data and it still should be lower case, but I was just not aware of this)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/639
https://github.com/scverse/scanpy/issues/639:120,safety,error,error,120,"> Hi,. > . > Can I ask if you are using the data from the tutorial? If not, then there are a a few potential sources of error here:. > . > 1. You may have not aligned against the mitochondrial genome (Cell Ranger versions < 2.0 don't do this). > 2. You may have mouse data, in which case it should be lower case `mt-` instead of `MT-`. > 3. Your data may have been pre-processed to take out mitochondrial genes. > . > I recall looking through quite a few datasets where there were really no mitochondrial genes. Scanpy doesn't automatically filter out mitochondrial genes. You could also check if you have any mitochondrial genes by just outputting this line: `adata.var_names.str.startswith('MT-')`. It really helps! Thanks very much!(by the way I am using zebrafish data and it still should be lower case, but I was just not aware of this)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/639
https://github.com/scverse/scanpy/issues/639:527,testability,automat,automatically,527,"> Hi,. > . > Can I ask if you are using the data from the tutorial? If not, then there are a a few potential sources of error here:. > . > 1. You may have not aligned against the mitochondrial genome (Cell Ranger versions < 2.0 don't do this). > 2. You may have mouse data, in which case it should be lower case `mt-` instead of `MT-`. > 3. Your data may have been pre-processed to take out mitochondrial genes. > . > I recall looking through quite a few datasets where there were really no mitochondrial genes. Scanpy doesn't automatically filter out mitochondrial genes. You could also check if you have any mitochondrial genes by just outputting this line: `adata.var_names.str.startswith('MT-')`. It really helps! Thanks very much!(by the way I am using zebrafish data and it still should be lower case, but I was just not aware of this)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/639
https://github.com/scverse/scanpy/issues/639:120,usability,error,error,120,"> Hi,. > . > Can I ask if you are using the data from the tutorial? If not, then there are a a few potential sources of error here:. > . > 1. You may have not aligned against the mitochondrial genome (Cell Ranger versions < 2.0 don't do this). > 2. You may have mouse data, in which case it should be lower case `mt-` instead of `MT-`. > 3. Your data may have been pre-processed to take out mitochondrial genes. > . > I recall looking through quite a few datasets where there were really no mitochondrial genes. Scanpy doesn't automatically filter out mitochondrial genes. You could also check if you have any mitochondrial genes by just outputting this line: `adata.var_names.str.startswith('MT-')`. It really helps! Thanks very much!(by the way I am using zebrafish data and it still should be lower case, but I was just not aware of this)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/639
https://github.com/scverse/scanpy/issues/639:262,usability,mous,mouse,262,"> Hi,. > . > Can I ask if you are using the data from the tutorial? If not, then there are a a few potential sources of error here:. > . > 1. You may have not aligned against the mitochondrial genome (Cell Ranger versions < 2.0 don't do this). > 2. You may have mouse data, in which case it should be lower case `mt-` instead of `MT-`. > 3. Your data may have been pre-processed to take out mitochondrial genes. > . > I recall looking through quite a few datasets where there were really no mitochondrial genes. Scanpy doesn't automatically filter out mitochondrial genes. You could also check if you have any mitochondrial genes by just outputting this line: `adata.var_names.str.startswith('MT-')`. It really helps! Thanks very much!(by the way I am using zebrafish data and it still should be lower case, but I was just not aware of this)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/639
https://github.com/scverse/scanpy/issues/639:711,usability,help,helps,711,"> Hi,. > . > Can I ask if you are using the data from the tutorial? If not, then there are a a few potential sources of error here:. > . > 1. You may have not aligned against the mitochondrial genome (Cell Ranger versions < 2.0 don't do this). > 2. You may have mouse data, in which case it should be lower case `mt-` instead of `MT-`. > 3. Your data may have been pre-processed to take out mitochondrial genes. > . > I recall looking through quite a few datasets where there were really no mitochondrial genes. Scanpy doesn't automatically filter out mitochondrial genes. You could also check if you have any mitochondrial genes by just outputting this line: `adata.var_names.str.startswith('MT-')`. It really helps! Thanks very much!(by the way I am using zebrafish data and it still should be lower case, but I was just not aware of this)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/639
https://github.com/scverse/scanpy/pull/640:43,deployability,fail,failing,43,"@gokceneraslan Do you know why the test is failing? Did you change some of the defaults? I like the change, is really an improvement.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/640
https://github.com/scverse/scanpy/pull/640:43,reliability,fail,failing,43,"@gokceneraslan Do you know why the test is failing? Did you change some of the defaults? I like the change, is really an improvement.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/640
https://github.com/scverse/scanpy/pull/640:35,safety,test,test,35,"@gokceneraslan Do you know why the test is failing? Did you change some of the defaults? I like the change, is really an improvement.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/640
https://github.com/scverse/scanpy/pull/640:35,testability,test,test,35,"@gokceneraslan Do you know why the test is failing? Did you change some of the defaults? I like the change, is really an improvement.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/640
https://github.com/scverse/scanpy/pull/640:43,deployability,fail,fails,43,"I didn't change any defaults I think. Test fails in rename_category of pandas in pbmc, not sure it's related.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/640
https://github.com/scverse/scanpy/pull/640:43,reliability,fail,fails,43,"I didn't change any defaults I think. Test fails in rename_category of pandas in pbmc, not sure it's related.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/640
https://github.com/scverse/scanpy/pull/640:38,safety,Test,Test,38,"I didn't change any defaults I think. Test fails in rename_category of pandas in pbmc, not sure it's related.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/640
https://github.com/scverse/scanpy/pull/640:38,testability,Test,Test,38,"I didn't change any defaults I think. Test fails in rename_category of pandas in pbmc, not sure it's related.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/640
https://github.com/scverse/scanpy/issues/641:277,availability,error,error,277,"Hi,. This bug report may be better placed in the single-cell-tutorial repo as you're having issues with how the tools are chained. A few things to check:. - Are you using sparse matrices in your data? - are the size factors you get all real numbers (not `NaN`)? I've seen this error before... just can't remember what the issue was. I think it was sparse matrices.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:277,performance,error,error,277,"Hi,. This bug report may be better placed in the single-cell-tutorial repo as you're having issues with how the tools are chained. A few things to check:. - Are you using sparse matrices in your data? - are the size factors you get all real numbers (not `NaN`)? I've seen this error before... just can't remember what the issue was. I think it was sparse matrices.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:277,safety,error,error,277,"Hi,. This bug report may be better placed in the single-cell-tutorial repo as you're having issues with how the tools are chained. A few things to check:. - Are you using sparse matrices in your data? - are the size factors you get all real numbers (not `NaN`)? I've seen this error before... just can't remember what the issue was. I think it was sparse matrices.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:304,safety,reme,remember,304,"Hi,. This bug report may be better placed in the single-cell-tutorial repo as you're having issues with how the tools are chained. A few things to check:. - Are you using sparse matrices in your data? - are the size factors you get all real numbers (not `NaN`)? I've seen this error before... just can't remember what the issue was. I think it was sparse matrices.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:112,usability,tool,tools,112,"Hi,. This bug report may be better placed in the single-cell-tutorial repo as you're having issues with how the tools are chained. A few things to check:. - Are you using sparse matrices in your data? - are the size factors you get all real numbers (not `NaN`)? I've seen this error before... just can't remember what the issue was. I think it was sparse matrices.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:277,usability,error,error,277,"Hi,. This bug report may be better placed in the single-cell-tutorial repo as you're having issues with how the tools are chained. A few things to check:. - Are you using sparse matrices in your data? - are the size factors you get all real numbers (not `NaN`)? I've seen this error before... just can't remember what the issue was. I think it was sparse matrices.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:19,safety,test,test,19,"Hi,. I just made a test and it seems that it is indeed a problem of non-sparse matrix. How can I render the matrix sparse again from one that is dense? Is there something similar to the `adata.X.todense()` command?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:19,testability,test,test,19,"Hi,. I just made a test and it seems that it is indeed a problem of non-sparse matrix. How can I render the matrix sparse again from one that is dense? Is there something similar to the `adata.X.todense()` command?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:206,usability,command,command,206,"Hi,. I just made a test and it seems that it is indeed a problem of non-sparse matrix. How can I render the matrix sparse again from one that is dense? Is there something similar to the `adata.X.todense()` command?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:46,usability,tool,tool,46,"Hi again,. after normalizing with an external tool I resparsified the matrix as it follows:. `adata.X = scipy.sparse.csr_matrix(adata.X)`. and everything works. Thanks for help. Cheers.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:172,usability,help,help,172,"Hi again,. after normalizing with an external tool I resparsified the matrix as it follows:. `adata.X = scipy.sparse.csr_matrix(adata.X)`. and everything works. Thanks for help. Cheers.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:398,deployability,version,version,398,"So for future reference. `scran` and other `R` tools don't work on sparse matrices. Whenever you need to do batch correction, that doesn't matter anyway, as you'll probably have to densify your matrix for batch correction anyway (or it will be dense after batch correction). Edit: This is referring to the use of `scran` and other `R` tools in the `single-cell-tutorial` case study notebook (up to version 1904). The `R` tools themselves of course work with sparse matrices. Also, the newer version of the tutorial notebook allows for sparse matrix conversion as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:491,deployability,version,version,491,"So for future reference. `scran` and other `R` tools don't work on sparse matrices. Whenever you need to do batch correction, that doesn't matter anyway, as you'll probably have to densify your matrix for batch correction anyway (or it will be dense after batch correction). Edit: This is referring to the use of `scran` and other `R` tools in the `single-cell-tutorial` case study notebook (up to version 1904). The `R` tools themselves of course work with sparse matrices. Also, the newer version of the tutorial notebook allows for sparse matrix conversion as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:108,integrability,batch,batch,108,"So for future reference. `scran` and other `R` tools don't work on sparse matrices. Whenever you need to do batch correction, that doesn't matter anyway, as you'll probably have to densify your matrix for batch correction anyway (or it will be dense after batch correction). Edit: This is referring to the use of `scran` and other `R` tools in the `single-cell-tutorial` case study notebook (up to version 1904). The `R` tools themselves of course work with sparse matrices. Also, the newer version of the tutorial notebook allows for sparse matrix conversion as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:205,integrability,batch,batch,205,"So for future reference. `scran` and other `R` tools don't work on sparse matrices. Whenever you need to do batch correction, that doesn't matter anyway, as you'll probably have to densify your matrix for batch correction anyway (or it will be dense after batch correction). Edit: This is referring to the use of `scran` and other `R` tools in the `single-cell-tutorial` case study notebook (up to version 1904). The `R` tools themselves of course work with sparse matrices. Also, the newer version of the tutorial notebook allows for sparse matrix conversion as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:256,integrability,batch,batch,256,"So for future reference. `scran` and other `R` tools don't work on sparse matrices. Whenever you need to do batch correction, that doesn't matter anyway, as you'll probably have to densify your matrix for batch correction anyway (or it will be dense after batch correction). Edit: This is referring to the use of `scran` and other `R` tools in the `single-cell-tutorial` case study notebook (up to version 1904). The `R` tools themselves of course work with sparse matrices. Also, the newer version of the tutorial notebook allows for sparse matrix conversion as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:398,integrability,version,version,398,"So for future reference. `scran` and other `R` tools don't work on sparse matrices. Whenever you need to do batch correction, that doesn't matter anyway, as you'll probably have to densify your matrix for batch correction anyway (or it will be dense after batch correction). Edit: This is referring to the use of `scran` and other `R` tools in the `single-cell-tutorial` case study notebook (up to version 1904). The `R` tools themselves of course work with sparse matrices. Also, the newer version of the tutorial notebook allows for sparse matrix conversion as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:491,integrability,version,version,491,"So for future reference. `scran` and other `R` tools don't work on sparse matrices. Whenever you need to do batch correction, that doesn't matter anyway, as you'll probably have to densify your matrix for batch correction anyway (or it will be dense after batch correction). Edit: This is referring to the use of `scran` and other `R` tools in the `single-cell-tutorial` case study notebook (up to version 1904). The `R` tools themselves of course work with sparse matrices. Also, the newer version of the tutorial notebook allows for sparse matrix conversion as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:549,interoperability,convers,conversion,549,"So for future reference. `scran` and other `R` tools don't work on sparse matrices. Whenever you need to do batch correction, that doesn't matter anyway, as you'll probably have to densify your matrix for batch correction anyway (or it will be dense after batch correction). Edit: This is referring to the use of `scran` and other `R` tools in the `single-cell-tutorial` case study notebook (up to version 1904). The `R` tools themselves of course work with sparse matrices. Also, the newer version of the tutorial notebook allows for sparse matrix conversion as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:398,modifiability,version,version,398,"So for future reference. `scran` and other `R` tools don't work on sparse matrices. Whenever you need to do batch correction, that doesn't matter anyway, as you'll probably have to densify your matrix for batch correction anyway (or it will be dense after batch correction). Edit: This is referring to the use of `scran` and other `R` tools in the `single-cell-tutorial` case study notebook (up to version 1904). The `R` tools themselves of course work with sparse matrices. Also, the newer version of the tutorial notebook allows for sparse matrix conversion as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:491,modifiability,version,version,491,"So for future reference. `scran` and other `R` tools don't work on sparse matrices. Whenever you need to do batch correction, that doesn't matter anyway, as you'll probably have to densify your matrix for batch correction anyway (or it will be dense after batch correction). Edit: This is referring to the use of `scran` and other `R` tools in the `single-cell-tutorial` case study notebook (up to version 1904). The `R` tools themselves of course work with sparse matrices. Also, the newer version of the tutorial notebook allows for sparse matrix conversion as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:108,performance,batch,batch,108,"So for future reference. `scran` and other `R` tools don't work on sparse matrices. Whenever you need to do batch correction, that doesn't matter anyway, as you'll probably have to densify your matrix for batch correction anyway (or it will be dense after batch correction). Edit: This is referring to the use of `scran` and other `R` tools in the `single-cell-tutorial` case study notebook (up to version 1904). The `R` tools themselves of course work with sparse matrices. Also, the newer version of the tutorial notebook allows for sparse matrix conversion as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:205,performance,batch,batch,205,"So for future reference. `scran` and other `R` tools don't work on sparse matrices. Whenever you need to do batch correction, that doesn't matter anyway, as you'll probably have to densify your matrix for batch correction anyway (or it will be dense after batch correction). Edit: This is referring to the use of `scran` and other `R` tools in the `single-cell-tutorial` case study notebook (up to version 1904). The `R` tools themselves of course work with sparse matrices. Also, the newer version of the tutorial notebook allows for sparse matrix conversion as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:256,performance,batch,batch,256,"So for future reference. `scran` and other `R` tools don't work on sparse matrices. Whenever you need to do batch correction, that doesn't matter anyway, as you'll probably have to densify your matrix for batch correction anyway (or it will be dense after batch correction). Edit: This is referring to the use of `scran` and other `R` tools in the `single-cell-tutorial` case study notebook (up to version 1904). The `R` tools themselves of course work with sparse matrices. Also, the newer version of the tutorial notebook allows for sparse matrix conversion as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:131,reliability,doe,doesn,131,"So for future reference. `scran` and other `R` tools don't work on sparse matrices. Whenever you need to do batch correction, that doesn't matter anyway, as you'll probably have to densify your matrix for batch correction anyway (or it will be dense after batch correction). Edit: This is referring to the use of `scran` and other `R` tools in the `single-cell-tutorial` case study notebook (up to version 1904). The `R` tools themselves of course work with sparse matrices. Also, the newer version of the tutorial notebook allows for sparse matrix conversion as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:47,usability,tool,tools,47,"So for future reference. `scran` and other `R` tools don't work on sparse matrices. Whenever you need to do batch correction, that doesn't matter anyway, as you'll probably have to densify your matrix for batch correction anyway (or it will be dense after batch correction). Edit: This is referring to the use of `scran` and other `R` tools in the `single-cell-tutorial` case study notebook (up to version 1904). The `R` tools themselves of course work with sparse matrices. Also, the newer version of the tutorial notebook allows for sparse matrix conversion as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:335,usability,tool,tools,335,"So for future reference. `scran` and other `R` tools don't work on sparse matrices. Whenever you need to do batch correction, that doesn't matter anyway, as you'll probably have to densify your matrix for batch correction anyway (or it will be dense after batch correction). Edit: This is referring to the use of `scran` and other `R` tools in the `single-cell-tutorial` case study notebook (up to version 1904). The `R` tools themselves of course work with sparse matrices. Also, the newer version of the tutorial notebook allows for sparse matrix conversion as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:421,usability,tool,tools,421,"So for future reference. `scran` and other `R` tools don't work on sparse matrices. Whenever you need to do batch correction, that doesn't matter anyway, as you'll probably have to densify your matrix for batch correction anyway (or it will be dense after batch correction). Edit: This is referring to the use of `scran` and other `R` tools in the `single-cell-tutorial` case study notebook (up to version 1904). The `R` tools themselves of course work with sparse matrices. Also, the newer version of the tutorial notebook allows for sparse matrix conversion as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:47,usability,tool,tools,47,"scran works on sparse stuff, maybe not all its tools yet",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:128,deployability,version,version,128,The above comment was referring to the old `rpy2`/`anndata2ri` implementation of the `single-cell-tutorial` notebook. The newer version of the notebook (Version 1906) indeed does allow for sparse matrix conversion into `SingleCellExperiment` objects.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:153,deployability,Version,Version,153,The above comment was referring to the old `rpy2`/`anndata2ri` implementation of the `single-cell-tutorial` notebook. The newer version of the notebook (Version 1906) indeed does allow for sparse matrix conversion into `SingleCellExperiment` objects.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:128,integrability,version,version,128,The above comment was referring to the old `rpy2`/`anndata2ri` implementation of the `single-cell-tutorial` notebook. The newer version of the notebook (Version 1906) indeed does allow for sparse matrix conversion into `SingleCellExperiment` objects.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:153,integrability,Version,Version,153,The above comment was referring to the old `rpy2`/`anndata2ri` implementation of the `single-cell-tutorial` notebook. The newer version of the notebook (Version 1906) indeed does allow for sparse matrix conversion into `SingleCellExperiment` objects.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:203,interoperability,convers,conversion,203,The above comment was referring to the old `rpy2`/`anndata2ri` implementation of the `single-cell-tutorial` notebook. The newer version of the notebook (Version 1906) indeed does allow for sparse matrix conversion into `SingleCellExperiment` objects.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:128,modifiability,version,version,128,The above comment was referring to the old `rpy2`/`anndata2ri` implementation of the `single-cell-tutorial` notebook. The newer version of the notebook (Version 1906) indeed does allow for sparse matrix conversion into `SingleCellExperiment` objects.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:153,modifiability,Version,Version,153,The above comment was referring to the old `rpy2`/`anndata2ri` implementation of the `single-cell-tutorial` notebook. The newer version of the notebook (Version 1906) indeed does allow for sparse matrix conversion into `SingleCellExperiment` objects.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/641:174,reliability,doe,does,174,The above comment was referring to the old `rpy2`/`anndata2ri` implementation of the `single-cell-tutorial` notebook. The newer version of the notebook (Version 1906) indeed does allow for sparse matrix conversion into `SingleCellExperiment` objects.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/641
https://github.com/scverse/scanpy/issues/643:17,deployability,instal,install,17,"Hi, I just tried install a higher version of scipy, and it works now. ```. pip install scipy==1.2.1. ```. Refer to: https://www.scipy.org",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/643
https://github.com/scverse/scanpy/issues/643:34,deployability,version,version,34,"Hi, I just tried install a higher version of scipy, and it works now. ```. pip install scipy==1.2.1. ```. Refer to: https://www.scipy.org",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/643
https://github.com/scverse/scanpy/issues/643:79,deployability,instal,install,79,"Hi, I just tried install a higher version of scipy, and it works now. ```. pip install scipy==1.2.1. ```. Refer to: https://www.scipy.org",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/643
https://github.com/scverse/scanpy/issues/643:34,integrability,version,version,34,"Hi, I just tried install a higher version of scipy, and it works now. ```. pip install scipy==1.2.1. ```. Refer to: https://www.scipy.org",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/643
https://github.com/scverse/scanpy/issues/643:34,modifiability,version,version,34,"Hi, I just tried install a higher version of scipy, and it works now. ```. pip install scipy==1.2.1. ```. Refer to: https://www.scipy.org",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/643
https://github.com/scverse/scanpy/issues/643:84,deployability,version,versions,84,I also ran into this issue when using scanpy==1.4.2 and scipy==1.3.0 (which are the versions installed when I install using conda). Enforcing scipy==1.2.1 in my conda environment file fixed it.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/643
https://github.com/scverse/scanpy/issues/643:93,deployability,instal,installed,93,I also ran into this issue when using scanpy==1.4.2 and scipy==1.3.0 (which are the versions installed when I install using conda). Enforcing scipy==1.2.1 in my conda environment file fixed it.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/643
https://github.com/scverse/scanpy/issues/643:110,deployability,instal,install,110,I also ran into this issue when using scanpy==1.4.2 and scipy==1.3.0 (which are the versions installed when I install using conda). Enforcing scipy==1.2.1 in my conda environment file fixed it.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/643
https://github.com/scverse/scanpy/issues/643:84,integrability,version,versions,84,I also ran into this issue when using scanpy==1.4.2 and scipy==1.3.0 (which are the versions installed when I install using conda). Enforcing scipy==1.2.1 in my conda environment file fixed it.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/643
https://github.com/scverse/scanpy/issues/643:84,modifiability,version,versions,84,I also ran into this issue when using scanpy==1.4.2 and scipy==1.3.0 (which are the versions installed when I install using conda). Enforcing scipy==1.2.1 in my conda environment file fixed it.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/643
https://github.com/scverse/scanpy/issues/643:266,availability,error,error,266,"Hi. I'm struggling with the same issue that is not resolved with the suggestions above (using other scipy version==1.2.1). I have scanpy==1.6.0 and scipy==1.5.2 in my environment. I am using Ubuntu in Windows. And when I import scanpy in jupyter notebook, I get the error:. **`ImportError: cannot import name 'IndexMixin' from 'scipy.sparse.sputils' (/home/levinbioinformatics/anaconda3/envs/scgen-env/lib/python3.7/site-packages/scipy/sparse/sputils.py)`**. Please advise!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/643
https://github.com/scverse/scanpy/issues/643:106,deployability,version,version,106,"Hi. I'm struggling with the same issue that is not resolved with the suggestions above (using other scipy version==1.2.1). I have scanpy==1.6.0 and scipy==1.5.2 in my environment. I am using Ubuntu in Windows. And when I import scanpy in jupyter notebook, I get the error:. **`ImportError: cannot import name 'IndexMixin' from 'scipy.sparse.sputils' (/home/levinbioinformatics/anaconda3/envs/scgen-env/lib/python3.7/site-packages/scipy/sparse/sputils.py)`**. Please advise!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/643
https://github.com/scverse/scanpy/issues/643:106,integrability,version,version,106,"Hi. I'm struggling with the same issue that is not resolved with the suggestions above (using other scipy version==1.2.1). I have scanpy==1.6.0 and scipy==1.5.2 in my environment. I am using Ubuntu in Windows. And when I import scanpy in jupyter notebook, I get the error:. **`ImportError: cannot import name 'IndexMixin' from 'scipy.sparse.sputils' (/home/levinbioinformatics/anaconda3/envs/scgen-env/lib/python3.7/site-packages/scipy/sparse/sputils.py)`**. Please advise!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/643
https://github.com/scverse/scanpy/issues/643:106,modifiability,version,version,106,"Hi. I'm struggling with the same issue that is not resolved with the suggestions above (using other scipy version==1.2.1). I have scanpy==1.6.0 and scipy==1.5.2 in my environment. I am using Ubuntu in Windows. And when I import scanpy in jupyter notebook, I get the error:. **`ImportError: cannot import name 'IndexMixin' from 'scipy.sparse.sputils' (/home/levinbioinformatics/anaconda3/envs/scgen-env/lib/python3.7/site-packages/scipy/sparse/sputils.py)`**. Please advise!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/643
https://github.com/scverse/scanpy/issues/643:421,modifiability,pac,packages,421,"Hi. I'm struggling with the same issue that is not resolved with the suggestions above (using other scipy version==1.2.1). I have scanpy==1.6.0 and scipy==1.5.2 in my environment. I am using Ubuntu in Windows. And when I import scanpy in jupyter notebook, I get the error:. **`ImportError: cannot import name 'IndexMixin' from 'scipy.sparse.sputils' (/home/levinbioinformatics/anaconda3/envs/scgen-env/lib/python3.7/site-packages/scipy/sparse/sputils.py)`**. Please advise!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/643
https://github.com/scverse/scanpy/issues/643:266,performance,error,error,266,"Hi. I'm struggling with the same issue that is not resolved with the suggestions above (using other scipy version==1.2.1). I have scanpy==1.6.0 and scipy==1.5.2 in my environment. I am using Ubuntu in Windows. And when I import scanpy in jupyter notebook, I get the error:. **`ImportError: cannot import name 'IndexMixin' from 'scipy.sparse.sputils' (/home/levinbioinformatics/anaconda3/envs/scgen-env/lib/python3.7/site-packages/scipy/sparse/sputils.py)`**. Please advise!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/643
https://github.com/scverse/scanpy/issues/643:266,safety,error,error,266,"Hi. I'm struggling with the same issue that is not resolved with the suggestions above (using other scipy version==1.2.1). I have scanpy==1.6.0 and scipy==1.5.2 in my environment. I am using Ubuntu in Windows. And when I import scanpy in jupyter notebook, I get the error:. **`ImportError: cannot import name 'IndexMixin' from 'scipy.sparse.sputils' (/home/levinbioinformatics/anaconda3/envs/scgen-env/lib/python3.7/site-packages/scipy/sparse/sputils.py)`**. Please advise!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/643
https://github.com/scverse/scanpy/issues/643:266,usability,error,error,266,"Hi. I'm struggling with the same issue that is not resolved with the suggestions above (using other scipy version==1.2.1). I have scanpy==1.6.0 and scipy==1.5.2 in my environment. I am using Ubuntu in Windows. And when I import scanpy in jupyter notebook, I get the error:. **`ImportError: cannot import name 'IndexMixin' from 'scipy.sparse.sputils' (/home/levinbioinformatics/anaconda3/envs/scgen-env/lib/python3.7/site-packages/scipy/sparse/sputils.py)`**. Please advise!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/643
https://github.com/scverse/scanpy/issues/643:17,availability,Down,Downgrading,17,Same issue here. Downgrading scipy from 1.5.4 to 1.2.1 helps.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/643
https://github.com/scverse/scanpy/issues/643:55,usability,help,helps,55,Same issue here. Downgrading scipy from 1.5.4 to 1.2.1 helps.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/643
https://github.com/scverse/scanpy/pull/644:66,safety,test,tests,66,"> Thereâ€™s a few items Iâ€™d like to see changed, and you should add tests. I have modified your suggestions , thanks. For tests you mean the 'test cases' or some 'example' data to run the program? . Thanks ...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:120,safety,test,tests,120,"> Thereâ€™s a few items Iâ€™d like to see changed, and you should add tests. I have modified your suggestions , thanks. For tests you mean the 'test cases' or some 'example' data to run the program? . Thanks ...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:140,safety,test,test,140,"> Thereâ€™s a few items Iâ€™d like to see changed, and you should add tests. I have modified your suggestions , thanks. For tests you mean the 'test cases' or some 'example' data to run the program? . Thanks ...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:80,security,modif,modified,80,"> Thereâ€™s a few items Iâ€™d like to see changed, and you should add tests. I have modified your suggestions , thanks. For tests you mean the 'test cases' or some 'example' data to run the program? . Thanks ...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:66,testability,test,tests,66,"> Thereâ€™s a few items Iâ€™d like to see changed, and you should add tests. I have modified your suggestions , thanks. For tests you mean the 'test cases' or some 'example' data to run the program? . Thanks ...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:120,testability,test,tests,120,"> Thereâ€™s a few items Iâ€™d like to see changed, and you should add tests. I have modified your suggestions , thanks. For tests you mean the 'test cases' or some 'example' data to run the program? . Thanks ...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:140,testability,test,test,140,"> Thereâ€™s a few items Iâ€™d like to see changed, and you should add tests. I have modified your suggestions , thanks. For tests you mean the 'test cases' or some 'example' data to run the program? . Thanks ...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:75,availability,state,statements,75,Thank you! With â€œtestsâ€ I mean â€œfunctions named `test_*` with `assert ...` statements insideâ€. See here: https://github.com/theislab/scanpy/tree/master/scanpy/tests,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:75,integrability,state,statements,75,Thank you! With â€œtestsâ€ I mean â€œfunctions named `test_*` with `assert ...` statements insideâ€. See here: https://github.com/theislab/scanpy/tree/master/scanpy/tests,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:17,safety,test,tests,17,Thank you! With â€œtestsâ€ I mean â€œfunctions named `test_*` with `assert ...` statements insideâ€. See here: https://github.com/theislab/scanpy/tree/master/scanpy/tests,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:159,safety,test,tests,159,Thank you! With â€œtestsâ€ I mean â€œfunctions named `test_*` with `assert ...` statements insideâ€. See here: https://github.com/theislab/scanpy/tree/master/scanpy/tests,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:17,testability,test,tests,17,Thank you! With â€œtestsâ€ I mean â€œfunctions named `test_*` with `assert ...` statements insideâ€. See here: https://github.com/theislab/scanpy/tree/master/scanpy/tests,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:63,testability,assert,assert,63,Thank you! With â€œtestsâ€ I mean â€œfunctions named `test_*` with `assert ...` statements insideâ€. See here: https://github.com/theislab/scanpy/tree/master/scanpy/tests,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:159,testability,test,tests,159,Thank you! With â€œtestsâ€ I mean â€œfunctions named `test_*` with `assert ...` statements insideâ€. See here: https://github.com/theislab/scanpy/tree/master/scanpy/tests,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:77,availability,state,statements,77,"> Thank you! With â€œtestsâ€ I mean â€œfunctions named `test_*` with `assert ...` statements insideâ€. . Thanks for your guidance, I have added `test_weightedSampling.py` with a folder named `weighted_sampled` in _data folder. . I have updated scanpy for weighted sampling for later tasks (clustering, finding marking genes and plotting). I also suggest to support it for initial tasks like PCA for data where each observation has weight (as in MATLAB). . Regards, . Khalid .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:284,availability,cluster,clustering,284,"> Thank you! With â€œtestsâ€ I mean â€œfunctions named `test_*` with `assert ...` statements insideâ€. . Thanks for your guidance, I have added `test_weightedSampling.py` with a folder named `weighted_sampled` in _data folder. . I have updated scanpy for weighted sampling for later tasks (clustering, finding marking genes and plotting). I also suggest to support it for initial tasks like PCA for data where each observation has weight (as in MATLAB). . Regards, . Khalid .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:230,deployability,updat,updated,230,"> Thank you! With â€œtestsâ€ I mean â€œfunctions named `test_*` with `assert ...` statements insideâ€. . Thanks for your guidance, I have added `test_weightedSampling.py` with a folder named `weighted_sampled` in _data folder. . I have updated scanpy for weighted sampling for later tasks (clustering, finding marking genes and plotting). I also suggest to support it for initial tasks like PCA for data where each observation has weight (as in MATLAB). . Regards, . Khalid .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:284,deployability,cluster,clustering,284,"> Thank you! With â€œtestsâ€ I mean â€œfunctions named `test_*` with `assert ...` statements insideâ€. . Thanks for your guidance, I have added `test_weightedSampling.py` with a folder named `weighted_sampled` in _data folder. . I have updated scanpy for weighted sampling for later tasks (clustering, finding marking genes and plotting). I also suggest to support it for initial tasks like PCA for data where each observation has weight (as in MATLAB). . Regards, . Khalid .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:409,deployability,observ,observation,409,"> Thank you! With â€œtestsâ€ I mean â€œfunctions named `test_*` with `assert ...` statements insideâ€. . Thanks for your guidance, I have added `test_weightedSampling.py` with a folder named `weighted_sampled` in _data folder. . I have updated scanpy for weighted sampling for later tasks (clustering, finding marking genes and plotting). I also suggest to support it for initial tasks like PCA for data where each observation has weight (as in MATLAB). . Regards, . Khalid .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:77,integrability,state,statements,77,"> Thank you! With â€œtestsâ€ I mean â€œfunctions named `test_*` with `assert ...` statements insideâ€. . Thanks for your guidance, I have added `test_weightedSampling.py` with a folder named `weighted_sampled` in _data folder. . I have updated scanpy for weighted sampling for later tasks (clustering, finding marking genes and plotting). I also suggest to support it for initial tasks like PCA for data where each observation has weight (as in MATLAB). . Regards, . Khalid .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:19,safety,test,tests,19,"> Thank you! With â€œtestsâ€ I mean â€œfunctions named `test_*` with `assert ...` statements insideâ€. . Thanks for your guidance, I have added `test_weightedSampling.py` with a folder named `weighted_sampled` in _data folder. . I have updated scanpy for weighted sampling for later tasks (clustering, finding marking genes and plotting). I also suggest to support it for initial tasks like PCA for data where each observation has weight (as in MATLAB). . Regards, . Khalid .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:230,safety,updat,updated,230,"> Thank you! With â€œtestsâ€ I mean â€œfunctions named `test_*` with `assert ...` statements insideâ€. . Thanks for your guidance, I have added `test_weightedSampling.py` with a folder named `weighted_sampled` in _data folder. . I have updated scanpy for weighted sampling for later tasks (clustering, finding marking genes and plotting). I also suggest to support it for initial tasks like PCA for data where each observation has weight (as in MATLAB). . Regards, . Khalid .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:230,security,updat,updated,230,"> Thank you! With â€œtestsâ€ I mean â€œfunctions named `test_*` with `assert ...` statements insideâ€. . Thanks for your guidance, I have added `test_weightedSampling.py` with a folder named `weighted_sampled` in _data folder. . I have updated scanpy for weighted sampling for later tasks (clustering, finding marking genes and plotting). I also suggest to support it for initial tasks like PCA for data where each observation has weight (as in MATLAB). . Regards, . Khalid .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:19,testability,test,tests,19,"> Thank you! With â€œtestsâ€ I mean â€œfunctions named `test_*` with `assert ...` statements insideâ€. . Thanks for your guidance, I have added `test_weightedSampling.py` with a folder named `weighted_sampled` in _data folder. . I have updated scanpy for weighted sampling for later tasks (clustering, finding marking genes and plotting). I also suggest to support it for initial tasks like PCA for data where each observation has weight (as in MATLAB). . Regards, . Khalid .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:65,testability,assert,assert,65,"> Thank you! With â€œtestsâ€ I mean â€œfunctions named `test_*` with `assert ...` statements insideâ€. . Thanks for your guidance, I have added `test_weightedSampling.py` with a folder named `weighted_sampled` in _data folder. . I have updated scanpy for weighted sampling for later tasks (clustering, finding marking genes and plotting). I also suggest to support it for initial tasks like PCA for data where each observation has weight (as in MATLAB). . Regards, . Khalid .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:409,testability,observ,observation,409,"> Thank you! With â€œtestsâ€ I mean â€œfunctions named `test_*` with `assert ...` statements insideâ€. . Thanks for your guidance, I have added `test_weightedSampling.py` with a folder named `weighted_sampled` in _data folder. . I have updated scanpy for weighted sampling for later tasks (clustering, finding marking genes and plotting). I also suggest to support it for initial tasks like PCA for data where each observation has weight (as in MATLAB). . Regards, . Khalid .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:115,usability,guidanc,guidance,115,"> Thank you! With â€œtestsâ€ I mean â€œfunctions named `test_*` with `assert ...` statements insideâ€. . Thanks for your guidance, I have added `test_weightedSampling.py` with a folder named `weighted_sampled` in _data folder. . I have updated scanpy for weighted sampling for later tasks (clustering, finding marking genes and plotting). I also suggest to support it for initial tasks like PCA for data where each observation has weight (as in MATLAB). . Regards, . Khalid .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:351,usability,support,support,351,"> Thank you! With â€œtestsâ€ I mean â€œfunctions named `test_*` with `assert ...` statements insideâ€. . Thanks for your guidance, I have added `test_weightedSampling.py` with a folder named `weighted_sampled` in _data folder. . I have updated scanpy for weighted sampling for later tasks (clustering, finding marking genes and plotting). I also suggest to support it for initial tasks like PCA for data where each observation has weight (as in MATLAB). . Regards, . Khalid .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:236,performance,time,time,236,"I have removed issue from the pull request by the testing tool, now the tools showed me duplications, which are mostly from other code and 1-2 from my code. Please have a look into it. It's my first pull request and its taking too much time :(. Thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:50,safety,test,testing,50,"I have removed issue from the pull request by the testing tool, now the tools showed me duplications, which are mostly from other code and 1-2 from my code. Please have a look into it. It's my first pull request and its taking too much time :(. Thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:50,testability,test,testing,50,"I have removed issue from the pull request by the testing tool, now the tools showed me duplications, which are mostly from other code and 1-2 from my code. Please have a look into it. It's my first pull request and its taking too much time :(. Thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:58,usability,tool,tool,58,"I have removed issue from the pull request by the testing tool, now the tools showed me duplications, which are mostly from other code and 1-2 from my code. Please have a look into it. It's my first pull request and its taking too much time :(. Thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:72,usability,tool,tools,72,"I have removed issue from the pull request by the testing tool, now the tools showed me duplications, which are mostly from other code and 1-2 from my code. Please have a look into it. It's my first pull request and its taking too much time :(. Thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:576,deployability,contain,contain,576,"Hi, looks great! Ignore the tool, I think itâ€™s a bit broken. I need to figure out whatâ€™s wrong with it. The only duplicated code left is that `_prepare_weighted_dataframe` is very similar to `_prepare_dataframe`. I think you can delete `_prepare_weighted_dataframe` and just change `_prepare_dataframe` so it does `return categories, obs_tidy, categorical`. Then you can change each line like `categories, obs_tidy = _prepare_dataframe(â€¦)` to `categories, obs_tidy, _ = _prepare_dataframe(â€¦)`. Other than that, thereâ€™s only few things left:. 1. The tests without plots should contain assertions. I.e. in `test_genes_ranking()` you should do `assert np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...])` or so! 2. For the plot tests, you need to add these lines to the test file:. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L4. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L8-L13. And do each test like this (replace â€œxyzâ€ with whatever you want):. ```py. def test_xyz(image_comparer):. save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). [â€¦]. sc.pl.xyz(adata, â€¦). save_and_compare_images('xyz'). ```. This will make the tests save your plots to `scanpy/tests/figures` and compare them to the images in `scanpy/test/_images`. The tests will fail because `scanpy/test/_images/xyz.png` doesnâ€™t exist. You need to copy the pngs from `scanpy/tests/figures`â†’`scanpy/test/_images` and `git commit` them. 3. This needs to be fixed: https://github.com/theislab/scanpy/pull/644#discussion_r284652144. 4. I think the test data might be too large. @falexwolf do we have a recommended size for new test data? @Khalid-Usman Iâ€™m sorry if you find that this takes long and is frustrating. If this is the case, just step away for a while and do something else! But I think you wonâ€™t regret doing this. Youâ€™re learning good coding practices here that will ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1402,deployability,fail,fail,1402,"out whatâ€™s wrong with it. The only duplicated code left is that `_prepare_weighted_dataframe` is very similar to `_prepare_dataframe`. I think you can delete `_prepare_weighted_dataframe` and just change `_prepare_dataframe` so it does `return categories, obs_tidy, categorical`. Then you can change each line like `categories, obs_tidy = _prepare_dataframe(â€¦)` to `categories, obs_tidy, _ = _prepare_dataframe(â€¦)`. Other than that, thereâ€™s only few things left:. 1. The tests without plots should contain assertions. I.e. in `test_genes_ranking()` you should do `assert np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...])` or so! 2. For the plot tests, you need to add these lines to the test file:. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L4. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L8-L13. And do each test like this (replace â€œxyzâ€ with whatever you want):. ```py. def test_xyz(image_comparer):. save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). [â€¦]. sc.pl.xyz(adata, â€¦). save_and_compare_images('xyz'). ```. This will make the tests save your plots to `scanpy/tests/figures` and compare them to the images in `scanpy/test/_images`. The tests will fail because `scanpy/test/_images/xyz.png` doesnâ€™t exist. You need to copy the pngs from `scanpy/tests/figures`â†’`scanpy/test/_images` and `git commit` them. 3. This needs to be fixed: https://github.com/theislab/scanpy/pull/644#discussion_r284652144. 4. I think the test data might be too large. @falexwolf do we have a recommended size for new test data? @Khalid-Usman Iâ€™m sorry if you find that this takes long and is frustrating. If this is the case, just step away for a while and do something else! But I think you wonâ€™t regret doing this. Youâ€™re learning good coding practices here that will come in handy in the future, I promise! Thank you for your contribution :tada:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:309,reliability,doe,does,309,"Hi, looks great! Ignore the tool, I think itâ€™s a bit broken. I need to figure out whatâ€™s wrong with it. The only duplicated code left is that `_prepare_weighted_dataframe` is very similar to `_prepare_dataframe`. I think you can delete `_prepare_weighted_dataframe` and just change `_prepare_dataframe` so it does `return categories, obs_tidy, categorical`. Then you can change each line like `categories, obs_tidy = _prepare_dataframe(â€¦)` to `categories, obs_tidy, _ = _prepare_dataframe(â€¦)`. Other than that, thereâ€™s only few things left:. 1. The tests without plots should contain assertions. I.e. in `test_genes_ranking()` you should do `assert np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...])` or so! 2. For the plot tests, you need to add these lines to the test file:. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L4. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L8-L13. And do each test like this (replace â€œxyzâ€ with whatever you want):. ```py. def test_xyz(image_comparer):. save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). [â€¦]. sc.pl.xyz(adata, â€¦). save_and_compare_images('xyz'). ```. This will make the tests save your plots to `scanpy/tests/figures` and compare them to the images in `scanpy/test/_images`. The tests will fail because `scanpy/test/_images/xyz.png` doesnâ€™t exist. You need to copy the pngs from `scanpy/tests/figures`â†’`scanpy/test/_images` and `git commit` them. 3. This needs to be fixed: https://github.com/theislab/scanpy/pull/644#discussion_r284652144. 4. I think the test data might be too large. @falexwolf do we have a recommended size for new test data? @Khalid-Usman Iâ€™m sorry if you find that this takes long and is frustrating. If this is the case, just step away for a while and do something else! But I think you wonâ€™t regret doing this. Youâ€™re learning good coding practices here that will ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1402,reliability,fail,fail,1402,"out whatâ€™s wrong with it. The only duplicated code left is that `_prepare_weighted_dataframe` is very similar to `_prepare_dataframe`. I think you can delete `_prepare_weighted_dataframe` and just change `_prepare_dataframe` so it does `return categories, obs_tidy, categorical`. Then you can change each line like `categories, obs_tidy = _prepare_dataframe(â€¦)` to `categories, obs_tidy, _ = _prepare_dataframe(â€¦)`. Other than that, thereâ€™s only few things left:. 1. The tests without plots should contain assertions. I.e. in `test_genes_ranking()` you should do `assert np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...])` or so! 2. For the plot tests, you need to add these lines to the test file:. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L4. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L8-L13. And do each test like this (replace â€œxyzâ€ with whatever you want):. ```py. def test_xyz(image_comparer):. save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). [â€¦]. sc.pl.xyz(adata, â€¦). save_and_compare_images('xyz'). ```. This will make the tests save your plots to `scanpy/tests/figures` and compare them to the images in `scanpy/test/_images`. The tests will fail because `scanpy/test/_images/xyz.png` doesnâ€™t exist. You need to copy the pngs from `scanpy/tests/figures`â†’`scanpy/test/_images` and `git commit` them. 3. This needs to be fixed: https://github.com/theislab/scanpy/pull/644#discussion_r284652144. 4. I think the test data might be too large. @falexwolf do we have a recommended size for new test data? @Khalid-Usman Iâ€™m sorry if you find that this takes long and is frustrating. If this is the case, just step away for a while and do something else! But I think you wonâ€™t regret doing this. Youâ€™re learning good coding practices here that will come in handy in the future, I promise! Thank you for your contribution :tada:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1445,reliability,doe,doesn,1445,"out whatâ€™s wrong with it. The only duplicated code left is that `_prepare_weighted_dataframe` is very similar to `_prepare_dataframe`. I think you can delete `_prepare_weighted_dataframe` and just change `_prepare_dataframe` so it does `return categories, obs_tidy, categorical`. Then you can change each line like `categories, obs_tidy = _prepare_dataframe(â€¦)` to `categories, obs_tidy, _ = _prepare_dataframe(â€¦)`. Other than that, thereâ€™s only few things left:. 1. The tests without plots should contain assertions. I.e. in `test_genes_ranking()` you should do `assert np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...])` or so! 2. For the plot tests, you need to add these lines to the test file:. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L4. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L8-L13. And do each test like this (replace â€œxyzâ€ with whatever you want):. ```py. def test_xyz(image_comparer):. save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). [â€¦]. sc.pl.xyz(adata, â€¦). save_and_compare_images('xyz'). ```. This will make the tests save your plots to `scanpy/tests/figures` and compare them to the images in `scanpy/test/_images`. The tests will fail because `scanpy/test/_images/xyz.png` doesnâ€™t exist. You need to copy the pngs from `scanpy/tests/figures`â†’`scanpy/test/_images` and `git commit` them. 3. This needs to be fixed: https://github.com/theislab/scanpy/pull/644#discussion_r284652144. 4. I think the test data might be too large. @falexwolf do we have a recommended size for new test data? @Khalid-Usman Iâ€™m sorry if you find that this takes long and is frustrating. If this is the case, just step away for a while and do something else! But I think you wonâ€™t regret doing this. Youâ€™re learning good coding practices here that will come in handy in the future, I promise! Thank you for your contribution :tada:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1975,reliability,pra,practices,1975,"out whatâ€™s wrong with it. The only duplicated code left is that `_prepare_weighted_dataframe` is very similar to `_prepare_dataframe`. I think you can delete `_prepare_weighted_dataframe` and just change `_prepare_dataframe` so it does `return categories, obs_tidy, categorical`. Then you can change each line like `categories, obs_tidy = _prepare_dataframe(â€¦)` to `categories, obs_tidy, _ = _prepare_dataframe(â€¦)`. Other than that, thereâ€™s only few things left:. 1. The tests without plots should contain assertions. I.e. in `test_genes_ranking()` you should do `assert np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...])` or so! 2. For the plot tests, you need to add these lines to the test file:. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L4. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L8-L13. And do each test like this (replace â€œxyzâ€ with whatever you want):. ```py. def test_xyz(image_comparer):. save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). [â€¦]. sc.pl.xyz(adata, â€¦). save_and_compare_images('xyz'). ```. This will make the tests save your plots to `scanpy/tests/figures` and compare them to the images in `scanpy/test/_images`. The tests will fail because `scanpy/test/_images/xyz.png` doesnâ€™t exist. You need to copy the pngs from `scanpy/tests/figures`â†’`scanpy/test/_images` and `git commit` them. 3. This needs to be fixed: https://github.com/theislab/scanpy/pull/644#discussion_r284652144. 4. I think the test data might be too large. @falexwolf do we have a recommended size for new test data? @Khalid-Usman Iâ€™m sorry if you find that this takes long and is frustrating. If this is the case, just step away for a while and do something else! But I think you wonâ€™t regret doing this. Youâ€™re learning good coding practices here that will come in handy in the future, I promise! Thank you for your contribution :tada:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:549,safety,test,tests,549,"Hi, looks great! Ignore the tool, I think itâ€™s a bit broken. I need to figure out whatâ€™s wrong with it. The only duplicated code left is that `_prepare_weighted_dataframe` is very similar to `_prepare_dataframe`. I think you can delete `_prepare_weighted_dataframe` and just change `_prepare_dataframe` so it does `return categories, obs_tidy, categorical`. Then you can change each line like `categories, obs_tidy = _prepare_dataframe(â€¦)` to `categories, obs_tidy, _ = _prepare_dataframe(â€¦)`. Other than that, thereâ€™s only few things left:. 1. The tests without plots should contain assertions. I.e. in `test_genes_ranking()` you should do `assert np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...])` or so! 2. For the plot tests, you need to add these lines to the test file:. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L4. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L8-L13. And do each test like this (replace â€œxyzâ€ with whatever you want):. ```py. def test_xyz(image_comparer):. save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). [â€¦]. sc.pl.xyz(adata, â€¦). save_and_compare_images('xyz'). ```. This will make the tests save your plots to `scanpy/tests/figures` and compare them to the images in `scanpy/test/_images`. The tests will fail because `scanpy/test/_images/xyz.png` doesnâ€™t exist. You need to copy the pngs from `scanpy/tests/figures`â†’`scanpy/test/_images` and `git commit` them. 3. This needs to be fixed: https://github.com/theislab/scanpy/pull/644#discussion_r284652144. 4. I think the test data might be too large. @falexwolf do we have a recommended size for new test data? @Khalid-Usman Iâ€™m sorry if you find that this takes long and is frustrating. If this is the case, just step away for a while and do something else! But I think you wonâ€™t regret doing this. Youâ€™re learning good coding practices here that will ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:741,safety,test,tests,741,"Hi, looks great! Ignore the tool, I think itâ€™s a bit broken. I need to figure out whatâ€™s wrong with it. The only duplicated code left is that `_prepare_weighted_dataframe` is very similar to `_prepare_dataframe`. I think you can delete `_prepare_weighted_dataframe` and just change `_prepare_dataframe` so it does `return categories, obs_tidy, categorical`. Then you can change each line like `categories, obs_tidy = _prepare_dataframe(â€¦)` to `categories, obs_tidy, _ = _prepare_dataframe(â€¦)`. Other than that, thereâ€™s only few things left:. 1. The tests without plots should contain assertions. I.e. in `test_genes_ranking()` you should do `assert np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...])` or so! 2. For the plot tests, you need to add these lines to the test file:. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L4. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L8-L13. And do each test like this (replace â€œxyzâ€ with whatever you want):. ```py. def test_xyz(image_comparer):. save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). [â€¦]. sc.pl.xyz(adata, â€¦). save_and_compare_images('xyz'). ```. This will make the tests save your plots to `scanpy/tests/figures` and compare them to the images in `scanpy/test/_images`. The tests will fail because `scanpy/test/_images/xyz.png` doesnâ€™t exist. You need to copy the pngs from `scanpy/tests/figures`â†’`scanpy/test/_images` and `git commit` them. 3. This needs to be fixed: https://github.com/theislab/scanpy/pull/644#discussion_r284652144. 4. I think the test data might be too large. @falexwolf do we have a recommended size for new test data? @Khalid-Usman Iâ€™m sorry if you find that this takes long and is frustrating. If this is the case, just step away for a while and do something else! But I think you wonâ€™t regret doing this. Youâ€™re learning good coding practices here that will ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:783,safety,test,test,783,"Hi, looks great! Ignore the tool, I think itâ€™s a bit broken. I need to figure out whatâ€™s wrong with it. The only duplicated code left is that `_prepare_weighted_dataframe` is very similar to `_prepare_dataframe`. I think you can delete `_prepare_weighted_dataframe` and just change `_prepare_dataframe` so it does `return categories, obs_tidy, categorical`. Then you can change each line like `categories, obs_tidy = _prepare_dataframe(â€¦)` to `categories, obs_tidy, _ = _prepare_dataframe(â€¦)`. Other than that, thereâ€™s only few things left:. 1. The tests without plots should contain assertions. I.e. in `test_genes_ranking()` you should do `assert np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...])` or so! 2. For the plot tests, you need to add these lines to the test file:. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L4. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L8-L13. And do each test like this (replace â€œxyzâ€ with whatever you want):. ```py. def test_xyz(image_comparer):. save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). [â€¦]. sc.pl.xyz(adata, â€¦). save_and_compare_images('xyz'). ```. This will make the tests save your plots to `scanpy/tests/figures` and compare them to the images in `scanpy/test/_images`. The tests will fail because `scanpy/test/_images/xyz.png` doesnâ€™t exist. You need to copy the pngs from `scanpy/tests/figures`â†’`scanpy/test/_images` and `git commit` them. 3. This needs to be fixed: https://github.com/theislab/scanpy/pull/644#discussion_r284652144. 4. I think the test data might be too large. @falexwolf do we have a recommended size for new test data? @Khalid-Usman Iâ€™m sorry if you find that this takes long and is frustrating. If this is the case, just step away for a while and do something else! But I think you wonâ€™t regret doing this. Youâ€™re learning good coding practices here that will ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:883,safety,test,tests,883,"Hi, looks great! Ignore the tool, I think itâ€™s a bit broken. I need to figure out whatâ€™s wrong with it. The only duplicated code left is that `_prepare_weighted_dataframe` is very similar to `_prepare_dataframe`. I think you can delete `_prepare_weighted_dataframe` and just change `_prepare_dataframe` so it does `return categories, obs_tidy, categorical`. Then you can change each line like `categories, obs_tidy = _prepare_dataframe(â€¦)` to `categories, obs_tidy, _ = _prepare_dataframe(â€¦)`. Other than that, thereâ€™s only few things left:. 1. The tests without plots should contain assertions. I.e. in `test_genes_ranking()` you should do `assert np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...])` or so! 2. For the plot tests, you need to add these lines to the test file:. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L4. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L8-L13. And do each test like this (replace â€œxyzâ€ with whatever you want):. ```py. def test_xyz(image_comparer):. save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). [â€¦]. sc.pl.xyz(adata, â€¦). save_and_compare_images('xyz'). ```. This will make the tests save your plots to `scanpy/tests/figures` and compare them to the images in `scanpy/test/_images`. The tests will fail because `scanpy/test/_images/xyz.png` doesnâ€™t exist. You need to copy the pngs from `scanpy/tests/figures`â†’`scanpy/test/_images` and `git commit` them. 3. This needs to be fixed: https://github.com/theislab/scanpy/pull/644#discussion_r284652144. 4. I think the test data might be too large. @falexwolf do we have a recommended size for new test data? @Khalid-Usman Iâ€™m sorry if you find that this takes long and is frustrating. If this is the case, just step away for a while and do something else! But I think you wonâ€™t regret doing this. Youâ€™re learning good coding practices here that will ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1001,safety,test,tests,1001,"looks great! Ignore the tool, I think itâ€™s a bit broken. I need to figure out whatâ€™s wrong with it. The only duplicated code left is that `_prepare_weighted_dataframe` is very similar to `_prepare_dataframe`. I think you can delete `_prepare_weighted_dataframe` and just change `_prepare_dataframe` so it does `return categories, obs_tidy, categorical`. Then you can change each line like `categories, obs_tidy = _prepare_dataframe(â€¦)` to `categories, obs_tidy, _ = _prepare_dataframe(â€¦)`. Other than that, thereâ€™s only few things left:. 1. The tests without plots should contain assertions. I.e. in `test_genes_ranking()` you should do `assert np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...])` or so! 2. For the plot tests, you need to add these lines to the test file:. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L4. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L8-L13. And do each test like this (replace â€œxyzâ€ with whatever you want):. ```py. def test_xyz(image_comparer):. save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). [â€¦]. sc.pl.xyz(adata, â€¦). save_and_compare_images('xyz'). ```. This will make the tests save your plots to `scanpy/tests/figures` and compare them to the images in `scanpy/test/_images`. The tests will fail because `scanpy/test/_images/xyz.png` doesnâ€™t exist. You need to copy the pngs from `scanpy/tests/figures`â†’`scanpy/test/_images` and `git commit` them. 3. This needs to be fixed: https://github.com/theislab/scanpy/pull/644#discussion_r284652144. 4. I think the test data might be too large. @falexwolf do we have a recommended size for new test data? @Khalid-Usman Iâ€™m sorry if you find that this takes long and is frustrating. If this is the case, just step away for a while and do something else! But I think you wonâ€™t regret doing this. Youâ€™re learning good coding practices here that will come",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1044,safety,test,test,1044," a bit broken. I need to figure out whatâ€™s wrong with it. The only duplicated code left is that `_prepare_weighted_dataframe` is very similar to `_prepare_dataframe`. I think you can delete `_prepare_weighted_dataframe` and just change `_prepare_dataframe` so it does `return categories, obs_tidy, categorical`. Then you can change each line like `categories, obs_tidy = _prepare_dataframe(â€¦)` to `categories, obs_tidy, _ = _prepare_dataframe(â€¦)`. Other than that, thereâ€™s only few things left:. 1. The tests without plots should contain assertions. I.e. in `test_genes_ranking()` you should do `assert np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...])` or so! 2. For the plot tests, you need to add these lines to the test file:. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L4. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L8-L13. And do each test like this (replace â€œxyzâ€ with whatever you want):. ```py. def test_xyz(image_comparer):. save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). [â€¦]. sc.pl.xyz(adata, â€¦). save_and_compare_images('xyz'). ```. This will make the tests save your plots to `scanpy/tests/figures` and compare them to the images in `scanpy/test/_images`. The tests will fail because `scanpy/test/_images/xyz.png` doesnâ€™t exist. You need to copy the pngs from `scanpy/tests/figures`â†’`scanpy/test/_images` and `git commit` them. 3. This needs to be fixed: https://github.com/theislab/scanpy/pull/644#discussion_r284652144. 4. I think the test data might be too large. @falexwolf do we have a recommended size for new test data? @Khalid-Usman Iâ€™m sorry if you find that this takes long and is frustrating. If this is the case, just step away for a while and do something else! But I think you wonâ€™t regret doing this. Youâ€™re learning good coding practices here that will come in handy in the future, I promise! Thank ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1282,safety,test,tests,1282,"out whatâ€™s wrong with it. The only duplicated code left is that `_prepare_weighted_dataframe` is very similar to `_prepare_dataframe`. I think you can delete `_prepare_weighted_dataframe` and just change `_prepare_dataframe` so it does `return categories, obs_tidy, categorical`. Then you can change each line like `categories, obs_tidy = _prepare_dataframe(â€¦)` to `categories, obs_tidy, _ = _prepare_dataframe(â€¦)`. Other than that, thereâ€™s only few things left:. 1. The tests without plots should contain assertions. I.e. in `test_genes_ranking()` you should do `assert np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...])` or so! 2. For the plot tests, you need to add these lines to the test file:. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L4. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L8-L13. And do each test like this (replace â€œxyzâ€ with whatever you want):. ```py. def test_xyz(image_comparer):. save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). [â€¦]. sc.pl.xyz(adata, â€¦). save_and_compare_images('xyz'). ```. This will make the tests save your plots to `scanpy/tests/figures` and compare them to the images in `scanpy/test/_images`. The tests will fail because `scanpy/test/_images/xyz.png` doesnâ€™t exist. You need to copy the pngs from `scanpy/tests/figures`â†’`scanpy/test/_images` and `git commit` them. 3. This needs to be fixed: https://github.com/theislab/scanpy/pull/644#discussion_r284652144. 4. I think the test data might be too large. @falexwolf do we have a recommended size for new test data? @Khalid-Usman Iâ€™m sorry if you find that this takes long and is frustrating. If this is the case, just step away for a while and do something else! But I think you wonâ€™t regret doing this. Youâ€™re learning good coding practices here that will come in handy in the future, I promise! Thank you for your contribution :tada:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1315,safety,test,tests,1315,"out whatâ€™s wrong with it. The only duplicated code left is that `_prepare_weighted_dataframe` is very similar to `_prepare_dataframe`. I think you can delete `_prepare_weighted_dataframe` and just change `_prepare_dataframe` so it does `return categories, obs_tidy, categorical`. Then you can change each line like `categories, obs_tidy = _prepare_dataframe(â€¦)` to `categories, obs_tidy, _ = _prepare_dataframe(â€¦)`. Other than that, thereâ€™s only few things left:. 1. The tests without plots should contain assertions. I.e. in `test_genes_ranking()` you should do `assert np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...])` or so! 2. For the plot tests, you need to add these lines to the test file:. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L4. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L8-L13. And do each test like this (replace â€œxyzâ€ with whatever you want):. ```py. def test_xyz(image_comparer):. save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). [â€¦]. sc.pl.xyz(adata, â€¦). save_and_compare_images('xyz'). ```. This will make the tests save your plots to `scanpy/tests/figures` and compare them to the images in `scanpy/test/_images`. The tests will fail because `scanpy/test/_images/xyz.png` doesnâ€™t exist. You need to copy the pngs from `scanpy/tests/figures`â†’`scanpy/test/_images` and `git commit` them. 3. This needs to be fixed: https://github.com/theislab/scanpy/pull/644#discussion_r284652144. 4. I think the test data might be too large. @falexwolf do we have a recommended size for new test data? @Khalid-Usman Iâ€™m sorry if you find that this takes long and is frustrating. If this is the case, just step away for a while and do something else! But I think you wonâ€™t regret doing this. Youâ€™re learning good coding practices here that will come in handy in the future, I promise! Thank you for your contribution :tada:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1372,safety,test,test,1372,"out whatâ€™s wrong with it. The only duplicated code left is that `_prepare_weighted_dataframe` is very similar to `_prepare_dataframe`. I think you can delete `_prepare_weighted_dataframe` and just change `_prepare_dataframe` so it does `return categories, obs_tidy, categorical`. Then you can change each line like `categories, obs_tidy = _prepare_dataframe(â€¦)` to `categories, obs_tidy, _ = _prepare_dataframe(â€¦)`. Other than that, thereâ€™s only few things left:. 1. The tests without plots should contain assertions. I.e. in `test_genes_ranking()` you should do `assert np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...])` or so! 2. For the plot tests, you need to add these lines to the test file:. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L4. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L8-L13. And do each test like this (replace â€œxyzâ€ with whatever you want):. ```py. def test_xyz(image_comparer):. save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). [â€¦]. sc.pl.xyz(adata, â€¦). save_and_compare_images('xyz'). ```. This will make the tests save your plots to `scanpy/tests/figures` and compare them to the images in `scanpy/test/_images`. The tests will fail because `scanpy/test/_images/xyz.png` doesnâ€™t exist. You need to copy the pngs from `scanpy/tests/figures`â†’`scanpy/test/_images` and `git commit` them. 3. This needs to be fixed: https://github.com/theislab/scanpy/pull/644#discussion_r284652144. 4. I think the test data might be too large. @falexwolf do we have a recommended size for new test data? @Khalid-Usman Iâ€™m sorry if you find that this takes long and is frustrating. If this is the case, just step away for a while and do something else! But I think you wonâ€™t regret doing this. Youâ€™re learning good coding practices here that will come in handy in the future, I promise! Thank you for your contribution :tada:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1391,safety,test,tests,1391,"out whatâ€™s wrong with it. The only duplicated code left is that `_prepare_weighted_dataframe` is very similar to `_prepare_dataframe`. I think you can delete `_prepare_weighted_dataframe` and just change `_prepare_dataframe` so it does `return categories, obs_tidy, categorical`. Then you can change each line like `categories, obs_tidy = _prepare_dataframe(â€¦)` to `categories, obs_tidy, _ = _prepare_dataframe(â€¦)`. Other than that, thereâ€™s only few things left:. 1. The tests without plots should contain assertions. I.e. in `test_genes_ranking()` you should do `assert np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...])` or so! 2. For the plot tests, you need to add these lines to the test file:. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L4. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L8-L13. And do each test like this (replace â€œxyzâ€ with whatever you want):. ```py. def test_xyz(image_comparer):. save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). [â€¦]. sc.pl.xyz(adata, â€¦). save_and_compare_images('xyz'). ```. This will make the tests save your plots to `scanpy/tests/figures` and compare them to the images in `scanpy/test/_images`. The tests will fail because `scanpy/test/_images/xyz.png` doesnâ€™t exist. You need to copy the pngs from `scanpy/tests/figures`â†’`scanpy/test/_images` and `git commit` them. 3. This needs to be fixed: https://github.com/theislab/scanpy/pull/644#discussion_r284652144. 4. I think the test data might be too large. @falexwolf do we have a recommended size for new test data? @Khalid-Usman Iâ€™m sorry if you find that this takes long and is frustrating. If this is the case, just step away for a while and do something else! But I think you wonâ€™t regret doing this. Youâ€™re learning good coding practices here that will come in handy in the future, I promise! Thank you for your contribution :tada:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1423,safety,test,test,1423,"out whatâ€™s wrong with it. The only duplicated code left is that `_prepare_weighted_dataframe` is very similar to `_prepare_dataframe`. I think you can delete `_prepare_weighted_dataframe` and just change `_prepare_dataframe` so it does `return categories, obs_tidy, categorical`. Then you can change each line like `categories, obs_tidy = _prepare_dataframe(â€¦)` to `categories, obs_tidy, _ = _prepare_dataframe(â€¦)`. Other than that, thereâ€™s only few things left:. 1. The tests without plots should contain assertions. I.e. in `test_genes_ranking()` you should do `assert np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...])` or so! 2. For the plot tests, you need to add these lines to the test file:. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L4. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L8-L13. And do each test like this (replace â€œxyzâ€ with whatever you want):. ```py. def test_xyz(image_comparer):. save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). [â€¦]. sc.pl.xyz(adata, â€¦). save_and_compare_images('xyz'). ```. This will make the tests save your plots to `scanpy/tests/figures` and compare them to the images in `scanpy/test/_images`. The tests will fail because `scanpy/test/_images/xyz.png` doesnâ€™t exist. You need to copy the pngs from `scanpy/tests/figures`â†’`scanpy/test/_images` and `git commit` them. 3. This needs to be fixed: https://github.com/theislab/scanpy/pull/644#discussion_r284652144. 4. I think the test data might be too large. @falexwolf do we have a recommended size for new test data? @Khalid-Usman Iâ€™m sorry if you find that this takes long and is frustrating. If this is the case, just step away for a while and do something else! But I think you wonâ€™t regret doing this. Youâ€™re learning good coding practices here that will come in handy in the future, I promise! Thank you for your contribution :tada:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1499,safety,test,tests,1499,"out whatâ€™s wrong with it. The only duplicated code left is that `_prepare_weighted_dataframe` is very similar to `_prepare_dataframe`. I think you can delete `_prepare_weighted_dataframe` and just change `_prepare_dataframe` so it does `return categories, obs_tidy, categorical`. Then you can change each line like `categories, obs_tidy = _prepare_dataframe(â€¦)` to `categories, obs_tidy, _ = _prepare_dataframe(â€¦)`. Other than that, thereâ€™s only few things left:. 1. The tests without plots should contain assertions. I.e. in `test_genes_ranking()` you should do `assert np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...])` or so! 2. For the plot tests, you need to add these lines to the test file:. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L4. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L8-L13. And do each test like this (replace â€œxyzâ€ with whatever you want):. ```py. def test_xyz(image_comparer):. save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). [â€¦]. sc.pl.xyz(adata, â€¦). save_and_compare_images('xyz'). ```. This will make the tests save your plots to `scanpy/tests/figures` and compare them to the images in `scanpy/test/_images`. The tests will fail because `scanpy/test/_images/xyz.png` doesnâ€™t exist. You need to copy the pngs from `scanpy/tests/figures`â†’`scanpy/test/_images` and `git commit` them. 3. This needs to be fixed: https://github.com/theislab/scanpy/pull/644#discussion_r284652144. 4. I think the test data might be too large. @falexwolf do we have a recommended size for new test data? @Khalid-Usman Iâ€™m sorry if you find that this takes long and is frustrating. If this is the case, just step away for a while and do something else! But I think you wonâ€™t regret doing this. Youâ€™re learning good coding practices here that will come in handy in the future, I promise! Thank you for your contribution :tada:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1522,safety,test,test,1522,"out whatâ€™s wrong with it. The only duplicated code left is that `_prepare_weighted_dataframe` is very similar to `_prepare_dataframe`. I think you can delete `_prepare_weighted_dataframe` and just change `_prepare_dataframe` so it does `return categories, obs_tidy, categorical`. Then you can change each line like `categories, obs_tidy = _prepare_dataframe(â€¦)` to `categories, obs_tidy, _ = _prepare_dataframe(â€¦)`. Other than that, thereâ€™s only few things left:. 1. The tests without plots should contain assertions. I.e. in `test_genes_ranking()` you should do `assert np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...])` or so! 2. For the plot tests, you need to add these lines to the test file:. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L4. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L8-L13. And do each test like this (replace â€œxyzâ€ with whatever you want):. ```py. def test_xyz(image_comparer):. save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). [â€¦]. sc.pl.xyz(adata, â€¦). save_and_compare_images('xyz'). ```. This will make the tests save your plots to `scanpy/tests/figures` and compare them to the images in `scanpy/test/_images`. The tests will fail because `scanpy/test/_images/xyz.png` doesnâ€™t exist. You need to copy the pngs from `scanpy/tests/figures`â†’`scanpy/test/_images` and `git commit` them. 3. This needs to be fixed: https://github.com/theislab/scanpy/pull/644#discussion_r284652144. 4. I think the test data might be too large. @falexwolf do we have a recommended size for new test data? @Khalid-Usman Iâ€™m sorry if you find that this takes long and is frustrating. If this is the case, just step away for a while and do something else! But I think you wonâ€™t regret doing this. Youâ€™re learning good coding practices here that will come in handy in the future, I promise! Thank you for your contribution :tada:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1668,safety,test,test,1668,"out whatâ€™s wrong with it. The only duplicated code left is that `_prepare_weighted_dataframe` is very similar to `_prepare_dataframe`. I think you can delete `_prepare_weighted_dataframe` and just change `_prepare_dataframe` so it does `return categories, obs_tidy, categorical`. Then you can change each line like `categories, obs_tidy = _prepare_dataframe(â€¦)` to `categories, obs_tidy, _ = _prepare_dataframe(â€¦)`. Other than that, thereâ€™s only few things left:. 1. The tests without plots should contain assertions. I.e. in `test_genes_ranking()` you should do `assert np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...])` or so! 2. For the plot tests, you need to add these lines to the test file:. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L4. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L8-L13. And do each test like this (replace â€œxyzâ€ with whatever you want):. ```py. def test_xyz(image_comparer):. save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). [â€¦]. sc.pl.xyz(adata, â€¦). save_and_compare_images('xyz'). ```. This will make the tests save your plots to `scanpy/tests/figures` and compare them to the images in `scanpy/test/_images`. The tests will fail because `scanpy/test/_images/xyz.png` doesnâ€™t exist. You need to copy the pngs from `scanpy/tests/figures`â†’`scanpy/test/_images` and `git commit` them. 3. This needs to be fixed: https://github.com/theislab/scanpy/pull/644#discussion_r284652144. 4. I think the test data might be too large. @falexwolf do we have a recommended size for new test data? @Khalid-Usman Iâ€™m sorry if you find that this takes long and is frustrating. If this is the case, just step away for a while and do something else! But I think you wonâ€™t regret doing this. Youâ€™re learning good coding practices here that will come in handy in the future, I promise! Thank you for your contribution :tada:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1747,safety,test,test,1747,"out whatâ€™s wrong with it. The only duplicated code left is that `_prepare_weighted_dataframe` is very similar to `_prepare_dataframe`. I think you can delete `_prepare_weighted_dataframe` and just change `_prepare_dataframe` so it does `return categories, obs_tidy, categorical`. Then you can change each line like `categories, obs_tidy = _prepare_dataframe(â€¦)` to `categories, obs_tidy, _ = _prepare_dataframe(â€¦)`. Other than that, thereâ€™s only few things left:. 1. The tests without plots should contain assertions. I.e. in `test_genes_ranking()` you should do `assert np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...])` or so! 2. For the plot tests, you need to add these lines to the test file:. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L4. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L8-L13. And do each test like this (replace â€œxyzâ€ with whatever you want):. ```py. def test_xyz(image_comparer):. save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). [â€¦]. sc.pl.xyz(adata, â€¦). save_and_compare_images('xyz'). ```. This will make the tests save your plots to `scanpy/tests/figures` and compare them to the images in `scanpy/test/_images`. The tests will fail because `scanpy/test/_images/xyz.png` doesnâ€™t exist. You need to copy the pngs from `scanpy/tests/figures`â†’`scanpy/test/_images` and `git commit` them. 3. This needs to be fixed: https://github.com/theislab/scanpy/pull/644#discussion_r284652144. 4. I think the test data might be too large. @falexwolf do we have a recommended size for new test data? @Khalid-Usman Iâ€™m sorry if you find that this takes long and is frustrating. If this is the case, just step away for a while and do something else! But I think you wonâ€™t regret doing this. Youâ€™re learning good coding practices here that will come in handy in the future, I promise! Thank you for your contribution :tada:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:549,testability,test,tests,549,"Hi, looks great! Ignore the tool, I think itâ€™s a bit broken. I need to figure out whatâ€™s wrong with it. The only duplicated code left is that `_prepare_weighted_dataframe` is very similar to `_prepare_dataframe`. I think you can delete `_prepare_weighted_dataframe` and just change `_prepare_dataframe` so it does `return categories, obs_tidy, categorical`. Then you can change each line like `categories, obs_tidy = _prepare_dataframe(â€¦)` to `categories, obs_tidy, _ = _prepare_dataframe(â€¦)`. Other than that, thereâ€™s only few things left:. 1. The tests without plots should contain assertions. I.e. in `test_genes_ranking()` you should do `assert np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...])` or so! 2. For the plot tests, you need to add these lines to the test file:. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L4. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L8-L13. And do each test like this (replace â€œxyzâ€ with whatever you want):. ```py. def test_xyz(image_comparer):. save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). [â€¦]. sc.pl.xyz(adata, â€¦). save_and_compare_images('xyz'). ```. This will make the tests save your plots to `scanpy/tests/figures` and compare them to the images in `scanpy/test/_images`. The tests will fail because `scanpy/test/_images/xyz.png` doesnâ€™t exist. You need to copy the pngs from `scanpy/tests/figures`â†’`scanpy/test/_images` and `git commit` them. 3. This needs to be fixed: https://github.com/theislab/scanpy/pull/644#discussion_r284652144. 4. I think the test data might be too large. @falexwolf do we have a recommended size for new test data? @Khalid-Usman Iâ€™m sorry if you find that this takes long and is frustrating. If this is the case, just step away for a while and do something else! But I think you wonâ€™t regret doing this. Youâ€™re learning good coding practices here that will ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:584,testability,assert,assertions,584,"Hi, looks great! Ignore the tool, I think itâ€™s a bit broken. I need to figure out whatâ€™s wrong with it. The only duplicated code left is that `_prepare_weighted_dataframe` is very similar to `_prepare_dataframe`. I think you can delete `_prepare_weighted_dataframe` and just change `_prepare_dataframe` so it does `return categories, obs_tidy, categorical`. Then you can change each line like `categories, obs_tidy = _prepare_dataframe(â€¦)` to `categories, obs_tidy, _ = _prepare_dataframe(â€¦)`. Other than that, thereâ€™s only few things left:. 1. The tests without plots should contain assertions. I.e. in `test_genes_ranking()` you should do `assert np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...])` or so! 2. For the plot tests, you need to add these lines to the test file:. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L4. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L8-L13. And do each test like this (replace â€œxyzâ€ with whatever you want):. ```py. def test_xyz(image_comparer):. save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). [â€¦]. sc.pl.xyz(adata, â€¦). save_and_compare_images('xyz'). ```. This will make the tests save your plots to `scanpy/tests/figures` and compare them to the images in `scanpy/test/_images`. The tests will fail because `scanpy/test/_images/xyz.png` doesnâ€™t exist. You need to copy the pngs from `scanpy/tests/figures`â†’`scanpy/test/_images` and `git commit` them. 3. This needs to be fixed: https://github.com/theislab/scanpy/pull/644#discussion_r284652144. 4. I think the test data might be too large. @falexwolf do we have a recommended size for new test data? @Khalid-Usman Iâ€™m sorry if you find that this takes long and is frustrating. If this is the case, just step away for a while and do something else! But I think you wonâ€™t regret doing this. Youâ€™re learning good coding practices here that will ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:642,testability,assert,assert,642,"Hi, looks great! Ignore the tool, I think itâ€™s a bit broken. I need to figure out whatâ€™s wrong with it. The only duplicated code left is that `_prepare_weighted_dataframe` is very similar to `_prepare_dataframe`. I think you can delete `_prepare_weighted_dataframe` and just change `_prepare_dataframe` so it does `return categories, obs_tidy, categorical`. Then you can change each line like `categories, obs_tidy = _prepare_dataframe(â€¦)` to `categories, obs_tidy, _ = _prepare_dataframe(â€¦)`. Other than that, thereâ€™s only few things left:. 1. The tests without plots should contain assertions. I.e. in `test_genes_ranking()` you should do `assert np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...])` or so! 2. For the plot tests, you need to add these lines to the test file:. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L4. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L8-L13. And do each test like this (replace â€œxyzâ€ with whatever you want):. ```py. def test_xyz(image_comparer):. save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). [â€¦]. sc.pl.xyz(adata, â€¦). save_and_compare_images('xyz'). ```. This will make the tests save your plots to `scanpy/tests/figures` and compare them to the images in `scanpy/test/_images`. The tests will fail because `scanpy/test/_images/xyz.png` doesnâ€™t exist. You need to copy the pngs from `scanpy/tests/figures`â†’`scanpy/test/_images` and `git commit` them. 3. This needs to be fixed: https://github.com/theislab/scanpy/pull/644#discussion_r284652144. 4. I think the test data might be too large. @falexwolf do we have a recommended size for new test data? @Khalid-Usman Iâ€™m sorry if you find that this takes long and is frustrating. If this is the case, just step away for a while and do something else! But I think you wonâ€™t regret doing this. Youâ€™re learning good coding practices here that will ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:741,testability,test,tests,741,"Hi, looks great! Ignore the tool, I think itâ€™s a bit broken. I need to figure out whatâ€™s wrong with it. The only duplicated code left is that `_prepare_weighted_dataframe` is very similar to `_prepare_dataframe`. I think you can delete `_prepare_weighted_dataframe` and just change `_prepare_dataframe` so it does `return categories, obs_tidy, categorical`. Then you can change each line like `categories, obs_tidy = _prepare_dataframe(â€¦)` to `categories, obs_tidy, _ = _prepare_dataframe(â€¦)`. Other than that, thereâ€™s only few things left:. 1. The tests without plots should contain assertions. I.e. in `test_genes_ranking()` you should do `assert np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...])` or so! 2. For the plot tests, you need to add these lines to the test file:. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L4. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L8-L13. And do each test like this (replace â€œxyzâ€ with whatever you want):. ```py. def test_xyz(image_comparer):. save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). [â€¦]. sc.pl.xyz(adata, â€¦). save_and_compare_images('xyz'). ```. This will make the tests save your plots to `scanpy/tests/figures` and compare them to the images in `scanpy/test/_images`. The tests will fail because `scanpy/test/_images/xyz.png` doesnâ€™t exist. You need to copy the pngs from `scanpy/tests/figures`â†’`scanpy/test/_images` and `git commit` them. 3. This needs to be fixed: https://github.com/theislab/scanpy/pull/644#discussion_r284652144. 4. I think the test data might be too large. @falexwolf do we have a recommended size for new test data? @Khalid-Usman Iâ€™m sorry if you find that this takes long and is frustrating. If this is the case, just step away for a while and do something else! But I think you wonâ€™t regret doing this. Youâ€™re learning good coding practices here that will ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:783,testability,test,test,783,"Hi, looks great! Ignore the tool, I think itâ€™s a bit broken. I need to figure out whatâ€™s wrong with it. The only duplicated code left is that `_prepare_weighted_dataframe` is very similar to `_prepare_dataframe`. I think you can delete `_prepare_weighted_dataframe` and just change `_prepare_dataframe` so it does `return categories, obs_tidy, categorical`. Then you can change each line like `categories, obs_tidy = _prepare_dataframe(â€¦)` to `categories, obs_tidy, _ = _prepare_dataframe(â€¦)`. Other than that, thereâ€™s only few things left:. 1. The tests without plots should contain assertions. I.e. in `test_genes_ranking()` you should do `assert np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...])` or so! 2. For the plot tests, you need to add these lines to the test file:. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L4. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L8-L13. And do each test like this (replace â€œxyzâ€ with whatever you want):. ```py. def test_xyz(image_comparer):. save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). [â€¦]. sc.pl.xyz(adata, â€¦). save_and_compare_images('xyz'). ```. This will make the tests save your plots to `scanpy/tests/figures` and compare them to the images in `scanpy/test/_images`. The tests will fail because `scanpy/test/_images/xyz.png` doesnâ€™t exist. You need to copy the pngs from `scanpy/tests/figures`â†’`scanpy/test/_images` and `git commit` them. 3. This needs to be fixed: https://github.com/theislab/scanpy/pull/644#discussion_r284652144. 4. I think the test data might be too large. @falexwolf do we have a recommended size for new test data? @Khalid-Usman Iâ€™m sorry if you find that this takes long and is frustrating. If this is the case, just step away for a while and do something else! But I think you wonâ€™t regret doing this. Youâ€™re learning good coding practices here that will ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:883,testability,test,tests,883,"Hi, looks great! Ignore the tool, I think itâ€™s a bit broken. I need to figure out whatâ€™s wrong with it. The only duplicated code left is that `_prepare_weighted_dataframe` is very similar to `_prepare_dataframe`. I think you can delete `_prepare_weighted_dataframe` and just change `_prepare_dataframe` so it does `return categories, obs_tidy, categorical`. Then you can change each line like `categories, obs_tidy = _prepare_dataframe(â€¦)` to `categories, obs_tidy, _ = _prepare_dataframe(â€¦)`. Other than that, thereâ€™s only few things left:. 1. The tests without plots should contain assertions. I.e. in `test_genes_ranking()` you should do `assert np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...])` or so! 2. For the plot tests, you need to add these lines to the test file:. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L4. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L8-L13. And do each test like this (replace â€œxyzâ€ with whatever you want):. ```py. def test_xyz(image_comparer):. save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). [â€¦]. sc.pl.xyz(adata, â€¦). save_and_compare_images('xyz'). ```. This will make the tests save your plots to `scanpy/tests/figures` and compare them to the images in `scanpy/test/_images`. The tests will fail because `scanpy/test/_images/xyz.png` doesnâ€™t exist. You need to copy the pngs from `scanpy/tests/figures`â†’`scanpy/test/_images` and `git commit` them. 3. This needs to be fixed: https://github.com/theislab/scanpy/pull/644#discussion_r284652144. 4. I think the test data might be too large. @falexwolf do we have a recommended size for new test data? @Khalid-Usman Iâ€™m sorry if you find that this takes long and is frustrating. If this is the case, just step away for a while and do something else! But I think you wonâ€™t regret doing this. Youâ€™re learning good coding practices here that will ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1001,testability,test,tests,1001,"looks great! Ignore the tool, I think itâ€™s a bit broken. I need to figure out whatâ€™s wrong with it. The only duplicated code left is that `_prepare_weighted_dataframe` is very similar to `_prepare_dataframe`. I think you can delete `_prepare_weighted_dataframe` and just change `_prepare_dataframe` so it does `return categories, obs_tidy, categorical`. Then you can change each line like `categories, obs_tidy = _prepare_dataframe(â€¦)` to `categories, obs_tidy, _ = _prepare_dataframe(â€¦)`. Other than that, thereâ€™s only few things left:. 1. The tests without plots should contain assertions. I.e. in `test_genes_ranking()` you should do `assert np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...])` or so! 2. For the plot tests, you need to add these lines to the test file:. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L4. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L8-L13. And do each test like this (replace â€œxyzâ€ with whatever you want):. ```py. def test_xyz(image_comparer):. save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). [â€¦]. sc.pl.xyz(adata, â€¦). save_and_compare_images('xyz'). ```. This will make the tests save your plots to `scanpy/tests/figures` and compare them to the images in `scanpy/test/_images`. The tests will fail because `scanpy/test/_images/xyz.png` doesnâ€™t exist. You need to copy the pngs from `scanpy/tests/figures`â†’`scanpy/test/_images` and `git commit` them. 3. This needs to be fixed: https://github.com/theislab/scanpy/pull/644#discussion_r284652144. 4. I think the test data might be too large. @falexwolf do we have a recommended size for new test data? @Khalid-Usman Iâ€™m sorry if you find that this takes long and is frustrating. If this is the case, just step away for a while and do something else! But I think you wonâ€™t regret doing this. Youâ€™re learning good coding practices here that will come",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1044,testability,test,test,1044," a bit broken. I need to figure out whatâ€™s wrong with it. The only duplicated code left is that `_prepare_weighted_dataframe` is very similar to `_prepare_dataframe`. I think you can delete `_prepare_weighted_dataframe` and just change `_prepare_dataframe` so it does `return categories, obs_tidy, categorical`. Then you can change each line like `categories, obs_tidy = _prepare_dataframe(â€¦)` to `categories, obs_tidy, _ = _prepare_dataframe(â€¦)`. Other than that, thereâ€™s only few things left:. 1. The tests without plots should contain assertions. I.e. in `test_genes_ranking()` you should do `assert np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...])` or so! 2. For the plot tests, you need to add these lines to the test file:. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L4. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L8-L13. And do each test like this (replace â€œxyzâ€ with whatever you want):. ```py. def test_xyz(image_comparer):. save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). [â€¦]. sc.pl.xyz(adata, â€¦). save_and_compare_images('xyz'). ```. This will make the tests save your plots to `scanpy/tests/figures` and compare them to the images in `scanpy/test/_images`. The tests will fail because `scanpy/test/_images/xyz.png` doesnâ€™t exist. You need to copy the pngs from `scanpy/tests/figures`â†’`scanpy/test/_images` and `git commit` them. 3. This needs to be fixed: https://github.com/theislab/scanpy/pull/644#discussion_r284652144. 4. I think the test data might be too large. @falexwolf do we have a recommended size for new test data? @Khalid-Usman Iâ€™m sorry if you find that this takes long and is frustrating. If this is the case, just step away for a while and do something else! But I think you wonâ€™t regret doing this. Youâ€™re learning good coding practices here that will come in handy in the future, I promise! Thank ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1282,testability,test,tests,1282,"out whatâ€™s wrong with it. The only duplicated code left is that `_prepare_weighted_dataframe` is very similar to `_prepare_dataframe`. I think you can delete `_prepare_weighted_dataframe` and just change `_prepare_dataframe` so it does `return categories, obs_tidy, categorical`. Then you can change each line like `categories, obs_tidy = _prepare_dataframe(â€¦)` to `categories, obs_tidy, _ = _prepare_dataframe(â€¦)`. Other than that, thereâ€™s only few things left:. 1. The tests without plots should contain assertions. I.e. in `test_genes_ranking()` you should do `assert np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...])` or so! 2. For the plot tests, you need to add these lines to the test file:. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L4. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L8-L13. And do each test like this (replace â€œxyzâ€ with whatever you want):. ```py. def test_xyz(image_comparer):. save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). [â€¦]. sc.pl.xyz(adata, â€¦). save_and_compare_images('xyz'). ```. This will make the tests save your plots to `scanpy/tests/figures` and compare them to the images in `scanpy/test/_images`. The tests will fail because `scanpy/test/_images/xyz.png` doesnâ€™t exist. You need to copy the pngs from `scanpy/tests/figures`â†’`scanpy/test/_images` and `git commit` them. 3. This needs to be fixed: https://github.com/theislab/scanpy/pull/644#discussion_r284652144. 4. I think the test data might be too large. @falexwolf do we have a recommended size for new test data? @Khalid-Usman Iâ€™m sorry if you find that this takes long and is frustrating. If this is the case, just step away for a while and do something else! But I think you wonâ€™t regret doing this. Youâ€™re learning good coding practices here that will come in handy in the future, I promise! Thank you for your contribution :tada:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1315,testability,test,tests,1315,"out whatâ€™s wrong with it. The only duplicated code left is that `_prepare_weighted_dataframe` is very similar to `_prepare_dataframe`. I think you can delete `_prepare_weighted_dataframe` and just change `_prepare_dataframe` so it does `return categories, obs_tidy, categorical`. Then you can change each line like `categories, obs_tidy = _prepare_dataframe(â€¦)` to `categories, obs_tidy, _ = _prepare_dataframe(â€¦)`. Other than that, thereâ€™s only few things left:. 1. The tests without plots should contain assertions. I.e. in `test_genes_ranking()` you should do `assert np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...])` or so! 2. For the plot tests, you need to add these lines to the test file:. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L4. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L8-L13. And do each test like this (replace â€œxyzâ€ with whatever you want):. ```py. def test_xyz(image_comparer):. save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). [â€¦]. sc.pl.xyz(adata, â€¦). save_and_compare_images('xyz'). ```. This will make the tests save your plots to `scanpy/tests/figures` and compare them to the images in `scanpy/test/_images`. The tests will fail because `scanpy/test/_images/xyz.png` doesnâ€™t exist. You need to copy the pngs from `scanpy/tests/figures`â†’`scanpy/test/_images` and `git commit` them. 3. This needs to be fixed: https://github.com/theislab/scanpy/pull/644#discussion_r284652144. 4. I think the test data might be too large. @falexwolf do we have a recommended size for new test data? @Khalid-Usman Iâ€™m sorry if you find that this takes long and is frustrating. If this is the case, just step away for a while and do something else! But I think you wonâ€™t regret doing this. Youâ€™re learning good coding practices here that will come in handy in the future, I promise! Thank you for your contribution :tada:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1372,testability,test,test,1372,"out whatâ€™s wrong with it. The only duplicated code left is that `_prepare_weighted_dataframe` is very similar to `_prepare_dataframe`. I think you can delete `_prepare_weighted_dataframe` and just change `_prepare_dataframe` so it does `return categories, obs_tidy, categorical`. Then you can change each line like `categories, obs_tidy = _prepare_dataframe(â€¦)` to `categories, obs_tidy, _ = _prepare_dataframe(â€¦)`. Other than that, thereâ€™s only few things left:. 1. The tests without plots should contain assertions. I.e. in `test_genes_ranking()` you should do `assert np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...])` or so! 2. For the plot tests, you need to add these lines to the test file:. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L4. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L8-L13. And do each test like this (replace â€œxyzâ€ with whatever you want):. ```py. def test_xyz(image_comparer):. save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). [â€¦]. sc.pl.xyz(adata, â€¦). save_and_compare_images('xyz'). ```. This will make the tests save your plots to `scanpy/tests/figures` and compare them to the images in `scanpy/test/_images`. The tests will fail because `scanpy/test/_images/xyz.png` doesnâ€™t exist. You need to copy the pngs from `scanpy/tests/figures`â†’`scanpy/test/_images` and `git commit` them. 3. This needs to be fixed: https://github.com/theislab/scanpy/pull/644#discussion_r284652144. 4. I think the test data might be too large. @falexwolf do we have a recommended size for new test data? @Khalid-Usman Iâ€™m sorry if you find that this takes long and is frustrating. If this is the case, just step away for a while and do something else! But I think you wonâ€™t regret doing this. Youâ€™re learning good coding practices here that will come in handy in the future, I promise! Thank you for your contribution :tada:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1391,testability,test,tests,1391,"out whatâ€™s wrong with it. The only duplicated code left is that `_prepare_weighted_dataframe` is very similar to `_prepare_dataframe`. I think you can delete `_prepare_weighted_dataframe` and just change `_prepare_dataframe` so it does `return categories, obs_tidy, categorical`. Then you can change each line like `categories, obs_tidy = _prepare_dataframe(â€¦)` to `categories, obs_tidy, _ = _prepare_dataframe(â€¦)`. Other than that, thereâ€™s only few things left:. 1. The tests without plots should contain assertions. I.e. in `test_genes_ranking()` you should do `assert np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...])` or so! 2. For the plot tests, you need to add these lines to the test file:. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L4. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L8-L13. And do each test like this (replace â€œxyzâ€ with whatever you want):. ```py. def test_xyz(image_comparer):. save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). [â€¦]. sc.pl.xyz(adata, â€¦). save_and_compare_images('xyz'). ```. This will make the tests save your plots to `scanpy/tests/figures` and compare them to the images in `scanpy/test/_images`. The tests will fail because `scanpy/test/_images/xyz.png` doesnâ€™t exist. You need to copy the pngs from `scanpy/tests/figures`â†’`scanpy/test/_images` and `git commit` them. 3. This needs to be fixed: https://github.com/theislab/scanpy/pull/644#discussion_r284652144. 4. I think the test data might be too large. @falexwolf do we have a recommended size for new test data? @Khalid-Usman Iâ€™m sorry if you find that this takes long and is frustrating. If this is the case, just step away for a while and do something else! But I think you wonâ€™t regret doing this. Youâ€™re learning good coding practices here that will come in handy in the future, I promise! Thank you for your contribution :tada:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1423,testability,test,test,1423,"out whatâ€™s wrong with it. The only duplicated code left is that `_prepare_weighted_dataframe` is very similar to `_prepare_dataframe`. I think you can delete `_prepare_weighted_dataframe` and just change `_prepare_dataframe` so it does `return categories, obs_tidy, categorical`. Then you can change each line like `categories, obs_tidy = _prepare_dataframe(â€¦)` to `categories, obs_tidy, _ = _prepare_dataframe(â€¦)`. Other than that, thereâ€™s only few things left:. 1. The tests without plots should contain assertions. I.e. in `test_genes_ranking()` you should do `assert np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...])` or so! 2. For the plot tests, you need to add these lines to the test file:. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L4. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L8-L13. And do each test like this (replace â€œxyzâ€ with whatever you want):. ```py. def test_xyz(image_comparer):. save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). [â€¦]. sc.pl.xyz(adata, â€¦). save_and_compare_images('xyz'). ```. This will make the tests save your plots to `scanpy/tests/figures` and compare them to the images in `scanpy/test/_images`. The tests will fail because `scanpy/test/_images/xyz.png` doesnâ€™t exist. You need to copy the pngs from `scanpy/tests/figures`â†’`scanpy/test/_images` and `git commit` them. 3. This needs to be fixed: https://github.com/theislab/scanpy/pull/644#discussion_r284652144. 4. I think the test data might be too large. @falexwolf do we have a recommended size for new test data? @Khalid-Usman Iâ€™m sorry if you find that this takes long and is frustrating. If this is the case, just step away for a while and do something else! But I think you wonâ€™t regret doing this. Youâ€™re learning good coding practices here that will come in handy in the future, I promise! Thank you for your contribution :tada:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1499,testability,test,tests,1499,"out whatâ€™s wrong with it. The only duplicated code left is that `_prepare_weighted_dataframe` is very similar to `_prepare_dataframe`. I think you can delete `_prepare_weighted_dataframe` and just change `_prepare_dataframe` so it does `return categories, obs_tidy, categorical`. Then you can change each line like `categories, obs_tidy = _prepare_dataframe(â€¦)` to `categories, obs_tidy, _ = _prepare_dataframe(â€¦)`. Other than that, thereâ€™s only few things left:. 1. The tests without plots should contain assertions. I.e. in `test_genes_ranking()` you should do `assert np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...])` or so! 2. For the plot tests, you need to add these lines to the test file:. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L4. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L8-L13. And do each test like this (replace â€œxyzâ€ with whatever you want):. ```py. def test_xyz(image_comparer):. save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). [â€¦]. sc.pl.xyz(adata, â€¦). save_and_compare_images('xyz'). ```. This will make the tests save your plots to `scanpy/tests/figures` and compare them to the images in `scanpy/test/_images`. The tests will fail because `scanpy/test/_images/xyz.png` doesnâ€™t exist. You need to copy the pngs from `scanpy/tests/figures`â†’`scanpy/test/_images` and `git commit` them. 3. This needs to be fixed: https://github.com/theislab/scanpy/pull/644#discussion_r284652144. 4. I think the test data might be too large. @falexwolf do we have a recommended size for new test data? @Khalid-Usman Iâ€™m sorry if you find that this takes long and is frustrating. If this is the case, just step away for a while and do something else! But I think you wonâ€™t regret doing this. Youâ€™re learning good coding practices here that will come in handy in the future, I promise! Thank you for your contribution :tada:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1522,testability,test,test,1522,"out whatâ€™s wrong with it. The only duplicated code left is that `_prepare_weighted_dataframe` is very similar to `_prepare_dataframe`. I think you can delete `_prepare_weighted_dataframe` and just change `_prepare_dataframe` so it does `return categories, obs_tidy, categorical`. Then you can change each line like `categories, obs_tidy = _prepare_dataframe(â€¦)` to `categories, obs_tidy, _ = _prepare_dataframe(â€¦)`. Other than that, thereâ€™s only few things left:. 1. The tests without plots should contain assertions. I.e. in `test_genes_ranking()` you should do `assert np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...])` or so! 2. For the plot tests, you need to add these lines to the test file:. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L4. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L8-L13. And do each test like this (replace â€œxyzâ€ with whatever you want):. ```py. def test_xyz(image_comparer):. save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). [â€¦]. sc.pl.xyz(adata, â€¦). save_and_compare_images('xyz'). ```. This will make the tests save your plots to `scanpy/tests/figures` and compare them to the images in `scanpy/test/_images`. The tests will fail because `scanpy/test/_images/xyz.png` doesnâ€™t exist. You need to copy the pngs from `scanpy/tests/figures`â†’`scanpy/test/_images` and `git commit` them. 3. This needs to be fixed: https://github.com/theislab/scanpy/pull/644#discussion_r284652144. 4. I think the test data might be too large. @falexwolf do we have a recommended size for new test data? @Khalid-Usman Iâ€™m sorry if you find that this takes long and is frustrating. If this is the case, just step away for a while and do something else! But I think you wonâ€™t regret doing this. Youâ€™re learning good coding practices here that will come in handy in the future, I promise! Thank you for your contribution :tada:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1668,testability,test,test,1668,"out whatâ€™s wrong with it. The only duplicated code left is that `_prepare_weighted_dataframe` is very similar to `_prepare_dataframe`. I think you can delete `_prepare_weighted_dataframe` and just change `_prepare_dataframe` so it does `return categories, obs_tidy, categorical`. Then you can change each line like `categories, obs_tidy = _prepare_dataframe(â€¦)` to `categories, obs_tidy, _ = _prepare_dataframe(â€¦)`. Other than that, thereâ€™s only few things left:. 1. The tests without plots should contain assertions. I.e. in `test_genes_ranking()` you should do `assert np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...])` or so! 2. For the plot tests, you need to add these lines to the test file:. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L4. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L8-L13. And do each test like this (replace â€œxyzâ€ with whatever you want):. ```py. def test_xyz(image_comparer):. save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). [â€¦]. sc.pl.xyz(adata, â€¦). save_and_compare_images('xyz'). ```. This will make the tests save your plots to `scanpy/tests/figures` and compare them to the images in `scanpy/test/_images`. The tests will fail because `scanpy/test/_images/xyz.png` doesnâ€™t exist. You need to copy the pngs from `scanpy/tests/figures`â†’`scanpy/test/_images` and `git commit` them. 3. This needs to be fixed: https://github.com/theislab/scanpy/pull/644#discussion_r284652144. 4. I think the test data might be too large. @falexwolf do we have a recommended size for new test data? @Khalid-Usman Iâ€™m sorry if you find that this takes long and is frustrating. If this is the case, just step away for a while and do something else! But I think you wonâ€™t regret doing this. Youâ€™re learning good coding practices here that will come in handy in the future, I promise! Thank you for your contribution :tada:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1747,testability,test,test,1747,"out whatâ€™s wrong with it. The only duplicated code left is that `_prepare_weighted_dataframe` is very similar to `_prepare_dataframe`. I think you can delete `_prepare_weighted_dataframe` and just change `_prepare_dataframe` so it does `return categories, obs_tidy, categorical`. Then you can change each line like `categories, obs_tidy = _prepare_dataframe(â€¦)` to `categories, obs_tidy, _ = _prepare_dataframe(â€¦)`. Other than that, thereâ€™s only few things left:. 1. The tests without plots should contain assertions. I.e. in `test_genes_ranking()` you should do `assert np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...])` or so! 2. For the plot tests, you need to add these lines to the test file:. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L4. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L8-L13. And do each test like this (replace â€œxyzâ€ with whatever you want):. ```py. def test_xyz(image_comparer):. save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). [â€¦]. sc.pl.xyz(adata, â€¦). save_and_compare_images('xyz'). ```. This will make the tests save your plots to `scanpy/tests/figures` and compare them to the images in `scanpy/test/_images`. The tests will fail because `scanpy/test/_images/xyz.png` doesnâ€™t exist. You need to copy the pngs from `scanpy/tests/figures`â†’`scanpy/test/_images` and `git commit` them. 3. This needs to be fixed: https://github.com/theislab/scanpy/pull/644#discussion_r284652144. 4. I think the test data might be too large. @falexwolf do we have a recommended size for new test data? @Khalid-Usman Iâ€™m sorry if you find that this takes long and is frustrating. If this is the case, just step away for a while and do something else! But I think you wonâ€™t regret doing this. Youâ€™re learning good coding practices here that will come in handy in the future, I promise! Thank you for your contribution :tada:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:28,usability,tool,tool,28,"Hi, looks great! Ignore the tool, I think itâ€™s a bit broken. I need to figure out whatâ€™s wrong with it. The only duplicated code left is that `_prepare_weighted_dataframe` is very similar to `_prepare_dataframe`. I think you can delete `_prepare_weighted_dataframe` and just change `_prepare_dataframe` so it does `return categories, obs_tidy, categorical`. Then you can change each line like `categories, obs_tidy = _prepare_dataframe(â€¦)` to `categories, obs_tidy, _ = _prepare_dataframe(â€¦)`. Other than that, thereâ€™s only few things left:. 1. The tests without plots should contain assertions. I.e. in `test_genes_ranking()` you should do `assert np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...])` or so! 2. For the plot tests, you need to add these lines to the test file:. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L4. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L8-L13. And do each test like this (replace â€œxyzâ€ with whatever you want):. ```py. def test_xyz(image_comparer):. save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). [â€¦]. sc.pl.xyz(adata, â€¦). save_and_compare_images('xyz'). ```. This will make the tests save your plots to `scanpy/tests/figures` and compare them to the images in `scanpy/test/_images`. The tests will fail because `scanpy/test/_images/xyz.png` doesnâ€™t exist. You need to copy the pngs from `scanpy/tests/figures`â†’`scanpy/test/_images` and `git commit` them. 3. This needs to be fixed: https://github.com/theislab/scanpy/pull/644#discussion_r284652144. 4. I think the test data might be too large. @falexwolf do we have a recommended size for new test data? @Khalid-Usman Iâ€™m sorry if you find that this takes long and is frustrating. If this is the case, just step away for a while and do something else! But I think you wonâ€™t regret doing this. Youâ€™re learning good coding practices here that will ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1954,usability,learn,learning,1954,"out whatâ€™s wrong with it. The only duplicated code left is that `_prepare_weighted_dataframe` is very similar to `_prepare_dataframe`. I think you can delete `_prepare_weighted_dataframe` and just change `_prepare_dataframe` so it does `return categories, obs_tidy, categorical`. Then you can change each line like `categories, obs_tidy = _prepare_dataframe(â€¦)` to `categories, obs_tidy, _ = _prepare_dataframe(â€¦)`. Other than that, thereâ€™s only few things left:. 1. The tests without plots should contain assertions. I.e. in `test_genes_ranking()` you should do `assert np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...])` or so! 2. For the plot tests, you need to add these lines to the test file:. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L4. https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L8-L13. And do each test like this (replace â€œxyzâ€ with whatever you want):. ```py. def test_xyz(image_comparer):. save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). [â€¦]. sc.pl.xyz(adata, â€¦). save_and_compare_images('xyz'). ```. This will make the tests save your plots to `scanpy/tests/figures` and compare them to the images in `scanpy/test/_images`. The tests will fail because `scanpy/test/_images/xyz.png` doesnâ€™t exist. You need to copy the pngs from `scanpy/tests/figures`â†’`scanpy/test/_images` and `git commit` them. 3. This needs to be fixed: https://github.com/theislab/scanpy/pull/644#discussion_r284652144. 4. I think the test data might be too large. @falexwolf do we have a recommended size for new test data? @Khalid-Usman Iâ€™m sorry if you find that this takes long and is frustrating. If this is the case, just step away for a while and do something else! But I think you wonâ€™t regret doing this. Youâ€™re learning good coding practices here that will come in handy in the future, I promise! Thank you for your contribution :tada:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:206,security,modif,modified,206,"I was using scanpy for single cell dataset, but I have sampled data with. weights instead of using all rows. So I found scanpy don't handle it and i. was using genes ranking and some plots from scanpy so I modified the code. of scanpy to support weighted sampled data where each data point has some. non-zero weight. On Tue, May 21, 2019 at 1:22 AM MalteDLuecken <notifications@github.com>. wrote:. > Hi,. > Can I ask what weighted sampling is? And what it is used for? >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOC6K57EMJHO6YMKTATPWLM3XA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVZQKNI#issuecomment-494077237>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOGLFSGTBRORNQPU7JLPWLM3XANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:876,security,auth,auth,876,"I was using scanpy for single cell dataset, but I have sampled data with. weights instead of using all rows. So I found scanpy don't handle it and i. was using genes ranking and some plots from scanpy so I modified the code. of scanpy to support weighted sampled data where each data point has some. non-zero weight. On Tue, May 21, 2019 at 1:22 AM MalteDLuecken <notifications@github.com>. wrote:. > Hi,. > Can I ask what weighted sampling is? And what it is used for? >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOC6K57EMJHO6YMKTATPWLM3XA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVZQKNI#issuecomment-494077237>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOGLFSGTBRORNQPU7JLPWLM3XANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:238,usability,support,support,238,"I was using scanpy for single cell dataset, but I have sampled data with. weights instead of using all rows. So I found scanpy don't handle it and i. was using genes ranking and some plots from scanpy so I modified the code. of scanpy to support weighted sampled data where each data point has some. non-zero weight. On Tue, May 21, 2019 at 1:22 AM MalteDLuecken <notifications@github.com>. wrote:. > Hi,. > Can I ask what weighted sampling is? And what it is used for? >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOC6K57EMJHO6YMKTATPWLM3XA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVZQKNI#issuecomment-494077237>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOGLFSGTBRORNQPU7JLPWLM3XANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:14,testability,understand,understand,14,I don't quite understand what sampled data with weights on the rows are. How do you weight individual cells in a dataset? What do weights like this mean?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:128,performance,perform,perform,128,"E.g. if your original input matrix has 1,000,000 number of cells and 100. genes. You don't want to process all rows, so you can perform either. uniform sampling or weighted sampling on the data. I have performed. weighted sampling and sampled e.g. only 1,000 rows then each rows will have. a weight. On Tue, May 21, 2019 at 2:19 AM MalteDLuecken <notifications@github.com>. wrote:. > I don't quite understand what sampled data with weights on the rows are. > How do you weight individual cells in a dataset? What do weights like this. > mean? >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGODJ5CPJTB4HKVOI4M3PWLTUXA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVZVDRQ#issuecomment-494096838>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOAGG3AY6DAVVZREM3TPWLTUXANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:202,performance,perform,performed,202,"E.g. if your original input matrix has 1,000,000 number of cells and 100. genes. You don't want to process all rows, so you can perform either. uniform sampling or weighted sampling on the data. I have performed. weighted sampling and sampled e.g. only 1,000 rows then each rows will have. a weight. On Tue, May 21, 2019 at 2:19 AM MalteDLuecken <notifications@github.com>. wrote:. > I don't quite understand what sampled data with weights on the rows are. > How do you weight individual cells in a dataset? What do weights like this. > mean? >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGODJ5CPJTB4HKVOI4M3PWLTUXA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVZVDRQ#issuecomment-494096838>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOAGG3AY6DAVVZREM3TPWLTUXANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:22,safety,input,input,22,"E.g. if your original input matrix has 1,000,000 number of cells and 100. genes. You don't want to process all rows, so you can perform either. uniform sampling or weighted sampling on the data. I have performed. weighted sampling and sampled e.g. only 1,000 rows then each rows will have. a weight. On Tue, May 21, 2019 at 2:19 AM MalteDLuecken <notifications@github.com>. wrote:. > I don't quite understand what sampled data with weights on the rows are. > How do you weight individual cells in a dataset? What do weights like this. > mean? >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGODJ5CPJTB4HKVOI4M3PWLTUXA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVZVDRQ#issuecomment-494096838>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOAGG3AY6DAVVZREM3TPWLTUXANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:949,security,auth,auth,949,"E.g. if your original input matrix has 1,000,000 number of cells and 100. genes. You don't want to process all rows, so you can perform either. uniform sampling or weighted sampling on the data. I have performed. weighted sampling and sampled e.g. only 1,000 rows then each rows will have. a weight. On Tue, May 21, 2019 at 2:19 AM MalteDLuecken <notifications@github.com>. wrote:. > I don't quite understand what sampled data with weights on the rows are. > How do you weight individual cells in a dataset? What do weights like this. > mean? >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGODJ5CPJTB4HKVOI4M3PWLTUXA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVZVDRQ#issuecomment-494096838>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOAGG3AY6DAVVZREM3TPWLTUXANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:398,testability,understand,understand,398,"E.g. if your original input matrix has 1,000,000 number of cells and 100. genes. You don't want to process all rows, so you can perform either. uniform sampling or weighted sampling on the data. I have performed. weighted sampling and sampled e.g. only 1,000 rows then each rows will have. a weight. On Tue, May 21, 2019 at 2:19 AM MalteDLuecken <notifications@github.com>. wrote:. > I don't quite understand what sampled data with weights on the rows are. > How do you weight individual cells in a dataset? What do weights like this. > mean? >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGODJ5CPJTB4HKVOI4M3PWLTUXA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVZVDRQ#issuecomment-494096838>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOAGG3AY6DAVVZREM3TPWLTUXANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:22,usability,input,input,22,"E.g. if your original input matrix has 1,000,000 number of cells and 100. genes. You don't want to process all rows, so you can perform either. uniform sampling or weighted sampling on the data. I have performed. weighted sampling and sampled e.g. only 1,000 rows then each rows will have. a weight. On Tue, May 21, 2019 at 2:19 AM MalteDLuecken <notifications@github.com>. wrote:. > I don't quite understand what sampled data with weights on the rows are. > How do you weight individual cells in a dataset? What do weights like this. > mean? >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGODJ5CPJTB4HKVOI4M3PWLTUXA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVZVDRQ#issuecomment-494096838>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOAGG3AY6DAVVZREM3TPWLTUXANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:128,usability,perform,perform,128,"E.g. if your original input matrix has 1,000,000 number of cells and 100. genes. You don't want to process all rows, so you can perform either. uniform sampling or weighted sampling on the data. I have performed. weighted sampling and sampled e.g. only 1,000 rows then each rows will have. a weight. On Tue, May 21, 2019 at 2:19 AM MalteDLuecken <notifications@github.com>. wrote:. > I don't quite understand what sampled data with weights on the rows are. > How do you weight individual cells in a dataset? What do weights like this. > mean? >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGODJ5CPJTB4HKVOI4M3PWLTUXA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVZVDRQ#issuecomment-494096838>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOAGG3AY6DAVVZREM3TPWLTUXANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:202,usability,perform,performed,202,"E.g. if your original input matrix has 1,000,000 number of cells and 100. genes. You don't want to process all rows, so you can perform either. uniform sampling or weighted sampling on the data. I have performed. weighted sampling and sampled e.g. only 1,000 rows then each rows will have. a weight. On Tue, May 21, 2019 at 2:19 AM MalteDLuecken <notifications@github.com>. wrote:. > I don't quite understand what sampled data with weights on the rows are. > How do you weight individual cells in a dataset? What do weights like this. > mean? >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGODJ5CPJTB4HKVOI4M3PWLTUXA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVZVDRQ#issuecomment-494096838>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOAGG3AY6DAVVZREM3TPWLTUXANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:20,deployability,updat,updated,20,"Hi Philipp,. I have updated accordingly, again no issue but the duplication and i. analysed most of them are from previous code. Regards,. Khalid. On Mon, May 20, 2019 at 5:23 PM Philipp A. <notifications@github.com> wrote:. > Hi, looks great! >. > The only duplicated code left is that _prepare_weighted_dataframe is very. > similar to _prepare_dataframe. I think you can delete. > _prepare_weighted_dataframe and just change _prepare_dataframe so it does return. > categories, obs_tidy, categorical. Then you can change each line like categories,. > obs_tidy = _prepare_dataframe(â€¦) to categories, obs_tidy, _ =. > _prepare_dataframe(â€¦). >. > Other than that, thereâ€™s only few things left:. >. > 1. >. > The tests without plots should contain assertions. I.e. in. > test_genes_ranking() you should do assert. > np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...]) or. > so! > 2. >. > For the plot tests, you need to add these lines to the test file:. >. >. > https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L13. >. > And do each test like this (replace â€œxyzâ€ with whatever you want):. >. > def test_xyz(image_comparer):. >. > save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). >. > [â€¦]. >. > sc.pl.xyz(adata, â€¦). >. > save_and_compare_images('xyz'). >. > This will make the tests save your plots to scanpy/tests/figures and. > compare them to the images in scanpy/test/_images. The tests will fail. > because scanpy/test/_images/xyz.png doesnâ€™t exist. You need to copy. > the pngs from scanpy/tests/figuresâ†’scanpy/test/_images and git commit. > them. > 3. >. > This needs to be fixed: #644 (comment). > <https://github.com/theislab/scanpy/pull/644#discussion_r284652144>. > 4. >. > I think the test data might be too large. @falexwolf. > <https://github.com/falexwolf> do we have a recommended size for new. > test data? >. > @Khalid-Usman <https://github.com/Khalid-Usman> Iâ€™m sorry if you find. > that th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:737,deployability,contain,contain,737,"Hi Philipp,. I have updated accordingly, again no issue but the duplication and i. analysed most of them are from previous code. Regards,. Khalid. On Mon, May 20, 2019 at 5:23 PM Philipp A. <notifications@github.com> wrote:. > Hi, looks great! >. > The only duplicated code left is that _prepare_weighted_dataframe is very. > similar to _prepare_dataframe. I think you can delete. > _prepare_weighted_dataframe and just change _prepare_dataframe so it does return. > categories, obs_tidy, categorical. Then you can change each line like categories,. > obs_tidy = _prepare_dataframe(â€¦) to categories, obs_tidy, _ =. > _prepare_dataframe(â€¦). >. > Other than that, thereâ€™s only few things left:. >. > 1. >. > The tests without plots should contain assertions. I.e. in. > test_genes_ranking() you should do assert. > np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...]) or. > so! > 2. >. > For the plot tests, you need to add these lines to the test file:. >. >. > https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L13. >. > And do each test like this (replace â€œxyzâ€ with whatever you want):. >. > def test_xyz(image_comparer):. >. > save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). >. > [â€¦]. >. > sc.pl.xyz(adata, â€¦). >. > save_and_compare_images('xyz'). >. > This will make the tests save your plots to scanpy/tests/figures and. > compare them to the images in scanpy/test/_images. The tests will fail. > because scanpy/test/_images/xyz.png doesnâ€™t exist. You need to copy. > the pngs from scanpy/tests/figuresâ†’scanpy/test/_images and git commit. > them. > 3. >. > This needs to be fixed: #644 (comment). > <https://github.com/theislab/scanpy/pull/644#discussion_r284652144>. > 4. >. > I think the test data might be too large. @falexwolf. > <https://github.com/falexwolf> do we have a recommended size for new. > test data? >. > @Khalid-Usman <https://github.com/Khalid-Usman> Iâ€™m sorry if you find. > that th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1487,deployability,fail,fail,1487,"categorical. Then you can change each line like categories,. > obs_tidy = _prepare_dataframe(â€¦) to categories, obs_tidy, _ =. > _prepare_dataframe(â€¦). >. > Other than that, thereâ€™s only few things left:. >. > 1. >. > The tests without plots should contain assertions. I.e. in. > test_genes_ranking() you should do assert. > np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...]) or. > so! > 2. >. > For the plot tests, you need to add these lines to the test file:. >. >. > https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L13. >. > And do each test like this (replace â€œxyzâ€ with whatever you want):. >. > def test_xyz(image_comparer):. >. > save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). >. > [â€¦]. >. > sc.pl.xyz(adata, â€¦). >. > save_and_compare_images('xyz'). >. > This will make the tests save your plots to scanpy/tests/figures and. > compare them to the images in scanpy/test/_images. The tests will fail. > because scanpy/test/_images/xyz.png doesnâ€™t exist. You need to copy. > the pngs from scanpy/tests/figuresâ†’scanpy/test/_images and git commit. > them. > 3. >. > This needs to be fixed: #644 (comment). > <https://github.com/theislab/scanpy/pull/644#discussion_r284652144>. > 4. >. > I think the test data might be too large. @falexwolf. > <https://github.com/falexwolf> do we have a recommended size for new. > test data? >. > @Khalid-Usman <https://github.com/Khalid-Usman> Iâ€™m sorry if you find. > that this takes long and is frustrating. If this is the case, just step. > away for a while and do something else! But I think you wonâ€™t regret doing. > this. Youâ€™re learning good coding practices here that will come in handy in. > the future, I promise! >. > Thank you for your contribution ðŸŽ‰. >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABR",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:452,reliability,doe,does,452,"Hi Philipp,. I have updated accordingly, again no issue but the duplication and i. analysed most of them are from previous code. Regards,. Khalid. On Mon, May 20, 2019 at 5:23 PM Philipp A. <notifications@github.com> wrote:. > Hi, looks great! >. > The only duplicated code left is that _prepare_weighted_dataframe is very. > similar to _prepare_dataframe. I think you can delete. > _prepare_weighted_dataframe and just change _prepare_dataframe so it does return. > categories, obs_tidy, categorical. Then you can change each line like categories,. > obs_tidy = _prepare_dataframe(â€¦) to categories, obs_tidy, _ =. > _prepare_dataframe(â€¦). >. > Other than that, thereâ€™s only few things left:. >. > 1. >. > The tests without plots should contain assertions. I.e. in. > test_genes_ranking() you should do assert. > np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...]) or. > so! > 2. >. > For the plot tests, you need to add these lines to the test file:. >. >. > https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L13. >. > And do each test like this (replace â€œxyzâ€ with whatever you want):. >. > def test_xyz(image_comparer):. >. > save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). >. > [â€¦]. >. > sc.pl.xyz(adata, â€¦). >. > save_and_compare_images('xyz'). >. > This will make the tests save your plots to scanpy/tests/figures and. > compare them to the images in scanpy/test/_images. The tests will fail. > because scanpy/test/_images/xyz.png doesnâ€™t exist. You need to copy. > the pngs from scanpy/tests/figuresâ†’scanpy/test/_images and git commit. > them. > 3. >. > This needs to be fixed: #644 (comment). > <https://github.com/theislab/scanpy/pull/644#discussion_r284652144>. > 4. >. > I think the test data might be too large. @falexwolf. > <https://github.com/falexwolf> do we have a recommended size for new. > test data? >. > @Khalid-Usman <https://github.com/Khalid-Usman> Iâ€™m sorry if you find. > that th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1487,reliability,fail,fail,1487,"categorical. Then you can change each line like categories,. > obs_tidy = _prepare_dataframe(â€¦) to categories, obs_tidy, _ =. > _prepare_dataframe(â€¦). >. > Other than that, thereâ€™s only few things left:. >. > 1. >. > The tests without plots should contain assertions. I.e. in. > test_genes_ranking() you should do assert. > np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...]) or. > so! > 2. >. > For the plot tests, you need to add these lines to the test file:. >. >. > https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L13. >. > And do each test like this (replace â€œxyzâ€ with whatever you want):. >. > def test_xyz(image_comparer):. >. > save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). >. > [â€¦]. >. > sc.pl.xyz(adata, â€¦). >. > save_and_compare_images('xyz'). >. > This will make the tests save your plots to scanpy/tests/figures and. > compare them to the images in scanpy/test/_images. The tests will fail. > because scanpy/test/_images/xyz.png doesnâ€™t exist. You need to copy. > the pngs from scanpy/tests/figuresâ†’scanpy/test/_images and git commit. > them. > 3. >. > This needs to be fixed: #644 (comment). > <https://github.com/theislab/scanpy/pull/644#discussion_r284652144>. > 4. >. > I think the test data might be too large. @falexwolf. > <https://github.com/falexwolf> do we have a recommended size for new. > test data? >. > @Khalid-Usman <https://github.com/Khalid-Usman> Iâ€™m sorry if you find. > that this takes long and is frustrating. If this is the case, just step. > away for a while and do something else! But I think you wonâ€™t regret doing. > this. Youâ€™re learning good coding practices here that will come in handy in. > the future, I promise! >. > Thank you for your contribution ðŸŽ‰. >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABR",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1531,reliability,doe,doesn,1531,"ke categories,. > obs_tidy = _prepare_dataframe(â€¦) to categories, obs_tidy, _ =. > _prepare_dataframe(â€¦). >. > Other than that, thereâ€™s only few things left:. >. > 1. >. > The tests without plots should contain assertions. I.e. in. > test_genes_ranking() you should do assert. > np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...]) or. > so! > 2. >. > For the plot tests, you need to add these lines to the test file:. >. >. > https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L13. >. > And do each test like this (replace â€œxyzâ€ with whatever you want):. >. > def test_xyz(image_comparer):. >. > save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). >. > [â€¦]. >. > sc.pl.xyz(adata, â€¦). >. > save_and_compare_images('xyz'). >. > This will make the tests save your plots to scanpy/tests/figures and. > compare them to the images in scanpy/test/_images. The tests will fail. > because scanpy/test/_images/xyz.png doesnâ€™t exist. You need to copy. > the pngs from scanpy/tests/figuresâ†’scanpy/test/_images and git commit. > them. > 3. >. > This needs to be fixed: #644 (comment). > <https://github.com/theislab/scanpy/pull/644#discussion_r284652144>. > 4. >. > I think the test data might be too large. @falexwolf. > <https://github.com/falexwolf> do we have a recommended size for new. > test data? >. > @Khalid-Usman <https://github.com/Khalid-Usman> Iâ€™m sorry if you find. > that this takes long and is frustrating. If this is the case, just step. > away for a while and do something else! But I think you wonâ€™t regret doing. > this. Youâ€™re learning good coding practices here that will come in handy in. > the future, I promise! >. > Thank you for your contribution ðŸŽ‰. >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGODHLQPIDWZGLGTKXGLPWJUZNA5CNFSM4HMZ5G72YY3P",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:2180,reliability,pra,practices,2180,"ons. I.e. in. > test_genes_ranking() you should do assert. > np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...]) or. > so! > 2. >. > For the plot tests, you need to add these lines to the test file:. >. >. > https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L13. >. > And do each test like this (replace â€œxyzâ€ with whatever you want):. >. > def test_xyz(image_comparer):. >. > save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). >. > [â€¦]. >. > sc.pl.xyz(adata, â€¦). >. > save_and_compare_images('xyz'). >. > This will make the tests save your plots to scanpy/tests/figures and. > compare them to the images in scanpy/test/_images. The tests will fail. > because scanpy/test/_images/xyz.png doesnâ€™t exist. You need to copy. > the pngs from scanpy/tests/figuresâ†’scanpy/test/_images and git commit. > them. > 3. >. > This needs to be fixed: #644 (comment). > <https://github.com/theislab/scanpy/pull/644#discussion_r284652144>. > 4. >. > I think the test data might be too large. @falexwolf. > <https://github.com/falexwolf> do we have a recommended size for new. > test data? >. > @Khalid-Usman <https://github.com/Khalid-Usman> Iâ€™m sorry if you find. > that this takes long and is frustrating. If this is the case, just step. > away for a while and do something else! But I think you wonâ€™t regret doing. > this. Youâ€™re learning good coding practices here that will come in handy in. > the future, I promise! >. > Thank you for your contribution ðŸŽ‰. >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGODHLQPIDWZGLGTKXGLPWJUZNA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVYG3VA#issuecomment-493907412>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOG73UUYPXGY7UICKODPWJUZNANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:20,safety,updat,updated,20,"Hi Philipp,. I have updated accordingly, again no issue but the duplication and i. analysed most of them are from previous code. Regards,. Khalid. On Mon, May 20, 2019 at 5:23 PM Philipp A. <notifications@github.com> wrote:. > Hi, looks great! >. > The only duplicated code left is that _prepare_weighted_dataframe is very. > similar to _prepare_dataframe. I think you can delete. > _prepare_weighted_dataframe and just change _prepare_dataframe so it does return. > categories, obs_tidy, categorical. Then you can change each line like categories,. > obs_tidy = _prepare_dataframe(â€¦) to categories, obs_tidy, _ =. > _prepare_dataframe(â€¦). >. > Other than that, thereâ€™s only few things left:. >. > 1. >. > The tests without plots should contain assertions. I.e. in. > test_genes_ranking() you should do assert. > np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...]) or. > so! > 2. >. > For the plot tests, you need to add these lines to the test file:. >. >. > https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L13. >. > And do each test like this (replace â€œxyzâ€ with whatever you want):. >. > def test_xyz(image_comparer):. >. > save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). >. > [â€¦]. >. > sc.pl.xyz(adata, â€¦). >. > save_and_compare_images('xyz'). >. > This will make the tests save your plots to scanpy/tests/figures and. > compare them to the images in scanpy/test/_images. The tests will fail. > because scanpy/test/_images/xyz.png doesnâ€™t exist. You need to copy. > the pngs from scanpy/tests/figuresâ†’scanpy/test/_images and git commit. > them. > 3. >. > This needs to be fixed: #644 (comment). > <https://github.com/theislab/scanpy/pull/644#discussion_r284652144>. > 4. >. > I think the test data might be too large. @falexwolf. > <https://github.com/falexwolf> do we have a recommended size for new. > test data? >. > @Khalid-Usman <https://github.com/Khalid-Usman> Iâ€™m sorry if you find. > that th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:710,safety,test,tests,710,"Hi Philipp,. I have updated accordingly, again no issue but the duplication and i. analysed most of them are from previous code. Regards,. Khalid. On Mon, May 20, 2019 at 5:23 PM Philipp A. <notifications@github.com> wrote:. > Hi, looks great! >. > The only duplicated code left is that _prepare_weighted_dataframe is very. > similar to _prepare_dataframe. I think you can delete. > _prepare_weighted_dataframe and just change _prepare_dataframe so it does return. > categories, obs_tidy, categorical. Then you can change each line like categories,. > obs_tidy = _prepare_dataframe(â€¦) to categories, obs_tidy, _ =. > _prepare_dataframe(â€¦). >. > Other than that, thereâ€™s only few things left:. >. > 1. >. > The tests without plots should contain assertions. I.e. in. > test_genes_ranking() you should do assert. > np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...]) or. > so! > 2. >. > For the plot tests, you need to add these lines to the test file:. >. >. > https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L13. >. > And do each test like this (replace â€œxyzâ€ with whatever you want):. >. > def test_xyz(image_comparer):. >. > save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). >. > [â€¦]. >. > sc.pl.xyz(adata, â€¦). >. > save_and_compare_images('xyz'). >. > This will make the tests save your plots to scanpy/tests/figures and. > compare them to the images in scanpy/test/_images. The tests will fail. > because scanpy/test/_images/xyz.png doesnâ€™t exist. You need to copy. > the pngs from scanpy/tests/figuresâ†’scanpy/test/_images and git commit. > them. > 3. >. > This needs to be fixed: #644 (comment). > <https://github.com/theislab/scanpy/pull/644#discussion_r284652144>. > 4. >. > I think the test data might be too large. @falexwolf. > <https://github.com/falexwolf> do we have a recommended size for new. > test data? >. > @Khalid-Usman <https://github.com/Khalid-Usman> Iâ€™m sorry if you find. > that th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:914,safety,test,tests,914,"Hi Philipp,. I have updated accordingly, again no issue but the duplication and i. analysed most of them are from previous code. Regards,. Khalid. On Mon, May 20, 2019 at 5:23 PM Philipp A. <notifications@github.com> wrote:. > Hi, looks great! >. > The only duplicated code left is that _prepare_weighted_dataframe is very. > similar to _prepare_dataframe. I think you can delete. > _prepare_weighted_dataframe and just change _prepare_dataframe so it does return. > categories, obs_tidy, categorical. Then you can change each line like categories,. > obs_tidy = _prepare_dataframe(â€¦) to categories, obs_tidy, _ =. > _prepare_dataframe(â€¦). >. > Other than that, thereâ€™s only few things left:. >. > 1. >. > The tests without plots should contain assertions. I.e. in. > test_genes_ranking() you should do assert. > np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...]) or. > so! > 2. >. > For the plot tests, you need to add these lines to the test file:. >. >. > https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L13. >. > And do each test like this (replace â€œxyzâ€ with whatever you want):. >. > def test_xyz(image_comparer):. >. > save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). >. > [â€¦]. >. > sc.pl.xyz(adata, â€¦). >. > save_and_compare_images('xyz'). >. > This will make the tests save your plots to scanpy/tests/figures and. > compare them to the images in scanpy/test/_images. The tests will fail. > because scanpy/test/_images/xyz.png doesnâ€™t exist. You need to copy. > the pngs from scanpy/tests/figuresâ†’scanpy/test/_images and git commit. > them. > 3. >. > This needs to be fixed: #644 (comment). > <https://github.com/theislab/scanpy/pull/644#discussion_r284652144>. > 4. >. > I think the test data might be too large. @falexwolf. > <https://github.com/falexwolf> do we have a recommended size for new. > test data? >. > @Khalid-Usman <https://github.com/Khalid-Usman> Iâ€™m sorry if you find. > that th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:956,safety,test,test,956,"Hi Philipp,. I have updated accordingly, again no issue but the duplication and i. analysed most of them are from previous code. Regards,. Khalid. On Mon, May 20, 2019 at 5:23 PM Philipp A. <notifications@github.com> wrote:. > Hi, looks great! >. > The only duplicated code left is that _prepare_weighted_dataframe is very. > similar to _prepare_dataframe. I think you can delete. > _prepare_weighted_dataframe and just change _prepare_dataframe so it does return. > categories, obs_tidy, categorical. Then you can change each line like categories,. > obs_tidy = _prepare_dataframe(â€¦) to categories, obs_tidy, _ =. > _prepare_dataframe(â€¦). >. > Other than that, thereâ€™s only few things left:. >. > 1. >. > The tests without plots should contain assertions. I.e. in. > test_genes_ranking() you should do assert. > np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...]) or. > so! > 2. >. > For the plot tests, you need to add these lines to the test file:. >. >. > https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L13. >. > And do each test like this (replace â€œxyzâ€ with whatever you want):. >. > def test_xyz(image_comparer):. >. > save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). >. > [â€¦]. >. > sc.pl.xyz(adata, â€¦). >. > save_and_compare_images('xyz'). >. > This will make the tests save your plots to scanpy/tests/figures and. > compare them to the images in scanpy/test/_images. The tests will fail. > because scanpy/test/_images/xyz.png doesnâ€™t exist. You need to copy. > the pngs from scanpy/tests/figuresâ†’scanpy/test/_images and git commit. > them. > 3. >. > This needs to be fixed: #644 (comment). > <https://github.com/theislab/scanpy/pull/644#discussion_r284652144>. > 4. >. > I think the test data might be too large. @falexwolf. > <https://github.com/falexwolf> do we have a recommended size for new. > test data? >. > @Khalid-Usman <https://github.com/Khalid-Usman> Iâ€™m sorry if you find. > that th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1064,safety,test,tests,1064,"lication and i. analysed most of them are from previous code. Regards,. Khalid. On Mon, May 20, 2019 at 5:23 PM Philipp A. <notifications@github.com> wrote:. > Hi, looks great! >. > The only duplicated code left is that _prepare_weighted_dataframe is very. > similar to _prepare_dataframe. I think you can delete. > _prepare_weighted_dataframe and just change _prepare_dataframe so it does return. > categories, obs_tidy, categorical. Then you can change each line like categories,. > obs_tidy = _prepare_dataframe(â€¦) to categories, obs_tidy, _ =. > _prepare_dataframe(â€¦). >. > Other than that, thereâ€™s only few things left:. >. > 1. >. > The tests without plots should contain assertions. I.e. in. > test_genes_ranking() you should do assert. > np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...]) or. > so! > 2. >. > For the plot tests, you need to add these lines to the test file:. >. >. > https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L13. >. > And do each test like this (replace â€œxyzâ€ with whatever you want):. >. > def test_xyz(image_comparer):. >. > save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). >. > [â€¦]. >. > sc.pl.xyz(adata, â€¦). >. > save_and_compare_images('xyz'). >. > This will make the tests save your plots to scanpy/tests/figures and. > compare them to the images in scanpy/test/_images. The tests will fail. > because scanpy/test/_images/xyz.png doesnâ€™t exist. You need to copy. > the pngs from scanpy/tests/figuresâ†’scanpy/test/_images and git commit. > them. > 3. >. > This needs to be fixed: #644 (comment). > <https://github.com/theislab/scanpy/pull/644#discussion_r284652144>. > 4. >. > I think the test data might be too large. @falexwolf. > <https://github.com/falexwolf> do we have a recommended size for new. > test data? >. > @Khalid-Usman <https://github.com/Khalid-Usman> Iâ€™m sorry if you find. > that this takes long and is frustrating. If this is the case, just step. >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1112,safety,test,test,1112,"previous code. Regards,. Khalid. On Mon, May 20, 2019 at 5:23 PM Philipp A. <notifications@github.com> wrote:. > Hi, looks great! >. > The only duplicated code left is that _prepare_weighted_dataframe is very. > similar to _prepare_dataframe. I think you can delete. > _prepare_weighted_dataframe and just change _prepare_dataframe so it does return. > categories, obs_tidy, categorical. Then you can change each line like categories,. > obs_tidy = _prepare_dataframe(â€¦) to categories, obs_tidy, _ =. > _prepare_dataframe(â€¦). >. > Other than that, thereâ€™s only few things left:. >. > 1. >. > The tests without plots should contain assertions. I.e. in. > test_genes_ranking() you should do assert. > np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...]) or. > so! > 2. >. > For the plot tests, you need to add these lines to the test file:. >. >. > https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L13. >. > And do each test like this (replace â€œxyzâ€ with whatever you want):. >. > def test_xyz(image_comparer):. >. > save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). >. > [â€¦]. >. > sc.pl.xyz(adata, â€¦). >. > save_and_compare_images('xyz'). >. > This will make the tests save your plots to scanpy/tests/figures and. > compare them to the images in scanpy/test/_images. The tests will fail. > because scanpy/test/_images/xyz.png doesnâ€™t exist. You need to copy. > the pngs from scanpy/tests/figuresâ†’scanpy/test/_images and git commit. > them. > 3. >. > This needs to be fixed: #644 (comment). > <https://github.com/theislab/scanpy/pull/644#discussion_r284652144>. > 4. >. > I think the test data might be too large. @falexwolf. > <https://github.com/falexwolf> do we have a recommended size for new. > test data? >. > @Khalid-Usman <https://github.com/Khalid-Usman> Iâ€™m sorry if you find. > that this takes long and is frustrating. If this is the case, just step. > away for a while and do something else! But I ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1368,safety,test,tests,1368,"n delete. > _prepare_weighted_dataframe and just change _prepare_dataframe so it does return. > categories, obs_tidy, categorical. Then you can change each line like categories,. > obs_tidy = _prepare_dataframe(â€¦) to categories, obs_tidy, _ =. > _prepare_dataframe(â€¦). >. > Other than that, thereâ€™s only few things left:. >. > 1. >. > The tests without plots should contain assertions. I.e. in. > test_genes_ranking() you should do assert. > np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...]) or. > so! > 2. >. > For the plot tests, you need to add these lines to the test file:. >. >. > https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L13. >. > And do each test like this (replace â€œxyzâ€ with whatever you want):. >. > def test_xyz(image_comparer):. >. > save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). >. > [â€¦]. >. > sc.pl.xyz(adata, â€¦). >. > save_and_compare_images('xyz'). >. > This will make the tests save your plots to scanpy/tests/figures and. > compare them to the images in scanpy/test/_images. The tests will fail. > because scanpy/test/_images/xyz.png doesnâ€™t exist. You need to copy. > the pngs from scanpy/tests/figuresâ†’scanpy/test/_images and git commit. > them. > 3. >. > This needs to be fixed: #644 (comment). > <https://github.com/theislab/scanpy/pull/644#discussion_r284652144>. > 4. >. > I think the test data might be too large. @falexwolf. > <https://github.com/falexwolf> do we have a recommended size for new. > test data? >. > @Khalid-Usman <https://github.com/Khalid-Usman> Iâ€™m sorry if you find. > that this takes long and is frustrating. If this is the case, just step. > away for a while and do something else! But I think you wonâ€™t regret doing. > this. Youâ€™re learning good coding practices here that will come in handy in. > the future, I promise! >. > Thank you for your contribution ðŸŽ‰. >. > â€”. > You are receiving this because you were mentioned. > Reply to this email ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1400,safety,test,tests,1400,"taframe and just change _prepare_dataframe so it does return. > categories, obs_tidy, categorical. Then you can change each line like categories,. > obs_tidy = _prepare_dataframe(â€¦) to categories, obs_tidy, _ =. > _prepare_dataframe(â€¦). >. > Other than that, thereâ€™s only few things left:. >. > 1. >. > The tests without plots should contain assertions. I.e. in. > test_genes_ranking() you should do assert. > np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...]) or. > so! > 2. >. > For the plot tests, you need to add these lines to the test file:. >. >. > https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L13. >. > And do each test like this (replace â€œxyzâ€ with whatever you want):. >. > def test_xyz(image_comparer):. >. > save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). >. > [â€¦]. >. > sc.pl.xyz(adata, â€¦). >. > save_and_compare_images('xyz'). >. > This will make the tests save your plots to scanpy/tests/figures and. > compare them to the images in scanpy/test/_images. The tests will fail. > because scanpy/test/_images/xyz.png doesnâ€™t exist. You need to copy. > the pngs from scanpy/tests/figuresâ†’scanpy/test/_images and git commit. > them. > 3. >. > This needs to be fixed: #644 (comment). > <https://github.com/theislab/scanpy/pull/644#discussion_r284652144>. > 4. >. > I think the test data might be too large. @falexwolf. > <https://github.com/falexwolf> do we have a recommended size for new. > test data? >. > @Khalid-Usman <https://github.com/Khalid-Usman> Iâ€™m sorry if you find. > that this takes long and is frustrating. If this is the case, just step. > away for a while and do something else! But I think you wonâ€™t regret doing. > this. Youâ€™re learning good coding practices here that will come in handy in. > the future, I promise! >. > Thank you for your contribution ðŸŽ‰. >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1458,safety,test,test,1458,"urn. > categories, obs_tidy, categorical. Then you can change each line like categories,. > obs_tidy = _prepare_dataframe(â€¦) to categories, obs_tidy, _ =. > _prepare_dataframe(â€¦). >. > Other than that, thereâ€™s only few things left:. >. > 1. >. > The tests without plots should contain assertions. I.e. in. > test_genes_ranking() you should do assert. > np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...]) or. > so! > 2. >. > For the plot tests, you need to add these lines to the test file:. >. >. > https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L13. >. > And do each test like this (replace â€œxyzâ€ with whatever you want):. >. > def test_xyz(image_comparer):. >. > save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). >. > [â€¦]. >. > sc.pl.xyz(adata, â€¦). >. > save_and_compare_images('xyz'). >. > This will make the tests save your plots to scanpy/tests/figures and. > compare them to the images in scanpy/test/_images. The tests will fail. > because scanpy/test/_images/xyz.png doesnâ€™t exist. You need to copy. > the pngs from scanpy/tests/figuresâ†’scanpy/test/_images and git commit. > them. > 3. >. > This needs to be fixed: #644 (comment). > <https://github.com/theislab/scanpy/pull/644#discussion_r284652144>. > 4. >. > I think the test data might be too large. @falexwolf. > <https://github.com/falexwolf> do we have a recommended size for new. > test data? >. > @Khalid-Usman <https://github.com/Khalid-Usman> Iâ€™m sorry if you find. > that this takes long and is frustrating. If this is the case, just step. > away for a while and do something else! But I think you wonâ€™t regret doing. > this. Youâ€™re learning good coding practices here that will come in handy in. > the future, I promise! >. > Thank you for your contribution ðŸŽ‰. >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1476,safety,test,tests,1476,"obs_tidy, categorical. Then you can change each line like categories,. > obs_tidy = _prepare_dataframe(â€¦) to categories, obs_tidy, _ =. > _prepare_dataframe(â€¦). >. > Other than that, thereâ€™s only few things left:. >. > 1. >. > The tests without plots should contain assertions. I.e. in. > test_genes_ranking() you should do assert. > np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...]) or. > so! > 2. >. > For the plot tests, you need to add these lines to the test file:. >. >. > https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L13. >. > And do each test like this (replace â€œxyzâ€ with whatever you want):. >. > def test_xyz(image_comparer):. >. > save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). >. > [â€¦]. >. > sc.pl.xyz(adata, â€¦). >. > save_and_compare_images('xyz'). >. > This will make the tests save your plots to scanpy/tests/figures and. > compare them to the images in scanpy/test/_images. The tests will fail. > because scanpy/test/_images/xyz.png doesnâ€™t exist. You need to copy. > the pngs from scanpy/tests/figuresâ†’scanpy/test/_images and git commit. > them. > 3. >. > This needs to be fixed: #644 (comment). > <https://github.com/theislab/scanpy/pull/644#discussion_r284652144>. > 4. >. > I think the test data might be too large. @falexwolf. > <https://github.com/falexwolf> do we have a recommended size for new. > test data? >. > @Khalid-Usman <https://github.com/Khalid-Usman> Iâ€™m sorry if you find. > that this takes long and is frustrating. If this is the case, just step. > away for a while and do something else! But I think you wonâ€™t regret doing. > this. Youâ€™re learning good coding practices here that will come in handy in. > the future, I promise! >. > Thank you for your contribution ðŸŽ‰. >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1510,safety,test,test,1510,"an change each line like categories,. > obs_tidy = _prepare_dataframe(â€¦) to categories, obs_tidy, _ =. > _prepare_dataframe(â€¦). >. > Other than that, thereâ€™s only few things left:. >. > 1. >. > The tests without plots should contain assertions. I.e. in. > test_genes_ranking() you should do assert. > np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...]) or. > so! > 2. >. > For the plot tests, you need to add these lines to the test file:. >. >. > https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L13. >. > And do each test like this (replace â€œxyzâ€ with whatever you want):. >. > def test_xyz(image_comparer):. >. > save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). >. > [â€¦]. >. > sc.pl.xyz(adata, â€¦). >. > save_and_compare_images('xyz'). >. > This will make the tests save your plots to scanpy/tests/figures and. > compare them to the images in scanpy/test/_images. The tests will fail. > because scanpy/test/_images/xyz.png doesnâ€™t exist. You need to copy. > the pngs from scanpy/tests/figuresâ†’scanpy/test/_images and git commit. > them. > 3. >. > This needs to be fixed: #644 (comment). > <https://github.com/theislab/scanpy/pull/644#discussion_r284652144>. > 4. >. > I think the test data might be too large. @falexwolf. > <https://github.com/falexwolf> do we have a recommended size for new. > test data? >. > @Khalid-Usman <https://github.com/Khalid-Usman> Iâ€™m sorry if you find. > that this takes long and is frustrating. If this is the case, just step. > away for a while and do something else! But I think you wonâ€™t regret doing. > this. Youâ€™re learning good coding practices here that will come in handy in. > the future, I promise! >. > Thank you for your contribution ðŸŽ‰. >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGODHLQPIDWZGLGTKXGLPWJ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1587,safety,test,tests,1587,"tegories, obs_tidy, _ =. > _prepare_dataframe(â€¦). >. > Other than that, thereâ€™s only few things left:. >. > 1. >. > The tests without plots should contain assertions. I.e. in. > test_genes_ranking() you should do assert. > np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...]) or. > so! > 2. >. > For the plot tests, you need to add these lines to the test file:. >. >. > https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L13. >. > And do each test like this (replace â€œxyzâ€ with whatever you want):. >. > def test_xyz(image_comparer):. >. > save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). >. > [â€¦]. >. > sc.pl.xyz(adata, â€¦). >. > save_and_compare_images('xyz'). >. > This will make the tests save your plots to scanpy/tests/figures and. > compare them to the images in scanpy/test/_images. The tests will fail. > because scanpy/test/_images/xyz.png doesnâ€™t exist. You need to copy. > the pngs from scanpy/tests/figuresâ†’scanpy/test/_images and git commit. > them. > 3. >. > This needs to be fixed: #644 (comment). > <https://github.com/theislab/scanpy/pull/644#discussion_r284652144>. > 4. >. > I think the test data might be too large. @falexwolf. > <https://github.com/falexwolf> do we have a recommended size for new. > test data? >. > @Khalid-Usman <https://github.com/Khalid-Usman> Iâ€™m sorry if you find. > that this takes long and is frustrating. If this is the case, just step. > away for a while and do something else! But I think you wonâ€™t regret doing. > this. Youâ€™re learning good coding practices here that will come in handy in. > the future, I promise! >. > Thank you for your contribution ðŸŽ‰. >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGODHLQPIDWZGLGTKXGLPWJUZNA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGO",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1608,safety,test,test,1608,"_ =. > _prepare_dataframe(â€¦). >. > Other than that, thereâ€™s only few things left:. >. > 1. >. > The tests without plots should contain assertions. I.e. in. > test_genes_ranking() you should do assert. > np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...]) or. > so! > 2. >. > For the plot tests, you need to add these lines to the test file:. >. >. > https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L13. >. > And do each test like this (replace â€œxyzâ€ with whatever you want):. >. > def test_xyz(image_comparer):. >. > save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). >. > [â€¦]. >. > sc.pl.xyz(adata, â€¦). >. > save_and_compare_images('xyz'). >. > This will make the tests save your plots to scanpy/tests/figures and. > compare them to the images in scanpy/test/_images. The tests will fail. > because scanpy/test/_images/xyz.png doesnâ€™t exist. You need to copy. > the pngs from scanpy/tests/figuresâ†’scanpy/test/_images and git commit. > them. > 3. >. > This needs to be fixed: #644 (comment). > <https://github.com/theislab/scanpy/pull/644#discussion_r284652144>. > 4. >. > I think the test data might be too large. @falexwolf. > <https://github.com/falexwolf> do we have a recommended size for new. > test data? >. > @Khalid-Usman <https://github.com/Khalid-Usman> Iâ€™m sorry if you find. > that this takes long and is frustrating. If this is the case, just step. > away for a while and do something else! But I think you wonâ€™t regret doing. > this. Youâ€™re learning good coding practices here that will come in handy in. > the future, I promise! >. > Thank you for your contribution ðŸŽ‰. >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGODHLQPIDWZGLGTKXGLPWJUZNA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVYG3VA#issuecomment",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1788,safety,test,test,1788,"ons. I.e. in. > test_genes_ranking() you should do assert. > np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...]) or. > so! > 2. >. > For the plot tests, you need to add these lines to the test file:. >. >. > https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L13. >. > And do each test like this (replace â€œxyzâ€ with whatever you want):. >. > def test_xyz(image_comparer):. >. > save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). >. > [â€¦]. >. > sc.pl.xyz(adata, â€¦). >. > save_and_compare_images('xyz'). >. > This will make the tests save your plots to scanpy/tests/figures and. > compare them to the images in scanpy/test/_images. The tests will fail. > because scanpy/test/_images/xyz.png doesnâ€™t exist. You need to copy. > the pngs from scanpy/tests/figuresâ†’scanpy/test/_images and git commit. > them. > 3. >. > This needs to be fixed: #644 (comment). > <https://github.com/theislab/scanpy/pull/644#discussion_r284652144>. > 4. >. > I think the test data might be too large. @falexwolf. > <https://github.com/falexwolf> do we have a recommended size for new. > test data? >. > @Khalid-Usman <https://github.com/Khalid-Usman> Iâ€™m sorry if you find. > that this takes long and is frustrating. If this is the case, just step. > away for a while and do something else! But I think you wonâ€™t regret doing. > this. Youâ€™re learning good coding practices here that will come in handy in. > the future, I promise! >. > Thank you for your contribution ðŸŽ‰. >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGODHLQPIDWZGLGTKXGLPWJUZNA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVYG3VA#issuecomment-493907412>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOG73UUYPXGY7UICKODPWJUZNANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1904,safety,test,test,1904,"ons. I.e. in. > test_genes_ranking() you should do assert. > np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...]) or. > so! > 2. >. > For the plot tests, you need to add these lines to the test file:. >. >. > https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L13. >. > And do each test like this (replace â€œxyzâ€ with whatever you want):. >. > def test_xyz(image_comparer):. >. > save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). >. > [â€¦]. >. > sc.pl.xyz(adata, â€¦). >. > save_and_compare_images('xyz'). >. > This will make the tests save your plots to scanpy/tests/figures and. > compare them to the images in scanpy/test/_images. The tests will fail. > because scanpy/test/_images/xyz.png doesnâ€™t exist. You need to copy. > the pngs from scanpy/tests/figuresâ†’scanpy/test/_images and git commit. > them. > 3. >. > This needs to be fixed: #644 (comment). > <https://github.com/theislab/scanpy/pull/644#discussion_r284652144>. > 4. >. > I think the test data might be too large. @falexwolf. > <https://github.com/falexwolf> do we have a recommended size for new. > test data? >. > @Khalid-Usman <https://github.com/Khalid-Usman> Iâ€™m sorry if you find. > that this takes long and is frustrating. If this is the case, just step. > away for a while and do something else! But I think you wonâ€™t regret doing. > this. Youâ€™re learning good coding practices here that will come in handy in. > the future, I promise! >. > Thank you for your contribution ðŸŽ‰. >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGODHLQPIDWZGLGTKXGLPWJUZNA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVYG3VA#issuecomment-493907412>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOG73UUYPXGY7UICKODPWJUZNANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:20,security,updat,updated,20,"Hi Philipp,. I have updated accordingly, again no issue but the duplication and i. analysed most of them are from previous code. Regards,. Khalid. On Mon, May 20, 2019 at 5:23 PM Philipp A. <notifications@github.com> wrote:. > Hi, looks great! >. > The only duplicated code left is that _prepare_weighted_dataframe is very. > similar to _prepare_dataframe. I think you can delete. > _prepare_weighted_dataframe and just change _prepare_dataframe so it does return. > categories, obs_tidy, categorical. Then you can change each line like categories,. > obs_tidy = _prepare_dataframe(â€¦) to categories, obs_tidy, _ =. > _prepare_dataframe(â€¦). >. > Other than that, thereâ€™s only few things left:. >. > 1. >. > The tests without plots should contain assertions. I.e. in. > test_genes_ranking() you should do assert. > np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...]) or. > so! > 2. >. > For the plot tests, you need to add these lines to the test file:. >. >. > https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L13. >. > And do each test like this (replace â€œxyzâ€ with whatever you want):. >. > def test_xyz(image_comparer):. >. > save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). >. > [â€¦]. >. > sc.pl.xyz(adata, â€¦). >. > save_and_compare_images('xyz'). >. > This will make the tests save your plots to scanpy/tests/figures and. > compare them to the images in scanpy/test/_images. The tests will fail. > because scanpy/test/_images/xyz.png doesnâ€™t exist. You need to copy. > the pngs from scanpy/tests/figuresâ†’scanpy/test/_images and git commit. > them. > 3. >. > This needs to be fixed: #644 (comment). > <https://github.com/theislab/scanpy/pull/644#discussion_r284652144>. > 4. >. > I think the test data might be too large. @falexwolf. > <https://github.com/falexwolf> do we have a recommended size for new. > test data? >. > @Khalid-Usman <https://github.com/Khalid-Usman> Iâ€™m sorry if you find. > that th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:2694,security,auth,auth,2694,"ons. I.e. in. > test_genes_ranking() you should do assert. > np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...]) or. > so! > 2. >. > For the plot tests, you need to add these lines to the test file:. >. >. > https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L13. >. > And do each test like this (replace â€œxyzâ€ with whatever you want):. >. > def test_xyz(image_comparer):. >. > save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). >. > [â€¦]. >. > sc.pl.xyz(adata, â€¦). >. > save_and_compare_images('xyz'). >. > This will make the tests save your plots to scanpy/tests/figures and. > compare them to the images in scanpy/test/_images. The tests will fail. > because scanpy/test/_images/xyz.png doesnâ€™t exist. You need to copy. > the pngs from scanpy/tests/figuresâ†’scanpy/test/_images and git commit. > them. > 3. >. > This needs to be fixed: #644 (comment). > <https://github.com/theislab/scanpy/pull/644#discussion_r284652144>. > 4. >. > I think the test data might be too large. @falexwolf. > <https://github.com/falexwolf> do we have a recommended size for new. > test data? >. > @Khalid-Usman <https://github.com/Khalid-Usman> Iâ€™m sorry if you find. > that this takes long and is frustrating. If this is the case, just step. > away for a while and do something else! But I think you wonâ€™t regret doing. > this. Youâ€™re learning good coding practices here that will come in handy in. > the future, I promise! >. > Thank you for your contribution ðŸŽ‰. >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGODHLQPIDWZGLGTKXGLPWJUZNA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVYG3VA#issuecomment-493907412>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOG73UUYPXGY7UICKODPWJUZNANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:710,testability,test,tests,710,"Hi Philipp,. I have updated accordingly, again no issue but the duplication and i. analysed most of them are from previous code. Regards,. Khalid. On Mon, May 20, 2019 at 5:23 PM Philipp A. <notifications@github.com> wrote:. > Hi, looks great! >. > The only duplicated code left is that _prepare_weighted_dataframe is very. > similar to _prepare_dataframe. I think you can delete. > _prepare_weighted_dataframe and just change _prepare_dataframe so it does return. > categories, obs_tidy, categorical. Then you can change each line like categories,. > obs_tidy = _prepare_dataframe(â€¦) to categories, obs_tidy, _ =. > _prepare_dataframe(â€¦). >. > Other than that, thereâ€™s only few things left:. >. > 1. >. > The tests without plots should contain assertions. I.e. in. > test_genes_ranking() you should do assert. > np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...]) or. > so! > 2. >. > For the plot tests, you need to add these lines to the test file:. >. >. > https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L13. >. > And do each test like this (replace â€œxyzâ€ with whatever you want):. >. > def test_xyz(image_comparer):. >. > save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). >. > [â€¦]. >. > sc.pl.xyz(adata, â€¦). >. > save_and_compare_images('xyz'). >. > This will make the tests save your plots to scanpy/tests/figures and. > compare them to the images in scanpy/test/_images. The tests will fail. > because scanpy/test/_images/xyz.png doesnâ€™t exist. You need to copy. > the pngs from scanpy/tests/figuresâ†’scanpy/test/_images and git commit. > them. > 3. >. > This needs to be fixed: #644 (comment). > <https://github.com/theislab/scanpy/pull/644#discussion_r284652144>. > 4. >. > I think the test data might be too large. @falexwolf. > <https://github.com/falexwolf> do we have a recommended size for new. > test data? >. > @Khalid-Usman <https://github.com/Khalid-Usman> Iâ€™m sorry if you find. > that th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:745,testability,assert,assertions,745,"Hi Philipp,. I have updated accordingly, again no issue but the duplication and i. analysed most of them are from previous code. Regards,. Khalid. On Mon, May 20, 2019 at 5:23 PM Philipp A. <notifications@github.com> wrote:. > Hi, looks great! >. > The only duplicated code left is that _prepare_weighted_dataframe is very. > similar to _prepare_dataframe. I think you can delete. > _prepare_weighted_dataframe and just change _prepare_dataframe so it does return. > categories, obs_tidy, categorical. Then you can change each line like categories,. > obs_tidy = _prepare_dataframe(â€¦) to categories, obs_tidy, _ =. > _prepare_dataframe(â€¦). >. > Other than that, thereâ€™s only few things left:. >. > 1. >. > The tests without plots should contain assertions. I.e. in. > test_genes_ranking() you should do assert. > np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...]) or. > so! > 2. >. > For the plot tests, you need to add these lines to the test file:. >. >. > https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L13. >. > And do each test like this (replace â€œxyzâ€ with whatever you want):. >. > def test_xyz(image_comparer):. >. > save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). >. > [â€¦]. >. > sc.pl.xyz(adata, â€¦). >. > save_and_compare_images('xyz'). >. > This will make the tests save your plots to scanpy/tests/figures and. > compare them to the images in scanpy/test/_images. The tests will fail. > because scanpy/test/_images/xyz.png doesnâ€™t exist. You need to copy. > the pngs from scanpy/tests/figuresâ†’scanpy/test/_images and git commit. > them. > 3. >. > This needs to be fixed: #644 (comment). > <https://github.com/theislab/scanpy/pull/644#discussion_r284652144>. > 4. >. > I think the test data might be too large. @falexwolf. > <https://github.com/falexwolf> do we have a recommended size for new. > test data? >. > @Khalid-Usman <https://github.com/Khalid-Usman> Iâ€™m sorry if you find. > that th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:803,testability,assert,assert,803,"Hi Philipp,. I have updated accordingly, again no issue but the duplication and i. analysed most of them are from previous code. Regards,. Khalid. On Mon, May 20, 2019 at 5:23 PM Philipp A. <notifications@github.com> wrote:. > Hi, looks great! >. > The only duplicated code left is that _prepare_weighted_dataframe is very. > similar to _prepare_dataframe. I think you can delete. > _prepare_weighted_dataframe and just change _prepare_dataframe so it does return. > categories, obs_tidy, categorical. Then you can change each line like categories,. > obs_tidy = _prepare_dataframe(â€¦) to categories, obs_tidy, _ =. > _prepare_dataframe(â€¦). >. > Other than that, thereâ€™s only few things left:. >. > 1. >. > The tests without plots should contain assertions. I.e. in. > test_genes_ranking() you should do assert. > np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...]) or. > so! > 2. >. > For the plot tests, you need to add these lines to the test file:. >. >. > https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L13. >. > And do each test like this (replace â€œxyzâ€ with whatever you want):. >. > def test_xyz(image_comparer):. >. > save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). >. > [â€¦]. >. > sc.pl.xyz(adata, â€¦). >. > save_and_compare_images('xyz'). >. > This will make the tests save your plots to scanpy/tests/figures and. > compare them to the images in scanpy/test/_images. The tests will fail. > because scanpy/test/_images/xyz.png doesnâ€™t exist. You need to copy. > the pngs from scanpy/tests/figuresâ†’scanpy/test/_images and git commit. > them. > 3. >. > This needs to be fixed: #644 (comment). > <https://github.com/theislab/scanpy/pull/644#discussion_r284652144>. > 4. >. > I think the test data might be too large. @falexwolf. > <https://github.com/falexwolf> do we have a recommended size for new. > test data? >. > @Khalid-Usman <https://github.com/Khalid-Usman> Iâ€™m sorry if you find. > that th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:914,testability,test,tests,914,"Hi Philipp,. I have updated accordingly, again no issue but the duplication and i. analysed most of them are from previous code. Regards,. Khalid. On Mon, May 20, 2019 at 5:23 PM Philipp A. <notifications@github.com> wrote:. > Hi, looks great! >. > The only duplicated code left is that _prepare_weighted_dataframe is very. > similar to _prepare_dataframe. I think you can delete. > _prepare_weighted_dataframe and just change _prepare_dataframe so it does return. > categories, obs_tidy, categorical. Then you can change each line like categories,. > obs_tidy = _prepare_dataframe(â€¦) to categories, obs_tidy, _ =. > _prepare_dataframe(â€¦). >. > Other than that, thereâ€™s only few things left:. >. > 1. >. > The tests without plots should contain assertions. I.e. in. > test_genes_ranking() you should do assert. > np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...]) or. > so! > 2. >. > For the plot tests, you need to add these lines to the test file:. >. >. > https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L13. >. > And do each test like this (replace â€œxyzâ€ with whatever you want):. >. > def test_xyz(image_comparer):. >. > save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). >. > [â€¦]. >. > sc.pl.xyz(adata, â€¦). >. > save_and_compare_images('xyz'). >. > This will make the tests save your plots to scanpy/tests/figures and. > compare them to the images in scanpy/test/_images. The tests will fail. > because scanpy/test/_images/xyz.png doesnâ€™t exist. You need to copy. > the pngs from scanpy/tests/figuresâ†’scanpy/test/_images and git commit. > them. > 3. >. > This needs to be fixed: #644 (comment). > <https://github.com/theislab/scanpy/pull/644#discussion_r284652144>. > 4. >. > I think the test data might be too large. @falexwolf. > <https://github.com/falexwolf> do we have a recommended size for new. > test data? >. > @Khalid-Usman <https://github.com/Khalid-Usman> Iâ€™m sorry if you find. > that th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:956,testability,test,test,956,"Hi Philipp,. I have updated accordingly, again no issue but the duplication and i. analysed most of them are from previous code. Regards,. Khalid. On Mon, May 20, 2019 at 5:23 PM Philipp A. <notifications@github.com> wrote:. > Hi, looks great! >. > The only duplicated code left is that _prepare_weighted_dataframe is very. > similar to _prepare_dataframe. I think you can delete. > _prepare_weighted_dataframe and just change _prepare_dataframe so it does return. > categories, obs_tidy, categorical. Then you can change each line like categories,. > obs_tidy = _prepare_dataframe(â€¦) to categories, obs_tidy, _ =. > _prepare_dataframe(â€¦). >. > Other than that, thereâ€™s only few things left:. >. > 1. >. > The tests without plots should contain assertions. I.e. in. > test_genes_ranking() you should do assert. > np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...]) or. > so! > 2. >. > For the plot tests, you need to add these lines to the test file:. >. >. > https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L13. >. > And do each test like this (replace â€œxyzâ€ with whatever you want):. >. > def test_xyz(image_comparer):. >. > save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). >. > [â€¦]. >. > sc.pl.xyz(adata, â€¦). >. > save_and_compare_images('xyz'). >. > This will make the tests save your plots to scanpy/tests/figures and. > compare them to the images in scanpy/test/_images. The tests will fail. > because scanpy/test/_images/xyz.png doesnâ€™t exist. You need to copy. > the pngs from scanpy/tests/figuresâ†’scanpy/test/_images and git commit. > them. > 3. >. > This needs to be fixed: #644 (comment). > <https://github.com/theislab/scanpy/pull/644#discussion_r284652144>. > 4. >. > I think the test data might be too large. @falexwolf. > <https://github.com/falexwolf> do we have a recommended size for new. > test data? >. > @Khalid-Usman <https://github.com/Khalid-Usman> Iâ€™m sorry if you find. > that th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1064,testability,test,tests,1064,"lication and i. analysed most of them are from previous code. Regards,. Khalid. On Mon, May 20, 2019 at 5:23 PM Philipp A. <notifications@github.com> wrote:. > Hi, looks great! >. > The only duplicated code left is that _prepare_weighted_dataframe is very. > similar to _prepare_dataframe. I think you can delete. > _prepare_weighted_dataframe and just change _prepare_dataframe so it does return. > categories, obs_tidy, categorical. Then you can change each line like categories,. > obs_tidy = _prepare_dataframe(â€¦) to categories, obs_tidy, _ =. > _prepare_dataframe(â€¦). >. > Other than that, thereâ€™s only few things left:. >. > 1. >. > The tests without plots should contain assertions. I.e. in. > test_genes_ranking() you should do assert. > np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...]) or. > so! > 2. >. > For the plot tests, you need to add these lines to the test file:. >. >. > https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L13. >. > And do each test like this (replace â€œxyzâ€ with whatever you want):. >. > def test_xyz(image_comparer):. >. > save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). >. > [â€¦]. >. > sc.pl.xyz(adata, â€¦). >. > save_and_compare_images('xyz'). >. > This will make the tests save your plots to scanpy/tests/figures and. > compare them to the images in scanpy/test/_images. The tests will fail. > because scanpy/test/_images/xyz.png doesnâ€™t exist. You need to copy. > the pngs from scanpy/tests/figuresâ†’scanpy/test/_images and git commit. > them. > 3. >. > This needs to be fixed: #644 (comment). > <https://github.com/theislab/scanpy/pull/644#discussion_r284652144>. > 4. >. > I think the test data might be too large. @falexwolf. > <https://github.com/falexwolf> do we have a recommended size for new. > test data? >. > @Khalid-Usman <https://github.com/Khalid-Usman> Iâ€™m sorry if you find. > that this takes long and is frustrating. If this is the case, just step. >",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1112,testability,test,test,1112,"previous code. Regards,. Khalid. On Mon, May 20, 2019 at 5:23 PM Philipp A. <notifications@github.com> wrote:. > Hi, looks great! >. > The only duplicated code left is that _prepare_weighted_dataframe is very. > similar to _prepare_dataframe. I think you can delete. > _prepare_weighted_dataframe and just change _prepare_dataframe so it does return. > categories, obs_tidy, categorical. Then you can change each line like categories,. > obs_tidy = _prepare_dataframe(â€¦) to categories, obs_tidy, _ =. > _prepare_dataframe(â€¦). >. > Other than that, thereâ€™s only few things left:. >. > 1. >. > The tests without plots should contain assertions. I.e. in. > test_genes_ranking() you should do assert. > np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...]) or. > so! > 2. >. > For the plot tests, you need to add these lines to the test file:. >. >. > https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L13. >. > And do each test like this (replace â€œxyzâ€ with whatever you want):. >. > def test_xyz(image_comparer):. >. > save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). >. > [â€¦]. >. > sc.pl.xyz(adata, â€¦). >. > save_and_compare_images('xyz'). >. > This will make the tests save your plots to scanpy/tests/figures and. > compare them to the images in scanpy/test/_images. The tests will fail. > because scanpy/test/_images/xyz.png doesnâ€™t exist. You need to copy. > the pngs from scanpy/tests/figuresâ†’scanpy/test/_images and git commit. > them. > 3. >. > This needs to be fixed: #644 (comment). > <https://github.com/theislab/scanpy/pull/644#discussion_r284652144>. > 4. >. > I think the test data might be too large. @falexwolf. > <https://github.com/falexwolf> do we have a recommended size for new. > test data? >. > @Khalid-Usman <https://github.com/Khalid-Usman> Iâ€™m sorry if you find. > that this takes long and is frustrating. If this is the case, just step. > away for a while and do something else! But I ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1368,testability,test,tests,1368,"n delete. > _prepare_weighted_dataframe and just change _prepare_dataframe so it does return. > categories, obs_tidy, categorical. Then you can change each line like categories,. > obs_tidy = _prepare_dataframe(â€¦) to categories, obs_tidy, _ =. > _prepare_dataframe(â€¦). >. > Other than that, thereâ€™s only few things left:. >. > 1. >. > The tests without plots should contain assertions. I.e. in. > test_genes_ranking() you should do assert. > np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...]) or. > so! > 2. >. > For the plot tests, you need to add these lines to the test file:. >. >. > https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L13. >. > And do each test like this (replace â€œxyzâ€ with whatever you want):. >. > def test_xyz(image_comparer):. >. > save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). >. > [â€¦]. >. > sc.pl.xyz(adata, â€¦). >. > save_and_compare_images('xyz'). >. > This will make the tests save your plots to scanpy/tests/figures and. > compare them to the images in scanpy/test/_images. The tests will fail. > because scanpy/test/_images/xyz.png doesnâ€™t exist. You need to copy. > the pngs from scanpy/tests/figuresâ†’scanpy/test/_images and git commit. > them. > 3. >. > This needs to be fixed: #644 (comment). > <https://github.com/theislab/scanpy/pull/644#discussion_r284652144>. > 4. >. > I think the test data might be too large. @falexwolf. > <https://github.com/falexwolf> do we have a recommended size for new. > test data? >. > @Khalid-Usman <https://github.com/Khalid-Usman> Iâ€™m sorry if you find. > that this takes long and is frustrating. If this is the case, just step. > away for a while and do something else! But I think you wonâ€™t regret doing. > this. Youâ€™re learning good coding practices here that will come in handy in. > the future, I promise! >. > Thank you for your contribution ðŸŽ‰. >. > â€”. > You are receiving this because you were mentioned. > Reply to this email ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1400,testability,test,tests,1400,"taframe and just change _prepare_dataframe so it does return. > categories, obs_tidy, categorical. Then you can change each line like categories,. > obs_tidy = _prepare_dataframe(â€¦) to categories, obs_tidy, _ =. > _prepare_dataframe(â€¦). >. > Other than that, thereâ€™s only few things left:. >. > 1. >. > The tests without plots should contain assertions. I.e. in. > test_genes_ranking() you should do assert. > np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...]) or. > so! > 2. >. > For the plot tests, you need to add these lines to the test file:. >. >. > https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L13. >. > And do each test like this (replace â€œxyzâ€ with whatever you want):. >. > def test_xyz(image_comparer):. >. > save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). >. > [â€¦]. >. > sc.pl.xyz(adata, â€¦). >. > save_and_compare_images('xyz'). >. > This will make the tests save your plots to scanpy/tests/figures and. > compare them to the images in scanpy/test/_images. The tests will fail. > because scanpy/test/_images/xyz.png doesnâ€™t exist. You need to copy. > the pngs from scanpy/tests/figuresâ†’scanpy/test/_images and git commit. > them. > 3. >. > This needs to be fixed: #644 (comment). > <https://github.com/theislab/scanpy/pull/644#discussion_r284652144>. > 4. >. > I think the test data might be too large. @falexwolf. > <https://github.com/falexwolf> do we have a recommended size for new. > test data? >. > @Khalid-Usman <https://github.com/Khalid-Usman> Iâ€™m sorry if you find. > that this takes long and is frustrating. If this is the case, just step. > away for a while and do something else! But I think you wonâ€™t regret doing. > this. Youâ€™re learning good coding practices here that will come in handy in. > the future, I promise! >. > Thank you for your contribution ðŸŽ‰. >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1458,testability,test,test,1458,"urn. > categories, obs_tidy, categorical. Then you can change each line like categories,. > obs_tidy = _prepare_dataframe(â€¦) to categories, obs_tidy, _ =. > _prepare_dataframe(â€¦). >. > Other than that, thereâ€™s only few things left:. >. > 1. >. > The tests without plots should contain assertions. I.e. in. > test_genes_ranking() you should do assert. > np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...]) or. > so! > 2. >. > For the plot tests, you need to add these lines to the test file:. >. >. > https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L13. >. > And do each test like this (replace â€œxyzâ€ with whatever you want):. >. > def test_xyz(image_comparer):. >. > save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). >. > [â€¦]. >. > sc.pl.xyz(adata, â€¦). >. > save_and_compare_images('xyz'). >. > This will make the tests save your plots to scanpy/tests/figures and. > compare them to the images in scanpy/test/_images. The tests will fail. > because scanpy/test/_images/xyz.png doesnâ€™t exist. You need to copy. > the pngs from scanpy/tests/figuresâ†’scanpy/test/_images and git commit. > them. > 3. >. > This needs to be fixed: #644 (comment). > <https://github.com/theislab/scanpy/pull/644#discussion_r284652144>. > 4. >. > I think the test data might be too large. @falexwolf. > <https://github.com/falexwolf> do we have a recommended size for new. > test data? >. > @Khalid-Usman <https://github.com/Khalid-Usman> Iâ€™m sorry if you find. > that this takes long and is frustrating. If this is the case, just step. > away for a while and do something else! But I think you wonâ€™t regret doing. > this. Youâ€™re learning good coding practices here that will come in handy in. > the future, I promise! >. > Thank you for your contribution ðŸŽ‰. >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1476,testability,test,tests,1476,"obs_tidy, categorical. Then you can change each line like categories,. > obs_tidy = _prepare_dataframe(â€¦) to categories, obs_tidy, _ =. > _prepare_dataframe(â€¦). >. > Other than that, thereâ€™s only few things left:. >. > 1. >. > The tests without plots should contain assertions. I.e. in. > test_genes_ranking() you should do assert. > np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...]) or. > so! > 2. >. > For the plot tests, you need to add these lines to the test file:. >. >. > https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L13. >. > And do each test like this (replace â€œxyzâ€ with whatever you want):. >. > def test_xyz(image_comparer):. >. > save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). >. > [â€¦]. >. > sc.pl.xyz(adata, â€¦). >. > save_and_compare_images('xyz'). >. > This will make the tests save your plots to scanpy/tests/figures and. > compare them to the images in scanpy/test/_images. The tests will fail. > because scanpy/test/_images/xyz.png doesnâ€™t exist. You need to copy. > the pngs from scanpy/tests/figuresâ†’scanpy/test/_images and git commit. > them. > 3. >. > This needs to be fixed: #644 (comment). > <https://github.com/theislab/scanpy/pull/644#discussion_r284652144>. > 4. >. > I think the test data might be too large. @falexwolf. > <https://github.com/falexwolf> do we have a recommended size for new. > test data? >. > @Khalid-Usman <https://github.com/Khalid-Usman> Iâ€™m sorry if you find. > that this takes long and is frustrating. If this is the case, just step. > away for a while and do something else! But I think you wonâ€™t regret doing. > this. Youâ€™re learning good coding practices here that will come in handy in. > the future, I promise! >. > Thank you for your contribution ðŸŽ‰. >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1510,testability,test,test,1510,"an change each line like categories,. > obs_tidy = _prepare_dataframe(â€¦) to categories, obs_tidy, _ =. > _prepare_dataframe(â€¦). >. > Other than that, thereâ€™s only few things left:. >. > 1. >. > The tests without plots should contain assertions. I.e. in. > test_genes_ranking() you should do assert. > np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...]) or. > so! > 2. >. > For the plot tests, you need to add these lines to the test file:. >. >. > https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L13. >. > And do each test like this (replace â€œxyzâ€ with whatever you want):. >. > def test_xyz(image_comparer):. >. > save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). >. > [â€¦]. >. > sc.pl.xyz(adata, â€¦). >. > save_and_compare_images('xyz'). >. > This will make the tests save your plots to scanpy/tests/figures and. > compare them to the images in scanpy/test/_images. The tests will fail. > because scanpy/test/_images/xyz.png doesnâ€™t exist. You need to copy. > the pngs from scanpy/tests/figuresâ†’scanpy/test/_images and git commit. > them. > 3. >. > This needs to be fixed: #644 (comment). > <https://github.com/theislab/scanpy/pull/644#discussion_r284652144>. > 4. >. > I think the test data might be too large. @falexwolf. > <https://github.com/falexwolf> do we have a recommended size for new. > test data? >. > @Khalid-Usman <https://github.com/Khalid-Usman> Iâ€™m sorry if you find. > that this takes long and is frustrating. If this is the case, just step. > away for a while and do something else! But I think you wonâ€™t regret doing. > this. Youâ€™re learning good coding practices here that will come in handy in. > the future, I promise! >. > Thank you for your contribution ðŸŽ‰. >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGODHLQPIDWZGLGTKXGLPWJ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1587,testability,test,tests,1587,"tegories, obs_tidy, _ =. > _prepare_dataframe(â€¦). >. > Other than that, thereâ€™s only few things left:. >. > 1. >. > The tests without plots should contain assertions. I.e. in. > test_genes_ranking() you should do assert. > np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...]) or. > so! > 2. >. > For the plot tests, you need to add these lines to the test file:. >. >. > https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L13. >. > And do each test like this (replace â€œxyzâ€ with whatever you want):. >. > def test_xyz(image_comparer):. >. > save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). >. > [â€¦]. >. > sc.pl.xyz(adata, â€¦). >. > save_and_compare_images('xyz'). >. > This will make the tests save your plots to scanpy/tests/figures and. > compare them to the images in scanpy/test/_images. The tests will fail. > because scanpy/test/_images/xyz.png doesnâ€™t exist. You need to copy. > the pngs from scanpy/tests/figuresâ†’scanpy/test/_images and git commit. > them. > 3. >. > This needs to be fixed: #644 (comment). > <https://github.com/theislab/scanpy/pull/644#discussion_r284652144>. > 4. >. > I think the test data might be too large. @falexwolf. > <https://github.com/falexwolf> do we have a recommended size for new. > test data? >. > @Khalid-Usman <https://github.com/Khalid-Usman> Iâ€™m sorry if you find. > that this takes long and is frustrating. If this is the case, just step. > away for a while and do something else! But I think you wonâ€™t regret doing. > this. Youâ€™re learning good coding practices here that will come in handy in. > the future, I promise! >. > Thank you for your contribution ðŸŽ‰. >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGODHLQPIDWZGLGTKXGLPWJUZNA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGO",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1608,testability,test,test,1608,"_ =. > _prepare_dataframe(â€¦). >. > Other than that, thereâ€™s only few things left:. >. > 1. >. > The tests without plots should contain assertions. I.e. in. > test_genes_ranking() you should do assert. > np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...]) or. > so! > 2. >. > For the plot tests, you need to add these lines to the test file:. >. >. > https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L13. >. > And do each test like this (replace â€œxyzâ€ with whatever you want):. >. > def test_xyz(image_comparer):. >. > save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). >. > [â€¦]. >. > sc.pl.xyz(adata, â€¦). >. > save_and_compare_images('xyz'). >. > This will make the tests save your plots to scanpy/tests/figures and. > compare them to the images in scanpy/test/_images. The tests will fail. > because scanpy/test/_images/xyz.png doesnâ€™t exist. You need to copy. > the pngs from scanpy/tests/figuresâ†’scanpy/test/_images and git commit. > them. > 3. >. > This needs to be fixed: #644 (comment). > <https://github.com/theislab/scanpy/pull/644#discussion_r284652144>. > 4. >. > I think the test data might be too large. @falexwolf. > <https://github.com/falexwolf> do we have a recommended size for new. > test data? >. > @Khalid-Usman <https://github.com/Khalid-Usman> Iâ€™m sorry if you find. > that this takes long and is frustrating. If this is the case, just step. > away for a while and do something else! But I think you wonâ€™t regret doing. > this. Youâ€™re learning good coding practices here that will come in handy in. > the future, I promise! >. > Thank you for your contribution ðŸŽ‰. >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGODHLQPIDWZGLGTKXGLPWJUZNA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVYG3VA#issuecomment",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1788,testability,test,test,1788,"ons. I.e. in. > test_genes_ranking() you should do assert. > np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...]) or. > so! > 2. >. > For the plot tests, you need to add these lines to the test file:. >. >. > https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L13. >. > And do each test like this (replace â€œxyzâ€ with whatever you want):. >. > def test_xyz(image_comparer):. >. > save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). >. > [â€¦]. >. > sc.pl.xyz(adata, â€¦). >. > save_and_compare_images('xyz'). >. > This will make the tests save your plots to scanpy/tests/figures and. > compare them to the images in scanpy/test/_images. The tests will fail. > because scanpy/test/_images/xyz.png doesnâ€™t exist. You need to copy. > the pngs from scanpy/tests/figuresâ†’scanpy/test/_images and git commit. > them. > 3. >. > This needs to be fixed: #644 (comment). > <https://github.com/theislab/scanpy/pull/644#discussion_r284652144>. > 4. >. > I think the test data might be too large. @falexwolf. > <https://github.com/falexwolf> do we have a recommended size for new. > test data? >. > @Khalid-Usman <https://github.com/Khalid-Usman> Iâ€™m sorry if you find. > that this takes long and is frustrating. If this is the case, just step. > away for a while and do something else! But I think you wonâ€™t regret doing. > this. Youâ€™re learning good coding practices here that will come in handy in. > the future, I promise! >. > Thank you for your contribution ðŸŽ‰. >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGODHLQPIDWZGLGTKXGLPWJUZNA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVYG3VA#issuecomment-493907412>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOG73UUYPXGY7UICKODPWJUZNANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1904,testability,test,test,1904,"ons. I.e. in. > test_genes_ranking() you should do assert. > np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...]) or. > so! > 2. >. > For the plot tests, you need to add these lines to the test file:. >. >. > https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L13. >. > And do each test like this (replace â€œxyzâ€ with whatever you want):. >. > def test_xyz(image_comparer):. >. > save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). >. > [â€¦]. >. > sc.pl.xyz(adata, â€¦). >. > save_and_compare_images('xyz'). >. > This will make the tests save your plots to scanpy/tests/figures and. > compare them to the images in scanpy/test/_images. The tests will fail. > because scanpy/test/_images/xyz.png doesnâ€™t exist. You need to copy. > the pngs from scanpy/tests/figuresâ†’scanpy/test/_images and git commit. > them. > 3. >. > This needs to be fixed: #644 (comment). > <https://github.com/theislab/scanpy/pull/644#discussion_r284652144>. > 4. >. > I think the test data might be too large. @falexwolf. > <https://github.com/falexwolf> do we have a recommended size for new. > test data? >. > @Khalid-Usman <https://github.com/Khalid-Usman> Iâ€™m sorry if you find. > that this takes long and is frustrating. If this is the case, just step. > away for a while and do something else! But I think you wonâ€™t regret doing. > this. Youâ€™re learning good coding practices here that will come in handy in. > the future, I promise! >. > Thank you for your contribution ðŸŽ‰. >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGODHLQPIDWZGLGTKXGLPWJUZNA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVYG3VA#issuecomment-493907412>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOG73UUYPXGY7UICKODPWJUZNANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:2159,usability,learn,learning,2159,"ons. I.e. in. > test_genes_ranking() you should do assert. > np.all(adata.uns['wilcoxon']['names'][:5] == ['Gene1, 'Gene2, ...]) or. > so! > 2. >. > For the plot tests, you need to add these lines to the test file:. >. >. > https://github.com/theislab/scanpy/blob/d979267f48607fd609954c96cd5c586b6135dc30/scanpy/tests/test_plotting.py#L3-L13. >. > And do each test like this (replace â€œxyzâ€ with whatever you want):. >. > def test_xyz(image_comparer):. >. > save_and_compare_images = image_comparer(ROOT, FIGS, tol=15). >. > [â€¦]. >. > sc.pl.xyz(adata, â€¦). >. > save_and_compare_images('xyz'). >. > This will make the tests save your plots to scanpy/tests/figures and. > compare them to the images in scanpy/test/_images. The tests will fail. > because scanpy/test/_images/xyz.png doesnâ€™t exist. You need to copy. > the pngs from scanpy/tests/figuresâ†’scanpy/test/_images and git commit. > them. > 3. >. > This needs to be fixed: #644 (comment). > <https://github.com/theislab/scanpy/pull/644#discussion_r284652144>. > 4. >. > I think the test data might be too large. @falexwolf. > <https://github.com/falexwolf> do we have a recommended size for new. > test data? >. > @Khalid-Usman <https://github.com/Khalid-Usman> Iâ€™m sorry if you find. > that this takes long and is frustrating. If this is the case, just step. > away for a while and do something else! But I think you wonâ€™t regret doing. > this. Youâ€™re learning good coding practices here that will come in handy in. > the future, I promise! >. > Thank you for your contribution ðŸŽ‰. >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGODHLQPIDWZGLGTKXGLPWJUZNA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVYG3VA#issuecomment-493907412>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOG73UUYPXGY7UICKODPWJUZNANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:21,availability,down,downweight,21,"So you want to e.g., downweight the likelihood of sampling cells with a particular feature (like a common cell type), and upweight others. What do you want to use this weighting for now in the `sc.tl.rank_genes_groups` function? Or in the visualization functions you changed?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:239,usability,visual,visualization,239,"So you want to e.g., downweight the likelihood of sampling cells with a particular feature (like a common cell type), and upweight others. What do you want to use this weighting for now in the `sc.tl.rank_genes_groups` function? Or in the visualization functions you changed?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:616,availability,cluster,clustering,616,"1. No, I have sampled cells with weights, out of those 1000 rows most. having weight=1, e.g. 1 row has weight 125, then in gene ranking the. expression all genes will multiplied with that specific weight of cell, so. I updated code by calculated weighted mean and variance. Before updating. this I was getting wrong marker genes. Same for plotting points in dotplot,. stacked_violin and heatmap. 2. I suggest that scanpy should support weighted data, I mean PCA should. also be computed for data with weighted observations (PCA in matlab support. weighted observations). Currently my input is weighted PCA data for. clustering, so I don't need to update PCA code, but in future it will be a. good thing to support scanpy for weighted sampled data as well. Thanks,. Khalid. On Tue, May 21, 2019 at 3:00 AM MalteDLuecken <notifications@github.com>. wrote:. > So you want to e.g., downweight the likelihood of sampling cells with a. > particular feature (like a common cell type), and upweight others. What do. > you want to use this weighting for now in the sc.tl.rank_genes_groups. > function? Or in the visualization functions you changed? >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOHR7Q62WL6MCWX7UWTPWLYOLA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVZYS3I#issuecomment-494111085>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOHHMFZBUTLM4VIJCTLPWLYOLANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:878,availability,down,downweight,878,"1. No, I have sampled cells with weights, out of those 1000 rows most. having weight=1, e.g. 1 row has weight 125, then in gene ranking the. expression all genes will multiplied with that specific weight of cell, so. I updated code by calculated weighted mean and variance. Before updating. this I was getting wrong marker genes. Same for plotting points in dotplot,. stacked_violin and heatmap. 2. I suggest that scanpy should support weighted data, I mean PCA should. also be computed for data with weighted observations (PCA in matlab support. weighted observations). Currently my input is weighted PCA data for. clustering, so I don't need to update PCA code, but in future it will be a. good thing to support scanpy for weighted sampled data as well. Thanks,. Khalid. On Tue, May 21, 2019 at 3:00 AM MalteDLuecken <notifications@github.com>. wrote:. > So you want to e.g., downweight the likelihood of sampling cells with a. > particular feature (like a common cell type), and upweight others. What do. > you want to use this weighting for now in the sc.tl.rank_genes_groups. > function? Or in the visualization functions you changed? >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOHR7Q62WL6MCWX7UWTPWLYOLA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVZYS3I#issuecomment-494111085>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOHHMFZBUTLM4VIJCTLPWLYOLANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:219,deployability,updat,updated,219,"1. No, I have sampled cells with weights, out of those 1000 rows most. having weight=1, e.g. 1 row has weight 125, then in gene ranking the. expression all genes will multiplied with that specific weight of cell, so. I updated code by calculated weighted mean and variance. Before updating. this I was getting wrong marker genes. Same for plotting points in dotplot,. stacked_violin and heatmap. 2. I suggest that scanpy should support weighted data, I mean PCA should. also be computed for data with weighted observations (PCA in matlab support. weighted observations). Currently my input is weighted PCA data for. clustering, so I don't need to update PCA code, but in future it will be a. good thing to support scanpy for weighted sampled data as well. Thanks,. Khalid. On Tue, May 21, 2019 at 3:00 AM MalteDLuecken <notifications@github.com>. wrote:. > So you want to e.g., downweight the likelihood of sampling cells with a. > particular feature (like a common cell type), and upweight others. What do. > you want to use this weighting for now in the sc.tl.rank_genes_groups. > function? Or in the visualization functions you changed? >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOHR7Q62WL6MCWX7UWTPWLYOLA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVZYS3I#issuecomment-494111085>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOHHMFZBUTLM4VIJCTLPWLYOLANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:281,deployability,updat,updating,281,"1. No, I have sampled cells with weights, out of those 1000 rows most. having weight=1, e.g. 1 row has weight 125, then in gene ranking the. expression all genes will multiplied with that specific weight of cell, so. I updated code by calculated weighted mean and variance. Before updating. this I was getting wrong marker genes. Same for plotting points in dotplot,. stacked_violin and heatmap. 2. I suggest that scanpy should support weighted data, I mean PCA should. also be computed for data with weighted observations (PCA in matlab support. weighted observations). Currently my input is weighted PCA data for. clustering, so I don't need to update PCA code, but in future it will be a. good thing to support scanpy for weighted sampled data as well. Thanks,. Khalid. On Tue, May 21, 2019 at 3:00 AM MalteDLuecken <notifications@github.com>. wrote:. > So you want to e.g., downweight the likelihood of sampling cells with a. > particular feature (like a common cell type), and upweight others. What do. > you want to use this weighting for now in the sc.tl.rank_genes_groups. > function? Or in the visualization functions you changed? >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOHR7Q62WL6MCWX7UWTPWLYOLA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVZYS3I#issuecomment-494111085>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOHHMFZBUTLM4VIJCTLPWLYOLANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:510,deployability,observ,observations,510,"1. No, I have sampled cells with weights, out of those 1000 rows most. having weight=1, e.g. 1 row has weight 125, then in gene ranking the. expression all genes will multiplied with that specific weight of cell, so. I updated code by calculated weighted mean and variance. Before updating. this I was getting wrong marker genes. Same for plotting points in dotplot,. stacked_violin and heatmap. 2. I suggest that scanpy should support weighted data, I mean PCA should. also be computed for data with weighted observations (PCA in matlab support. weighted observations). Currently my input is weighted PCA data for. clustering, so I don't need to update PCA code, but in future it will be a. good thing to support scanpy for weighted sampled data as well. Thanks,. Khalid. On Tue, May 21, 2019 at 3:00 AM MalteDLuecken <notifications@github.com>. wrote:. > So you want to e.g., downweight the likelihood of sampling cells with a. > particular feature (like a common cell type), and upweight others. What do. > you want to use this weighting for now in the sc.tl.rank_genes_groups. > function? Or in the visualization functions you changed? >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOHR7Q62WL6MCWX7UWTPWLYOLA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVZYS3I#issuecomment-494111085>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOHHMFZBUTLM4VIJCTLPWLYOLANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:556,deployability,observ,observations,556,"1. No, I have sampled cells with weights, out of those 1000 rows most. having weight=1, e.g. 1 row has weight 125, then in gene ranking the. expression all genes will multiplied with that specific weight of cell, so. I updated code by calculated weighted mean and variance. Before updating. this I was getting wrong marker genes. Same for plotting points in dotplot,. stacked_violin and heatmap. 2. I suggest that scanpy should support weighted data, I mean PCA should. also be computed for data with weighted observations (PCA in matlab support. weighted observations). Currently my input is weighted PCA data for. clustering, so I don't need to update PCA code, but in future it will be a. good thing to support scanpy for weighted sampled data as well. Thanks,. Khalid. On Tue, May 21, 2019 at 3:00 AM MalteDLuecken <notifications@github.com>. wrote:. > So you want to e.g., downweight the likelihood of sampling cells with a. > particular feature (like a common cell type), and upweight others. What do. > you want to use this weighting for now in the sc.tl.rank_genes_groups. > function? Or in the visualization functions you changed? >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOHR7Q62WL6MCWX7UWTPWLYOLA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVZYS3I#issuecomment-494111085>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOHHMFZBUTLM4VIJCTLPWLYOLANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:616,deployability,cluster,clustering,616,"1. No, I have sampled cells with weights, out of those 1000 rows most. having weight=1, e.g. 1 row has weight 125, then in gene ranking the. expression all genes will multiplied with that specific weight of cell, so. I updated code by calculated weighted mean and variance. Before updating. this I was getting wrong marker genes. Same for plotting points in dotplot,. stacked_violin and heatmap. 2. I suggest that scanpy should support weighted data, I mean PCA should. also be computed for data with weighted observations (PCA in matlab support. weighted observations). Currently my input is weighted PCA data for. clustering, so I don't need to update PCA code, but in future it will be a. good thing to support scanpy for weighted sampled data as well. Thanks,. Khalid. On Tue, May 21, 2019 at 3:00 AM MalteDLuecken <notifications@github.com>. wrote:. > So you want to e.g., downweight the likelihood of sampling cells with a. > particular feature (like a common cell type), and upweight others. What do. > you want to use this weighting for now in the sc.tl.rank_genes_groups. > function? Or in the visualization functions you changed? >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOHR7Q62WL6MCWX7UWTPWLYOLA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVZYS3I#issuecomment-494111085>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOHHMFZBUTLM4VIJCTLPWLYOLANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:647,deployability,updat,update,647,"1. No, I have sampled cells with weights, out of those 1000 rows most. having weight=1, e.g. 1 row has weight 125, then in gene ranking the. expression all genes will multiplied with that specific weight of cell, so. I updated code by calculated weighted mean and variance. Before updating. this I was getting wrong marker genes. Same for plotting points in dotplot,. stacked_violin and heatmap. 2. I suggest that scanpy should support weighted data, I mean PCA should. also be computed for data with weighted observations (PCA in matlab support. weighted observations). Currently my input is weighted PCA data for. clustering, so I don't need to update PCA code, but in future it will be a. good thing to support scanpy for weighted sampled data as well. Thanks,. Khalid. On Tue, May 21, 2019 at 3:00 AM MalteDLuecken <notifications@github.com>. wrote:. > So you want to e.g., downweight the likelihood of sampling cells with a. > particular feature (like a common cell type), and upweight others. What do. > you want to use this weighting for now in the sc.tl.rank_genes_groups. > function? Or in the visualization functions you changed? >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOHR7Q62WL6MCWX7UWTPWLYOLA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVZYS3I#issuecomment-494111085>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOHHMFZBUTLM4VIJCTLPWLYOLANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:387,energy efficiency,heat,heatmap,387,"1. No, I have sampled cells with weights, out of those 1000 rows most. having weight=1, e.g. 1 row has weight 125, then in gene ranking the. expression all genes will multiplied with that specific weight of cell, so. I updated code by calculated weighted mean and variance. Before updating. this I was getting wrong marker genes. Same for plotting points in dotplot,. stacked_violin and heatmap. 2. I suggest that scanpy should support weighted data, I mean PCA should. also be computed for data with weighted observations (PCA in matlab support. weighted observations). Currently my input is weighted PCA data for. clustering, so I don't need to update PCA code, but in future it will be a. good thing to support scanpy for weighted sampled data as well. Thanks,. Khalid. On Tue, May 21, 2019 at 3:00 AM MalteDLuecken <notifications@github.com>. wrote:. > So you want to e.g., downweight the likelihood of sampling cells with a. > particular feature (like a common cell type), and upweight others. What do. > you want to use this weighting for now in the sc.tl.rank_genes_groups. > function? Or in the visualization functions you changed? >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOHR7Q62WL6MCWX7UWTPWLYOLA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVZYS3I#issuecomment-494111085>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOHHMFZBUTLM4VIJCTLPWLYOLANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:571,energy efficiency,Current,Currently,571,"1. No, I have sampled cells with weights, out of those 1000 rows most. having weight=1, e.g. 1 row has weight 125, then in gene ranking the. expression all genes will multiplied with that specific weight of cell, so. I updated code by calculated weighted mean and variance. Before updating. this I was getting wrong marker genes. Same for plotting points in dotplot,. stacked_violin and heatmap. 2. I suggest that scanpy should support weighted data, I mean PCA should. also be computed for data with weighted observations (PCA in matlab support. weighted observations). Currently my input is weighted PCA data for. clustering, so I don't need to update PCA code, but in future it will be a. good thing to support scanpy for weighted sampled data as well. Thanks,. Khalid. On Tue, May 21, 2019 at 3:00 AM MalteDLuecken <notifications@github.com>. wrote:. > So you want to e.g., downweight the likelihood of sampling cells with a. > particular feature (like a common cell type), and upweight others. What do. > you want to use this weighting for now in the sc.tl.rank_genes_groups. > function? Or in the visualization functions you changed? >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOHR7Q62WL6MCWX7UWTPWLYOLA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVZYS3I#issuecomment-494111085>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOHHMFZBUTLM4VIJCTLPWLYOLANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:188,interoperability,specif,specific,188,"1. No, I have sampled cells with weights, out of those 1000 rows most. having weight=1, e.g. 1 row has weight 125, then in gene ranking the. expression all genes will multiplied with that specific weight of cell, so. I updated code by calculated weighted mean and variance. Before updating. this I was getting wrong marker genes. Same for plotting points in dotplot,. stacked_violin and heatmap. 2. I suggest that scanpy should support weighted data, I mean PCA should. also be computed for data with weighted observations (PCA in matlab support. weighted observations). Currently my input is weighted PCA data for. clustering, so I don't need to update PCA code, but in future it will be a. good thing to support scanpy for weighted sampled data as well. Thanks,. Khalid. On Tue, May 21, 2019 at 3:00 AM MalteDLuecken <notifications@github.com>. wrote:. > So you want to e.g., downweight the likelihood of sampling cells with a. > particular feature (like a common cell type), and upweight others. What do. > you want to use this weighting for now in the sc.tl.rank_genes_groups. > function? Or in the visualization functions you changed? >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOHR7Q62WL6MCWX7UWTPWLYOLA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVZYS3I#issuecomment-494111085>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOHHMFZBUTLM4VIJCTLPWLYOLANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:219,safety,updat,updated,219,"1. No, I have sampled cells with weights, out of those 1000 rows most. having weight=1, e.g. 1 row has weight 125, then in gene ranking the. expression all genes will multiplied with that specific weight of cell, so. I updated code by calculated weighted mean and variance. Before updating. this I was getting wrong marker genes. Same for plotting points in dotplot,. stacked_violin and heatmap. 2. I suggest that scanpy should support weighted data, I mean PCA should. also be computed for data with weighted observations (PCA in matlab support. weighted observations). Currently my input is weighted PCA data for. clustering, so I don't need to update PCA code, but in future it will be a. good thing to support scanpy for weighted sampled data as well. Thanks,. Khalid. On Tue, May 21, 2019 at 3:00 AM MalteDLuecken <notifications@github.com>. wrote:. > So you want to e.g., downweight the likelihood of sampling cells with a. > particular feature (like a common cell type), and upweight others. What do. > you want to use this weighting for now in the sc.tl.rank_genes_groups. > function? Or in the visualization functions you changed? >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOHR7Q62WL6MCWX7UWTPWLYOLA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVZYS3I#issuecomment-494111085>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOHHMFZBUTLM4VIJCTLPWLYOLANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:281,safety,updat,updating,281,"1. No, I have sampled cells with weights, out of those 1000 rows most. having weight=1, e.g. 1 row has weight 125, then in gene ranking the. expression all genes will multiplied with that specific weight of cell, so. I updated code by calculated weighted mean and variance. Before updating. this I was getting wrong marker genes. Same for plotting points in dotplot,. stacked_violin and heatmap. 2. I suggest that scanpy should support weighted data, I mean PCA should. also be computed for data with weighted observations (PCA in matlab support. weighted observations). Currently my input is weighted PCA data for. clustering, so I don't need to update PCA code, but in future it will be a. good thing to support scanpy for weighted sampled data as well. Thanks,. Khalid. On Tue, May 21, 2019 at 3:00 AM MalteDLuecken <notifications@github.com>. wrote:. > So you want to e.g., downweight the likelihood of sampling cells with a. > particular feature (like a common cell type), and upweight others. What do. > you want to use this weighting for now in the sc.tl.rank_genes_groups. > function? Or in the visualization functions you changed? >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOHR7Q62WL6MCWX7UWTPWLYOLA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVZYS3I#issuecomment-494111085>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOHHMFZBUTLM4VIJCTLPWLYOLANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:584,safety,input,input,584,"1. No, I have sampled cells with weights, out of those 1000 rows most. having weight=1, e.g. 1 row has weight 125, then in gene ranking the. expression all genes will multiplied with that specific weight of cell, so. I updated code by calculated weighted mean and variance. Before updating. this I was getting wrong marker genes. Same for plotting points in dotplot,. stacked_violin and heatmap. 2. I suggest that scanpy should support weighted data, I mean PCA should. also be computed for data with weighted observations (PCA in matlab support. weighted observations). Currently my input is weighted PCA data for. clustering, so I don't need to update PCA code, but in future it will be a. good thing to support scanpy for weighted sampled data as well. Thanks,. Khalid. On Tue, May 21, 2019 at 3:00 AM MalteDLuecken <notifications@github.com>. wrote:. > So you want to e.g., downweight the likelihood of sampling cells with a. > particular feature (like a common cell type), and upweight others. What do. > you want to use this weighting for now in the sc.tl.rank_genes_groups. > function? Or in the visualization functions you changed? >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOHR7Q62WL6MCWX7UWTPWLYOLA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVZYS3I#issuecomment-494111085>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOHHMFZBUTLM4VIJCTLPWLYOLANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:647,safety,updat,update,647,"1. No, I have sampled cells with weights, out of those 1000 rows most. having weight=1, e.g. 1 row has weight 125, then in gene ranking the. expression all genes will multiplied with that specific weight of cell, so. I updated code by calculated weighted mean and variance. Before updating. this I was getting wrong marker genes. Same for plotting points in dotplot,. stacked_violin and heatmap. 2. I suggest that scanpy should support weighted data, I mean PCA should. also be computed for data with weighted observations (PCA in matlab support. weighted observations). Currently my input is weighted PCA data for. clustering, so I don't need to update PCA code, but in future it will be a. good thing to support scanpy for weighted sampled data as well. Thanks,. Khalid. On Tue, May 21, 2019 at 3:00 AM MalteDLuecken <notifications@github.com>. wrote:. > So you want to e.g., downweight the likelihood of sampling cells with a. > particular feature (like a common cell type), and upweight others. What do. > you want to use this weighting for now in the sc.tl.rank_genes_groups. > function? Or in the visualization functions you changed? >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOHR7Q62WL6MCWX7UWTPWLYOLA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVZYS3I#issuecomment-494111085>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOHHMFZBUTLM4VIJCTLPWLYOLANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:219,security,updat,updated,219,"1. No, I have sampled cells with weights, out of those 1000 rows most. having weight=1, e.g. 1 row has weight 125, then in gene ranking the. expression all genes will multiplied with that specific weight of cell, so. I updated code by calculated weighted mean and variance. Before updating. this I was getting wrong marker genes. Same for plotting points in dotplot,. stacked_violin and heatmap. 2. I suggest that scanpy should support weighted data, I mean PCA should. also be computed for data with weighted observations (PCA in matlab support. weighted observations). Currently my input is weighted PCA data for. clustering, so I don't need to update PCA code, but in future it will be a. good thing to support scanpy for weighted sampled data as well. Thanks,. Khalid. On Tue, May 21, 2019 at 3:00 AM MalteDLuecken <notifications@github.com>. wrote:. > So you want to e.g., downweight the likelihood of sampling cells with a. > particular feature (like a common cell type), and upweight others. What do. > you want to use this weighting for now in the sc.tl.rank_genes_groups. > function? Or in the visualization functions you changed? >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOHR7Q62WL6MCWX7UWTPWLYOLA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVZYS3I#issuecomment-494111085>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOHHMFZBUTLM4VIJCTLPWLYOLANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:281,security,updat,updating,281,"1. No, I have sampled cells with weights, out of those 1000 rows most. having weight=1, e.g. 1 row has weight 125, then in gene ranking the. expression all genes will multiplied with that specific weight of cell, so. I updated code by calculated weighted mean and variance. Before updating. this I was getting wrong marker genes. Same for plotting points in dotplot,. stacked_violin and heatmap. 2. I suggest that scanpy should support weighted data, I mean PCA should. also be computed for data with weighted observations (PCA in matlab support. weighted observations). Currently my input is weighted PCA data for. clustering, so I don't need to update PCA code, but in future it will be a. good thing to support scanpy for weighted sampled data as well. Thanks,. Khalid. On Tue, May 21, 2019 at 3:00 AM MalteDLuecken <notifications@github.com>. wrote:. > So you want to e.g., downweight the likelihood of sampling cells with a. > particular feature (like a common cell type), and upweight others. What do. > you want to use this weighting for now in the sc.tl.rank_genes_groups. > function? Or in the visualization functions you changed? >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOHR7Q62WL6MCWX7UWTPWLYOLA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVZYS3I#issuecomment-494111085>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOHHMFZBUTLM4VIJCTLPWLYOLANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:647,security,updat,update,647,"1. No, I have sampled cells with weights, out of those 1000 rows most. having weight=1, e.g. 1 row has weight 125, then in gene ranking the. expression all genes will multiplied with that specific weight of cell, so. I updated code by calculated weighted mean and variance. Before updating. this I was getting wrong marker genes. Same for plotting points in dotplot,. stacked_violin and heatmap. 2. I suggest that scanpy should support weighted data, I mean PCA should. also be computed for data with weighted observations (PCA in matlab support. weighted observations). Currently my input is weighted PCA data for. clustering, so I don't need to update PCA code, but in future it will be a. good thing to support scanpy for weighted sampled data as well. Thanks,. Khalid. On Tue, May 21, 2019 at 3:00 AM MalteDLuecken <notifications@github.com>. wrote:. > So you want to e.g., downweight the likelihood of sampling cells with a. > particular feature (like a common cell type), and upweight others. What do. > you want to use this weighting for now in the sc.tl.rank_genes_groups. > function? Or in the visualization functions you changed? >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOHR7Q62WL6MCWX7UWTPWLYOLA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVZYS3I#issuecomment-494111085>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOHHMFZBUTLM4VIJCTLPWLYOLANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1546,security,auth,auth,1546,"1. No, I have sampled cells with weights, out of those 1000 rows most. having weight=1, e.g. 1 row has weight 125, then in gene ranking the. expression all genes will multiplied with that specific weight of cell, so. I updated code by calculated weighted mean and variance. Before updating. this I was getting wrong marker genes. Same for plotting points in dotplot,. stacked_violin and heatmap. 2. I suggest that scanpy should support weighted data, I mean PCA should. also be computed for data with weighted observations (PCA in matlab support. weighted observations). Currently my input is weighted PCA data for. clustering, so I don't need to update PCA code, but in future it will be a. good thing to support scanpy for weighted sampled data as well. Thanks,. Khalid. On Tue, May 21, 2019 at 3:00 AM MalteDLuecken <notifications@github.com>. wrote:. > So you want to e.g., downweight the likelihood of sampling cells with a. > particular feature (like a common cell type), and upweight others. What do. > you want to use this weighting for now in the sc.tl.rank_genes_groups. > function? Or in the visualization functions you changed? >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOHR7Q62WL6MCWX7UWTPWLYOLA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVZYS3I#issuecomment-494111085>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOHHMFZBUTLM4VIJCTLPWLYOLANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:510,testability,observ,observations,510,"1. No, I have sampled cells with weights, out of those 1000 rows most. having weight=1, e.g. 1 row has weight 125, then in gene ranking the. expression all genes will multiplied with that specific weight of cell, so. I updated code by calculated weighted mean and variance. Before updating. this I was getting wrong marker genes. Same for plotting points in dotplot,. stacked_violin and heatmap. 2. I suggest that scanpy should support weighted data, I mean PCA should. also be computed for data with weighted observations (PCA in matlab support. weighted observations). Currently my input is weighted PCA data for. clustering, so I don't need to update PCA code, but in future it will be a. good thing to support scanpy for weighted sampled data as well. Thanks,. Khalid. On Tue, May 21, 2019 at 3:00 AM MalteDLuecken <notifications@github.com>. wrote:. > So you want to e.g., downweight the likelihood of sampling cells with a. > particular feature (like a common cell type), and upweight others. What do. > you want to use this weighting for now in the sc.tl.rank_genes_groups. > function? Or in the visualization functions you changed? >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOHR7Q62WL6MCWX7UWTPWLYOLA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVZYS3I#issuecomment-494111085>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOHHMFZBUTLM4VIJCTLPWLYOLANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:556,testability,observ,observations,556,"1. No, I have sampled cells with weights, out of those 1000 rows most. having weight=1, e.g. 1 row has weight 125, then in gene ranking the. expression all genes will multiplied with that specific weight of cell, so. I updated code by calculated weighted mean and variance. Before updating. this I was getting wrong marker genes. Same for plotting points in dotplot,. stacked_violin and heatmap. 2. I suggest that scanpy should support weighted data, I mean PCA should. also be computed for data with weighted observations (PCA in matlab support. weighted observations). Currently my input is weighted PCA data for. clustering, so I don't need to update PCA code, but in future it will be a. good thing to support scanpy for weighted sampled data as well. Thanks,. Khalid. On Tue, May 21, 2019 at 3:00 AM MalteDLuecken <notifications@github.com>. wrote:. > So you want to e.g., downweight the likelihood of sampling cells with a. > particular feature (like a common cell type), and upweight others. What do. > you want to use this weighting for now in the sc.tl.rank_genes_groups. > function? Or in the visualization functions you changed? >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOHR7Q62WL6MCWX7UWTPWLYOLA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVZYS3I#issuecomment-494111085>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOHHMFZBUTLM4VIJCTLPWLYOLANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:428,usability,support,support,428,"1. No, I have sampled cells with weights, out of those 1000 rows most. having weight=1, e.g. 1 row has weight 125, then in gene ranking the. expression all genes will multiplied with that specific weight of cell, so. I updated code by calculated weighted mean and variance. Before updating. this I was getting wrong marker genes. Same for plotting points in dotplot,. stacked_violin and heatmap. 2. I suggest that scanpy should support weighted data, I mean PCA should. also be computed for data with weighted observations (PCA in matlab support. weighted observations). Currently my input is weighted PCA data for. clustering, so I don't need to update PCA code, but in future it will be a. good thing to support scanpy for weighted sampled data as well. Thanks,. Khalid. On Tue, May 21, 2019 at 3:00 AM MalteDLuecken <notifications@github.com>. wrote:. > So you want to e.g., downweight the likelihood of sampling cells with a. > particular feature (like a common cell type), and upweight others. What do. > you want to use this weighting for now in the sc.tl.rank_genes_groups. > function? Or in the visualization functions you changed? >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOHR7Q62WL6MCWX7UWTPWLYOLA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVZYS3I#issuecomment-494111085>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOHHMFZBUTLM4VIJCTLPWLYOLANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:538,usability,support,support,538,"1. No, I have sampled cells with weights, out of those 1000 rows most. having weight=1, e.g. 1 row has weight 125, then in gene ranking the. expression all genes will multiplied with that specific weight of cell, so. I updated code by calculated weighted mean and variance. Before updating. this I was getting wrong marker genes. Same for plotting points in dotplot,. stacked_violin and heatmap. 2. I suggest that scanpy should support weighted data, I mean PCA should. also be computed for data with weighted observations (PCA in matlab support. weighted observations). Currently my input is weighted PCA data for. clustering, so I don't need to update PCA code, but in future it will be a. good thing to support scanpy for weighted sampled data as well. Thanks,. Khalid. On Tue, May 21, 2019 at 3:00 AM MalteDLuecken <notifications@github.com>. wrote:. > So you want to e.g., downweight the likelihood of sampling cells with a. > particular feature (like a common cell type), and upweight others. What do. > you want to use this weighting for now in the sc.tl.rank_genes_groups. > function? Or in the visualization functions you changed? >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOHR7Q62WL6MCWX7UWTPWLYOLA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVZYS3I#issuecomment-494111085>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOHHMFZBUTLM4VIJCTLPWLYOLANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:584,usability,input,input,584,"1. No, I have sampled cells with weights, out of those 1000 rows most. having weight=1, e.g. 1 row has weight 125, then in gene ranking the. expression all genes will multiplied with that specific weight of cell, so. I updated code by calculated weighted mean and variance. Before updating. this I was getting wrong marker genes. Same for plotting points in dotplot,. stacked_violin and heatmap. 2. I suggest that scanpy should support weighted data, I mean PCA should. also be computed for data with weighted observations (PCA in matlab support. weighted observations). Currently my input is weighted PCA data for. clustering, so I don't need to update PCA code, but in future it will be a. good thing to support scanpy for weighted sampled data as well. Thanks,. Khalid. On Tue, May 21, 2019 at 3:00 AM MalteDLuecken <notifications@github.com>. wrote:. > So you want to e.g., downweight the likelihood of sampling cells with a. > particular feature (like a common cell type), and upweight others. What do. > you want to use this weighting for now in the sc.tl.rank_genes_groups. > function? Or in the visualization functions you changed? >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOHR7Q62WL6MCWX7UWTPWLYOLA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVZYS3I#issuecomment-494111085>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOHHMFZBUTLM4VIJCTLPWLYOLANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:706,usability,support,support,706,"1. No, I have sampled cells with weights, out of those 1000 rows most. having weight=1, e.g. 1 row has weight 125, then in gene ranking the. expression all genes will multiplied with that specific weight of cell, so. I updated code by calculated weighted mean and variance. Before updating. this I was getting wrong marker genes. Same for plotting points in dotplot,. stacked_violin and heatmap. 2. I suggest that scanpy should support weighted data, I mean PCA should. also be computed for data with weighted observations (PCA in matlab support. weighted observations). Currently my input is weighted PCA data for. clustering, so I don't need to update PCA code, but in future it will be a. good thing to support scanpy for weighted sampled data as well. Thanks,. Khalid. On Tue, May 21, 2019 at 3:00 AM MalteDLuecken <notifications@github.com>. wrote:. > So you want to e.g., downweight the likelihood of sampling cells with a. > particular feature (like a common cell type), and upweight others. What do. > you want to use this weighting for now in the sc.tl.rank_genes_groups. > function? Or in the visualization functions you changed? >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOHR7Q62WL6MCWX7UWTPWLYOLA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVZYS3I#issuecomment-494111085>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOHHMFZBUTLM4VIJCTLPWLYOLANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1103,usability,visual,visualization,1103,"1. No, I have sampled cells with weights, out of those 1000 rows most. having weight=1, e.g. 1 row has weight 125, then in gene ranking the. expression all genes will multiplied with that specific weight of cell, so. I updated code by calculated weighted mean and variance. Before updating. this I was getting wrong marker genes. Same for plotting points in dotplot,. stacked_violin and heatmap. 2. I suggest that scanpy should support weighted data, I mean PCA should. also be computed for data with weighted observations (PCA in matlab support. weighted observations). Currently my input is weighted PCA data for. clustering, so I don't need to update PCA code, but in future it will be a. good thing to support scanpy for weighted sampled data as well. Thanks,. Khalid. On Tue, May 21, 2019 at 3:00 AM MalteDLuecken <notifications@github.com>. wrote:. > So you want to e.g., downweight the likelihood of sampling cells with a. > particular feature (like a common cell type), and upweight others. What do. > you want to use this weighting for now in the sc.tl.rank_genes_groups. > function? Or in the visualization functions you changed? >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOHR7Q62WL6MCWX7UWTPWLYOLA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVZYS3I#issuecomment-494111085>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOHHMFZBUTLM4VIJCTLPWLYOLANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:584,availability,error,error,584,I'm not sure I entirely understand what the weights are based on. I'm trying to understand when you would suggest someone use your approach. Why do you give one cell a weight of 125? With this type of weight distribution you are basically manually changing the marker gene calculation focusing nearly only on a single cell. That seems strange to me. I'm trying to understand the need for scanpy to support weighted observations. At the moment I don't see when you would want to differently weight the observations... I'm familiar with using weights if I have some form of measurement error or uncertainty between samples. I don't really see how that holds here. Do you weight the cells based on some kind of quality score?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:415,deployability,observ,observations,415,I'm not sure I entirely understand what the weights are based on. I'm trying to understand when you would suggest someone use your approach. Why do you give one cell a weight of 125? With this type of weight distribution you are basically manually changing the marker gene calculation focusing nearly only on a single cell. That seems strange to me. I'm trying to understand the need for scanpy to support weighted observations. At the moment I don't see when you would want to differently weight the observations... I'm familiar with using weights if I have some form of measurement error or uncertainty between samples. I don't really see how that holds here. Do you weight the cells based on some kind of quality score?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:501,deployability,observ,observations,501,I'm not sure I entirely understand what the weights are based on. I'm trying to understand when you would suggest someone use your approach. Why do you give one cell a weight of 125? With this type of weight distribution you are basically manually changing the marker gene calculation focusing nearly only on a single cell. That seems strange to me. I'm trying to understand the need for scanpy to support weighted observations. At the moment I don't see when you would want to differently weight the observations... I'm familiar with using weights if I have some form of measurement error or uncertainty between samples. I don't really see how that holds here. Do you weight the cells based on some kind of quality score?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:572,energy efficiency,measur,measurement,572,I'm not sure I entirely understand what the weights are based on. I'm trying to understand when you would suggest someone use your approach. Why do you give one cell a weight of 125? With this type of weight distribution you are basically manually changing the marker gene calculation focusing nearly only on a single cell. That seems strange to me. I'm trying to understand the need for scanpy to support weighted observations. At the moment I don't see when you would want to differently weight the observations... I'm familiar with using weights if I have some form of measurement error or uncertainty between samples. I don't really see how that holds here. Do you weight the cells based on some kind of quality score?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:208,interoperability,distribut,distribution,208,I'm not sure I entirely understand what the weights are based on. I'm trying to understand when you would suggest someone use your approach. Why do you give one cell a weight of 125? With this type of weight distribution you are basically manually changing the marker gene calculation focusing nearly only on a single cell. That seems strange to me. I'm trying to understand the need for scanpy to support weighted observations. At the moment I don't see when you would want to differently weight the observations... I'm familiar with using weights if I have some form of measurement error or uncertainty between samples. I don't really see how that holds here. Do you weight the cells based on some kind of quality score?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:584,performance,error,error,584,I'm not sure I entirely understand what the weights are based on. I'm trying to understand when you would suggest someone use your approach. Why do you give one cell a weight of 125? With this type of weight distribution you are basically manually changing the marker gene calculation focusing nearly only on a single cell. That seems strange to me. I'm trying to understand the need for scanpy to support weighted observations. At the moment I don't see when you would want to differently weight the observations... I'm familiar with using weights if I have some form of measurement error or uncertainty between samples. I don't really see how that holds here. Do you weight the cells based on some kind of quality score?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:584,safety,error,error,584,I'm not sure I entirely understand what the weights are based on. I'm trying to understand when you would suggest someone use your approach. Why do you give one cell a weight of 125? With this type of weight distribution you are basically manually changing the marker gene calculation focusing nearly only on a single cell. That seems strange to me. I'm trying to understand the need for scanpy to support weighted observations. At the moment I don't see when you would want to differently weight the observations... I'm familiar with using weights if I have some form of measurement error or uncertainty between samples. I don't really see how that holds here. Do you weight the cells based on some kind of quality score?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:24,testability,understand,understand,24,I'm not sure I entirely understand what the weights are based on. I'm trying to understand when you would suggest someone use your approach. Why do you give one cell a weight of 125? With this type of weight distribution you are basically manually changing the marker gene calculation focusing nearly only on a single cell. That seems strange to me. I'm trying to understand the need for scanpy to support weighted observations. At the moment I don't see when you would want to differently weight the observations... I'm familiar with using weights if I have some form of measurement error or uncertainty between samples. I don't really see how that holds here. Do you weight the cells based on some kind of quality score?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:80,testability,understand,understand,80,I'm not sure I entirely understand what the weights are based on. I'm trying to understand when you would suggest someone use your approach. Why do you give one cell a weight of 125? With this type of weight distribution you are basically manually changing the marker gene calculation focusing nearly only on a single cell. That seems strange to me. I'm trying to understand the need for scanpy to support weighted observations. At the moment I don't see when you would want to differently weight the observations... I'm familiar with using weights if I have some form of measurement error or uncertainty between samples. I don't really see how that holds here. Do you weight the cells based on some kind of quality score?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:364,testability,understand,understand,364,I'm not sure I entirely understand what the weights are based on. I'm trying to understand when you would suggest someone use your approach. Why do you give one cell a weight of 125? With this type of weight distribution you are basically manually changing the marker gene calculation focusing nearly only on a single cell. That seems strange to me. I'm trying to understand the need for scanpy to support weighted observations. At the moment I don't see when you would want to differently weight the observations... I'm familiar with using weights if I have some form of measurement error or uncertainty between samples. I don't really see how that holds here. Do you weight the cells based on some kind of quality score?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:415,testability,observ,observations,415,I'm not sure I entirely understand what the weights are based on. I'm trying to understand when you would suggest someone use your approach. Why do you give one cell a weight of 125? With this type of weight distribution you are basically manually changing the marker gene calculation focusing nearly only on a single cell. That seems strange to me. I'm trying to understand the need for scanpy to support weighted observations. At the moment I don't see when you would want to differently weight the observations... I'm familiar with using weights if I have some form of measurement error or uncertainty between samples. I don't really see how that holds here. Do you weight the cells based on some kind of quality score?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:501,testability,observ,observations,501,I'm not sure I entirely understand what the weights are based on. I'm trying to understand when you would suggest someone use your approach. Why do you give one cell a weight of 125? With this type of weight distribution you are basically manually changing the marker gene calculation focusing nearly only on a single cell. That seems strange to me. I'm trying to understand the need for scanpy to support weighted observations. At the moment I don't see when you would want to differently weight the observations... I'm familiar with using weights if I have some form of measurement error or uncertainty between samples. I don't really see how that holds here. Do you weight the cells based on some kind of quality score?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:398,usability,support,support,398,I'm not sure I entirely understand what the weights are based on. I'm trying to understand when you would suggest someone use your approach. Why do you give one cell a weight of 125? With this type of weight distribution you are basically manually changing the marker gene calculation focusing nearly only on a single cell. That seems strange to me. I'm trying to understand the need for scanpy to support weighted observations. At the moment I don't see when you would want to differently weight the observations... I'm familiar with using weights if I have some form of measurement error or uncertainty between samples. I don't really see how that holds here. Do you weight the cells based on some kind of quality score?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:584,usability,error,error,584,I'm not sure I entirely understand what the weights are based on. I'm trying to understand when you would suggest someone use your approach. Why do you give one cell a weight of 125? With this type of weight distribution you are basically manually changing the marker gene calculation focusing nearly only on a single cell. That seems strange to me. I'm trying to understand the need for scanpy to support weighted observations. At the moment I don't see when you would want to differently weight the observations... I'm familiar with using weights if I have some form of measurement error or uncertainty between samples. I don't really see how that holds here. Do you weight the cells based on some kind of quality score?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:865,availability,error,error,865,"I am using a sampling technique, which samples few rows without descreasing. performance. So speed is more than 10X time faster for larger dataset with. similar accuracy. On Tue, May 21, 2019 at 3:37 AM MalteDLuecken <notifications@github.com>. wrote:. > I'm not sure I entirely understand what the weights are based on. I'm. > trying to understand when you would suggest someone use your approach. Why. > do you give one cell a weight of 125? With this type of weight distribution. > you are basically manually changing the marker gene calculation focusing. > nearly only on a single cell. That seems strange to me. >. > I'm trying to understand the need for scanpy to support weighted. > observations. At the moment I don't see when you would want to differently. > weight the observations... I'm familiar with using weights if I have some. > form of measurement error or uncertainty between samples. I don't really. > see how that holds here. Do you weight the cells based on some kind of. > quality score? >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOC4EI2YTU53XEGMJI3PWL4XZA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVZ3LJA#issuecomment-494122404>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOFRJXHAWVT6W4YKY63PWL4XZANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:690,deployability,observ,observations,690,"I am using a sampling technique, which samples few rows without descreasing. performance. So speed is more than 10X time faster for larger dataset with. similar accuracy. On Tue, May 21, 2019 at 3:37 AM MalteDLuecken <notifications@github.com>. wrote:. > I'm not sure I entirely understand what the weights are based on. I'm. > trying to understand when you would suggest someone use your approach. Why. > do you give one cell a weight of 125? With this type of weight distribution. > you are basically manually changing the marker gene calculation focusing. > nearly only on a single cell. That seems strange to me. >. > I'm trying to understand the need for scanpy to support weighted. > observations. At the moment I don't see when you would want to differently. > weight the observations... I'm familiar with using weights if I have some. > form of measurement error or uncertainty between samples. I don't really. > see how that holds here. Do you weight the cells based on some kind of. > quality score? >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOC4EI2YTU53XEGMJI3PWL4XZA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVZ3LJA#issuecomment-494122404>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOFRJXHAWVT6W4YKY63PWL4XZANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:779,deployability,observ,observations,779,"I am using a sampling technique, which samples few rows without descreasing. performance. So speed is more than 10X time faster for larger dataset with. similar accuracy. On Tue, May 21, 2019 at 3:37 AM MalteDLuecken <notifications@github.com>. wrote:. > I'm not sure I entirely understand what the weights are based on. I'm. > trying to understand when you would suggest someone use your approach. Why. > do you give one cell a weight of 125? With this type of weight distribution. > you are basically manually changing the marker gene calculation focusing. > nearly only on a single cell. That seems strange to me. >. > I'm trying to understand the need for scanpy to support weighted. > observations. At the moment I don't see when you would want to differently. > weight the observations... I'm familiar with using weights if I have some. > form of measurement error or uncertainty between samples. I don't really. > see how that holds here. Do you weight the cells based on some kind of. > quality score? >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOC4EI2YTU53XEGMJI3PWL4XZA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVZ3LJA#issuecomment-494122404>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOFRJXHAWVT6W4YKY63PWL4XZANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:853,energy efficiency,measur,measurement,853,"I am using a sampling technique, which samples few rows without descreasing. performance. So speed is more than 10X time faster for larger dataset with. similar accuracy. On Tue, May 21, 2019 at 3:37 AM MalteDLuecken <notifications@github.com>. wrote:. > I'm not sure I entirely understand what the weights are based on. I'm. > trying to understand when you would suggest someone use your approach. Why. > do you give one cell a weight of 125? With this type of weight distribution. > you are basically manually changing the marker gene calculation focusing. > nearly only on a single cell. That seems strange to me. >. > I'm trying to understand the need for scanpy to support weighted. > observations. At the moment I don't see when you would want to differently. > weight the observations... I'm familiar with using weights if I have some. > form of measurement error or uncertainty between samples. I don't really. > see how that holds here. Do you weight the cells based on some kind of. > quality score? >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOC4EI2YTU53XEGMJI3PWL4XZA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVZ3LJA#issuecomment-494122404>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOFRJXHAWVT6W4YKY63PWL4XZANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:469,interoperability,distribut,distribution,469,"I am using a sampling technique, which samples few rows without descreasing. performance. So speed is more than 10X time faster for larger dataset with. similar accuracy. On Tue, May 21, 2019 at 3:37 AM MalteDLuecken <notifications@github.com>. wrote:. > I'm not sure I entirely understand what the weights are based on. I'm. > trying to understand when you would suggest someone use your approach. Why. > do you give one cell a weight of 125? With this type of weight distribution. > you are basically manually changing the marker gene calculation focusing. > nearly only on a single cell. That seems strange to me. >. > I'm trying to understand the need for scanpy to support weighted. > observations. At the moment I don't see when you would want to differently. > weight the observations... I'm familiar with using weights if I have some. > form of measurement error or uncertainty between samples. I don't really. > see how that holds here. Do you weight the cells based on some kind of. > quality score? >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOC4EI2YTU53XEGMJI3PWL4XZA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVZ3LJA#issuecomment-494122404>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOFRJXHAWVT6W4YKY63PWL4XZANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:77,performance,perform,performance,77,"I am using a sampling technique, which samples few rows without descreasing. performance. So speed is more than 10X time faster for larger dataset with. similar accuracy. On Tue, May 21, 2019 at 3:37 AM MalteDLuecken <notifications@github.com>. wrote:. > I'm not sure I entirely understand what the weights are based on. I'm. > trying to understand when you would suggest someone use your approach. Why. > do you give one cell a weight of 125? With this type of weight distribution. > you are basically manually changing the marker gene calculation focusing. > nearly only on a single cell. That seems strange to me. >. > I'm trying to understand the need for scanpy to support weighted. > observations. At the moment I don't see when you would want to differently. > weight the observations... I'm familiar with using weights if I have some. > form of measurement error or uncertainty between samples. I don't really. > see how that holds here. Do you weight the cells based on some kind of. > quality score? >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOC4EI2YTU53XEGMJI3PWL4XZA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVZ3LJA#issuecomment-494122404>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOFRJXHAWVT6W4YKY63PWL4XZANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:116,performance,time,time,116,"I am using a sampling technique, which samples few rows without descreasing. performance. So speed is more than 10X time faster for larger dataset with. similar accuracy. On Tue, May 21, 2019 at 3:37 AM MalteDLuecken <notifications@github.com>. wrote:. > I'm not sure I entirely understand what the weights are based on. I'm. > trying to understand when you would suggest someone use your approach. Why. > do you give one cell a weight of 125? With this type of weight distribution. > you are basically manually changing the marker gene calculation focusing. > nearly only on a single cell. That seems strange to me. >. > I'm trying to understand the need for scanpy to support weighted. > observations. At the moment I don't see when you would want to differently. > weight the observations... I'm familiar with using weights if I have some. > form of measurement error or uncertainty between samples. I don't really. > see how that holds here. Do you weight the cells based on some kind of. > quality score? >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOC4EI2YTU53XEGMJI3PWL4XZA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVZ3LJA#issuecomment-494122404>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOFRJXHAWVT6W4YKY63PWL4XZANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:865,performance,error,error,865,"I am using a sampling technique, which samples few rows without descreasing. performance. So speed is more than 10X time faster for larger dataset with. similar accuracy. On Tue, May 21, 2019 at 3:37 AM MalteDLuecken <notifications@github.com>. wrote:. > I'm not sure I entirely understand what the weights are based on. I'm. > trying to understand when you would suggest someone use your approach. Why. > do you give one cell a weight of 125? With this type of weight distribution. > you are basically manually changing the marker gene calculation focusing. > nearly only on a single cell. That seems strange to me. >. > I'm trying to understand the need for scanpy to support weighted. > observations. At the moment I don't see when you would want to differently. > weight the observations... I'm familiar with using weights if I have some. > form of measurement error or uncertainty between samples. I don't really. > see how that holds here. Do you weight the cells based on some kind of. > quality score? >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOC4EI2YTU53XEGMJI3PWL4XZA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVZ3LJA#issuecomment-494122404>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOFRJXHAWVT6W4YKY63PWL4XZANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:865,safety,error,error,865,"I am using a sampling technique, which samples few rows without descreasing. performance. So speed is more than 10X time faster for larger dataset with. similar accuracy. On Tue, May 21, 2019 at 3:37 AM MalteDLuecken <notifications@github.com>. wrote:. > I'm not sure I entirely understand what the weights are based on. I'm. > trying to understand when you would suggest someone use your approach. Why. > do you give one cell a weight of 125? With this type of weight distribution. > you are basically manually changing the marker gene calculation focusing. > nearly only on a single cell. That seems strange to me. >. > I'm trying to understand the need for scanpy to support weighted. > observations. At the moment I don't see when you would want to differently. > weight the observations... I'm familiar with using weights if I have some. > form of measurement error or uncertainty between samples. I don't really. > see how that holds here. Do you weight the cells based on some kind of. > quality score? >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOC4EI2YTU53XEGMJI3PWL4XZA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVZ3LJA#issuecomment-494122404>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOFRJXHAWVT6W4YKY63PWL4XZANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1416,security,auth,auth,1416,"I am using a sampling technique, which samples few rows without descreasing. performance. So speed is more than 10X time faster for larger dataset with. similar accuracy. On Tue, May 21, 2019 at 3:37 AM MalteDLuecken <notifications@github.com>. wrote:. > I'm not sure I entirely understand what the weights are based on. I'm. > trying to understand when you would suggest someone use your approach. Why. > do you give one cell a weight of 125? With this type of weight distribution. > you are basically manually changing the marker gene calculation focusing. > nearly only on a single cell. That seems strange to me. >. > I'm trying to understand the need for scanpy to support weighted. > observations. At the moment I don't see when you would want to differently. > weight the observations... I'm familiar with using weights if I have some. > form of measurement error or uncertainty between samples. I don't really. > see how that holds here. Do you weight the cells based on some kind of. > quality score? >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOC4EI2YTU53XEGMJI3PWL4XZA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVZ3LJA#issuecomment-494122404>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOFRJXHAWVT6W4YKY63PWL4XZANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:279,testability,understand,understand,279,"I am using a sampling technique, which samples few rows without descreasing. performance. So speed is more than 10X time faster for larger dataset with. similar accuracy. On Tue, May 21, 2019 at 3:37 AM MalteDLuecken <notifications@github.com>. wrote:. > I'm not sure I entirely understand what the weights are based on. I'm. > trying to understand when you would suggest someone use your approach. Why. > do you give one cell a weight of 125? With this type of weight distribution. > you are basically manually changing the marker gene calculation focusing. > nearly only on a single cell. That seems strange to me. >. > I'm trying to understand the need for scanpy to support weighted. > observations. At the moment I don't see when you would want to differently. > weight the observations... I'm familiar with using weights if I have some. > form of measurement error or uncertainty between samples. I don't really. > see how that holds here. Do you weight the cells based on some kind of. > quality score? >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOC4EI2YTU53XEGMJI3PWL4XZA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVZ3LJA#issuecomment-494122404>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOFRJXHAWVT6W4YKY63PWL4XZANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:338,testability,understand,understand,338,"I am using a sampling technique, which samples few rows without descreasing. performance. So speed is more than 10X time faster for larger dataset with. similar accuracy. On Tue, May 21, 2019 at 3:37 AM MalteDLuecken <notifications@github.com>. wrote:. > I'm not sure I entirely understand what the weights are based on. I'm. > trying to understand when you would suggest someone use your approach. Why. > do you give one cell a weight of 125? With this type of weight distribution. > you are basically manually changing the marker gene calculation focusing. > nearly only on a single cell. That seems strange to me. >. > I'm trying to understand the need for scanpy to support weighted. > observations. At the moment I don't see when you would want to differently. > weight the observations... I'm familiar with using weights if I have some. > form of measurement error or uncertainty between samples. I don't really. > see how that holds here. Do you weight the cells based on some kind of. > quality score? >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOC4EI2YTU53XEGMJI3PWL4XZA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVZ3LJA#issuecomment-494122404>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOFRJXHAWVT6W4YKY63PWL4XZANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:636,testability,understand,understand,636,"I am using a sampling technique, which samples few rows without descreasing. performance. So speed is more than 10X time faster for larger dataset with. similar accuracy. On Tue, May 21, 2019 at 3:37 AM MalteDLuecken <notifications@github.com>. wrote:. > I'm not sure I entirely understand what the weights are based on. I'm. > trying to understand when you would suggest someone use your approach. Why. > do you give one cell a weight of 125? With this type of weight distribution. > you are basically manually changing the marker gene calculation focusing. > nearly only on a single cell. That seems strange to me. >. > I'm trying to understand the need for scanpy to support weighted. > observations. At the moment I don't see when you would want to differently. > weight the observations... I'm familiar with using weights if I have some. > form of measurement error or uncertainty between samples. I don't really. > see how that holds here. Do you weight the cells based on some kind of. > quality score? >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOC4EI2YTU53XEGMJI3PWL4XZA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVZ3LJA#issuecomment-494122404>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOFRJXHAWVT6W4YKY63PWL4XZANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:690,testability,observ,observations,690,"I am using a sampling technique, which samples few rows without descreasing. performance. So speed is more than 10X time faster for larger dataset with. similar accuracy. On Tue, May 21, 2019 at 3:37 AM MalteDLuecken <notifications@github.com>. wrote:. > I'm not sure I entirely understand what the weights are based on. I'm. > trying to understand when you would suggest someone use your approach. Why. > do you give one cell a weight of 125? With this type of weight distribution. > you are basically manually changing the marker gene calculation focusing. > nearly only on a single cell. That seems strange to me. >. > I'm trying to understand the need for scanpy to support weighted. > observations. At the moment I don't see when you would want to differently. > weight the observations... I'm familiar with using weights if I have some. > form of measurement error or uncertainty between samples. I don't really. > see how that holds here. Do you weight the cells based on some kind of. > quality score? >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOC4EI2YTU53XEGMJI3PWL4XZA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVZ3LJA#issuecomment-494122404>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOFRJXHAWVT6W4YKY63PWL4XZANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:779,testability,observ,observations,779,"I am using a sampling technique, which samples few rows without descreasing. performance. So speed is more than 10X time faster for larger dataset with. similar accuracy. On Tue, May 21, 2019 at 3:37 AM MalteDLuecken <notifications@github.com>. wrote:. > I'm not sure I entirely understand what the weights are based on. I'm. > trying to understand when you would suggest someone use your approach. Why. > do you give one cell a weight of 125? With this type of weight distribution. > you are basically manually changing the marker gene calculation focusing. > nearly only on a single cell. That seems strange to me. >. > I'm trying to understand the need for scanpy to support weighted. > observations. At the moment I don't see when you would want to differently. > weight the observations... I'm familiar with using weights if I have some. > form of measurement error or uncertainty between samples. I don't really. > see how that holds here. Do you weight the cells based on some kind of. > quality score? >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOC4EI2YTU53XEGMJI3PWL4XZA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVZ3LJA#issuecomment-494122404>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOFRJXHAWVT6W4YKY63PWL4XZANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:77,usability,perform,performance,77,"I am using a sampling technique, which samples few rows without descreasing. performance. So speed is more than 10X time faster for larger dataset with. similar accuracy. On Tue, May 21, 2019 at 3:37 AM MalteDLuecken <notifications@github.com>. wrote:. > I'm not sure I entirely understand what the weights are based on. I'm. > trying to understand when you would suggest someone use your approach. Why. > do you give one cell a weight of 125? With this type of weight distribution. > you are basically manually changing the marker gene calculation focusing. > nearly only on a single cell. That seems strange to me. >. > I'm trying to understand the need for scanpy to support weighted. > observations. At the moment I don't see when you would want to differently. > weight the observations... I'm familiar with using weights if I have some. > form of measurement error or uncertainty between samples. I don't really. > see how that holds here. Do you weight the cells based on some kind of. > quality score? >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOC4EI2YTU53XEGMJI3PWL4XZA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVZ3LJA#issuecomment-494122404>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOFRJXHAWVT6W4YKY63PWL4XZANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:670,usability,support,support,670,"I am using a sampling technique, which samples few rows without descreasing. performance. So speed is more than 10X time faster for larger dataset with. similar accuracy. On Tue, May 21, 2019 at 3:37 AM MalteDLuecken <notifications@github.com>. wrote:. > I'm not sure I entirely understand what the weights are based on. I'm. > trying to understand when you would suggest someone use your approach. Why. > do you give one cell a weight of 125? With this type of weight distribution. > you are basically manually changing the marker gene calculation focusing. > nearly only on a single cell. That seems strange to me. >. > I'm trying to understand the need for scanpy to support weighted. > observations. At the moment I don't see when you would want to differently. > weight the observations... I'm familiar with using weights if I have some. > form of measurement error or uncertainty between samples. I don't really. > see how that holds here. Do you weight the cells based on some kind of. > quality score? >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOC4EI2YTU53XEGMJI3PWL4XZA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVZ3LJA#issuecomment-494122404>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOFRJXHAWVT6W4YKY63PWL4XZANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:865,usability,error,error,865,"I am using a sampling technique, which samples few rows without descreasing. performance. So speed is more than 10X time faster for larger dataset with. similar accuracy. On Tue, May 21, 2019 at 3:37 AM MalteDLuecken <notifications@github.com>. wrote:. > I'm not sure I entirely understand what the weights are based on. I'm. > trying to understand when you would suggest someone use your approach. Why. > do you give one cell a weight of 125? With this type of weight distribution. > you are basically manually changing the marker gene calculation focusing. > nearly only on a single cell. That seems strange to me. >. > I'm trying to understand the need for scanpy to support weighted. > observations. At the moment I don't see when you would want to differently. > weight the observations... I'm familiar with using weights if I have some. > form of measurement error or uncertainty between samples. I don't really. > see how that holds here. Do you weight the cells based on some kind of. > quality score? >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOC4EI2YTU53XEGMJI3PWL4XZA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVZ3LJA#issuecomment-494122404>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOFRJXHAWVT6W4YKY63PWL4XZANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:150,performance,perform,perform,150,"I understand the benefits of sampling regarding computational speed up. What I'm not clear on is how you choose your weights for the calculations you perform here. You mentioned that you get wrong marker gene results when you sample and don't use weights. That makes sense if you get a non-representative set of cells in your sample. I wonder how you select the weights to fix this. I guess you don't just try a lot of different values until one works, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:2,testability,understand,understand,2,"I understand the benefits of sampling regarding computational speed up. What I'm not clear on is how you choose your weights for the calculations you perform here. You mentioned that you get wrong marker gene results when you sample and don't use weights. That makes sense if you get a non-representative set of cells in your sample. I wonder how you select the weights to fix this. I guess you don't just try a lot of different values until one works, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:85,usability,clear,clear,85,"I understand the benefits of sampling regarding computational speed up. What I'm not clear on is how you choose your weights for the calculations you perform here. You mentioned that you get wrong marker gene results when you sample and don't use weights. That makes sense if you get a non-representative set of cells in your sample. I wonder how you select the weights to fix this. I guess you don't just try a lot of different values until one works, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:150,usability,perform,perform,150,"I understand the benefits of sampling regarding computational speed up. What I'm not clear on is how you choose your weights for the calculations you perform here. You mentioned that you get wrong marker gene results when you sample and don't use weights. That makes sense if you get a non-representative set of cells in your sample. I wonder how you select the weights to fix this. I guess you don't just try a lot of different values until one works, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:55,energy efficiency,core,coreset,55,"Yes , the sampling is done with weights and I used the coreset technique. for it. On Tue, May 21, 2019 at 5:29 PM MalteDLuecken <notifications@github.com>. wrote:. > I understand the benefits of sampling regarding computational speed up. > What I'm not clear on is how you choose your weights for the calculations. > you perform here. You mentioned that you get wrong marker gene results when. > you sample and don't use weights. That makes sense if you get a. > non-representative set of cells in your sample. I wonder how you select the. > weights to fix this. I guess you don't just try a lot of different values. > until one works, right? >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGODYC4N7U5Y3T5XAEG3PWO6HTA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV3KJSY#issuecomment-494314699>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGODAWNXYF2AZPHG25P3PWO6HTANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:321,performance,perform,perform,321,"Yes , the sampling is done with weights and I used the coreset technique. for it. On Tue, May 21, 2019 at 5:29 PM MalteDLuecken <notifications@github.com>. wrote:. > I understand the benefits of sampling regarding computational speed up. > What I'm not clear on is how you choose your weights for the calculations. > you perform here. You mentioned that you get wrong marker gene results when. > you sample and don't use weights. That makes sense if you get a. > non-representative set of cells in your sample. I wonder how you select the. > weights to fix this. I guess you don't just try a lot of different values. > until one works, right? >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGODYC4N7U5Y3T5XAEG3PWO6HTA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV3KJSY#issuecomment-494314699>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGODAWNXYF2AZPHG25P3PWO6HTANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1049,security,auth,auth,1049,"Yes , the sampling is done with weights and I used the coreset technique. for it. On Tue, May 21, 2019 at 5:29 PM MalteDLuecken <notifications@github.com>. wrote:. > I understand the benefits of sampling regarding computational speed up. > What I'm not clear on is how you choose your weights for the calculations. > you perform here. You mentioned that you get wrong marker gene results when. > you sample and don't use weights. That makes sense if you get a. > non-representative set of cells in your sample. I wonder how you select the. > weights to fix this. I guess you don't just try a lot of different values. > until one works, right? >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGODYC4N7U5Y3T5XAEG3PWO6HTA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV3KJSY#issuecomment-494314699>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGODAWNXYF2AZPHG25P3PWO6HTANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:168,testability,understand,understand,168,"Yes , the sampling is done with weights and I used the coreset technique. for it. On Tue, May 21, 2019 at 5:29 PM MalteDLuecken <notifications@github.com>. wrote:. > I understand the benefits of sampling regarding computational speed up. > What I'm not clear on is how you choose your weights for the calculations. > you perform here. You mentioned that you get wrong marker gene results when. > you sample and don't use weights. That makes sense if you get a. > non-representative set of cells in your sample. I wonder how you select the. > weights to fix this. I guess you don't just try a lot of different values. > until one works, right? >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGODYC4N7U5Y3T5XAEG3PWO6HTA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV3KJSY#issuecomment-494314699>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGODAWNXYF2AZPHG25P3PWO6HTANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:253,usability,clear,clear,253,"Yes , the sampling is done with weights and I used the coreset technique. for it. On Tue, May 21, 2019 at 5:29 PM MalteDLuecken <notifications@github.com>. wrote:. > I understand the benefits of sampling regarding computational speed up. > What I'm not clear on is how you choose your weights for the calculations. > you perform here. You mentioned that you get wrong marker gene results when. > you sample and don't use weights. That makes sense if you get a. > non-representative set of cells in your sample. I wonder how you select the. > weights to fix this. I guess you don't just try a lot of different values. > until one works, right? >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGODYC4N7U5Y3T5XAEG3PWO6HTA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV3KJSY#issuecomment-494314699>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGODAWNXYF2AZPHG25P3PWO6HTANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:321,usability,perform,perform,321,"Yes , the sampling is done with weights and I used the coreset technique. for it. On Tue, May 21, 2019 at 5:29 PM MalteDLuecken <notifications@github.com>. wrote:. > I understand the benefits of sampling regarding computational speed up. > What I'm not clear on is how you choose your weights for the calculations. > you perform here. You mentioned that you get wrong marker gene results when. > you sample and don't use weights. That makes sense if you get a. > non-representative set of cells in your sample. I wonder how you select the. > weights to fix this. I guess you don't just try a lot of different values. > until one works, right? >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGODYC4N7U5Y3T5XAEG3PWO6HTA5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV3KJSY#issuecomment-494314699>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGODAWNXYF2AZPHG25P3PWO6HTANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:149,performance,perform,perform,149,"Ah, okay... so you sample based on how representative a cell is of its neighbours, and then you use that weight to calculated PCA, marker genes, and perform visualizations. Is that correct?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:149,usability,perform,perform,149,"Ah, okay... so you sample based on how representative a cell is of its neighbours, and then you use that weight to calculated PCA, marker genes, and perform visualizations. Is that correct?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:157,usability,visual,visualizations,157,"Ah, okay... so you sample based on how representative a cell is of its neighbours, and then you use that weight to calculated PCA, marker genes, and perform visualizations. Is that correct?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:213,availability,cluster,cluster-size-related,213,"> This is how I understood it anyway. You circumvent the problems caused by random sampling by doing a biased sampling, and the weights correspond to the size of the represented community. If the weights are just cluster-size-related, then this would not be necessary as you are calculating cluster-specific values to compare them in e.g., dot plots and statistical tests. This is a weighting that affects the relative importance of cells in the same cluster.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:291,availability,cluster,cluster-specific,291,"> This is how I understood it anyway. You circumvent the problems caused by random sampling by doing a biased sampling, and the weights correspond to the size of the represented community. If the weights are just cluster-size-related, then this would not be necessary as you are calculating cluster-specific values to compare them in e.g., dot plots and statistical tests. This is a weighting that affects the relative importance of cells in the same cluster.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:451,availability,cluster,cluster,451,"> This is how I understood it anyway. You circumvent the problems caused by random sampling by doing a biased sampling, and the weights correspond to the size of the represented community. If the weights are just cluster-size-related, then this would not be necessary as you are calculating cluster-specific values to compare them in e.g., dot plots and statistical tests. This is a weighting that affects the relative importance of cells in the same cluster.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:213,deployability,cluster,cluster-size-related,213,"> This is how I understood it anyway. You circumvent the problems caused by random sampling by doing a biased sampling, and the weights correspond to the size of the represented community. If the weights are just cluster-size-related, then this would not be necessary as you are calculating cluster-specific values to compare them in e.g., dot plots and statistical tests. This is a weighting that affects the relative importance of cells in the same cluster.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:291,deployability,cluster,cluster-specific,291,"> This is how I understood it anyway. You circumvent the problems caused by random sampling by doing a biased sampling, and the weights correspond to the size of the represented community. If the weights are just cluster-size-related, then this would not be necessary as you are calculating cluster-specific values to compare them in e.g., dot plots and statistical tests. This is a weighting that affects the relative importance of cells in the same cluster.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:451,deployability,cluster,cluster,451,"> This is how I understood it anyway. You circumvent the problems caused by random sampling by doing a biased sampling, and the weights correspond to the size of the represented community. If the weights are just cluster-size-related, then this would not be necessary as you are calculating cluster-specific values to compare them in e.g., dot plots and statistical tests. This is a weighting that affects the relative importance of cells in the same cluster.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:299,interoperability,specif,specific,299,"> This is how I understood it anyway. You circumvent the problems caused by random sampling by doing a biased sampling, and the weights correspond to the size of the represented community. If the weights are just cluster-size-related, then this would not be necessary as you are calculating cluster-specific values to compare them in e.g., dot plots and statistical tests. This is a weighting that affects the relative importance of cells in the same cluster.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:366,safety,test,tests,366,"> This is how I understood it anyway. You circumvent the problems caused by random sampling by doing a biased sampling, and the weights correspond to the size of the represented community. If the weights are just cluster-size-related, then this would not be necessary as you are calculating cluster-specific values to compare them in e.g., dot plots and statistical tests. This is a weighting that affects the relative importance of cells in the same cluster.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:366,testability,test,tests,366,"> This is how I understood it anyway. You circumvent the problems caused by random sampling by doing a biased sampling, and the weights correspond to the size of the represented community. If the weights are just cluster-size-related, then this would not be necessary as you are calculating cluster-specific values to compare them in e.g., dot plots and statistical tests. This is a weighting that affects the relative importance of cells in the same cluster.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:4,availability,cluster,clusters,4,"Not clusters, I meant that cells are selected to represent a group of cells, and the weights are to specify how big that group is. E.g. if you have 5 cells that are very similar, you pick one and give it a weight that says â€œI represent 5 cells in the original dataâ€. Right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:4,deployability,cluster,clusters,4,"Not clusters, I meant that cells are selected to represent a group of cells, and the weights are to specify how big that group is. E.g. if you have 5 cells that are very similar, you pick one and give it a weight that says â€œI represent 5 cells in the original dataâ€. Right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:100,interoperability,specif,specify,100,"Not clusters, I meant that cells are selected to represent a group of cells, and the weights are to specify how big that group is. E.g. if you have 5 cells that are very similar, you pick one and give it a weight that says â€œI represent 5 cells in the original dataâ€. Right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:146,availability,cluster,clusters,146,"Exactly, It's similar as Philipp explained. Regards,. Khalid. On Tue, May 21, 2019 at 8:31 PM Philipp A. <notifications@github.com> wrote:. > Not clusters, I meant that cells are selected to represent a group of. > cells, and the weights are to specify how big that group is. E.g. if you. > have 5 cells that are very similar, you pick one and give it a weight that. > says â€œI represent 5 cells in the original dataâ€. Right? >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOBMAPGORTCAWUSDUZLPWPTR7A5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV3YBOA#issuecomment-494371000>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOBSYOJCNUB72M5ELD3PWPTR7ANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:146,deployability,cluster,clusters,146,"Exactly, It's similar as Philipp explained. Regards,. Khalid. On Tue, May 21, 2019 at 8:31 PM Philipp A. <notifications@github.com> wrote:. > Not clusters, I meant that cells are selected to represent a group of. > cells, and the weights are to specify how big that group is. E.g. if you. > have 5 cells that are very similar, you pick one and give it a weight that. > says â€œI represent 5 cells in the original dataâ€. Right? >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOBMAPGORTCAWUSDUZLPWPTR7A5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV3YBOA#issuecomment-494371000>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOBSYOJCNUB72M5ELD3PWPTR7ANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:245,interoperability,specif,specify,245,"Exactly, It's similar as Philipp explained. Regards,. Khalid. On Tue, May 21, 2019 at 8:31 PM Philipp A. <notifications@github.com> wrote:. > Not clusters, I meant that cells are selected to represent a group of. > cells, and the weights are to specify how big that group is. E.g. if you. > have 5 cells that are very similar, you pick one and give it a weight that. > says â€œI represent 5 cells in the original dataâ€. Right? >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOBMAPGORTCAWUSDUZLPWPTR7A5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV3YBOA#issuecomment-494371000>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOBSYOJCNUB72M5ELD3PWPTR7ANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:831,security,auth,auth,831,"Exactly, It's similar as Philipp explained. Regards,. Khalid. On Tue, May 21, 2019 at 8:31 PM Philipp A. <notifications@github.com> wrote:. > Not clusters, I meant that cells are selected to represent a group of. > cells, and the weights are to specify how big that group is. E.g. if you. > have 5 cells that are very similar, you pick one and give it a weight that. > says â€œI represent 5 cells in the original dataâ€. Right? >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOBMAPGORTCAWUSDUZLPWPTR7A5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV3YBOA#issuecomment-494371000>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGOBSYOJCNUB72M5ELD3PWPTR7ANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:332,availability,cluster,clusters,332,"So, what's next ? :). I think no more issues in code , but little duplications. Thanks. On Tue, May 21, 2019 at 10:06 PM khalid usman <khalid0491@gmail.com> wrote:. > Exactly, It's similar as Philipp explained. >. > Regards,. > Khalid. >. > On Tue, May 21, 2019 at 8:31 PM Philipp A. <notifications@github.com>. > wrote:. >. >> Not clusters, I meant that cells are selected to represent a group of. >> cells, and the weights are to specify how big that group is. E.g. if you. >> have 5 cells that are very similar, you pick one and give it a weight that. >> says â€œI represent 5 cells in the original dataâ€. Right? >>. >> â€”. >> You are receiving this because you were mentioned. >> Reply to this email directly, view it on GitHub. >> <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOBMAPGORTCAWUSDUZLPWPTR7A5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV3YBOA#issuecomment-494371000>,. >> or mute the thread. >> <https://github.com/notifications/unsubscribe-auth/ABREGOBSYOJCNUB72M5ELD3PWPTR7ANCNFSM4HMZ5G7Q>. >> . >>. >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:332,deployability,cluster,clusters,332,"So, what's next ? :). I think no more issues in code , but little duplications. Thanks. On Tue, May 21, 2019 at 10:06 PM khalid usman <khalid0491@gmail.com> wrote:. > Exactly, It's similar as Philipp explained. >. > Regards,. > Khalid. >. > On Tue, May 21, 2019 at 8:31 PM Philipp A. <notifications@github.com>. > wrote:. >. >> Not clusters, I meant that cells are selected to represent a group of. >> cells, and the weights are to specify how big that group is. E.g. if you. >> have 5 cells that are very similar, you pick one and give it a weight that. >> says â€œI represent 5 cells in the original dataâ€. Right? >>. >> â€”. >> You are receiving this because you were mentioned. >> Reply to this email directly, view it on GitHub. >> <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOBMAPGORTCAWUSDUZLPWPTR7A5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV3YBOA#issuecomment-494371000>,. >> or mute the thread. >> <https://github.com/notifications/unsubscribe-auth/ABREGOBSYOJCNUB72M5ELD3PWPTR7ANCNFSM4HMZ5G7Q>. >> . >>. >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:432,interoperability,specif,specify,432,"So, what's next ? :). I think no more issues in code , but little duplications. Thanks. On Tue, May 21, 2019 at 10:06 PM khalid usman <khalid0491@gmail.com> wrote:. > Exactly, It's similar as Philipp explained. >. > Regards,. > Khalid. >. > On Tue, May 21, 2019 at 8:31 PM Philipp A. <notifications@github.com>. > wrote:. >. >> Not clusters, I meant that cells are selected to represent a group of. >> cells, and the weights are to specify how big that group is. E.g. if you. >> have 5 cells that are very similar, you pick one and give it a weight that. >> says â€œI represent 5 cells in the original dataâ€. Right? >>. >> â€”. >> You are receiving this because you were mentioned. >> Reply to this email directly, view it on GitHub. >> <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOBMAPGORTCAWUSDUZLPWPTR7A5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV3YBOA#issuecomment-494371000>,. >> or mute the thread. >> <https://github.com/notifications/unsubscribe-auth/ABREGOBSYOJCNUB72M5ELD3PWPTR7ANCNFSM4HMZ5G7Q>. >> . >>. >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1027,security,auth,auth,1027,"So, what's next ? :). I think no more issues in code , but little duplications. Thanks. On Tue, May 21, 2019 at 10:06 PM khalid usman <khalid0491@gmail.com> wrote:. > Exactly, It's similar as Philipp explained. >. > Regards,. > Khalid. >. > On Tue, May 21, 2019 at 8:31 PM Philipp A. <notifications@github.com>. > wrote:. >. >> Not clusters, I meant that cells are selected to represent a group of. >> cells, and the weights are to specify how big that group is. E.g. if you. >> have 5 cells that are very similar, you pick one and give it a weight that. >> says â€œI represent 5 cells in the original dataâ€. Right? >>. >> â€”. >> You are receiving this because you were mentioned. >> Reply to this email directly, view it on GitHub. >> <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOBMAPGORTCAWUSDUZLPWPTR7A5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV3YBOA#issuecomment-494371000>,. >> or mute the thread. >> <https://github.com/notifications/unsubscribe-auth/ABREGOBSYOJCNUB72M5ELD3PWPTR7ANCNFSM4HMZ5G7Q>. >> . >>. >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:421,deployability,API,API,421,"OK, this looks really good now! The two remaining issues are:. 1. https://github.com/theislab/scanpy/pull/644#discussion_r284651181. I think you should not do `df_weights = weights.copy(deep=True)`, but something like. ```py. df_weights = pd.DataFrame(dict(Wt=weights)). ```. You need to change your code so you call the functions with a `np.ndarray` or so instead of your 1-column dataframe, but I think itâ€™s the better API. 2. You should add the `weights` argument at the end of `rank_genes_groups`â€™ parameters, not the middle.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:421,integrability,API,API,421,"OK, this looks really good now! The two remaining issues are:. 1. https://github.com/theislab/scanpy/pull/644#discussion_r284651181. I think you should not do `df_weights = weights.copy(deep=True)`, but something like. ```py. df_weights = pd.DataFrame(dict(Wt=weights)). ```. You need to change your code so you call the functions with a `np.ndarray` or so instead of your 1-column dataframe, but I think itâ€™s the better API. 2. You should add the `weights` argument at the end of `rank_genes_groups`â€™ parameters, not the middle.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:421,interoperability,API,API,421,"OK, this looks really good now! The two remaining issues are:. 1. https://github.com/theislab/scanpy/pull/644#discussion_r284651181. I think you should not do `df_weights = weights.copy(deep=True)`, but something like. ```py. df_weights = pd.DataFrame(dict(Wt=weights)). ```. You need to change your code so you call the functions with a `np.ndarray` or so instead of your 1-column dataframe, but I think itâ€™s the better API. 2. You should add the `weights` argument at the end of `rank_genes_groups`â€™ parameters, not the middle.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:502,modifiability,paramet,parameters,502,"OK, this looks really good now! The two remaining issues are:. 1. https://github.com/theislab/scanpy/pull/644#discussion_r284651181. I think you should not do `df_weights = weights.copy(deep=True)`, but something like. ```py. df_weights = pd.DataFrame(dict(Wt=weights)). ```. You need to change your code so you call the functions with a `np.ndarray` or so instead of your 1-column dataframe, but I think itâ€™s the better API. 2. You should add the `weights` argument at the end of `rank_genes_groups`â€™ parameters, not the middle.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:498,deployability,automat,automatically,498,"Long-term, we should think about the design here: Passing weights in every function call is possible, but not very nice for users. So a few questions come to mind:. Should we add `scanpy.pp.coreset`, which would create a sampling and add `adata.obs['coreset_weights']` or simply `adata.obs['weights']`? If we do that or plan to in the future, how should the added `weights` parameter to all these functions work? I think it might default to `'coreset_weights'`/`'weights'`, and the functions would automatically use that `.obs` column if it exists. Users should also still be able to specify weights manually as in this PR. So the type of the parameter would be `Union[str, Sequence[Union[float, int]]]`. ---. All of that doesnâ€™t really affect this PR, as we can merge it as it is and include anndata-stored weights later.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:190,energy efficiency,core,coreset,190,"Long-term, we should think about the design here: Passing weights in every function call is possible, but not very nice for users. So a few questions come to mind:. Should we add `scanpy.pp.coreset`, which would create a sampling and add `adata.obs['coreset_weights']` or simply `adata.obs['weights']`? If we do that or plan to in the future, how should the added `weights` parameter to all these functions work? I think it might default to `'coreset_weights'`/`'weights'`, and the functions would automatically use that `.obs` column if it exists. Users should also still be able to specify weights manually as in this PR. So the type of the parameter would be `Union[str, Sequence[Union[float, int]]]`. ---. All of that doesnâ€™t really affect this PR, as we can merge it as it is and include anndata-stored weights later.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:584,interoperability,specif,specify,584,"Long-term, we should think about the design here: Passing weights in every function call is possible, but not very nice for users. So a few questions come to mind:. Should we add `scanpy.pp.coreset`, which would create a sampling and add `adata.obs['coreset_weights']` or simply `adata.obs['weights']`? If we do that or plan to in the future, how should the added `weights` parameter to all these functions work? I think it might default to `'coreset_weights'`/`'weights'`, and the functions would automatically use that `.obs` column if it exists. Users should also still be able to specify weights manually as in this PR. So the type of the parameter would be `Union[str, Sequence[Union[float, int]]]`. ---. All of that doesnâ€™t really affect this PR, as we can merge it as it is and include anndata-stored weights later.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:374,modifiability,paramet,parameter,374,"Long-term, we should think about the design here: Passing weights in every function call is possible, but not very nice for users. So a few questions come to mind:. Should we add `scanpy.pp.coreset`, which would create a sampling and add `adata.obs['coreset_weights']` or simply `adata.obs['weights']`? If we do that or plan to in the future, how should the added `weights` parameter to all these functions work? I think it might default to `'coreset_weights'`/`'weights'`, and the functions would automatically use that `.obs` column if it exists. Users should also still be able to specify weights manually as in this PR. So the type of the parameter would be `Union[str, Sequence[Union[float, int]]]`. ---. All of that doesnâ€™t really affect this PR, as we can merge it as it is and include anndata-stored weights later.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:643,modifiability,paramet,parameter,643,"Long-term, we should think about the design here: Passing weights in every function call is possible, but not very nice for users. So a few questions come to mind:. Should we add `scanpy.pp.coreset`, which would create a sampling and add `adata.obs['coreset_weights']` or simply `adata.obs['weights']`? If we do that or plan to in the future, how should the added `weights` parameter to all these functions work? I think it might default to `'coreset_weights'`/`'weights'`, and the functions would automatically use that `.obs` column if it exists. Users should also still be able to specify weights manually as in this PR. So the type of the parameter would be `Union[str, Sequence[Union[float, int]]]`. ---. All of that doesnâ€™t really affect this PR, as we can merge it as it is and include anndata-stored weights later.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:722,reliability,doe,doesn,722,"Long-term, we should think about the design here: Passing weights in every function call is possible, but not very nice for users. So a few questions come to mind:. Should we add `scanpy.pp.coreset`, which would create a sampling and add `adata.obs['coreset_weights']` or simply `adata.obs['weights']`? If we do that or plan to in the future, how should the added `weights` parameter to all these functions work? I think it might default to `'coreset_weights'`/`'weights'`, and the functions would automatically use that `.obs` column if it exists. Users should also still be able to specify weights manually as in this PR. So the type of the parameter would be `Union[str, Sequence[Union[float, int]]]`. ---. All of that doesnâ€™t really affect this PR, as we can merge it as it is and include anndata-stored weights later.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:272,testability,simpl,simply,272,"Long-term, we should think about the design here: Passing weights in every function call is possible, but not very nice for users. So a few questions come to mind:. Should we add `scanpy.pp.coreset`, which would create a sampling and add `adata.obs['coreset_weights']` or simply `adata.obs['weights']`? If we do that or plan to in the future, how should the added `weights` parameter to all these functions work? I think it might default to `'coreset_weights'`/`'weights'`, and the functions would automatically use that `.obs` column if it exists. Users should also still be able to specify weights manually as in this PR. So the type of the parameter would be `Union[str, Sequence[Union[float, int]]]`. ---. All of that doesnâ€™t really affect this PR, as we can merge it as it is and include anndata-stored weights later.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:320,testability,plan,plan,320,"Long-term, we should think about the design here: Passing weights in every function call is possible, but not very nice for users. So a few questions come to mind:. Should we add `scanpy.pp.coreset`, which would create a sampling and add `adata.obs['coreset_weights']` or simply `adata.obs['weights']`? If we do that or plan to in the future, how should the added `weights` parameter to all these functions work? I think it might default to `'coreset_weights'`/`'weights'`, and the functions would automatically use that `.obs` column if it exists. Users should also still be able to specify weights manually as in this PR. So the type of the parameter would be `Union[str, Sequence[Union[float, int]]]`. ---. All of that doesnâ€™t really affect this PR, as we can merge it as it is and include anndata-stored weights later.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:498,testability,automat,automatically,498,"Long-term, we should think about the design here: Passing weights in every function call is possible, but not very nice for users. So a few questions come to mind:. Should we add `scanpy.pp.coreset`, which would create a sampling and add `adata.obs['coreset_weights']` or simply `adata.obs['weights']`? If we do that or plan to in the future, how should the added `weights` parameter to all these functions work? I think it might default to `'coreset_weights'`/`'weights'`, and the functions would automatically use that `.obs` column if it exists. Users should also still be able to specify weights manually as in this PR. So the type of the parameter would be `Union[str, Sequence[Union[float, int]]]`. ---. All of that doesnâ€™t really affect this PR, as we can merge it as it is and include anndata-stored weights later.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:124,usability,user,users,124,"Long-term, we should think about the design here: Passing weights in every function call is possible, but not very nice for users. So a few questions come to mind:. Should we add `scanpy.pp.coreset`, which would create a sampling and add `adata.obs['coreset_weights']` or simply `adata.obs['weights']`? If we do that or plan to in the future, how should the added `weights` parameter to all these functions work? I think it might default to `'coreset_weights'`/`'weights'`, and the functions would automatically use that `.obs` column if it exists. Users should also still be able to specify weights manually as in this PR. So the type of the parameter would be `Union[str, Sequence[Union[float, int]]]`. ---. All of that doesnâ€™t really affect this PR, as we can merge it as it is and include anndata-stored weights later.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:272,usability,simpl,simply,272,"Long-term, we should think about the design here: Passing weights in every function call is possible, but not very nice for users. So a few questions come to mind:. Should we add `scanpy.pp.coreset`, which would create a sampling and add `adata.obs['coreset_weights']` or simply `adata.obs['weights']`? If we do that or plan to in the future, how should the added `weights` parameter to all these functions work? I think it might default to `'coreset_weights'`/`'weights'`, and the functions would automatically use that `.obs` column if it exists. Users should also still be able to specify weights manually as in this PR. So the type of the parameter would be `Union[str, Sequence[Union[float, int]]]`. ---. All of that doesnâ€™t really affect this PR, as we can merge it as it is and include anndata-stored weights later.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:549,usability,User,Users,549,"Long-term, we should think about the design here: Passing weights in every function call is possible, but not very nice for users. So a few questions come to mind:. Should we add `scanpy.pp.coreset`, which would create a sampling and add `adata.obs['coreset_weights']` or simply `adata.obs['weights']`? If we do that or plan to in the future, how should the added `weights` parameter to all these functions work? I think it might default to `'coreset_weights'`/`'weights'`, and the functions would automatically use that `.obs` column if it exists. Users should also still be able to specify weights manually as in this PR. So the type of the parameter would be `Union[str, Sequence[Union[float, int]]]`. ---. All of that doesnâ€™t really affect this PR, as we can merge it as it is and include anndata-stored weights later.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:194,deployability,observ,observations,194,"Thanks ,. But i will suggest to just support weights instead of coreset, may be user. want to sample data with some other weighting technique. So we should ask. them to just put the weights for observations, then we need to modify PCA. as well and i think my code will support most of plots and marker genes,. but not PCA, because my input is PCA matrix with weights for each. observations. Thanks,. Khalid. On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com> wrote:. > Long-term, we should think about the design here: Specifying weights all. > the time is possible, but not very nice for users. So a few questions come. > to mind:. >. > Should we add scanpy.pp.coreset, which would create a sampling and add. > adata.obs['coreset_weights'] or simply adata.obs['weights']? >. > If we do that or plan to in the future, how should the added weights. > parameter to all these functions work? >. > I think it might default to 'coreset_weights', and the functions would. > automatically use that .obs column if it exists. Users should also still. > be able to specify weights manually as in this PR. >. > So the type of the parameter would be Union[str, pd.DataFrame,. > Sequence[Union[float, int]]]. > ------------------------------. >. > All of that doesnâ€™t really affect this PR, as we can merge it as it is and. > include anndata-stored weights later. >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOC4K5CCAJSUVYSIAFDPWUEC5A5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV6M55Q#issuecomment-494718710>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGODWOGS6RD2JQ4LW5LDPWUEC5ANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:377,deployability,observ,observations,377,"Thanks ,. But i will suggest to just support weights instead of coreset, may be user. want to sample data with some other weighting technique. So we should ask. them to just put the weights for observations, then we need to modify PCA. as well and i think my code will support most of plots and marker genes,. but not PCA, because my input is PCA matrix with weights for each. observations. Thanks,. Khalid. On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com> wrote:. > Long-term, we should think about the design here: Specifying weights all. > the time is possible, but not very nice for users. So a few questions come. > to mind:. >. > Should we add scanpy.pp.coreset, which would create a sampling and add. > adata.obs['coreset_weights'] or simply adata.obs['weights']? >. > If we do that or plan to in the future, how should the added weights. > parameter to all these functions work? >. > I think it might default to 'coreset_weights', and the functions would. > automatically use that .obs column if it exists. Users should also still. > be able to specify weights manually as in this PR. >. > So the type of the parameter would be Union[str, pd.DataFrame,. > Sequence[Union[float, int]]]. > ------------------------------. >. > All of that doesnâ€™t really affect this PR, as we can merge it as it is and. > include anndata-stored weights later. >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOC4K5CCAJSUVYSIAFDPWUEC5A5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV6M55Q#issuecomment-494718710>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGODWOGS6RD2JQ4LW5LDPWUEC5ANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:987,deployability,automat,automatically,987,"Thanks ,. But i will suggest to just support weights instead of coreset, may be user. want to sample data with some other weighting technique. So we should ask. them to just put the weights for observations, then we need to modify PCA. as well and i think my code will support most of plots and marker genes,. but not PCA, because my input is PCA matrix with weights for each. observations. Thanks,. Khalid. On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com> wrote:. > Long-term, we should think about the design here: Specifying weights all. > the time is possible, but not very nice for users. So a few questions come. > to mind:. >. > Should we add scanpy.pp.coreset, which would create a sampling and add. > adata.obs['coreset_weights'] or simply adata.obs['weights']? >. > If we do that or plan to in the future, how should the added weights. > parameter to all these functions work? >. > I think it might default to 'coreset_weights', and the functions would. > automatically use that .obs column if it exists. Users should also still. > be able to specify weights manually as in this PR. >. > So the type of the parameter would be Union[str, pd.DataFrame,. > Sequence[Union[float, int]]]. > ------------------------------. >. > All of that doesnâ€™t really affect this PR, as we can merge it as it is and. > include anndata-stored weights later. >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOC4K5CCAJSUVYSIAFDPWUEC5A5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV6M55Q#issuecomment-494718710>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGODWOGS6RD2JQ4LW5LDPWUEC5ANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:64,energy efficiency,core,coreset,64,"Thanks ,. But i will suggest to just support weights instead of coreset, may be user. want to sample data with some other weighting technique. So we should ask. them to just put the weights for observations, then we need to modify PCA. as well and i think my code will support most of plots and marker genes,. but not PCA, because my input is PCA matrix with weights for each. observations. Thanks,. Khalid. On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com> wrote:. > Long-term, we should think about the design here: Specifying weights all. > the time is possible, but not very nice for users. So a few questions come. > to mind:. >. > Should we add scanpy.pp.coreset, which would create a sampling and add. > adata.obs['coreset_weights'] or simply adata.obs['weights']? >. > If we do that or plan to in the future, how should the added weights. > parameter to all these functions work? >. > I think it might default to 'coreset_weights', and the functions would. > automatically use that .obs column if it exists. Users should also still. > be able to specify weights manually as in this PR. >. > So the type of the parameter would be Union[str, pd.DataFrame,. > Sequence[Union[float, int]]]. > ------------------------------. >. > All of that doesnâ€™t really affect this PR, as we can merge it as it is and. > include anndata-stored weights later. >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOC4K5CCAJSUVYSIAFDPWUEC5A5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV6M55Q#issuecomment-494718710>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGODWOGS6RD2JQ4LW5LDPWUEC5ANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:681,energy efficiency,core,coreset,681,"Thanks ,. But i will suggest to just support weights instead of coreset, may be user. want to sample data with some other weighting technique. So we should ask. them to just put the weights for observations, then we need to modify PCA. as well and i think my code will support most of plots and marker genes,. but not PCA, because my input is PCA matrix with weights for each. observations. Thanks,. Khalid. On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com> wrote:. > Long-term, we should think about the design here: Specifying weights all. > the time is possible, but not very nice for users. So a few questions come. > to mind:. >. > Should we add scanpy.pp.coreset, which would create a sampling and add. > adata.obs['coreset_weights'] or simply adata.obs['weights']? >. > If we do that or plan to in the future, how should the added weights. > parameter to all these functions work? >. > I think it might default to 'coreset_weights', and the functions would. > automatically use that .obs column if it exists. Users should also still. > be able to specify weights manually as in this PR. >. > So the type of the parameter would be Union[str, pd.DataFrame,. > Sequence[Union[float, int]]]. > ------------------------------. >. > All of that doesnâ€™t really affect this PR, as we can merge it as it is and. > include anndata-stored weights later. >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOC4K5CCAJSUVYSIAFDPWUEC5A5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV6M55Q#issuecomment-494718710>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGODWOGS6RD2JQ4LW5LDPWUEC5ANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:538,interoperability,Specif,Specifying,538,"Thanks ,. But i will suggest to just support weights instead of coreset, may be user. want to sample data with some other weighting technique. So we should ask. them to just put the weights for observations, then we need to modify PCA. as well and i think my code will support most of plots and marker genes,. but not PCA, because my input is PCA matrix with weights for each. observations. Thanks,. Khalid. On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com> wrote:. > Long-term, we should think about the design here: Specifying weights all. > the time is possible, but not very nice for users. So a few questions come. > to mind:. >. > Should we add scanpy.pp.coreset, which would create a sampling and add. > adata.obs['coreset_weights'] or simply adata.obs['weights']? >. > If we do that or plan to in the future, how should the added weights. > parameter to all these functions work? >. > I think it might default to 'coreset_weights', and the functions would. > automatically use that .obs column if it exists. Users should also still. > be able to specify weights manually as in this PR. >. > So the type of the parameter would be Union[str, pd.DataFrame,. > Sequence[Union[float, int]]]. > ------------------------------. >. > All of that doesnâ€™t really affect this PR, as we can merge it as it is and. > include anndata-stored weights later. >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOC4K5CCAJSUVYSIAFDPWUEC5A5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV6M55Q#issuecomment-494718710>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGODWOGS6RD2JQ4LW5LDPWUEC5ANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1074,interoperability,specif,specify,1074,"Thanks ,. But i will suggest to just support weights instead of coreset, may be user. want to sample data with some other weighting technique. So we should ask. them to just put the weights for observations, then we need to modify PCA. as well and i think my code will support most of plots and marker genes,. but not PCA, because my input is PCA matrix with weights for each. observations. Thanks,. Khalid. On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com> wrote:. > Long-term, we should think about the design here: Specifying weights all. > the time is possible, but not very nice for users. So a few questions come. > to mind:. >. > Should we add scanpy.pp.coreset, which would create a sampling and add. > adata.obs['coreset_weights'] or simply adata.obs['weights']? >. > If we do that or plan to in the future, how should the added weights. > parameter to all these functions work? >. > I think it might default to 'coreset_weights', and the functions would. > automatically use that .obs column if it exists. Users should also still. > be able to specify weights manually as in this PR. >. > So the type of the parameter would be Union[str, pd.DataFrame,. > Sequence[Union[float, int]]]. > ------------------------------. >. > All of that doesnâ€™t really affect this PR, as we can merge it as it is and. > include anndata-stored weights later. >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOC4K5CCAJSUVYSIAFDPWUEC5A5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV6M55Q#issuecomment-494718710>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGODWOGS6RD2JQ4LW5LDPWUEC5ANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:869,modifiability,paramet,parameter,869,"Thanks ,. But i will suggest to just support weights instead of coreset, may be user. want to sample data with some other weighting technique. So we should ask. them to just put the weights for observations, then we need to modify PCA. as well and i think my code will support most of plots and marker genes,. but not PCA, because my input is PCA matrix with weights for each. observations. Thanks,. Khalid. On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com> wrote:. > Long-term, we should think about the design here: Specifying weights all. > the time is possible, but not very nice for users. So a few questions come. > to mind:. >. > Should we add scanpy.pp.coreset, which would create a sampling and add. > adata.obs['coreset_weights'] or simply adata.obs['weights']? >. > If we do that or plan to in the future, how should the added weights. > parameter to all these functions work? >. > I think it might default to 'coreset_weights', and the functions would. > automatically use that .obs column if it exists. Users should also still. > be able to specify weights manually as in this PR. >. > So the type of the parameter would be Union[str, pd.DataFrame,. > Sequence[Union[float, int]]]. > ------------------------------. >. > All of that doesnâ€™t really affect this PR, as we can merge it as it is and. > include anndata-stored weights later. >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOC4K5CCAJSUVYSIAFDPWUEC5A5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV6M55Q#issuecomment-494718710>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGODWOGS6RD2JQ4LW5LDPWUEC5ANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1138,modifiability,paramet,parameter,1138,"Thanks ,. But i will suggest to just support weights instead of coreset, may be user. want to sample data with some other weighting technique. So we should ask. them to just put the weights for observations, then we need to modify PCA. as well and i think my code will support most of plots and marker genes,. but not PCA, because my input is PCA matrix with weights for each. observations. Thanks,. Khalid. On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com> wrote:. > Long-term, we should think about the design here: Specifying weights all. > the time is possible, but not very nice for users. So a few questions come. > to mind:. >. > Should we add scanpy.pp.coreset, which would create a sampling and add. > adata.obs['coreset_weights'] or simply adata.obs['weights']? >. > If we do that or plan to in the future, how should the added weights. > parameter to all these functions work? >. > I think it might default to 'coreset_weights', and the functions would. > automatically use that .obs column if it exists. Users should also still. > be able to specify weights manually as in this PR. >. > So the type of the parameter would be Union[str, pd.DataFrame,. > Sequence[Union[float, int]]]. > ------------------------------. >. > All of that doesnâ€™t really affect this PR, as we can merge it as it is and. > include anndata-stored weights later. >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOC4K5CCAJSUVYSIAFDPWUEC5A5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV6M55Q#issuecomment-494718710>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGODWOGS6RD2JQ4LW5LDPWUEC5ANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:568,performance,time,time,568,"Thanks ,. But i will suggest to just support weights instead of coreset, may be user. want to sample data with some other weighting technique. So we should ask. them to just put the weights for observations, then we need to modify PCA. as well and i think my code will support most of plots and marker genes,. but not PCA, because my input is PCA matrix with weights for each. observations. Thanks,. Khalid. On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com> wrote:. > Long-term, we should think about the design here: Specifying weights all. > the time is possible, but not very nice for users. So a few questions come. > to mind:. >. > Should we add scanpy.pp.coreset, which would create a sampling and add. > adata.obs['coreset_weights'] or simply adata.obs['weights']? >. > If we do that or plan to in the future, how should the added weights. > parameter to all these functions work? >. > I think it might default to 'coreset_weights', and the functions would. > automatically use that .obs column if it exists. Users should also still. > be able to specify weights manually as in this PR. >. > So the type of the parameter would be Union[str, pd.DataFrame,. > Sequence[Union[float, int]]]. > ------------------------------. >. > All of that doesnâ€™t really affect this PR, as we can merge it as it is and. > include anndata-stored weights later. >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOC4K5CCAJSUVYSIAFDPWUEC5A5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV6M55Q#issuecomment-494718710>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGODWOGS6RD2JQ4LW5LDPWUEC5ANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1266,reliability,doe,doesn,1266,"Thanks ,. But i will suggest to just support weights instead of coreset, may be user. want to sample data with some other weighting technique. So we should ask. them to just put the weights for observations, then we need to modify PCA. as well and i think my code will support most of plots and marker genes,. but not PCA, because my input is PCA matrix with weights for each. observations. Thanks,. Khalid. On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com> wrote:. > Long-term, we should think about the design here: Specifying weights all. > the time is possible, but not very nice for users. So a few questions come. > to mind:. >. > Should we add scanpy.pp.coreset, which would create a sampling and add. > adata.obs['coreset_weights'] or simply adata.obs['weights']? >. > If we do that or plan to in the future, how should the added weights. > parameter to all these functions work? >. > I think it might default to 'coreset_weights', and the functions would. > automatically use that .obs column if it exists. Users should also still. > be able to specify weights manually as in this PR. >. > So the type of the parameter would be Union[str, pd.DataFrame,. > Sequence[Union[float, int]]]. > ------------------------------. >. > All of that doesnâ€™t really affect this PR, as we can merge it as it is and. > include anndata-stored weights later. >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOC4K5CCAJSUVYSIAFDPWUEC5A5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV6M55Q#issuecomment-494718710>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGODWOGS6RD2JQ4LW5LDPWUEC5ANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:334,safety,input,input,334,"Thanks ,. But i will suggest to just support weights instead of coreset, may be user. want to sample data with some other weighting technique. So we should ask. them to just put the weights for observations, then we need to modify PCA. as well and i think my code will support most of plots and marker genes,. but not PCA, because my input is PCA matrix with weights for each. observations. Thanks,. Khalid. On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com> wrote:. > Long-term, we should think about the design here: Specifying weights all. > the time is possible, but not very nice for users. So a few questions come. > to mind:. >. > Should we add scanpy.pp.coreset, which would create a sampling and add. > adata.obs['coreset_weights'] or simply adata.obs['weights']? >. > If we do that or plan to in the future, how should the added weights. > parameter to all these functions work? >. > I think it might default to 'coreset_weights', and the functions would. > automatically use that .obs column if it exists. Users should also still. > be able to specify weights manually as in this PR. >. > So the type of the parameter would be Union[str, pd.DataFrame,. > Sequence[Union[float, int]]]. > ------------------------------. >. > All of that doesnâ€™t really affect this PR, as we can merge it as it is and. > include anndata-stored weights later. >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOC4K5CCAJSUVYSIAFDPWUEC5A5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV6M55Q#issuecomment-494718710>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGODWOGS6RD2JQ4LW5LDPWUEC5ANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:224,security,modif,modify,224,"Thanks ,. But i will suggest to just support weights instead of coreset, may be user. want to sample data with some other weighting technique. So we should ask. them to just put the weights for observations, then we need to modify PCA. as well and i think my code will support most of plots and marker genes,. but not PCA, because my input is PCA matrix with weights for each. observations. Thanks,. Khalid. On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com> wrote:. > Long-term, we should think about the design here: Specifying weights all. > the time is possible, but not very nice for users. So a few questions come. > to mind:. >. > Should we add scanpy.pp.coreset, which would create a sampling and add. > adata.obs['coreset_weights'] or simply adata.obs['weights']? >. > If we do that or plan to in the future, how should the added weights. > parameter to all these functions work? >. > I think it might default to 'coreset_weights', and the functions would. > automatically use that .obs column if it exists. Users should also still. > be able to specify weights manually as in this PR. >. > So the type of the parameter would be Union[str, pd.DataFrame,. > Sequence[Union[float, int]]]. > ------------------------------. >. > All of that doesnâ€™t really affect this PR, as we can merge it as it is and. > include anndata-stored weights later. >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOC4K5CCAJSUVYSIAFDPWUEC5A5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV6M55Q#issuecomment-494718710>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGODWOGS6RD2JQ4LW5LDPWUEC5ANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1776,security,auth,auth,1776,"Thanks ,. But i will suggest to just support weights instead of coreset, may be user. want to sample data with some other weighting technique. So we should ask. them to just put the weights for observations, then we need to modify PCA. as well and i think my code will support most of plots and marker genes,. but not PCA, because my input is PCA matrix with weights for each. observations. Thanks,. Khalid. On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com> wrote:. > Long-term, we should think about the design here: Specifying weights all. > the time is possible, but not very nice for users. So a few questions come. > to mind:. >. > Should we add scanpy.pp.coreset, which would create a sampling and add. > adata.obs['coreset_weights'] or simply adata.obs['weights']? >. > If we do that or plan to in the future, how should the added weights. > parameter to all these functions work? >. > I think it might default to 'coreset_weights', and the functions would. > automatically use that .obs column if it exists. Users should also still. > be able to specify weights manually as in this PR. >. > So the type of the parameter would be Union[str, pd.DataFrame,. > Sequence[Union[float, int]]]. > ------------------------------. >. > All of that doesnâ€™t really affect this PR, as we can merge it as it is and. > include anndata-stored weights later. >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOC4K5CCAJSUVYSIAFDPWUEC5A5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV6M55Q#issuecomment-494718710>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGODWOGS6RD2JQ4LW5LDPWUEC5ANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:194,testability,observ,observations,194,"Thanks ,. But i will suggest to just support weights instead of coreset, may be user. want to sample data with some other weighting technique. So we should ask. them to just put the weights for observations, then we need to modify PCA. as well and i think my code will support most of plots and marker genes,. but not PCA, because my input is PCA matrix with weights for each. observations. Thanks,. Khalid. On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com> wrote:. > Long-term, we should think about the design here: Specifying weights all. > the time is possible, but not very nice for users. So a few questions come. > to mind:. >. > Should we add scanpy.pp.coreset, which would create a sampling and add. > adata.obs['coreset_weights'] or simply adata.obs['weights']? >. > If we do that or plan to in the future, how should the added weights. > parameter to all these functions work? >. > I think it might default to 'coreset_weights', and the functions would. > automatically use that .obs column if it exists. Users should also still. > be able to specify weights manually as in this PR. >. > So the type of the parameter would be Union[str, pd.DataFrame,. > Sequence[Union[float, int]]]. > ------------------------------. >. > All of that doesnâ€™t really affect this PR, as we can merge it as it is and. > include anndata-stored weights later. >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOC4K5CCAJSUVYSIAFDPWUEC5A5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV6M55Q#issuecomment-494718710>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGODWOGS6RD2JQ4LW5LDPWUEC5ANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:377,testability,observ,observations,377,"Thanks ,. But i will suggest to just support weights instead of coreset, may be user. want to sample data with some other weighting technique. So we should ask. them to just put the weights for observations, then we need to modify PCA. as well and i think my code will support most of plots and marker genes,. but not PCA, because my input is PCA matrix with weights for each. observations. Thanks,. Khalid. On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com> wrote:. > Long-term, we should think about the design here: Specifying weights all. > the time is possible, but not very nice for users. So a few questions come. > to mind:. >. > Should we add scanpy.pp.coreset, which would create a sampling and add. > adata.obs['coreset_weights'] or simply adata.obs['weights']? >. > If we do that or plan to in the future, how should the added weights. > parameter to all these functions work? >. > I think it might default to 'coreset_weights', and the functions would. > automatically use that .obs column if it exists. Users should also still. > be able to specify weights manually as in this PR. >. > So the type of the parameter would be Union[str, pd.DataFrame,. > Sequence[Union[float, int]]]. > ------------------------------. >. > All of that doesnâ€™t really affect this PR, as we can merge it as it is and. > include anndata-stored weights later. >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOC4K5CCAJSUVYSIAFDPWUEC5A5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV6M55Q#issuecomment-494718710>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGODWOGS6RD2JQ4LW5LDPWUEC5ANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:763,testability,simpl,simply,763,"Thanks ,. But i will suggest to just support weights instead of coreset, may be user. want to sample data with some other weighting technique. So we should ask. them to just put the weights for observations, then we need to modify PCA. as well and i think my code will support most of plots and marker genes,. but not PCA, because my input is PCA matrix with weights for each. observations. Thanks,. Khalid. On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com> wrote:. > Long-term, we should think about the design here: Specifying weights all. > the time is possible, but not very nice for users. So a few questions come. > to mind:. >. > Should we add scanpy.pp.coreset, which would create a sampling and add. > adata.obs['coreset_weights'] or simply adata.obs['weights']? >. > If we do that or plan to in the future, how should the added weights. > parameter to all these functions work? >. > I think it might default to 'coreset_weights', and the functions would. > automatically use that .obs column if it exists. Users should also still. > be able to specify weights manually as in this PR. >. > So the type of the parameter would be Union[str, pd.DataFrame,. > Sequence[Union[float, int]]]. > ------------------------------. >. > All of that doesnâ€™t really affect this PR, as we can merge it as it is and. > include anndata-stored weights later. >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOC4K5CCAJSUVYSIAFDPWUEC5A5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV6M55Q#issuecomment-494718710>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGODWOGS6RD2JQ4LW5LDPWUEC5ANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:814,testability,plan,plan,814,"Thanks ,. But i will suggest to just support weights instead of coreset, may be user. want to sample data with some other weighting technique. So we should ask. them to just put the weights for observations, then we need to modify PCA. as well and i think my code will support most of plots and marker genes,. but not PCA, because my input is PCA matrix with weights for each. observations. Thanks,. Khalid. On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com> wrote:. > Long-term, we should think about the design here: Specifying weights all. > the time is possible, but not very nice for users. So a few questions come. > to mind:. >. > Should we add scanpy.pp.coreset, which would create a sampling and add. > adata.obs['coreset_weights'] or simply adata.obs['weights']? >. > If we do that or plan to in the future, how should the added weights. > parameter to all these functions work? >. > I think it might default to 'coreset_weights', and the functions would. > automatically use that .obs column if it exists. Users should also still. > be able to specify weights manually as in this PR. >. > So the type of the parameter would be Union[str, pd.DataFrame,. > Sequence[Union[float, int]]]. > ------------------------------. >. > All of that doesnâ€™t really affect this PR, as we can merge it as it is and. > include anndata-stored weights later. >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOC4K5CCAJSUVYSIAFDPWUEC5A5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV6M55Q#issuecomment-494718710>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGODWOGS6RD2JQ4LW5LDPWUEC5ANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:987,testability,automat,automatically,987,"Thanks ,. But i will suggest to just support weights instead of coreset, may be user. want to sample data with some other weighting technique. So we should ask. them to just put the weights for observations, then we need to modify PCA. as well and i think my code will support most of plots and marker genes,. but not PCA, because my input is PCA matrix with weights for each. observations. Thanks,. Khalid. On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com> wrote:. > Long-term, we should think about the design here: Specifying weights all. > the time is possible, but not very nice for users. So a few questions come. > to mind:. >. > Should we add scanpy.pp.coreset, which would create a sampling and add. > adata.obs['coreset_weights'] or simply adata.obs['weights']? >. > If we do that or plan to in the future, how should the added weights. > parameter to all these functions work? >. > I think it might default to 'coreset_weights', and the functions would. > automatically use that .obs column if it exists. Users should also still. > be able to specify weights manually as in this PR. >. > So the type of the parameter would be Union[str, pd.DataFrame,. > Sequence[Union[float, int]]]. > ------------------------------. >. > All of that doesnâ€™t really affect this PR, as we can merge it as it is and. > include anndata-stored weights later. >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOC4K5CCAJSUVYSIAFDPWUEC5A5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV6M55Q#issuecomment-494718710>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGODWOGS6RD2JQ4LW5LDPWUEC5ANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:37,usability,support,support,37,"Thanks ,. But i will suggest to just support weights instead of coreset, may be user. want to sample data with some other weighting technique. So we should ask. them to just put the weights for observations, then we need to modify PCA. as well and i think my code will support most of plots and marker genes,. but not PCA, because my input is PCA matrix with weights for each. observations. Thanks,. Khalid. On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com> wrote:. > Long-term, we should think about the design here: Specifying weights all. > the time is possible, but not very nice for users. So a few questions come. > to mind:. >. > Should we add scanpy.pp.coreset, which would create a sampling and add. > adata.obs['coreset_weights'] or simply adata.obs['weights']? >. > If we do that or plan to in the future, how should the added weights. > parameter to all these functions work? >. > I think it might default to 'coreset_weights', and the functions would. > automatically use that .obs column if it exists. Users should also still. > be able to specify weights manually as in this PR. >. > So the type of the parameter would be Union[str, pd.DataFrame,. > Sequence[Union[float, int]]]. > ------------------------------. >. > All of that doesnâ€™t really affect this PR, as we can merge it as it is and. > include anndata-stored weights later. >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOC4K5CCAJSUVYSIAFDPWUEC5A5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV6M55Q#issuecomment-494718710>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGODWOGS6RD2JQ4LW5LDPWUEC5ANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:80,usability,user,user,80,"Thanks ,. But i will suggest to just support weights instead of coreset, may be user. want to sample data with some other weighting technique. So we should ask. them to just put the weights for observations, then we need to modify PCA. as well and i think my code will support most of plots and marker genes,. but not PCA, because my input is PCA matrix with weights for each. observations. Thanks,. Khalid. On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com> wrote:. > Long-term, we should think about the design here: Specifying weights all. > the time is possible, but not very nice for users. So a few questions come. > to mind:. >. > Should we add scanpy.pp.coreset, which would create a sampling and add. > adata.obs['coreset_weights'] or simply adata.obs['weights']? >. > If we do that or plan to in the future, how should the added weights. > parameter to all these functions work? >. > I think it might default to 'coreset_weights', and the functions would. > automatically use that .obs column if it exists. Users should also still. > be able to specify weights manually as in this PR. >. > So the type of the parameter would be Union[str, pd.DataFrame,. > Sequence[Union[float, int]]]. > ------------------------------. >. > All of that doesnâ€™t really affect this PR, as we can merge it as it is and. > include anndata-stored weights later. >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOC4K5CCAJSUVYSIAFDPWUEC5A5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV6M55Q#issuecomment-494718710>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGODWOGS6RD2JQ4LW5LDPWUEC5ANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:269,usability,support,support,269,"Thanks ,. But i will suggest to just support weights instead of coreset, may be user. want to sample data with some other weighting technique. So we should ask. them to just put the weights for observations, then we need to modify PCA. as well and i think my code will support most of plots and marker genes,. but not PCA, because my input is PCA matrix with weights for each. observations. Thanks,. Khalid. On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com> wrote:. > Long-term, we should think about the design here: Specifying weights all. > the time is possible, but not very nice for users. So a few questions come. > to mind:. >. > Should we add scanpy.pp.coreset, which would create a sampling and add. > adata.obs['coreset_weights'] or simply adata.obs['weights']? >. > If we do that or plan to in the future, how should the added weights. > parameter to all these functions work? >. > I think it might default to 'coreset_weights', and the functions would. > automatically use that .obs column if it exists. Users should also still. > be able to specify weights manually as in this PR. >. > So the type of the parameter would be Union[str, pd.DataFrame,. > Sequence[Union[float, int]]]. > ------------------------------. >. > All of that doesnâ€™t really affect this PR, as we can merge it as it is and. > include anndata-stored weights later. >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOC4K5CCAJSUVYSIAFDPWUEC5A5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV6M55Q#issuecomment-494718710>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGODWOGS6RD2JQ4LW5LDPWUEC5ANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:334,usability,input,input,334,"Thanks ,. But i will suggest to just support weights instead of coreset, may be user. want to sample data with some other weighting technique. So we should ask. them to just put the weights for observations, then we need to modify PCA. as well and i think my code will support most of plots and marker genes,. but not PCA, because my input is PCA matrix with weights for each. observations. Thanks,. Khalid. On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com> wrote:. > Long-term, we should think about the design here: Specifying weights all. > the time is possible, but not very nice for users. So a few questions come. > to mind:. >. > Should we add scanpy.pp.coreset, which would create a sampling and add. > adata.obs['coreset_weights'] or simply adata.obs['weights']? >. > If we do that or plan to in the future, how should the added weights. > parameter to all these functions work? >. > I think it might default to 'coreset_weights', and the functions would. > automatically use that .obs column if it exists. Users should also still. > be able to specify weights manually as in this PR. >. > So the type of the parameter would be Union[str, pd.DataFrame,. > Sequence[Union[float, int]]]. > ------------------------------. >. > All of that doesnâ€™t really affect this PR, as we can merge it as it is and. > include anndata-stored weights later. >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOC4K5CCAJSUVYSIAFDPWUEC5A5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV6M55Q#issuecomment-494718710>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGODWOGS6RD2JQ4LW5LDPWUEC5ANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:608,usability,user,users,608,"Thanks ,. But i will suggest to just support weights instead of coreset, may be user. want to sample data with some other weighting technique. So we should ask. them to just put the weights for observations, then we need to modify PCA. as well and i think my code will support most of plots and marker genes,. but not PCA, because my input is PCA matrix with weights for each. observations. Thanks,. Khalid. On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com> wrote:. > Long-term, we should think about the design here: Specifying weights all. > the time is possible, but not very nice for users. So a few questions come. > to mind:. >. > Should we add scanpy.pp.coreset, which would create a sampling and add. > adata.obs['coreset_weights'] or simply adata.obs['weights']? >. > If we do that or plan to in the future, how should the added weights. > parameter to all these functions work? >. > I think it might default to 'coreset_weights', and the functions would. > automatically use that .obs column if it exists. Users should also still. > be able to specify weights manually as in this PR. >. > So the type of the parameter would be Union[str, pd.DataFrame,. > Sequence[Union[float, int]]]. > ------------------------------. >. > All of that doesnâ€™t really affect this PR, as we can merge it as it is and. > include anndata-stored weights later. >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOC4K5CCAJSUVYSIAFDPWUEC5A5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV6M55Q#issuecomment-494718710>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGODWOGS6RD2JQ4LW5LDPWUEC5ANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:763,usability,simpl,simply,763,"Thanks ,. But i will suggest to just support weights instead of coreset, may be user. want to sample data with some other weighting technique. So we should ask. them to just put the weights for observations, then we need to modify PCA. as well and i think my code will support most of plots and marker genes,. but not PCA, because my input is PCA matrix with weights for each. observations. Thanks,. Khalid. On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com> wrote:. > Long-term, we should think about the design here: Specifying weights all. > the time is possible, but not very nice for users. So a few questions come. > to mind:. >. > Should we add scanpy.pp.coreset, which would create a sampling and add. > adata.obs['coreset_weights'] or simply adata.obs['weights']? >. > If we do that or plan to in the future, how should the added weights. > parameter to all these functions work? >. > I think it might default to 'coreset_weights', and the functions would. > automatically use that .obs column if it exists. Users should also still. > be able to specify weights manually as in this PR. >. > So the type of the parameter would be Union[str, pd.DataFrame,. > Sequence[Union[float, int]]]. > ------------------------------. >. > All of that doesnâ€™t really affect this PR, as we can merge it as it is and. > include anndata-stored weights later. >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOC4K5CCAJSUVYSIAFDPWUEC5A5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV6M55Q#issuecomment-494718710>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGODWOGS6RD2JQ4LW5LDPWUEC5ANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1036,usability,User,Users,1036,"Thanks ,. But i will suggest to just support weights instead of coreset, may be user. want to sample data with some other weighting technique. So we should ask. them to just put the weights for observations, then we need to modify PCA. as well and i think my code will support most of plots and marker genes,. but not PCA, because my input is PCA matrix with weights for each. observations. Thanks,. Khalid. On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com> wrote:. > Long-term, we should think about the design here: Specifying weights all. > the time is possible, but not very nice for users. So a few questions come. > to mind:. >. > Should we add scanpy.pp.coreset, which would create a sampling and add. > adata.obs['coreset_weights'] or simply adata.obs['weights']? >. > If we do that or plan to in the future, how should the added weights. > parameter to all these functions work? >. > I think it might default to 'coreset_weights', and the functions would. > automatically use that .obs column if it exists. Users should also still. > be able to specify weights manually as in this PR. >. > So the type of the parameter would be Union[str, pd.DataFrame,. > Sequence[Union[float, int]]]. > ------------------------------. >. > All of that doesnâ€™t really affect this PR, as we can merge it as it is and. > include anndata-stored weights later. >. > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOC4K5CCAJSUVYSIAFDPWUEC5A5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV6M55Q#issuecomment-494718710>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABREGODWOGS6RD2JQ4LW5LDPWUEC5ANCNFSM4HMZ5G7Q>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:23,deployability,updat,updated,23,"Thanks Philipp, I have updated and push the code. I hope you will accept. this pull request now. To support PCA and scanpy for weighted sampling, you. can just set a parameter , observations/samples weights at the time user. input matrix and then we can modify PCA and remaining this code is fine. I. am asking for weights because user may extracted those weights either with. sampling technique or may be sometime user can give weights of his own. desired e.g. he want to focus one cell type etc. So we should support. weights generally rather specifically. Thanks,. Khalid. On Wed, May 22, 2019 at 5:21 PM khalid usman <khalid0491@gmail.com> wrote:. > Thanks ,. >. > But i will suggest to just support weights instead of coreset, may be user. > want to sample data with some other weighting technique. So we should ask. > them to just put the weights for observations, then we need to modify PCA. > as well and i think my code will support most of plots and marker genes,. > but not PCA, because my input is PCA matrix with weights for each. > observations. >. > Thanks,. > Khalid. >. > On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com>. > wrote:. >. >> Long-term, we should think about the design here: Specifying weights all. >> the time is possible, but not very nice for users. So a few questions come. >> to mind:. >>. >> Should we add scanpy.pp.coreset, which would create a sampling and add. >> adata.obs['coreset_weights'] or simply adata.obs['weights']? >>. >> If we do that or plan to in the future, how should the added weights. >> parameter to all these functions work? >>. >> I think it might default to 'coreset_weights', and the functions would. >> automatically use that .obs column if it exists. Users should also still. >> be able to specify weights manually as in this PR. >>. >> So the type of the parameter would be Union[str, pd.DataFrame,. >> Sequence[Union[float, int]]]. >> ------------------------------. >>. >> All of that doesnâ€™t really affect this P",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:178,deployability,observ,observations,178,"Thanks Philipp, I have updated and push the code. I hope you will accept. this pull request now. To support PCA and scanpy for weighted sampling, you. can just set a parameter , observations/samples weights at the time user. input matrix and then we can modify PCA and remaining this code is fine. I. am asking for weights because user may extracted those weights either with. sampling technique or may be sometime user can give weights of his own. desired e.g. he want to focus one cell type etc. So we should support. weights generally rather specifically. Thanks,. Khalid. On Wed, May 22, 2019 at 5:21 PM khalid usman <khalid0491@gmail.com> wrote:. > Thanks ,. >. > But i will suggest to just support weights instead of coreset, may be user. > want to sample data with some other weighting technique. So we should ask. > them to just put the weights for observations, then we need to modify PCA. > as well and i think my code will support most of plots and marker genes,. > but not PCA, because my input is PCA matrix with weights for each. > observations. >. > Thanks,. > Khalid. >. > On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com>. > wrote:. >. >> Long-term, we should think about the design here: Specifying weights all. >> the time is possible, but not very nice for users. So a few questions come. >> to mind:. >>. >> Should we add scanpy.pp.coreset, which would create a sampling and add. >> adata.obs['coreset_weights'] or simply adata.obs['weights']? >>. >> If we do that or plan to in the future, how should the added weights. >> parameter to all these functions work? >>. >> I think it might default to 'coreset_weights', and the functions would. >> automatically use that .obs column if it exists. Users should also still. >> be able to specify weights manually as in this PR. >>. >> So the type of the parameter would be Union[str, pd.DataFrame,. >> Sequence[Union[float, int]]]. >> ------------------------------. >>. >> All of that doesnâ€™t really affect this P",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:857,deployability,observ,observations,857,"Thanks Philipp, I have updated and push the code. I hope you will accept. this pull request now. To support PCA and scanpy for weighted sampling, you. can just set a parameter , observations/samples weights at the time user. input matrix and then we can modify PCA and remaining this code is fine. I. am asking for weights because user may extracted those weights either with. sampling technique or may be sometime user can give weights of his own. desired e.g. he want to focus one cell type etc. So we should support. weights generally rather specifically. Thanks,. Khalid. On Wed, May 22, 2019 at 5:21 PM khalid usman <khalid0491@gmail.com> wrote:. > Thanks ,. >. > But i will suggest to just support weights instead of coreset, may be user. > want to sample data with some other weighting technique. So we should ask. > them to just put the weights for observations, then we need to modify PCA. > as well and i think my code will support most of plots and marker genes,. > but not PCA, because my input is PCA matrix with weights for each. > observations. >. > Thanks,. > Khalid. >. > On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com>. > wrote:. >. >> Long-term, we should think about the design here: Specifying weights all. >> the time is possible, but not very nice for users. So a few questions come. >> to mind:. >>. >> Should we add scanpy.pp.coreset, which would create a sampling and add. >> adata.obs['coreset_weights'] or simply adata.obs['weights']? >>. >> If we do that or plan to in the future, how should the added weights. >> parameter to all these functions work? >>. >> I think it might default to 'coreset_weights', and the functions would. >> automatically use that .obs column if it exists. Users should also still. >> be able to specify weights manually as in this PR. >>. >> So the type of the parameter would be Union[str, pd.DataFrame,. >> Sequence[Union[float, int]]]. >> ------------------------------. >>. >> All of that doesnâ€™t really affect this P",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1046,deployability,observ,observations,1046,"hope you will accept. this pull request now. To support PCA and scanpy for weighted sampling, you. can just set a parameter , observations/samples weights at the time user. input matrix and then we can modify PCA and remaining this code is fine. I. am asking for weights because user may extracted those weights either with. sampling technique or may be sometime user can give weights of his own. desired e.g. he want to focus one cell type etc. So we should support. weights generally rather specifically. Thanks,. Khalid. On Wed, May 22, 2019 at 5:21 PM khalid usman <khalid0491@gmail.com> wrote:. > Thanks ,. >. > But i will suggest to just support weights instead of coreset, may be user. > want to sample data with some other weighting technique. So we should ask. > them to just put the weights for observations, then we need to modify PCA. > as well and i think my code will support most of plots and marker genes,. > but not PCA, because my input is PCA matrix with weights for each. > observations. >. > Thanks,. > Khalid. >. > On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com>. > wrote:. >. >> Long-term, we should think about the design here: Specifying weights all. >> the time is possible, but not very nice for users. So a few questions come. >> to mind:. >>. >> Should we add scanpy.pp.coreset, which would create a sampling and add. >> adata.obs['coreset_weights'] or simply adata.obs['weights']? >>. >> If we do that or plan to in the future, how should the added weights. >> parameter to all these functions work? >>. >> I think it might default to 'coreset_weights', and the functions would. >> automatically use that .obs column if it exists. Users should also still. >> be able to specify weights manually as in this PR. >>. >> So the type of the parameter would be Union[str, pd.DataFrame,. >> Sequence[Union[float, int]]]. >> ------------------------------. >>. >> All of that doesnâ€™t really affect this PR, as we can merge it as it is. >> and include annda",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1686,deployability,automat,automatically,1686,"ally. Thanks,. Khalid. On Wed, May 22, 2019 at 5:21 PM khalid usman <khalid0491@gmail.com> wrote:. > Thanks ,. >. > But i will suggest to just support weights instead of coreset, may be user. > want to sample data with some other weighting technique. So we should ask. > them to just put the weights for observations, then we need to modify PCA. > as well and i think my code will support most of plots and marker genes,. > but not PCA, because my input is PCA matrix with weights for each. > observations. >. > Thanks,. > Khalid. >. > On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com>. > wrote:. >. >> Long-term, we should think about the design here: Specifying weights all. >> the time is possible, but not very nice for users. So a few questions come. >> to mind:. >>. >> Should we add scanpy.pp.coreset, which would create a sampling and add. >> adata.obs['coreset_weights'] or simply adata.obs['weights']? >>. >> If we do that or plan to in the future, how should the added weights. >> parameter to all these functions work? >>. >> I think it might default to 'coreset_weights', and the functions would. >> automatically use that .obs column if it exists. Users should also still. >> be able to specify weights manually as in this PR. >>. >> So the type of the parameter would be Union[str, pd.DataFrame,. >> Sequence[Union[float, int]]]. >> ------------------------------. >>. >> All of that doesnâ€™t really affect this PR, as we can merge it as it is. >> and include anndata-stored weights later. >>. >> â€”. >> You are receiving this because you were mentioned. >> Reply to this email directly, view it on GitHub. >> <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOC4K5CCAJSUVYSIAFDPWUEC5A5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV6M55Q#issuecomment-494718710>,. >> or mute the thread. >> <https://github.com/notifications/unsubscribe-auth/ABREGODWOGS6RD2JQ4LW5LDPWUEC5ANCNFSM4HMZ5G7Q>. >> . >>. >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:723,energy efficiency,core,coreset,723,"Thanks Philipp, I have updated and push the code. I hope you will accept. this pull request now. To support PCA and scanpy for weighted sampling, you. can just set a parameter , observations/samples weights at the time user. input matrix and then we can modify PCA and remaining this code is fine. I. am asking for weights because user may extracted those weights either with. sampling technique or may be sometime user can give weights of his own. desired e.g. he want to focus one cell type etc. So we should support. weights generally rather specifically. Thanks,. Khalid. On Wed, May 22, 2019 at 5:21 PM khalid usman <khalid0491@gmail.com> wrote:. > Thanks ,. >. > But i will suggest to just support weights instead of coreset, may be user. > want to sample data with some other weighting technique. So we should ask. > them to just put the weights for observations, then we need to modify PCA. > as well and i think my code will support most of plots and marker genes,. > but not PCA, because my input is PCA matrix with weights for each. > observations. >. > Thanks,. > Khalid. >. > On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com>. > wrote:. >. >> Long-term, we should think about the design here: Specifying weights all. >> the time is possible, but not very nice for users. So a few questions come. >> to mind:. >>. >> Should we add scanpy.pp.coreset, which would create a sampling and add. >> adata.obs['coreset_weights'] or simply adata.obs['weights']? >>. >> If we do that or plan to in the future, how should the added weights. >> parameter to all these functions work? >>. >> I think it might default to 'coreset_weights', and the functions would. >> automatically use that .obs column if it exists. Users should also still. >> be able to specify weights manually as in this PR. >>. >> So the type of the parameter would be Union[str, pd.DataFrame,. >> Sequence[Union[float, int]]]. >> ------------------------------. >>. >> All of that doesnâ€™t really affect this P",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1373,energy efficiency,core,coreset,1373,"sampling technique or may be sometime user can give weights of his own. desired e.g. he want to focus one cell type etc. So we should support. weights generally rather specifically. Thanks,. Khalid. On Wed, May 22, 2019 at 5:21 PM khalid usman <khalid0491@gmail.com> wrote:. > Thanks ,. >. > But i will suggest to just support weights instead of coreset, may be user. > want to sample data with some other weighting technique. So we should ask. > them to just put the weights for observations, then we need to modify PCA. > as well and i think my code will support most of plots and marker genes,. > but not PCA, because my input is PCA matrix with weights for each. > observations. >. > Thanks,. > Khalid. >. > On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com>. > wrote:. >. >> Long-term, we should think about the design here: Specifying weights all. >> the time is possible, but not very nice for users. So a few questions come. >> to mind:. >>. >> Should we add scanpy.pp.coreset, which would create a sampling and add. >> adata.obs['coreset_weights'] or simply adata.obs['weights']? >>. >> If we do that or plan to in the future, how should the added weights. >> parameter to all these functions work? >>. >> I think it might default to 'coreset_weights', and the functions would. >> automatically use that .obs column if it exists. Users should also still. >> be able to specify weights manually as in this PR. >>. >> So the type of the parameter would be Union[str, pd.DataFrame,. >> Sequence[Union[float, int]]]. >> ------------------------------. >>. >> All of that doesnâ€™t really affect this PR, as we can merge it as it is. >> and include anndata-stored weights later. >>. >> â€”. >> You are receiving this because you were mentioned. >> Reply to this email directly, view it on GitHub. >> <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOC4K5CCAJSUVYSIAFDPWUEC5A5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOO",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:545,interoperability,specif,specifically,545,"Thanks Philipp, I have updated and push the code. I hope you will accept. this pull request now. To support PCA and scanpy for weighted sampling, you. can just set a parameter , observations/samples weights at the time user. input matrix and then we can modify PCA and remaining this code is fine. I. am asking for weights because user may extracted those weights either with. sampling technique or may be sometime user can give weights of his own. desired e.g. he want to focus one cell type etc. So we should support. weights generally rather specifically. Thanks,. Khalid. On Wed, May 22, 2019 at 5:21 PM khalid usman <khalid0491@gmail.com> wrote:. > Thanks ,. >. > But i will suggest to just support weights instead of coreset, may be user. > want to sample data with some other weighting technique. So we should ask. > them to just put the weights for observations, then we need to modify PCA. > as well and i think my code will support most of plots and marker genes,. > but not PCA, because my input is PCA matrix with weights for each. > observations. >. > Thanks,. > Khalid. >. > On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com>. > wrote:. >. >> Long-term, we should think about the design here: Specifying weights all. >> the time is possible, but not very nice for users. So a few questions come. >> to mind:. >>. >> Should we add scanpy.pp.coreset, which would create a sampling and add. >> adata.obs['coreset_weights'] or simply adata.obs['weights']? >>. >> If we do that or plan to in the future, how should the added weights. >> parameter to all these functions work? >>. >> I think it might default to 'coreset_weights', and the functions would. >> automatically use that .obs column if it exists. Users should also still. >> be able to specify weights manually as in this PR. >>. >> So the type of the parameter would be Union[str, pd.DataFrame,. >> Sequence[Union[float, int]]]. >> ------------------------------. >>. >> All of that doesnâ€™t really affect this P",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1226,interoperability,Specif,Specifying,1226,"matrix and then we can modify PCA and remaining this code is fine. I. am asking for weights because user may extracted those weights either with. sampling technique or may be sometime user can give weights of his own. desired e.g. he want to focus one cell type etc. So we should support. weights generally rather specifically. Thanks,. Khalid. On Wed, May 22, 2019 at 5:21 PM khalid usman <khalid0491@gmail.com> wrote:. > Thanks ,. >. > But i will suggest to just support weights instead of coreset, may be user. > want to sample data with some other weighting technique. So we should ask. > them to just put the weights for observations, then we need to modify PCA. > as well and i think my code will support most of plots and marker genes,. > but not PCA, because my input is PCA matrix with weights for each. > observations. >. > Thanks,. > Khalid. >. > On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com>. > wrote:. >. >> Long-term, we should think about the design here: Specifying weights all. >> the time is possible, but not very nice for users. So a few questions come. >> to mind:. >>. >> Should we add scanpy.pp.coreset, which would create a sampling and add. >> adata.obs['coreset_weights'] or simply adata.obs['weights']? >>. >> If we do that or plan to in the future, how should the added weights. >> parameter to all these functions work? >>. >> I think it might default to 'coreset_weights', and the functions would. >> automatically use that .obs column if it exists. Users should also still. >> be able to specify weights manually as in this PR. >>. >> So the type of the parameter would be Union[str, pd.DataFrame,. >> Sequence[Union[float, int]]]. >> ------------------------------. >>. >> All of that doesnâ€™t really affect this PR, as we can merge it as it is. >> and include anndata-stored weights later. >>. >> â€”. >> You are receiving this because you were mentioned. >> Reply to this email directly, view it on GitHub. >> <https://github.com/theislab/scanpy",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1774,interoperability,specif,specify,1774,"ally. Thanks,. Khalid. On Wed, May 22, 2019 at 5:21 PM khalid usman <khalid0491@gmail.com> wrote:. > Thanks ,. >. > But i will suggest to just support weights instead of coreset, may be user. > want to sample data with some other weighting technique. So we should ask. > them to just put the weights for observations, then we need to modify PCA. > as well and i think my code will support most of plots and marker genes,. > but not PCA, because my input is PCA matrix with weights for each. > observations. >. > Thanks,. > Khalid. >. > On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com>. > wrote:. >. >> Long-term, we should think about the design here: Specifying weights all. >> the time is possible, but not very nice for users. So a few questions come. >> to mind:. >>. >> Should we add scanpy.pp.coreset, which would create a sampling and add. >> adata.obs['coreset_weights'] or simply adata.obs['weights']? >>. >> If we do that or plan to in the future, how should the added weights. >> parameter to all these functions work? >>. >> I think it might default to 'coreset_weights', and the functions would. >> automatically use that .obs column if it exists. Users should also still. >> be able to specify weights manually as in this PR. >>. >> So the type of the parameter would be Union[str, pd.DataFrame,. >> Sequence[Union[float, int]]]. >> ------------------------------. >>. >> All of that doesnâ€™t really affect this PR, as we can merge it as it is. >> and include anndata-stored weights later. >>. >> â€”. >> You are receiving this because you were mentioned. >> Reply to this email directly, view it on GitHub. >> <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOC4K5CCAJSUVYSIAFDPWUEC5A5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV6M55Q#issuecomment-494718710>,. >> or mute the thread. >> <https://github.com/notifications/unsubscribe-auth/ABREGODWOGS6RD2JQ4LW5LDPWUEC5ANCNFSM4HMZ5G7Q>. >> . >>. >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:166,modifiability,paramet,parameter,166,"Thanks Philipp, I have updated and push the code. I hope you will accept. this pull request now. To support PCA and scanpy for weighted sampling, you. can just set a parameter , observations/samples weights at the time user. input matrix and then we can modify PCA and remaining this code is fine. I. am asking for weights because user may extracted those weights either with. sampling technique or may be sometime user can give weights of his own. desired e.g. he want to focus one cell type etc. So we should support. weights generally rather specifically. Thanks,. Khalid. On Wed, May 22, 2019 at 5:21 PM khalid usman <khalid0491@gmail.com> wrote:. > Thanks ,. >. > But i will suggest to just support weights instead of coreset, may be user. > want to sample data with some other weighting technique. So we should ask. > them to just put the weights for observations, then we need to modify PCA. > as well and i think my code will support most of plots and marker genes,. > but not PCA, because my input is PCA matrix with weights for each. > observations. >. > Thanks,. > Khalid. >. > On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com>. > wrote:. >. >> Long-term, we should think about the design here: Specifying weights all. >> the time is possible, but not very nice for users. So a few questions come. >> to mind:. >>. >> Should we add scanpy.pp.coreset, which would create a sampling and add. >> adata.obs['coreset_weights'] or simply adata.obs['weights']? >>. >> If we do that or plan to in the future, how should the added weights. >> parameter to all these functions work? >>. >> I think it might default to 'coreset_weights', and the functions would. >> automatically use that .obs column if it exists. Users should also still. >> be able to specify weights manually as in this PR. >>. >> So the type of the parameter would be Union[str, pd.DataFrame,. >> Sequence[Union[float, int]]]. >> ------------------------------. >>. >> All of that doesnâ€™t really affect this P",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1565,modifiability,paramet,parameter,1565,"ally. Thanks,. Khalid. On Wed, May 22, 2019 at 5:21 PM khalid usman <khalid0491@gmail.com> wrote:. > Thanks ,. >. > But i will suggest to just support weights instead of coreset, may be user. > want to sample data with some other weighting technique. So we should ask. > them to just put the weights for observations, then we need to modify PCA. > as well and i think my code will support most of plots and marker genes,. > but not PCA, because my input is PCA matrix with weights for each. > observations. >. > Thanks,. > Khalid. >. > On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com>. > wrote:. >. >> Long-term, we should think about the design here: Specifying weights all. >> the time is possible, but not very nice for users. So a few questions come. >> to mind:. >>. >> Should we add scanpy.pp.coreset, which would create a sampling and add. >> adata.obs['coreset_weights'] or simply adata.obs['weights']? >>. >> If we do that or plan to in the future, how should the added weights. >> parameter to all these functions work? >>. >> I think it might default to 'coreset_weights', and the functions would. >> automatically use that .obs column if it exists. Users should also still. >> be able to specify weights manually as in this PR. >>. >> So the type of the parameter would be Union[str, pd.DataFrame,. >> Sequence[Union[float, int]]]. >> ------------------------------. >>. >> All of that doesnâ€™t really affect this PR, as we can merge it as it is. >> and include anndata-stored weights later. >>. >> â€”. >> You are receiving this because you were mentioned. >> Reply to this email directly, view it on GitHub. >> <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOC4K5CCAJSUVYSIAFDPWUEC5A5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV6M55Q#issuecomment-494718710>,. >> or mute the thread. >> <https://github.com/notifications/unsubscribe-auth/ABREGODWOGS6RD2JQ4LW5LDPWUEC5ANCNFSM4HMZ5G7Q>. >> . >>. >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1840,modifiability,paramet,parameter,1840,"ally. Thanks,. Khalid. On Wed, May 22, 2019 at 5:21 PM khalid usman <khalid0491@gmail.com> wrote:. > Thanks ,. >. > But i will suggest to just support weights instead of coreset, may be user. > want to sample data with some other weighting technique. So we should ask. > them to just put the weights for observations, then we need to modify PCA. > as well and i think my code will support most of plots and marker genes,. > but not PCA, because my input is PCA matrix with weights for each. > observations. >. > Thanks,. > Khalid. >. > On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com>. > wrote:. >. >> Long-term, we should think about the design here: Specifying weights all. >> the time is possible, but not very nice for users. So a few questions come. >> to mind:. >>. >> Should we add scanpy.pp.coreset, which would create a sampling and add. >> adata.obs['coreset_weights'] or simply adata.obs['weights']? >>. >> If we do that or plan to in the future, how should the added weights. >> parameter to all these functions work? >>. >> I think it might default to 'coreset_weights', and the functions would. >> automatically use that .obs column if it exists. Users should also still. >> be able to specify weights manually as in this PR. >>. >> So the type of the parameter would be Union[str, pd.DataFrame,. >> Sequence[Union[float, int]]]. >> ------------------------------. >>. >> All of that doesnâ€™t really affect this PR, as we can merge it as it is. >> and include anndata-stored weights later. >>. >> â€”. >> You are receiving this because you were mentioned. >> Reply to this email directly, view it on GitHub. >> <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOC4K5CCAJSUVYSIAFDPWUEC5A5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV6M55Q#issuecomment-494718710>,. >> or mute the thread. >> <https://github.com/notifications/unsubscribe-auth/ABREGODWOGS6RD2JQ4LW5LDPWUEC5ANCNFSM4HMZ5G7Q>. >> . >>. >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:214,performance,time,time,214,"Thanks Philipp, I have updated and push the code. I hope you will accept. this pull request now. To support PCA and scanpy for weighted sampling, you. can just set a parameter , observations/samples weights at the time user. input matrix and then we can modify PCA and remaining this code is fine. I. am asking for weights because user may extracted those weights either with. sampling technique or may be sometime user can give weights of his own. desired e.g. he want to focus one cell type etc. So we should support. weights generally rather specifically. Thanks,. Khalid. On Wed, May 22, 2019 at 5:21 PM khalid usman <khalid0491@gmail.com> wrote:. > Thanks ,. >. > But i will suggest to just support weights instead of coreset, may be user. > want to sample data with some other weighting technique. So we should ask. > them to just put the weights for observations, then we need to modify PCA. > as well and i think my code will support most of plots and marker genes,. > but not PCA, because my input is PCA matrix with weights for each. > observations. >. > Thanks,. > Khalid. >. > On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com>. > wrote:. >. >> Long-term, we should think about the design here: Specifying weights all. >> the time is possible, but not very nice for users. So a few questions come. >> to mind:. >>. >> Should we add scanpy.pp.coreset, which would create a sampling and add. >> adata.obs['coreset_weights'] or simply adata.obs['weights']? >>. >> If we do that or plan to in the future, how should the added weights. >> parameter to all these functions work? >>. >> I think it might default to 'coreset_weights', and the functions would. >> automatically use that .obs column if it exists. Users should also still. >> be able to specify weights manually as in this PR. >>. >> So the type of the parameter would be Union[str, pd.DataFrame,. >> Sequence[Union[float, int]]]. >> ------------------------------. >>. >> All of that doesnâ€™t really affect this P",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1257,performance,time,time,1257,"y PCA and remaining this code is fine. I. am asking for weights because user may extracted those weights either with. sampling technique or may be sometime user can give weights of his own. desired e.g. he want to focus one cell type etc. So we should support. weights generally rather specifically. Thanks,. Khalid. On Wed, May 22, 2019 at 5:21 PM khalid usman <khalid0491@gmail.com> wrote:. > Thanks ,. >. > But i will suggest to just support weights instead of coreset, may be user. > want to sample data with some other weighting technique. So we should ask. > them to just put the weights for observations, then we need to modify PCA. > as well and i think my code will support most of plots and marker genes,. > but not PCA, because my input is PCA matrix with weights for each. > observations. >. > Thanks,. > Khalid. >. > On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com>. > wrote:. >. >> Long-term, we should think about the design here: Specifying weights all. >> the time is possible, but not very nice for users. So a few questions come. >> to mind:. >>. >> Should we add scanpy.pp.coreset, which would create a sampling and add. >> adata.obs['coreset_weights'] or simply adata.obs['weights']? >>. >> If we do that or plan to in the future, how should the added weights. >> parameter to all these functions work? >>. >> I think it might default to 'coreset_weights', and the functions would. >> automatically use that .obs column if it exists. Users should also still. >> be able to specify weights manually as in this PR. >>. >> So the type of the parameter would be Union[str, pd.DataFrame,. >> Sequence[Union[float, int]]]. >> ------------------------------. >>. >> All of that doesnâ€™t really affect this PR, as we can merge it as it is. >> and include anndata-stored weights later. >>. >> â€”. >> You are receiving this because you were mentioned. >> Reply to this email directly, view it on GitHub. >> <https://github.com/theislab/scanpy/pull/644?email_source=notif",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1972,reliability,doe,doesn,1972,"ally. Thanks,. Khalid. On Wed, May 22, 2019 at 5:21 PM khalid usman <khalid0491@gmail.com> wrote:. > Thanks ,. >. > But i will suggest to just support weights instead of coreset, may be user. > want to sample data with some other weighting technique. So we should ask. > them to just put the weights for observations, then we need to modify PCA. > as well and i think my code will support most of plots and marker genes,. > but not PCA, because my input is PCA matrix with weights for each. > observations. >. > Thanks,. > Khalid. >. > On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com>. > wrote:. >. >> Long-term, we should think about the design here: Specifying weights all. >> the time is possible, but not very nice for users. So a few questions come. >> to mind:. >>. >> Should we add scanpy.pp.coreset, which would create a sampling and add. >> adata.obs['coreset_weights'] or simply adata.obs['weights']? >>. >> If we do that or plan to in the future, how should the added weights. >> parameter to all these functions work? >>. >> I think it might default to 'coreset_weights', and the functions would. >> automatically use that .obs column if it exists. Users should also still. >> be able to specify weights manually as in this PR. >>. >> So the type of the parameter would be Union[str, pd.DataFrame,. >> Sequence[Union[float, int]]]. >> ------------------------------. >>. >> All of that doesnâ€™t really affect this PR, as we can merge it as it is. >> and include anndata-stored weights later. >>. >> â€”. >> You are receiving this because you were mentioned. >> Reply to this email directly, view it on GitHub. >> <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOC4K5CCAJSUVYSIAFDPWUEC5A5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV6M55Q#issuecomment-494718710>,. >> or mute the thread. >> <https://github.com/notifications/unsubscribe-auth/ABREGODWOGS6RD2JQ4LW5LDPWUEC5ANCNFSM4HMZ5G7Q>. >> . >>. >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:23,safety,updat,updated,23,"Thanks Philipp, I have updated and push the code. I hope you will accept. this pull request now. To support PCA and scanpy for weighted sampling, you. can just set a parameter , observations/samples weights at the time user. input matrix and then we can modify PCA and remaining this code is fine. I. am asking for weights because user may extracted those weights either with. sampling technique or may be sometime user can give weights of his own. desired e.g. he want to focus one cell type etc. So we should support. weights generally rather specifically. Thanks,. Khalid. On Wed, May 22, 2019 at 5:21 PM khalid usman <khalid0491@gmail.com> wrote:. > Thanks ,. >. > But i will suggest to just support weights instead of coreset, may be user. > want to sample data with some other weighting technique. So we should ask. > them to just put the weights for observations, then we need to modify PCA. > as well and i think my code will support most of plots and marker genes,. > but not PCA, because my input is PCA matrix with weights for each. > observations. >. > Thanks,. > Khalid. >. > On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com>. > wrote:. >. >> Long-term, we should think about the design here: Specifying weights all. >> the time is possible, but not very nice for users. So a few questions come. >> to mind:. >>. >> Should we add scanpy.pp.coreset, which would create a sampling and add. >> adata.obs['coreset_weights'] or simply adata.obs['weights']? >>. >> If we do that or plan to in the future, how should the added weights. >> parameter to all these functions work? >>. >> I think it might default to 'coreset_weights', and the functions would. >> automatically use that .obs column if it exists. Users should also still. >> be able to specify weights manually as in this PR. >>. >> So the type of the parameter would be Union[str, pd.DataFrame,. >> Sequence[Union[float, int]]]. >> ------------------------------. >>. >> All of that doesnâ€™t really affect this P",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:225,safety,input,input,225,"Thanks Philipp, I have updated and push the code. I hope you will accept. this pull request now. To support PCA and scanpy for weighted sampling, you. can just set a parameter , observations/samples weights at the time user. input matrix and then we can modify PCA and remaining this code is fine. I. am asking for weights because user may extracted those weights either with. sampling technique or may be sometime user can give weights of his own. desired e.g. he want to focus one cell type etc. So we should support. weights generally rather specifically. Thanks,. Khalid. On Wed, May 22, 2019 at 5:21 PM khalid usman <khalid0491@gmail.com> wrote:. > Thanks ,. >. > But i will suggest to just support weights instead of coreset, may be user. > want to sample data with some other weighting technique. So we should ask. > them to just put the weights for observations, then we need to modify PCA. > as well and i think my code will support most of plots and marker genes,. > but not PCA, because my input is PCA matrix with weights for each. > observations. >. > Thanks,. > Khalid. >. > On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com>. > wrote:. >. >> Long-term, we should think about the design here: Specifying weights all. >> the time is possible, but not very nice for users. So a few questions come. >> to mind:. >>. >> Should we add scanpy.pp.coreset, which would create a sampling and add. >> adata.obs['coreset_weights'] or simply adata.obs['weights']? >>. >> If we do that or plan to in the future, how should the added weights. >> parameter to all these functions work? >>. >> I think it might default to 'coreset_weights', and the functions would. >> automatically use that .obs column if it exists. Users should also still. >> be able to specify weights manually as in this PR. >>. >> So the type of the parameter would be Union[str, pd.DataFrame,. >> Sequence[Union[float, int]]]. >> ------------------------------. >>. >> All of that doesnâ€™t really affect this P",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1001,safety,input,input,1001,"ks Philipp, I have updated and push the code. I hope you will accept. this pull request now. To support PCA and scanpy for weighted sampling, you. can just set a parameter , observations/samples weights at the time user. input matrix and then we can modify PCA and remaining this code is fine. I. am asking for weights because user may extracted those weights either with. sampling technique or may be sometime user can give weights of his own. desired e.g. he want to focus one cell type etc. So we should support. weights generally rather specifically. Thanks,. Khalid. On Wed, May 22, 2019 at 5:21 PM khalid usman <khalid0491@gmail.com> wrote:. > Thanks ,. >. > But i will suggest to just support weights instead of coreset, may be user. > want to sample data with some other weighting technique. So we should ask. > them to just put the weights for observations, then we need to modify PCA. > as well and i think my code will support most of plots and marker genes,. > but not PCA, because my input is PCA matrix with weights for each. > observations. >. > Thanks,. > Khalid. >. > On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com>. > wrote:. >. >> Long-term, we should think about the design here: Specifying weights all. >> the time is possible, but not very nice for users. So a few questions come. >> to mind:. >>. >> Should we add scanpy.pp.coreset, which would create a sampling and add. >> adata.obs['coreset_weights'] or simply adata.obs['weights']? >>. >> If we do that or plan to in the future, how should the added weights. >> parameter to all these functions work? >>. >> I think it might default to 'coreset_weights', and the functions would. >> automatically use that .obs column if it exists. Users should also still. >> be able to specify weights manually as in this PR. >>. >> So the type of the parameter would be Union[str, pd.DataFrame,. >> Sequence[Union[float, int]]]. >> ------------------------------. >>. >> All of that doesnâ€™t really affect this PR, a",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:23,security,updat,updated,23,"Thanks Philipp, I have updated and push the code. I hope you will accept. this pull request now. To support PCA and scanpy for weighted sampling, you. can just set a parameter , observations/samples weights at the time user. input matrix and then we can modify PCA and remaining this code is fine. I. am asking for weights because user may extracted those weights either with. sampling technique or may be sometime user can give weights of his own. desired e.g. he want to focus one cell type etc. So we should support. weights generally rather specifically. Thanks,. Khalid. On Wed, May 22, 2019 at 5:21 PM khalid usman <khalid0491@gmail.com> wrote:. > Thanks ,. >. > But i will suggest to just support weights instead of coreset, may be user. > want to sample data with some other weighting technique. So we should ask. > them to just put the weights for observations, then we need to modify PCA. > as well and i think my code will support most of plots and marker genes,. > but not PCA, because my input is PCA matrix with weights for each. > observations. >. > Thanks,. > Khalid. >. > On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com>. > wrote:. >. >> Long-term, we should think about the design here: Specifying weights all. >> the time is possible, but not very nice for users. So a few questions come. >> to mind:. >>. >> Should we add scanpy.pp.coreset, which would create a sampling and add. >> adata.obs['coreset_weights'] or simply adata.obs['weights']? >>. >> If we do that or plan to in the future, how should the added weights. >> parameter to all these functions work? >>. >> I think it might default to 'coreset_weights', and the functions would. >> automatically use that .obs column if it exists. Users should also still. >> be able to specify weights manually as in this PR. >>. >> So the type of the parameter would be Union[str, pd.DataFrame,. >> Sequence[Union[float, int]]]. >> ------------------------------. >>. >> All of that doesnâ€™t really affect this P",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:254,security,modif,modify,254,"Thanks Philipp, I have updated and push the code. I hope you will accept. this pull request now. To support PCA and scanpy for weighted sampling, you. can just set a parameter , observations/samples weights at the time user. input matrix and then we can modify PCA and remaining this code is fine. I. am asking for weights because user may extracted those weights either with. sampling technique or may be sometime user can give weights of his own. desired e.g. he want to focus one cell type etc. So we should support. weights generally rather specifically. Thanks,. Khalid. On Wed, May 22, 2019 at 5:21 PM khalid usman <khalid0491@gmail.com> wrote:. > Thanks ,. >. > But i will suggest to just support weights instead of coreset, may be user. > want to sample data with some other weighting technique. So we should ask. > them to just put the weights for observations, then we need to modify PCA. > as well and i think my code will support most of plots and marker genes,. > but not PCA, because my input is PCA matrix with weights for each. > observations. >. > Thanks,. > Khalid. >. > On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com>. > wrote:. >. >> Long-term, we should think about the design here: Specifying weights all. >> the time is possible, but not very nice for users. So a few questions come. >> to mind:. >>. >> Should we add scanpy.pp.coreset, which would create a sampling and add. >> adata.obs['coreset_weights'] or simply adata.obs['weights']? >>. >> If we do that or plan to in the future, how should the added weights. >> parameter to all these functions work? >>. >> I think it might default to 'coreset_weights', and the functions would. >> automatically use that .obs column if it exists. Users should also still. >> be able to specify weights manually as in this PR. >>. >> So the type of the parameter would be Union[str, pd.DataFrame,. >> Sequence[Union[float, int]]]. >> ------------------------------. >>. >> All of that doesnâ€™t really affect this P",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:887,security,modif,modify,887,"Thanks Philipp, I have updated and push the code. I hope you will accept. this pull request now. To support PCA and scanpy for weighted sampling, you. can just set a parameter , observations/samples weights at the time user. input matrix and then we can modify PCA and remaining this code is fine. I. am asking for weights because user may extracted those weights either with. sampling technique or may be sometime user can give weights of his own. desired e.g. he want to focus one cell type etc. So we should support. weights generally rather specifically. Thanks,. Khalid. On Wed, May 22, 2019 at 5:21 PM khalid usman <khalid0491@gmail.com> wrote:. > Thanks ,. >. > But i will suggest to just support weights instead of coreset, may be user. > want to sample data with some other weighting technique. So we should ask. > them to just put the weights for observations, then we need to modify PCA. > as well and i think my code will support most of plots and marker genes,. > but not PCA, because my input is PCA matrix with weights for each. > observations. >. > Thanks,. > Khalid. >. > On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com>. > wrote:. >. >> Long-term, we should think about the design here: Specifying weights all. >> the time is possible, but not very nice for users. So a few questions come. >> to mind:. >>. >> Should we add scanpy.pp.coreset, which would create a sampling and add. >> adata.obs['coreset_weights'] or simply adata.obs['weights']? >>. >> If we do that or plan to in the future, how should the added weights. >> parameter to all these functions work? >>. >> I think it might default to 'coreset_weights', and the functions would. >> automatically use that .obs column if it exists. Users should also still. >> be able to specify weights manually as in this PR. >>. >> So the type of the parameter would be Union[str, pd.DataFrame,. >> Sequence[Union[float, int]]]. >> ------------------------------. >>. >> All of that doesnâ€™t really affect this P",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:2490,security,auth,auth,2490,"ally. Thanks,. Khalid. On Wed, May 22, 2019 at 5:21 PM khalid usman <khalid0491@gmail.com> wrote:. > Thanks ,. >. > But i will suggest to just support weights instead of coreset, may be user. > want to sample data with some other weighting technique. So we should ask. > them to just put the weights for observations, then we need to modify PCA. > as well and i think my code will support most of plots and marker genes,. > but not PCA, because my input is PCA matrix with weights for each. > observations. >. > Thanks,. > Khalid. >. > On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com>. > wrote:. >. >> Long-term, we should think about the design here: Specifying weights all. >> the time is possible, but not very nice for users. So a few questions come. >> to mind:. >>. >> Should we add scanpy.pp.coreset, which would create a sampling and add. >> adata.obs['coreset_weights'] or simply adata.obs['weights']? >>. >> If we do that or plan to in the future, how should the added weights. >> parameter to all these functions work? >>. >> I think it might default to 'coreset_weights', and the functions would. >> automatically use that .obs column if it exists. Users should also still. >> be able to specify weights manually as in this PR. >>. >> So the type of the parameter would be Union[str, pd.DataFrame,. >> Sequence[Union[float, int]]]. >> ------------------------------. >>. >> All of that doesnâ€™t really affect this PR, as we can merge it as it is. >> and include anndata-stored weights later. >>. >> â€”. >> You are receiving this because you were mentioned. >> Reply to this email directly, view it on GitHub. >> <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOC4K5CCAJSUVYSIAFDPWUEC5A5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV6M55Q#issuecomment-494718710>,. >> or mute the thread. >> <https://github.com/notifications/unsubscribe-auth/ABREGODWOGS6RD2JQ4LW5LDPWUEC5ANCNFSM4HMZ5G7Q>. >> . >>. >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:178,testability,observ,observations,178,"Thanks Philipp, I have updated and push the code. I hope you will accept. this pull request now. To support PCA and scanpy for weighted sampling, you. can just set a parameter , observations/samples weights at the time user. input matrix and then we can modify PCA and remaining this code is fine. I. am asking for weights because user may extracted those weights either with. sampling technique or may be sometime user can give weights of his own. desired e.g. he want to focus one cell type etc. So we should support. weights generally rather specifically. Thanks,. Khalid. On Wed, May 22, 2019 at 5:21 PM khalid usman <khalid0491@gmail.com> wrote:. > Thanks ,. >. > But i will suggest to just support weights instead of coreset, may be user. > want to sample data with some other weighting technique. So we should ask. > them to just put the weights for observations, then we need to modify PCA. > as well and i think my code will support most of plots and marker genes,. > but not PCA, because my input is PCA matrix with weights for each. > observations. >. > Thanks,. > Khalid. >. > On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com>. > wrote:. >. >> Long-term, we should think about the design here: Specifying weights all. >> the time is possible, but not very nice for users. So a few questions come. >> to mind:. >>. >> Should we add scanpy.pp.coreset, which would create a sampling and add. >> adata.obs['coreset_weights'] or simply adata.obs['weights']? >>. >> If we do that or plan to in the future, how should the added weights. >> parameter to all these functions work? >>. >> I think it might default to 'coreset_weights', and the functions would. >> automatically use that .obs column if it exists. Users should also still. >> be able to specify weights manually as in this PR. >>. >> So the type of the parameter would be Union[str, pd.DataFrame,. >> Sequence[Union[float, int]]]. >> ------------------------------. >>. >> All of that doesnâ€™t really affect this P",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:857,testability,observ,observations,857,"Thanks Philipp, I have updated and push the code. I hope you will accept. this pull request now. To support PCA and scanpy for weighted sampling, you. can just set a parameter , observations/samples weights at the time user. input matrix and then we can modify PCA and remaining this code is fine. I. am asking for weights because user may extracted those weights either with. sampling technique or may be sometime user can give weights of his own. desired e.g. he want to focus one cell type etc. So we should support. weights generally rather specifically. Thanks,. Khalid. On Wed, May 22, 2019 at 5:21 PM khalid usman <khalid0491@gmail.com> wrote:. > Thanks ,. >. > But i will suggest to just support weights instead of coreset, may be user. > want to sample data with some other weighting technique. So we should ask. > them to just put the weights for observations, then we need to modify PCA. > as well and i think my code will support most of plots and marker genes,. > but not PCA, because my input is PCA matrix with weights for each. > observations. >. > Thanks,. > Khalid. >. > On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com>. > wrote:. >. >> Long-term, we should think about the design here: Specifying weights all. >> the time is possible, but not very nice for users. So a few questions come. >> to mind:. >>. >> Should we add scanpy.pp.coreset, which would create a sampling and add. >> adata.obs['coreset_weights'] or simply adata.obs['weights']? >>. >> If we do that or plan to in the future, how should the added weights. >> parameter to all these functions work? >>. >> I think it might default to 'coreset_weights', and the functions would. >> automatically use that .obs column if it exists. Users should also still. >> be able to specify weights manually as in this PR. >>. >> So the type of the parameter would be Union[str, pd.DataFrame,. >> Sequence[Union[float, int]]]. >> ------------------------------. >>. >> All of that doesnâ€™t really affect this P",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1046,testability,observ,observations,1046,"hope you will accept. this pull request now. To support PCA and scanpy for weighted sampling, you. can just set a parameter , observations/samples weights at the time user. input matrix and then we can modify PCA and remaining this code is fine. I. am asking for weights because user may extracted those weights either with. sampling technique or may be sometime user can give weights of his own. desired e.g. he want to focus one cell type etc. So we should support. weights generally rather specifically. Thanks,. Khalid. On Wed, May 22, 2019 at 5:21 PM khalid usman <khalid0491@gmail.com> wrote:. > Thanks ,. >. > But i will suggest to just support weights instead of coreset, may be user. > want to sample data with some other weighting technique. So we should ask. > them to just put the weights for observations, then we need to modify PCA. > as well and i think my code will support most of plots and marker genes,. > but not PCA, because my input is PCA matrix with weights for each. > observations. >. > Thanks,. > Khalid. >. > On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com>. > wrote:. >. >> Long-term, we should think about the design here: Specifying weights all. >> the time is possible, but not very nice for users. So a few questions come. >> to mind:. >>. >> Should we add scanpy.pp.coreset, which would create a sampling and add. >> adata.obs['coreset_weights'] or simply adata.obs['weights']? >>. >> If we do that or plan to in the future, how should the added weights. >> parameter to all these functions work? >>. >> I think it might default to 'coreset_weights', and the functions would. >> automatically use that .obs column if it exists. Users should also still. >> be able to specify weights manually as in this PR. >>. >> So the type of the parameter would be Union[str, pd.DataFrame,. >> Sequence[Union[float, int]]]. >> ------------------------------. >>. >> All of that doesnâ€™t really affect this PR, as we can merge it as it is. >> and include annda",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1456,testability,simpl,simply,1456,"g. he want to focus one cell type etc. So we should support. weights generally rather specifically. Thanks,. Khalid. On Wed, May 22, 2019 at 5:21 PM khalid usman <khalid0491@gmail.com> wrote:. > Thanks ,. >. > But i will suggest to just support weights instead of coreset, may be user. > want to sample data with some other weighting technique. So we should ask. > them to just put the weights for observations, then we need to modify PCA. > as well and i think my code will support most of plots and marker genes,. > but not PCA, because my input is PCA matrix with weights for each. > observations. >. > Thanks,. > Khalid. >. > On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com>. > wrote:. >. >> Long-term, we should think about the design here: Specifying weights all. >> the time is possible, but not very nice for users. So a few questions come. >> to mind:. >>. >> Should we add scanpy.pp.coreset, which would create a sampling and add. >> adata.obs['coreset_weights'] or simply adata.obs['weights']? >>. >> If we do that or plan to in the future, how should the added weights. >> parameter to all these functions work? >>. >> I think it might default to 'coreset_weights', and the functions would. >> automatically use that .obs column if it exists. Users should also still. >> be able to specify weights manually as in this PR. >>. >> So the type of the parameter would be Union[str, pd.DataFrame,. >> Sequence[Union[float, int]]]. >> ------------------------------. >>. >> All of that doesnâ€™t really affect this PR, as we can merge it as it is. >> and include anndata-stored weights later. >>. >> â€”. >> You are receiving this because you were mentioned. >> Reply to this email directly, view it on GitHub. >> <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOC4K5CCAJSUVYSIAFDPWUEC5A5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV6M55Q#issuecomment-494718710>,. >> or mute the thread. >> <https://github",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1509,testability,plan,plan,1509,"support. weights generally rather specifically. Thanks,. Khalid. On Wed, May 22, 2019 at 5:21 PM khalid usman <khalid0491@gmail.com> wrote:. > Thanks ,. >. > But i will suggest to just support weights instead of coreset, may be user. > want to sample data with some other weighting technique. So we should ask. > them to just put the weights for observations, then we need to modify PCA. > as well and i think my code will support most of plots and marker genes,. > but not PCA, because my input is PCA matrix with weights for each. > observations. >. > Thanks,. > Khalid. >. > On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com>. > wrote:. >. >> Long-term, we should think about the design here: Specifying weights all. >> the time is possible, but not very nice for users. So a few questions come. >> to mind:. >>. >> Should we add scanpy.pp.coreset, which would create a sampling and add. >> adata.obs['coreset_weights'] or simply adata.obs['weights']? >>. >> If we do that or plan to in the future, how should the added weights. >> parameter to all these functions work? >>. >> I think it might default to 'coreset_weights', and the functions would. >> automatically use that .obs column if it exists. Users should also still. >> be able to specify weights manually as in this PR. >>. >> So the type of the parameter would be Union[str, pd.DataFrame,. >> Sequence[Union[float, int]]]. >> ------------------------------. >>. >> All of that doesnâ€™t really affect this PR, as we can merge it as it is. >> and include anndata-stored weights later. >>. >> â€”. >> You are receiving this because you were mentioned. >> Reply to this email directly, view it on GitHub. >> <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOC4K5CCAJSUVYSIAFDPWUEC5A5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV6M55Q#issuecomment-494718710>,. >> or mute the thread. >> <https://github.com/notifications/unsubscribe-auth/ABREGODWOGS6RD2J",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1686,testability,automat,automatically,1686,"ally. Thanks,. Khalid. On Wed, May 22, 2019 at 5:21 PM khalid usman <khalid0491@gmail.com> wrote:. > Thanks ,. >. > But i will suggest to just support weights instead of coreset, may be user. > want to sample data with some other weighting technique. So we should ask. > them to just put the weights for observations, then we need to modify PCA. > as well and i think my code will support most of plots and marker genes,. > but not PCA, because my input is PCA matrix with weights for each. > observations. >. > Thanks,. > Khalid. >. > On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com>. > wrote:. >. >> Long-term, we should think about the design here: Specifying weights all. >> the time is possible, but not very nice for users. So a few questions come. >> to mind:. >>. >> Should we add scanpy.pp.coreset, which would create a sampling and add. >> adata.obs['coreset_weights'] or simply adata.obs['weights']? >>. >> If we do that or plan to in the future, how should the added weights. >> parameter to all these functions work? >>. >> I think it might default to 'coreset_weights', and the functions would. >> automatically use that .obs column if it exists. Users should also still. >> be able to specify weights manually as in this PR. >>. >> So the type of the parameter would be Union[str, pd.DataFrame,. >> Sequence[Union[float, int]]]. >> ------------------------------. >>. >> All of that doesnâ€™t really affect this PR, as we can merge it as it is. >> and include anndata-stored weights later. >>. >> â€”. >> You are receiving this because you were mentioned. >> Reply to this email directly, view it on GitHub. >> <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOC4K5CCAJSUVYSIAFDPWUEC5A5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV6M55Q#issuecomment-494718710>,. >> or mute the thread. >> <https://github.com/notifications/unsubscribe-auth/ABREGODWOGS6RD2JQ4LW5LDPWUEC5ANCNFSM4HMZ5G7Q>. >> . >>. >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:100,usability,support,support,100,"Thanks Philipp, I have updated and push the code. I hope you will accept. this pull request now. To support PCA and scanpy for weighted sampling, you. can just set a parameter , observations/samples weights at the time user. input matrix and then we can modify PCA and remaining this code is fine. I. am asking for weights because user may extracted those weights either with. sampling technique or may be sometime user can give weights of his own. desired e.g. he want to focus one cell type etc. So we should support. weights generally rather specifically. Thanks,. Khalid. On Wed, May 22, 2019 at 5:21 PM khalid usman <khalid0491@gmail.com> wrote:. > Thanks ,. >. > But i will suggest to just support weights instead of coreset, may be user. > want to sample data with some other weighting technique. So we should ask. > them to just put the weights for observations, then we need to modify PCA. > as well and i think my code will support most of plots and marker genes,. > but not PCA, because my input is PCA matrix with weights for each. > observations. >. > Thanks,. > Khalid. >. > On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com>. > wrote:. >. >> Long-term, we should think about the design here: Specifying weights all. >> the time is possible, but not very nice for users. So a few questions come. >> to mind:. >>. >> Should we add scanpy.pp.coreset, which would create a sampling and add. >> adata.obs['coreset_weights'] or simply adata.obs['weights']? >>. >> If we do that or plan to in the future, how should the added weights. >> parameter to all these functions work? >>. >> I think it might default to 'coreset_weights', and the functions would. >> automatically use that .obs column if it exists. Users should also still. >> be able to specify weights manually as in this PR. >>. >> So the type of the parameter would be Union[str, pd.DataFrame,. >> Sequence[Union[float, int]]]. >> ------------------------------. >>. >> All of that doesnâ€™t really affect this P",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:219,usability,user,user,219,"Thanks Philipp, I have updated and push the code. I hope you will accept. this pull request now. To support PCA and scanpy for weighted sampling, you. can just set a parameter , observations/samples weights at the time user. input matrix and then we can modify PCA and remaining this code is fine. I. am asking for weights because user may extracted those weights either with. sampling technique or may be sometime user can give weights of his own. desired e.g. he want to focus one cell type etc. So we should support. weights generally rather specifically. Thanks,. Khalid. On Wed, May 22, 2019 at 5:21 PM khalid usman <khalid0491@gmail.com> wrote:. > Thanks ,. >. > But i will suggest to just support weights instead of coreset, may be user. > want to sample data with some other weighting technique. So we should ask. > them to just put the weights for observations, then we need to modify PCA. > as well and i think my code will support most of plots and marker genes,. > but not PCA, because my input is PCA matrix with weights for each. > observations. >. > Thanks,. > Khalid. >. > On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com>. > wrote:. >. >> Long-term, we should think about the design here: Specifying weights all. >> the time is possible, but not very nice for users. So a few questions come. >> to mind:. >>. >> Should we add scanpy.pp.coreset, which would create a sampling and add. >> adata.obs['coreset_weights'] or simply adata.obs['weights']? >>. >> If we do that or plan to in the future, how should the added weights. >> parameter to all these functions work? >>. >> I think it might default to 'coreset_weights', and the functions would. >> automatically use that .obs column if it exists. Users should also still. >> be able to specify weights manually as in this PR. >>. >> So the type of the parameter would be Union[str, pd.DataFrame,. >> Sequence[Union[float, int]]]. >> ------------------------------. >>. >> All of that doesnâ€™t really affect this P",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:225,usability,input,input,225,"Thanks Philipp, I have updated and push the code. I hope you will accept. this pull request now. To support PCA and scanpy for weighted sampling, you. can just set a parameter , observations/samples weights at the time user. input matrix and then we can modify PCA and remaining this code is fine. I. am asking for weights because user may extracted those weights either with. sampling technique or may be sometime user can give weights of his own. desired e.g. he want to focus one cell type etc. So we should support. weights generally rather specifically. Thanks,. Khalid. On Wed, May 22, 2019 at 5:21 PM khalid usman <khalid0491@gmail.com> wrote:. > Thanks ,. >. > But i will suggest to just support weights instead of coreset, may be user. > want to sample data with some other weighting technique. So we should ask. > them to just put the weights for observations, then we need to modify PCA. > as well and i think my code will support most of plots and marker genes,. > but not PCA, because my input is PCA matrix with weights for each. > observations. >. > Thanks,. > Khalid. >. > On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com>. > wrote:. >. >> Long-term, we should think about the design here: Specifying weights all. >> the time is possible, but not very nice for users. So a few questions come. >> to mind:. >>. >> Should we add scanpy.pp.coreset, which would create a sampling and add. >> adata.obs['coreset_weights'] or simply adata.obs['weights']? >>. >> If we do that or plan to in the future, how should the added weights. >> parameter to all these functions work? >>. >> I think it might default to 'coreset_weights', and the functions would. >> automatically use that .obs column if it exists. Users should also still. >> be able to specify weights manually as in this PR. >>. >> So the type of the parameter would be Union[str, pd.DataFrame,. >> Sequence[Union[float, int]]]. >> ------------------------------. >>. >> All of that doesnâ€™t really affect this P",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:331,usability,user,user,331,"Thanks Philipp, I have updated and push the code. I hope you will accept. this pull request now. To support PCA and scanpy for weighted sampling, you. can just set a parameter , observations/samples weights at the time user. input matrix and then we can modify PCA and remaining this code is fine. I. am asking for weights because user may extracted those weights either with. sampling technique or may be sometime user can give weights of his own. desired e.g. he want to focus one cell type etc. So we should support. weights generally rather specifically. Thanks,. Khalid. On Wed, May 22, 2019 at 5:21 PM khalid usman <khalid0491@gmail.com> wrote:. > Thanks ,. >. > But i will suggest to just support weights instead of coreset, may be user. > want to sample data with some other weighting technique. So we should ask. > them to just put the weights for observations, then we need to modify PCA. > as well and i think my code will support most of plots and marker genes,. > but not PCA, because my input is PCA matrix with weights for each. > observations. >. > Thanks,. > Khalid. >. > On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com>. > wrote:. >. >> Long-term, we should think about the design here: Specifying weights all. >> the time is possible, but not very nice for users. So a few questions come. >> to mind:. >>. >> Should we add scanpy.pp.coreset, which would create a sampling and add. >> adata.obs['coreset_weights'] or simply adata.obs['weights']? >>. >> If we do that or plan to in the future, how should the added weights. >> parameter to all these functions work? >>. >> I think it might default to 'coreset_weights', and the functions would. >> automatically use that .obs column if it exists. Users should also still. >> be able to specify weights manually as in this PR. >>. >> So the type of the parameter would be Union[str, pd.DataFrame,. >> Sequence[Union[float, int]]]. >> ------------------------------. >>. >> All of that doesnâ€™t really affect this P",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:415,usability,user,user,415,"Thanks Philipp, I have updated and push the code. I hope you will accept. this pull request now. To support PCA and scanpy for weighted sampling, you. can just set a parameter , observations/samples weights at the time user. input matrix and then we can modify PCA and remaining this code is fine. I. am asking for weights because user may extracted those weights either with. sampling technique or may be sometime user can give weights of his own. desired e.g. he want to focus one cell type etc. So we should support. weights generally rather specifically. Thanks,. Khalid. On Wed, May 22, 2019 at 5:21 PM khalid usman <khalid0491@gmail.com> wrote:. > Thanks ,. >. > But i will suggest to just support weights instead of coreset, may be user. > want to sample data with some other weighting technique. So we should ask. > them to just put the weights for observations, then we need to modify PCA. > as well and i think my code will support most of plots and marker genes,. > but not PCA, because my input is PCA matrix with weights for each. > observations. >. > Thanks,. > Khalid. >. > On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com>. > wrote:. >. >> Long-term, we should think about the design here: Specifying weights all. >> the time is possible, but not very nice for users. So a few questions come. >> to mind:. >>. >> Should we add scanpy.pp.coreset, which would create a sampling and add. >> adata.obs['coreset_weights'] or simply adata.obs['weights']? >>. >> If we do that or plan to in the future, how should the added weights. >> parameter to all these functions work? >>. >> I think it might default to 'coreset_weights', and the functions would. >> automatically use that .obs column if it exists. Users should also still. >> be able to specify weights manually as in this PR. >>. >> So the type of the parameter would be Union[str, pd.DataFrame,. >> Sequence[Union[float, int]]]. >> ------------------------------. >>. >> All of that doesnâ€™t really affect this P",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:511,usability,support,support,511,"Thanks Philipp, I have updated and push the code. I hope you will accept. this pull request now. To support PCA and scanpy for weighted sampling, you. can just set a parameter , observations/samples weights at the time user. input matrix and then we can modify PCA and remaining this code is fine. I. am asking for weights because user may extracted those weights either with. sampling technique or may be sometime user can give weights of his own. desired e.g. he want to focus one cell type etc. So we should support. weights generally rather specifically. Thanks,. Khalid. On Wed, May 22, 2019 at 5:21 PM khalid usman <khalid0491@gmail.com> wrote:. > Thanks ,. >. > But i will suggest to just support weights instead of coreset, may be user. > want to sample data with some other weighting technique. So we should ask. > them to just put the weights for observations, then we need to modify PCA. > as well and i think my code will support most of plots and marker genes,. > but not PCA, because my input is PCA matrix with weights for each. > observations. >. > Thanks,. > Khalid. >. > On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com>. > wrote:. >. >> Long-term, we should think about the design here: Specifying weights all. >> the time is possible, but not very nice for users. So a few questions come. >> to mind:. >>. >> Should we add scanpy.pp.coreset, which would create a sampling and add. >> adata.obs['coreset_weights'] or simply adata.obs['weights']? >>. >> If we do that or plan to in the future, how should the added weights. >> parameter to all these functions work? >>. >> I think it might default to 'coreset_weights', and the functions would. >> automatically use that .obs column if it exists. Users should also still. >> be able to specify weights manually as in this PR. >>. >> So the type of the parameter would be Union[str, pd.DataFrame,. >> Sequence[Union[float, int]]]. >> ------------------------------. >>. >> All of that doesnâ€™t really affect this P",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:696,usability,support,support,696,"Thanks Philipp, I have updated and push the code. I hope you will accept. this pull request now. To support PCA and scanpy for weighted sampling, you. can just set a parameter , observations/samples weights at the time user. input matrix and then we can modify PCA and remaining this code is fine. I. am asking for weights because user may extracted those weights either with. sampling technique or may be sometime user can give weights of his own. desired e.g. he want to focus one cell type etc. So we should support. weights generally rather specifically. Thanks,. Khalid. On Wed, May 22, 2019 at 5:21 PM khalid usman <khalid0491@gmail.com> wrote:. > Thanks ,. >. > But i will suggest to just support weights instead of coreset, may be user. > want to sample data with some other weighting technique. So we should ask. > them to just put the weights for observations, then we need to modify PCA. > as well and i think my code will support most of plots and marker genes,. > but not PCA, because my input is PCA matrix with weights for each. > observations. >. > Thanks,. > Khalid. >. > On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com>. > wrote:. >. >> Long-term, we should think about the design here: Specifying weights all. >> the time is possible, but not very nice for users. So a few questions come. >> to mind:. >>. >> Should we add scanpy.pp.coreset, which would create a sampling and add. >> adata.obs['coreset_weights'] or simply adata.obs['weights']? >>. >> If we do that or plan to in the future, how should the added weights. >> parameter to all these functions work? >>. >> I think it might default to 'coreset_weights', and the functions would. >> automatically use that .obs column if it exists. Users should also still. >> be able to specify weights manually as in this PR. >>. >> So the type of the parameter would be Union[str, pd.DataFrame,. >> Sequence[Union[float, int]]]. >> ------------------------------. >>. >> All of that doesnâ€™t really affect this P",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:739,usability,user,user,739,"Thanks Philipp, I have updated and push the code. I hope you will accept. this pull request now. To support PCA and scanpy for weighted sampling, you. can just set a parameter , observations/samples weights at the time user. input matrix and then we can modify PCA and remaining this code is fine. I. am asking for weights because user may extracted those weights either with. sampling technique or may be sometime user can give weights of his own. desired e.g. he want to focus one cell type etc. So we should support. weights generally rather specifically. Thanks,. Khalid. On Wed, May 22, 2019 at 5:21 PM khalid usman <khalid0491@gmail.com> wrote:. > Thanks ,. >. > But i will suggest to just support weights instead of coreset, may be user. > want to sample data with some other weighting technique. So we should ask. > them to just put the weights for observations, then we need to modify PCA. > as well and i think my code will support most of plots and marker genes,. > but not PCA, because my input is PCA matrix with weights for each. > observations. >. > Thanks,. > Khalid. >. > On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com>. > wrote:. >. >> Long-term, we should think about the design here: Specifying weights all. >> the time is possible, but not very nice for users. So a few questions come. >> to mind:. >>. >> Should we add scanpy.pp.coreset, which would create a sampling and add. >> adata.obs['coreset_weights'] or simply adata.obs['weights']? >>. >> If we do that or plan to in the future, how should the added weights. >> parameter to all these functions work? >>. >> I think it might default to 'coreset_weights', and the functions would. >> automatically use that .obs column if it exists. Users should also still. >> be able to specify weights manually as in this PR. >>. >> So the type of the parameter would be Union[str, pd.DataFrame,. >> Sequence[Union[float, int]]]. >> ------------------------------. >>. >> All of that doesnâ€™t really affect this P",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:934,usability,support,support,934,"Thanks Philipp, I have updated and push the code. I hope you will accept. this pull request now. To support PCA and scanpy for weighted sampling, you. can just set a parameter , observations/samples weights at the time user. input matrix and then we can modify PCA and remaining this code is fine. I. am asking for weights because user may extracted those weights either with. sampling technique or may be sometime user can give weights of his own. desired e.g. he want to focus one cell type etc. So we should support. weights generally rather specifically. Thanks,. Khalid. On Wed, May 22, 2019 at 5:21 PM khalid usman <khalid0491@gmail.com> wrote:. > Thanks ,. >. > But i will suggest to just support weights instead of coreset, may be user. > want to sample data with some other weighting technique. So we should ask. > them to just put the weights for observations, then we need to modify PCA. > as well and i think my code will support most of plots and marker genes,. > but not PCA, because my input is PCA matrix with weights for each. > observations. >. > Thanks,. > Khalid. >. > On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com>. > wrote:. >. >> Long-term, we should think about the design here: Specifying weights all. >> the time is possible, but not very nice for users. So a few questions come. >> to mind:. >>. >> Should we add scanpy.pp.coreset, which would create a sampling and add. >> adata.obs['coreset_weights'] or simply adata.obs['weights']? >>. >> If we do that or plan to in the future, how should the added weights. >> parameter to all these functions work? >>. >> I think it might default to 'coreset_weights', and the functions would. >> automatically use that .obs column if it exists. Users should also still. >> be able to specify weights manually as in this PR. >>. >> So the type of the parameter would be Union[str, pd.DataFrame,. >> Sequence[Union[float, int]]]. >> ------------------------------. >>. >> All of that doesnâ€™t really affect this P",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1001,usability,input,input,1001,"ks Philipp, I have updated and push the code. I hope you will accept. this pull request now. To support PCA and scanpy for weighted sampling, you. can just set a parameter , observations/samples weights at the time user. input matrix and then we can modify PCA and remaining this code is fine. I. am asking for weights because user may extracted those weights either with. sampling technique or may be sometime user can give weights of his own. desired e.g. he want to focus one cell type etc. So we should support. weights generally rather specifically. Thanks,. Khalid. On Wed, May 22, 2019 at 5:21 PM khalid usman <khalid0491@gmail.com> wrote:. > Thanks ,. >. > But i will suggest to just support weights instead of coreset, may be user. > want to sample data with some other weighting technique. So we should ask. > them to just put the weights for observations, then we need to modify PCA. > as well and i think my code will support most of plots and marker genes,. > but not PCA, because my input is PCA matrix with weights for each. > observations. >. > Thanks,. > Khalid. >. > On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com>. > wrote:. >. >> Long-term, we should think about the design here: Specifying weights all. >> the time is possible, but not very nice for users. So a few questions come. >> to mind:. >>. >> Should we add scanpy.pp.coreset, which would create a sampling and add. >> adata.obs['coreset_weights'] or simply adata.obs['weights']? >>. >> If we do that or plan to in the future, how should the added weights. >> parameter to all these functions work? >>. >> I think it might default to 'coreset_weights', and the functions would. >> automatically use that .obs column if it exists. Users should also still. >> be able to specify weights manually as in this PR. >>. >> So the type of the parameter would be Union[str, pd.DataFrame,. >> Sequence[Union[float, int]]]. >> ------------------------------. >>. >> All of that doesnâ€™t really affect this PR, a",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1297,usability,user,users,1297," am asking for weights because user may extracted those weights either with. sampling technique or may be sometime user can give weights of his own. desired e.g. he want to focus one cell type etc. So we should support. weights generally rather specifically. Thanks,. Khalid. On Wed, May 22, 2019 at 5:21 PM khalid usman <khalid0491@gmail.com> wrote:. > Thanks ,. >. > But i will suggest to just support weights instead of coreset, may be user. > want to sample data with some other weighting technique. So we should ask. > them to just put the weights for observations, then we need to modify PCA. > as well and i think my code will support most of plots and marker genes,. > but not PCA, because my input is PCA matrix with weights for each. > observations. >. > Thanks,. > Khalid. >. > On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com>. > wrote:. >. >> Long-term, we should think about the design here: Specifying weights all. >> the time is possible, but not very nice for users. So a few questions come. >> to mind:. >>. >> Should we add scanpy.pp.coreset, which would create a sampling and add. >> adata.obs['coreset_weights'] or simply adata.obs['weights']? >>. >> If we do that or plan to in the future, how should the added weights. >> parameter to all these functions work? >>. >> I think it might default to 'coreset_weights', and the functions would. >> automatically use that .obs column if it exists. Users should also still. >> be able to specify weights manually as in this PR. >>. >> So the type of the parameter would be Union[str, pd.DataFrame,. >> Sequence[Union[float, int]]]. >> ------------------------------. >>. >> All of that doesnâ€™t really affect this PR, as we can merge it as it is. >> and include anndata-stored weights later. >>. >> â€”. >> You are receiving this because you were mentioned. >> Reply to this email directly, view it on GitHub. >> <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOC4K5CCAJSUVYSI",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1456,usability,simpl,simply,1456,"g. he want to focus one cell type etc. So we should support. weights generally rather specifically. Thanks,. Khalid. On Wed, May 22, 2019 at 5:21 PM khalid usman <khalid0491@gmail.com> wrote:. > Thanks ,. >. > But i will suggest to just support weights instead of coreset, may be user. > want to sample data with some other weighting technique. So we should ask. > them to just put the weights for observations, then we need to modify PCA. > as well and i think my code will support most of plots and marker genes,. > but not PCA, because my input is PCA matrix with weights for each. > observations. >. > Thanks,. > Khalid. >. > On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com>. > wrote:. >. >> Long-term, we should think about the design here: Specifying weights all. >> the time is possible, but not very nice for users. So a few questions come. >> to mind:. >>. >> Should we add scanpy.pp.coreset, which would create a sampling and add. >> adata.obs['coreset_weights'] or simply adata.obs['weights']? >>. >> If we do that or plan to in the future, how should the added weights. >> parameter to all these functions work? >>. >> I think it might default to 'coreset_weights', and the functions would. >> automatically use that .obs column if it exists. Users should also still. >> be able to specify weights manually as in this PR. >>. >> So the type of the parameter would be Union[str, pd.DataFrame,. >> Sequence[Union[float, int]]]. >> ------------------------------. >>. >> All of that doesnâ€™t really affect this PR, as we can merge it as it is. >> and include anndata-stored weights later. >>. >> â€”. >> You are receiving this because you were mentioned. >> Reply to this email directly, view it on GitHub. >> <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOC4K5CCAJSUVYSIAFDPWUEC5A5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV6M55Q#issuecomment-494718710>,. >> or mute the thread. >> <https://github",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:1735,usability,User,Users,1735,"ally. Thanks,. Khalid. On Wed, May 22, 2019 at 5:21 PM khalid usman <khalid0491@gmail.com> wrote:. > Thanks ,. >. > But i will suggest to just support weights instead of coreset, may be user. > want to sample data with some other weighting technique. So we should ask. > them to just put the weights for observations, then we need to modify PCA. > as well and i think my code will support most of plots and marker genes,. > but not PCA, because my input is PCA matrix with weights for each. > observations. >. > Thanks,. > Khalid. >. > On Wed, May 22, 2019 at 5:04 PM Philipp A. <notifications@github.com>. > wrote:. >. >> Long-term, we should think about the design here: Specifying weights all. >> the time is possible, but not very nice for users. So a few questions come. >> to mind:. >>. >> Should we add scanpy.pp.coreset, which would create a sampling and add. >> adata.obs['coreset_weights'] or simply adata.obs['weights']? >>. >> If we do that or plan to in the future, how should the added weights. >> parameter to all these functions work? >>. >> I think it might default to 'coreset_weights', and the functions would. >> automatically use that .obs column if it exists. Users should also still. >> be able to specify weights manually as in this PR. >>. >> So the type of the parameter would be Union[str, pd.DataFrame,. >> Sequence[Union[float, int]]]. >> ------------------------------. >>. >> All of that doesnâ€™t really affect this PR, as we can merge it as it is. >> and include anndata-stored weights later. >>. >> â€”. >> You are receiving this because you were mentioned. >> Reply to this email directly, view it on GitHub. >> <https://github.com/theislab/scanpy/pull/644?email_source=notifications&email_token=ABREGOC4K5CCAJSUVYSIAFDPWUEC5A5CNFSM4HMZ5G72YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV6M55Q#issuecomment-494718710>,. >> or mute the thread. >> <https://github.com/notifications/unsubscribe-auth/ABREGODWOGS6RD2JQ4LW5LDPWUEC5ANCNFSM4HMZ5G7Q>. >> . >>. >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:393,deployability,log,log,393,"Yeah, mostly. Thereâ€™s still a bit to do, some of which I did (see the commits). The things left are that. 1. all functions should only accept sequences (lists, ndarray, â€¦) as `weights`, not dataframes. 2. there are multiple blocks that all look like this and can be replaced by a helper function:. ```py. categories, obs_tidy, catego = _prepare_dataframe(. adata, var_names, groupby, use_raw, log, num_categories, layer=layer,. ). if weights is not None:. mean_obs = _compute_gw_Avg_of_dataframe(obs_tidy, weights, catego, groupby). mean_obs = mean_obs.drop('Wt', axis=1). else:. mean_obs = obs_tidy.groupby(level=0).mean(). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:414,modifiability,layer,layer,414,"Yeah, mostly. Thereâ€™s still a bit to do, some of which I did (see the commits). The things left are that. 1. all functions should only accept sequences (lists, ndarray, â€¦) as `weights`, not dataframes. 2. there are multiple blocks that all look like this and can be replaced by a helper function:. ```py. categories, obs_tidy, catego = _prepare_dataframe(. adata, var_names, groupby, use_raw, log, num_categories, layer=layer,. ). if weights is not None:. mean_obs = _compute_gw_Avg_of_dataframe(obs_tidy, weights, catego, groupby). mean_obs = mean_obs.drop('Wt', axis=1). else:. mean_obs = obs_tidy.groupby(level=0).mean(). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:420,modifiability,layer,layer,420,"Yeah, mostly. Thereâ€™s still a bit to do, some of which I did (see the commits). The things left are that. 1. all functions should only accept sequences (lists, ndarray, â€¦) as `weights`, not dataframes. 2. there are multiple blocks that all look like this and can be replaced by a helper function:. ```py. categories, obs_tidy, catego = _prepare_dataframe(. adata, var_names, groupby, use_raw, log, num_categories, layer=layer,. ). if weights is not None:. mean_obs = _compute_gw_Avg_of_dataframe(obs_tidy, weights, catego, groupby). mean_obs = mean_obs.drop('Wt', axis=1). else:. mean_obs = obs_tidy.groupby(level=0).mean(). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:393,safety,log,log,393,"Yeah, mostly. Thereâ€™s still a bit to do, some of which I did (see the commits). The things left are that. 1. all functions should only accept sequences (lists, ndarray, â€¦) as `weights`, not dataframes. 2. there are multiple blocks that all look like this and can be replaced by a helper function:. ```py. categories, obs_tidy, catego = _prepare_dataframe(. adata, var_names, groupby, use_raw, log, num_categories, layer=layer,. ). if weights is not None:. mean_obs = _compute_gw_Avg_of_dataframe(obs_tidy, weights, catego, groupby). mean_obs = mean_obs.drop('Wt', axis=1). else:. mean_obs = obs_tidy.groupby(level=0).mean(). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:393,security,log,log,393,"Yeah, mostly. Thereâ€™s still a bit to do, some of which I did (see the commits). The things left are that. 1. all functions should only accept sequences (lists, ndarray, â€¦) as `weights`, not dataframes. 2. there are multiple blocks that all look like this and can be replaced by a helper function:. ```py. categories, obs_tidy, catego = _prepare_dataframe(. adata, var_names, groupby, use_raw, log, num_categories, layer=layer,. ). if weights is not None:. mean_obs = _compute_gw_Avg_of_dataframe(obs_tidy, weights, catego, groupby). mean_obs = mean_obs.drop('Wt', axis=1). else:. mean_obs = obs_tidy.groupby(level=0).mean(). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:393,testability,log,log,393,"Yeah, mostly. Thereâ€™s still a bit to do, some of which I did (see the commits). The things left are that. 1. all functions should only accept sequences (lists, ndarray, â€¦) as `weights`, not dataframes. 2. there are multiple blocks that all look like this and can be replaced by a helper function:. ```py. categories, obs_tidy, catego = _prepare_dataframe(. adata, var_names, groupby, use_raw, log, num_categories, layer=layer,. ). if weights is not None:. mean_obs = _compute_gw_Avg_of_dataframe(obs_tidy, weights, catego, groupby). mean_obs = mean_obs.drop('Wt', axis=1). else:. mean_obs = obs_tidy.groupby(level=0).mean(). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/pull/644:280,usability,help,helper,280,"Yeah, mostly. Thereâ€™s still a bit to do, some of which I did (see the commits). The things left are that. 1. all functions should only accept sequences (lists, ndarray, â€¦) as `weights`, not dataframes. 2. there are multiple blocks that all look like this and can be replaced by a helper function:. ```py. categories, obs_tidy, catego = _prepare_dataframe(. adata, var_names, groupby, use_raw, log, num_categories, layer=layer,. ). if weights is not None:. mean_obs = _compute_gw_Avg_of_dataframe(obs_tidy, weights, catego, groupby). mean_obs = mean_obs.drop('Wt', axis=1). else:. mean_obs = obs_tidy.groupby(level=0).mean(). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/644
https://github.com/scverse/scanpy/issues/645:34,deployability,updat,updated,34,"Hi,. I think it would help if you updated anndata to 0.6.19. there was a change in 0.6.18 that wrote unordered categoricals, where before categoricals were changed to ordered by default. Hope that helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/645
https://github.com/scverse/scanpy/issues/645:34,safety,updat,updated,34,"Hi,. I think it would help if you updated anndata to 0.6.19. there was a change in 0.6.18 that wrote unordered categoricals, where before categoricals were changed to ordered by default. Hope that helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/645
https://github.com/scverse/scanpy/issues/645:34,security,updat,updated,34,"Hi,. I think it would help if you updated anndata to 0.6.19. there was a change in 0.6.18 that wrote unordered categoricals, where before categoricals were changed to ordered by default. Hope that helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/645
https://github.com/scverse/scanpy/issues/645:22,usability,help,help,22,"Hi,. I think it would help if you updated anndata to 0.6.19. there was a change in 0.6.18 that wrote unordered categoricals, where before categoricals were changed to ordered by default. Hope that helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/645
https://github.com/scverse/scanpy/issues/645:197,usability,help,helps,197,"Hi,. I think it would help if you updated anndata to 0.6.19. there was a change in 0.6.18 that wrote unordered categoricals, where before categoricals were changed to ordered by default. Hope that helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/645
https://github.com/scverse/scanpy/issues/646:18,performance,time,time,18,I do this all the time as well. Could you enlighten me as to the `var_group_*` you are talking about? That sounds like it could be very helpful for me.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/646:136,usability,help,helpful,136,I do this all the time as well. Could you enlighten me as to the `var_group_*` you are talking about? That sounds like it could be very helpful for me.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/646:80,deployability,Stack,Stacked-violins,80,https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html#Stacked-violins. What I'm talking about is actually literature-driven marker equivalent of rank_genes_groups plots.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/646:50,usability,visual,visualizing-marker-genes,50,https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html#Stacked-violins. What I'm talking about is actually literature-driven marker equivalent of rank_genes_groups plots.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/646:146,availability,cluster,clusters,146,Cool... I haven't used the new plots yet. Looks very handy. Your wrapper is an automated use of `var_group_X` to label marker genes of particular clusters in these plots?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/646:79,deployability,automat,automated,79,Cool... I haven't used the new plots yet. Looks very handy. Your wrapper is an automated use of `var_group_X` to label marker genes of particular clusters in these plots?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/646:146,deployability,cluster,clusters,146,Cool... I haven't used the new plots yet. Looks very handy. Your wrapper is an automated use of `var_group_X` to label marker genes of particular clusters in these plots?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/646:0,energy efficiency,Cool,Cool,0,Cool... I haven't used the new plots yet. Looks very handy. Your wrapper is an automated use of `var_group_X` to label marker genes of particular clusters in these plots?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/646:65,integrability,wrap,wrapper,65,Cool... I haven't used the new plots yet. Looks very handy. Your wrapper is an automated use of `var_group_X` to label marker genes of particular clusters in these plots?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/646:65,interoperability,wrapper,wrapper,65,Cool... I haven't used the new plots yet. Looks very handy. Your wrapper is an automated use of `var_group_X` to label marker genes of particular clusters in these plots?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/646:79,testability,automat,automated,79,Cool... I haven't used the new plots yet. Looks very handy. Your wrapper is an automated use of `var_group_X` to label marker genes of particular clusters in these plots?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/646:293,deployability,automat,automatically,293,"I should dig on a function that I have somewhere that translates a dict into the proper input for the plotting functions. This is a use case that I frequently find. . I think that a good solution would be to check if the var names is a dict, in which case the var_labels and var_positions are automatically set. Or do you have any other idea? I donâ€™t know if this will make the documentation too complicated. . At the moment I am travelling, but next week I can look into this. . > On 15 May 2019, at 15:07, GÃ¶kÃ§en Eraslan <notifications@github.com> wrote:. > . > https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html#Stacked-violins. > . > What I'm talking about is actually literature-driven marker equivalent of rank_genes_groups plots. > . > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/646:644,deployability,Stack,Stacked-violins,644,"I should dig on a function that I have somewhere that translates a dict into the proper input for the plotting functions. This is a use case that I frequently find. . I think that a good solution would be to check if the var names is a dict, in which case the var_labels and var_positions are automatically set. Or do you have any other idea? I donâ€™t know if this will make the documentation too complicated. . At the moment I am travelling, but next week I can look into this. . > On 15 May 2019, at 15:07, GÃ¶kÃ§en Eraslan <notifications@github.com> wrote:. > . > https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html#Stacked-violins. > . > What I'm talking about is actually literature-driven marker equivalent of rank_genes_groups plots. > . > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/646:54,integrability,translat,translates,54,"I should dig on a function that I have somewhere that translates a dict into the proper input for the plotting functions. This is a use case that I frequently find. . I think that a good solution would be to check if the var names is a dict, in which case the var_labels and var_positions are automatically set. Or do you have any other idea? I donâ€™t know if this will make the documentation too complicated. . At the moment I am travelling, but next week I can look into this. . > On 15 May 2019, at 15:07, GÃ¶kÃ§en Eraslan <notifications@github.com> wrote:. > . > https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html#Stacked-violins. > . > What I'm talking about is actually literature-driven marker equivalent of rank_genes_groups plots. > . > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/646:54,interoperability,translat,translates,54,"I should dig on a function that I have somewhere that translates a dict into the proper input for the plotting functions. This is a use case that I frequently find. . I think that a good solution would be to check if the var names is a dict, in which case the var_labels and var_positions are automatically set. Or do you have any other idea? I donâ€™t know if this will make the documentation too complicated. . At the moment I am travelling, but next week I can look into this. . > On 15 May 2019, at 15:07, GÃ¶kÃ§en Eraslan <notifications@github.com> wrote:. > . > https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html#Stacked-violins. > . > What I'm talking about is actually literature-driven marker equivalent of rank_genes_groups plots. > . > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/646:88,safety,input,input,88,"I should dig on a function that I have somewhere that translates a dict into the proper input for the plotting functions. This is a use case that I frequently find. . I think that a good solution would be to check if the var names is a dict, in which case the var_labels and var_positions are automatically set. Or do you have any other idea? I donâ€™t know if this will make the documentation too complicated. . At the moment I am travelling, but next week I can look into this. . > On 15 May 2019, at 15:07, GÃ¶kÃ§en Eraslan <notifications@github.com> wrote:. > . > https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html#Stacked-violins. > . > What I'm talking about is actually literature-driven marker equivalent of rank_genes_groups plots. > . > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/646:396,safety,compl,complicated,396,"I should dig on a function that I have somewhere that translates a dict into the proper input for the plotting functions. This is a use case that I frequently find. . I think that a good solution would be to check if the var names is a dict, in which case the var_labels and var_positions are automatically set. Or do you have any other idea? I donâ€™t know if this will make the documentation too complicated. . At the moment I am travelling, but next week I can look into this. . > On 15 May 2019, at 15:07, GÃ¶kÃ§en Eraslan <notifications@github.com> wrote:. > . > https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html#Stacked-violins. > . > What I'm talking about is actually literature-driven marker equivalent of rank_genes_groups plots. > . > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/646:396,security,compl,complicated,396,"I should dig on a function that I have somewhere that translates a dict into the proper input for the plotting functions. This is a use case that I frequently find. . I think that a good solution would be to check if the var names is a dict, in which case the var_labels and var_positions are automatically set. Or do you have any other idea? I donâ€™t know if this will make the documentation too complicated. . At the moment I am travelling, but next week I can look into this. . > On 15 May 2019, at 15:07, GÃ¶kÃ§en Eraslan <notifications@github.com> wrote:. > . > https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html#Stacked-violins. > . > What I'm talking about is actually literature-driven marker equivalent of rank_genes_groups plots. > . > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/646:293,testability,automat,automatically,293,"I should dig on a function that I have somewhere that translates a dict into the proper input for the plotting functions. This is a use case that I frequently find. . I think that a good solution would be to check if the var names is a dict, in which case the var_labels and var_positions are automatically set. Or do you have any other idea? I donâ€™t know if this will make the documentation too complicated. . At the moment I am travelling, but next week I can look into this. . > On 15 May 2019, at 15:07, GÃ¶kÃ§en Eraslan <notifications@github.com> wrote:. > . > https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html#Stacked-violins. > . > What I'm talking about is actually literature-driven marker equivalent of rank_genes_groups plots. > . > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/646:88,usability,input,input,88,"I should dig on a function that I have somewhere that translates a dict into the proper input for the plotting functions. This is a use case that I frequently find. . I think that a good solution would be to check if the var names is a dict, in which case the var_labels and var_positions are automatically set. Or do you have any other idea? I donâ€™t know if this will make the documentation too complicated. . At the moment I am travelling, but next week I can look into this. . > On 15 May 2019, at 15:07, GÃ¶kÃ§en Eraslan <notifications@github.com> wrote:. > . > https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html#Stacked-violins. > . > What I'm talking about is actually literature-driven marker equivalent of rank_genes_groups plots. > . > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/646:378,usability,document,documentation,378,"I should dig on a function that I have somewhere that translates a dict into the proper input for the plotting functions. This is a use case that I frequently find. . I think that a good solution would be to check if the var names is a dict, in which case the var_labels and var_positions are automatically set. Or do you have any other idea? I donâ€™t know if this will make the documentation too complicated. . At the moment I am travelling, but next week I can look into this. . > On 15 May 2019, at 15:07, GÃ¶kÃ§en Eraslan <notifications@github.com> wrote:. > . > https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html#Stacked-violins. > . > What I'm talking about is actually literature-driven marker equivalent of rank_genes_groups plots. > . > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/646:614,usability,visual,visualizing-marker-genes,614,"I should dig on a function that I have somewhere that translates a dict into the proper input for the plotting functions. This is a use case that I frequently find. . I think that a good solution would be to check if the var names is a dict, in which case the var_labels and var_positions are automatically set. Or do you have any other idea? I donâ€™t know if this will make the documentation too complicated. . At the moment I am travelling, but next week I can look into this. . > On 15 May 2019, at 15:07, GÃ¶kÃ§en Eraslan <notifications@github.com> wrote:. > . > https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html#Stacked-violins. > . > What I'm talking about is actually literature-driven marker equivalent of rank_genes_groups plots. > . > â€”. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/646:568,availability,consist,consistently,568,"I know this is a bit specific, but the current sc.pl.rank_genes_groups() and sc.pl.rank_genes_groups_violin() have a 'gene_symbols' argument to allow display based on other columns in .var. For example, if I have this in Var:. ```. Var columns:. gene_symbol. index . ENSMUSG00000000001 Gnai3. ENSMUSG00000000028 Cdc45. ENSMUSG00000000031 H19. ENSMUSG00000000037 Scml2. ENSMUSG00000000049 Apoh. ```. I can just pass 'gene_symbol' as the column I want to be used for any/all displays instead of the ensembl ID all the time. It would be great if something like this were consistently available anywhere gene labels were needed in the plotting functions. Perhaps the common argument would be better called 'display_label' or something like that instead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/646:581,availability,avail,available,581,"I know this is a bit specific, but the current sc.pl.rank_genes_groups() and sc.pl.rank_genes_groups_violin() have a 'gene_symbols' argument to allow display based on other columns in .var. For example, if I have this in Var:. ```. Var columns:. gene_symbol. index . ENSMUSG00000000001 Gnai3. ENSMUSG00000000028 Cdc45. ENSMUSG00000000031 H19. ENSMUSG00000000037 Scml2. ENSMUSG00000000049 Apoh. ```. I can just pass 'gene_symbol' as the column I want to be used for any/all displays instead of the ensembl ID all the time. It would be great if something like this were consistently available anywhere gene labels were needed in the plotting functions. Perhaps the common argument would be better called 'display_label' or something like that instead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/646:39,energy efficiency,current,current,39,"I know this is a bit specific, but the current sc.pl.rank_genes_groups() and sc.pl.rank_genes_groups_violin() have a 'gene_symbols' argument to allow display based on other columns in .var. For example, if I have this in Var:. ```. Var columns:. gene_symbol. index . ENSMUSG00000000001 Gnai3. ENSMUSG00000000028 Cdc45. ENSMUSG00000000031 H19. ENSMUSG00000000037 Scml2. ENSMUSG00000000049 Apoh. ```. I can just pass 'gene_symbol' as the column I want to be used for any/all displays instead of the ensembl ID all the time. It would be great if something like this were consistently available anywhere gene labels were needed in the plotting functions. Perhaps the common argument would be better called 'display_label' or something like that instead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/646:21,interoperability,specif,specific,21,"I know this is a bit specific, but the current sc.pl.rank_genes_groups() and sc.pl.rank_genes_groups_violin() have a 'gene_symbols' argument to allow display based on other columns in .var. For example, if I have this in Var:. ```. Var columns:. gene_symbol. index . ENSMUSG00000000001 Gnai3. ENSMUSG00000000028 Cdc45. ENSMUSG00000000031 H19. ENSMUSG00000000037 Scml2. ENSMUSG00000000049 Apoh. ```. I can just pass 'gene_symbol' as the column I want to be used for any/all displays instead of the ensembl ID all the time. It would be great if something like this were consistently available anywhere gene labels were needed in the plotting functions. Perhaps the common argument would be better called 'display_label' or something like that instead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/646:516,performance,time,time,516,"I know this is a bit specific, but the current sc.pl.rank_genes_groups() and sc.pl.rank_genes_groups_violin() have a 'gene_symbols' argument to allow display based on other columns in .var. For example, if I have this in Var:. ```. Var columns:. gene_symbol. index . ENSMUSG00000000001 Gnai3. ENSMUSG00000000028 Cdc45. ENSMUSG00000000031 H19. ENSMUSG00000000037 Scml2. ENSMUSG00000000049 Apoh. ```. I can just pass 'gene_symbol' as the column I want to be used for any/all displays instead of the ensembl ID all the time. It would be great if something like this were consistently available anywhere gene labels were needed in the plotting functions. Perhaps the common argument would be better called 'display_label' or something like that instead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/646:581,reliability,availab,available,581,"I know this is a bit specific, but the current sc.pl.rank_genes_groups() and sc.pl.rank_genes_groups_violin() have a 'gene_symbols' argument to allow display based on other columns in .var. For example, if I have this in Var:. ```. Var columns:. gene_symbol. index . ENSMUSG00000000001 Gnai3. ENSMUSG00000000028 Cdc45. ENSMUSG00000000031 H19. ENSMUSG00000000037 Scml2. ENSMUSG00000000049 Apoh. ```. I can just pass 'gene_symbol' as the column I want to be used for any/all displays instead of the ensembl ID all the time. It would be great if something like this were consistently available anywhere gene labels were needed in the plotting functions. Perhaps the common argument would be better called 'display_label' or something like that instead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/646:581,safety,avail,available,581,"I know this is a bit specific, but the current sc.pl.rank_genes_groups() and sc.pl.rank_genes_groups_violin() have a 'gene_symbols' argument to allow display based on other columns in .var. For example, if I have this in Var:. ```. Var columns:. gene_symbol. index . ENSMUSG00000000001 Gnai3. ENSMUSG00000000028 Cdc45. ENSMUSG00000000031 H19. ENSMUSG00000000037 Scml2. ENSMUSG00000000049 Apoh. ```. I can just pass 'gene_symbol' as the column I want to be used for any/all displays instead of the ensembl ID all the time. It would be great if something like this were consistently available anywhere gene labels were needed in the plotting functions. Perhaps the common argument would be better called 'display_label' or something like that instead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/646:581,security,availab,available,581,"I know this is a bit specific, but the current sc.pl.rank_genes_groups() and sc.pl.rank_genes_groups_violin() have a 'gene_symbols' argument to allow display based on other columns in .var. For example, if I have this in Var:. ```. Var columns:. gene_symbol. index . ENSMUSG00000000001 Gnai3. ENSMUSG00000000028 Cdc45. ENSMUSG00000000031 H19. ENSMUSG00000000037 Scml2. ENSMUSG00000000049 Apoh. ```. I can just pass 'gene_symbol' as the column I want to be used for any/all displays instead of the ensembl ID all the time. It would be great if something like this were consistently available anywhere gene labels were needed in the plotting functions. Perhaps the common argument would be better called 'display_label' or something like that instead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/646:568,usability,consist,consistently,568,"I know this is a bit specific, but the current sc.pl.rank_genes_groups() and sc.pl.rank_genes_groups_violin() have a 'gene_symbols' argument to allow display based on other columns in .var. For example, if I have this in Var:. ```. Var columns:. gene_symbol. index . ENSMUSG00000000001 Gnai3. ENSMUSG00000000028 Cdc45. ENSMUSG00000000031 H19. ENSMUSG00000000037 Scml2. ENSMUSG00000000049 Apoh. ```. I can just pass 'gene_symbol' as the column I want to be used for any/all displays instead of the ensembl ID all the time. It would be great if something like this were consistently available anywhere gene labels were needed in the plotting functions. Perhaps the common argument would be better called 'display_label' or something like that instead.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/646:37,deployability,updat,updated,37,"Quite possibly, I need to look. Just updated from 1.3.x to 1.4.3 this week.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/646:37,safety,updat,updated,37,"Quite possibly, I need to look. Just updated from 1.3.x to 1.4.3 this week.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/646:37,security,updat,updated,37,"Quite possibly, I need to look. Just updated from 1.3.x to 1.4.3 this week.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/646:148,availability,cluster,clusters,148,"> Cool... I haven't used the new plots yet. Looks very handy. Your wrapper is an automated use of `var_group_X` to label marker genes of particular clusters in these plots? Just to clarify what I meant, current API:. ```python. sc.pl.stacked_violin(pbmc, marker_genes, groupby='bulk_labels', . var_group_positions=[(7, 8)], var_group_labels=['NK']). ```. My suggestion:. ```python. markers = {'NK': ['GNLY', 'NKG7']}. sc.pl.stacked_violin(pbmc, marker_genes, groupby='bulk_labels', var_groups=markers). ```. It's ok to keep var_group_* parameters I think, for backward compatibility.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/646:81,deployability,automat,automated,81,"> Cool... I haven't used the new plots yet. Looks very handy. Your wrapper is an automated use of `var_group_X` to label marker genes of particular clusters in these plots? Just to clarify what I meant, current API:. ```python. sc.pl.stacked_violin(pbmc, marker_genes, groupby='bulk_labels', . var_group_positions=[(7, 8)], var_group_labels=['NK']). ```. My suggestion:. ```python. markers = {'NK': ['GNLY', 'NKG7']}. sc.pl.stacked_violin(pbmc, marker_genes, groupby='bulk_labels', var_groups=markers). ```. It's ok to keep var_group_* parameters I think, for backward compatibility.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/646:148,deployability,cluster,clusters,148,"> Cool... I haven't used the new plots yet. Looks very handy. Your wrapper is an automated use of `var_group_X` to label marker genes of particular clusters in these plots? Just to clarify what I meant, current API:. ```python. sc.pl.stacked_violin(pbmc, marker_genes, groupby='bulk_labels', . var_group_positions=[(7, 8)], var_group_labels=['NK']). ```. My suggestion:. ```python. markers = {'NK': ['GNLY', 'NKG7']}. sc.pl.stacked_violin(pbmc, marker_genes, groupby='bulk_labels', var_groups=markers). ```. It's ok to keep var_group_* parameters I think, for backward compatibility.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/646:211,deployability,API,API,211,"> Cool... I haven't used the new plots yet. Looks very handy. Your wrapper is an automated use of `var_group_X` to label marker genes of particular clusters in these plots? Just to clarify what I meant, current API:. ```python. sc.pl.stacked_violin(pbmc, marker_genes, groupby='bulk_labels', . var_group_positions=[(7, 8)], var_group_labels=['NK']). ```. My suggestion:. ```python. markers = {'NK': ['GNLY', 'NKG7']}. sc.pl.stacked_violin(pbmc, marker_genes, groupby='bulk_labels', var_groups=markers). ```. It's ok to keep var_group_* parameters I think, for backward compatibility.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/646:2,energy efficiency,Cool,Cool,2,"> Cool... I haven't used the new plots yet. Looks very handy. Your wrapper is an automated use of `var_group_X` to label marker genes of particular clusters in these plots? Just to clarify what I meant, current API:. ```python. sc.pl.stacked_violin(pbmc, marker_genes, groupby='bulk_labels', . var_group_positions=[(7, 8)], var_group_labels=['NK']). ```. My suggestion:. ```python. markers = {'NK': ['GNLY', 'NKG7']}. sc.pl.stacked_violin(pbmc, marker_genes, groupby='bulk_labels', var_groups=markers). ```. It's ok to keep var_group_* parameters I think, for backward compatibility.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/646:203,energy efficiency,current,current,203,"> Cool... I haven't used the new plots yet. Looks very handy. Your wrapper is an automated use of `var_group_X` to label marker genes of particular clusters in these plots? Just to clarify what I meant, current API:. ```python. sc.pl.stacked_violin(pbmc, marker_genes, groupby='bulk_labels', . var_group_positions=[(7, 8)], var_group_labels=['NK']). ```. My suggestion:. ```python. markers = {'NK': ['GNLY', 'NKG7']}. sc.pl.stacked_violin(pbmc, marker_genes, groupby='bulk_labels', var_groups=markers). ```. It's ok to keep var_group_* parameters I think, for backward compatibility.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/646:67,integrability,wrap,wrapper,67,"> Cool... I haven't used the new plots yet. Looks very handy. Your wrapper is an automated use of `var_group_X` to label marker genes of particular clusters in these plots? Just to clarify what I meant, current API:. ```python. sc.pl.stacked_violin(pbmc, marker_genes, groupby='bulk_labels', . var_group_positions=[(7, 8)], var_group_labels=['NK']). ```. My suggestion:. ```python. markers = {'NK': ['GNLY', 'NKG7']}. sc.pl.stacked_violin(pbmc, marker_genes, groupby='bulk_labels', var_groups=markers). ```. It's ok to keep var_group_* parameters I think, for backward compatibility.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/646:211,integrability,API,API,211,"> Cool... I haven't used the new plots yet. Looks very handy. Your wrapper is an automated use of `var_group_X` to label marker genes of particular clusters in these plots? Just to clarify what I meant, current API:. ```python. sc.pl.stacked_violin(pbmc, marker_genes, groupby='bulk_labels', . var_group_positions=[(7, 8)], var_group_labels=['NK']). ```. My suggestion:. ```python. markers = {'NK': ['GNLY', 'NKG7']}. sc.pl.stacked_violin(pbmc, marker_genes, groupby='bulk_labels', var_groups=markers). ```. It's ok to keep var_group_* parameters I think, for backward compatibility.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/646:67,interoperability,wrapper,wrapper,67,"> Cool... I haven't used the new plots yet. Looks very handy. Your wrapper is an automated use of `var_group_X` to label marker genes of particular clusters in these plots? Just to clarify what I meant, current API:. ```python. sc.pl.stacked_violin(pbmc, marker_genes, groupby='bulk_labels', . var_group_positions=[(7, 8)], var_group_labels=['NK']). ```. My suggestion:. ```python. markers = {'NK': ['GNLY', 'NKG7']}. sc.pl.stacked_violin(pbmc, marker_genes, groupby='bulk_labels', var_groups=markers). ```. It's ok to keep var_group_* parameters I think, for backward compatibility.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/646:211,interoperability,API,API,211,"> Cool... I haven't used the new plots yet. Looks very handy. Your wrapper is an automated use of `var_group_X` to label marker genes of particular clusters in these plots? Just to clarify what I meant, current API:. ```python. sc.pl.stacked_violin(pbmc, marker_genes, groupby='bulk_labels', . var_group_positions=[(7, 8)], var_group_labels=['NK']). ```. My suggestion:. ```python. markers = {'NK': ['GNLY', 'NKG7']}. sc.pl.stacked_violin(pbmc, marker_genes, groupby='bulk_labels', var_groups=markers). ```. It's ok to keep var_group_* parameters I think, for backward compatibility.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/646:569,interoperability,compatib,compatibility,569,"> Cool... I haven't used the new plots yet. Looks very handy. Your wrapper is an automated use of `var_group_X` to label marker genes of particular clusters in these plots? Just to clarify what I meant, current API:. ```python. sc.pl.stacked_violin(pbmc, marker_genes, groupby='bulk_labels', . var_group_positions=[(7, 8)], var_group_labels=['NK']). ```. My suggestion:. ```python. markers = {'NK': ['GNLY', 'NKG7']}. sc.pl.stacked_violin(pbmc, marker_genes, groupby='bulk_labels', var_groups=markers). ```. It's ok to keep var_group_* parameters I think, for backward compatibility.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/646:536,modifiability,paramet,parameters,536,"> Cool... I haven't used the new plots yet. Looks very handy. Your wrapper is an automated use of `var_group_X` to label marker genes of particular clusters in these plots? Just to clarify what I meant, current API:. ```python. sc.pl.stacked_violin(pbmc, marker_genes, groupby='bulk_labels', . var_group_positions=[(7, 8)], var_group_labels=['NK']). ```. My suggestion:. ```python. markers = {'NK': ['GNLY', 'NKG7']}. sc.pl.stacked_violin(pbmc, marker_genes, groupby='bulk_labels', var_groups=markers). ```. It's ok to keep var_group_* parameters I think, for backward compatibility.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/646:81,testability,automat,automated,81,"> Cool... I haven't used the new plots yet. Looks very handy. Your wrapper is an automated use of `var_group_X` to label marker genes of particular clusters in these plots? Just to clarify what I meant, current API:. ```python. sc.pl.stacked_violin(pbmc, marker_genes, groupby='bulk_labels', . var_group_positions=[(7, 8)], var_group_labels=['NK']). ```. My suggestion:. ```python. markers = {'NK': ['GNLY', 'NKG7']}. sc.pl.stacked_violin(pbmc, marker_genes, groupby='bulk_labels', var_groups=markers). ```. It's ok to keep var_group_* parameters I think, for backward compatibility.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/646
https://github.com/scverse/scanpy/issues/647:93,security,control,control,93,"Hello All,. I am a totally new one in learning single cell RNA_seq. When I was doing quality control, I met this problem. Can anyone help me? Thank you so much",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/647
https://github.com/scverse/scanpy/issues/647:93,testability,control,control,93,"Hello All,. I am a totally new one in learning single cell RNA_seq. When I was doing quality control, I met this problem. Can anyone help me? Thank you so much",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/647
https://github.com/scverse/scanpy/issues/647:38,usability,learn,learning,38,"Hello All,. I am a totally new one in learning single cell RNA_seq. When I was doing quality control, I met this problem. Can anyone help me? Thank you so much",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/647
https://github.com/scverse/scanpy/issues/647:133,usability,help,help,133,"Hello All,. I am a totally new one in learning single cell RNA_seq. When I was doing quality control, I met this problem. Can anyone help me? Thank you so much",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/647
https://github.com/scverse/scanpy/issues/647:129,availability,error,error,129,"Hi,. Is your `mito_genes` vector all boolean? And does it have a non-zero sum? You seem to be getting NA values according to the error I guess.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/647
https://github.com/scverse/scanpy/issues/647:129,performance,error,error,129,"Hi,. Is your `mito_genes` vector all boolean? And does it have a non-zero sum? You seem to be getting NA values according to the error I guess.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/647
https://github.com/scverse/scanpy/issues/647:50,reliability,doe,does,50,"Hi,. Is your `mito_genes` vector all boolean? And does it have a non-zero sum? You seem to be getting NA values according to the error I guess.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/647
https://github.com/scverse/scanpy/issues/647:129,safety,error,error,129,"Hi,. Is your `mito_genes` vector all boolean? And does it have a non-zero sum? You seem to be getting NA values according to the error I guess.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/647
https://github.com/scverse/scanpy/issues/647:129,usability,error,error,129,"Hi,. Is your `mito_genes` vector all boolean? And does it have a non-zero sum? You seem to be getting NA values according to the error I guess.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/647
https://github.com/scverse/scanpy/issues/647:137,availability,error,error,137,"> Hi,. > . > Is your `mito_genes` vector all boolean? And does it have a non-zero sum? You seem to be getting NA values according to the error I guess. Thank you for your reply. I think the 'mito_genes' vector was all boolean. Maybe it has a zero-sum. So, how can I resolve this problem, Do you have any suggestions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/647
https://github.com/scverse/scanpy/issues/647:137,performance,error,error,137,"> Hi,. > . > Is your `mito_genes` vector all boolean? And does it have a non-zero sum? You seem to be getting NA values according to the error I guess. Thank you for your reply. I think the 'mito_genes' vector was all boolean. Maybe it has a zero-sum. So, how can I resolve this problem, Do you have any suggestions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/647
https://github.com/scverse/scanpy/issues/647:58,reliability,doe,does,58,"> Hi,. > . > Is your `mito_genes` vector all boolean? And does it have a non-zero sum? You seem to be getting NA values according to the error I guess. Thank you for your reply. I think the 'mito_genes' vector was all boolean. Maybe it has a zero-sum. So, how can I resolve this problem, Do you have any suggestions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/647
https://github.com/scverse/scanpy/issues/647:137,safety,error,error,137,"> Hi,. > . > Is your `mito_genes` vector all boolean? And does it have a non-zero sum? You seem to be getting NA values according to the error I guess. Thank you for your reply. I think the 'mito_genes' vector was all boolean. Maybe it has a zero-sum. So, how can I resolve this problem, Do you have any suggestions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/647
https://github.com/scverse/scanpy/issues/647:137,usability,error,error,137,"> Hi,. > . > Is your `mito_genes` vector all boolean? And does it have a non-zero sum? You seem to be getting NA values according to the error I guess. Thank you for your reply. I think the 'mito_genes' vector was all boolean. Maybe it has a zero-sum. So, how can I resolve this problem, Do you have any suggestions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/647
https://github.com/scverse/scanpy/issues/647:298,deployability,version,versions,298,"If you have no mitochondrial genes, you can't plot them. The first line of your code looks for genes whose names start with ""MT-"". For mouse data that should be ""mt-"", or maybe you have a different nomenclature... or you don't have any mitochondrial genes in your dataset (possible for Cell ranger versions < 2.0). I can't really debug this, as it requires looking and playing with your dataset. Good luck!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/647
https://github.com/scverse/scanpy/issues/647:298,integrability,version,versions,298,"If you have no mitochondrial genes, you can't plot them. The first line of your code looks for genes whose names start with ""MT-"". For mouse data that should be ""mt-"", or maybe you have a different nomenclature... or you don't have any mitochondrial genes in your dataset (possible for Cell ranger versions < 2.0). I can't really debug this, as it requires looking and playing with your dataset. Good luck!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/647
https://github.com/scverse/scanpy/issues/647:298,modifiability,version,versions,298,"If you have no mitochondrial genes, you can't plot them. The first line of your code looks for genes whose names start with ""MT-"". For mouse data that should be ""mt-"", or maybe you have a different nomenclature... or you don't have any mitochondrial genes in your dataset (possible for Cell ranger versions < 2.0). I can't really debug this, as it requires looking and playing with your dataset. Good luck!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/647
https://github.com/scverse/scanpy/issues/647:135,usability,mous,mouse,135,"If you have no mitochondrial genes, you can't plot them. The first line of your code looks for genes whose names start with ""MT-"". For mouse data that should be ""mt-"", or maybe you have a different nomenclature... or you don't have any mitochondrial genes in your dataset (possible for Cell ranger versions < 2.0). I can't really debug this, as it requires looking and playing with your dataset. Good luck!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/647
https://github.com/scverse/scanpy/issues/647:300,deployability,version,versions,300,"> If you have no mitochondrial genes, you can't plot them. The first line of your code looks for genes whose names start with ""MT-"". For mouse data that should be ""mt-"", or maybe you have a different nomenclature... or you don't have any mitochondrial genes in your dataset (possible for Cell ranger versions < 2.0). > . > I can't really debug this, as it requires looking and playing with your dataset. > . > Good luck! Thank you for your help. My data is Drosophila data. I think maybe there are no mitochondrial genes in the data. Your reply is very helpful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/647
https://github.com/scverse/scanpy/issues/647:300,integrability,version,versions,300,"> If you have no mitochondrial genes, you can't plot them. The first line of your code looks for genes whose names start with ""MT-"". For mouse data that should be ""mt-"", or maybe you have a different nomenclature... or you don't have any mitochondrial genes in your dataset (possible for Cell ranger versions < 2.0). > . > I can't really debug this, as it requires looking and playing with your dataset. > . > Good luck! Thank you for your help. My data is Drosophila data. I think maybe there are no mitochondrial genes in the data. Your reply is very helpful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/647
https://github.com/scverse/scanpy/issues/647:300,modifiability,version,versions,300,"> If you have no mitochondrial genes, you can't plot them. The first line of your code looks for genes whose names start with ""MT-"". For mouse data that should be ""mt-"", or maybe you have a different nomenclature... or you don't have any mitochondrial genes in your dataset (possible for Cell ranger versions < 2.0). > . > I can't really debug this, as it requires looking and playing with your dataset. > . > Good luck! Thank you for your help. My data is Drosophila data. I think maybe there are no mitochondrial genes in the data. Your reply is very helpful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/647
https://github.com/scverse/scanpy/issues/647:137,usability,mous,mouse,137,"> If you have no mitochondrial genes, you can't plot them. The first line of your code looks for genes whose names start with ""MT-"". For mouse data that should be ""mt-"", or maybe you have a different nomenclature... or you don't have any mitochondrial genes in your dataset (possible for Cell ranger versions < 2.0). > . > I can't really debug this, as it requires looking and playing with your dataset. > . > Good luck! Thank you for your help. My data is Drosophila data. I think maybe there are no mitochondrial genes in the data. Your reply is very helpful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/647
https://github.com/scverse/scanpy/issues/647:440,usability,help,help,440,"> If you have no mitochondrial genes, you can't plot them. The first line of your code looks for genes whose names start with ""MT-"". For mouse data that should be ""mt-"", or maybe you have a different nomenclature... or you don't have any mitochondrial genes in your dataset (possible for Cell ranger versions < 2.0). > . > I can't really debug this, as it requires looking and playing with your dataset. > . > Good luck! Thank you for your help. My data is Drosophila data. I think maybe there are no mitochondrial genes in the data. Your reply is very helpful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/647
https://github.com/scverse/scanpy/issues/647:553,usability,help,helpful,553,"> If you have no mitochondrial genes, you can't plot them. The first line of your code looks for genes whose names start with ""MT-"". For mouse data that should be ""mt-"", or maybe you have a different nomenclature... or you don't have any mitochondrial genes in your dataset (possible for Cell ranger versions < 2.0). > . > I can't really debug this, as it requires looking and playing with your dataset. > . > Good luck! Thank you for your help. My data is Drosophila data. I think maybe there are no mitochondrial genes in the data. Your reply is very helpful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/647
https://github.com/scverse/scanpy/pull/648:53,usability,help,help,53,Thank you bot. @theislab/scanpy I think that app can help us eliminate duplicate code and other code smells: https://app.codacy.com/project/theislab/scanpy/dashboard,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/648
https://github.com/scverse/scanpy/pull/648:61,availability,error,errors,61,"I don't like codacy, it's really flaky with its checks (e.g. errors appear and disappear at random). That's why we disabled it in the first place.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/648
https://github.com/scverse/scanpy/pull/648:61,performance,error,errors,61,"I don't like codacy, it's really flaky with its checks (e.g. errors appear and disappear at random). That's why we disabled it in the first place.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/648
https://github.com/scverse/scanpy/pull/648:61,safety,error,errors,61,"I don't like codacy, it's really flaky with its checks (e.g. errors appear and disappear at random). That's why we disabled it in the first place.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/648
https://github.com/scverse/scanpy/pull/648:61,usability,error,errors,61,"I don't like codacy, it's really flaky with its checks (e.g. errors appear and disappear at random). That's why we disabled it in the first place.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/648
https://github.com/scverse/scanpy/pull/648:13,usability,close,close,13,Then I would close this PR,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/648
https://github.com/scverse/scanpy/issues/650:44,integrability,filter,filtering,44,"Hi, @jorvis . There is no functionality for filtering in the backed mode.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650
https://github.com/scverse/scanpy/issues/650:65,energy efficiency,load,loading,65,"Any suggestions around this? Without reading in backed mode just loading the dataset of around 200,000 cells by 30,000 genes is using over 40GB of RAM. The filtering steps help us reduce this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650
https://github.com/scverse/scanpy/issues/650:180,energy efficiency,reduc,reduce,180,"Any suggestions around this? Without reading in backed mode just loading the dataset of around 200,000 cells by 30,000 genes is using over 40GB of RAM. The filtering steps help us reduce this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650
https://github.com/scverse/scanpy/issues/650:156,integrability,filter,filtering,156,"Any suggestions around this? Without reading in backed mode just loading the dataset of around 200,000 cells by 30,000 genes is using over 40GB of RAM. The filtering steps help us reduce this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650
https://github.com/scverse/scanpy/issues/650:65,performance,load,loading,65,"Any suggestions around this? Without reading in backed mode just loading the dataset of around 200,000 cells by 30,000 genes is using over 40GB of RAM. The filtering steps help us reduce this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650
https://github.com/scverse/scanpy/issues/650:172,usability,help,help,172,"Any suggestions around this? Without reading in backed mode just loading the dataset of around 200,000 cells by 30,000 genes is using over 40GB of RAM. The filtering steps help us reduce this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650
https://github.com/scverse/scanpy/issues/650:111,integrability,filter,filtering,111,"Yeah, i understand the problem. There are some some things that can be done in backed mode (like pca), but not filtering for now. Adding annotation based ""filtering"" that will work fine with the backed mode is planned but i can't give any exact timeline.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650
https://github.com/scverse/scanpy/issues/650:155,integrability,filter,filtering,155,"Yeah, i understand the problem. There are some some things that can be done in backed mode (like pca), but not filtering for now. Adding annotation based ""filtering"" that will work fine with the backed mode is planned but i can't give any exact timeline.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650
https://github.com/scverse/scanpy/issues/650:245,performance,time,timeline,245,"Yeah, i understand the problem. There are some some things that can be done in backed mode (like pca), but not filtering for now. Adding annotation based ""filtering"" that will work fine with the backed mode is planned but i can't give any exact timeline.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650
https://github.com/scverse/scanpy/issues/650:8,testability,understand,understand,8,"Yeah, i understand the problem. There are some some things that can be done in backed mode (like pca), but not filtering for now. Adding annotation based ""filtering"" that will work fine with the backed mode is planned but i can't give any exact timeline.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650
https://github.com/scverse/scanpy/issues/650:210,testability,plan,planned,210,"Yeah, i understand the problem. There are some some things that can be done in backed mode (like pca), but not filtering for now. Adding annotation based ""filtering"" that will work fine with the backed mode is planned but i can't give any exact timeline.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650
https://github.com/scverse/scanpy/issues/650:19,energy efficiency,current,currently,19,"@jorvis, while you currently can't filter (since that modifies the AnnData object) you could subset it, generating a view (i.e. `adata[:, adata.var[""n_cells_by_counts""] <= foo]`. Would you mind telling us a little about what you'd like to be able to do?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650
https://github.com/scverse/scanpy/issues/650:35,integrability,filter,filter,35,"@jorvis, while you currently can't filter (since that modifies the AnnData object) you could subset it, generating a view (i.e. `adata[:, adata.var[""n_cells_by_counts""] <= foo]`. Would you mind telling us a little about what you'd like to be able to do?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650
https://github.com/scverse/scanpy/issues/650:93,integrability,sub,subset,93,"@jorvis, while you currently can't filter (since that modifies the AnnData object) you could subset it, generating a view (i.e. `adata[:, adata.var[""n_cells_by_counts""] <= foo]`. Would you mind telling us a little about what you'd like to be able to do?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650
https://github.com/scverse/scanpy/issues/650:54,security,modif,modifies,54,"@jorvis, while you currently can't filter (since that modifies the AnnData object) you could subset it, generating a view (i.e. `adata[:, adata.var[""n_cells_by_counts""] <= foo]`. Would you mind telling us a little about what you'd like to be able to do?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650
https://github.com/scverse/scanpy/issues/650:76,availability,cluster,clustering,76,"OK, that's helpful, thanks. I really just want to get through the [standard clustering pipeline](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html), but in backed mode since we have some datasets which are too expensive to load into RAM (40-50GB). We'll accept the speed trade-off of backed mode to allow for scalability here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650
https://github.com/scverse/scanpy/issues/650:76,deployability,cluster,clustering,76,"OK, that's helpful, thanks. I really just want to get through the [standard clustering pipeline](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html), but in backed mode since we have some datasets which are too expensive to load into RAM (40-50GB). We'll accept the speed trade-off of backed mode to allow for scalability here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650
https://github.com/scverse/scanpy/issues/650:87,deployability,pipelin,pipeline,87,"OK, that's helpful, thanks. I really just want to get through the [standard clustering pipeline](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html), but in backed mode since we have some datasets which are too expensive to load into RAM (40-50GB). We'll accept the speed trade-off of backed mode to allow for scalability here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650
https://github.com/scverse/scanpy/issues/650:235,energy efficiency,load,load,235,"OK, that's helpful, thanks. I really just want to get through the [standard clustering pipeline](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html), but in backed mode since we have some datasets which are too expensive to load into RAM (40-50GB). We'll accept the speed trade-off of backed mode to allow for scalability here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650
https://github.com/scverse/scanpy/issues/650:87,integrability,pipelin,pipeline,87,"OK, that's helpful, thanks. I really just want to get through the [standard clustering pipeline](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html), but in backed mode since we have some datasets which are too expensive to load into RAM (40-50GB). We'll accept the speed trade-off of backed mode to allow for scalability here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650
https://github.com/scverse/scanpy/issues/650:67,interoperability,standard,standard,67,"OK, that's helpful, thanks. I really just want to get through the [standard clustering pipeline](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html), but in backed mode since we have some datasets which are too expensive to load into RAM (40-50GB). We'll accept the speed trade-off of backed mode to allow for scalability here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650
https://github.com/scverse/scanpy/issues/650:321,modifiability,scal,scalability,321,"OK, that's helpful, thanks. I really just want to get through the [standard clustering pipeline](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html), but in backed mode since we have some datasets which are too expensive to load into RAM (40-50GB). We'll accept the speed trade-off of backed mode to allow for scalability here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650
https://github.com/scverse/scanpy/issues/650:235,performance,load,load,235,"OK, that's helpful, thanks. I really just want to get through the [standard clustering pipeline](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html), but in backed mode since we have some datasets which are too expensive to load into RAM (40-50GB). We'll accept the speed trade-off of backed mode to allow for scalability here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650
https://github.com/scverse/scanpy/issues/650:321,performance,scalab,scalability,321,"OK, that's helpful, thanks. I really just want to get through the [standard clustering pipeline](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html), but in backed mode since we have some datasets which are too expensive to load into RAM (40-50GB). We'll accept the speed trade-off of backed mode to allow for scalability here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650
https://github.com/scverse/scanpy/issues/650:11,usability,help,helpful,11,"OK, that's helpful, thanks. I really just want to get through the [standard clustering pipeline](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html), but in backed mode since we have some datasets which are too expensive to load into RAM (40-50GB). We'll accept the speed trade-off of backed mode to allow for scalability here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650
https://github.com/scverse/scanpy/issues/650:57,energy efficiency,load,load,57,"Hmm. If I'm understanding correctly, are you not able to load the full dataset into memory on your machine? Can you give me an idea of what your memory restrictions are?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650
https://github.com/scverse/scanpy/issues/650:57,performance,load,load,57,"Hmm. If I'm understanding correctly, are you not able to load the full dataset into memory on your machine? Can you give me an idea of what your memory restrictions are?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650
https://github.com/scverse/scanpy/issues/650:84,performance,memor,memory,84,"Hmm. If I'm understanding correctly, are you not able to load the full dataset into memory on your machine? Can you give me an idea of what your memory restrictions are?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650
https://github.com/scverse/scanpy/issues/650:145,performance,memor,memory,145,"Hmm. If I'm understanding correctly, are you not able to load the full dataset into memory on your machine? Can you give me an idea of what your memory restrictions are?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650
https://github.com/scverse/scanpy/issues/650:12,testability,understand,understanding,12,"Hmm. If I'm understanding correctly, are you not able to load the full dataset into memory on your machine? Can you give me an idea of what your memory restrictions are?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650
https://github.com/scverse/scanpy/issues/650:84,usability,memor,memory,84,"Hmm. If I'm understanding correctly, are you not able to load the full dataset into memory on your machine? Can you give me an idea of what your memory restrictions are?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650
https://github.com/scverse/scanpy/issues/650:145,usability,memor,memory,145,"Hmm. If I'm understanding correctly, are you not able to load the full dataset into memory on your machine? Can you give me an idea of what your memory restrictions are?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650
https://github.com/scverse/scanpy/issues/650:183,deployability,resourc,resources,183,"We've actually built a site which allows users to browse hundreds of datasets and use functions like those provided by scanpy in an interactive way. So while the server does have the resources to open files like this one, it can't support 10+ users doing it at the same time. It would be prohibitively expensive to run a VM with 1TB of RAM to support the existing user base, so I was trying to autodetect the input file size and if over a certain threshold use backed mode rather than reading into memory. I hope this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650
https://github.com/scverse/scanpy/issues/650:183,energy efficiency,resourc,resources,183,"We've actually built a site which allows users to browse hundreds of datasets and use functions like those provided by scanpy in an interactive way. So while the server does have the resources to open files like this one, it can't support 10+ users doing it at the same time. It would be prohibitively expensive to run a VM with 1TB of RAM to support the existing user base, so I was trying to autodetect the input file size and if over a certain threshold use backed mode rather than reading into memory. I hope this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650
https://github.com/scverse/scanpy/issues/650:183,performance,resourc,resources,183,"We've actually built a site which allows users to browse hundreds of datasets and use functions like those provided by scanpy in an interactive way. So while the server does have the resources to open files like this one, it can't support 10+ users doing it at the same time. It would be prohibitively expensive to run a VM with 1TB of RAM to support the existing user base, so I was trying to autodetect the input file size and if over a certain threshold use backed mode rather than reading into memory. I hope this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650
https://github.com/scverse/scanpy/issues/650:270,performance,time,time,270,"We've actually built a site which allows users to browse hundreds of datasets and use functions like those provided by scanpy in an interactive way. So while the server does have the resources to open files like this one, it can't support 10+ users doing it at the same time. It would be prohibitively expensive to run a VM with 1TB of RAM to support the existing user base, so I was trying to autodetect the input file size and if over a certain threshold use backed mode rather than reading into memory. I hope this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650
https://github.com/scverse/scanpy/issues/650:498,performance,memor,memory,498,"We've actually built a site which allows users to browse hundreds of datasets and use functions like those provided by scanpy in an interactive way. So while the server does have the resources to open files like this one, it can't support 10+ users doing it at the same time. It would be prohibitively expensive to run a VM with 1TB of RAM to support the existing user base, so I was trying to autodetect the input file size and if over a certain threshold use backed mode rather than reading into memory. I hope this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650
https://github.com/scverse/scanpy/issues/650:169,reliability,doe,does,169,"We've actually built a site which allows users to browse hundreds of datasets and use functions like those provided by scanpy in an interactive way. So while the server does have the resources to open files like this one, it can't support 10+ users doing it at the same time. It would be prohibitively expensive to run a VM with 1TB of RAM to support the existing user base, so I was trying to autodetect the input file size and if over a certain threshold use backed mode rather than reading into memory. I hope this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650
https://github.com/scverse/scanpy/issues/650:183,safety,resourc,resources,183,"We've actually built a site which allows users to browse hundreds of datasets and use functions like those provided by scanpy in an interactive way. So while the server does have the resources to open files like this one, it can't support 10+ users doing it at the same time. It would be prohibitively expensive to run a VM with 1TB of RAM to support the existing user base, so I was trying to autodetect the input file size and if over a certain threshold use backed mode rather than reading into memory. I hope this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650
https://github.com/scverse/scanpy/issues/650:409,safety,input,input,409,"We've actually built a site which allows users to browse hundreds of datasets and use functions like those provided by scanpy in an interactive way. So while the server does have the resources to open files like this one, it can't support 10+ users doing it at the same time. It would be prohibitively expensive to run a VM with 1TB of RAM to support the existing user base, so I was trying to autodetect the input file size and if over a certain threshold use backed mode rather than reading into memory. I hope this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650
https://github.com/scverse/scanpy/issues/650:183,testability,resourc,resources,183,"We've actually built a site which allows users to browse hundreds of datasets and use functions like those provided by scanpy in an interactive way. So while the server does have the resources to open files like this one, it can't support 10+ users doing it at the same time. It would be prohibitively expensive to run a VM with 1TB of RAM to support the existing user base, so I was trying to autodetect the input file size and if over a certain threshold use backed mode rather than reading into memory. I hope this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650
https://github.com/scverse/scanpy/issues/650:41,usability,user,users,41,"We've actually built a site which allows users to browse hundreds of datasets and use functions like those provided by scanpy in an interactive way. So while the server does have the resources to open files like this one, it can't support 10+ users doing it at the same time. It would be prohibitively expensive to run a VM with 1TB of RAM to support the existing user base, so I was trying to autodetect the input file size and if over a certain threshold use backed mode rather than reading into memory. I hope this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650
https://github.com/scverse/scanpy/issues/650:132,usability,interact,interactive,132,"We've actually built a site which allows users to browse hundreds of datasets and use functions like those provided by scanpy in an interactive way. So while the server does have the resources to open files like this one, it can't support 10+ users doing it at the same time. It would be prohibitively expensive to run a VM with 1TB of RAM to support the existing user base, so I was trying to autodetect the input file size and if over a certain threshold use backed mode rather than reading into memory. I hope this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650
https://github.com/scverse/scanpy/issues/650:231,usability,support,support,231,"We've actually built a site which allows users to browse hundreds of datasets and use functions like those provided by scanpy in an interactive way. So while the server does have the resources to open files like this one, it can't support 10+ users doing it at the same time. It would be prohibitively expensive to run a VM with 1TB of RAM to support the existing user base, so I was trying to autodetect the input file size and if over a certain threshold use backed mode rather than reading into memory. I hope this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650
https://github.com/scverse/scanpy/issues/650:243,usability,user,users,243,"We've actually built a site which allows users to browse hundreds of datasets and use functions like those provided by scanpy in an interactive way. So while the server does have the resources to open files like this one, it can't support 10+ users doing it at the same time. It would be prohibitively expensive to run a VM with 1TB of RAM to support the existing user base, so I was trying to autodetect the input file size and if over a certain threshold use backed mode rather than reading into memory. I hope this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650
https://github.com/scverse/scanpy/issues/650:343,usability,support,support,343,"We've actually built a site which allows users to browse hundreds of datasets and use functions like those provided by scanpy in an interactive way. So while the server does have the resources to open files like this one, it can't support 10+ users doing it at the same time. It would be prohibitively expensive to run a VM with 1TB of RAM to support the existing user base, so I was trying to autodetect the input file size and if over a certain threshold use backed mode rather than reading into memory. I hope this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650
https://github.com/scverse/scanpy/issues/650:364,usability,user,user,364,"We've actually built a site which allows users to browse hundreds of datasets and use functions like those provided by scanpy in an interactive way. So while the server does have the resources to open files like this one, it can't support 10+ users doing it at the same time. It would be prohibitively expensive to run a VM with 1TB of RAM to support the existing user base, so I was trying to autodetect the input file size and if over a certain threshold use backed mode rather than reading into memory. I hope this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650
https://github.com/scverse/scanpy/issues/650:409,usability,input,input,409,"We've actually built a site which allows users to browse hundreds of datasets and use functions like those provided by scanpy in an interactive way. So while the server does have the resources to open files like this one, it can't support 10+ users doing it at the same time. It would be prohibitively expensive to run a VM with 1TB of RAM to support the existing user base, so I was trying to autodetect the input file size and if over a certain threshold use backed mode rather than reading into memory. I hope this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650
https://github.com/scverse/scanpy/issues/650:498,usability,memor,memory,498,"We've actually built a site which allows users to browse hundreds of datasets and use functions like those provided by scanpy in an interactive way. So while the server does have the resources to open files like this one, it can't support 10+ users doing it at the same time. It would be prohibitively expensive to run a VM with 1TB of RAM to support the existing user base, so I was trying to autodetect the input file size and if over a certain threshold use backed mode rather than reading into memory. I hope this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650
https://github.com/scverse/scanpy/issues/650:518,usability,help,helps,518,"We've actually built a site which allows users to browse hundreds of datasets and use functions like those provided by scanpy in an interactive way. So while the server does have the resources to open files like this one, it can't support 10+ users doing it at the same time. It would be prohibitively expensive to run a VM with 1TB of RAM to support the existing user base, so I was trying to autodetect the input file size and if over a certain threshold use backed mode rather than reading into memory. I hope this helps.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/650
https://github.com/scverse/scanpy/pull/651:143,usability,close,close,143,"> I think `map_labels` should be named `map_labels_knn`. Why change the name from `knn_classify`? I think it fits, especially since this is so close to sklearn's `KNeighborClassifier`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:151,usability,close,close,151,"> > I think `map_labels` should be named `map_labels_knn`. > . > Why change the name from `knn_classify`? I think it fits, especially since this is so close to sklearn's `KNeighborClassifier`? @ivirshup this was the decision of @falexwolf . Maybe something like `map_labels(..., method='knn')` which allows adding other methods when they are implemented?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:34,deployability,api,api,34,@falexwolf @ivirshup . New Ingest api usage. https://github.com/Koncopd/anndata-scanpy-benchmarks/blob/master/Ingest.ipynb,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:34,integrability,api,api,34,@falexwolf @ivirshup . New Ingest api usage. https://github.com/Koncopd/anndata-scanpy-benchmarks/blob/master/Ingest.ipynb,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:34,interoperability,api,api,34,@falexwolf @ivirshup . New Ingest api usage. https://github.com/Koncopd/anndata-scanpy-benchmarks/blob/master/Ingest.ipynb,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:18,energy efficiency,cool,cool,18,This looks really cool! So when you do `ing.to_adata()` you only get back the mapped datapoints and not the whole dataset? It would be interesting to see a plot of both on top of one another to appreciate the label mapping.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:82,deployability,depend,depending,82,"@LuckyMD `ingest.to_adata` just returns a copy of `adata_new` (or the same object depending on inplace) with all mapped representations (pca, umap) and labels.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:82,integrability,depend,depending,82,"@LuckyMD `ingest.to_adata` just returns a copy of `adata_new` (or the same object depending on inplace) with all mapped representations (pca, umap) and labels.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:82,modifiability,depend,depending,82,"@LuckyMD `ingest.to_adata` just returns a copy of `adata_new` (or the same object depending on inplace) with all mapped representations (pca, umap) and labels.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:82,safety,depend,depending,82,"@LuckyMD `ingest.to_adata` just returns a copy of `adata_new` (or the same object depending on inplace) with all mapped representations (pca, umap) and labels.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:82,testability,depend,depending,82,"@LuckyMD `ingest.to_adata` just returns a copy of `adata_new` (or the same object depending on inplace) with all mapped representations (pca, umap) and labels.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:0,deployability,Updat,Updated,0,Updated the notebook above with `to_adata_joint` example - this function returns the concatenation of `adata_ref` and `adata_new` with projected representations and mapped labels.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:0,safety,Updat,Updated,0,Updated the notebook above with `to_adata_joint` example - this function returns the concatenation of `adata_ref` and `adata_new` with projected representations and mapped labels.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:0,security,Updat,Updated,0,Updated the notebook above with `to_adata_joint` example - this function returns the concatenation of `adata_ref` and `adata_new` with projected representations and mapped labels.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:9,deployability,fail,failing,9,Test are failing because of the new version of dask.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:36,deployability,version,version,36,Test are failing because of the new version of dask.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:36,integrability,version,version,36,Test are failing because of the new version of dask.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:36,modifiability,version,version,36,Test are failing because of the new version of dask.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:9,reliability,fail,failing,9,Test are failing because of the new version of dask.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:0,safety,Test,Test,0,Test are failing because of the new version of dask.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:0,testability,Test,Test,0,Test are failing because of the new version of dask.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:27,safety,compl,complete,27,I think it is more or less complete. Here are the tutorials. https://github.com/Koncopd/anndata-scanpy-benchmarks/blob/master/Ingest-realistic.ipynb. https://github.com/Koncopd/anndata-scanpy-benchmarks/blob/master/Ingest-simple.ipynb,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:27,security,compl,complete,27,I think it is more or less complete. Here are the tutorials. https://github.com/Koncopd/anndata-scanpy-benchmarks/blob/master/Ingest-realistic.ipynb. https://github.com/Koncopd/anndata-scanpy-benchmarks/blob/master/Ingest-simple.ipynb,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:222,testability,simpl,simple,222,I think it is more or less complete. Here are the tutorials. https://github.com/Koncopd/anndata-scanpy-benchmarks/blob/master/Ingest-realistic.ipynb. https://github.com/Koncopd/anndata-scanpy-benchmarks/blob/master/Ingest-simple.ipynb,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:222,usability,simpl,simple,222,I think it is more or less complete. Here are the tutorials. https://github.com/Koncopd/anndata-scanpy-benchmarks/blob/master/Ingest-realistic.ipynb. https://github.com/Koncopd/anndata-scanpy-benchmarks/blob/master/Ingest-simple.ipynb,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:1607,deployability,modul,module,1607,".columns = unique_names. return new_annot. def process(dset):. dset.layers[""counts""] = dset.X.copy(). sc.pp.normalize_total(dset). sc.pp.log1p(dset). sc.pp.highly_variable_genes(dset). sc.pp.pca(dset). sc.pp.neighbors(dset, n_neighbors=30). sc.tl.umap(dset). dset1 = sc.datasets.ebi_expression_atlas(""E-GEOD-81608"", filter_boring=True) . dset2 = sc.datasets.ebi_expression_atlas(""E-GEOD-83139"", filter_boring=True). # dset3 = sc.datasets.ebi_expression_atlas(""E-ENAD-27"", filter_boring=True). # dsets = [dset1, dset2, dset3]. dsets = [dset1, dset2]. for dset in dsets:. dset.obs = simplify_annot(dset.obs). sc.pp.calculate_qc_metrics(dset, inplace=True). shared_genes = reduce(np.intersect1d, [dset.var_names for dset in dsets]). dsets = [dset[:, shared_genes].copy() for dset in dsets]. for dset in dsets:. process(dset). # dset1, dset2, dset3 = dsets. dset1, dset2 = dsets. dset1.obs[""inferred cell type (dset1)""] = dset1.obs[""inferred cell type""]. dset12 = ingest(dset2, dset1, obs=""inferred cell type (dset1)"", return_joint=True). ```. Traceback:. ```python. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/isaac/github/scanpy/scanpy/tools/_ingest.py"", line 33, in ingest. return ing.to_adata(inplace) if not return_joint else ing.to_adata_joint(). File ""/Users/isaac/github/scanpy/scanpy/tools/_ingest.py"", line 222, in to_adata_joint. adata = AnnData(np.vstack((self._adata_ref.X, self._adata_new.X))). File ""/Users/isaac/github/anndata/anndata/core/anndata.py"", line 566, in __init__. filename=filename, filemode=filemode). File ""/Users/isaac/github/anndata/anndata/core/anndata.py"", line 735, in _init_as_actual. X = X.astype(dtype, copy=False). ValueError: setting an array element with a sequence. ```. </details>. I've reproduced on 2bffd6a1400 and after merging master with that. This occurs whether inplace is true or not. It also occurs on anndata master as well as the current release. As you'd expect, it doesn't happen with `return_joint=False`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:2410,deployability,releas,release,2410,".columns = unique_names. return new_annot. def process(dset):. dset.layers[""counts""] = dset.X.copy(). sc.pp.normalize_total(dset). sc.pp.log1p(dset). sc.pp.highly_variable_genes(dset). sc.pp.pca(dset). sc.pp.neighbors(dset, n_neighbors=30). sc.tl.umap(dset). dset1 = sc.datasets.ebi_expression_atlas(""E-GEOD-81608"", filter_boring=True) . dset2 = sc.datasets.ebi_expression_atlas(""E-GEOD-83139"", filter_boring=True). # dset3 = sc.datasets.ebi_expression_atlas(""E-ENAD-27"", filter_boring=True). # dsets = [dset1, dset2, dset3]. dsets = [dset1, dset2]. for dset in dsets:. dset.obs = simplify_annot(dset.obs). sc.pp.calculate_qc_metrics(dset, inplace=True). shared_genes = reduce(np.intersect1d, [dset.var_names for dset in dsets]). dsets = [dset[:, shared_genes].copy() for dset in dsets]. for dset in dsets:. process(dset). # dset1, dset2, dset3 = dsets. dset1, dset2 = dsets. dset1.obs[""inferred cell type (dset1)""] = dset1.obs[""inferred cell type""]. dset12 = ingest(dset2, dset1, obs=""inferred cell type (dset1)"", return_joint=True). ```. Traceback:. ```python. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/isaac/github/scanpy/scanpy/tools/_ingest.py"", line 33, in ingest. return ing.to_adata(inplace) if not return_joint else ing.to_adata_joint(). File ""/Users/isaac/github/scanpy/scanpy/tools/_ingest.py"", line 222, in to_adata_joint. adata = AnnData(np.vstack((self._adata_ref.X, self._adata_new.X))). File ""/Users/isaac/github/anndata/anndata/core/anndata.py"", line 566, in __init__. filename=filename, filemode=filemode). File ""/Users/isaac/github/anndata/anndata/core/anndata.py"", line 735, in _init_as_actual. X = X.astype(dtype, copy=False). ValueError: setting an array element with a sequence. ```. </details>. I've reproduced on 2bffd6a1400 and after merging master with that. This occurs whether inplace is true or not. It also occurs on anndata master as well as the current release. As you'd expect, it doesn't happen with `return_joint=False`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:272,energy efficiency,reduc,reduce,272,"Just starting to try this out, but I hit a bug. Here's a script to reproduce and the traceback:. <details>. <summary>Script </summary>. ```python. import scanpy as sc. from scanpy.tools._ingest import ingest. import numpy as np. import pandas as pd. from functools import reduce. def simplify_annot(annot):. names = annot.columns.str.extract(r""\[(.*)\].*$"", expand=False). unique_names, idxs = np.unique(names, return_index=True). new_annot = annot.iloc[:, idxs].copy(). new_annot.columns = unique_names. return new_annot. def process(dset):. dset.layers[""counts""] = dset.X.copy(). sc.pp.normalize_total(dset). sc.pp.log1p(dset). sc.pp.highly_variable_genes(dset). sc.pp.pca(dset). sc.pp.neighbors(dset, n_neighbors=30). sc.tl.umap(dset). dset1 = sc.datasets.ebi_expression_atlas(""E-GEOD-81608"", filter_boring=True) . dset2 = sc.datasets.ebi_expression_atlas(""E-GEOD-83139"", filter_boring=True). # dset3 = sc.datasets.ebi_expression_atlas(""E-ENAD-27"", filter_boring=True). # dsets = [dset1, dset2, dset3]. dsets = [dset1, dset2]. for dset in dsets:. dset.obs = simplify_annot(dset.obs). sc.pp.calculate_qc_metrics(dset, inplace=True). shared_genes = reduce(np.intersect1d, [dset.var_names for dset in dsets]). dsets = [dset[:, shared_genes].copy() for dset in dsets]. for dset in dsets:. process(dset). # dset1, dset2, dset3 = dsets. dset1, dset2 = dsets. dset1.obs[""inferred cell type (dset1)""] = dset1.obs[""inferred cell type""]. dset12 = ingest(dset2, dset1, obs=""inferred cell type (dset1)"", return_joint=True). ```. Traceback:. ```python. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/isaac/github/scanpy/scanpy/tools/_ingest.py"", line 33, in ingest. return ing.to_adata(inplace) if not return_joint else ing.to_adata_joint(). File ""/Users/isaac/github/scanpy/scanpy/tools/_ingest.py"", line 222, in to_adata_joint. adata = AnnData(np.vstack((self._adata_ref.X, self._adata_new.X))). File ""/Users/isaac/github/anndata/anndata/core/anndata.py"", line 566, in ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:1150,energy efficiency,reduc,reduce,1150," scanpy as sc. from scanpy.tools._ingest import ingest. import numpy as np. import pandas as pd. from functools import reduce. def simplify_annot(annot):. names = annot.columns.str.extract(r""\[(.*)\].*$"", expand=False). unique_names, idxs = np.unique(names, return_index=True). new_annot = annot.iloc[:, idxs].copy(). new_annot.columns = unique_names. return new_annot. def process(dset):. dset.layers[""counts""] = dset.X.copy(). sc.pp.normalize_total(dset). sc.pp.log1p(dset). sc.pp.highly_variable_genes(dset). sc.pp.pca(dset). sc.pp.neighbors(dset, n_neighbors=30). sc.tl.umap(dset). dset1 = sc.datasets.ebi_expression_atlas(""E-GEOD-81608"", filter_boring=True) . dset2 = sc.datasets.ebi_expression_atlas(""E-GEOD-83139"", filter_boring=True). # dset3 = sc.datasets.ebi_expression_atlas(""E-ENAD-27"", filter_boring=True). # dsets = [dset1, dset2, dset3]. dsets = [dset1, dset2]. for dset in dsets:. dset.obs = simplify_annot(dset.obs). sc.pp.calculate_qc_metrics(dset, inplace=True). shared_genes = reduce(np.intersect1d, [dset.var_names for dset in dsets]). dsets = [dset[:, shared_genes].copy() for dset in dsets]. for dset in dsets:. process(dset). # dset1, dset2, dset3 = dsets. dset1, dset2 = dsets. dset1.obs[""inferred cell type (dset1)""] = dset1.obs[""inferred cell type""]. dset12 = ingest(dset2, dset1, obs=""inferred cell type (dset1)"", return_joint=True). ```. Traceback:. ```python. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/isaac/github/scanpy/scanpy/tools/_ingest.py"", line 33, in ingest. return ing.to_adata(inplace) if not return_joint else ing.to_adata_joint(). File ""/Users/isaac/github/scanpy/scanpy/tools/_ingest.py"", line 222, in to_adata_joint. adata = AnnData(np.vstack((self._adata_ref.X, self._adata_new.X))). File ""/Users/isaac/github/anndata/anndata/core/anndata.py"", line 566, in __init__. filename=filename, filemode=filemode). File ""/Users/isaac/github/anndata/anndata/core/anndata.py"", line 735, in _init_as_actual. X = X.astype(d",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:1969,energy efficiency,core,core,1969,".columns = unique_names. return new_annot. def process(dset):. dset.layers[""counts""] = dset.X.copy(). sc.pp.normalize_total(dset). sc.pp.log1p(dset). sc.pp.highly_variable_genes(dset). sc.pp.pca(dset). sc.pp.neighbors(dset, n_neighbors=30). sc.tl.umap(dset). dset1 = sc.datasets.ebi_expression_atlas(""E-GEOD-81608"", filter_boring=True) . dset2 = sc.datasets.ebi_expression_atlas(""E-GEOD-83139"", filter_boring=True). # dset3 = sc.datasets.ebi_expression_atlas(""E-ENAD-27"", filter_boring=True). # dsets = [dset1, dset2, dset3]. dsets = [dset1, dset2]. for dset in dsets:. dset.obs = simplify_annot(dset.obs). sc.pp.calculate_qc_metrics(dset, inplace=True). shared_genes = reduce(np.intersect1d, [dset.var_names for dset in dsets]). dsets = [dset[:, shared_genes].copy() for dset in dsets]. for dset in dsets:. process(dset). # dset1, dset2, dset3 = dsets. dset1, dset2 = dsets. dset1.obs[""inferred cell type (dset1)""] = dset1.obs[""inferred cell type""]. dset12 = ingest(dset2, dset1, obs=""inferred cell type (dset1)"", return_joint=True). ```. Traceback:. ```python. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/isaac/github/scanpy/scanpy/tools/_ingest.py"", line 33, in ingest. return ing.to_adata(inplace) if not return_joint else ing.to_adata_joint(). File ""/Users/isaac/github/scanpy/scanpy/tools/_ingest.py"", line 222, in to_adata_joint. adata = AnnData(np.vstack((self._adata_ref.X, self._adata_new.X))). File ""/Users/isaac/github/anndata/anndata/core/anndata.py"", line 566, in __init__. filename=filename, filemode=filemode). File ""/Users/isaac/github/anndata/anndata/core/anndata.py"", line 735, in _init_as_actual. X = X.astype(dtype, copy=False). ValueError: setting an array element with a sequence. ```. </details>. I've reproduced on 2bffd6a1400 and after merging master with that. This occurs whether inplace is true or not. It also occurs on anndata master as well as the current release. As you'd expect, it doesn't happen with `return_joint=False`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:2091,energy efficiency,core,core,2091,".columns = unique_names. return new_annot. def process(dset):. dset.layers[""counts""] = dset.X.copy(). sc.pp.normalize_total(dset). sc.pp.log1p(dset). sc.pp.highly_variable_genes(dset). sc.pp.pca(dset). sc.pp.neighbors(dset, n_neighbors=30). sc.tl.umap(dset). dset1 = sc.datasets.ebi_expression_atlas(""E-GEOD-81608"", filter_boring=True) . dset2 = sc.datasets.ebi_expression_atlas(""E-GEOD-83139"", filter_boring=True). # dset3 = sc.datasets.ebi_expression_atlas(""E-ENAD-27"", filter_boring=True). # dsets = [dset1, dset2, dset3]. dsets = [dset1, dset2]. for dset in dsets:. dset.obs = simplify_annot(dset.obs). sc.pp.calculate_qc_metrics(dset, inplace=True). shared_genes = reduce(np.intersect1d, [dset.var_names for dset in dsets]). dsets = [dset[:, shared_genes].copy() for dset in dsets]. for dset in dsets:. process(dset). # dset1, dset2, dset3 = dsets. dset1, dset2 = dsets. dset1.obs[""inferred cell type (dset1)""] = dset1.obs[""inferred cell type""]. dset12 = ingest(dset2, dset1, obs=""inferred cell type (dset1)"", return_joint=True). ```. Traceback:. ```python. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/isaac/github/scanpy/scanpy/tools/_ingest.py"", line 33, in ingest. return ing.to_adata(inplace) if not return_joint else ing.to_adata_joint(). File ""/Users/isaac/github/scanpy/scanpy/tools/_ingest.py"", line 222, in to_adata_joint. adata = AnnData(np.vstack((self._adata_ref.X, self._adata_new.X))). File ""/Users/isaac/github/anndata/anndata/core/anndata.py"", line 566, in __init__. filename=filename, filemode=filemode). File ""/Users/isaac/github/anndata/anndata/core/anndata.py"", line 735, in _init_as_actual. X = X.astype(dtype, copy=False). ValueError: setting an array element with a sequence. ```. </details>. I've reproduced on 2bffd6a1400 and after merging master with that. This occurs whether inplace is true or not. It also occurs on anndata master as well as the current release. As you'd expect, it doesn't happen with `return_joint=False`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:2402,energy efficiency,current,current,2402,".columns = unique_names. return new_annot. def process(dset):. dset.layers[""counts""] = dset.X.copy(). sc.pp.normalize_total(dset). sc.pp.log1p(dset). sc.pp.highly_variable_genes(dset). sc.pp.pca(dset). sc.pp.neighbors(dset, n_neighbors=30). sc.tl.umap(dset). dset1 = sc.datasets.ebi_expression_atlas(""E-GEOD-81608"", filter_boring=True) . dset2 = sc.datasets.ebi_expression_atlas(""E-GEOD-83139"", filter_boring=True). # dset3 = sc.datasets.ebi_expression_atlas(""E-ENAD-27"", filter_boring=True). # dsets = [dset1, dset2, dset3]. dsets = [dset1, dset2]. for dset in dsets:. dset.obs = simplify_annot(dset.obs). sc.pp.calculate_qc_metrics(dset, inplace=True). shared_genes = reduce(np.intersect1d, [dset.var_names for dset in dsets]). dsets = [dset[:, shared_genes].copy() for dset in dsets]. for dset in dsets:. process(dset). # dset1, dset2, dset3 = dsets. dset1, dset2 = dsets. dset1.obs[""inferred cell type (dset1)""] = dset1.obs[""inferred cell type""]. dset12 = ingest(dset2, dset1, obs=""inferred cell type (dset1)"", return_joint=True). ```. Traceback:. ```python. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/isaac/github/scanpy/scanpy/tools/_ingest.py"", line 33, in ingest. return ing.to_adata(inplace) if not return_joint else ing.to_adata_joint(). File ""/Users/isaac/github/scanpy/scanpy/tools/_ingest.py"", line 222, in to_adata_joint. adata = AnnData(np.vstack((self._adata_ref.X, self._adata_new.X))). File ""/Users/isaac/github/anndata/anndata/core/anndata.py"", line 566, in __init__. filename=filename, filemode=filemode). File ""/Users/isaac/github/anndata/anndata/core/anndata.py"", line 735, in _init_as_actual. X = X.astype(dtype, copy=False). ValueError: setting an array element with a sequence. ```. </details>. I've reproduced on 2bffd6a1400 and after merging master with that. This occurs whether inplace is true or not. It also occurs on anndata master as well as the current release. As you'd expect, it doesn't happen with `return_joint=False`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:548,modifiability,layer,layers,548,"Just starting to try this out, but I hit a bug. Here's a script to reproduce and the traceback:. <details>. <summary>Script </summary>. ```python. import scanpy as sc. from scanpy.tools._ingest import ingest. import numpy as np. import pandas as pd. from functools import reduce. def simplify_annot(annot):. names = annot.columns.str.extract(r""\[(.*)\].*$"", expand=False). unique_names, idxs = np.unique(names, return_index=True). new_annot = annot.iloc[:, idxs].copy(). new_annot.columns = unique_names. return new_annot. def process(dset):. dset.layers[""counts""] = dset.X.copy(). sc.pp.normalize_total(dset). sc.pp.log1p(dset). sc.pp.highly_variable_genes(dset). sc.pp.pca(dset). sc.pp.neighbors(dset, n_neighbors=30). sc.tl.umap(dset). dset1 = sc.datasets.ebi_expression_atlas(""E-GEOD-81608"", filter_boring=True) . dset2 = sc.datasets.ebi_expression_atlas(""E-GEOD-83139"", filter_boring=True). # dset3 = sc.datasets.ebi_expression_atlas(""E-ENAD-27"", filter_boring=True). # dsets = [dset1, dset2, dset3]. dsets = [dset1, dset2]. for dset in dsets:. dset.obs = simplify_annot(dset.obs). sc.pp.calculate_qc_metrics(dset, inplace=True). shared_genes = reduce(np.intersect1d, [dset.var_names for dset in dsets]). dsets = [dset[:, shared_genes].copy() for dset in dsets]. for dset in dsets:. process(dset). # dset1, dset2, dset3 = dsets. dset1, dset2 = dsets. dset1.obs[""inferred cell type (dset1)""] = dset1.obs[""inferred cell type""]. dset12 = ingest(dset2, dset1, obs=""inferred cell type (dset1)"", return_joint=True). ```. Traceback:. ```python. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/isaac/github/scanpy/scanpy/tools/_ingest.py"", line 33, in ingest. return ing.to_adata(inplace) if not return_joint else ing.to_adata_joint(). File ""/Users/isaac/github/scanpy/scanpy/tools/_ingest.py"", line 222, in to_adata_joint. adata = AnnData(np.vstack((self._adata_ref.X, self._adata_new.X))). File ""/Users/isaac/github/anndata/anndata/core/anndata.py"", line 566, in ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:1607,modifiability,modul,module,1607,".columns = unique_names. return new_annot. def process(dset):. dset.layers[""counts""] = dset.X.copy(). sc.pp.normalize_total(dset). sc.pp.log1p(dset). sc.pp.highly_variable_genes(dset). sc.pp.pca(dset). sc.pp.neighbors(dset, n_neighbors=30). sc.tl.umap(dset). dset1 = sc.datasets.ebi_expression_atlas(""E-GEOD-81608"", filter_boring=True) . dset2 = sc.datasets.ebi_expression_atlas(""E-GEOD-83139"", filter_boring=True). # dset3 = sc.datasets.ebi_expression_atlas(""E-ENAD-27"", filter_boring=True). # dsets = [dset1, dset2, dset3]. dsets = [dset1, dset2]. for dset in dsets:. dset.obs = simplify_annot(dset.obs). sc.pp.calculate_qc_metrics(dset, inplace=True). shared_genes = reduce(np.intersect1d, [dset.var_names for dset in dsets]). dsets = [dset[:, shared_genes].copy() for dset in dsets]. for dset in dsets:. process(dset). # dset1, dset2, dset3 = dsets. dset1, dset2 = dsets. dset1.obs[""inferred cell type (dset1)""] = dset1.obs[""inferred cell type""]. dset12 = ingest(dset2, dset1, obs=""inferred cell type (dset1)"", return_joint=True). ```. Traceback:. ```python. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/isaac/github/scanpy/scanpy/tools/_ingest.py"", line 33, in ingest. return ing.to_adata(inplace) if not return_joint else ing.to_adata_joint(). File ""/Users/isaac/github/scanpy/scanpy/tools/_ingest.py"", line 222, in to_adata_joint. adata = AnnData(np.vstack((self._adata_ref.X, self._adata_new.X))). File ""/Users/isaac/github/anndata/anndata/core/anndata.py"", line 566, in __init__. filename=filename, filemode=filemode). File ""/Users/isaac/github/anndata/anndata/core/anndata.py"", line 735, in _init_as_actual. X = X.astype(dtype, copy=False). ValueError: setting an array element with a sequence. ```. </details>. I've reproduced on 2bffd6a1400 and after merging master with that. This occurs whether inplace is true or not. It also occurs on anndata master as well as the current release. As you'd expect, it doesn't happen with `return_joint=False`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:2439,reliability,doe,doesn,2439,".columns = unique_names. return new_annot. def process(dset):. dset.layers[""counts""] = dset.X.copy(). sc.pp.normalize_total(dset). sc.pp.log1p(dset). sc.pp.highly_variable_genes(dset). sc.pp.pca(dset). sc.pp.neighbors(dset, n_neighbors=30). sc.tl.umap(dset). dset1 = sc.datasets.ebi_expression_atlas(""E-GEOD-81608"", filter_boring=True) . dset2 = sc.datasets.ebi_expression_atlas(""E-GEOD-83139"", filter_boring=True). # dset3 = sc.datasets.ebi_expression_atlas(""E-ENAD-27"", filter_boring=True). # dsets = [dset1, dset2, dset3]. dsets = [dset1, dset2]. for dset in dsets:. dset.obs = simplify_annot(dset.obs). sc.pp.calculate_qc_metrics(dset, inplace=True). shared_genes = reduce(np.intersect1d, [dset.var_names for dset in dsets]). dsets = [dset[:, shared_genes].copy() for dset in dsets]. for dset in dsets:. process(dset). # dset1, dset2, dset3 = dsets. dset1, dset2 = dsets. dset1.obs[""inferred cell type (dset1)""] = dset1.obs[""inferred cell type""]. dset12 = ingest(dset2, dset1, obs=""inferred cell type (dset1)"", return_joint=True). ```. Traceback:. ```python. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/isaac/github/scanpy/scanpy/tools/_ingest.py"", line 33, in ingest. return ing.to_adata(inplace) if not return_joint else ing.to_adata_joint(). File ""/Users/isaac/github/scanpy/scanpy/tools/_ingest.py"", line 222, in to_adata_joint. adata = AnnData(np.vstack((self._adata_ref.X, self._adata_new.X))). File ""/Users/isaac/github/anndata/anndata/core/anndata.py"", line 566, in __init__. filename=filename, filemode=filemode). File ""/Users/isaac/github/anndata/anndata/core/anndata.py"", line 735, in _init_as_actual. X = X.astype(dtype, copy=False). ValueError: setting an array element with a sequence. ```. </details>. I've reproduced on 2bffd6a1400 and after merging master with that. This occurs whether inplace is true or not. It also occurs on anndata master as well as the current release. As you'd expect, it doesn't happen with `return_joint=False`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:1607,safety,modul,module,1607,".columns = unique_names. return new_annot. def process(dset):. dset.layers[""counts""] = dset.X.copy(). sc.pp.normalize_total(dset). sc.pp.log1p(dset). sc.pp.highly_variable_genes(dset). sc.pp.pca(dset). sc.pp.neighbors(dset, n_neighbors=30). sc.tl.umap(dset). dset1 = sc.datasets.ebi_expression_atlas(""E-GEOD-81608"", filter_boring=True) . dset2 = sc.datasets.ebi_expression_atlas(""E-GEOD-83139"", filter_boring=True). # dset3 = sc.datasets.ebi_expression_atlas(""E-ENAD-27"", filter_boring=True). # dsets = [dset1, dset2, dset3]. dsets = [dset1, dset2]. for dset in dsets:. dset.obs = simplify_annot(dset.obs). sc.pp.calculate_qc_metrics(dset, inplace=True). shared_genes = reduce(np.intersect1d, [dset.var_names for dset in dsets]). dsets = [dset[:, shared_genes].copy() for dset in dsets]. for dset in dsets:. process(dset). # dset1, dset2, dset3 = dsets. dset1, dset2 = dsets. dset1.obs[""inferred cell type (dset1)""] = dset1.obs[""inferred cell type""]. dset12 = ingest(dset2, dset1, obs=""inferred cell type (dset1)"", return_joint=True). ```. Traceback:. ```python. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/isaac/github/scanpy/scanpy/tools/_ingest.py"", line 33, in ingest. return ing.to_adata(inplace) if not return_joint else ing.to_adata_joint(). File ""/Users/isaac/github/scanpy/scanpy/tools/_ingest.py"", line 222, in to_adata_joint. adata = AnnData(np.vstack((self._adata_ref.X, self._adata_new.X))). File ""/Users/isaac/github/anndata/anndata/core/anndata.py"", line 566, in __init__. filename=filename, filemode=filemode). File ""/Users/isaac/github/anndata/anndata/core/anndata.py"", line 735, in _init_as_actual. X = X.astype(dtype, copy=False). ValueError: setting an array element with a sequence. ```. </details>. I've reproduced on 2bffd6a1400 and after merging master with that. This occurs whether inplace is true or not. It also occurs on anndata master as well as the current release. As you'd expect, it doesn't happen with `return_joint=False`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:85,testability,trace,traceback,85,"Just starting to try this out, but I hit a bug. Here's a script to reproduce and the traceback:. <details>. <summary>Script </summary>. ```python. import scanpy as sc. from scanpy.tools._ingest import ingest. import numpy as np. import pandas as pd. from functools import reduce. def simplify_annot(annot):. names = annot.columns.str.extract(r""\[(.*)\].*$"", expand=False). unique_names, idxs = np.unique(names, return_index=True). new_annot = annot.iloc[:, idxs].copy(). new_annot.columns = unique_names. return new_annot. def process(dset):. dset.layers[""counts""] = dset.X.copy(). sc.pp.normalize_total(dset). sc.pp.log1p(dset). sc.pp.highly_variable_genes(dset). sc.pp.pca(dset). sc.pp.neighbors(dset, n_neighbors=30). sc.tl.umap(dset). dset1 = sc.datasets.ebi_expression_atlas(""E-GEOD-81608"", filter_boring=True) . dset2 = sc.datasets.ebi_expression_atlas(""E-GEOD-83139"", filter_boring=True). # dset3 = sc.datasets.ebi_expression_atlas(""E-ENAD-27"", filter_boring=True). # dsets = [dset1, dset2, dset3]. dsets = [dset1, dset2]. for dset in dsets:. dset.obs = simplify_annot(dset.obs). sc.pp.calculate_qc_metrics(dset, inplace=True). shared_genes = reduce(np.intersect1d, [dset.var_names for dset in dsets]). dsets = [dset[:, shared_genes].copy() for dset in dsets]. for dset in dsets:. process(dset). # dset1, dset2, dset3 = dsets. dset1, dset2 = dsets. dset1.obs[""inferred cell type (dset1)""] = dset1.obs[""inferred cell type""]. dset12 = ingest(dset2, dset1, obs=""inferred cell type (dset1)"", return_joint=True). ```. Traceback:. ```python. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/isaac/github/scanpy/scanpy/tools/_ingest.py"", line 33, in ingest. return ing.to_adata(inplace) if not return_joint else ing.to_adata_joint(). File ""/Users/isaac/github/scanpy/scanpy/tools/_ingest.py"", line 222, in to_adata_joint. adata = AnnData(np.vstack((self._adata_ref.X, self._adata_new.X))). File ""/Users/isaac/github/anndata/anndata/core/anndata.py"", line 566, in ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:1520,testability,Trace,Traceback,1520,".columns = unique_names. return new_annot. def process(dset):. dset.layers[""counts""] = dset.X.copy(). sc.pp.normalize_total(dset). sc.pp.log1p(dset). sc.pp.highly_variable_genes(dset). sc.pp.pca(dset). sc.pp.neighbors(dset, n_neighbors=30). sc.tl.umap(dset). dset1 = sc.datasets.ebi_expression_atlas(""E-GEOD-81608"", filter_boring=True) . dset2 = sc.datasets.ebi_expression_atlas(""E-GEOD-83139"", filter_boring=True). # dset3 = sc.datasets.ebi_expression_atlas(""E-ENAD-27"", filter_boring=True). # dsets = [dset1, dset2, dset3]. dsets = [dset1, dset2]. for dset in dsets:. dset.obs = simplify_annot(dset.obs). sc.pp.calculate_qc_metrics(dset, inplace=True). shared_genes = reduce(np.intersect1d, [dset.var_names for dset in dsets]). dsets = [dset[:, shared_genes].copy() for dset in dsets]. for dset in dsets:. process(dset). # dset1, dset2, dset3 = dsets. dset1, dset2 = dsets. dset1.obs[""inferred cell type (dset1)""] = dset1.obs[""inferred cell type""]. dset12 = ingest(dset2, dset1, obs=""inferred cell type (dset1)"", return_joint=True). ```. Traceback:. ```python. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/isaac/github/scanpy/scanpy/tools/_ingest.py"", line 33, in ingest. return ing.to_adata(inplace) if not return_joint else ing.to_adata_joint(). File ""/Users/isaac/github/scanpy/scanpy/tools/_ingest.py"", line 222, in to_adata_joint. adata = AnnData(np.vstack((self._adata_ref.X, self._adata_new.X))). File ""/Users/isaac/github/anndata/anndata/core/anndata.py"", line 566, in __init__. filename=filename, filemode=filemode). File ""/Users/isaac/github/anndata/anndata/core/anndata.py"", line 735, in _init_as_actual. X = X.astype(dtype, copy=False). ValueError: setting an array element with a sequence. ```. </details>. I've reproduced on 2bffd6a1400 and after merging master with that. This occurs whether inplace is true or not. It also occurs on anndata master as well as the current release. As you'd expect, it doesn't happen with `return_joint=False`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:1543,testability,Trace,Traceback,1543,".columns = unique_names. return new_annot. def process(dset):. dset.layers[""counts""] = dset.X.copy(). sc.pp.normalize_total(dset). sc.pp.log1p(dset). sc.pp.highly_variable_genes(dset). sc.pp.pca(dset). sc.pp.neighbors(dset, n_neighbors=30). sc.tl.umap(dset). dset1 = sc.datasets.ebi_expression_atlas(""E-GEOD-81608"", filter_boring=True) . dset2 = sc.datasets.ebi_expression_atlas(""E-GEOD-83139"", filter_boring=True). # dset3 = sc.datasets.ebi_expression_atlas(""E-ENAD-27"", filter_boring=True). # dsets = [dset1, dset2, dset3]. dsets = [dset1, dset2]. for dset in dsets:. dset.obs = simplify_annot(dset.obs). sc.pp.calculate_qc_metrics(dset, inplace=True). shared_genes = reduce(np.intersect1d, [dset.var_names for dset in dsets]). dsets = [dset[:, shared_genes].copy() for dset in dsets]. for dset in dsets:. process(dset). # dset1, dset2, dset3 = dsets. dset1, dset2 = dsets. dset1.obs[""inferred cell type (dset1)""] = dset1.obs[""inferred cell type""]. dset12 = ingest(dset2, dset1, obs=""inferred cell type (dset1)"", return_joint=True). ```. Traceback:. ```python. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/isaac/github/scanpy/scanpy/tools/_ingest.py"", line 33, in ingest. return ing.to_adata(inplace) if not return_joint else ing.to_adata_joint(). File ""/Users/isaac/github/scanpy/scanpy/tools/_ingest.py"", line 222, in to_adata_joint. adata = AnnData(np.vstack((self._adata_ref.X, self._adata_new.X))). File ""/Users/isaac/github/anndata/anndata/core/anndata.py"", line 566, in __init__. filename=filename, filemode=filemode). File ""/Users/isaac/github/anndata/anndata/core/anndata.py"", line 735, in _init_as_actual. X = X.astype(dtype, copy=False). ValueError: setting an array element with a sequence. ```. </details>. I've reproduced on 2bffd6a1400 and after merging master with that. This occurs whether inplace is true or not. It also occurs on anndata master as well as the current release. As you'd expect, it doesn't happen with `return_joint=False`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:180,usability,tool,tools,180,"Just starting to try this out, but I hit a bug. Here's a script to reproduce and the traceback:. <details>. <summary>Script </summary>. ```python. import scanpy as sc. from scanpy.tools._ingest import ingest. import numpy as np. import pandas as pd. from functools import reduce. def simplify_annot(annot):. names = annot.columns.str.extract(r""\[(.*)\].*$"", expand=False). unique_names, idxs = np.unique(names, return_index=True). new_annot = annot.iloc[:, idxs].copy(). new_annot.columns = unique_names. return new_annot. def process(dset):. dset.layers[""counts""] = dset.X.copy(). sc.pp.normalize_total(dset). sc.pp.log1p(dset). sc.pp.highly_variable_genes(dset). sc.pp.pca(dset). sc.pp.neighbors(dset, n_neighbors=30). sc.tl.umap(dset). dset1 = sc.datasets.ebi_expression_atlas(""E-GEOD-81608"", filter_boring=True) . dset2 = sc.datasets.ebi_expression_atlas(""E-GEOD-83139"", filter_boring=True). # dset3 = sc.datasets.ebi_expression_atlas(""E-ENAD-27"", filter_boring=True). # dsets = [dset1, dset2, dset3]. dsets = [dset1, dset2]. for dset in dsets:. dset.obs = simplify_annot(dset.obs). sc.pp.calculate_qc_metrics(dset, inplace=True). shared_genes = reduce(np.intersect1d, [dset.var_names for dset in dsets]). dsets = [dset[:, shared_genes].copy() for dset in dsets]. for dset in dsets:. process(dset). # dset1, dset2, dset3 = dsets. dset1, dset2 = dsets. dset1.obs[""inferred cell type (dset1)""] = dset1.obs[""inferred cell type""]. dset12 = ingest(dset2, dset1, obs=""inferred cell type (dset1)"", return_joint=True). ```. Traceback:. ```python. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/isaac/github/scanpy/scanpy/tools/_ingest.py"", line 33, in ingest. return ing.to_adata(inplace) if not return_joint else ing.to_adata_joint(). File ""/Users/isaac/github/scanpy/scanpy/tools/_ingest.py"", line 222, in to_adata_joint. adata = AnnData(np.vstack((self._adata_ref.X, self._adata_new.X))). File ""/Users/isaac/github/anndata/anndata/core/anndata.py"", line 566, in ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:1623,usability,User,Users,1623,".columns = unique_names. return new_annot. def process(dset):. dset.layers[""counts""] = dset.X.copy(). sc.pp.normalize_total(dset). sc.pp.log1p(dset). sc.pp.highly_variable_genes(dset). sc.pp.pca(dset). sc.pp.neighbors(dset, n_neighbors=30). sc.tl.umap(dset). dset1 = sc.datasets.ebi_expression_atlas(""E-GEOD-81608"", filter_boring=True) . dset2 = sc.datasets.ebi_expression_atlas(""E-GEOD-83139"", filter_boring=True). # dset3 = sc.datasets.ebi_expression_atlas(""E-ENAD-27"", filter_boring=True). # dsets = [dset1, dset2, dset3]. dsets = [dset1, dset2]. for dset in dsets:. dset.obs = simplify_annot(dset.obs). sc.pp.calculate_qc_metrics(dset, inplace=True). shared_genes = reduce(np.intersect1d, [dset.var_names for dset in dsets]). dsets = [dset[:, shared_genes].copy() for dset in dsets]. for dset in dsets:. process(dset). # dset1, dset2, dset3 = dsets. dset1, dset2 = dsets. dset1.obs[""inferred cell type (dset1)""] = dset1.obs[""inferred cell type""]. dset12 = ingest(dset2, dset1, obs=""inferred cell type (dset1)"", return_joint=True). ```. Traceback:. ```python. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/isaac/github/scanpy/scanpy/tools/_ingest.py"", line 33, in ingest. return ing.to_adata(inplace) if not return_joint else ing.to_adata_joint(). File ""/Users/isaac/github/scanpy/scanpy/tools/_ingest.py"", line 222, in to_adata_joint. adata = AnnData(np.vstack((self._adata_ref.X, self._adata_new.X))). File ""/Users/isaac/github/anndata/anndata/core/anndata.py"", line 566, in __init__. filename=filename, filemode=filemode). File ""/Users/isaac/github/anndata/anndata/core/anndata.py"", line 735, in _init_as_actual. X = X.astype(dtype, copy=False). ValueError: setting an array element with a sequence. ```. </details>. I've reproduced on 2bffd6a1400 and after merging master with that. This occurs whether inplace is true or not. It also occurs on anndata master as well as the current release. As you'd expect, it doesn't happen with `return_joint=False`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:1656,usability,tool,tools,1656,".columns = unique_names. return new_annot. def process(dset):. dset.layers[""counts""] = dset.X.copy(). sc.pp.normalize_total(dset). sc.pp.log1p(dset). sc.pp.highly_variable_genes(dset). sc.pp.pca(dset). sc.pp.neighbors(dset, n_neighbors=30). sc.tl.umap(dset). dset1 = sc.datasets.ebi_expression_atlas(""E-GEOD-81608"", filter_boring=True) . dset2 = sc.datasets.ebi_expression_atlas(""E-GEOD-83139"", filter_boring=True). # dset3 = sc.datasets.ebi_expression_atlas(""E-ENAD-27"", filter_boring=True). # dsets = [dset1, dset2, dset3]. dsets = [dset1, dset2]. for dset in dsets:. dset.obs = simplify_annot(dset.obs). sc.pp.calculate_qc_metrics(dset, inplace=True). shared_genes = reduce(np.intersect1d, [dset.var_names for dset in dsets]). dsets = [dset[:, shared_genes].copy() for dset in dsets]. for dset in dsets:. process(dset). # dset1, dset2, dset3 = dsets. dset1, dset2 = dsets. dset1.obs[""inferred cell type (dset1)""] = dset1.obs[""inferred cell type""]. dset12 = ingest(dset2, dset1, obs=""inferred cell type (dset1)"", return_joint=True). ```. Traceback:. ```python. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/isaac/github/scanpy/scanpy/tools/_ingest.py"", line 33, in ingest. return ing.to_adata(inplace) if not return_joint else ing.to_adata_joint(). File ""/Users/isaac/github/scanpy/scanpy/tools/_ingest.py"", line 222, in to_adata_joint. adata = AnnData(np.vstack((self._adata_ref.X, self._adata_new.X))). File ""/Users/isaac/github/anndata/anndata/core/anndata.py"", line 566, in __init__. filename=filename, filemode=filemode). File ""/Users/isaac/github/anndata/anndata/core/anndata.py"", line 735, in _init_as_actual. X = X.astype(dtype, copy=False). ValueError: setting an array element with a sequence. ```. </details>. I've reproduced on 2bffd6a1400 and after merging master with that. This occurs whether inplace is true or not. It also occurs on anndata master as well as the current release. As you'd expect, it doesn't happen with `return_joint=False`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:1778,usability,User,Users,1778,".columns = unique_names. return new_annot. def process(dset):. dset.layers[""counts""] = dset.X.copy(). sc.pp.normalize_total(dset). sc.pp.log1p(dset). sc.pp.highly_variable_genes(dset). sc.pp.pca(dset). sc.pp.neighbors(dset, n_neighbors=30). sc.tl.umap(dset). dset1 = sc.datasets.ebi_expression_atlas(""E-GEOD-81608"", filter_boring=True) . dset2 = sc.datasets.ebi_expression_atlas(""E-GEOD-83139"", filter_boring=True). # dset3 = sc.datasets.ebi_expression_atlas(""E-ENAD-27"", filter_boring=True). # dsets = [dset1, dset2, dset3]. dsets = [dset1, dset2]. for dset in dsets:. dset.obs = simplify_annot(dset.obs). sc.pp.calculate_qc_metrics(dset, inplace=True). shared_genes = reduce(np.intersect1d, [dset.var_names for dset in dsets]). dsets = [dset[:, shared_genes].copy() for dset in dsets]. for dset in dsets:. process(dset). # dset1, dset2, dset3 = dsets. dset1, dset2 = dsets. dset1.obs[""inferred cell type (dset1)""] = dset1.obs[""inferred cell type""]. dset12 = ingest(dset2, dset1, obs=""inferred cell type (dset1)"", return_joint=True). ```. Traceback:. ```python. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/isaac/github/scanpy/scanpy/tools/_ingest.py"", line 33, in ingest. return ing.to_adata(inplace) if not return_joint else ing.to_adata_joint(). File ""/Users/isaac/github/scanpy/scanpy/tools/_ingest.py"", line 222, in to_adata_joint. adata = AnnData(np.vstack((self._adata_ref.X, self._adata_new.X))). File ""/Users/isaac/github/anndata/anndata/core/anndata.py"", line 566, in __init__. filename=filename, filemode=filemode). File ""/Users/isaac/github/anndata/anndata/core/anndata.py"", line 735, in _init_as_actual. X = X.astype(dtype, copy=False). ValueError: setting an array element with a sequence. ```. </details>. I've reproduced on 2bffd6a1400 and after merging master with that. This occurs whether inplace is true or not. It also occurs on anndata master as well as the current release. As you'd expect, it doesn't happen with `return_joint=False`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:1811,usability,tool,tools,1811,".columns = unique_names. return new_annot. def process(dset):. dset.layers[""counts""] = dset.X.copy(). sc.pp.normalize_total(dset). sc.pp.log1p(dset). sc.pp.highly_variable_genes(dset). sc.pp.pca(dset). sc.pp.neighbors(dset, n_neighbors=30). sc.tl.umap(dset). dset1 = sc.datasets.ebi_expression_atlas(""E-GEOD-81608"", filter_boring=True) . dset2 = sc.datasets.ebi_expression_atlas(""E-GEOD-83139"", filter_boring=True). # dset3 = sc.datasets.ebi_expression_atlas(""E-ENAD-27"", filter_boring=True). # dsets = [dset1, dset2, dset3]. dsets = [dset1, dset2]. for dset in dsets:. dset.obs = simplify_annot(dset.obs). sc.pp.calculate_qc_metrics(dset, inplace=True). shared_genes = reduce(np.intersect1d, [dset.var_names for dset in dsets]). dsets = [dset[:, shared_genes].copy() for dset in dsets]. for dset in dsets:. process(dset). # dset1, dset2, dset3 = dsets. dset1, dset2 = dsets. dset1.obs[""inferred cell type (dset1)""] = dset1.obs[""inferred cell type""]. dset12 = ingest(dset2, dset1, obs=""inferred cell type (dset1)"", return_joint=True). ```. Traceback:. ```python. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/isaac/github/scanpy/scanpy/tools/_ingest.py"", line 33, in ingest. return ing.to_adata(inplace) if not return_joint else ing.to_adata_joint(). File ""/Users/isaac/github/scanpy/scanpy/tools/_ingest.py"", line 222, in to_adata_joint. adata = AnnData(np.vstack((self._adata_ref.X, self._adata_new.X))). File ""/Users/isaac/github/anndata/anndata/core/anndata.py"", line 566, in __init__. filename=filename, filemode=filemode). File ""/Users/isaac/github/anndata/anndata/core/anndata.py"", line 735, in _init_as_actual. X = X.astype(dtype, copy=False). ValueError: setting an array element with a sequence. ```. </details>. I've reproduced on 2bffd6a1400 and after merging master with that. This occurs whether inplace is true or not. It also occurs on anndata master as well as the current release. As you'd expect, it doesn't happen with `return_joint=False`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:1934,usability,User,Users,1934,".columns = unique_names. return new_annot. def process(dset):. dset.layers[""counts""] = dset.X.copy(). sc.pp.normalize_total(dset). sc.pp.log1p(dset). sc.pp.highly_variable_genes(dset). sc.pp.pca(dset). sc.pp.neighbors(dset, n_neighbors=30). sc.tl.umap(dset). dset1 = sc.datasets.ebi_expression_atlas(""E-GEOD-81608"", filter_boring=True) . dset2 = sc.datasets.ebi_expression_atlas(""E-GEOD-83139"", filter_boring=True). # dset3 = sc.datasets.ebi_expression_atlas(""E-ENAD-27"", filter_boring=True). # dsets = [dset1, dset2, dset3]. dsets = [dset1, dset2]. for dset in dsets:. dset.obs = simplify_annot(dset.obs). sc.pp.calculate_qc_metrics(dset, inplace=True). shared_genes = reduce(np.intersect1d, [dset.var_names for dset in dsets]). dsets = [dset[:, shared_genes].copy() for dset in dsets]. for dset in dsets:. process(dset). # dset1, dset2, dset3 = dsets. dset1, dset2 = dsets. dset1.obs[""inferred cell type (dset1)""] = dset1.obs[""inferred cell type""]. dset12 = ingest(dset2, dset1, obs=""inferred cell type (dset1)"", return_joint=True). ```. Traceback:. ```python. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/isaac/github/scanpy/scanpy/tools/_ingest.py"", line 33, in ingest. return ing.to_adata(inplace) if not return_joint else ing.to_adata_joint(). File ""/Users/isaac/github/scanpy/scanpy/tools/_ingest.py"", line 222, in to_adata_joint. adata = AnnData(np.vstack((self._adata_ref.X, self._adata_new.X))). File ""/Users/isaac/github/anndata/anndata/core/anndata.py"", line 566, in __init__. filename=filename, filemode=filemode). File ""/Users/isaac/github/anndata/anndata/core/anndata.py"", line 735, in _init_as_actual. X = X.astype(dtype, copy=False). ValueError: setting an array element with a sequence. ```. </details>. I've reproduced on 2bffd6a1400 and after merging master with that. This occurs whether inplace is true or not. It also occurs on anndata master as well as the current release. As you'd expect, it doesn't happen with `return_joint=False`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:2056,usability,User,Users,2056,".columns = unique_names. return new_annot. def process(dset):. dset.layers[""counts""] = dset.X.copy(). sc.pp.normalize_total(dset). sc.pp.log1p(dset). sc.pp.highly_variable_genes(dset). sc.pp.pca(dset). sc.pp.neighbors(dset, n_neighbors=30). sc.tl.umap(dset). dset1 = sc.datasets.ebi_expression_atlas(""E-GEOD-81608"", filter_boring=True) . dset2 = sc.datasets.ebi_expression_atlas(""E-GEOD-83139"", filter_boring=True). # dset3 = sc.datasets.ebi_expression_atlas(""E-ENAD-27"", filter_boring=True). # dsets = [dset1, dset2, dset3]. dsets = [dset1, dset2]. for dset in dsets:. dset.obs = simplify_annot(dset.obs). sc.pp.calculate_qc_metrics(dset, inplace=True). shared_genes = reduce(np.intersect1d, [dset.var_names for dset in dsets]). dsets = [dset[:, shared_genes].copy() for dset in dsets]. for dset in dsets:. process(dset). # dset1, dset2, dset3 = dsets. dset1, dset2 = dsets. dset1.obs[""inferred cell type (dset1)""] = dset1.obs[""inferred cell type""]. dset12 = ingest(dset2, dset1, obs=""inferred cell type (dset1)"", return_joint=True). ```. Traceback:. ```python. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/Users/isaac/github/scanpy/scanpy/tools/_ingest.py"", line 33, in ingest. return ing.to_adata(inplace) if not return_joint else ing.to_adata_joint(). File ""/Users/isaac/github/scanpy/scanpy/tools/_ingest.py"", line 222, in to_adata_joint. adata = AnnData(np.vstack((self._adata_ref.X, self._adata_new.X))). File ""/Users/isaac/github/anndata/anndata/core/anndata.py"", line 566, in __init__. filename=filename, filemode=filemode). File ""/Users/isaac/github/anndata/anndata/core/anndata.py"", line 735, in _init_as_actual. X = X.astype(dtype, copy=False). ValueError: setting an array element with a sequence. ```. </details>. I've reproduced on 2bffd6a1400 and after merging master with that. This occurs whether inplace is true or not. It also occurs on anndata master as well as the current release. As you'd expect, it doesn't happen with `return_joint=False`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:80,integrability,coupl,couple,80,"Since I can get some labels out from just not getting an object back, I tried a couple things. Using the labelled half of `E-ENAD-27` to add labels to the other cells:. <details>. <summary> plots </summary>. ![image](https://user-images.githubusercontent.com/8238804/62707611-03b90180-ba35-11e9-9aab-a039c8580805.png). </details>. Using `E-GEOD-81608` (many cells, few labels) to label `E-GEOD-83139` (few cells, many labels) seemed to work fine considering we're moving to a lower resolution label set. Here's the latter projected onto the former:. <details>. <summary> plots </summary>. ![image](https://user-images.githubusercontent.com/8238804/62708095-036d3600-ba36-11e9-8e7c-de1bba482ba3.png). </details>. The reverse (former projected onto the latter) didn't seem to work so well:. <details>. <summary> plots </summary>. ![image](https://user-images.githubusercontent.com/8238804/62708220-44fde100-ba36-11e9-8927-a2a43a7e7df0.png). </details>. If it'd be useful, I could clean up the notebooks I ran these in and get you some confusion matrices tomorrow.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:80,modifiability,coupl,couple,80,"Since I can get some labels out from just not getting an object back, I tried a couple things. Using the labelled half of `E-ENAD-27` to add labels to the other cells:. <details>. <summary> plots </summary>. ![image](https://user-images.githubusercontent.com/8238804/62707611-03b90180-ba35-11e9-9aab-a039c8580805.png). </details>. Using `E-GEOD-81608` (many cells, few labels) to label `E-GEOD-83139` (few cells, many labels) seemed to work fine considering we're moving to a lower resolution label set. Here's the latter projected onto the former:. <details>. <summary> plots </summary>. ![image](https://user-images.githubusercontent.com/8238804/62708095-036d3600-ba36-11e9-8e7c-de1bba482ba3.png). </details>. The reverse (former projected onto the latter) didn't seem to work so well:. <details>. <summary> plots </summary>. ![image](https://user-images.githubusercontent.com/8238804/62708220-44fde100-ba36-11e9-8927-a2a43a7e7df0.png). </details>. If it'd be useful, I could clean up the notebooks I ran these in and get you some confusion matrices tomorrow.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:80,testability,coupl,couple,80,"Since I can get some labels out from just not getting an object back, I tried a couple things. Using the labelled half of `E-ENAD-27` to add labels to the other cells:. <details>. <summary> plots </summary>. ![image](https://user-images.githubusercontent.com/8238804/62707611-03b90180-ba35-11e9-9aab-a039c8580805.png). </details>. Using `E-GEOD-81608` (many cells, few labels) to label `E-GEOD-83139` (few cells, many labels) seemed to work fine considering we're moving to a lower resolution label set. Here's the latter projected onto the former:. <details>. <summary> plots </summary>. ![image](https://user-images.githubusercontent.com/8238804/62708095-036d3600-ba36-11e9-8e7c-de1bba482ba3.png). </details>. The reverse (former projected onto the latter) didn't seem to work so well:. <details>. <summary> plots </summary>. ![image](https://user-images.githubusercontent.com/8238804/62708220-44fde100-ba36-11e9-8927-a2a43a7e7df0.png). </details>. If it'd be useful, I could clean up the notebooks I ran these in and get you some confusion matrices tomorrow.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:225,usability,user,user-images,225,"Since I can get some labels out from just not getting an object back, I tried a couple things. Using the labelled half of `E-ENAD-27` to add labels to the other cells:. <details>. <summary> plots </summary>. ![image](https://user-images.githubusercontent.com/8238804/62707611-03b90180-ba35-11e9-9aab-a039c8580805.png). </details>. Using `E-GEOD-81608` (many cells, few labels) to label `E-GEOD-83139` (few cells, many labels) seemed to work fine considering we're moving to a lower resolution label set. Here's the latter projected onto the former:. <details>. <summary> plots </summary>. ![image](https://user-images.githubusercontent.com/8238804/62708095-036d3600-ba36-11e9-8e7c-de1bba482ba3.png). </details>. The reverse (former projected onto the latter) didn't seem to work so well:. <details>. <summary> plots </summary>. ![image](https://user-images.githubusercontent.com/8238804/62708220-44fde100-ba36-11e9-8927-a2a43a7e7df0.png). </details>. If it'd be useful, I could clean up the notebooks I ran these in and get you some confusion matrices tomorrow.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:606,usability,user,user-images,606,"Since I can get some labels out from just not getting an object back, I tried a couple things. Using the labelled half of `E-ENAD-27` to add labels to the other cells:. <details>. <summary> plots </summary>. ![image](https://user-images.githubusercontent.com/8238804/62707611-03b90180-ba35-11e9-9aab-a039c8580805.png). </details>. Using `E-GEOD-81608` (many cells, few labels) to label `E-GEOD-83139` (few cells, many labels) seemed to work fine considering we're moving to a lower resolution label set. Here's the latter projected onto the former:. <details>. <summary> plots </summary>. ![image](https://user-images.githubusercontent.com/8238804/62708095-036d3600-ba36-11e9-8e7c-de1bba482ba3.png). </details>. The reverse (former projected onto the latter) didn't seem to work so well:. <details>. <summary> plots </summary>. ![image](https://user-images.githubusercontent.com/8238804/62708220-44fde100-ba36-11e9-8927-a2a43a7e7df0.png). </details>. If it'd be useful, I could clean up the notebooks I ran these in and get you some confusion matrices tomorrow.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:845,usability,user,user-images,845,"Since I can get some labels out from just not getting an object back, I tried a couple things. Using the labelled half of `E-ENAD-27` to add labels to the other cells:. <details>. <summary> plots </summary>. ![image](https://user-images.githubusercontent.com/8238804/62707611-03b90180-ba35-11e9-9aab-a039c8580805.png). </details>. Using `E-GEOD-81608` (many cells, few labels) to label `E-GEOD-83139` (few cells, many labels) seemed to work fine considering we're moving to a lower resolution label set. Here's the latter projected onto the former:. <details>. <summary> plots </summary>. ![image](https://user-images.githubusercontent.com/8238804/62708095-036d3600-ba36-11e9-8e7c-de1bba482ba3.png). </details>. The reverse (former projected onto the latter) didn't seem to work so well:. <details>. <summary> plots </summary>. ![image](https://user-images.githubusercontent.com/8238804/62708220-44fde100-ba36-11e9-8927-a2a43a7e7df0.png). </details>. If it'd be useful, I could clean up the notebooks I ran these in and get you some confusion matrices tomorrow.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:166,deployability,integr,integration,166,"Great! I'll check it out when I have a chance. If this is close to ready, could it also start getting some tests? Just to clarify, would a notebook with the pancreas integration stuff be useful to you?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:166,integrability,integr,integration,166,"Great! I'll check it out when I have a chance. If this is close to ready, could it also start getting some tests? Just to clarify, would a notebook with the pancreas integration stuff be useful to you?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:166,interoperability,integr,integration,166,"Great! I'll check it out when I have a chance. If this is close to ready, could it also start getting some tests? Just to clarify, would a notebook with the pancreas integration stuff be useful to you?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:166,modifiability,integr,integration,166,"Great! I'll check it out when I have a chance. If this is close to ready, could it also start getting some tests? Just to clarify, would a notebook with the pancreas integration stuff be useful to you?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:166,reliability,integr,integration,166,"Great! I'll check it out when I have a chance. If this is close to ready, could it also start getting some tests? Just to clarify, would a notebook with the pancreas integration stuff be useful to you?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:107,safety,test,tests,107,"Great! I'll check it out when I have a chance. If this is close to ready, could it also start getting some tests? Just to clarify, would a notebook with the pancreas integration stuff be useful to you?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:166,security,integr,integration,166,"Great! I'll check it out when I have a chance. If this is close to ready, could it also start getting some tests? Just to clarify, would a notebook with the pancreas integration stuff be useful to you?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:107,testability,test,tests,107,"Great! I'll check it out when I have a chance. If this is close to ready, could it also start getting some tests? Just to clarify, would a notebook with the pancreas integration stuff be useful to you?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:166,testability,integr,integration,166,"Great! I'll check it out when I have a chance. If this is close to ready, could it also start getting some tests? Just to clarify, would a notebook with the pancreas integration stuff be useful to you?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:58,usability,close,close,58,"Great! I'll check it out when I have a chance. If this is close to ready, could it also start getting some tests? Just to clarify, would a notebook with the pancreas integration stuff be useful to you?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:88,performance,time,time,88,"Hi, yes, i will start adding tests. . And i think it can be useful but only if you have time for it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:29,safety,test,tests,29,"Hi, yes, i will start adding tests. . And i think it can be useful but only if you have time for it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:29,testability,test,tests,29,"Hi, yes, i will start adding tests. . And i think it can be useful but only if you have time for it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:23,safety,Test,Tests,23,@falexwolf @ivirshup . Tests are also ready.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:23,testability,Test,Tests,23,@falexwolf @ivirshup . Tests are also ready.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:165,integrability,pub,public,165,"@ivirshup . Thank you for your analysis. I will definitely add all needed arguments' descriptions a bit later. As for the `Ingest` class, i don't think it should be public, the public thing is the `ingest` function. Also i added the possibility of specifying `adata.concatenate` arguments in both `ingest` and `to_adata_joint`. So to get the same `obs_names` you can use `sc.tl.ingest(..., index_unique=None)` or `ing.to_adata_joint(index_unique=None)`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:177,integrability,pub,public,177,"@ivirshup . Thank you for your analysis. I will definitely add all needed arguments' descriptions a bit later. As for the `Ingest` class, i don't think it should be public, the public thing is the `ingest` function. Also i added the possibility of specifying `adata.concatenate` arguments in both `ingest` and `to_adata_joint`. So to get the same `obs_names` you can use `sc.tl.ingest(..., index_unique=None)` or `ing.to_adata_joint(index_unique=None)`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:248,interoperability,specif,specifying,248,"@ivirshup . Thank you for your analysis. I will definitely add all needed arguments' descriptions a bit later. As for the `Ingest` class, i don't think it should be public, the public thing is the `ingest` function. Also i added the possibility of specifying `adata.concatenate` arguments in both `ingest` and `to_adata_joint`. So to get the same `obs_names` you can use `sc.tl.ingest(..., index_unique=None)` or `ing.to_adata_joint(index_unique=None)`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:75,safety,review,review,75,"@Koncopd once this is ready from your side, you can re-request @ivirshupâ€™s review.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:75,testability,review,review,75,"@Koncopd once this is ready from your side, you can re-request @ivirshupâ€™s review.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:55,safety,review,review,55,"I'd be fine with it, but happy to wait for @ivirshup's review!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:55,testability,review,review,55,"I'd be fine with it, but happy to wait for @ivirshup's review!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:44,interoperability,convers,conversations,44,"Well, he reviewed it before and none of the conversations is marked as resolved. Letâ€™s let them close those and then we can squash & merge.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:9,safety,review,reviewed,9,"Well, he reviewed it before and none of the conversations is marked as resolved. Letâ€™s let them close those and then we can squash & merge.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:9,testability,review,reviewed,9,"Well, he reviewed it before and none of the conversations is marked as resolved. Letâ€™s let them close those and then we can squash & merge.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:96,usability,close,close,96,"Well, he reviewed it before and none of the conversations is marked as resolved. Letâ€™s let them close those and then we can squash & merge.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:176,reliability,doe,doesn,176,"@flying-sheep Yeah, sorry, missed this. Thank you for correcting. As for black, i don't understand why it happened. I changed the quotes back and run black on the file, now it doesn't change it... I will go through the discussion with @ivirshup again, fix some things and then will re-request the review.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:297,safety,review,review,297,"@flying-sheep Yeah, sorry, missed this. Thank you for correcting. As for black, i don't understand why it happened. I changed the quotes back and run black on the file, now it doesn't change it... I will go through the discussion with @ivirshup again, fix some things and then will re-request the review.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:88,testability,understand,understand,88,"@flying-sheep Yeah, sorry, missed this. Thank you for correcting. As for black, i don't understand why it happened. I changed the quotes back and run black on the file, now it doesn't change it... I will go through the discussion with @ivirshup again, fix some things and then will re-request the review.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:297,testability,review,review,297,"@flying-sheep Yeah, sorry, missed this. Thank you for correcting. As for black, i don't understand why it happened. I changed the quotes back and run black on the file, now it doesn't change it... I will go through the discussion with @ivirshup again, fix some things and then will re-request the review.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:102,usability,effectiv,effective,102,"Great, thank you a lot, this seems like it was quite some work! I think the resolve feature is pretty effective in keeping track of what one already adressed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:73,usability,close,closely,73,"@Koncopd, I'm happy to go over this again whenever. I haven't looked too closely yet, but it looks like a lot has been addressed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:172,deployability,releas,release,172,"Guys, I'll merge this for now so that I can conveniently play around with it in practical settings and potentially improve. We don't need to advertise for now and the next release might be a bit ahead anyway. I'm reading the latest comments as you being essentially positive. â˜ºï¸",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/pull/651:80,reliability,pra,practical,80,"Guys, I'll merge this for now so that I can conveniently play around with it in practical settings and potentially improve. We don't need to advertise for now and the next release might be a bit ahead anyway. I'm reading the latest comments as you being essentially positive. â˜ºï¸",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/651
https://github.com/scverse/scanpy/issues/653:67,integrability,sub,subsetting,67,"Hey! Just something that crossed my mind... could it be that after subsetting, you have 0 variance in some genes? You may have to rerun `sc.pp.filter_genes()` to take out genes that are 0 everywhere after subsetting. This would give you an `NaN` in the testing. Maybe check that the genes you filter out are the ones that gave you the issues in the initial run.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:205,integrability,sub,subsetting,205,"Hey! Just something that crossed my mind... could it be that after subsetting, you have 0 variance in some genes? You may have to rerun `sc.pp.filter_genes()` to take out genes that are 0 everywhere after subsetting. This would give you an `NaN` in the testing. Maybe check that the genes you filter out are the ones that gave you the issues in the initial run.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:293,integrability,filter,filter,293,"Hey! Just something that crossed my mind... could it be that after subsetting, you have 0 variance in some genes? You may have to rerun `sc.pp.filter_genes()` to take out genes that are 0 everywhere after subsetting. This would give you an `NaN` in the testing. Maybe check that the genes you filter out are the ones that gave you the issues in the initial run.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:253,safety,test,testing,253,"Hey! Just something that crossed my mind... could it be that after subsetting, you have 0 variance in some genes? You may have to rerun `sc.pp.filter_genes()` to take out genes that are 0 everywhere after subsetting. This would give you an `NaN` in the testing. Maybe check that the genes you filter out are the ones that gave you the issues in the initial run.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:253,testability,test,testing,253,"Hey! Just something that crossed my mind... could it be that after subsetting, you have 0 variance in some genes? You may have to rerun `sc.pp.filter_genes()` to take out genes that are 0 everywhere after subsetting. This would give you an `NaN` in the testing. Maybe check that the genes you filter out are the ones that gave you the issues in the initial run.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:182,availability,cluster,clustering,182,"@LuckyMD Hi, Thanks a lot for your reply. Well the thing is I have already filtered my cells before and after the preprocessing I did imputation on my data using MAGIC and the first clustering was with no warning. I wonder how would it be possible my values have changed because I just subset some clusters and if there was something wrong within some gene values it should have popped up before too because basically I am taking the same genes into consideration for my subset analysis...that is very confusing to me...as long as it doesn't really effect my results I wouldn't mind it but I want to be sure about it and also discover the reason behind it but so far I am clueless",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:298,availability,cluster,clusters,298,"@LuckyMD Hi, Thanks a lot for your reply. Well the thing is I have already filtered my cells before and after the preprocessing I did imputation on my data using MAGIC and the first clustering was with no warning. I wonder how would it be possible my values have changed because I just subset some clusters and if there was something wrong within some gene values it should have popped up before too because basically I am taking the same genes into consideration for my subset analysis...that is very confusing to me...as long as it doesn't really effect my results I wouldn't mind it but I want to be sure about it and also discover the reason behind it but so far I am clueless",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:182,deployability,cluster,clustering,182,"@LuckyMD Hi, Thanks a lot for your reply. Well the thing is I have already filtered my cells before and after the preprocessing I did imputation on my data using MAGIC and the first clustering was with no warning. I wonder how would it be possible my values have changed because I just subset some clusters and if there was something wrong within some gene values it should have popped up before too because basically I am taking the same genes into consideration for my subset analysis...that is very confusing to me...as long as it doesn't really effect my results I wouldn't mind it but I want to be sure about it and also discover the reason behind it but so far I am clueless",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:298,deployability,cluster,clusters,298,"@LuckyMD Hi, Thanks a lot for your reply. Well the thing is I have already filtered my cells before and after the preprocessing I did imputation on my data using MAGIC and the first clustering was with no warning. I wonder how would it be possible my values have changed because I just subset some clusters and if there was something wrong within some gene values it should have popped up before too because basically I am taking the same genes into consideration for my subset analysis...that is very confusing to me...as long as it doesn't really effect my results I wouldn't mind it but I want to be sure about it and also discover the reason behind it but so far I am clueless",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:75,integrability,filter,filtered,75,"@LuckyMD Hi, Thanks a lot for your reply. Well the thing is I have already filtered my cells before and after the preprocessing I did imputation on my data using MAGIC and the first clustering was with no warning. I wonder how would it be possible my values have changed because I just subset some clusters and if there was something wrong within some gene values it should have popped up before too because basically I am taking the same genes into consideration for my subset analysis...that is very confusing to me...as long as it doesn't really effect my results I wouldn't mind it but I want to be sure about it and also discover the reason behind it but so far I am clueless",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:286,integrability,sub,subset,286,"@LuckyMD Hi, Thanks a lot for your reply. Well the thing is I have already filtered my cells before and after the preprocessing I did imputation on my data using MAGIC and the first clustering was with no warning. I wonder how would it be possible my values have changed because I just subset some clusters and if there was something wrong within some gene values it should have popped up before too because basically I am taking the same genes into consideration for my subset analysis...that is very confusing to me...as long as it doesn't really effect my results I wouldn't mind it but I want to be sure about it and also discover the reason behind it but so far I am clueless",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:471,integrability,sub,subset,471,"@LuckyMD Hi, Thanks a lot for your reply. Well the thing is I have already filtered my cells before and after the preprocessing I did imputation on my data using MAGIC and the first clustering was with no warning. I wonder how would it be possible my values have changed because I just subset some clusters and if there was something wrong within some gene values it should have popped up before too because basically I am taking the same genes into consideration for my subset analysis...that is very confusing to me...as long as it doesn't really effect my results I wouldn't mind it but I want to be sure about it and also discover the reason behind it but so far I am clueless",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:626,integrability,discover,discover,626,"@LuckyMD Hi, Thanks a lot for your reply. Well the thing is I have already filtered my cells before and after the preprocessing I did imputation on my data using MAGIC and the first clustering was with no warning. I wonder how would it be possible my values have changed because I just subset some clusters and if there was something wrong within some gene values it should have popped up before too because basically I am taking the same genes into consideration for my subset analysis...that is very confusing to me...as long as it doesn't really effect my results I wouldn't mind it but I want to be sure about it and also discover the reason behind it but so far I am clueless",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:626,interoperability,discover,discover,626,"@LuckyMD Hi, Thanks a lot for your reply. Well the thing is I have already filtered my cells before and after the preprocessing I did imputation on my data using MAGIC and the first clustering was with no warning. I wonder how would it be possible my values have changed because I just subset some clusters and if there was something wrong within some gene values it should have popped up before too because basically I am taking the same genes into consideration for my subset analysis...that is very confusing to me...as long as it doesn't really effect my results I wouldn't mind it but I want to be sure about it and also discover the reason behind it but so far I am clueless",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:534,reliability,doe,doesn,534,"@LuckyMD Hi, Thanks a lot for your reply. Well the thing is I have already filtered my cells before and after the preprocessing I did imputation on my data using MAGIC and the first clustering was with no warning. I wonder how would it be possible my values have changed because I just subset some clusters and if there was something wrong within some gene values it should have popped up before too because basically I am taking the same genes into consideration for my subset analysis...that is very confusing to me...as long as it doesn't really effect my results I wouldn't mind it but I want to be sure about it and also discover the reason behind it but so far I am clueless",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:626,usability,discov,discover,626,"@LuckyMD Hi, Thanks a lot for your reply. Well the thing is I have already filtered my cells before and after the preprocessing I did imputation on my data using MAGIC and the first clustering was with no warning. I wonder how would it be possible my values have changed because I just subset some clusters and if there was something wrong within some gene values it should have popped up before too because basically I am taking the same genes into consideration for my subset analysis...that is very confusing to me...as long as it doesn't really effect my results I wouldn't mind it but I want to be sure about it and also discover the reason behind it but so far I am clueless",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:114,availability,cluster,cluster,114,"So my idea was the following:. If you have a full dataset and some genes are 0 everywhere, except in the cells in cluster A, then you filter out cluster A in your new dataset, and recompute everything... those genes now have 0 variance in your filtered dataset. That would give you an error that didn't appear before.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:145,availability,cluster,cluster,145,"So my idea was the following:. If you have a full dataset and some genes are 0 everywhere, except in the cells in cluster A, then you filter out cluster A in your new dataset, and recompute everything... those genes now have 0 variance in your filtered dataset. That would give you an error that didn't appear before.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:285,availability,error,error,285,"So my idea was the following:. If you have a full dataset and some genes are 0 everywhere, except in the cells in cluster A, then you filter out cluster A in your new dataset, and recompute everything... those genes now have 0 variance in your filtered dataset. That would give you an error that didn't appear before.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:114,deployability,cluster,cluster,114,"So my idea was the following:. If you have a full dataset and some genes are 0 everywhere, except in the cells in cluster A, then you filter out cluster A in your new dataset, and recompute everything... those genes now have 0 variance in your filtered dataset. That would give you an error that didn't appear before.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:145,deployability,cluster,cluster,145,"So my idea was the following:. If you have a full dataset and some genes are 0 everywhere, except in the cells in cluster A, then you filter out cluster A in your new dataset, and recompute everything... those genes now have 0 variance in your filtered dataset. That would give you an error that didn't appear before.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:134,integrability,filter,filter,134,"So my idea was the following:. If you have a full dataset and some genes are 0 everywhere, except in the cells in cluster A, then you filter out cluster A in your new dataset, and recompute everything... those genes now have 0 variance in your filtered dataset. That would give you an error that didn't appear before.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:244,integrability,filter,filtered,244,"So my idea was the following:. If you have a full dataset and some genes are 0 everywhere, except in the cells in cluster A, then you filter out cluster A in your new dataset, and recompute everything... those genes now have 0 variance in your filtered dataset. That would give you an error that didn't appear before.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:285,performance,error,error,285,"So my idea was the following:. If you have a full dataset and some genes are 0 everywhere, except in the cells in cluster A, then you filter out cluster A in your new dataset, and recompute everything... those genes now have 0 variance in your filtered dataset. That would give you an error that didn't appear before.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:91,safety,except,except,91,"So my idea was the following:. If you have a full dataset and some genes are 0 everywhere, except in the cells in cluster A, then you filter out cluster A in your new dataset, and recompute everything... those genes now have 0 variance in your filtered dataset. That would give you an error that didn't appear before.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:285,safety,error,error,285,"So my idea was the following:. If you have a full dataset and some genes are 0 everywhere, except in the cells in cluster A, then you filter out cluster A in your new dataset, and recompute everything... those genes now have 0 variance in your filtered dataset. That would give you an error that didn't appear before.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:285,usability,error,error,285,"So my idea was the following:. If you have a full dataset and some genes are 0 everywhere, except in the cells in cluster A, then you filter out cluster A in your new dataset, and recompute everything... those genes now have 0 variance in your filtered dataset. That would give you an error that didn't appear before.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:59,integrability,filter,filtered,59,"@LuckyMD Dear Malter, Thanks, now I got your point. I just filtered my cells and genes again using . `sc.pp.filter_cells(adata, min_genes=200). sc.pp.filter_genes(adata, min_cells=3)`. but I still get the same warning message",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:218,integrability,messag,message,218,"@LuckyMD Dear Malter, Thanks, now I got your point. I just filtered my cells and genes again using . `sc.pp.filter_cells(adata, min_genes=200). sc.pp.filter_genes(adata, min_cells=3)`. but I still get the same warning message",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:218,interoperability,messag,message,218,"@LuckyMD Dear Malter, Thanks, now I got your point. I just filtered my cells and genes again using . `sc.pp.filter_cells(adata, min_genes=200). sc.pp.filter_genes(adata, min_cells=3)`. but I still get the same warning message",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:91,reliability,doe,does,91,In that case my idea was wrong and that was not the warning. The invalid value encountered does sound like a `NaN` or a negative value that you get though... maybe check your results and check the code what it could have been. Do you have log2FC values with `NaN`?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:89,deployability,observ,observed,89,"@LuckyMD The thing is after imputation for sure I do get some negative values and I have observed it but such warning was not popping up before and NaN I am doubtful because otherwise I could see a warning message when I ran the imputation for all of my genes. . p.s. This is how I made my subset. > adata_magic_sub=adata_magic[(adata_magic.obs.louvain_04==""3"")|(adata_magic.obs.louvain_04==""7"")|(adata_magic.obs.louvain_04==""8"")|(adata_magic.obs.louvain_04==""11"")]",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:206,integrability,messag,message,206,"@LuckyMD The thing is after imputation for sure I do get some negative values and I have observed it but such warning was not popping up before and NaN I am doubtful because otherwise I could see a warning message when I ran the imputation for all of my genes. . p.s. This is how I made my subset. > adata_magic_sub=adata_magic[(adata_magic.obs.louvain_04==""3"")|(adata_magic.obs.louvain_04==""7"")|(adata_magic.obs.louvain_04==""8"")|(adata_magic.obs.louvain_04==""11"")]",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:290,integrability,sub,subset,290,"@LuckyMD The thing is after imputation for sure I do get some negative values and I have observed it but such warning was not popping up before and NaN I am doubtful because otherwise I could see a warning message when I ran the imputation for all of my genes. . p.s. This is how I made my subset. > adata_magic_sub=adata_magic[(adata_magic.obs.louvain_04==""3"")|(adata_magic.obs.louvain_04==""7"")|(adata_magic.obs.louvain_04==""8"")|(adata_magic.obs.louvain_04==""11"")]",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:206,interoperability,messag,message,206,"@LuckyMD The thing is after imputation for sure I do get some negative values and I have observed it but such warning was not popping up before and NaN I am doubtful because otherwise I could see a warning message when I ran the imputation for all of my genes. . p.s. This is how I made my subset. > adata_magic_sub=adata_magic[(adata_magic.obs.louvain_04==""3"")|(adata_magic.obs.louvain_04==""7"")|(adata_magic.obs.louvain_04==""8"")|(adata_magic.obs.louvain_04==""11"")]",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:89,testability,observ,observed,89,"@LuckyMD The thing is after imputation for sure I do get some negative values and I have observed it but such warning was not popping up before and NaN I am doubtful because otherwise I could see a warning message when I ran the imputation for all of my genes. . p.s. This is how I made my subset. > adata_magic_sub=adata_magic[(adata_magic.obs.louvain_04==""3"")|(adata_magic.obs.louvain_04==""7"")|(adata_magic.obs.louvain_04==""8"")|(adata_magic.obs.louvain_04==""11"")]",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:958,availability,cluster,cluster,958,"I think you are using a view of the anndata object, rather than the object with that method of subsetting. That shouldn't be related to the issue, but if you want to work with the subset, I would use `.copy()` at the end. Also, does this give you the number of cells and genes as intended? I typically put a `:` for the genes to get something like `adata[cell_filter,:].copy()`. Not sure if that's necessary though. So I think the issue is the `foldchanges[global_indices]` that has values that you can't input into `np.log2`. That can be `NaN` or negative values. A fold change is something like `(average expression in condition 1)/(average expression in condition 2)` if expression values can be negative, then one of those values can be negative, giving a negative fold change. I would guess that these testing frameworks don't play well with negative values. . It is likely that this only pops up now, as the average expression value for the e.g., ""not cluster 3"" data is now negative, where before it wasn't as there was different data to average over. If this is the issue, I'm not entirely sure what to do about this... fold changes aren't defined for such a case. I would either:. 1. rescale the data to be between two non-negative values. 2. Set all negative values to 0. You could take the code in the `sc.tl.rank_genes_groups()` function and calculate the fold changes for your genes step by step to see if this is the problem. I assume that it is.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:958,deployability,cluster,cluster,958,"I think you are using a view of the anndata object, rather than the object with that method of subsetting. That shouldn't be related to the issue, but if you want to work with the subset, I would use `.copy()` at the end. Also, does this give you the number of cells and genes as intended? I typically put a `:` for the genes to get something like `adata[cell_filter,:].copy()`. Not sure if that's necessary though. So I think the issue is the `foldchanges[global_indices]` that has values that you can't input into `np.log2`. That can be `NaN` or negative values. A fold change is something like `(average expression in condition 1)/(average expression in condition 2)` if expression values can be negative, then one of those values can be negative, giving a negative fold change. I would guess that these testing frameworks don't play well with negative values. . It is likely that this only pops up now, as the average expression value for the e.g., ""not cluster 3"" data is now negative, where before it wasn't as there was different data to average over. If this is the issue, I'm not entirely sure what to do about this... fold changes aren't defined for such a case. I would either:. 1. rescale the data to be between two non-negative values. 2. Set all negative values to 0. You could take the code in the `sc.tl.rank_genes_groups()` function and calculate the fold changes for your genes step by step to see if this is the problem. I assume that it is.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:95,integrability,sub,subsetting,95,"I think you are using a view of the anndata object, rather than the object with that method of subsetting. That shouldn't be related to the issue, but if you want to work with the subset, I would use `.copy()` at the end. Also, does this give you the number of cells and genes as intended? I typically put a `:` for the genes to get something like `adata[cell_filter,:].copy()`. Not sure if that's necessary though. So I think the issue is the `foldchanges[global_indices]` that has values that you can't input into `np.log2`. That can be `NaN` or negative values. A fold change is something like `(average expression in condition 1)/(average expression in condition 2)` if expression values can be negative, then one of those values can be negative, giving a negative fold change. I would guess that these testing frameworks don't play well with negative values. . It is likely that this only pops up now, as the average expression value for the e.g., ""not cluster 3"" data is now negative, where before it wasn't as there was different data to average over. If this is the issue, I'm not entirely sure what to do about this... fold changes aren't defined for such a case. I would either:. 1. rescale the data to be between two non-negative values. 2. Set all negative values to 0. You could take the code in the `sc.tl.rank_genes_groups()` function and calculate the fold changes for your genes step by step to see if this is the problem. I assume that it is.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:180,integrability,sub,subset,180,"I think you are using a view of the anndata object, rather than the object with that method of subsetting. That shouldn't be related to the issue, but if you want to work with the subset, I would use `.copy()` at the end. Also, does this give you the number of cells and genes as intended? I typically put a `:` for the genes to get something like `adata[cell_filter,:].copy()`. Not sure if that's necessary though. So I think the issue is the `foldchanges[global_indices]` that has values that you can't input into `np.log2`. That can be `NaN` or negative values. A fold change is something like `(average expression in condition 1)/(average expression in condition 2)` if expression values can be negative, then one of those values can be negative, giving a negative fold change. I would guess that these testing frameworks don't play well with negative values. . It is likely that this only pops up now, as the average expression value for the e.g., ""not cluster 3"" data is now negative, where before it wasn't as there was different data to average over. If this is the issue, I'm not entirely sure what to do about this... fold changes aren't defined for such a case. I would either:. 1. rescale the data to be between two non-negative values. 2. Set all negative values to 0. You could take the code in the `sc.tl.rank_genes_groups()` function and calculate the fold changes for your genes step by step to see if this is the problem. I assume that it is.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:228,reliability,doe,does,228,"I think you are using a view of the anndata object, rather than the object with that method of subsetting. That shouldn't be related to the issue, but if you want to work with the subset, I would use `.copy()` at the end. Also, does this give you the number of cells and genes as intended? I typically put a `:` for the genes to get something like `adata[cell_filter,:].copy()`. Not sure if that's necessary though. So I think the issue is the `foldchanges[global_indices]` that has values that you can't input into `np.log2`. That can be `NaN` or negative values. A fold change is something like `(average expression in condition 1)/(average expression in condition 2)` if expression values can be negative, then one of those values can be negative, giving a negative fold change. I would guess that these testing frameworks don't play well with negative values. . It is likely that this only pops up now, as the average expression value for the e.g., ""not cluster 3"" data is now negative, where before it wasn't as there was different data to average over. If this is the issue, I'm not entirely sure what to do about this... fold changes aren't defined for such a case. I would either:. 1. rescale the data to be between two non-negative values. 2. Set all negative values to 0. You could take the code in the `sc.tl.rank_genes_groups()` function and calculate the fold changes for your genes step by step to see if this is the problem. I assume that it is.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:505,safety,input,input,505,"I think you are using a view of the anndata object, rather than the object with that method of subsetting. That shouldn't be related to the issue, but if you want to work with the subset, I would use `.copy()` at the end. Also, does this give you the number of cells and genes as intended? I typically put a `:` for the genes to get something like `adata[cell_filter,:].copy()`. Not sure if that's necessary though. So I think the issue is the `foldchanges[global_indices]` that has values that you can't input into `np.log2`. That can be `NaN` or negative values. A fold change is something like `(average expression in condition 1)/(average expression in condition 2)` if expression values can be negative, then one of those values can be negative, giving a negative fold change. I would guess that these testing frameworks don't play well with negative values. . It is likely that this only pops up now, as the average expression value for the e.g., ""not cluster 3"" data is now negative, where before it wasn't as there was different data to average over. If this is the issue, I'm not entirely sure what to do about this... fold changes aren't defined for such a case. I would either:. 1. rescale the data to be between two non-negative values. 2. Set all negative values to 0. You could take the code in the `sc.tl.rank_genes_groups()` function and calculate the fold changes for your genes step by step to see if this is the problem. I assume that it is.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:807,safety,test,testing,807,"I think you are using a view of the anndata object, rather than the object with that method of subsetting. That shouldn't be related to the issue, but if you want to work with the subset, I would use `.copy()` at the end. Also, does this give you the number of cells and genes as intended? I typically put a `:` for the genes to get something like `adata[cell_filter,:].copy()`. Not sure if that's necessary though. So I think the issue is the `foldchanges[global_indices]` that has values that you can't input into `np.log2`. That can be `NaN` or negative values. A fold change is something like `(average expression in condition 1)/(average expression in condition 2)` if expression values can be negative, then one of those values can be negative, giving a negative fold change. I would guess that these testing frameworks don't play well with negative values. . It is likely that this only pops up now, as the average expression value for the e.g., ""not cluster 3"" data is now negative, where before it wasn't as there was different data to average over. If this is the issue, I'm not entirely sure what to do about this... fold changes aren't defined for such a case. I would either:. 1. rescale the data to be between two non-negative values. 2. Set all negative values to 0. You could take the code in the `sc.tl.rank_genes_groups()` function and calculate the fold changes for your genes step by step to see if this is the problem. I assume that it is.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:807,testability,test,testing,807,"I think you are using a view of the anndata object, rather than the object with that method of subsetting. That shouldn't be related to the issue, but if you want to work with the subset, I would use `.copy()` at the end. Also, does this give you the number of cells and genes as intended? I typically put a `:` for the genes to get something like `adata[cell_filter,:].copy()`. Not sure if that's necessary though. So I think the issue is the `foldchanges[global_indices]` that has values that you can't input into `np.log2`. That can be `NaN` or negative values. A fold change is something like `(average expression in condition 1)/(average expression in condition 2)` if expression values can be negative, then one of those values can be negative, giving a negative fold change. I would guess that these testing frameworks don't play well with negative values. . It is likely that this only pops up now, as the average expression value for the e.g., ""not cluster 3"" data is now negative, where before it wasn't as there was different data to average over. If this is the issue, I'm not entirely sure what to do about this... fold changes aren't defined for such a case. I would either:. 1. rescale the data to be between two non-negative values. 2. Set all negative values to 0. You could take the code in the `sc.tl.rank_genes_groups()` function and calculate the fold changes for your genes step by step to see if this is the problem. I assume that it is.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:505,usability,input,input,505,"I think you are using a view of the anndata object, rather than the object with that method of subsetting. That shouldn't be related to the issue, but if you want to work with the subset, I would use `.copy()` at the end. Also, does this give you the number of cells and genes as intended? I typically put a `:` for the genes to get something like `adata[cell_filter,:].copy()`. Not sure if that's necessary though. So I think the issue is the `foldchanges[global_indices]` that has values that you can't input into `np.log2`. That can be `NaN` or negative values. A fold change is something like `(average expression in condition 1)/(average expression in condition 2)` if expression values can be negative, then one of those values can be negative, giving a negative fold change. I would guess that these testing frameworks don't play well with negative values. . It is likely that this only pops up now, as the average expression value for the e.g., ""not cluster 3"" data is now negative, where before it wasn't as there was different data to average over. If this is the issue, I'm not entirely sure what to do about this... fold changes aren't defined for such a case. I would either:. 1. rescale the data to be between two non-negative values. 2. Set all negative values to 0. You could take the code in the `sc.tl.rank_genes_groups()` function and calculate the fold changes for your genes step by step to see if this is the problem. I assume that it is.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:562,availability,down,downstream,562,"@LuckyMD Dear Malte, Thanks a lot for your hint and reply. Regarding to the subset, well I got actually the cells that I needed with same number of the genes that I had before so I assume it is fine. I did a rescale of my data to 10 again but unfortunately the same warning is happening! . I don't know really if turning all negative values to zero would really make sense because first of all as I mentioned I had negative values before and it was not a problem and if I turn all those negative values to zero I guess I will lose quite a bit of genes during my downstream analysis. What I can imagine is those problematic values are anyway not considered during the marker analysis so in the case they are NaN turning them to zero wouldn't affect my result. my fear was mainly that those genes may be really something and due to a bug or a miscommand I am not getting them but I think it is a rare probability. I guess I should leave it as it is. I will though try to take the sc.tl.rank_genes_groups function code and run it step by step",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:76,integrability,sub,subset,76,"@LuckyMD Dear Malte, Thanks a lot for your hint and reply. Regarding to the subset, well I got actually the cells that I needed with same number of the genes that I had before so I assume it is fine. I did a rescale of my data to 10 again but unfortunately the same warning is happening! . I don't know really if turning all negative values to zero would really make sense because first of all as I mentioned I had negative values before and it was not a problem and if I turn all those negative values to zero I guess I will lose quite a bit of genes during my downstream analysis. What I can imagine is those problematic values are anyway not considered during the marker analysis so in the case they are NaN turning them to zero wouldn't affect my result. my fear was mainly that those genes may be really something and due to a bug or a miscommand I am not getting them but I think it is a rare probability. I guess I should leave it as it is. I will though try to take the sc.tl.rank_genes_groups function code and run it step by step",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:43,usability,hint,hint,43,"@LuckyMD Dear Malte, Thanks a lot for your hint and reply. Regarding to the subset, well I got actually the cells that I needed with same number of the genes that I had before so I assume it is fine. I did a rescale of my data to 10 again but unfortunately the same warning is happening! . I don't know really if turning all negative values to zero would really make sense because first of all as I mentioned I had negative values before and it was not a problem and if I turn all those negative values to zero I guess I will lose quite a bit of genes during my downstream analysis. What I can imagine is those problematic values are anyway not considered during the marker analysis so in the case they are NaN turning them to zero wouldn't affect my result. my fear was mainly that those genes may be really something and due to a bug or a miscommand I am not getting them but I think it is a rare probability. I guess I should leave it as it is. I will though try to take the sc.tl.rank_genes_groups function code and run it step by step",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:160,availability,cluster,cluster,160,"So the reason you didn't get this before, would be that if you do a `sc.tl.rank_genes_groups()` call with default parameters, you compare the expression in one cluster with the expression in the rest of the dataset. The ""rest of the dataset"" has changed, so you could now get -ve average expression for genes in the ""rest of the dataset"". This will likely create -ve fold changes, which cannot be logged and probably give you `NaN`. This is hiding a signal that is actually there, and not because the gene is not actually differentially expressed. > I did a rescale of my data to 10 again but unfortunately the same warning is happening! What do you mean by this? Turning negative values to 0, doesn't mean you lose the data. You have some expression space, of which 0 is a valid number. The question is really what does a negative expression value mean after MAGIC? Is it just a confidence of the gene not being expressed? Then putting it to zero makes sense. Again... if you ignore this, you will just ignore particular genes which are likely differentially expression, because MAGIC has rescaled the expression values in the ""rest of the dataset"" to a negative value.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:160,deployability,cluster,cluster,160,"So the reason you didn't get this before, would be that if you do a `sc.tl.rank_genes_groups()` call with default parameters, you compare the expression in one cluster with the expression in the rest of the dataset. The ""rest of the dataset"" has changed, so you could now get -ve average expression for genes in the ""rest of the dataset"". This will likely create -ve fold changes, which cannot be logged and probably give you `NaN`. This is hiding a signal that is actually there, and not because the gene is not actually differentially expressed. > I did a rescale of my data to 10 again but unfortunately the same warning is happening! What do you mean by this? Turning negative values to 0, doesn't mean you lose the data. You have some expression space, of which 0 is a valid number. The question is really what does a negative expression value mean after MAGIC? Is it just a confidence of the gene not being expressed? Then putting it to zero makes sense. Again... if you ignore this, you will just ignore particular genes which are likely differentially expression, because MAGIC has rescaled the expression values in the ""rest of the dataset"" to a negative value.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:397,deployability,log,logged,397,"So the reason you didn't get this before, would be that if you do a `sc.tl.rank_genes_groups()` call with default parameters, you compare the expression in one cluster with the expression in the rest of the dataset. The ""rest of the dataset"" has changed, so you could now get -ve average expression for genes in the ""rest of the dataset"". This will likely create -ve fold changes, which cannot be logged and probably give you `NaN`. This is hiding a signal that is actually there, and not because the gene is not actually differentially expressed. > I did a rescale of my data to 10 again but unfortunately the same warning is happening! What do you mean by this? Turning negative values to 0, doesn't mean you lose the data. You have some expression space, of which 0 is a valid number. The question is really what does a negative expression value mean after MAGIC? Is it just a confidence of the gene not being expressed? Then putting it to zero makes sense. Again... if you ignore this, you will just ignore particular genes which are likely differentially expression, because MAGIC has rescaled the expression values in the ""rest of the dataset"" to a negative value.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:114,modifiability,paramet,parameters,114,"So the reason you didn't get this before, would be that if you do a `sc.tl.rank_genes_groups()` call with default parameters, you compare the expression in one cluster with the expression in the rest of the dataset. The ""rest of the dataset"" has changed, so you could now get -ve average expression for genes in the ""rest of the dataset"". This will likely create -ve fold changes, which cannot be logged and probably give you `NaN`. This is hiding a signal that is actually there, and not because the gene is not actually differentially expressed. > I did a rescale of my data to 10 again but unfortunately the same warning is happening! What do you mean by this? Turning negative values to 0, doesn't mean you lose the data. You have some expression space, of which 0 is a valid number. The question is really what does a negative expression value mean after MAGIC? Is it just a confidence of the gene not being expressed? Then putting it to zero makes sense. Again... if you ignore this, you will just ignore particular genes which are likely differentially expression, because MAGIC has rescaled the expression values in the ""rest of the dataset"" to a negative value.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:694,reliability,doe,doesn,694,"So the reason you didn't get this before, would be that if you do a `sc.tl.rank_genes_groups()` call with default parameters, you compare the expression in one cluster with the expression in the rest of the dataset. The ""rest of the dataset"" has changed, so you could now get -ve average expression for genes in the ""rest of the dataset"". This will likely create -ve fold changes, which cannot be logged and probably give you `NaN`. This is hiding a signal that is actually there, and not because the gene is not actually differentially expressed. > I did a rescale of my data to 10 again but unfortunately the same warning is happening! What do you mean by this? Turning negative values to 0, doesn't mean you lose the data. You have some expression space, of which 0 is a valid number. The question is really what does a negative expression value mean after MAGIC? Is it just a confidence of the gene not being expressed? Then putting it to zero makes sense. Again... if you ignore this, you will just ignore particular genes which are likely differentially expression, because MAGIC has rescaled the expression values in the ""rest of the dataset"" to a negative value.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:816,reliability,doe,does,816,"So the reason you didn't get this before, would be that if you do a `sc.tl.rank_genes_groups()` call with default parameters, you compare the expression in one cluster with the expression in the rest of the dataset. The ""rest of the dataset"" has changed, so you could now get -ve average expression for genes in the ""rest of the dataset"". This will likely create -ve fold changes, which cannot be logged and probably give you `NaN`. This is hiding a signal that is actually there, and not because the gene is not actually differentially expressed. > I did a rescale of my data to 10 again but unfortunately the same warning is happening! What do you mean by this? Turning negative values to 0, doesn't mean you lose the data. You have some expression space, of which 0 is a valid number. The question is really what does a negative expression value mean after MAGIC? Is it just a confidence of the gene not being expressed? Then putting it to zero makes sense. Again... if you ignore this, you will just ignore particular genes which are likely differentially expression, because MAGIC has rescaled the expression values in the ""rest of the dataset"" to a negative value.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:397,safety,log,logged,397,"So the reason you didn't get this before, would be that if you do a `sc.tl.rank_genes_groups()` call with default parameters, you compare the expression in one cluster with the expression in the rest of the dataset. The ""rest of the dataset"" has changed, so you could now get -ve average expression for genes in the ""rest of the dataset"". This will likely create -ve fold changes, which cannot be logged and probably give you `NaN`. This is hiding a signal that is actually there, and not because the gene is not actually differentially expressed. > I did a rescale of my data to 10 again but unfortunately the same warning is happening! What do you mean by this? Turning negative values to 0, doesn't mean you lose the data. You have some expression space, of which 0 is a valid number. The question is really what does a negative expression value mean after MAGIC? Is it just a confidence of the gene not being expressed? Then putting it to zero makes sense. Again... if you ignore this, you will just ignore particular genes which are likely differentially expression, because MAGIC has rescaled the expression values in the ""rest of the dataset"" to a negative value.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:774,safety,valid,valid,774,"So the reason you didn't get this before, would be that if you do a `sc.tl.rank_genes_groups()` call with default parameters, you compare the expression in one cluster with the expression in the rest of the dataset. The ""rest of the dataset"" has changed, so you could now get -ve average expression for genes in the ""rest of the dataset"". This will likely create -ve fold changes, which cannot be logged and probably give you `NaN`. This is hiding a signal that is actually there, and not because the gene is not actually differentially expressed. > I did a rescale of my data to 10 again but unfortunately the same warning is happening! What do you mean by this? Turning negative values to 0, doesn't mean you lose the data. You have some expression space, of which 0 is a valid number. The question is really what does a negative expression value mean after MAGIC? Is it just a confidence of the gene not being expressed? Then putting it to zero makes sense. Again... if you ignore this, you will just ignore particular genes which are likely differentially expression, because MAGIC has rescaled the expression values in the ""rest of the dataset"" to a negative value.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:397,security,log,logged,397,"So the reason you didn't get this before, would be that if you do a `sc.tl.rank_genes_groups()` call with default parameters, you compare the expression in one cluster with the expression in the rest of the dataset. The ""rest of the dataset"" has changed, so you could now get -ve average expression for genes in the ""rest of the dataset"". This will likely create -ve fold changes, which cannot be logged and probably give you `NaN`. This is hiding a signal that is actually there, and not because the gene is not actually differentially expressed. > I did a rescale of my data to 10 again but unfortunately the same warning is happening! What do you mean by this? Turning negative values to 0, doesn't mean you lose the data. You have some expression space, of which 0 is a valid number. The question is really what does a negative expression value mean after MAGIC? Is it just a confidence of the gene not being expressed? Then putting it to zero makes sense. Again... if you ignore this, you will just ignore particular genes which are likely differentially expression, because MAGIC has rescaled the expression values in the ""rest of the dataset"" to a negative value.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:450,security,sign,signal,450,"So the reason you didn't get this before, would be that if you do a `sc.tl.rank_genes_groups()` call with default parameters, you compare the expression in one cluster with the expression in the rest of the dataset. The ""rest of the dataset"" has changed, so you could now get -ve average expression for genes in the ""rest of the dataset"". This will likely create -ve fold changes, which cannot be logged and probably give you `NaN`. This is hiding a signal that is actually there, and not because the gene is not actually differentially expressed. > I did a rescale of my data to 10 again but unfortunately the same warning is happening! What do you mean by this? Turning negative values to 0, doesn't mean you lose the data. You have some expression space, of which 0 is a valid number. The question is really what does a negative expression value mean after MAGIC? Is it just a confidence of the gene not being expressed? Then putting it to zero makes sense. Again... if you ignore this, you will just ignore particular genes which are likely differentially expression, because MAGIC has rescaled the expression values in the ""rest of the dataset"" to a negative value.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:397,testability,log,logged,397,"So the reason you didn't get this before, would be that if you do a `sc.tl.rank_genes_groups()` call with default parameters, you compare the expression in one cluster with the expression in the rest of the dataset. The ""rest of the dataset"" has changed, so you could now get -ve average expression for genes in the ""rest of the dataset"". This will likely create -ve fold changes, which cannot be logged and probably give you `NaN`. This is hiding a signal that is actually there, and not because the gene is not actually differentially expressed. > I did a rescale of my data to 10 again but unfortunately the same warning is happening! What do you mean by this? Turning negative values to 0, doesn't mean you lose the data. You have some expression space, of which 0 is a valid number. The question is really what does a negative expression value mean after MAGIC? Is it just a confidence of the gene not being expressed? Then putting it to zero makes sense. Again... if you ignore this, you will just ignore particular genes which are likely differentially expression, because MAGIC has rescaled the expression values in the ""rest of the dataset"" to a negative value.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:50,deployability,scale,scale,50,"@LuckyMD by rescaling to 10 I meant this. > sc.pp.scale(adata_magic, max_value=10). And regarding to the negative values in MAGIC, this is what one the creators has mentioned about it . > The negative values are an artifact of the imputation process, but the absolute values of expression are not really important, since normalized scRNAseq data is only really a measure of relative expression anyway. TBH I am not sure if being an artifact would mean that it's ok to put them into zero. Would it be still your suggestion ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:215,deployability,artifact,artifact,215,"@LuckyMD by rescaling to 10 I meant this. > sc.pp.scale(adata_magic, max_value=10). And regarding to the negative values in MAGIC, this is what one the creators has mentioned about it . > The negative values are an artifact of the imputation process, but the absolute values of expression are not really important, since normalized scRNAseq data is only really a measure of relative expression anyway. TBH I am not sure if being an artifact would mean that it's ok to put them into zero. Would it be still your suggestion ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:432,deployability,artifact,artifact,432,"@LuckyMD by rescaling to 10 I meant this. > sc.pp.scale(adata_magic, max_value=10). And regarding to the negative values in MAGIC, this is what one the creators has mentioned about it . > The negative values are an artifact of the imputation process, but the absolute values of expression are not really important, since normalized scRNAseq data is only really a measure of relative expression anyway. TBH I am not sure if being an artifact would mean that it's ok to put them into zero. Would it be still your suggestion ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:50,energy efficiency,scale,scale,50,"@LuckyMD by rescaling to 10 I meant this. > sc.pp.scale(adata_magic, max_value=10). And regarding to the negative values in MAGIC, this is what one the creators has mentioned about it . > The negative values are an artifact of the imputation process, but the absolute values of expression are not really important, since normalized scRNAseq data is only really a measure of relative expression anyway. TBH I am not sure if being an artifact would mean that it's ok to put them into zero. Would it be still your suggestion ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:363,energy efficiency,measur,measure,363,"@LuckyMD by rescaling to 10 I meant this. > sc.pp.scale(adata_magic, max_value=10). And regarding to the negative values in MAGIC, this is what one the creators has mentioned about it . > The negative values are an artifact of the imputation process, but the absolute values of expression are not really important, since normalized scRNAseq data is only really a measure of relative expression anyway. TBH I am not sure if being an artifact would mean that it's ok to put them into zero. Would it be still your suggestion ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:50,modifiability,scal,scale,50,"@LuckyMD by rescaling to 10 I meant this. > sc.pp.scale(adata_magic, max_value=10). And regarding to the negative values in MAGIC, this is what one the creators has mentioned about it . > The negative values are an artifact of the imputation process, but the absolute values of expression are not really important, since normalized scRNAseq data is only really a measure of relative expression anyway. TBH I am not sure if being an artifact would mean that it's ok to put them into zero. Would it be still your suggestion ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:50,performance,scale,scale,50,"@LuckyMD by rescaling to 10 I meant this. > sc.pp.scale(adata_magic, max_value=10). And regarding to the negative values in MAGIC, this is what one the creators has mentioned about it . > The negative values are an artifact of the imputation process, but the absolute values of expression are not really important, since normalized scRNAseq data is only really a measure of relative expression anyway. TBH I am not sure if being an artifact would mean that it's ok to put them into zero. Would it be still your suggestion ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:101,deployability,scale,scale,101,"I guess if they regard the relative values as accurate, then scaling is the way forward. Does `sc.pp.scale()` leave any negative values?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:101,energy efficiency,scale,scale,101,"I guess if they regard the relative values as accurate, then scaling is the way forward. Does `sc.pp.scale()` leave any negative values?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:61,modifiability,scal,scaling,61,"I guess if they regard the relative values as accurate, then scaling is the way forward. Does `sc.pp.scale()` leave any negative values?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:101,modifiability,scal,scale,101,"I guess if they regard the relative values as accurate, then scaling is the way forward. Does `sc.pp.scale()` leave any negative values?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:101,performance,scale,scale,101,"I guess if they regard the relative values as accurate, then scaling is the way forward. Does `sc.pp.scale()` leave any negative values?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:89,reliability,Doe,Does,89,"I guess if they regard the relative values as accurate, then scaling is the way forward. Does `sc.pp.scale()` leave any negative values?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:50,modifiability,scal,scaling,50,"@LuckyMD yes, one get still negative values after scaling",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:180,availability,cluster,clusters,180,@LuckyMD Ok so I basically found the reason of it. it's simply the scaling! What I used to do is that after imputation and selecting HVG I was scaling my data and then getting the clusters and subsetting and running hvg agaian. But apparently the scaled data cause the warning but I am not sure why ! because it doesn't turn any of my expression to NaN when I checked my adata.X and negative values shouldn't be the source of the warning too because I already had negative values after imputation and didn't cause any warning. One thing I also should mention is that this warning happens too if I compute the HVG so its not really cluster specific or so,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:631,availability,cluster,cluster,631,@LuckyMD Ok so I basically found the reason of it. it's simply the scaling! What I used to do is that after imputation and selecting HVG I was scaling my data and then getting the clusters and subsetting and running hvg agaian. But apparently the scaled data cause the warning but I am not sure why ! because it doesn't turn any of my expression to NaN when I checked my adata.X and negative values shouldn't be the source of the warning too because I already had negative values after imputation and didn't cause any warning. One thing I also should mention is that this warning happens too if I compute the HVG so its not really cluster specific or so,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:180,deployability,cluster,clusters,180,@LuckyMD Ok so I basically found the reason of it. it's simply the scaling! What I used to do is that after imputation and selecting HVG I was scaling my data and then getting the clusters and subsetting and running hvg agaian. But apparently the scaled data cause the warning but I am not sure why ! because it doesn't turn any of my expression to NaN when I checked my adata.X and negative values shouldn't be the source of the warning too because I already had negative values after imputation and didn't cause any warning. One thing I also should mention is that this warning happens too if I compute the HVG so its not really cluster specific or so,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:247,deployability,scale,scaled,247,@LuckyMD Ok so I basically found the reason of it. it's simply the scaling! What I used to do is that after imputation and selecting HVG I was scaling my data and then getting the clusters and subsetting and running hvg agaian. But apparently the scaled data cause the warning but I am not sure why ! because it doesn't turn any of my expression to NaN when I checked my adata.X and negative values shouldn't be the source of the warning too because I already had negative values after imputation and didn't cause any warning. One thing I also should mention is that this warning happens too if I compute the HVG so its not really cluster specific or so,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:631,deployability,cluster,cluster,631,@LuckyMD Ok so I basically found the reason of it. it's simply the scaling! What I used to do is that after imputation and selecting HVG I was scaling my data and then getting the clusters and subsetting and running hvg agaian. But apparently the scaled data cause the warning but I am not sure why ! because it doesn't turn any of my expression to NaN when I checked my adata.X and negative values shouldn't be the source of the warning too because I already had negative values after imputation and didn't cause any warning. One thing I also should mention is that this warning happens too if I compute the HVG so its not really cluster specific or so,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:247,energy efficiency,scale,scaled,247,@LuckyMD Ok so I basically found the reason of it. it's simply the scaling! What I used to do is that after imputation and selecting HVG I was scaling my data and then getting the clusters and subsetting and running hvg agaian. But apparently the scaled data cause the warning but I am not sure why ! because it doesn't turn any of my expression to NaN when I checked my adata.X and negative values shouldn't be the source of the warning too because I already had negative values after imputation and didn't cause any warning. One thing I also should mention is that this warning happens too if I compute the HVG so its not really cluster specific or so,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:193,integrability,sub,subsetting,193,@LuckyMD Ok so I basically found the reason of it. it's simply the scaling! What I used to do is that after imputation and selecting HVG I was scaling my data and then getting the clusters and subsetting and running hvg agaian. But apparently the scaled data cause the warning but I am not sure why ! because it doesn't turn any of my expression to NaN when I checked my adata.X and negative values shouldn't be the source of the warning too because I already had negative values after imputation and didn't cause any warning. One thing I also should mention is that this warning happens too if I compute the HVG so its not really cluster specific or so,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:639,interoperability,specif,specific,639,@LuckyMD Ok so I basically found the reason of it. it's simply the scaling! What I used to do is that after imputation and selecting HVG I was scaling my data and then getting the clusters and subsetting and running hvg agaian. But apparently the scaled data cause the warning but I am not sure why ! because it doesn't turn any of my expression to NaN when I checked my adata.X and negative values shouldn't be the source of the warning too because I already had negative values after imputation and didn't cause any warning. One thing I also should mention is that this warning happens too if I compute the HVG so its not really cluster specific or so,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:67,modifiability,scal,scaling,67,@LuckyMD Ok so I basically found the reason of it. it's simply the scaling! What I used to do is that after imputation and selecting HVG I was scaling my data and then getting the clusters and subsetting and running hvg agaian. But apparently the scaled data cause the warning but I am not sure why ! because it doesn't turn any of my expression to NaN when I checked my adata.X and negative values shouldn't be the source of the warning too because I already had negative values after imputation and didn't cause any warning. One thing I also should mention is that this warning happens too if I compute the HVG so its not really cluster specific or so,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:143,modifiability,scal,scaling,143,@LuckyMD Ok so I basically found the reason of it. it's simply the scaling! What I used to do is that after imputation and selecting HVG I was scaling my data and then getting the clusters and subsetting and running hvg agaian. But apparently the scaled data cause the warning but I am not sure why ! because it doesn't turn any of my expression to NaN when I checked my adata.X and negative values shouldn't be the source of the warning too because I already had negative values after imputation and didn't cause any warning. One thing I also should mention is that this warning happens too if I compute the HVG so its not really cluster specific or so,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:247,modifiability,scal,scaled,247,@LuckyMD Ok so I basically found the reason of it. it's simply the scaling! What I used to do is that after imputation and selecting HVG I was scaling my data and then getting the clusters and subsetting and running hvg agaian. But apparently the scaled data cause the warning but I am not sure why ! because it doesn't turn any of my expression to NaN when I checked my adata.X and negative values shouldn't be the source of the warning too because I already had negative values after imputation and didn't cause any warning. One thing I also should mention is that this warning happens too if I compute the HVG so its not really cluster specific or so,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:247,performance,scale,scaled,247,@LuckyMD Ok so I basically found the reason of it. it's simply the scaling! What I used to do is that after imputation and selecting HVG I was scaling my data and then getting the clusters and subsetting and running hvg agaian. But apparently the scaled data cause the warning but I am not sure why ! because it doesn't turn any of my expression to NaN when I checked my adata.X and negative values shouldn't be the source of the warning too because I already had negative values after imputation and didn't cause any warning. One thing I also should mention is that this warning happens too if I compute the HVG so its not really cluster specific or so,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:312,reliability,doe,doesn,312,@LuckyMD Ok so I basically found the reason of it. it's simply the scaling! What I used to do is that after imputation and selecting HVG I was scaling my data and then getting the clusters and subsetting and running hvg agaian. But apparently the scaled data cause the warning but I am not sure why ! because it doesn't turn any of my expression to NaN when I checked my adata.X and negative values shouldn't be the source of the warning too because I already had negative values after imputation and didn't cause any warning. One thing I also should mention is that this warning happens too if I compute the HVG so its not really cluster specific or so,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:56,testability,simpl,simply,56,@LuckyMD Ok so I basically found the reason of it. it's simply the scaling! What I used to do is that after imputation and selecting HVG I was scaling my data and then getting the clusters and subsetting and running hvg agaian. But apparently the scaled data cause the warning but I am not sure why ! because it doesn't turn any of my expression to NaN when I checked my adata.X and negative values shouldn't be the source of the warning too because I already had negative values after imputation and didn't cause any warning. One thing I also should mention is that this warning happens too if I compute the HVG so its not really cluster specific or so,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:56,usability,simpl,simply,56,@LuckyMD Ok so I basically found the reason of it. it's simply the scaling! What I used to do is that after imputation and selecting HVG I was scaling my data and then getting the clusters and subsetting and running hvg agaian. But apparently the scaled data cause the warning but I am not sure why ! because it doesn't turn any of my expression to NaN when I checked my adata.X and negative values shouldn't be the source of the warning too because I already had negative values after imputation and didn't cause any warning. One thing I also should mention is that this warning happens too if I compute the HVG so its not really cluster specific or so,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:588,availability,error,error,588,"> So I think the issue is the `foldchanges[global_indices]` that has values that you can't input into `np.log2`. That can be `NaN` or negative values. A fold change is something like `(average expression in condition 1)/(average expression in condition 2)` if expression values can be negative, then one of those values can be negative, giving a negative fold change. I would guess that these testing frameworks don't play well with negative values. I believe this is the reason why it happens. If one of those two averages are negative, then your fold change is negative, and you get an error when feeding that into `np.log2()`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:588,performance,error,error,588,"> So I think the issue is the `foldchanges[global_indices]` that has values that you can't input into `np.log2`. That can be `NaN` or negative values. A fold change is something like `(average expression in condition 1)/(average expression in condition 2)` if expression values can be negative, then one of those values can be negative, giving a negative fold change. I would guess that these testing frameworks don't play well with negative values. I believe this is the reason why it happens. If one of those two averages are negative, then your fold change is negative, and you get an error when feeding that into `np.log2()`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:91,safety,input,input,91,"> So I think the issue is the `foldchanges[global_indices]` that has values that you can't input into `np.log2`. That can be `NaN` or negative values. A fold change is something like `(average expression in condition 1)/(average expression in condition 2)` if expression values can be negative, then one of those values can be negative, giving a negative fold change. I would guess that these testing frameworks don't play well with negative values. I believe this is the reason why it happens. If one of those two averages are negative, then your fold change is negative, and you get an error when feeding that into `np.log2()`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:393,safety,test,testing,393,"> So I think the issue is the `foldchanges[global_indices]` that has values that you can't input into `np.log2`. That can be `NaN` or negative values. A fold change is something like `(average expression in condition 1)/(average expression in condition 2)` if expression values can be negative, then one of those values can be negative, giving a negative fold change. I would guess that these testing frameworks don't play well with negative values. I believe this is the reason why it happens. If one of those two averages are negative, then your fold change is negative, and you get an error when feeding that into `np.log2()`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:588,safety,error,error,588,"> So I think the issue is the `foldchanges[global_indices]` that has values that you can't input into `np.log2`. That can be `NaN` or negative values. A fold change is something like `(average expression in condition 1)/(average expression in condition 2)` if expression values can be negative, then one of those values can be negative, giving a negative fold change. I would guess that these testing frameworks don't play well with negative values. I believe this is the reason why it happens. If one of those two averages are negative, then your fold change is negative, and you get an error when feeding that into `np.log2()`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:393,testability,test,testing,393,"> So I think the issue is the `foldchanges[global_indices]` that has values that you can't input into `np.log2`. That can be `NaN` or negative values. A fold change is something like `(average expression in condition 1)/(average expression in condition 2)` if expression values can be negative, then one of those values can be negative, giving a negative fold change. I would guess that these testing frameworks don't play well with negative values. I believe this is the reason why it happens. If one of those two averages are negative, then your fold change is negative, and you get an error when feeding that into `np.log2()`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:91,usability,input,input,91,"> So I think the issue is the `foldchanges[global_indices]` that has values that you can't input into `np.log2`. That can be `NaN` or negative values. A fold change is something like `(average expression in condition 1)/(average expression in condition 2)` if expression values can be negative, then one of those values can be negative, giving a negative fold change. I would guess that these testing frameworks don't play well with negative values. I believe this is the reason why it happens. If one of those two averages are negative, then your fold change is negative, and you get an error when feeding that into `np.log2()`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:588,usability,error,error,588,"> So I think the issue is the `foldchanges[global_indices]` that has values that you can't input into `np.log2`. That can be `NaN` or negative values. A fold change is something like `(average expression in condition 1)/(average expression in condition 2)` if expression values can be negative, then one of those values can be negative, giving a negative fold change. I would guess that these testing frameworks don't play well with negative values. I believe this is the reason why it happens. If one of those two averages are negative, then your fold change is negative, and you get an error when feeding that into `np.log2()`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:255,availability,down,downstream,255,@LuckyMD yea after some further investigation I do agree with you! do you already know a method which can I use to scale my data to non-negative values? then I can scale my data only once and right after imputation and that should be fine for the rest of downstream analysis including the sub-clustering,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:293,availability,cluster,clustering,293,@LuckyMD yea after some further investigation I do agree with you! do you already know a method which can I use to scale my data to non-negative values? then I can scale my data only once and right after imputation and that should be fine for the rest of downstream analysis including the sub-clustering,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:115,deployability,scale,scale,115,@LuckyMD yea after some further investigation I do agree with you! do you already know a method which can I use to scale my data to non-negative values? then I can scale my data only once and right after imputation and that should be fine for the rest of downstream analysis including the sub-clustering,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:164,deployability,scale,scale,164,@LuckyMD yea after some further investigation I do agree with you! do you already know a method which can I use to scale my data to non-negative values? then I can scale my data only once and right after imputation and that should be fine for the rest of downstream analysis including the sub-clustering,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:293,deployability,cluster,clustering,293,@LuckyMD yea after some further investigation I do agree with you! do you already know a method which can I use to scale my data to non-negative values? then I can scale my data only once and right after imputation and that should be fine for the rest of downstream analysis including the sub-clustering,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:115,energy efficiency,scale,scale,115,@LuckyMD yea after some further investigation I do agree with you! do you already know a method which can I use to scale my data to non-negative values? then I can scale my data only once and right after imputation and that should be fine for the rest of downstream analysis including the sub-clustering,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:164,energy efficiency,scale,scale,164,@LuckyMD yea after some further investigation I do agree with you! do you already know a method which can I use to scale my data to non-negative values? then I can scale my data only once and right after imputation and that should be fine for the rest of downstream analysis including the sub-clustering,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:289,integrability,sub,sub-clustering,289,@LuckyMD yea after some further investigation I do agree with you! do you already know a method which can I use to scale my data to non-negative values? then I can scale my data only once and right after imputation and that should be fine for the rest of downstream analysis including the sub-clustering,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:115,modifiability,scal,scale,115,@LuckyMD yea after some further investigation I do agree with you! do you already know a method which can I use to scale my data to non-negative values? then I can scale my data only once and right after imputation and that should be fine for the rest of downstream analysis including the sub-clustering,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:164,modifiability,scal,scale,164,@LuckyMD yea after some further investigation I do agree with you! do you already know a method which can I use to scale my data to non-negative values? then I can scale my data only once and right after imputation and that should be fine for the rest of downstream analysis including the sub-clustering,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:115,performance,scale,scale,115,@LuckyMD yea after some further investigation I do agree with you! do you already know a method which can I use to scale my data to non-negative values? then I can scale my data only once and right after imputation and that should be fine for the rest of downstream analysis including the sub-clustering,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:164,performance,scale,scale,164,@LuckyMD yea after some further investigation I do agree with you! do you already know a method which can I use to scale my data to non-negative values? then I can scale my data only once and right after imputation and that should be fine for the rest of downstream analysis including the sub-clustering,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:107,deployability,scale,scale,107,"I guess negative values can mean different things across imputation methods. So having one standard way to scale is maybe not the best approach. That being said, I would probably simply do this:. ![CodeCogsEqn](https://user-images.githubusercontent.com/13019956/58164949-15390b80-7c87-11e9-9534-65ddf492ebd5.gif). Here, you would also put expression values to 0 if all expression values are +ve and non-zero. Otherwise, you should only do this for genes where the min() is -ve. The above scaling solution would keep the relative scale between the genes. If you however prefer to scale to values between 0 and 1 (which I usually don't do, but others advocate; this would ensure equal weighting between genes for PCA), you can also rescale by expression range like this:. ![CodeCogsEqn(1)](https://user-images.githubusercontent.com/13019956/58165629-6dbcd880-7c88-11e9-8c29-d3b2684b7bb7.gif). Overall though, I'm not a big fan of imputation... especially after this [paper](https://f1000research.com/articles/7-1740/v1)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:529,deployability,scale,scale,529,"I guess negative values can mean different things across imputation methods. So having one standard way to scale is maybe not the best approach. That being said, I would probably simply do this:. ![CodeCogsEqn](https://user-images.githubusercontent.com/13019956/58164949-15390b80-7c87-11e9-9534-65ddf492ebd5.gif). Here, you would also put expression values to 0 if all expression values are +ve and non-zero. Otherwise, you should only do this for genes where the min() is -ve. The above scaling solution would keep the relative scale between the genes. If you however prefer to scale to values between 0 and 1 (which I usually don't do, but others advocate; this would ensure equal weighting between genes for PCA), you can also rescale by expression range like this:. ![CodeCogsEqn(1)](https://user-images.githubusercontent.com/13019956/58165629-6dbcd880-7c88-11e9-8c29-d3b2684b7bb7.gif). Overall though, I'm not a big fan of imputation... especially after this [paper](https://f1000research.com/articles/7-1740/v1)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:579,deployability,scale,scale,579,"I guess negative values can mean different things across imputation methods. So having one standard way to scale is maybe not the best approach. That being said, I would probably simply do this:. ![CodeCogsEqn](https://user-images.githubusercontent.com/13019956/58164949-15390b80-7c87-11e9-9534-65ddf492ebd5.gif). Here, you would also put expression values to 0 if all expression values are +ve and non-zero. Otherwise, you should only do this for genes where the min() is -ve. The above scaling solution would keep the relative scale between the genes. If you however prefer to scale to values between 0 and 1 (which I usually don't do, but others advocate; this would ensure equal weighting between genes for PCA), you can also rescale by expression range like this:. ![CodeCogsEqn(1)](https://user-images.githubusercontent.com/13019956/58165629-6dbcd880-7c88-11e9-8c29-d3b2684b7bb7.gif). Overall though, I'm not a big fan of imputation... especially after this [paper](https://f1000research.com/articles/7-1740/v1)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:107,energy efficiency,scale,scale,107,"I guess negative values can mean different things across imputation methods. So having one standard way to scale is maybe not the best approach. That being said, I would probably simply do this:. ![CodeCogsEqn](https://user-images.githubusercontent.com/13019956/58164949-15390b80-7c87-11e9-9534-65ddf492ebd5.gif). Here, you would also put expression values to 0 if all expression values are +ve and non-zero. Otherwise, you should only do this for genes where the min() is -ve. The above scaling solution would keep the relative scale between the genes. If you however prefer to scale to values between 0 and 1 (which I usually don't do, but others advocate; this would ensure equal weighting between genes for PCA), you can also rescale by expression range like this:. ![CodeCogsEqn(1)](https://user-images.githubusercontent.com/13019956/58165629-6dbcd880-7c88-11e9-8c29-d3b2684b7bb7.gif). Overall though, I'm not a big fan of imputation... especially after this [paper](https://f1000research.com/articles/7-1740/v1)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:529,energy efficiency,scale,scale,529,"I guess negative values can mean different things across imputation methods. So having one standard way to scale is maybe not the best approach. That being said, I would probably simply do this:. ![CodeCogsEqn](https://user-images.githubusercontent.com/13019956/58164949-15390b80-7c87-11e9-9534-65ddf492ebd5.gif). Here, you would also put expression values to 0 if all expression values are +ve and non-zero. Otherwise, you should only do this for genes where the min() is -ve. The above scaling solution would keep the relative scale between the genes. If you however prefer to scale to values between 0 and 1 (which I usually don't do, but others advocate; this would ensure equal weighting between genes for PCA), you can also rescale by expression range like this:. ![CodeCogsEqn(1)](https://user-images.githubusercontent.com/13019956/58165629-6dbcd880-7c88-11e9-8c29-d3b2684b7bb7.gif). Overall though, I'm not a big fan of imputation... especially after this [paper](https://f1000research.com/articles/7-1740/v1)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:579,energy efficiency,scale,scale,579,"I guess negative values can mean different things across imputation methods. So having one standard way to scale is maybe not the best approach. That being said, I would probably simply do this:. ![CodeCogsEqn](https://user-images.githubusercontent.com/13019956/58164949-15390b80-7c87-11e9-9534-65ddf492ebd5.gif). Here, you would also put expression values to 0 if all expression values are +ve and non-zero. Otherwise, you should only do this for genes where the min() is -ve. The above scaling solution would keep the relative scale between the genes. If you however prefer to scale to values between 0 and 1 (which I usually don't do, but others advocate; this would ensure equal weighting between genes for PCA), you can also rescale by expression range like this:. ![CodeCogsEqn(1)](https://user-images.githubusercontent.com/13019956/58165629-6dbcd880-7c88-11e9-8c29-d3b2684b7bb7.gif). Overall though, I'm not a big fan of imputation... especially after this [paper](https://f1000research.com/articles/7-1740/v1)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:91,interoperability,standard,standard,91,"I guess negative values can mean different things across imputation methods. So having one standard way to scale is maybe not the best approach. That being said, I would probably simply do this:. ![CodeCogsEqn](https://user-images.githubusercontent.com/13019956/58164949-15390b80-7c87-11e9-9534-65ddf492ebd5.gif). Here, you would also put expression values to 0 if all expression values are +ve and non-zero. Otherwise, you should only do this for genes where the min() is -ve. The above scaling solution would keep the relative scale between the genes. If you however prefer to scale to values between 0 and 1 (which I usually don't do, but others advocate; this would ensure equal weighting between genes for PCA), you can also rescale by expression range like this:. ![CodeCogsEqn(1)](https://user-images.githubusercontent.com/13019956/58165629-6dbcd880-7c88-11e9-8c29-d3b2684b7bb7.gif). Overall though, I'm not a big fan of imputation... especially after this [paper](https://f1000research.com/articles/7-1740/v1)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:107,modifiability,scal,scale,107,"I guess negative values can mean different things across imputation methods. So having one standard way to scale is maybe not the best approach. That being said, I would probably simply do this:. ![CodeCogsEqn](https://user-images.githubusercontent.com/13019956/58164949-15390b80-7c87-11e9-9534-65ddf492ebd5.gif). Here, you would also put expression values to 0 if all expression values are +ve and non-zero. Otherwise, you should only do this for genes where the min() is -ve. The above scaling solution would keep the relative scale between the genes. If you however prefer to scale to values between 0 and 1 (which I usually don't do, but others advocate; this would ensure equal weighting between genes for PCA), you can also rescale by expression range like this:. ![CodeCogsEqn(1)](https://user-images.githubusercontent.com/13019956/58165629-6dbcd880-7c88-11e9-8c29-d3b2684b7bb7.gif). Overall though, I'm not a big fan of imputation... especially after this [paper](https://f1000research.com/articles/7-1740/v1)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:488,modifiability,scal,scaling,488,"I guess negative values can mean different things across imputation methods. So having one standard way to scale is maybe not the best approach. That being said, I would probably simply do this:. ![CodeCogsEqn](https://user-images.githubusercontent.com/13019956/58164949-15390b80-7c87-11e9-9534-65ddf492ebd5.gif). Here, you would also put expression values to 0 if all expression values are +ve and non-zero. Otherwise, you should only do this for genes where the min() is -ve. The above scaling solution would keep the relative scale between the genes. If you however prefer to scale to values between 0 and 1 (which I usually don't do, but others advocate; this would ensure equal weighting between genes for PCA), you can also rescale by expression range like this:. ![CodeCogsEqn(1)](https://user-images.githubusercontent.com/13019956/58165629-6dbcd880-7c88-11e9-8c29-d3b2684b7bb7.gif). Overall though, I'm not a big fan of imputation... especially after this [paper](https://f1000research.com/articles/7-1740/v1)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:529,modifiability,scal,scale,529,"I guess negative values can mean different things across imputation methods. So having one standard way to scale is maybe not the best approach. That being said, I would probably simply do this:. ![CodeCogsEqn](https://user-images.githubusercontent.com/13019956/58164949-15390b80-7c87-11e9-9534-65ddf492ebd5.gif). Here, you would also put expression values to 0 if all expression values are +ve and non-zero. Otherwise, you should only do this for genes where the min() is -ve. The above scaling solution would keep the relative scale between the genes. If you however prefer to scale to values between 0 and 1 (which I usually don't do, but others advocate; this would ensure equal weighting between genes for PCA), you can also rescale by expression range like this:. ![CodeCogsEqn(1)](https://user-images.githubusercontent.com/13019956/58165629-6dbcd880-7c88-11e9-8c29-d3b2684b7bb7.gif). Overall though, I'm not a big fan of imputation... especially after this [paper](https://f1000research.com/articles/7-1740/v1)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:579,modifiability,scal,scale,579,"I guess negative values can mean different things across imputation methods. So having one standard way to scale is maybe not the best approach. That being said, I would probably simply do this:. ![CodeCogsEqn](https://user-images.githubusercontent.com/13019956/58164949-15390b80-7c87-11e9-9534-65ddf492ebd5.gif). Here, you would also put expression values to 0 if all expression values are +ve and non-zero. Otherwise, you should only do this for genes where the min() is -ve. The above scaling solution would keep the relative scale between the genes. If you however prefer to scale to values between 0 and 1 (which I usually don't do, but others advocate; this would ensure equal weighting between genes for PCA), you can also rescale by expression range like this:. ![CodeCogsEqn(1)](https://user-images.githubusercontent.com/13019956/58165629-6dbcd880-7c88-11e9-8c29-d3b2684b7bb7.gif). Overall though, I'm not a big fan of imputation... especially after this [paper](https://f1000research.com/articles/7-1740/v1)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:107,performance,scale,scale,107,"I guess negative values can mean different things across imputation methods. So having one standard way to scale is maybe not the best approach. That being said, I would probably simply do this:. ![CodeCogsEqn](https://user-images.githubusercontent.com/13019956/58164949-15390b80-7c87-11e9-9534-65ddf492ebd5.gif). Here, you would also put expression values to 0 if all expression values are +ve and non-zero. Otherwise, you should only do this for genes where the min() is -ve. The above scaling solution would keep the relative scale between the genes. If you however prefer to scale to values between 0 and 1 (which I usually don't do, but others advocate; this would ensure equal weighting between genes for PCA), you can also rescale by expression range like this:. ![CodeCogsEqn(1)](https://user-images.githubusercontent.com/13019956/58165629-6dbcd880-7c88-11e9-8c29-d3b2684b7bb7.gif). Overall though, I'm not a big fan of imputation... especially after this [paper](https://f1000research.com/articles/7-1740/v1)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:529,performance,scale,scale,529,"I guess negative values can mean different things across imputation methods. So having one standard way to scale is maybe not the best approach. That being said, I would probably simply do this:. ![CodeCogsEqn](https://user-images.githubusercontent.com/13019956/58164949-15390b80-7c87-11e9-9534-65ddf492ebd5.gif). Here, you would also put expression values to 0 if all expression values are +ve and non-zero. Otherwise, you should only do this for genes where the min() is -ve. The above scaling solution would keep the relative scale between the genes. If you however prefer to scale to values between 0 and 1 (which I usually don't do, but others advocate; this would ensure equal weighting between genes for PCA), you can also rescale by expression range like this:. ![CodeCogsEqn(1)](https://user-images.githubusercontent.com/13019956/58165629-6dbcd880-7c88-11e9-8c29-d3b2684b7bb7.gif). Overall though, I'm not a big fan of imputation... especially after this [paper](https://f1000research.com/articles/7-1740/v1)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:579,performance,scale,scale,579,"I guess negative values can mean different things across imputation methods. So having one standard way to scale is maybe not the best approach. That being said, I would probably simply do this:. ![CodeCogsEqn](https://user-images.githubusercontent.com/13019956/58164949-15390b80-7c87-11e9-9534-65ddf492ebd5.gif). Here, you would also put expression values to 0 if all expression values are +ve and non-zero. Otherwise, you should only do this for genes where the min() is -ve. The above scaling solution would keep the relative scale between the genes. If you however prefer to scale to values between 0 and 1 (which I usually don't do, but others advocate; this would ensure equal weighting between genes for PCA), you can also rescale by expression range like this:. ![CodeCogsEqn(1)](https://user-images.githubusercontent.com/13019956/58165629-6dbcd880-7c88-11e9-8c29-d3b2684b7bb7.gif). Overall though, I'm not a big fan of imputation... especially after this [paper](https://f1000research.com/articles/7-1740/v1)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:179,testability,simpl,simply,179,"I guess negative values can mean different things across imputation methods. So having one standard way to scale is maybe not the best approach. That being said, I would probably simply do this:. ![CodeCogsEqn](https://user-images.githubusercontent.com/13019956/58164949-15390b80-7c87-11e9-9534-65ddf492ebd5.gif). Here, you would also put expression values to 0 if all expression values are +ve and non-zero. Otherwise, you should only do this for genes where the min() is -ve. The above scaling solution would keep the relative scale between the genes. If you however prefer to scale to values between 0 and 1 (which I usually don't do, but others advocate; this would ensure equal weighting between genes for PCA), you can also rescale by expression range like this:. ![CodeCogsEqn(1)](https://user-images.githubusercontent.com/13019956/58165629-6dbcd880-7c88-11e9-8c29-d3b2684b7bb7.gif). Overall though, I'm not a big fan of imputation... especially after this [paper](https://f1000research.com/articles/7-1740/v1)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:179,usability,simpl,simply,179,"I guess negative values can mean different things across imputation methods. So having one standard way to scale is maybe not the best approach. That being said, I would probably simply do this:. ![CodeCogsEqn](https://user-images.githubusercontent.com/13019956/58164949-15390b80-7c87-11e9-9534-65ddf492ebd5.gif). Here, you would also put expression values to 0 if all expression values are +ve and non-zero. Otherwise, you should only do this for genes where the min() is -ve. The above scaling solution would keep the relative scale between the genes. If you however prefer to scale to values between 0 and 1 (which I usually don't do, but others advocate; this would ensure equal weighting between genes for PCA), you can also rescale by expression range like this:. ![CodeCogsEqn(1)](https://user-images.githubusercontent.com/13019956/58165629-6dbcd880-7c88-11e9-8c29-d3b2684b7bb7.gif). Overall though, I'm not a big fan of imputation... especially after this [paper](https://f1000research.com/articles/7-1740/v1)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:219,usability,user,user-images,219,"I guess negative values can mean different things across imputation methods. So having one standard way to scale is maybe not the best approach. That being said, I would probably simply do this:. ![CodeCogsEqn](https://user-images.githubusercontent.com/13019956/58164949-15390b80-7c87-11e9-9534-65ddf492ebd5.gif). Here, you would also put expression values to 0 if all expression values are +ve and non-zero. Otherwise, you should only do this for genes where the min() is -ve. The above scaling solution would keep the relative scale between the genes. If you however prefer to scale to values between 0 and 1 (which I usually don't do, but others advocate; this would ensure equal weighting between genes for PCA), you can also rescale by expression range like this:. ![CodeCogsEqn(1)](https://user-images.githubusercontent.com/13019956/58165629-6dbcd880-7c88-11e9-8c29-d3b2684b7bb7.gif). Overall though, I'm not a big fan of imputation... especially after this [paper](https://f1000research.com/articles/7-1740/v1)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:569,usability,prefer,prefer,569,"I guess negative values can mean different things across imputation methods. So having one standard way to scale is maybe not the best approach. That being said, I would probably simply do this:. ![CodeCogsEqn](https://user-images.githubusercontent.com/13019956/58164949-15390b80-7c87-11e9-9534-65ddf492ebd5.gif). Here, you would also put expression values to 0 if all expression values are +ve and non-zero. Otherwise, you should only do this for genes where the min() is -ve. The above scaling solution would keep the relative scale between the genes. If you however prefer to scale to values between 0 and 1 (which I usually don't do, but others advocate; this would ensure equal weighting between genes for PCA), you can also rescale by expression range like this:. ![CodeCogsEqn(1)](https://user-images.githubusercontent.com/13019956/58165629-6dbcd880-7c88-11e9-8c29-d3b2684b7bb7.gif). Overall though, I'm not a big fan of imputation... especially after this [paper](https://f1000research.com/articles/7-1740/v1)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:796,usability,user,user-images,796,"I guess negative values can mean different things across imputation methods. So having one standard way to scale is maybe not the best approach. That being said, I would probably simply do this:. ![CodeCogsEqn](https://user-images.githubusercontent.com/13019956/58164949-15390b80-7c87-11e9-9534-65ddf492ebd5.gif). Here, you would also put expression values to 0 if all expression values are +ve and non-zero. Otherwise, you should only do this for genes where the min() is -ve. The above scaling solution would keep the relative scale between the genes. If you however prefer to scale to values between 0 and 1 (which I usually don't do, but others advocate; this would ensure equal weighting between genes for PCA), you can also rescale by expression range like this:. ![CodeCogsEqn(1)](https://user-images.githubusercontent.com/13019956/58165629-6dbcd880-7c88-11e9-8c29-d3b2684b7bb7.gif). Overall though, I'm not a big fan of imputation... especially after this [paper](https://f1000research.com/articles/7-1740/v1)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:259,integrability,transform,transformed,259,Interesting paper... the question is which is worse false signals from experiment or from the imputation. Anyway. Great. Just the last thing that came to my mind is that the whole thingy is happening because I am doing imputation on my already normalized and transformed data. Wouldn't it make more sense if i do imputation on raw data after filtering and then normalize and transform my data or transform the data but normalizing it afterward? With this I won't face this problem and also I keep the original variability within my data,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:342,integrability,filter,filtering,342,Interesting paper... the question is which is worse false signals from experiment or from the imputation. Anyway. Great. Just the last thing that came to my mind is that the whole thingy is happening because I am doing imputation on my already normalized and transformed data. Wouldn't it make more sense if i do imputation on raw data after filtering and then normalize and transform my data or transform the data but normalizing it afterward? With this I won't face this problem and also I keep the original variability within my data,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:375,integrability,transform,transform,375,Interesting paper... the question is which is worse false signals from experiment or from the imputation. Anyway. Great. Just the last thing that came to my mind is that the whole thingy is happening because I am doing imputation on my already normalized and transformed data. Wouldn't it make more sense if i do imputation on raw data after filtering and then normalize and transform my data or transform the data but normalizing it afterward? With this I won't face this problem and also I keep the original variability within my data,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:396,integrability,transform,transform,396,Interesting paper... the question is which is worse false signals from experiment or from the imputation. Anyway. Great. Just the last thing that came to my mind is that the whole thingy is happening because I am doing imputation on my already normalized and transformed data. Wouldn't it make more sense if i do imputation on raw data after filtering and then normalize and transform my data or transform the data but normalizing it afterward? With this I won't face this problem and also I keep the original variability within my data,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:259,interoperability,transform,transformed,259,Interesting paper... the question is which is worse false signals from experiment or from the imputation. Anyway. Great. Just the last thing that came to my mind is that the whole thingy is happening because I am doing imputation on my already normalized and transformed data. Wouldn't it make more sense if i do imputation on raw data after filtering and then normalize and transform my data or transform the data but normalizing it afterward? With this I won't face this problem and also I keep the original variability within my data,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:375,interoperability,transform,transform,375,Interesting paper... the question is which is worse false signals from experiment or from the imputation. Anyway. Great. Just the last thing that came to my mind is that the whole thingy is happening because I am doing imputation on my already normalized and transformed data. Wouldn't it make more sense if i do imputation on raw data after filtering and then normalize and transform my data or transform the data but normalizing it afterward? With this I won't face this problem and also I keep the original variability within my data,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:396,interoperability,transform,transform,396,Interesting paper... the question is which is worse false signals from experiment or from the imputation. Anyway. Great. Just the last thing that came to my mind is that the whole thingy is happening because I am doing imputation on my already normalized and transformed data. Wouldn't it make more sense if i do imputation on raw data after filtering and then normalize and transform my data or transform the data but normalizing it afterward? With this I won't face this problem and also I keep the original variability within my data,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:510,modifiability,variab,variability,510,Interesting paper... the question is which is worse false signals from experiment or from the imputation. Anyway. Great. Just the last thing that came to my mind is that the whole thingy is happening because I am doing imputation on my already normalized and transformed data. Wouldn't it make more sense if i do imputation on raw data after filtering and then normalize and transform my data or transform the data but normalizing it afterward? With this I won't face this problem and also I keep the original variability within my data,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:58,security,sign,signals,58,Interesting paper... the question is which is worse false signals from experiment or from the imputation. Anyway. Great. Just the last thing that came to my mind is that the whole thingy is happening because I am doing imputation on my already normalized and transformed data. Wouldn't it make more sense if i do imputation on raw data after filtering and then normalize and transform my data or transform the data but normalizing it afterward? With this I won't face this problem and also I keep the original variability within my data,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:45,modifiability,variab,variability,45,"After imputation you don't have your initial variability in your data. Ideally you have only the biologically relevant variability, but that's another question. Also, imputation methods take different data as input. Our DCA method takes count data, but MAGIC takes pre-processed data I think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:119,modifiability,variab,variability,119,"After imputation you don't have your initial variability in your data. Ideally you have only the biologically relevant variability, but that's another question. Also, imputation methods take different data as input. Our DCA method takes count data, but MAGIC takes pre-processed data I think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:209,safety,input,input,209,"After imputation you don't have your initial variability in your data. Ideally you have only the biologically relevant variability, but that's another question. Also, imputation methods take different data as input. Our DCA method takes count data, but MAGIC takes pre-processed data I think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/653:209,usability,input,input,209,"After imputation you don't have your initial variability in your data. Ideally you have only the biologically relevant variability, but that's another question. Also, imputation methods take different data as input. Our DCA method takes count data, but MAGIC takes pre-processed data I think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/653
https://github.com/scverse/scanpy/issues/654:131,deployability,depend,depending,131,"Just to add to this, PCA plots look fine with the newer `scikit-learn` I believe. Maybe it's the umap neighbourhood graph function depending on sklearn for something?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/654
https://github.com/scverse/scanpy/issues/654:131,integrability,depend,depending,131,"Just to add to this, PCA plots look fine with the newer `scikit-learn` I believe. Maybe it's the umap neighbourhood graph function depending on sklearn for something?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/654
https://github.com/scverse/scanpy/issues/654:131,modifiability,depend,depending,131,"Just to add to this, PCA plots look fine with the newer `scikit-learn` I believe. Maybe it's the umap neighbourhood graph function depending on sklearn for something?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/654
https://github.com/scverse/scanpy/issues/654:131,safety,depend,depending,131,"Just to add to this, PCA plots look fine with the newer `scikit-learn` I believe. Maybe it's the umap neighbourhood graph function depending on sklearn for something?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/654
https://github.com/scverse/scanpy/issues/654:131,testability,depend,depending,131,"Just to add to this, PCA plots look fine with the newer `scikit-learn` I believe. Maybe it's the umap neighbourhood graph function depending on sklearn for something?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/654
https://github.com/scverse/scanpy/issues/654:64,usability,learn,learn,64,"Just to add to this, PCA plots look fine with the newer `scikit-learn` I believe. Maybe it's the umap neighbourhood graph function depending on sklearn for something?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/654
https://github.com/scverse/scanpy/issues/654:80,usability,user,user-images,80,Oh that's reaaaally bad. I did a quick git bisect on sklearn:. ![image](https://user-images.githubusercontent.com/1140359/58111323-40582800-7bbf-11e9-8905-e7f3a73cd057.png). Here is the commit that broke our umaps: https://github.com/scikit-learn/scikit-learn/pull/13554.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/654
https://github.com/scverse/scanpy/issues/654:241,usability,learn,learn,241,Oh that's reaaaally bad. I did a quick git bisect on sklearn:. ![image](https://user-images.githubusercontent.com/1140359/58111323-40582800-7bbf-11e9-8905-e7f3a73cd057.png). Here is the commit that broke our umaps: https://github.com/scikit-learn/scikit-learn/pull/13554.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/654
https://github.com/scverse/scanpy/issues/654:254,usability,learn,learn,254,Oh that's reaaaally bad. I did a quick git bisect on sklearn:. ![image](https://user-images.githubusercontent.com/1140359/58111323-40582800-7bbf-11e9-8905-e7f3a73cd057.png). Here is the commit that broke our umaps: https://github.com/scikit-learn/scikit-learn/pull/13554.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/654
https://github.com/scverse/scanpy/issues/654:33,usability,learn,learn,33,"Should this be relayed to scikit-learn then? If so, that should probably be done by someone who knows where in the `sc.pp.neighbors()` function this is breaking...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/654
https://github.com/scverse/scanpy/issues/654:219,deployability,version,versions,219,"OK, seems to be fixed in sklearn master branch (probably https://github.com/scikit-learn/scikit-learn/pull/13910), but this is such a huge bug and it has been going on since May 9th :( We could have blacklisted sklearn versions 0.21.0 and 0.21.1 if it was known, no? Some colleagues mentioned weird UMAP results with scanpy actually, it turns out they upgraded their sklearn...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/654
https://github.com/scverse/scanpy/issues/654:352,deployability,upgrad,upgraded,352,"OK, seems to be fixed in sklearn master branch (probably https://github.com/scikit-learn/scikit-learn/pull/13910), but this is such a huge bug and it has been going on since May 9th :( We could have blacklisted sklearn versions 0.21.0 and 0.21.1 if it was known, no? Some colleagues mentioned weird UMAP results with scanpy actually, it turns out they upgraded their sklearn...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/654
https://github.com/scverse/scanpy/issues/654:219,integrability,version,versions,219,"OK, seems to be fixed in sklearn master branch (probably https://github.com/scikit-learn/scikit-learn/pull/13910), but this is such a huge bug and it has been going on since May 9th :( We could have blacklisted sklearn versions 0.21.0 and 0.21.1 if it was known, no? Some colleagues mentioned weird UMAP results with scanpy actually, it turns out they upgraded their sklearn...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/654
https://github.com/scverse/scanpy/issues/654:219,modifiability,version,versions,219,"OK, seems to be fixed in sklearn master branch (probably https://github.com/scikit-learn/scikit-learn/pull/13910), but this is such a huge bug and it has been going on since May 9th :( We could have blacklisted sklearn versions 0.21.0 and 0.21.1 if it was known, no? Some colleagues mentioned weird UMAP results with scanpy actually, it turns out they upgraded their sklearn...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/654
https://github.com/scverse/scanpy/issues/654:352,modifiability,upgrad,upgraded,352,"OK, seems to be fixed in sklearn master branch (probably https://github.com/scikit-learn/scikit-learn/pull/13910), but this is such a huge bug and it has been going on since May 9th :( We could have blacklisted sklearn versions 0.21.0 and 0.21.1 if it was known, no? Some colleagues mentioned weird UMAP results with scanpy actually, it turns out they upgraded their sklearn...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/654
https://github.com/scverse/scanpy/issues/654:83,usability,learn,learn,83,"OK, seems to be fixed in sklearn master branch (probably https://github.com/scikit-learn/scikit-learn/pull/13910), but this is such a huge bug and it has been going on since May 9th :( We could have blacklisted sklearn versions 0.21.0 and 0.21.1 if it was known, no? Some colleagues mentioned weird UMAP results with scanpy actually, it turns out they upgraded their sklearn...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/654
https://github.com/scverse/scanpy/issues/654:96,usability,learn,learn,96,"OK, seems to be fixed in sklearn master branch (probably https://github.com/scikit-learn/scikit-learn/pull/13910), but this is such a huge bug and it has been going on since May 9th :( We could have blacklisted sklearn versions 0.21.0 and 0.21.1 if it was known, no? Some colleagues mentioned weird UMAP results with scanpy actually, it turns out they upgraded their sklearn...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/654
https://github.com/scverse/scanpy/issues/654:111,deployability,updat,update,111,"> @flying-sheep mentioned this was known and already fixed though? I meant the other breakage due to the scipy update, sorry. > We could have blacklisted sklearn versions 0.21.0 and 0.21.1 if it was known, no? We should do that now. We can do `sklearn >= 0.19.1, != 0.21.0, != 0.21.1` I think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/654
https://github.com/scverse/scanpy/issues/654:162,deployability,version,versions,162,"> @flying-sheep mentioned this was known and already fixed though? I meant the other breakage due to the scipy update, sorry. > We could have blacklisted sklearn versions 0.21.0 and 0.21.1 if it was known, no? We should do that now. We can do `sklearn >= 0.19.1, != 0.21.0, != 0.21.1` I think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/654
https://github.com/scverse/scanpy/issues/654:162,integrability,version,versions,162,"> @flying-sheep mentioned this was known and already fixed though? I meant the other breakage due to the scipy update, sorry. > We could have blacklisted sklearn versions 0.21.0 and 0.21.1 if it was known, no? We should do that now. We can do `sklearn >= 0.19.1, != 0.21.0, != 0.21.1` I think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/654
https://github.com/scverse/scanpy/issues/654:162,modifiability,version,versions,162,"> @flying-sheep mentioned this was known and already fixed though? I meant the other breakage due to the scipy update, sorry. > We could have blacklisted sklearn versions 0.21.0 and 0.21.1 if it was known, no? We should do that now. We can do `sklearn >= 0.19.1, != 0.21.0, != 0.21.1` I think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/654
https://github.com/scverse/scanpy/issues/654:111,safety,updat,update,111,"> @flying-sheep mentioned this was known and already fixed though? I meant the other breakage due to the scipy update, sorry. > We could have blacklisted sklearn versions 0.21.0 and 0.21.1 if it was known, no? We should do that now. We can do `sklearn >= 0.19.1, != 0.21.0, != 0.21.1` I think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/654
https://github.com/scverse/scanpy/issues/654:111,security,updat,update,111,"> @flying-sheep mentioned this was known and already fixed though? I meant the other breakage due to the scipy update, sorry. > We could have blacklisted sklearn versions 0.21.0 and 0.21.1 if it was known, no? We should do that now. We can do `sklearn >= 0.19.1, != 0.21.0, != 0.21.1` I think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/654
https://github.com/scverse/scanpy/issues/657:149,interoperability,specif,specified,149,"The function actually accepted `annotation` all along, yes. The docs say that itâ€™s not *required* if thereâ€™s an AnnData object passed, but it can be specified: https://github.com/rfechtner/pypairs/blob/7e35292260f65c7598bc335b1a85de149d06f1e0/pypairs/utils.py#L214-L216. Fixed in #814",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/657
https://github.com/scverse/scanpy/issues/658:164,reliability,doe,does,164,"To clarify a bit... I think it would be good to enable something like:. `sc.pl.umap(adata, color=(uns_column, obs_column))`. Where the `sc.pl.umap()` function then does:. ```. if isinstance(color, tuple):. color_vector = adata.uns[color[1]+""_linked_data""][color[0]][adata.obs[color[1]]. sc.pl.plot_scatter(adata, color=color_vector, ...). ```. It might need to be a pandas dataframe rather than a dictionary with the above setup.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/658
https://github.com/scverse/scanpy/issues/658:101,availability,redund,redundant,101,"I think that if you store a categorical value in a pandas dataframe (like. .obs) the storage of this redundant information is quite efficient. In a. quick search I found this:. https://towardsdatascience.com/make-working-with-large-dataframes-easier-at-least-for-your-memory-6f52b5f4b5c4. On Wed, May 22, 2019 at 3:59 PM MalteDLuecken <notifications@github.com>. wrote:. > To clarify a bit... I think it would be good to enable something like:. > sc.pl.umap(adata, color=(uns_dict_key, obs_column)). >. > Where the sc.pl.umap() function then does:. >. > if isinstance(color, tuple):. > color_vector = [adata.uns[color[1]+""_linked_data""][color[0]]][adata.obs[color[1]]]. > sc.pl.plot_scatter(adata, color=color_vector, ...). >. > It might need to be a pandas dataframe rather than a dictionary with the. > above setup. >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/658?email_source=notifications&email_token=ABF37VINUTKOPIDJADJOMMTPWVGT3A5CNFSM4HOUNBK2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV7ENLQ#issuecomment-494814894>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VL26UDSJCBGW67HNNLPWVGT3ANCNFSM4HOUNBKQ>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/658
https://github.com/scverse/scanpy/issues/658:101,deployability,redundan,redundant,101,"I think that if you store a categorical value in a pandas dataframe (like. .obs) the storage of this redundant information is quite efficient. In a. quick search I found this:. https://towardsdatascience.com/make-working-with-large-dataframes-easier-at-least-for-your-memory-6f52b5f4b5c4. On Wed, May 22, 2019 at 3:59 PM MalteDLuecken <notifications@github.com>. wrote:. > To clarify a bit... I think it would be good to enable something like:. > sc.pl.umap(adata, color=(uns_dict_key, obs_column)). >. > Where the sc.pl.umap() function then does:. >. > if isinstance(color, tuple):. > color_vector = [adata.uns[color[1]+""_linked_data""][color[0]]][adata.obs[color[1]]]. > sc.pl.plot_scatter(adata, color=color_vector, ...). >. > It might need to be a pandas dataframe rather than a dictionary with the. > above setup. >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/658?email_source=notifications&email_token=ABF37VINUTKOPIDJADJOMMTPWVGT3A5CNFSM4HOUNBK2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV7ENLQ#issuecomment-494814894>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VL26UDSJCBGW67HNNLPWVGT3ANCNFSM4HOUNBKQ>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/658
https://github.com/scverse/scanpy/issues/658:867,integrability,sub,subscribed,867,"I think that if you store a categorical value in a pandas dataframe (like. .obs) the storage of this redundant information is quite efficient. In a. quick search I found this:. https://towardsdatascience.com/make-working-with-large-dataframes-easier-at-least-for-your-memory-6f52b5f4b5c4. On Wed, May 22, 2019 at 3:59 PM MalteDLuecken <notifications@github.com>. wrote:. > To clarify a bit... I think it would be good to enable something like:. > sc.pl.umap(adata, color=(uns_dict_key, obs_column)). >. > Where the sc.pl.umap() function then does:. >. > if isinstance(color, tuple):. > color_vector = [adata.uns[color[1]+""_linked_data""][color[0]]][adata.obs[color[1]]]. > sc.pl.plot_scatter(adata, color=color_vector, ...). >. > It might need to be a pandas dataframe rather than a dictionary with the. > above setup. >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/658?email_source=notifications&email_token=ABF37VINUTKOPIDJADJOMMTPWVGT3A5CNFSM4HOUNBK2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV7ENLQ#issuecomment-494814894>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VL26UDSJCBGW67HNNLPWVGT3ANCNFSM4HOUNBKQ>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/658
https://github.com/scverse/scanpy/issues/658:268,performance,memor,memory-,268,"I think that if you store a categorical value in a pandas dataframe (like. .obs) the storage of this redundant information is quite efficient. In a. quick search I found this:. https://towardsdatascience.com/make-working-with-large-dataframes-easier-at-least-for-your-memory-6f52b5f4b5c4. On Wed, May 22, 2019 at 3:59 PM MalteDLuecken <notifications@github.com>. wrote:. > To clarify a bit... I think it would be good to enable something like:. > sc.pl.umap(adata, color=(uns_dict_key, obs_column)). >. > Where the sc.pl.umap() function then does:. >. > if isinstance(color, tuple):. > color_vector = [adata.uns[color[1]+""_linked_data""][color[0]]][adata.obs[color[1]]]. > sc.pl.plot_scatter(adata, color=color_vector, ...). >. > It might need to be a pandas dataframe rather than a dictionary with the. > above setup. >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/658?email_source=notifications&email_token=ABF37VINUTKOPIDJADJOMMTPWVGT3A5CNFSM4HOUNBK2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV7ENLQ#issuecomment-494814894>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VL26UDSJCBGW67HNNLPWVGT3ANCNFSM4HOUNBKQ>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/658
https://github.com/scverse/scanpy/issues/658:101,reliability,redundan,redundant,101,"I think that if you store a categorical value in a pandas dataframe (like. .obs) the storage of this redundant information is quite efficient. In a. quick search I found this:. https://towardsdatascience.com/make-working-with-large-dataframes-easier-at-least-for-your-memory-6f52b5f4b5c4. On Wed, May 22, 2019 at 3:59 PM MalteDLuecken <notifications@github.com>. wrote:. > To clarify a bit... I think it would be good to enable something like:. > sc.pl.umap(adata, color=(uns_dict_key, obs_column)). >. > Where the sc.pl.umap() function then does:. >. > if isinstance(color, tuple):. > color_vector = [adata.uns[color[1]+""_linked_data""][color[0]]][adata.obs[color[1]]]. > sc.pl.plot_scatter(adata, color=color_vector, ...). >. > It might need to be a pandas dataframe rather than a dictionary with the. > above setup. >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/658?email_source=notifications&email_token=ABF37VINUTKOPIDJADJOMMTPWVGT3A5CNFSM4HOUNBK2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV7ENLQ#issuecomment-494814894>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VL26UDSJCBGW67HNNLPWVGT3ANCNFSM4HOUNBKQ>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/658
https://github.com/scverse/scanpy/issues/658:542,reliability,doe,does,542,"I think that if you store a categorical value in a pandas dataframe (like. .obs) the storage of this redundant information is quite efficient. In a. quick search I found this:. https://towardsdatascience.com/make-working-with-large-dataframes-easier-at-least-for-your-memory-6f52b5f4b5c4. On Wed, May 22, 2019 at 3:59 PM MalteDLuecken <notifications@github.com>. wrote:. > To clarify a bit... I think it would be good to enable something like:. > sc.pl.umap(adata, color=(uns_dict_key, obs_column)). >. > Where the sc.pl.umap() function then does:. >. > if isinstance(color, tuple):. > color_vector = [adata.uns[color[1]+""_linked_data""][color[0]]][adata.obs[color[1]]]. > sc.pl.plot_scatter(adata, color=color_vector, ...). >. > It might need to be a pandas dataframe rather than a dictionary with the. > above setup. >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/658?email_source=notifications&email_token=ABF37VINUTKOPIDJADJOMMTPWVGT3A5CNFSM4HOUNBK2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV7ENLQ#issuecomment-494814894>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VL26UDSJCBGW67HNNLPWVGT3ANCNFSM4HOUNBKQ>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/658
https://github.com/scverse/scanpy/issues/658:101,safety,redund,redundant,101,"I think that if you store a categorical value in a pandas dataframe (like. .obs) the storage of this redundant information is quite efficient. In a. quick search I found this:. https://towardsdatascience.com/make-working-with-large-dataframes-easier-at-least-for-your-memory-6f52b5f4b5c4. On Wed, May 22, 2019 at 3:59 PM MalteDLuecken <notifications@github.com>. wrote:. > To clarify a bit... I think it would be good to enable something like:. > sc.pl.umap(adata, color=(uns_dict_key, obs_column)). >. > Where the sc.pl.umap() function then does:. >. > if isinstance(color, tuple):. > color_vector = [adata.uns[color[1]+""_linked_data""][color[0]]][adata.obs[color[1]]]. > sc.pl.plot_scatter(adata, color=color_vector, ...). >. > It might need to be a pandas dataframe rather than a dictionary with the. > above setup. >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/658?email_source=notifications&email_token=ABF37VINUTKOPIDJADJOMMTPWVGT3A5CNFSM4HOUNBK2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV7ENLQ#issuecomment-494814894>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VL26UDSJCBGW67HNNLPWVGT3ANCNFSM4HOUNBKQ>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/658
https://github.com/scverse/scanpy/issues/658:1241,security,auth,auth,1241,"I think that if you store a categorical value in a pandas dataframe (like. .obs) the storage of this redundant information is quite efficient. In a. quick search I found this:. https://towardsdatascience.com/make-working-with-large-dataframes-easier-at-least-for-your-memory-6f52b5f4b5c4. On Wed, May 22, 2019 at 3:59 PM MalteDLuecken <notifications@github.com>. wrote:. > To clarify a bit... I think it would be good to enable something like:. > sc.pl.umap(adata, color=(uns_dict_key, obs_column)). >. > Where the sc.pl.umap() function then does:. >. > if isinstance(color, tuple):. > color_vector = [adata.uns[color[1]+""_linked_data""][color[0]]][adata.obs[color[1]]]. > sc.pl.plot_scatter(adata, color=color_vector, ...). >. > It might need to be a pandas dataframe rather than a dictionary with the. > above setup. >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/658?email_source=notifications&email_token=ABF37VINUTKOPIDJADJOMMTPWVGT3A5CNFSM4HOUNBK2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV7ENLQ#issuecomment-494814894>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VL26UDSJCBGW67HNNLPWVGT3ANCNFSM4HOUNBKQ>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/658
https://github.com/scverse/scanpy/issues/658:132,usability,efficien,efficient,132,"I think that if you store a categorical value in a pandas dataframe (like. .obs) the storage of this redundant information is quite efficient. In a. quick search I found this:. https://towardsdatascience.com/make-working-with-large-dataframes-easier-at-least-for-your-memory-6f52b5f4b5c4. On Wed, May 22, 2019 at 3:59 PM MalteDLuecken <notifications@github.com>. wrote:. > To clarify a bit... I think it would be good to enable something like:. > sc.pl.umap(adata, color=(uns_dict_key, obs_column)). >. > Where the sc.pl.umap() function then does:. >. > if isinstance(color, tuple):. > color_vector = [adata.uns[color[1]+""_linked_data""][color[0]]][adata.obs[color[1]]]. > sc.pl.plot_scatter(adata, color=color_vector, ...). >. > It might need to be a pandas dataframe rather than a dictionary with the. > above setup. >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/658?email_source=notifications&email_token=ABF37VINUTKOPIDJADJOMMTPWVGT3A5CNFSM4HOUNBK2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV7ENLQ#issuecomment-494814894>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VL26UDSJCBGW67HNNLPWVGT3ANCNFSM4HOUNBKQ>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/658
https://github.com/scverse/scanpy/issues/658:268,usability,memor,memory-,268,"I think that if you store a categorical value in a pandas dataframe (like. .obs) the storage of this redundant information is quite efficient. In a. quick search I found this:. https://towardsdatascience.com/make-working-with-large-dataframes-easier-at-least-for-your-memory-6f52b5f4b5c4. On Wed, May 22, 2019 at 3:59 PM MalteDLuecken <notifications@github.com>. wrote:. > To clarify a bit... I think it would be good to enable something like:. > sc.pl.umap(adata, color=(uns_dict_key, obs_column)). >. > Where the sc.pl.umap() function then does:. >. > if isinstance(color, tuple):. > color_vector = [adata.uns[color[1]+""_linked_data""][color[0]]][adata.obs[color[1]]]. > sc.pl.plot_scatter(adata, color=color_vector, ...). >. > It might need to be a pandas dataframe rather than a dictionary with the. > above setup. >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/658?email_source=notifications&email_token=ABF37VINUTKOPIDJADJOMMTPWVGT3A5CNFSM4HOUNBK2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODV7ENLQ#issuecomment-494814894>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VL26UDSJCBGW67HNNLPWVGT3ANCNFSM4HOUNBKQ>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/658
https://github.com/scverse/scanpy/issues/658:158,energy efficiency,reduc,reduction,158,Categoricals being 95% more efficient than datetime is quite interesting. I wasn't aware of how efficient they are... I am however referring to a larger size reduction though. With multiple patients you often have ~5000 cells per patient. If you have patient-level (clinical) measurements you would essentially reduce it from a n_pat*5000 x n_annotations dataframe (if everything is stored in `.obs`) to an n_pat x n_annotations dataframe (stored in `.uns`). This would of course require us to be able to write dataframes in `.uns` to h5ad files.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/658
https://github.com/scverse/scanpy/issues/658:276,energy efficiency,measur,measurements,276,Categoricals being 95% more efficient than datetime is quite interesting. I wasn't aware of how efficient they are... I am however referring to a larger size reduction though. With multiple patients you often have ~5000 cells per patient. If you have patient-level (clinical) measurements you would essentially reduce it from a n_pat*5000 x n_annotations dataframe (if everything is stored in `.obs`) to an n_pat x n_annotations dataframe (stored in `.uns`). This would of course require us to be able to write dataframes in `.uns` to h5ad files.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/658
https://github.com/scverse/scanpy/issues/658:311,energy efficiency,reduc,reduce,311,Categoricals being 95% more efficient than datetime is quite interesting. I wasn't aware of how efficient they are... I am however referring to a larger size reduction though. With multiple patients you often have ~5000 cells per patient. If you have patient-level (clinical) measurements you would essentially reduce it from a n_pat*5000 x n_annotations dataframe (if everything is stored in `.obs`) to an n_pat x n_annotations dataframe (stored in `.uns`). This would of course require us to be able to write dataframes in `.uns` to h5ad files.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/658
https://github.com/scverse/scanpy/issues/658:28,usability,efficien,efficient,28,Categoricals being 95% more efficient than datetime is quite interesting. I wasn't aware of how efficient they are... I am however referring to a larger size reduction though. With multiple patients you often have ~5000 cells per patient. If you have patient-level (clinical) measurements you would essentially reduce it from a n_pat*5000 x n_annotations dataframe (if everything is stored in `.obs`) to an n_pat x n_annotations dataframe (stored in `.uns`). This would of course require us to be able to write dataframes in `.uns` to h5ad files.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/658
https://github.com/scverse/scanpy/issues/658:96,usability,efficien,efficient,96,Categoricals being 95% more efficient than datetime is quite interesting. I wasn't aware of how efficient they are... I am however referring to a larger size reduction though. With multiple patients you often have ~5000 cells per patient. If you have patient-level (clinical) measurements you would essentially reduce it from a n_pat*5000 x n_annotations dataframe (if everything is stored in `.obs`) to an n_pat x n_annotations dataframe (stored in `.uns`). This would of course require us to be able to write dataframes in `.uns` to h5ad files.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/658
https://github.com/scverse/scanpy/pull/659:5,energy efficiency,cool,cool,5,"Very cool! Thank you, Tom! Is Leland also going to make the change within UMAP? So that, in effect `from umap.umap_ import nearest_neighbors` is more or less equivalent to your implementation here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:111,deployability,depend,dependency,111,"The code in UMAP and pynndescent is very close now, but I don't know when UMAP is going to use the pyyndescent dependency. So I thought it would be useful to have this PR in the meantime.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:111,integrability,depend,dependency,111,"The code in UMAP and pynndescent is very close now, but I don't know when UMAP is going to use the pyyndescent dependency. So I thought it would be useful to have this PR in the meantime.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:111,modifiability,depend,dependency,111,"The code in UMAP and pynndescent is very close now, but I don't know when UMAP is going to use the pyyndescent dependency. So I thought it would be useful to have this PR in the meantime.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:111,safety,depend,dependency,111,"The code in UMAP and pynndescent is very close now, but I don't know when UMAP is going to use the pyyndescent dependency. So I thought it would be useful to have this PR in the meantime.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:111,testability,depend,dependency,111,"The code in UMAP and pynndescent is very close now, but I don't know when UMAP is going to use the pyyndescent dependency. So I thought it would be useful to have this PR in the meantime.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:41,usability,close,close,41,"The code in UMAP and pynndescent is very close now, but I don't know when UMAP is going to use the pyyndescent dependency. So I thought it would be useful to have this PR in the meantime.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:14,energy efficiency,cool,cool,14,This is super cool! Is the level of parallelism exposed here amenable to multi-machine parallelism (like the MapReduce strategy mentioned in the nn-descent paper)?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:36,performance,parallel,parallelism,36,This is super cool! Is the level of parallelism exposed here amenable to multi-machine parallelism (like the MapReduce strategy mentioned in the nn-descent paper)?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:87,performance,parallel,parallelism,87,This is super cool! Is the level of parallelism exposed here amenable to multi-machine parallelism (like the MapReduce strategy mentioned in the nn-descent paper)?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:48,security,expos,exposed,48,This is super cool! Is the level of parallelism exposed here amenable to multi-machine parallelism (like the MapReduce strategy mentioned in the nn-descent paper)?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:71,availability,operat,operating,71,"Thanks @ivirshup. The parallelism is achieved using a MapReduce scheme operating on a single NumPy array, as described here: https://github.com/lmcinnes/pynndescent/pull/12. This would be amenable to multi-machine parallelism, and in fact I have started a [Dask implementation](https://github.com/tomwhite/pynndescent/tree/dask) that should work on a cluster. However, I haven't benchmarked the Dask implementation, so I don't know how competitive it is with the single (multi-core) machine version using threads. I have successfully run pynndescent on 10^7 rows on a single machine (50 columns, 96 cores), and I don't see why it wouldn't go further than that, although the bottleneck is memory for the heap updates.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:351,availability,cluster,cluster,351,"Thanks @ivirshup. The parallelism is achieved using a MapReduce scheme operating on a single NumPy array, as described here: https://github.com/lmcinnes/pynndescent/pull/12. This would be amenable to multi-machine parallelism, and in fact I have started a [Dask implementation](https://github.com/tomwhite/pynndescent/tree/dask) that should work on a cluster. However, I haven't benchmarked the Dask implementation, so I don't know how competitive it is with the single (multi-core) machine version using threads. I have successfully run pynndescent on 10^7 rows on a single machine (50 columns, 96 cores), and I don't see why it wouldn't go further than that, although the bottleneck is memory for the heap updates.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:351,deployability,cluster,cluster,351,"Thanks @ivirshup. The parallelism is achieved using a MapReduce scheme operating on a single NumPy array, as described here: https://github.com/lmcinnes/pynndescent/pull/12. This would be amenable to multi-machine parallelism, and in fact I have started a [Dask implementation](https://github.com/tomwhite/pynndescent/tree/dask) that should work on a cluster. However, I haven't benchmarked the Dask implementation, so I don't know how competitive it is with the single (multi-core) machine version using threads. I have successfully run pynndescent on 10^7 rows on a single machine (50 columns, 96 cores), and I don't see why it wouldn't go further than that, although the bottleneck is memory for the heap updates.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:491,deployability,version,version,491,"Thanks @ivirshup. The parallelism is achieved using a MapReduce scheme operating on a single NumPy array, as described here: https://github.com/lmcinnes/pynndescent/pull/12. This would be amenable to multi-machine parallelism, and in fact I have started a [Dask implementation](https://github.com/tomwhite/pynndescent/tree/dask) that should work on a cluster. However, I haven't benchmarked the Dask implementation, so I don't know how competitive it is with the single (multi-core) machine version using threads. I have successfully run pynndescent on 10^7 rows on a single machine (50 columns, 96 cores), and I don't see why it wouldn't go further than that, although the bottleneck is memory for the heap updates.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:708,deployability,updat,updates,708,"Thanks @ivirshup. The parallelism is achieved using a MapReduce scheme operating on a single NumPy array, as described here: https://github.com/lmcinnes/pynndescent/pull/12. This would be amenable to multi-machine parallelism, and in fact I have started a [Dask implementation](https://github.com/tomwhite/pynndescent/tree/dask) that should work on a cluster. However, I haven't benchmarked the Dask implementation, so I don't know how competitive it is with the single (multi-core) machine version using threads. I have successfully run pynndescent on 10^7 rows on a single machine (50 columns, 96 cores), and I don't see why it wouldn't go further than that, although the bottleneck is memory for the heap updates.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:477,energy efficiency,core,core,477,"Thanks @ivirshup. The parallelism is achieved using a MapReduce scheme operating on a single NumPy array, as described here: https://github.com/lmcinnes/pynndescent/pull/12. This would be amenable to multi-machine parallelism, and in fact I have started a [Dask implementation](https://github.com/tomwhite/pynndescent/tree/dask) that should work on a cluster. However, I haven't benchmarked the Dask implementation, so I don't know how competitive it is with the single (multi-core) machine version using threads. I have successfully run pynndescent on 10^7 rows on a single machine (50 columns, 96 cores), and I don't see why it wouldn't go further than that, although the bottleneck is memory for the heap updates.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:599,energy efficiency,core,cores,599,"Thanks @ivirshup. The parallelism is achieved using a MapReduce scheme operating on a single NumPy array, as described here: https://github.com/lmcinnes/pynndescent/pull/12. This would be amenable to multi-machine parallelism, and in fact I have started a [Dask implementation](https://github.com/tomwhite/pynndescent/tree/dask) that should work on a cluster. However, I haven't benchmarked the Dask implementation, so I don't know how competitive it is with the single (multi-core) machine version using threads. I have successfully run pynndescent on 10^7 rows on a single machine (50 columns, 96 cores), and I don't see why it wouldn't go further than that, although the bottleneck is memory for the heap updates.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:491,integrability,version,version,491,"Thanks @ivirshup. The parallelism is achieved using a MapReduce scheme operating on a single NumPy array, as described here: https://github.com/lmcinnes/pynndescent/pull/12. This would be amenable to multi-machine parallelism, and in fact I have started a [Dask implementation](https://github.com/tomwhite/pynndescent/tree/dask) that should work on a cluster. However, I haven't benchmarked the Dask implementation, so I don't know how competitive it is with the single (multi-core) machine version using threads. I have successfully run pynndescent on 10^7 rows on a single machine (50 columns, 96 cores), and I don't see why it wouldn't go further than that, although the bottleneck is memory for the heap updates.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:491,modifiability,version,version,491,"Thanks @ivirshup. The parallelism is achieved using a MapReduce scheme operating on a single NumPy array, as described here: https://github.com/lmcinnes/pynndescent/pull/12. This would be amenable to multi-machine parallelism, and in fact I have started a [Dask implementation](https://github.com/tomwhite/pynndescent/tree/dask) that should work on a cluster. However, I haven't benchmarked the Dask implementation, so I don't know how competitive it is with the single (multi-core) machine version using threads. I have successfully run pynndescent on 10^7 rows on a single machine (50 columns, 96 cores), and I don't see why it wouldn't go further than that, although the bottleneck is memory for the heap updates.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:22,performance,parallel,parallelism,22,"Thanks @ivirshup. The parallelism is achieved using a MapReduce scheme operating on a single NumPy array, as described here: https://github.com/lmcinnes/pynndescent/pull/12. This would be amenable to multi-machine parallelism, and in fact I have started a [Dask implementation](https://github.com/tomwhite/pynndescent/tree/dask) that should work on a cluster. However, I haven't benchmarked the Dask implementation, so I don't know how competitive it is with the single (multi-core) machine version using threads. I have successfully run pynndescent on 10^7 rows on a single machine (50 columns, 96 cores), and I don't see why it wouldn't go further than that, although the bottleneck is memory for the heap updates.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:214,performance,parallel,parallelism,214,"Thanks @ivirshup. The parallelism is achieved using a MapReduce scheme operating on a single NumPy array, as described here: https://github.com/lmcinnes/pynndescent/pull/12. This would be amenable to multi-machine parallelism, and in fact I have started a [Dask implementation](https://github.com/tomwhite/pynndescent/tree/dask) that should work on a cluster. However, I haven't benchmarked the Dask implementation, so I don't know how competitive it is with the single (multi-core) machine version using threads. I have successfully run pynndescent on 10^7 rows on a single machine (50 columns, 96 cores), and I don't see why it wouldn't go further than that, although the bottleneck is memory for the heap updates.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:674,performance,bottleneck,bottleneck,674,"Thanks @ivirshup. The parallelism is achieved using a MapReduce scheme operating on a single NumPy array, as described here: https://github.com/lmcinnes/pynndescent/pull/12. This would be amenable to multi-machine parallelism, and in fact I have started a [Dask implementation](https://github.com/tomwhite/pynndescent/tree/dask) that should work on a cluster. However, I haven't benchmarked the Dask implementation, so I don't know how competitive it is with the single (multi-core) machine version using threads. I have successfully run pynndescent on 10^7 rows on a single machine (50 columns, 96 cores), and I don't see why it wouldn't go further than that, although the bottleneck is memory for the heap updates.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:688,performance,memor,memory,688,"Thanks @ivirshup. The parallelism is achieved using a MapReduce scheme operating on a single NumPy array, as described here: https://github.com/lmcinnes/pynndescent/pull/12. This would be amenable to multi-machine parallelism, and in fact I have started a [Dask implementation](https://github.com/tomwhite/pynndescent/tree/dask) that should work on a cluster. However, I haven't benchmarked the Dask implementation, so I don't know how competitive it is with the single (multi-core) machine version using threads. I have successfully run pynndescent on 10^7 rows on a single machine (50 columns, 96 cores), and I don't see why it wouldn't go further than that, although the bottleneck is memory for the heap updates.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:708,safety,updat,updates,708,"Thanks @ivirshup. The parallelism is achieved using a MapReduce scheme operating on a single NumPy array, as described here: https://github.com/lmcinnes/pynndescent/pull/12. This would be amenable to multi-machine parallelism, and in fact I have started a [Dask implementation](https://github.com/tomwhite/pynndescent/tree/dask) that should work on a cluster. However, I haven't benchmarked the Dask implementation, so I don't know how competitive it is with the single (multi-core) machine version using threads. I have successfully run pynndescent on 10^7 rows on a single machine (50 columns, 96 cores), and I don't see why it wouldn't go further than that, although the bottleneck is memory for the heap updates.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:708,security,updat,updates,708,"Thanks @ivirshup. The parallelism is achieved using a MapReduce scheme operating on a single NumPy array, as described here: https://github.com/lmcinnes/pynndescent/pull/12. This would be amenable to multi-machine parallelism, and in fact I have started a [Dask implementation](https://github.com/tomwhite/pynndescent/tree/dask) that should work on a cluster. However, I haven't benchmarked the Dask implementation, so I don't know how competitive it is with the single (multi-core) machine version using threads. I have successfully run pynndescent on 10^7 rows on a single machine (50 columns, 96 cores), and I don't see why it wouldn't go further than that, although the bottleneck is memory for the heap updates.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:688,usability,memor,memory,688,"Thanks @ivirshup. The parallelism is achieved using a MapReduce scheme operating on a single NumPy array, as described here: https://github.com/lmcinnes/pynndescent/pull/12. This would be amenable to multi-machine parallelism, and in fact I have started a [Dask implementation](https://github.com/tomwhite/pynndescent/tree/dask) that should work on a cluster. However, I haven't benchmarked the Dask implementation, so I don't know how competitive it is with the single (multi-core) machine version using threads. I have successfully run pynndescent on 10^7 rows on a single machine (50 columns, 96 cores), and I don't see why it wouldn't go further than that, although the bottleneck is memory for the heap updates.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:176,deployability,scale,scale,176,Even cooler! Do you recall whether the memory usage for a million cells was anything prohibitive? I was actually thinking of utilizing distribution across machines as a way to scale out if memory usage started becoming an issue (or even just using the same batching strategy on one machine). This could be very useful for large scale dataset integration. Though I'm curious if using nn-descent could introduce some bias towards merging data.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:328,deployability,scale,scale,328,Even cooler! Do you recall whether the memory usage for a million cells was anything prohibitive? I was actually thinking of utilizing distribution across machines as a way to scale out if memory usage started becoming an issue (or even just using the same batching strategy on one machine). This could be very useful for large scale dataset integration. Though I'm curious if using nn-descent could introduce some bias towards merging data.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:342,deployability,integr,integration,342,Even cooler! Do you recall whether the memory usage for a million cells was anything prohibitive? I was actually thinking of utilizing distribution across machines as a way to scale out if memory usage started becoming an issue (or even just using the same batching strategy on one machine). This could be very useful for large scale dataset integration. Though I'm curious if using nn-descent could introduce some bias towards merging data.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:5,energy efficiency,cool,cooler,5,Even cooler! Do you recall whether the memory usage for a million cells was anything prohibitive? I was actually thinking of utilizing distribution across machines as a way to scale out if memory usage started becoming an issue (or even just using the same batching strategy on one machine). This could be very useful for large scale dataset integration. Though I'm curious if using nn-descent could introduce some bias towards merging data.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:176,energy efficiency,scale,scale,176,Even cooler! Do you recall whether the memory usage for a million cells was anything prohibitive? I was actually thinking of utilizing distribution across machines as a way to scale out if memory usage started becoming an issue (or even just using the same batching strategy on one machine). This could be very useful for large scale dataset integration. Though I'm curious if using nn-descent could introduce some bias towards merging data.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:328,energy efficiency,scale,scale,328,Even cooler! Do you recall whether the memory usage for a million cells was anything prohibitive? I was actually thinking of utilizing distribution across machines as a way to scale out if memory usage started becoming an issue (or even just using the same batching strategy on one machine). This could be very useful for large scale dataset integration. Though I'm curious if using nn-descent could introduce some bias towards merging data.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:257,integrability,batch,batching,257,Even cooler! Do you recall whether the memory usage for a million cells was anything prohibitive? I was actually thinking of utilizing distribution across machines as a way to scale out if memory usage started becoming an issue (or even just using the same batching strategy on one machine). This could be very useful for large scale dataset integration. Though I'm curious if using nn-descent could introduce some bias towards merging data.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:342,integrability,integr,integration,342,Even cooler! Do you recall whether the memory usage for a million cells was anything prohibitive? I was actually thinking of utilizing distribution across machines as a way to scale out if memory usage started becoming an issue (or even just using the same batching strategy on one machine). This could be very useful for large scale dataset integration. Though I'm curious if using nn-descent could introduce some bias towards merging data.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:135,interoperability,distribut,distribution,135,Even cooler! Do you recall whether the memory usage for a million cells was anything prohibitive? I was actually thinking of utilizing distribution across machines as a way to scale out if memory usage started becoming an issue (or even just using the same batching strategy on one machine). This could be very useful for large scale dataset integration. Though I'm curious if using nn-descent could introduce some bias towards merging data.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:342,interoperability,integr,integration,342,Even cooler! Do you recall whether the memory usage for a million cells was anything prohibitive? I was actually thinking of utilizing distribution across machines as a way to scale out if memory usage started becoming an issue (or even just using the same batching strategy on one machine). This could be very useful for large scale dataset integration. Though I'm curious if using nn-descent could introduce some bias towards merging data.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:176,modifiability,scal,scale,176,Even cooler! Do you recall whether the memory usage for a million cells was anything prohibitive? I was actually thinking of utilizing distribution across machines as a way to scale out if memory usage started becoming an issue (or even just using the same batching strategy on one machine). This could be very useful for large scale dataset integration. Though I'm curious if using nn-descent could introduce some bias towards merging data.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:328,modifiability,scal,scale,328,Even cooler! Do you recall whether the memory usage for a million cells was anything prohibitive? I was actually thinking of utilizing distribution across machines as a way to scale out if memory usage started becoming an issue (or even just using the same batching strategy on one machine). This could be very useful for large scale dataset integration. Though I'm curious if using nn-descent could introduce some bias towards merging data.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:342,modifiability,integr,integration,342,Even cooler! Do you recall whether the memory usage for a million cells was anything prohibitive? I was actually thinking of utilizing distribution across machines as a way to scale out if memory usage started becoming an issue (or even just using the same batching strategy on one machine). This could be very useful for large scale dataset integration. Though I'm curious if using nn-descent could introduce some bias towards merging data.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:39,performance,memor,memory,39,Even cooler! Do you recall whether the memory usage for a million cells was anything prohibitive? I was actually thinking of utilizing distribution across machines as a way to scale out if memory usage started becoming an issue (or even just using the same batching strategy on one machine). This could be very useful for large scale dataset integration. Though I'm curious if using nn-descent could introduce some bias towards merging data.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:176,performance,scale,scale,176,Even cooler! Do you recall whether the memory usage for a million cells was anything prohibitive? I was actually thinking of utilizing distribution across machines as a way to scale out if memory usage started becoming an issue (or even just using the same batching strategy on one machine). This could be very useful for large scale dataset integration. Though I'm curious if using nn-descent could introduce some bias towards merging data.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:189,performance,memor,memory,189,Even cooler! Do you recall whether the memory usage for a million cells was anything prohibitive? I was actually thinking of utilizing distribution across machines as a way to scale out if memory usage started becoming an issue (or even just using the same batching strategy on one machine). This could be very useful for large scale dataset integration. Though I'm curious if using nn-descent could introduce some bias towards merging data.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:257,performance,batch,batching,257,Even cooler! Do you recall whether the memory usage for a million cells was anything prohibitive? I was actually thinking of utilizing distribution across machines as a way to scale out if memory usage started becoming an issue (or even just using the same batching strategy on one machine). This could be very useful for large scale dataset integration. Though I'm curious if using nn-descent could introduce some bias towards merging data.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:328,performance,scale,scale,328,Even cooler! Do you recall whether the memory usage for a million cells was anything prohibitive? I was actually thinking of utilizing distribution across machines as a way to scale out if memory usage started becoming an issue (or even just using the same batching strategy on one machine). This could be very useful for large scale dataset integration. Though I'm curious if using nn-descent could introduce some bias towards merging data.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:342,reliability,integr,integration,342,Even cooler! Do you recall whether the memory usage for a million cells was anything prohibitive? I was actually thinking of utilizing distribution across machines as a way to scale out if memory usage started becoming an issue (or even just using the same batching strategy on one machine). This could be very useful for large scale dataset integration. Though I'm curious if using nn-descent could introduce some bias towards merging data.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:342,security,integr,integration,342,Even cooler! Do you recall whether the memory usage for a million cells was anything prohibitive? I was actually thinking of utilizing distribution across machines as a way to scale out if memory usage started becoming an issue (or even just using the same batching strategy on one machine). This could be very useful for large scale dataset integration. Though I'm curious if using nn-descent could introduce some bias towards merging data.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:342,testability,integr,integration,342,Even cooler! Do you recall whether the memory usage for a million cells was anything prohibitive? I was actually thinking of utilizing distribution across machines as a way to scale out if memory usage started becoming an issue (or even just using the same batching strategy on one machine). This could be very useful for large scale dataset integration. Though I'm curious if using nn-descent could introduce some bias towards merging data.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:39,usability,memor,memory,39,Even cooler! Do you recall whether the memory usage for a million cells was anything prohibitive? I was actually thinking of utilizing distribution across machines as a way to scale out if memory usage started becoming an issue (or even just using the same batching strategy on one machine). This could be very useful for large scale dataset integration. Though I'm curious if using nn-descent could introduce some bias towards merging data.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:189,usability,memor,memory,189,Even cooler! Do you recall whether the memory usage for a million cells was anything prohibitive? I was actually thinking of utilizing distribution across machines as a way to scale out if memory usage started becoming an issue (or even just using the same batching strategy on one machine). This could be very useful for large scale dataset integration. Though I'm curious if using nn-descent could introduce some bias towards merging data.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:87,safety,test,tested,87,We might consider waiting until the sparse matrix support of pynndescent is thoroughly tested (e.g. https://github.com/lmcinnes/pynndescent/issues/65),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:87,testability,test,tested,87,We might consider waiting until the sparse matrix support of pynndescent is thoroughly tested (e.g. https://github.com/lmcinnes/pynndescent/issues/65),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:50,usability,support,support,50,We might consider waiting until the sparse matrix support of pynndescent is thoroughly tested (e.g. https://github.com/lmcinnes/pynndescent/issues/65),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:22,usability,statu,status,22,"@tomwhite, what's the status of this PR?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:49,deployability,releas,released,49,"This is no longer needed since UMAP 0.4 (not yet released) will use pynndescent if it is installed, see https://github.com/lmcinnes/umap/pull/278.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/pull/659:89,deployability,instal,installed,89,"This is no longer needed since UMAP 0.4 (not yet released) will use pynndescent if it is installed, see https://github.com/lmcinnes/umap/pull/278.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/659
https://github.com/scverse/scanpy/issues/660:83,availability,down,downgrade,83,There is an issue with the new Scipy. `statsmodels` is conflicting with it. Either downgrade scipy or upgrade statsmodels as soon as they fixed it.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/660
https://github.com/scverse/scanpy/issues/660:102,deployability,upgrad,upgrade,102,There is an issue with the new Scipy. `statsmodels` is conflicting with it. Either downgrade scipy or upgrade statsmodels as soon as they fixed it.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/660
https://github.com/scverse/scanpy/issues/660:55,interoperability,conflict,conflicting,55,There is an issue with the new Scipy. `statsmodels` is conflicting with it. Either downgrade scipy or upgrade statsmodels as soon as they fixed it.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/660
https://github.com/scverse/scanpy/issues/660:102,modifiability,upgrad,upgrade,102,There is an issue with the new Scipy. `statsmodels` is conflicting with it. Either downgrade scipy or upgrade statsmodels as soon as they fixed it.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/660
https://github.com/scverse/scanpy/pull/661:18,deployability,fail,fail,18,Some of the tests fail for reasons unrelated to the PR (`test_preprocessing`). Locally all tests pass for me. Any ideas?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:18,reliability,fail,fail,18,Some of the tests fail for reasons unrelated to the PR (`test_preprocessing`). Locally all tests pass for me. Any ideas?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:12,safety,test,tests,12,Some of the tests fail for reasons unrelated to the PR (`test_preprocessing`). Locally all tests pass for me. Any ideas?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:91,safety,test,tests,91,Some of the tests fail for reasons unrelated to the PR (`test_preprocessing`). Locally all tests pass for me. Any ideas?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:12,testability,test,tests,12,Some of the tests fail for reasons unrelated to the PR (`test_preprocessing`). Locally all tests pass for me. Any ideas?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:91,testability,test,tests,91,Some of the tests fail for reasons unrelated to the PR (`test_preprocessing`). Locally all tests pass for me. Any ideas?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:42,deployability,depend,dependency,42,I think itâ€™s again some flakiness of some dependency (or our interaction with it),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:42,integrability,depend,dependency,42,I think itâ€™s again some flakiness of some dependency (or our interaction with it),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:42,modifiability,depend,dependency,42,I think itâ€™s again some flakiness of some dependency (or our interaction with it),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:42,safety,depend,dependency,42,I think itâ€™s again some flakiness of some dependency (or our interaction with it),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:42,testability,depend,dependency,42,I think itâ€™s again some flakiness of some dependency (or our interaction with it),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:61,usability,interact,interaction,61,I think itâ€™s again some flakiness of some dependency (or our interaction with it),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:44,deployability,fail,failing,44,@flying-sheep I see that the same tests are failing in other PRs. Do you have any idea what and when some change was introduced that broke the tests,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:44,reliability,fail,failing,44,@flying-sheep I see that the same tests are failing in other PRs. Do you have any idea what and when some change was introduced that broke the tests,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:34,safety,test,tests,34,@flying-sheep I see that the same tests are failing in other PRs. Do you have any idea what and when some change was introduced that broke the tests,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:143,safety,test,tests,143,@flying-sheep I see that the same tests are failing in other PRs. Do you have any idea what and when some change was introduced that broke the tests,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:34,testability,test,tests,34,@flying-sheep I see that the same tests are failing in other PRs. Do you have any idea what and when some change was introduced that broke the tests,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:143,testability,test,tests,143,@flying-sheep I see that the same tests are failing in other PRs. Do you have any idea what and when some change was introduced that broke the tests,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:523,availability,error,errors,523,"Looking at the commits page I can see that the tests has been failing for some days already. . First was a problem with a notebook test (test_pbmc3k) that seems innocuous but should be addressed. This is related to release 1.4.3 (https://github.com/theislab/scanpy/commit/85acb6c8949d43d08a26437dceab4fa5db79e246). The commits are unrelated to the failing test so I assume that some dependency was updated . However, after this commit https://github.com/theislab/scanpy/commit/115d635bf950354509053d976b90c1db518bcffe more errors are found. But again, I don't see any relevant changes that will cause the problems. One of the errors is that statsmodels is using a deprecated module from scipy.misc:. ```. > from scipy.misc import factorial. E ImportError: cannot import name 'factorial'. ../../../virtualenv/python3.6.7/lib/python3.6/site-packages/statsmodels/distributions/edgeworth.py:7: ImportError. ```. This was introduced after scipy 1.3 was recently updated (https://github.com/statsmodels/statsmodels/issues/5759). It seems that currently, the only solution is to install statsmodels directly from the master branch. Or downgrade scipy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:626,availability,error,errors,626,"Looking at the commits page I can see that the tests has been failing for some days already. . First was a problem with a notebook test (test_pbmc3k) that seems innocuous but should be addressed. This is related to release 1.4.3 (https://github.com/theislab/scanpy/commit/85acb6c8949d43d08a26437dceab4fa5db79e246). The commits are unrelated to the failing test so I assume that some dependency was updated . However, after this commit https://github.com/theislab/scanpy/commit/115d635bf950354509053d976b90c1db518bcffe more errors are found. But again, I don't see any relevant changes that will cause the problems. One of the errors is that statsmodels is using a deprecated module from scipy.misc:. ```. > from scipy.misc import factorial. E ImportError: cannot import name 'factorial'. ../../../virtualenv/python3.6.7/lib/python3.6/site-packages/statsmodels/distributions/edgeworth.py:7: ImportError. ```. This was introduced after scipy 1.3 was recently updated (https://github.com/statsmodels/statsmodels/issues/5759). It seems that currently, the only solution is to install statsmodels directly from the master branch. Or downgrade scipy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:1128,availability,down,downgrade,1128,"Looking at the commits page I can see that the tests has been failing for some days already. . First was a problem with a notebook test (test_pbmc3k) that seems innocuous but should be addressed. This is related to release 1.4.3 (https://github.com/theislab/scanpy/commit/85acb6c8949d43d08a26437dceab4fa5db79e246). The commits are unrelated to the failing test so I assume that some dependency was updated . However, after this commit https://github.com/theislab/scanpy/commit/115d635bf950354509053d976b90c1db518bcffe more errors are found. But again, I don't see any relevant changes that will cause the problems. One of the errors is that statsmodels is using a deprecated module from scipy.misc:. ```. > from scipy.misc import factorial. E ImportError: cannot import name 'factorial'. ../../../virtualenv/python3.6.7/lib/python3.6/site-packages/statsmodels/distributions/edgeworth.py:7: ImportError. ```. This was introduced after scipy 1.3 was recently updated (https://github.com/statsmodels/statsmodels/issues/5759). It seems that currently, the only solution is to install statsmodels directly from the master branch. Or downgrade scipy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:62,deployability,fail,failing,62,"Looking at the commits page I can see that the tests has been failing for some days already. . First was a problem with a notebook test (test_pbmc3k) that seems innocuous but should be addressed. This is related to release 1.4.3 (https://github.com/theislab/scanpy/commit/85acb6c8949d43d08a26437dceab4fa5db79e246). The commits are unrelated to the failing test so I assume that some dependency was updated . However, after this commit https://github.com/theislab/scanpy/commit/115d635bf950354509053d976b90c1db518bcffe more errors are found. But again, I don't see any relevant changes that will cause the problems. One of the errors is that statsmodels is using a deprecated module from scipy.misc:. ```. > from scipy.misc import factorial. E ImportError: cannot import name 'factorial'. ../../../virtualenv/python3.6.7/lib/python3.6/site-packages/statsmodels/distributions/edgeworth.py:7: ImportError. ```. This was introduced after scipy 1.3 was recently updated (https://github.com/statsmodels/statsmodels/issues/5759). It seems that currently, the only solution is to install statsmodels directly from the master branch. Or downgrade scipy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:215,deployability,releas,release,215,"Looking at the commits page I can see that the tests has been failing for some days already. . First was a problem with a notebook test (test_pbmc3k) that seems innocuous but should be addressed. This is related to release 1.4.3 (https://github.com/theislab/scanpy/commit/85acb6c8949d43d08a26437dceab4fa5db79e246). The commits are unrelated to the failing test so I assume that some dependency was updated . However, after this commit https://github.com/theislab/scanpy/commit/115d635bf950354509053d976b90c1db518bcffe more errors are found. But again, I don't see any relevant changes that will cause the problems. One of the errors is that statsmodels is using a deprecated module from scipy.misc:. ```. > from scipy.misc import factorial. E ImportError: cannot import name 'factorial'. ../../../virtualenv/python3.6.7/lib/python3.6/site-packages/statsmodels/distributions/edgeworth.py:7: ImportError. ```. This was introduced after scipy 1.3 was recently updated (https://github.com/statsmodels/statsmodels/issues/5759). It seems that currently, the only solution is to install statsmodels directly from the master branch. Or downgrade scipy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:348,deployability,fail,failing,348,"Looking at the commits page I can see that the tests has been failing for some days already. . First was a problem with a notebook test (test_pbmc3k) that seems innocuous but should be addressed. This is related to release 1.4.3 (https://github.com/theislab/scanpy/commit/85acb6c8949d43d08a26437dceab4fa5db79e246). The commits are unrelated to the failing test so I assume that some dependency was updated . However, after this commit https://github.com/theislab/scanpy/commit/115d635bf950354509053d976b90c1db518bcffe more errors are found. But again, I don't see any relevant changes that will cause the problems. One of the errors is that statsmodels is using a deprecated module from scipy.misc:. ```. > from scipy.misc import factorial. E ImportError: cannot import name 'factorial'. ../../../virtualenv/python3.6.7/lib/python3.6/site-packages/statsmodels/distributions/edgeworth.py:7: ImportError. ```. This was introduced after scipy 1.3 was recently updated (https://github.com/statsmodels/statsmodels/issues/5759). It seems that currently, the only solution is to install statsmodels directly from the master branch. Or downgrade scipy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:383,deployability,depend,dependency,383,"Looking at the commits page I can see that the tests has been failing for some days already. . First was a problem with a notebook test (test_pbmc3k) that seems innocuous but should be addressed. This is related to release 1.4.3 (https://github.com/theislab/scanpy/commit/85acb6c8949d43d08a26437dceab4fa5db79e246). The commits are unrelated to the failing test so I assume that some dependency was updated . However, after this commit https://github.com/theislab/scanpy/commit/115d635bf950354509053d976b90c1db518bcffe more errors are found. But again, I don't see any relevant changes that will cause the problems. One of the errors is that statsmodels is using a deprecated module from scipy.misc:. ```. > from scipy.misc import factorial. E ImportError: cannot import name 'factorial'. ../../../virtualenv/python3.6.7/lib/python3.6/site-packages/statsmodels/distributions/edgeworth.py:7: ImportError. ```. This was introduced after scipy 1.3 was recently updated (https://github.com/statsmodels/statsmodels/issues/5759). It seems that currently, the only solution is to install statsmodels directly from the master branch. Or downgrade scipy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:398,deployability,updat,updated,398,"Looking at the commits page I can see that the tests has been failing for some days already. . First was a problem with a notebook test (test_pbmc3k) that seems innocuous but should be addressed. This is related to release 1.4.3 (https://github.com/theislab/scanpy/commit/85acb6c8949d43d08a26437dceab4fa5db79e246). The commits are unrelated to the failing test so I assume that some dependency was updated . However, after this commit https://github.com/theislab/scanpy/commit/115d635bf950354509053d976b90c1db518bcffe more errors are found. But again, I don't see any relevant changes that will cause the problems. One of the errors is that statsmodels is using a deprecated module from scipy.misc:. ```. > from scipy.misc import factorial. E ImportError: cannot import name 'factorial'. ../../../virtualenv/python3.6.7/lib/python3.6/site-packages/statsmodels/distributions/edgeworth.py:7: ImportError. ```. This was introduced after scipy 1.3 was recently updated (https://github.com/statsmodels/statsmodels/issues/5759). It seems that currently, the only solution is to install statsmodels directly from the master branch. Or downgrade scipy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:675,deployability,modul,module,675,"Looking at the commits page I can see that the tests has been failing for some days already. . First was a problem with a notebook test (test_pbmc3k) that seems innocuous but should be addressed. This is related to release 1.4.3 (https://github.com/theislab/scanpy/commit/85acb6c8949d43d08a26437dceab4fa5db79e246). The commits are unrelated to the failing test so I assume that some dependency was updated . However, after this commit https://github.com/theislab/scanpy/commit/115d635bf950354509053d976b90c1db518bcffe more errors are found. But again, I don't see any relevant changes that will cause the problems. One of the errors is that statsmodels is using a deprecated module from scipy.misc:. ```. > from scipy.misc import factorial. E ImportError: cannot import name 'factorial'. ../../../virtualenv/python3.6.7/lib/python3.6/site-packages/statsmodels/distributions/edgeworth.py:7: ImportError. ```. This was introduced after scipy 1.3 was recently updated (https://github.com/statsmodels/statsmodels/issues/5759). It seems that currently, the only solution is to install statsmodels directly from the master branch. Or downgrade scipy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:957,deployability,updat,updated,957,"Looking at the commits page I can see that the tests has been failing for some days already. . First was a problem with a notebook test (test_pbmc3k) that seems innocuous but should be addressed. This is related to release 1.4.3 (https://github.com/theislab/scanpy/commit/85acb6c8949d43d08a26437dceab4fa5db79e246). The commits are unrelated to the failing test so I assume that some dependency was updated . However, after this commit https://github.com/theislab/scanpy/commit/115d635bf950354509053d976b90c1db518bcffe more errors are found. But again, I don't see any relevant changes that will cause the problems. One of the errors is that statsmodels is using a deprecated module from scipy.misc:. ```. > from scipy.misc import factorial. E ImportError: cannot import name 'factorial'. ../../../virtualenv/python3.6.7/lib/python3.6/site-packages/statsmodels/distributions/edgeworth.py:7: ImportError. ```. This was introduced after scipy 1.3 was recently updated (https://github.com/statsmodels/statsmodels/issues/5759). It seems that currently, the only solution is to install statsmodels directly from the master branch. Or downgrade scipy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:1072,deployability,instal,install,1072,"Looking at the commits page I can see that the tests has been failing for some days already. . First was a problem with a notebook test (test_pbmc3k) that seems innocuous but should be addressed. This is related to release 1.4.3 (https://github.com/theislab/scanpy/commit/85acb6c8949d43d08a26437dceab4fa5db79e246). The commits are unrelated to the failing test so I assume that some dependency was updated . However, after this commit https://github.com/theislab/scanpy/commit/115d635bf950354509053d976b90c1db518bcffe more errors are found. But again, I don't see any relevant changes that will cause the problems. One of the errors is that statsmodels is using a deprecated module from scipy.misc:. ```. > from scipy.misc import factorial. E ImportError: cannot import name 'factorial'. ../../../virtualenv/python3.6.7/lib/python3.6/site-packages/statsmodels/distributions/edgeworth.py:7: ImportError. ```. This was introduced after scipy 1.3 was recently updated (https://github.com/statsmodels/statsmodels/issues/5759). It seems that currently, the only solution is to install statsmodels directly from the master branch. Or downgrade scipy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:1037,energy efficiency,current,currently,1037,"Looking at the commits page I can see that the tests has been failing for some days already. . First was a problem with a notebook test (test_pbmc3k) that seems innocuous but should be addressed. This is related to release 1.4.3 (https://github.com/theislab/scanpy/commit/85acb6c8949d43d08a26437dceab4fa5db79e246). The commits are unrelated to the failing test so I assume that some dependency was updated . However, after this commit https://github.com/theislab/scanpy/commit/115d635bf950354509053d976b90c1db518bcffe more errors are found. But again, I don't see any relevant changes that will cause the problems. One of the errors is that statsmodels is using a deprecated module from scipy.misc:. ```. > from scipy.misc import factorial. E ImportError: cannot import name 'factorial'. ../../../virtualenv/python3.6.7/lib/python3.6/site-packages/statsmodels/distributions/edgeworth.py:7: ImportError. ```. This was introduced after scipy 1.3 was recently updated (https://github.com/statsmodels/statsmodels/issues/5759). It seems that currently, the only solution is to install statsmodels directly from the master branch. Or downgrade scipy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:383,integrability,depend,dependency,383,"Looking at the commits page I can see that the tests has been failing for some days already. . First was a problem with a notebook test (test_pbmc3k) that seems innocuous but should be addressed. This is related to release 1.4.3 (https://github.com/theislab/scanpy/commit/85acb6c8949d43d08a26437dceab4fa5db79e246). The commits are unrelated to the failing test so I assume that some dependency was updated . However, after this commit https://github.com/theislab/scanpy/commit/115d635bf950354509053d976b90c1db518bcffe more errors are found. But again, I don't see any relevant changes that will cause the problems. One of the errors is that statsmodels is using a deprecated module from scipy.misc:. ```. > from scipy.misc import factorial. E ImportError: cannot import name 'factorial'. ../../../virtualenv/python3.6.7/lib/python3.6/site-packages/statsmodels/distributions/edgeworth.py:7: ImportError. ```. This was introduced after scipy 1.3 was recently updated (https://github.com/statsmodels/statsmodels/issues/5759). It seems that currently, the only solution is to install statsmodels directly from the master branch. Or downgrade scipy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:860,interoperability,distribut,distributions,860,"Looking at the commits page I can see that the tests has been failing for some days already. . First was a problem with a notebook test (test_pbmc3k) that seems innocuous but should be addressed. This is related to release 1.4.3 (https://github.com/theislab/scanpy/commit/85acb6c8949d43d08a26437dceab4fa5db79e246). The commits are unrelated to the failing test so I assume that some dependency was updated . However, after this commit https://github.com/theislab/scanpy/commit/115d635bf950354509053d976b90c1db518bcffe more errors are found. But again, I don't see any relevant changes that will cause the problems. One of the errors is that statsmodels is using a deprecated module from scipy.misc:. ```. > from scipy.misc import factorial. E ImportError: cannot import name 'factorial'. ../../../virtualenv/python3.6.7/lib/python3.6/site-packages/statsmodels/distributions/edgeworth.py:7: ImportError. ```. This was introduced after scipy 1.3 was recently updated (https://github.com/statsmodels/statsmodels/issues/5759). It seems that currently, the only solution is to install statsmodels directly from the master branch. Or downgrade scipy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:383,modifiability,depend,dependency,383,"Looking at the commits page I can see that the tests has been failing for some days already. . First was a problem with a notebook test (test_pbmc3k) that seems innocuous but should be addressed. This is related to release 1.4.3 (https://github.com/theislab/scanpy/commit/85acb6c8949d43d08a26437dceab4fa5db79e246). The commits are unrelated to the failing test so I assume that some dependency was updated . However, after this commit https://github.com/theislab/scanpy/commit/115d635bf950354509053d976b90c1db518bcffe more errors are found. But again, I don't see any relevant changes that will cause the problems. One of the errors is that statsmodels is using a deprecated module from scipy.misc:. ```. > from scipy.misc import factorial. E ImportError: cannot import name 'factorial'. ../../../virtualenv/python3.6.7/lib/python3.6/site-packages/statsmodels/distributions/edgeworth.py:7: ImportError. ```. This was introduced after scipy 1.3 was recently updated (https://github.com/statsmodels/statsmodels/issues/5759). It seems that currently, the only solution is to install statsmodels directly from the master branch. Or downgrade scipy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:675,modifiability,modul,module,675,"Looking at the commits page I can see that the tests has been failing for some days already. . First was a problem with a notebook test (test_pbmc3k) that seems innocuous but should be addressed. This is related to release 1.4.3 (https://github.com/theislab/scanpy/commit/85acb6c8949d43d08a26437dceab4fa5db79e246). The commits are unrelated to the failing test so I assume that some dependency was updated . However, after this commit https://github.com/theislab/scanpy/commit/115d635bf950354509053d976b90c1db518bcffe more errors are found. But again, I don't see any relevant changes that will cause the problems. One of the errors is that statsmodels is using a deprecated module from scipy.misc:. ```. > from scipy.misc import factorial. E ImportError: cannot import name 'factorial'. ../../../virtualenv/python3.6.7/lib/python3.6/site-packages/statsmodels/distributions/edgeworth.py:7: ImportError. ```. This was introduced after scipy 1.3 was recently updated (https://github.com/statsmodels/statsmodels/issues/5759). It seems that currently, the only solution is to install statsmodels directly from the master branch. Or downgrade scipy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:839,modifiability,pac,packages,839,"Looking at the commits page I can see that the tests has been failing for some days already. . First was a problem with a notebook test (test_pbmc3k) that seems innocuous but should be addressed. This is related to release 1.4.3 (https://github.com/theislab/scanpy/commit/85acb6c8949d43d08a26437dceab4fa5db79e246). The commits are unrelated to the failing test so I assume that some dependency was updated . However, after this commit https://github.com/theislab/scanpy/commit/115d635bf950354509053d976b90c1db518bcffe more errors are found. But again, I don't see any relevant changes that will cause the problems. One of the errors is that statsmodels is using a deprecated module from scipy.misc:. ```. > from scipy.misc import factorial. E ImportError: cannot import name 'factorial'. ../../../virtualenv/python3.6.7/lib/python3.6/site-packages/statsmodels/distributions/edgeworth.py:7: ImportError. ```. This was introduced after scipy 1.3 was recently updated (https://github.com/statsmodels/statsmodels/issues/5759). It seems that currently, the only solution is to install statsmodels directly from the master branch. Or downgrade scipy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:523,performance,error,errors,523,"Looking at the commits page I can see that the tests has been failing for some days already. . First was a problem with a notebook test (test_pbmc3k) that seems innocuous but should be addressed. This is related to release 1.4.3 (https://github.com/theislab/scanpy/commit/85acb6c8949d43d08a26437dceab4fa5db79e246). The commits are unrelated to the failing test so I assume that some dependency was updated . However, after this commit https://github.com/theislab/scanpy/commit/115d635bf950354509053d976b90c1db518bcffe more errors are found. But again, I don't see any relevant changes that will cause the problems. One of the errors is that statsmodels is using a deprecated module from scipy.misc:. ```. > from scipy.misc import factorial. E ImportError: cannot import name 'factorial'. ../../../virtualenv/python3.6.7/lib/python3.6/site-packages/statsmodels/distributions/edgeworth.py:7: ImportError. ```. This was introduced after scipy 1.3 was recently updated (https://github.com/statsmodels/statsmodels/issues/5759). It seems that currently, the only solution is to install statsmodels directly from the master branch. Or downgrade scipy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:626,performance,error,errors,626,"Looking at the commits page I can see that the tests has been failing for some days already. . First was a problem with a notebook test (test_pbmc3k) that seems innocuous but should be addressed. This is related to release 1.4.3 (https://github.com/theislab/scanpy/commit/85acb6c8949d43d08a26437dceab4fa5db79e246). The commits are unrelated to the failing test so I assume that some dependency was updated . However, after this commit https://github.com/theislab/scanpy/commit/115d635bf950354509053d976b90c1db518bcffe more errors are found. But again, I don't see any relevant changes that will cause the problems. One of the errors is that statsmodels is using a deprecated module from scipy.misc:. ```. > from scipy.misc import factorial. E ImportError: cannot import name 'factorial'. ../../../virtualenv/python3.6.7/lib/python3.6/site-packages/statsmodels/distributions/edgeworth.py:7: ImportError. ```. This was introduced after scipy 1.3 was recently updated (https://github.com/statsmodels/statsmodels/issues/5759). It seems that currently, the only solution is to install statsmodels directly from the master branch. Or downgrade scipy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:62,reliability,fail,failing,62,"Looking at the commits page I can see that the tests has been failing for some days already. . First was a problem with a notebook test (test_pbmc3k) that seems innocuous but should be addressed. This is related to release 1.4.3 (https://github.com/theislab/scanpy/commit/85acb6c8949d43d08a26437dceab4fa5db79e246). The commits are unrelated to the failing test so I assume that some dependency was updated . However, after this commit https://github.com/theislab/scanpy/commit/115d635bf950354509053d976b90c1db518bcffe more errors are found. But again, I don't see any relevant changes that will cause the problems. One of the errors is that statsmodels is using a deprecated module from scipy.misc:. ```. > from scipy.misc import factorial. E ImportError: cannot import name 'factorial'. ../../../virtualenv/python3.6.7/lib/python3.6/site-packages/statsmodels/distributions/edgeworth.py:7: ImportError. ```. This was introduced after scipy 1.3 was recently updated (https://github.com/statsmodels/statsmodels/issues/5759). It seems that currently, the only solution is to install statsmodels directly from the master branch. Or downgrade scipy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:348,reliability,fail,failing,348,"Looking at the commits page I can see that the tests has been failing for some days already. . First was a problem with a notebook test (test_pbmc3k) that seems innocuous but should be addressed. This is related to release 1.4.3 (https://github.com/theislab/scanpy/commit/85acb6c8949d43d08a26437dceab4fa5db79e246). The commits are unrelated to the failing test so I assume that some dependency was updated . However, after this commit https://github.com/theislab/scanpy/commit/115d635bf950354509053d976b90c1db518bcffe more errors are found. But again, I don't see any relevant changes that will cause the problems. One of the errors is that statsmodels is using a deprecated module from scipy.misc:. ```. > from scipy.misc import factorial. E ImportError: cannot import name 'factorial'. ../../../virtualenv/python3.6.7/lib/python3.6/site-packages/statsmodels/distributions/edgeworth.py:7: ImportError. ```. This was introduced after scipy 1.3 was recently updated (https://github.com/statsmodels/statsmodels/issues/5759). It seems that currently, the only solution is to install statsmodels directly from the master branch. Or downgrade scipy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:47,safety,test,tests,47,"Looking at the commits page I can see that the tests has been failing for some days already. . First was a problem with a notebook test (test_pbmc3k) that seems innocuous but should be addressed. This is related to release 1.4.3 (https://github.com/theislab/scanpy/commit/85acb6c8949d43d08a26437dceab4fa5db79e246). The commits are unrelated to the failing test so I assume that some dependency was updated . However, after this commit https://github.com/theislab/scanpy/commit/115d635bf950354509053d976b90c1db518bcffe more errors are found. But again, I don't see any relevant changes that will cause the problems. One of the errors is that statsmodels is using a deprecated module from scipy.misc:. ```. > from scipy.misc import factorial. E ImportError: cannot import name 'factorial'. ../../../virtualenv/python3.6.7/lib/python3.6/site-packages/statsmodels/distributions/edgeworth.py:7: ImportError. ```. This was introduced after scipy 1.3 was recently updated (https://github.com/statsmodels/statsmodels/issues/5759). It seems that currently, the only solution is to install statsmodels directly from the master branch. Or downgrade scipy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:131,safety,test,test,131,"Looking at the commits page I can see that the tests has been failing for some days already. . First was a problem with a notebook test (test_pbmc3k) that seems innocuous but should be addressed. This is related to release 1.4.3 (https://github.com/theislab/scanpy/commit/85acb6c8949d43d08a26437dceab4fa5db79e246). The commits are unrelated to the failing test so I assume that some dependency was updated . However, after this commit https://github.com/theislab/scanpy/commit/115d635bf950354509053d976b90c1db518bcffe more errors are found. But again, I don't see any relevant changes that will cause the problems. One of the errors is that statsmodels is using a deprecated module from scipy.misc:. ```. > from scipy.misc import factorial. E ImportError: cannot import name 'factorial'. ../../../virtualenv/python3.6.7/lib/python3.6/site-packages/statsmodels/distributions/edgeworth.py:7: ImportError. ```. This was introduced after scipy 1.3 was recently updated (https://github.com/statsmodels/statsmodels/issues/5759). It seems that currently, the only solution is to install statsmodels directly from the master branch. Or downgrade scipy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:356,safety,test,test,356,"Looking at the commits page I can see that the tests has been failing for some days already. . First was a problem with a notebook test (test_pbmc3k) that seems innocuous but should be addressed. This is related to release 1.4.3 (https://github.com/theislab/scanpy/commit/85acb6c8949d43d08a26437dceab4fa5db79e246). The commits are unrelated to the failing test so I assume that some dependency was updated . However, after this commit https://github.com/theislab/scanpy/commit/115d635bf950354509053d976b90c1db518bcffe more errors are found. But again, I don't see any relevant changes that will cause the problems. One of the errors is that statsmodels is using a deprecated module from scipy.misc:. ```. > from scipy.misc import factorial. E ImportError: cannot import name 'factorial'. ../../../virtualenv/python3.6.7/lib/python3.6/site-packages/statsmodels/distributions/edgeworth.py:7: ImportError. ```. This was introduced after scipy 1.3 was recently updated (https://github.com/statsmodels/statsmodels/issues/5759). It seems that currently, the only solution is to install statsmodels directly from the master branch. Or downgrade scipy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:383,safety,depend,dependency,383,"Looking at the commits page I can see that the tests has been failing for some days already. . First was a problem with a notebook test (test_pbmc3k) that seems innocuous but should be addressed. This is related to release 1.4.3 (https://github.com/theislab/scanpy/commit/85acb6c8949d43d08a26437dceab4fa5db79e246). The commits are unrelated to the failing test so I assume that some dependency was updated . However, after this commit https://github.com/theislab/scanpy/commit/115d635bf950354509053d976b90c1db518bcffe more errors are found. But again, I don't see any relevant changes that will cause the problems. One of the errors is that statsmodels is using a deprecated module from scipy.misc:. ```. > from scipy.misc import factorial. E ImportError: cannot import name 'factorial'. ../../../virtualenv/python3.6.7/lib/python3.6/site-packages/statsmodels/distributions/edgeworth.py:7: ImportError. ```. This was introduced after scipy 1.3 was recently updated (https://github.com/statsmodels/statsmodels/issues/5759). It seems that currently, the only solution is to install statsmodels directly from the master branch. Or downgrade scipy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:398,safety,updat,updated,398,"Looking at the commits page I can see that the tests has been failing for some days already. . First was a problem with a notebook test (test_pbmc3k) that seems innocuous but should be addressed. This is related to release 1.4.3 (https://github.com/theislab/scanpy/commit/85acb6c8949d43d08a26437dceab4fa5db79e246). The commits are unrelated to the failing test so I assume that some dependency was updated . However, after this commit https://github.com/theislab/scanpy/commit/115d635bf950354509053d976b90c1db518bcffe more errors are found. But again, I don't see any relevant changes that will cause the problems. One of the errors is that statsmodels is using a deprecated module from scipy.misc:. ```. > from scipy.misc import factorial. E ImportError: cannot import name 'factorial'. ../../../virtualenv/python3.6.7/lib/python3.6/site-packages/statsmodels/distributions/edgeworth.py:7: ImportError. ```. This was introduced after scipy 1.3 was recently updated (https://github.com/statsmodels/statsmodels/issues/5759). It seems that currently, the only solution is to install statsmodels directly from the master branch. Or downgrade scipy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:523,safety,error,errors,523,"Looking at the commits page I can see that the tests has been failing for some days already. . First was a problem with a notebook test (test_pbmc3k) that seems innocuous but should be addressed. This is related to release 1.4.3 (https://github.com/theislab/scanpy/commit/85acb6c8949d43d08a26437dceab4fa5db79e246). The commits are unrelated to the failing test so I assume that some dependency was updated . However, after this commit https://github.com/theislab/scanpy/commit/115d635bf950354509053d976b90c1db518bcffe more errors are found. But again, I don't see any relevant changes that will cause the problems. One of the errors is that statsmodels is using a deprecated module from scipy.misc:. ```. > from scipy.misc import factorial. E ImportError: cannot import name 'factorial'. ../../../virtualenv/python3.6.7/lib/python3.6/site-packages/statsmodels/distributions/edgeworth.py:7: ImportError. ```. This was introduced after scipy 1.3 was recently updated (https://github.com/statsmodels/statsmodels/issues/5759). It seems that currently, the only solution is to install statsmodels directly from the master branch. Or downgrade scipy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:626,safety,error,errors,626,"Looking at the commits page I can see that the tests has been failing for some days already. . First was a problem with a notebook test (test_pbmc3k) that seems innocuous but should be addressed. This is related to release 1.4.3 (https://github.com/theislab/scanpy/commit/85acb6c8949d43d08a26437dceab4fa5db79e246). The commits are unrelated to the failing test so I assume that some dependency was updated . However, after this commit https://github.com/theislab/scanpy/commit/115d635bf950354509053d976b90c1db518bcffe more errors are found. But again, I don't see any relevant changes that will cause the problems. One of the errors is that statsmodels is using a deprecated module from scipy.misc:. ```. > from scipy.misc import factorial. E ImportError: cannot import name 'factorial'. ../../../virtualenv/python3.6.7/lib/python3.6/site-packages/statsmodels/distributions/edgeworth.py:7: ImportError. ```. This was introduced after scipy 1.3 was recently updated (https://github.com/statsmodels/statsmodels/issues/5759). It seems that currently, the only solution is to install statsmodels directly from the master branch. Or downgrade scipy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:675,safety,modul,module,675,"Looking at the commits page I can see that the tests has been failing for some days already. . First was a problem with a notebook test (test_pbmc3k) that seems innocuous but should be addressed. This is related to release 1.4.3 (https://github.com/theislab/scanpy/commit/85acb6c8949d43d08a26437dceab4fa5db79e246). The commits are unrelated to the failing test so I assume that some dependency was updated . However, after this commit https://github.com/theislab/scanpy/commit/115d635bf950354509053d976b90c1db518bcffe more errors are found. But again, I don't see any relevant changes that will cause the problems. One of the errors is that statsmodels is using a deprecated module from scipy.misc:. ```. > from scipy.misc import factorial. E ImportError: cannot import name 'factorial'. ../../../virtualenv/python3.6.7/lib/python3.6/site-packages/statsmodels/distributions/edgeworth.py:7: ImportError. ```. This was introduced after scipy 1.3 was recently updated (https://github.com/statsmodels/statsmodels/issues/5759). It seems that currently, the only solution is to install statsmodels directly from the master branch. Or downgrade scipy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:957,safety,updat,updated,957,"Looking at the commits page I can see that the tests has been failing for some days already. . First was a problem with a notebook test (test_pbmc3k) that seems innocuous but should be addressed. This is related to release 1.4.3 (https://github.com/theislab/scanpy/commit/85acb6c8949d43d08a26437dceab4fa5db79e246). The commits are unrelated to the failing test so I assume that some dependency was updated . However, after this commit https://github.com/theislab/scanpy/commit/115d635bf950354509053d976b90c1db518bcffe more errors are found. But again, I don't see any relevant changes that will cause the problems. One of the errors is that statsmodels is using a deprecated module from scipy.misc:. ```. > from scipy.misc import factorial. E ImportError: cannot import name 'factorial'. ../../../virtualenv/python3.6.7/lib/python3.6/site-packages/statsmodels/distributions/edgeworth.py:7: ImportError. ```. This was introduced after scipy 1.3 was recently updated (https://github.com/statsmodels/statsmodels/issues/5759). It seems that currently, the only solution is to install statsmodels directly from the master branch. Or downgrade scipy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:398,security,updat,updated,398,"Looking at the commits page I can see that the tests has been failing for some days already. . First was a problem with a notebook test (test_pbmc3k) that seems innocuous but should be addressed. This is related to release 1.4.3 (https://github.com/theislab/scanpy/commit/85acb6c8949d43d08a26437dceab4fa5db79e246). The commits are unrelated to the failing test so I assume that some dependency was updated . However, after this commit https://github.com/theislab/scanpy/commit/115d635bf950354509053d976b90c1db518bcffe more errors are found. But again, I don't see any relevant changes that will cause the problems. One of the errors is that statsmodels is using a deprecated module from scipy.misc:. ```. > from scipy.misc import factorial. E ImportError: cannot import name 'factorial'. ../../../virtualenv/python3.6.7/lib/python3.6/site-packages/statsmodels/distributions/edgeworth.py:7: ImportError. ```. This was introduced after scipy 1.3 was recently updated (https://github.com/statsmodels/statsmodels/issues/5759). It seems that currently, the only solution is to install statsmodels directly from the master branch. Or downgrade scipy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:957,security,updat,updated,957,"Looking at the commits page I can see that the tests has been failing for some days already. . First was a problem with a notebook test (test_pbmc3k) that seems innocuous but should be addressed. This is related to release 1.4.3 (https://github.com/theislab/scanpy/commit/85acb6c8949d43d08a26437dceab4fa5db79e246). The commits are unrelated to the failing test so I assume that some dependency was updated . However, after this commit https://github.com/theislab/scanpy/commit/115d635bf950354509053d976b90c1db518bcffe more errors are found. But again, I don't see any relevant changes that will cause the problems. One of the errors is that statsmodels is using a deprecated module from scipy.misc:. ```. > from scipy.misc import factorial. E ImportError: cannot import name 'factorial'. ../../../virtualenv/python3.6.7/lib/python3.6/site-packages/statsmodels/distributions/edgeworth.py:7: ImportError. ```. This was introduced after scipy 1.3 was recently updated (https://github.com/statsmodels/statsmodels/issues/5759). It seems that currently, the only solution is to install statsmodels directly from the master branch. Or downgrade scipy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:47,testability,test,tests,47,"Looking at the commits page I can see that the tests has been failing for some days already. . First was a problem with a notebook test (test_pbmc3k) that seems innocuous but should be addressed. This is related to release 1.4.3 (https://github.com/theislab/scanpy/commit/85acb6c8949d43d08a26437dceab4fa5db79e246). The commits are unrelated to the failing test so I assume that some dependency was updated . However, after this commit https://github.com/theislab/scanpy/commit/115d635bf950354509053d976b90c1db518bcffe more errors are found. But again, I don't see any relevant changes that will cause the problems. One of the errors is that statsmodels is using a deprecated module from scipy.misc:. ```. > from scipy.misc import factorial. E ImportError: cannot import name 'factorial'. ../../../virtualenv/python3.6.7/lib/python3.6/site-packages/statsmodels/distributions/edgeworth.py:7: ImportError. ```. This was introduced after scipy 1.3 was recently updated (https://github.com/statsmodels/statsmodels/issues/5759). It seems that currently, the only solution is to install statsmodels directly from the master branch. Or downgrade scipy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:131,testability,test,test,131,"Looking at the commits page I can see that the tests has been failing for some days already. . First was a problem with a notebook test (test_pbmc3k) that seems innocuous but should be addressed. This is related to release 1.4.3 (https://github.com/theislab/scanpy/commit/85acb6c8949d43d08a26437dceab4fa5db79e246). The commits are unrelated to the failing test so I assume that some dependency was updated . However, after this commit https://github.com/theislab/scanpy/commit/115d635bf950354509053d976b90c1db518bcffe more errors are found. But again, I don't see any relevant changes that will cause the problems. One of the errors is that statsmodels is using a deprecated module from scipy.misc:. ```. > from scipy.misc import factorial. E ImportError: cannot import name 'factorial'. ../../../virtualenv/python3.6.7/lib/python3.6/site-packages/statsmodels/distributions/edgeworth.py:7: ImportError. ```. This was introduced after scipy 1.3 was recently updated (https://github.com/statsmodels/statsmodels/issues/5759). It seems that currently, the only solution is to install statsmodels directly from the master branch. Or downgrade scipy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:356,testability,test,test,356,"Looking at the commits page I can see that the tests has been failing for some days already. . First was a problem with a notebook test (test_pbmc3k) that seems innocuous but should be addressed. This is related to release 1.4.3 (https://github.com/theislab/scanpy/commit/85acb6c8949d43d08a26437dceab4fa5db79e246). The commits are unrelated to the failing test so I assume that some dependency was updated . However, after this commit https://github.com/theislab/scanpy/commit/115d635bf950354509053d976b90c1db518bcffe more errors are found. But again, I don't see any relevant changes that will cause the problems. One of the errors is that statsmodels is using a deprecated module from scipy.misc:. ```. > from scipy.misc import factorial. E ImportError: cannot import name 'factorial'. ../../../virtualenv/python3.6.7/lib/python3.6/site-packages/statsmodels/distributions/edgeworth.py:7: ImportError. ```. This was introduced after scipy 1.3 was recently updated (https://github.com/statsmodels/statsmodels/issues/5759). It seems that currently, the only solution is to install statsmodels directly from the master branch. Or downgrade scipy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:383,testability,depend,dependency,383,"Looking at the commits page I can see that the tests has been failing for some days already. . First was a problem with a notebook test (test_pbmc3k) that seems innocuous but should be addressed. This is related to release 1.4.3 (https://github.com/theislab/scanpy/commit/85acb6c8949d43d08a26437dceab4fa5db79e246). The commits are unrelated to the failing test so I assume that some dependency was updated . However, after this commit https://github.com/theislab/scanpy/commit/115d635bf950354509053d976b90c1db518bcffe more errors are found. But again, I don't see any relevant changes that will cause the problems. One of the errors is that statsmodels is using a deprecated module from scipy.misc:. ```. > from scipy.misc import factorial. E ImportError: cannot import name 'factorial'. ../../../virtualenv/python3.6.7/lib/python3.6/site-packages/statsmodels/distributions/edgeworth.py:7: ImportError. ```. This was introduced after scipy 1.3 was recently updated (https://github.com/statsmodels/statsmodels/issues/5759). It seems that currently, the only solution is to install statsmodels directly from the master branch. Or downgrade scipy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:523,usability,error,errors,523,"Looking at the commits page I can see that the tests has been failing for some days already. . First was a problem with a notebook test (test_pbmc3k) that seems innocuous but should be addressed. This is related to release 1.4.3 (https://github.com/theislab/scanpy/commit/85acb6c8949d43d08a26437dceab4fa5db79e246). The commits are unrelated to the failing test so I assume that some dependency was updated . However, after this commit https://github.com/theislab/scanpy/commit/115d635bf950354509053d976b90c1db518bcffe more errors are found. But again, I don't see any relevant changes that will cause the problems. One of the errors is that statsmodels is using a deprecated module from scipy.misc:. ```. > from scipy.misc import factorial. E ImportError: cannot import name 'factorial'. ../../../virtualenv/python3.6.7/lib/python3.6/site-packages/statsmodels/distributions/edgeworth.py:7: ImportError. ```. This was introduced after scipy 1.3 was recently updated (https://github.com/statsmodels/statsmodels/issues/5759). It seems that currently, the only solution is to install statsmodels directly from the master branch. Or downgrade scipy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:626,usability,error,errors,626,"Looking at the commits page I can see that the tests has been failing for some days already. . First was a problem with a notebook test (test_pbmc3k) that seems innocuous but should be addressed. This is related to release 1.4.3 (https://github.com/theislab/scanpy/commit/85acb6c8949d43d08a26437dceab4fa5db79e246). The commits are unrelated to the failing test so I assume that some dependency was updated . However, after this commit https://github.com/theislab/scanpy/commit/115d635bf950354509053d976b90c1db518bcffe more errors are found. But again, I don't see any relevant changes that will cause the problems. One of the errors is that statsmodels is using a deprecated module from scipy.misc:. ```. > from scipy.misc import factorial. E ImportError: cannot import name 'factorial'. ../../../virtualenv/python3.6.7/lib/python3.6/site-packages/statsmodels/distributions/edgeworth.py:7: ImportError. ```. This was introduced after scipy 1.3 was recently updated (https://github.com/statsmodels/statsmodels/issues/5759). It seems that currently, the only solution is to install statsmodels directly from the master branch. Or downgrade scipy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:33,availability,error,errors,33,setting scipy==1.2 fixes several errors but there is another one maybe related to matplotlib.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:33,performance,error,errors,33,setting scipy==1.2 fixes several errors but there is another one maybe related to matplotlib.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:33,safety,error,errors,33,setting scipy==1.2 fixes several errors but there is another one maybe related to matplotlib.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:33,usability,error,errors,33,setting scipy==1.2 fixes several errors but there is another one maybe related to matplotlib.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:110,availability,error,error,110,The remaining failed test is related to matplotlib 3.1.0 and 3d scatter plots. There is a report of a similar error (https://github.com/matplotlib/matplotlib/issues/14298). My suggestion is to wait for those issues to be solved and then upgrade the dependencies.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:14,deployability,fail,failed,14,The remaining failed test is related to matplotlib 3.1.0 and 3d scatter plots. There is a report of a similar error (https://github.com/matplotlib/matplotlib/issues/14298). My suggestion is to wait for those issues to be solved and then upgrade the dependencies.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:237,deployability,upgrad,upgrade,237,The remaining failed test is related to matplotlib 3.1.0 and 3d scatter plots. There is a report of a similar error (https://github.com/matplotlib/matplotlib/issues/14298). My suggestion is to wait for those issues to be solved and then upgrade the dependencies.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:249,deployability,depend,dependencies,249,The remaining failed test is related to matplotlib 3.1.0 and 3d scatter plots. There is a report of a similar error (https://github.com/matplotlib/matplotlib/issues/14298). My suggestion is to wait for those issues to be solved and then upgrade the dependencies.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:249,integrability,depend,dependencies,249,The remaining failed test is related to matplotlib 3.1.0 and 3d scatter plots. There is a report of a similar error (https://github.com/matplotlib/matplotlib/issues/14298). My suggestion is to wait for those issues to be solved and then upgrade the dependencies.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:237,modifiability,upgrad,upgrade,237,The remaining failed test is related to matplotlib 3.1.0 and 3d scatter plots. There is a report of a similar error (https://github.com/matplotlib/matplotlib/issues/14298). My suggestion is to wait for those issues to be solved and then upgrade the dependencies.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:249,modifiability,depend,dependencies,249,The remaining failed test is related to matplotlib 3.1.0 and 3d scatter plots. There is a report of a similar error (https://github.com/matplotlib/matplotlib/issues/14298). My suggestion is to wait for those issues to be solved and then upgrade the dependencies.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:110,performance,error,error,110,The remaining failed test is related to matplotlib 3.1.0 and 3d scatter plots. There is a report of a similar error (https://github.com/matplotlib/matplotlib/issues/14298). My suggestion is to wait for those issues to be solved and then upgrade the dependencies.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:14,reliability,fail,failed,14,The remaining failed test is related to matplotlib 3.1.0 and 3d scatter plots. There is a report of a similar error (https://github.com/matplotlib/matplotlib/issues/14298). My suggestion is to wait for those issues to be solved and then upgrade the dependencies.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:21,safety,test,test,21,The remaining failed test is related to matplotlib 3.1.0 and 3d scatter plots. There is a report of a similar error (https://github.com/matplotlib/matplotlib/issues/14298). My suggestion is to wait for those issues to be solved and then upgrade the dependencies.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:110,safety,error,error,110,The remaining failed test is related to matplotlib 3.1.0 and 3d scatter plots. There is a report of a similar error (https://github.com/matplotlib/matplotlib/issues/14298). My suggestion is to wait for those issues to be solved and then upgrade the dependencies.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:249,safety,depend,dependencies,249,The remaining failed test is related to matplotlib 3.1.0 and 3d scatter plots. There is a report of a similar error (https://github.com/matplotlib/matplotlib/issues/14298). My suggestion is to wait for those issues to be solved and then upgrade the dependencies.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:21,testability,test,test,21,The remaining failed test is related to matplotlib 3.1.0 and 3d scatter plots. There is a report of a similar error (https://github.com/matplotlib/matplotlib/issues/14298). My suggestion is to wait for those issues to be solved and then upgrade the dependencies.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:249,testability,depend,dependencies,249,The remaining failed test is related to matplotlib 3.1.0 and 3d scatter plots. There is a report of a similar error (https://github.com/matplotlib/matplotlib/issues/14298). My suggestion is to wait for those issues to be solved and then upgrade the dependencies.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/pull/661:110,usability,error,error,110,The remaining failed test is related to matplotlib 3.1.0 and 3d scatter plots. There is a report of a similar error (https://github.com/matplotlib/matplotlib/issues/14298). My suggestion is to wait for those issues to be solved and then upgrade the dependencies.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/661
https://github.com/scverse/scanpy/issues/662:4,deployability,updat,updates,4,Any updates on this one @flying-sheep? I keep having the same issue.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/662
https://github.com/scverse/scanpy/issues/662:4,safety,updat,updates,4,Any updates on this one @flying-sheep? I keep having the same issue.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/662
https://github.com/scverse/scanpy/issues/662:4,security,updat,updates,4,Any updates on this one @flying-sheep? I keep having the same issue.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/662
https://github.com/scverse/scanpy/issues/662:325,usability,help,help,325,"n_top_genes is used here:. https://github.com/theislab/scanpy/blob/6ac6440f154922027e7b416affc53be6d4a9978d/scanpy/preprocessing/_highly_variable_genes.py#L140-L148. I would assume that this only happens if thereâ€™s several genes with the exact same dispersion, is that possible? We need a reproducible example, else we canâ€™t help you further: Please give me some lines of code that I can paste into a notebook unchanged that will demonstrate the problem.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/662
https://github.com/scverse/scanpy/issues/662:305,integrability,filter,filtered,305,The number of HVGs not being exactly 1000 or 2000 is quite normal as dispersions can be exactly the same. 1488 is surprisingly high though. Maybe your dataset is very sparse so that you have a lot of dispersion ties for low count genes. I'm not sure what your issue with scaling is about though. Have you filtered out genes that are 0 using `sc.pp.filter_genes()`? This could be causing problems.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/662
https://github.com/scverse/scanpy/issues/662:271,modifiability,scal,scaling,271,The number of HVGs not being exactly 1000 or 2000 is quite normal as dispersions can be exactly the same. 1488 is surprisingly high though. Maybe your dataset is very sparse so that you have a lot of dispersion ties for low count genes. I'm not sure what your issue with scaling is about though. Have you filtered out genes that are 0 using `sc.pp.filter_genes()`? This could be causing problems.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/662
https://github.com/scverse/scanpy/issues/663:87,availability,down,downgrade,87,"The issue that you mention has been reported to matplotlib 3.1 and the. solution is to downgrade to 3.0*. I just updated the dependencies of. scanpy to be matplotlib 3.0. As soon as this is solved we will update the. dependencies. On Mon, May 27, 2019 at 3:33 PM bioguy2018 <notifications@github.com> wrote:. > Dear all. > I would like to project my umap from scanpy in 3d but I have faced the. > following problem:. >. > ValueError: operands could not be broadcast together with remapped shapes. > [original->remapped]: (0,4) and requested shape (816,4). >. > It's very strange because before I update some of my packages, I could run. > it it with no problem with the following packages:. >. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4. > scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760. > louvain==0.6.1. >. > but after updating some of my packages it was not possible due to that. > error! >. > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1. > pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0. > python-igraph==0.7.1+4.bed07760 louvain==0.6.1. >. > Should I roll back to the previous version of annadata or scanpy? has. > anyone ran this feature with my package version with no problems? >. > Thanks a lot. >. > Here are the packages I use. >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/663?email_source=notifications&email_token=ABF37VPMR3WSZT3FIGCFNJ3PXPPJ3A5CNFSM4HP4ASU2YY3PNVWWK3TUL52HS4DFUVEXG43VMWVGG33NNVSW45C7NFSM4GWBCDVA>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VJLTRD6ZHIBLZIRHYLPXPPJ3ANCNFSM4HP4ASUQ>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:941,availability,error,error,941,"The issue that you mention has been reported to matplotlib 3.1 and the. solution is to downgrade to 3.0*. I just updated the dependencies of. scanpy to be matplotlib 3.0. As soon as this is solved we will update the. dependencies. On Mon, May 27, 2019 at 3:33 PM bioguy2018 <notifications@github.com> wrote:. > Dear all. > I would like to project my umap from scanpy in 3d but I have faced the. > following problem:. >. > ValueError: operands could not be broadcast together with remapped shapes. > [original->remapped]: (0,4) and requested shape (816,4). >. > It's very strange because before I update some of my packages, I could run. > it it with no problem with the following packages:. >. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4. > scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760. > louvain==0.6.1. >. > but after updating some of my packages it was not possible due to that. > error! >. > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1. > pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0. > python-igraph==0.7.1+4.bed07760 louvain==0.6.1. >. > Should I roll back to the previous version of annadata or scanpy? has. > anyone ran this feature with my package version with no problems? >. > Thanks a lot. >. > Here are the packages I use. >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/663?email_source=notifications&email_token=ABF37VPMR3WSZT3FIGCFNJ3PXPPJ3A5CNFSM4HP4ASU2YY3PNVWWK3TUL52HS4DFUVEXG43VMWVGG33NNVSW45C7NFSM4GWBCDVA>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VJLTRD6ZHIBLZIRHYLPXPPJ3ANCNFSM4HP4ASUQ>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:113,deployability,updat,updated,113,"The issue that you mention has been reported to matplotlib 3.1 and the. solution is to downgrade to 3.0*. I just updated the dependencies of. scanpy to be matplotlib 3.0. As soon as this is solved we will update the. dependencies. On Mon, May 27, 2019 at 3:33 PM bioguy2018 <notifications@github.com> wrote:. > Dear all. > I would like to project my umap from scanpy in 3d but I have faced the. > following problem:. >. > ValueError: operands could not be broadcast together with remapped shapes. > [original->remapped]: (0,4) and requested shape (816,4). >. > It's very strange because before I update some of my packages, I could run. > it it with no problem with the following packages:. >. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4. > scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760. > louvain==0.6.1. >. > but after updating some of my packages it was not possible due to that. > error! >. > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1. > pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0. > python-igraph==0.7.1+4.bed07760 louvain==0.6.1. >. > Should I roll back to the previous version of annadata or scanpy? has. > anyone ran this feature with my package version with no problems? >. > Thanks a lot. >. > Here are the packages I use. >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/663?email_source=notifications&email_token=ABF37VPMR3WSZT3FIGCFNJ3PXPPJ3A5CNFSM4HP4ASU2YY3PNVWWK3TUL52HS4DFUVEXG43VMWVGG33NNVSW45C7NFSM4GWBCDVA>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VJLTRD6ZHIBLZIRHYLPXPPJ3ANCNFSM4HP4ASUQ>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:125,deployability,depend,dependencies,125,"The issue that you mention has been reported to matplotlib 3.1 and the. solution is to downgrade to 3.0*. I just updated the dependencies of. scanpy to be matplotlib 3.0. As soon as this is solved we will update the. dependencies. On Mon, May 27, 2019 at 3:33 PM bioguy2018 <notifications@github.com> wrote:. > Dear all. > I would like to project my umap from scanpy in 3d but I have faced the. > following problem:. >. > ValueError: operands could not be broadcast together with remapped shapes. > [original->remapped]: (0,4) and requested shape (816,4). >. > It's very strange because before I update some of my packages, I could run. > it it with no problem with the following packages:. >. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4. > scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760. > louvain==0.6.1. >. > but after updating some of my packages it was not possible due to that. > error! >. > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1. > pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0. > python-igraph==0.7.1+4.bed07760 louvain==0.6.1. >. > Should I roll back to the previous version of annadata or scanpy? has. > anyone ran this feature with my package version with no problems? >. > Thanks a lot. >. > Here are the packages I use. >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/663?email_source=notifications&email_token=ABF37VPMR3WSZT3FIGCFNJ3PXPPJ3A5CNFSM4HP4ASU2YY3PNVWWK3TUL52HS4DFUVEXG43VMWVGG33NNVSW45C7NFSM4GWBCDVA>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VJLTRD6ZHIBLZIRHYLPXPPJ3ANCNFSM4HP4ASUQ>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:205,deployability,updat,update,205,"The issue that you mention has been reported to matplotlib 3.1 and the. solution is to downgrade to 3.0*. I just updated the dependencies of. scanpy to be matplotlib 3.0. As soon as this is solved we will update the. dependencies. On Mon, May 27, 2019 at 3:33 PM bioguy2018 <notifications@github.com> wrote:. > Dear all. > I would like to project my umap from scanpy in 3d but I have faced the. > following problem:. >. > ValueError: operands could not be broadcast together with remapped shapes. > [original->remapped]: (0,4) and requested shape (816,4). >. > It's very strange because before I update some of my packages, I could run. > it it with no problem with the following packages:. >. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4. > scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760. > louvain==0.6.1. >. > but after updating some of my packages it was not possible due to that. > error! >. > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1. > pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0. > python-igraph==0.7.1+4.bed07760 louvain==0.6.1. >. > Should I roll back to the previous version of annadata or scanpy? has. > anyone ran this feature with my package version with no problems? >. > Thanks a lot. >. > Here are the packages I use. >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/663?email_source=notifications&email_token=ABF37VPMR3WSZT3FIGCFNJ3PXPPJ3A5CNFSM4HP4ASU2YY3PNVWWK3TUL52HS4DFUVEXG43VMWVGG33NNVSW45C7NFSM4GWBCDVA>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VJLTRD6ZHIBLZIRHYLPXPPJ3ANCNFSM4HP4ASUQ>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:217,deployability,depend,dependencies,217,"The issue that you mention has been reported to matplotlib 3.1 and the. solution is to downgrade to 3.0*. I just updated the dependencies of. scanpy to be matplotlib 3.0. As soon as this is solved we will update the. dependencies. On Mon, May 27, 2019 at 3:33 PM bioguy2018 <notifications@github.com> wrote:. > Dear all. > I would like to project my umap from scanpy in 3d but I have faced the. > following problem:. >. > ValueError: operands could not be broadcast together with remapped shapes. > [original->remapped]: (0,4) and requested shape (816,4). >. > It's very strange because before I update some of my packages, I could run. > it it with no problem with the following packages:. >. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4. > scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760. > louvain==0.6.1. >. > but after updating some of my packages it was not possible due to that. > error! >. > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1. > pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0. > python-igraph==0.7.1+4.bed07760 louvain==0.6.1. >. > Should I roll back to the previous version of annadata or scanpy? has. > anyone ran this feature with my package version with no problems? >. > Thanks a lot. >. > Here are the packages I use. >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/663?email_source=notifications&email_token=ABF37VPMR3WSZT3FIGCFNJ3PXPPJ3A5CNFSM4HP4ASU2YY3PNVWWK3TUL52HS4DFUVEXG43VMWVGG33NNVSW45C7NFSM4GWBCDVA>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VJLTRD6ZHIBLZIRHYLPXPPJ3ANCNFSM4HP4ASUQ>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:596,deployability,updat,update,596,"The issue that you mention has been reported to matplotlib 3.1 and the. solution is to downgrade to 3.0*. I just updated the dependencies of. scanpy to be matplotlib 3.0. As soon as this is solved we will update the. dependencies. On Mon, May 27, 2019 at 3:33 PM bioguy2018 <notifications@github.com> wrote:. > Dear all. > I would like to project my umap from scanpy in 3d but I have faced the. > following problem:. >. > ValueError: operands could not be broadcast together with remapped shapes. > [original->remapped]: (0,4) and requested shape (816,4). >. > It's very strange because before I update some of my packages, I could run. > it it with no problem with the following packages:. >. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4. > scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760. > louvain==0.6.1. >. > but after updating some of my packages it was not possible due to that. > error! >. > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1. > pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0. > python-igraph==0.7.1+4.bed07760 louvain==0.6.1. >. > Should I roll back to the previous version of annadata or scanpy? has. > anyone ran this feature with my package version with no problems? >. > Thanks a lot. >. > Here are the packages I use. >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/663?email_source=notifications&email_token=ABF37VPMR3WSZT3FIGCFNJ3PXPPJ3A5CNFSM4HP4ASU2YY3PNVWWK3TUL52HS4DFUVEXG43VMWVGG33NNVSW45C7NFSM4GWBCDVA>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VJLTRD6ZHIBLZIRHYLPXPPJ3ANCNFSM4HP4ASUQ>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:877,deployability,updat,updating,877,"The issue that you mention has been reported to matplotlib 3.1 and the. solution is to downgrade to 3.0*. I just updated the dependencies of. scanpy to be matplotlib 3.0. As soon as this is solved we will update the. dependencies. On Mon, May 27, 2019 at 3:33 PM bioguy2018 <notifications@github.com> wrote:. > Dear all. > I would like to project my umap from scanpy in 3d but I have faced the. > following problem:. >. > ValueError: operands could not be broadcast together with remapped shapes. > [original->remapped]: (0,4) and requested shape (816,4). >. > It's very strange because before I update some of my packages, I could run. > it it with no problem with the following packages:. >. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4. > scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760. > louvain==0.6.1. >. > but after updating some of my packages it was not possible due to that. > error! >. > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1. > pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0. > python-igraph==0.7.1+4.bed07760 louvain==0.6.1. >. > Should I roll back to the previous version of annadata or scanpy? has. > anyone ran this feature with my package version with no problems? >. > Thanks a lot. >. > Here are the packages I use. >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/663?email_source=notifications&email_token=ABF37VPMR3WSZT3FIGCFNJ3PXPPJ3A5CNFSM4HP4ASU2YY3PNVWWK3TUL52HS4DFUVEXG43VMWVGG33NNVSW45C7NFSM4GWBCDVA>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VJLTRD6ZHIBLZIRHYLPXPPJ3ANCNFSM4HP4ASUQ>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:1145,deployability,roll,roll,1145,"The issue that you mention has been reported to matplotlib 3.1 and the. solution is to downgrade to 3.0*. I just updated the dependencies of. scanpy to be matplotlib 3.0. As soon as this is solved we will update the. dependencies. On Mon, May 27, 2019 at 3:33 PM bioguy2018 <notifications@github.com> wrote:. > Dear all. > I would like to project my umap from scanpy in 3d but I have faced the. > following problem:. >. > ValueError: operands could not be broadcast together with remapped shapes. > [original->remapped]: (0,4) and requested shape (816,4). >. > It's very strange because before I update some of my packages, I could run. > it it with no problem with the following packages:. >. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4. > scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760. > louvain==0.6.1. >. > but after updating some of my packages it was not possible due to that. > error! >. > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1. > pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0. > python-igraph==0.7.1+4.bed07760 louvain==0.6.1. >. > Should I roll back to the previous version of annadata or scanpy? has. > anyone ran this feature with my package version with no problems? >. > Thanks a lot. >. > Here are the packages I use. >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/663?email_source=notifications&email_token=ABF37VPMR3WSZT3FIGCFNJ3PXPPJ3A5CNFSM4HP4ASU2YY3PNVWWK3TUL52HS4DFUVEXG43VMWVGG33NNVSW45C7NFSM4GWBCDVA>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VJLTRD6ZHIBLZIRHYLPXPPJ3ANCNFSM4HP4ASUQ>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:1171,deployability,version,version,1171,"The issue that you mention has been reported to matplotlib 3.1 and the. solution is to downgrade to 3.0*. I just updated the dependencies of. scanpy to be matplotlib 3.0. As soon as this is solved we will update the. dependencies. On Mon, May 27, 2019 at 3:33 PM bioguy2018 <notifications@github.com> wrote:. > Dear all. > I would like to project my umap from scanpy in 3d but I have faced the. > following problem:. >. > ValueError: operands could not be broadcast together with remapped shapes. > [original->remapped]: (0,4) and requested shape (816,4). >. > It's very strange because before I update some of my packages, I could run. > it it with no problem with the following packages:. >. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4. > scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760. > louvain==0.6.1. >. > but after updating some of my packages it was not possible due to that. > error! >. > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1. > pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0. > python-igraph==0.7.1+4.bed07760 louvain==0.6.1. >. > Should I roll back to the previous version of annadata or scanpy? has. > anyone ran this feature with my package version with no problems? >. > Thanks a lot. >. > Here are the packages I use. >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/663?email_source=notifications&email_token=ABF37VPMR3WSZT3FIGCFNJ3PXPPJ3A5CNFSM4HP4ASU2YY3PNVWWK3TUL52HS4DFUVEXG43VMWVGG33NNVSW45C7NFSM4GWBCDVA>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VJLTRD6ZHIBLZIRHYLPXPPJ3ANCNFSM4HP4ASUQ>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:1249,deployability,version,version,1249,"The issue that you mention has been reported to matplotlib 3.1 and the. solution is to downgrade to 3.0*. I just updated the dependencies of. scanpy to be matplotlib 3.0. As soon as this is solved we will update the. dependencies. On Mon, May 27, 2019 at 3:33 PM bioguy2018 <notifications@github.com> wrote:. > Dear all. > I would like to project my umap from scanpy in 3d but I have faced the. > following problem:. >. > ValueError: operands could not be broadcast together with remapped shapes. > [original->remapped]: (0,4) and requested shape (816,4). >. > It's very strange because before I update some of my packages, I could run. > it it with no problem with the following packages:. >. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4. > scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760. > louvain==0.6.1. >. > but after updating some of my packages it was not possible due to that. > error! >. > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1. > pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0. > python-igraph==0.7.1+4.bed07760 louvain==0.6.1. >. > Should I roll back to the previous version of annadata or scanpy? has. > anyone ran this feature with my package version with no problems? >. > Thanks a lot. >. > Here are the packages I use. >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/663?email_source=notifications&email_token=ABF37VPMR3WSZT3FIGCFNJ3PXPPJ3A5CNFSM4HP4ASU2YY3PNVWWK3TUL52HS4DFUVEXG43VMWVGG33NNVSW45C7NFSM4GWBCDVA>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VJLTRD6ZHIBLZIRHYLPXPPJ3ANCNFSM4HP4ASUQ>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:125,integrability,depend,dependencies,125,"The issue that you mention has been reported to matplotlib 3.1 and the. solution is to downgrade to 3.0*. I just updated the dependencies of. scanpy to be matplotlib 3.0. As soon as this is solved we will update the. dependencies. On Mon, May 27, 2019 at 3:33 PM bioguy2018 <notifications@github.com> wrote:. > Dear all. > I would like to project my umap from scanpy in 3d but I have faced the. > following problem:. >. > ValueError: operands could not be broadcast together with remapped shapes. > [original->remapped]: (0,4) and requested shape (816,4). >. > It's very strange because before I update some of my packages, I could run. > it it with no problem with the following packages:. >. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4. > scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760. > louvain==0.6.1. >. > but after updating some of my packages it was not possible due to that. > error! >. > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1. > pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0. > python-igraph==0.7.1+4.bed07760 louvain==0.6.1. >. > Should I roll back to the previous version of annadata or scanpy? has. > anyone ran this feature with my package version with no problems? >. > Thanks a lot. >. > Here are the packages I use. >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/663?email_source=notifications&email_token=ABF37VPMR3WSZT3FIGCFNJ3PXPPJ3A5CNFSM4HP4ASU2YY3PNVWWK3TUL52HS4DFUVEXG43VMWVGG33NNVSW45C7NFSM4GWBCDVA>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VJLTRD6ZHIBLZIRHYLPXPPJ3ANCNFSM4HP4ASUQ>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:217,integrability,depend,dependencies,217,"The issue that you mention has been reported to matplotlib 3.1 and the. solution is to downgrade to 3.0*. I just updated the dependencies of. scanpy to be matplotlib 3.0. As soon as this is solved we will update the. dependencies. On Mon, May 27, 2019 at 3:33 PM bioguy2018 <notifications@github.com> wrote:. > Dear all. > I would like to project my umap from scanpy in 3d but I have faced the. > following problem:. >. > ValueError: operands could not be broadcast together with remapped shapes. > [original->remapped]: (0,4) and requested shape (816,4). >. > It's very strange because before I update some of my packages, I could run. > it it with no problem with the following packages:. >. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4. > scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760. > louvain==0.6.1. >. > but after updating some of my packages it was not possible due to that. > error! >. > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1. > pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0. > python-igraph==0.7.1+4.bed07760 louvain==0.6.1. >. > Should I roll back to the previous version of annadata or scanpy? has. > anyone ran this feature with my package version with no problems? >. > Thanks a lot. >. > Here are the packages I use. >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/663?email_source=notifications&email_token=ABF37VPMR3WSZT3FIGCFNJ3PXPPJ3A5CNFSM4HP4ASU2YY3PNVWWK3TUL52HS4DFUVEXG43VMWVGG33NNVSW45C7NFSM4GWBCDVA>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VJLTRD6ZHIBLZIRHYLPXPPJ3ANCNFSM4HP4ASUQ>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:1171,integrability,version,version,1171,"The issue that you mention has been reported to matplotlib 3.1 and the. solution is to downgrade to 3.0*. I just updated the dependencies of. scanpy to be matplotlib 3.0. As soon as this is solved we will update the. dependencies. On Mon, May 27, 2019 at 3:33 PM bioguy2018 <notifications@github.com> wrote:. > Dear all. > I would like to project my umap from scanpy in 3d but I have faced the. > following problem:. >. > ValueError: operands could not be broadcast together with remapped shapes. > [original->remapped]: (0,4) and requested shape (816,4). >. > It's very strange because before I update some of my packages, I could run. > it it with no problem with the following packages:. >. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4. > scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760. > louvain==0.6.1. >. > but after updating some of my packages it was not possible due to that. > error! >. > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1. > pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0. > python-igraph==0.7.1+4.bed07760 louvain==0.6.1. >. > Should I roll back to the previous version of annadata or scanpy? has. > anyone ran this feature with my package version with no problems? >. > Thanks a lot. >. > Here are the packages I use. >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/663?email_source=notifications&email_token=ABF37VPMR3WSZT3FIGCFNJ3PXPPJ3A5CNFSM4HP4ASU2YY3PNVWWK3TUL52HS4DFUVEXG43VMWVGG33NNVSW45C7NFSM4GWBCDVA>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VJLTRD6ZHIBLZIRHYLPXPPJ3ANCNFSM4HP4ASUQ>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:1249,integrability,version,version,1249,"The issue that you mention has been reported to matplotlib 3.1 and the. solution is to downgrade to 3.0*. I just updated the dependencies of. scanpy to be matplotlib 3.0. As soon as this is solved we will update the. dependencies. On Mon, May 27, 2019 at 3:33 PM bioguy2018 <notifications@github.com> wrote:. > Dear all. > I would like to project my umap from scanpy in 3d but I have faced the. > following problem:. >. > ValueError: operands could not be broadcast together with remapped shapes. > [original->remapped]: (0,4) and requested shape (816,4). >. > It's very strange because before I update some of my packages, I could run. > it it with no problem with the following packages:. >. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4. > scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760. > louvain==0.6.1. >. > but after updating some of my packages it was not possible due to that. > error! >. > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1. > pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0. > python-igraph==0.7.1+4.bed07760 louvain==0.6.1. >. > Should I roll back to the previous version of annadata or scanpy? has. > anyone ran this feature with my package version with no problems? >. > Thanks a lot. >. > Here are the packages I use. >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/663?email_source=notifications&email_token=ABF37VPMR3WSZT3FIGCFNJ3PXPPJ3A5CNFSM4HP4ASU2YY3PNVWWK3TUL52HS4DFUVEXG43VMWVGG33NNVSW45C7NFSM4GWBCDVA>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VJLTRD6ZHIBLZIRHYLPXPPJ3ANCNFSM4HP4ASUQ>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:1377,integrability,sub,subscribed,1377,"The issue that you mention has been reported to matplotlib 3.1 and the. solution is to downgrade to 3.0*. I just updated the dependencies of. scanpy to be matplotlib 3.0. As soon as this is solved we will update the. dependencies. On Mon, May 27, 2019 at 3:33 PM bioguy2018 <notifications@github.com> wrote:. > Dear all. > I would like to project my umap from scanpy in 3d but I have faced the. > following problem:. >. > ValueError: operands could not be broadcast together with remapped shapes. > [original->remapped]: (0,4) and requested shape (816,4). >. > It's very strange because before I update some of my packages, I could run. > it it with no problem with the following packages:. >. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4. > scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760. > louvain==0.6.1. >. > but after updating some of my packages it was not possible due to that. > error! >. > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1. > pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0. > python-igraph==0.7.1+4.bed07760 louvain==0.6.1. >. > Should I roll back to the previous version of annadata or scanpy? has. > anyone ran this feature with my package version with no problems? >. > Thanks a lot. >. > Here are the packages I use. >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/663?email_source=notifications&email_token=ABF37VPMR3WSZT3FIGCFNJ3PXPPJ3A5CNFSM4HP4ASU2YY3PNVWWK3TUL52HS4DFUVEXG43VMWVGG33NNVSW45C7NFSM4GWBCDVA>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VJLTRD6ZHIBLZIRHYLPXPPJ3ANCNFSM4HP4ASUQ>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:125,modifiability,depend,dependencies,125,"The issue that you mention has been reported to matplotlib 3.1 and the. solution is to downgrade to 3.0*. I just updated the dependencies of. scanpy to be matplotlib 3.0. As soon as this is solved we will update the. dependencies. On Mon, May 27, 2019 at 3:33 PM bioguy2018 <notifications@github.com> wrote:. > Dear all. > I would like to project my umap from scanpy in 3d but I have faced the. > following problem:. >. > ValueError: operands could not be broadcast together with remapped shapes. > [original->remapped]: (0,4) and requested shape (816,4). >. > It's very strange because before I update some of my packages, I could run. > it it with no problem with the following packages:. >. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4. > scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760. > louvain==0.6.1. >. > but after updating some of my packages it was not possible due to that. > error! >. > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1. > pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0. > python-igraph==0.7.1+4.bed07760 louvain==0.6.1. >. > Should I roll back to the previous version of annadata or scanpy? has. > anyone ran this feature with my package version with no problems? >. > Thanks a lot. >. > Here are the packages I use. >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/663?email_source=notifications&email_token=ABF37VPMR3WSZT3FIGCFNJ3PXPPJ3A5CNFSM4HP4ASU2YY3PNVWWK3TUL52HS4DFUVEXG43VMWVGG33NNVSW45C7NFSM4GWBCDVA>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VJLTRD6ZHIBLZIRHYLPXPPJ3ANCNFSM4HP4ASUQ>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:217,modifiability,depend,dependencies,217,"The issue that you mention has been reported to matplotlib 3.1 and the. solution is to downgrade to 3.0*. I just updated the dependencies of. scanpy to be matplotlib 3.0. As soon as this is solved we will update the. dependencies. On Mon, May 27, 2019 at 3:33 PM bioguy2018 <notifications@github.com> wrote:. > Dear all. > I would like to project my umap from scanpy in 3d but I have faced the. > following problem:. >. > ValueError: operands could not be broadcast together with remapped shapes. > [original->remapped]: (0,4) and requested shape (816,4). >. > It's very strange because before I update some of my packages, I could run. > it it with no problem with the following packages:. >. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4. > scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760. > louvain==0.6.1. >. > but after updating some of my packages it was not possible due to that. > error! >. > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1. > pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0. > python-igraph==0.7.1+4.bed07760 louvain==0.6.1. >. > Should I roll back to the previous version of annadata or scanpy? has. > anyone ran this feature with my package version with no problems? >. > Thanks a lot. >. > Here are the packages I use. >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/663?email_source=notifications&email_token=ABF37VPMR3WSZT3FIGCFNJ3PXPPJ3A5CNFSM4HP4ASU2YY3PNVWWK3TUL52HS4DFUVEXG43VMWVGG33NNVSW45C7NFSM4GWBCDVA>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VJLTRD6ZHIBLZIRHYLPXPPJ3ANCNFSM4HP4ASUQ>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:614,modifiability,pac,packages,614,"The issue that you mention has been reported to matplotlib 3.1 and the. solution is to downgrade to 3.0*. I just updated the dependencies of. scanpy to be matplotlib 3.0. As soon as this is solved we will update the. dependencies. On Mon, May 27, 2019 at 3:33 PM bioguy2018 <notifications@github.com> wrote:. > Dear all. > I would like to project my umap from scanpy in 3d but I have faced the. > following problem:. >. > ValueError: operands could not be broadcast together with remapped shapes. > [original->remapped]: (0,4) and requested shape (816,4). >. > It's very strange because before I update some of my packages, I could run. > it it with no problem with the following packages:. >. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4. > scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760. > louvain==0.6.1. >. > but after updating some of my packages it was not possible due to that. > error! >. > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1. > pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0. > python-igraph==0.7.1+4.bed07760 louvain==0.6.1. >. > Should I roll back to the previous version of annadata or scanpy? has. > anyone ran this feature with my package version with no problems? >. > Thanks a lot. >. > Here are the packages I use. >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/663?email_source=notifications&email_token=ABF37VPMR3WSZT3FIGCFNJ3PXPPJ3A5CNFSM4HP4ASU2YY3PNVWWK3TUL52HS4DFUVEXG43VMWVGG33NNVSW45C7NFSM4GWBCDVA>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VJLTRD6ZHIBLZIRHYLPXPPJ3ANCNFSM4HP4ASUQ>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:680,modifiability,pac,packages,680,"The issue that you mention has been reported to matplotlib 3.1 and the. solution is to downgrade to 3.0*. I just updated the dependencies of. scanpy to be matplotlib 3.0. As soon as this is solved we will update the. dependencies. On Mon, May 27, 2019 at 3:33 PM bioguy2018 <notifications@github.com> wrote:. > Dear all. > I would like to project my umap from scanpy in 3d but I have faced the. > following problem:. >. > ValueError: operands could not be broadcast together with remapped shapes. > [original->remapped]: (0,4) and requested shape (816,4). >. > It's very strange because before I update some of my packages, I could run. > it it with no problem with the following packages:. >. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4. > scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760. > louvain==0.6.1. >. > but after updating some of my packages it was not possible due to that. > error! >. > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1. > pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0. > python-igraph==0.7.1+4.bed07760 louvain==0.6.1. >. > Should I roll back to the previous version of annadata or scanpy? has. > anyone ran this feature with my package version with no problems? >. > Thanks a lot. >. > Here are the packages I use. >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/663?email_source=notifications&email_token=ABF37VPMR3WSZT3FIGCFNJ3PXPPJ3A5CNFSM4HP4ASU2YY3PNVWWK3TUL52HS4DFUVEXG43VMWVGG33NNVSW45C7NFSM4GWBCDVA>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VJLTRD6ZHIBLZIRHYLPXPPJ3ANCNFSM4HP4ASUQ>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:897,modifiability,pac,packages,897,"The issue that you mention has been reported to matplotlib 3.1 and the. solution is to downgrade to 3.0*. I just updated the dependencies of. scanpy to be matplotlib 3.0. As soon as this is solved we will update the. dependencies. On Mon, May 27, 2019 at 3:33 PM bioguy2018 <notifications@github.com> wrote:. > Dear all. > I would like to project my umap from scanpy in 3d but I have faced the. > following problem:. >. > ValueError: operands could not be broadcast together with remapped shapes. > [original->remapped]: (0,4) and requested shape (816,4). >. > It's very strange because before I update some of my packages, I could run. > it it with no problem with the following packages:. >. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4. > scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760. > louvain==0.6.1. >. > but after updating some of my packages it was not possible due to that. > error! >. > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1. > pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0. > python-igraph==0.7.1+4.bed07760 louvain==0.6.1. >. > Should I roll back to the previous version of annadata or scanpy? has. > anyone ran this feature with my package version with no problems? >. > Thanks a lot. >. > Here are the packages I use. >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/663?email_source=notifications&email_token=ABF37VPMR3WSZT3FIGCFNJ3PXPPJ3A5CNFSM4HP4ASU2YY3PNVWWK3TUL52HS4DFUVEXG43VMWVGG33NNVSW45C7NFSM4GWBCDVA>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VJLTRD6ZHIBLZIRHYLPXPPJ3ANCNFSM4HP4ASUQ>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:1171,modifiability,version,version,1171,"The issue that you mention has been reported to matplotlib 3.1 and the. solution is to downgrade to 3.0*. I just updated the dependencies of. scanpy to be matplotlib 3.0. As soon as this is solved we will update the. dependencies. On Mon, May 27, 2019 at 3:33 PM bioguy2018 <notifications@github.com> wrote:. > Dear all. > I would like to project my umap from scanpy in 3d but I have faced the. > following problem:. >. > ValueError: operands could not be broadcast together with remapped shapes. > [original->remapped]: (0,4) and requested shape (816,4). >. > It's very strange because before I update some of my packages, I could run. > it it with no problem with the following packages:. >. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4. > scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760. > louvain==0.6.1. >. > but after updating some of my packages it was not possible due to that. > error! >. > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1. > pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0. > python-igraph==0.7.1+4.bed07760 louvain==0.6.1. >. > Should I roll back to the previous version of annadata or scanpy? has. > anyone ran this feature with my package version with no problems? >. > Thanks a lot. >. > Here are the packages I use. >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/663?email_source=notifications&email_token=ABF37VPMR3WSZT3FIGCFNJ3PXPPJ3A5CNFSM4HP4ASU2YY3PNVWWK3TUL52HS4DFUVEXG43VMWVGG33NNVSW45C7NFSM4GWBCDVA>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VJLTRD6ZHIBLZIRHYLPXPPJ3ANCNFSM4HP4ASUQ>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:1241,modifiability,pac,package,1241,"The issue that you mention has been reported to matplotlib 3.1 and the. solution is to downgrade to 3.0*. I just updated the dependencies of. scanpy to be matplotlib 3.0. As soon as this is solved we will update the. dependencies. On Mon, May 27, 2019 at 3:33 PM bioguy2018 <notifications@github.com> wrote:. > Dear all. > I would like to project my umap from scanpy in 3d but I have faced the. > following problem:. >. > ValueError: operands could not be broadcast together with remapped shapes. > [original->remapped]: (0,4) and requested shape (816,4). >. > It's very strange because before I update some of my packages, I could run. > it it with no problem with the following packages:. >. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4. > scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760. > louvain==0.6.1. >. > but after updating some of my packages it was not possible due to that. > error! >. > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1. > pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0. > python-igraph==0.7.1+4.bed07760 louvain==0.6.1. >. > Should I roll back to the previous version of annadata or scanpy? has. > anyone ran this feature with my package version with no problems? >. > Thanks a lot. >. > Here are the packages I use. >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/663?email_source=notifications&email_token=ABF37VPMR3WSZT3FIGCFNJ3PXPPJ3A5CNFSM4HP4ASU2YY3PNVWWK3TUL52HS4DFUVEXG43VMWVGG33NNVSW45C7NFSM4GWBCDVA>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VJLTRD6ZHIBLZIRHYLPXPPJ3ANCNFSM4HP4ASUQ>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:1249,modifiability,version,version,1249,"The issue that you mention has been reported to matplotlib 3.1 and the. solution is to downgrade to 3.0*. I just updated the dependencies of. scanpy to be matplotlib 3.0. As soon as this is solved we will update the. dependencies. On Mon, May 27, 2019 at 3:33 PM bioguy2018 <notifications@github.com> wrote:. > Dear all. > I would like to project my umap from scanpy in 3d but I have faced the. > following problem:. >. > ValueError: operands could not be broadcast together with remapped shapes. > [original->remapped]: (0,4) and requested shape (816,4). >. > It's very strange because before I update some of my packages, I could run. > it it with no problem with the following packages:. >. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4. > scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760. > louvain==0.6.1. >. > but after updating some of my packages it was not possible due to that. > error! >. > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1. > pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0. > python-igraph==0.7.1+4.bed07760 louvain==0.6.1. >. > Should I roll back to the previous version of annadata or scanpy? has. > anyone ran this feature with my package version with no problems? >. > Thanks a lot. >. > Here are the packages I use. >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/663?email_source=notifications&email_token=ABF37VPMR3WSZT3FIGCFNJ3PXPPJ3A5CNFSM4HP4ASU2YY3PNVWWK3TUL52HS4DFUVEXG43VMWVGG33NNVSW45C7NFSM4GWBCDVA>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VJLTRD6ZHIBLZIRHYLPXPPJ3ANCNFSM4HP4ASUQ>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:1312,modifiability,pac,packages,1312,"The issue that you mention has been reported to matplotlib 3.1 and the. solution is to downgrade to 3.0*. I just updated the dependencies of. scanpy to be matplotlib 3.0. As soon as this is solved we will update the. dependencies. On Mon, May 27, 2019 at 3:33 PM bioguy2018 <notifications@github.com> wrote:. > Dear all. > I would like to project my umap from scanpy in 3d but I have faced the. > following problem:. >. > ValueError: operands could not be broadcast together with remapped shapes. > [original->remapped]: (0,4) and requested shape (816,4). >. > It's very strange because before I update some of my packages, I could run. > it it with no problem with the following packages:. >. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4. > scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760. > louvain==0.6.1. >. > but after updating some of my packages it was not possible due to that. > error! >. > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1. > pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0. > python-igraph==0.7.1+4.bed07760 louvain==0.6.1. >. > Should I roll back to the previous version of annadata or scanpy? has. > anyone ran this feature with my package version with no problems? >. > Thanks a lot. >. > Here are the packages I use. >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/663?email_source=notifications&email_token=ABF37VPMR3WSZT3FIGCFNJ3PXPPJ3A5CNFSM4HP4ASU2YY3PNVWWK3TUL52HS4DFUVEXG43VMWVGG33NNVSW45C7NFSM4GWBCDVA>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VJLTRD6ZHIBLZIRHYLPXPPJ3ANCNFSM4HP4ASUQ>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:941,performance,error,error,941,"The issue that you mention has been reported to matplotlib 3.1 and the. solution is to downgrade to 3.0*. I just updated the dependencies of. scanpy to be matplotlib 3.0. As soon as this is solved we will update the. dependencies. On Mon, May 27, 2019 at 3:33 PM bioguy2018 <notifications@github.com> wrote:. > Dear all. > I would like to project my umap from scanpy in 3d but I have faced the. > following problem:. >. > ValueError: operands could not be broadcast together with remapped shapes. > [original->remapped]: (0,4) and requested shape (816,4). >. > It's very strange because before I update some of my packages, I could run. > it it with no problem with the following packages:. >. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4. > scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760. > louvain==0.6.1. >. > but after updating some of my packages it was not possible due to that. > error! >. > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1. > pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0. > python-igraph==0.7.1+4.bed07760 louvain==0.6.1. >. > Should I roll back to the previous version of annadata or scanpy? has. > anyone ran this feature with my package version with no problems? >. > Thanks a lot. >. > Here are the packages I use. >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/663?email_source=notifications&email_token=ABF37VPMR3WSZT3FIGCFNJ3PXPPJ3A5CNFSM4HP4ASU2YY3PNVWWK3TUL52HS4DFUVEXG43VMWVGG33NNVSW45C7NFSM4GWBCDVA>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VJLTRD6ZHIBLZIRHYLPXPPJ3ANCNFSM4HP4ASUQ>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:113,safety,updat,updated,113,"The issue that you mention has been reported to matplotlib 3.1 and the. solution is to downgrade to 3.0*. I just updated the dependencies of. scanpy to be matplotlib 3.0. As soon as this is solved we will update the. dependencies. On Mon, May 27, 2019 at 3:33 PM bioguy2018 <notifications@github.com> wrote:. > Dear all. > I would like to project my umap from scanpy in 3d but I have faced the. > following problem:. >. > ValueError: operands could not be broadcast together with remapped shapes. > [original->remapped]: (0,4) and requested shape (816,4). >. > It's very strange because before I update some of my packages, I could run. > it it with no problem with the following packages:. >. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4. > scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760. > louvain==0.6.1. >. > but after updating some of my packages it was not possible due to that. > error! >. > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1. > pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0. > python-igraph==0.7.1+4.bed07760 louvain==0.6.1. >. > Should I roll back to the previous version of annadata or scanpy? has. > anyone ran this feature with my package version with no problems? >. > Thanks a lot. >. > Here are the packages I use. >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/663?email_source=notifications&email_token=ABF37VPMR3WSZT3FIGCFNJ3PXPPJ3A5CNFSM4HP4ASU2YY3PNVWWK3TUL52HS4DFUVEXG43VMWVGG33NNVSW45C7NFSM4GWBCDVA>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VJLTRD6ZHIBLZIRHYLPXPPJ3ANCNFSM4HP4ASUQ>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:125,safety,depend,dependencies,125,"The issue that you mention has been reported to matplotlib 3.1 and the. solution is to downgrade to 3.0*. I just updated the dependencies of. scanpy to be matplotlib 3.0. As soon as this is solved we will update the. dependencies. On Mon, May 27, 2019 at 3:33 PM bioguy2018 <notifications@github.com> wrote:. > Dear all. > I would like to project my umap from scanpy in 3d but I have faced the. > following problem:. >. > ValueError: operands could not be broadcast together with remapped shapes. > [original->remapped]: (0,4) and requested shape (816,4). >. > It's very strange because before I update some of my packages, I could run. > it it with no problem with the following packages:. >. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4. > scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760. > louvain==0.6.1. >. > but after updating some of my packages it was not possible due to that. > error! >. > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1. > pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0. > python-igraph==0.7.1+4.bed07760 louvain==0.6.1. >. > Should I roll back to the previous version of annadata or scanpy? has. > anyone ran this feature with my package version with no problems? >. > Thanks a lot. >. > Here are the packages I use. >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/663?email_source=notifications&email_token=ABF37VPMR3WSZT3FIGCFNJ3PXPPJ3A5CNFSM4HP4ASU2YY3PNVWWK3TUL52HS4DFUVEXG43VMWVGG33NNVSW45C7NFSM4GWBCDVA>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VJLTRD6ZHIBLZIRHYLPXPPJ3ANCNFSM4HP4ASUQ>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:205,safety,updat,update,205,"The issue that you mention has been reported to matplotlib 3.1 and the. solution is to downgrade to 3.0*. I just updated the dependencies of. scanpy to be matplotlib 3.0. As soon as this is solved we will update the. dependencies. On Mon, May 27, 2019 at 3:33 PM bioguy2018 <notifications@github.com> wrote:. > Dear all. > I would like to project my umap from scanpy in 3d but I have faced the. > following problem:. >. > ValueError: operands could not be broadcast together with remapped shapes. > [original->remapped]: (0,4) and requested shape (816,4). >. > It's very strange because before I update some of my packages, I could run. > it it with no problem with the following packages:. >. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4. > scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760. > louvain==0.6.1. >. > but after updating some of my packages it was not possible due to that. > error! >. > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1. > pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0. > python-igraph==0.7.1+4.bed07760 louvain==0.6.1. >. > Should I roll back to the previous version of annadata or scanpy? has. > anyone ran this feature with my package version with no problems? >. > Thanks a lot. >. > Here are the packages I use. >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/663?email_source=notifications&email_token=ABF37VPMR3WSZT3FIGCFNJ3PXPPJ3A5CNFSM4HP4ASU2YY3PNVWWK3TUL52HS4DFUVEXG43VMWVGG33NNVSW45C7NFSM4GWBCDVA>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VJLTRD6ZHIBLZIRHYLPXPPJ3ANCNFSM4HP4ASUQ>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:217,safety,depend,dependencies,217,"The issue that you mention has been reported to matplotlib 3.1 and the. solution is to downgrade to 3.0*. I just updated the dependencies of. scanpy to be matplotlib 3.0. As soon as this is solved we will update the. dependencies. On Mon, May 27, 2019 at 3:33 PM bioguy2018 <notifications@github.com> wrote:. > Dear all. > I would like to project my umap from scanpy in 3d but I have faced the. > following problem:. >. > ValueError: operands could not be broadcast together with remapped shapes. > [original->remapped]: (0,4) and requested shape (816,4). >. > It's very strange because before I update some of my packages, I could run. > it it with no problem with the following packages:. >. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4. > scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760. > louvain==0.6.1. >. > but after updating some of my packages it was not possible due to that. > error! >. > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1. > pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0. > python-igraph==0.7.1+4.bed07760 louvain==0.6.1. >. > Should I roll back to the previous version of annadata or scanpy? has. > anyone ran this feature with my package version with no problems? >. > Thanks a lot. >. > Here are the packages I use. >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/663?email_source=notifications&email_token=ABF37VPMR3WSZT3FIGCFNJ3PXPPJ3A5CNFSM4HP4ASU2YY3PNVWWK3TUL52HS4DFUVEXG43VMWVGG33NNVSW45C7NFSM4GWBCDVA>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VJLTRD6ZHIBLZIRHYLPXPPJ3ANCNFSM4HP4ASUQ>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:596,safety,updat,update,596,"The issue that you mention has been reported to matplotlib 3.1 and the. solution is to downgrade to 3.0*. I just updated the dependencies of. scanpy to be matplotlib 3.0. As soon as this is solved we will update the. dependencies. On Mon, May 27, 2019 at 3:33 PM bioguy2018 <notifications@github.com> wrote:. > Dear all. > I would like to project my umap from scanpy in 3d but I have faced the. > following problem:. >. > ValueError: operands could not be broadcast together with remapped shapes. > [original->remapped]: (0,4) and requested shape (816,4). >. > It's very strange because before I update some of my packages, I could run. > it it with no problem with the following packages:. >. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4. > scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760. > louvain==0.6.1. >. > but after updating some of my packages it was not possible due to that. > error! >. > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1. > pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0. > python-igraph==0.7.1+4.bed07760 louvain==0.6.1. >. > Should I roll back to the previous version of annadata or scanpy? has. > anyone ran this feature with my package version with no problems? >. > Thanks a lot. >. > Here are the packages I use. >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/663?email_source=notifications&email_token=ABF37VPMR3WSZT3FIGCFNJ3PXPPJ3A5CNFSM4HP4ASU2YY3PNVWWK3TUL52HS4DFUVEXG43VMWVGG33NNVSW45C7NFSM4GWBCDVA>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VJLTRD6ZHIBLZIRHYLPXPPJ3ANCNFSM4HP4ASUQ>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:877,safety,updat,updating,877,"The issue that you mention has been reported to matplotlib 3.1 and the. solution is to downgrade to 3.0*. I just updated the dependencies of. scanpy to be matplotlib 3.0. As soon as this is solved we will update the. dependencies. On Mon, May 27, 2019 at 3:33 PM bioguy2018 <notifications@github.com> wrote:. > Dear all. > I would like to project my umap from scanpy in 3d but I have faced the. > following problem:. >. > ValueError: operands could not be broadcast together with remapped shapes. > [original->remapped]: (0,4) and requested shape (816,4). >. > It's very strange because before I update some of my packages, I could run. > it it with no problem with the following packages:. >. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4. > scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760. > louvain==0.6.1. >. > but after updating some of my packages it was not possible due to that. > error! >. > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1. > pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0. > python-igraph==0.7.1+4.bed07760 louvain==0.6.1. >. > Should I roll back to the previous version of annadata or scanpy? has. > anyone ran this feature with my package version with no problems? >. > Thanks a lot. >. > Here are the packages I use. >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/663?email_source=notifications&email_token=ABF37VPMR3WSZT3FIGCFNJ3PXPPJ3A5CNFSM4HP4ASU2YY3PNVWWK3TUL52HS4DFUVEXG43VMWVGG33NNVSW45C7NFSM4GWBCDVA>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VJLTRD6ZHIBLZIRHYLPXPPJ3ANCNFSM4HP4ASUQ>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:941,safety,error,error,941,"The issue that you mention has been reported to matplotlib 3.1 and the. solution is to downgrade to 3.0*. I just updated the dependencies of. scanpy to be matplotlib 3.0. As soon as this is solved we will update the. dependencies. On Mon, May 27, 2019 at 3:33 PM bioguy2018 <notifications@github.com> wrote:. > Dear all. > I would like to project my umap from scanpy in 3d but I have faced the. > following problem:. >. > ValueError: operands could not be broadcast together with remapped shapes. > [original->remapped]: (0,4) and requested shape (816,4). >. > It's very strange because before I update some of my packages, I could run. > it it with no problem with the following packages:. >. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4. > scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760. > louvain==0.6.1. >. > but after updating some of my packages it was not possible due to that. > error! >. > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1. > pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0. > python-igraph==0.7.1+4.bed07760 louvain==0.6.1. >. > Should I roll back to the previous version of annadata or scanpy? has. > anyone ran this feature with my package version with no problems? >. > Thanks a lot. >. > Here are the packages I use. >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/663?email_source=notifications&email_token=ABF37VPMR3WSZT3FIGCFNJ3PXPPJ3A5CNFSM4HP4ASU2YY3PNVWWK3TUL52HS4DFUVEXG43VMWVGG33NNVSW45C7NFSM4GWBCDVA>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VJLTRD6ZHIBLZIRHYLPXPPJ3ANCNFSM4HP4ASUQ>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:113,security,updat,updated,113,"The issue that you mention has been reported to matplotlib 3.1 and the. solution is to downgrade to 3.0*. I just updated the dependencies of. scanpy to be matplotlib 3.0. As soon as this is solved we will update the. dependencies. On Mon, May 27, 2019 at 3:33 PM bioguy2018 <notifications@github.com> wrote:. > Dear all. > I would like to project my umap from scanpy in 3d but I have faced the. > following problem:. >. > ValueError: operands could not be broadcast together with remapped shapes. > [original->remapped]: (0,4) and requested shape (816,4). >. > It's very strange because before I update some of my packages, I could run. > it it with no problem with the following packages:. >. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4. > scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760. > louvain==0.6.1. >. > but after updating some of my packages it was not possible due to that. > error! >. > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1. > pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0. > python-igraph==0.7.1+4.bed07760 louvain==0.6.1. >. > Should I roll back to the previous version of annadata or scanpy? has. > anyone ran this feature with my package version with no problems? >. > Thanks a lot. >. > Here are the packages I use. >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/663?email_source=notifications&email_token=ABF37VPMR3WSZT3FIGCFNJ3PXPPJ3A5CNFSM4HP4ASU2YY3PNVWWK3TUL52HS4DFUVEXG43VMWVGG33NNVSW45C7NFSM4GWBCDVA>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VJLTRD6ZHIBLZIRHYLPXPPJ3ANCNFSM4HP4ASUQ>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:205,security,updat,update,205,"The issue that you mention has been reported to matplotlib 3.1 and the. solution is to downgrade to 3.0*. I just updated the dependencies of. scanpy to be matplotlib 3.0. As soon as this is solved we will update the. dependencies. On Mon, May 27, 2019 at 3:33 PM bioguy2018 <notifications@github.com> wrote:. > Dear all. > I would like to project my umap from scanpy in 3d but I have faced the. > following problem:. >. > ValueError: operands could not be broadcast together with remapped shapes. > [original->remapped]: (0,4) and requested shape (816,4). >. > It's very strange because before I update some of my packages, I could run. > it it with no problem with the following packages:. >. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4. > scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760. > louvain==0.6.1. >. > but after updating some of my packages it was not possible due to that. > error! >. > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1. > pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0. > python-igraph==0.7.1+4.bed07760 louvain==0.6.1. >. > Should I roll back to the previous version of annadata or scanpy? has. > anyone ran this feature with my package version with no problems? >. > Thanks a lot. >. > Here are the packages I use. >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/663?email_source=notifications&email_token=ABF37VPMR3WSZT3FIGCFNJ3PXPPJ3A5CNFSM4HP4ASU2YY3PNVWWK3TUL52HS4DFUVEXG43VMWVGG33NNVSW45C7NFSM4GWBCDVA>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VJLTRD6ZHIBLZIRHYLPXPPJ3ANCNFSM4HP4ASUQ>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:596,security,updat,update,596,"The issue that you mention has been reported to matplotlib 3.1 and the. solution is to downgrade to 3.0*. I just updated the dependencies of. scanpy to be matplotlib 3.0. As soon as this is solved we will update the. dependencies. On Mon, May 27, 2019 at 3:33 PM bioguy2018 <notifications@github.com> wrote:. > Dear all. > I would like to project my umap from scanpy in 3d but I have faced the. > following problem:. >. > ValueError: operands could not be broadcast together with remapped shapes. > [original->remapped]: (0,4) and requested shape (816,4). >. > It's very strange because before I update some of my packages, I could run. > it it with no problem with the following packages:. >. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4. > scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760. > louvain==0.6.1. >. > but after updating some of my packages it was not possible due to that. > error! >. > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1. > pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0. > python-igraph==0.7.1+4.bed07760 louvain==0.6.1. >. > Should I roll back to the previous version of annadata or scanpy? has. > anyone ran this feature with my package version with no problems? >. > Thanks a lot. >. > Here are the packages I use. >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/663?email_source=notifications&email_token=ABF37VPMR3WSZT3FIGCFNJ3PXPPJ3A5CNFSM4HP4ASU2YY3PNVWWK3TUL52HS4DFUVEXG43VMWVGG33NNVSW45C7NFSM4GWBCDVA>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VJLTRD6ZHIBLZIRHYLPXPPJ3ANCNFSM4HP4ASUQ>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:877,security,updat,updating,877,"The issue that you mention has been reported to matplotlib 3.1 and the. solution is to downgrade to 3.0*. I just updated the dependencies of. scanpy to be matplotlib 3.0. As soon as this is solved we will update the. dependencies. On Mon, May 27, 2019 at 3:33 PM bioguy2018 <notifications@github.com> wrote:. > Dear all. > I would like to project my umap from scanpy in 3d but I have faced the. > following problem:. >. > ValueError: operands could not be broadcast together with remapped shapes. > [original->remapped]: (0,4) and requested shape (816,4). >. > It's very strange because before I update some of my packages, I could run. > it it with no problem with the following packages:. >. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4. > scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760. > louvain==0.6.1. >. > but after updating some of my packages it was not possible due to that. > error! >. > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1. > pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0. > python-igraph==0.7.1+4.bed07760 louvain==0.6.1. >. > Should I roll back to the previous version of annadata or scanpy? has. > anyone ran this feature with my package version with no problems? >. > Thanks a lot. >. > Here are the packages I use. >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/663?email_source=notifications&email_token=ABF37VPMR3WSZT3FIGCFNJ3PXPPJ3A5CNFSM4HP4ASU2YY3PNVWWK3TUL52HS4DFUVEXG43VMWVGG33NNVSW45C7NFSM4GWBCDVA>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VJLTRD6ZHIBLZIRHYLPXPPJ3ANCNFSM4HP4ASUQ>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:1717,security,auth,auth,1717,"The issue that you mention has been reported to matplotlib 3.1 and the. solution is to downgrade to 3.0*. I just updated the dependencies of. scanpy to be matplotlib 3.0. As soon as this is solved we will update the. dependencies. On Mon, May 27, 2019 at 3:33 PM bioguy2018 <notifications@github.com> wrote:. > Dear all. > I would like to project my umap from scanpy in 3d but I have faced the. > following problem:. >. > ValueError: operands could not be broadcast together with remapped shapes. > [original->remapped]: (0,4) and requested shape (816,4). >. > It's very strange because before I update some of my packages, I could run. > it it with no problem with the following packages:. >. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4. > scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760. > louvain==0.6.1. >. > but after updating some of my packages it was not possible due to that. > error! >. > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1. > pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0. > python-igraph==0.7.1+4.bed07760 louvain==0.6.1. >. > Should I roll back to the previous version of annadata or scanpy? has. > anyone ran this feature with my package version with no problems? >. > Thanks a lot. >. > Here are the packages I use. >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/663?email_source=notifications&email_token=ABF37VPMR3WSZT3FIGCFNJ3PXPPJ3A5CNFSM4HP4ASU2YY3PNVWWK3TUL52HS4DFUVEXG43VMWVGG33NNVSW45C7NFSM4GWBCDVA>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VJLTRD6ZHIBLZIRHYLPXPPJ3ANCNFSM4HP4ASUQ>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:125,testability,depend,dependencies,125,"The issue that you mention has been reported to matplotlib 3.1 and the. solution is to downgrade to 3.0*. I just updated the dependencies of. scanpy to be matplotlib 3.0. As soon as this is solved we will update the. dependencies. On Mon, May 27, 2019 at 3:33 PM bioguy2018 <notifications@github.com> wrote:. > Dear all. > I would like to project my umap from scanpy in 3d but I have faced the. > following problem:. >. > ValueError: operands could not be broadcast together with remapped shapes. > [original->remapped]: (0,4) and requested shape (816,4). >. > It's very strange because before I update some of my packages, I could run. > it it with no problem with the following packages:. >. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4. > scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760. > louvain==0.6.1. >. > but after updating some of my packages it was not possible due to that. > error! >. > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1. > pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0. > python-igraph==0.7.1+4.bed07760 louvain==0.6.1. >. > Should I roll back to the previous version of annadata or scanpy? has. > anyone ran this feature with my package version with no problems? >. > Thanks a lot. >. > Here are the packages I use. >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/663?email_source=notifications&email_token=ABF37VPMR3WSZT3FIGCFNJ3PXPPJ3A5CNFSM4HP4ASU2YY3PNVWWK3TUL52HS4DFUVEXG43VMWVGG33NNVSW45C7NFSM4GWBCDVA>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VJLTRD6ZHIBLZIRHYLPXPPJ3ANCNFSM4HP4ASUQ>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:217,testability,depend,dependencies,217,"The issue that you mention has been reported to matplotlib 3.1 and the. solution is to downgrade to 3.0*. I just updated the dependencies of. scanpy to be matplotlib 3.0. As soon as this is solved we will update the. dependencies. On Mon, May 27, 2019 at 3:33 PM bioguy2018 <notifications@github.com> wrote:. > Dear all. > I would like to project my umap from scanpy in 3d but I have faced the. > following problem:. >. > ValueError: operands could not be broadcast together with remapped shapes. > [original->remapped]: (0,4) and requested shape (816,4). >. > It's very strange because before I update some of my packages, I could run. > it it with no problem with the following packages:. >. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4. > scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760. > louvain==0.6.1. >. > but after updating some of my packages it was not possible due to that. > error! >. > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1. > pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0. > python-igraph==0.7.1+4.bed07760 louvain==0.6.1. >. > Should I roll back to the previous version of annadata or scanpy? has. > anyone ran this feature with my package version with no problems? >. > Thanks a lot. >. > Here are the packages I use. >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/663?email_source=notifications&email_token=ABF37VPMR3WSZT3FIGCFNJ3PXPPJ3A5CNFSM4HP4ASU2YY3PNVWWK3TUL52HS4DFUVEXG43VMWVGG33NNVSW45C7NFSM4GWBCDVA>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VJLTRD6ZHIBLZIRHYLPXPPJ3ANCNFSM4HP4ASUQ>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:778,usability,learn,learn,778,"The issue that you mention has been reported to matplotlib 3.1 and the. solution is to downgrade to 3.0*. I just updated the dependencies of. scanpy to be matplotlib 3.0. As soon as this is solved we will update the. dependencies. On Mon, May 27, 2019 at 3:33 PM bioguy2018 <notifications@github.com> wrote:. > Dear all. > I would like to project my umap from scanpy in 3d but I have faced the. > following problem:. >. > ValueError: operands could not be broadcast together with remapped shapes. > [original->remapped]: (0,4) and requested shape (816,4). >. > It's very strange because before I update some of my packages, I could run. > it it with no problem with the following packages:. >. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4. > scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760. > louvain==0.6.1. >. > but after updating some of my packages it was not possible due to that. > error! >. > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1. > pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0. > python-igraph==0.7.1+4.bed07760 louvain==0.6.1. >. > Should I roll back to the previous version of annadata or scanpy? has. > anyone ran this feature with my package version with no problems? >. > Thanks a lot. >. > Here are the packages I use. >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/663?email_source=notifications&email_token=ABF37VPMR3WSZT3FIGCFNJ3PXPPJ3A5CNFSM4HP4ASU2YY3PNVWWK3TUL52HS4DFUVEXG43VMWVGG33NNVSW45C7NFSM4GWBCDVA>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VJLTRD6ZHIBLZIRHYLPXPPJ3ANCNFSM4HP4ASUQ>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:941,usability,error,error,941,"The issue that you mention has been reported to matplotlib 3.1 and the. solution is to downgrade to 3.0*. I just updated the dependencies of. scanpy to be matplotlib 3.0. As soon as this is solved we will update the. dependencies. On Mon, May 27, 2019 at 3:33 PM bioguy2018 <notifications@github.com> wrote:. > Dear all. > I would like to project my umap from scanpy in 3d but I have faced the. > following problem:. >. > ValueError: operands could not be broadcast together with remapped shapes. > [original->remapped]: (0,4) and requested shape (816,4). >. > It's very strange because before I update some of my packages, I could run. > it it with no problem with the following packages:. >. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4. > scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760. > louvain==0.6.1. >. > but after updating some of my packages it was not possible due to that. > error! >. > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1. > pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0. > python-igraph==0.7.1+4.bed07760 louvain==0.6.1. >. > Should I roll back to the previous version of annadata or scanpy? has. > anyone ran this feature with my package version with no problems? >. > Thanks a lot. >. > Here are the packages I use. >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/663?email_source=notifications&email_token=ABF37VPMR3WSZT3FIGCFNJ3PXPPJ3A5CNFSM4HP4ASU2YY3PNVWWK3TUL52HS4DFUVEXG43VMWVGG33NNVSW45C7NFSM4GWBCDVA>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VJLTRD6ZHIBLZIRHYLPXPPJ3ANCNFSM4HP4ASUQ>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/663:1047,usability,learn,learn,1047,"The issue that you mention has been reported to matplotlib 3.1 and the. solution is to downgrade to 3.0*. I just updated the dependencies of. scanpy to be matplotlib 3.0. As soon as this is solved we will update the. dependencies. On Mon, May 27, 2019 at 3:33 PM bioguy2018 <notifications@github.com> wrote:. > Dear all. > I would like to project my umap from scanpy in 3d but I have faced the. > following problem:. >. > ValueError: operands could not be broadcast together with remapped shapes. > [original->remapped]: (0,4) and requested shape (816,4). >. > It's very strange because before I update some of my packages, I could run. > it it with no problem with the following packages:. >. > scanpy==1.4.1 anndata==0.6.19 numpy==1.16.3 scipy==1.2.1 pandas==0.23.4. > scikit-learn==0.20.3 statsmodels==0.9.0 python-igraph==0.7.1+4.bed07760. > louvain==0.6.1. >. > but after updating some of my packages it was not possible due to that. > error! >. > scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1. > pandas==0.23.4 scikit-learn==0.20.3 statsmodels==0.9.0. > python-igraph==0.7.1+4.bed07760 louvain==0.6.1. >. > Should I roll back to the previous version of annadata or scanpy? has. > anyone ran this feature with my package version with no problems? >. > Thanks a lot. >. > Here are the packages I use. >. > â€”. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/663?email_source=notifications&email_token=ABF37VPMR3WSZT3FIGCFNJ3PXPPJ3A5CNFSM4HP4ASU2YY3PNVWWK3TUL52HS4DFUVEXG43VMWVGG33NNVSW45C7NFSM4GWBCDVA>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/ABF37VJLTRD6ZHIBLZIRHYLPXPPJ3ANCNFSM4HP4ASUQ>. > . >. -- . Fidel Ramirez.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/663
https://github.com/scverse/scanpy/issues/666:235,availability,down,downgrading,235,This is the output of `sc.logging.print_versions()`:. ```. scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1. ``` . Probably downgrading of `scikit-learn` might help?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:26,deployability,log,logging,26,This is the output of `sc.logging.print_versions()`:. ```. scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1. ``` . Probably downgrading of `scikit-learn` might help?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:26,safety,log,logging,26,This is the output of `sc.logging.print_versions()`:. ```. scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1. ``` . Probably downgrading of `scikit-learn` might help?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:26,security,log,logging,26,This is the output of `sc.logging.print_versions()`:. ```. scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1. ``` . Probably downgrading of `scikit-learn` might help?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:26,testability,log,logging,26,This is the output of `sc.logging.print_versions()`:. ```. scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1. ``` . Probably downgrading of `scikit-learn` might help?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:150,usability,learn,learn,150,This is the output of `sc.logging.print_versions()`:. ```. scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1. ``` . Probably downgrading of `scikit-learn` might help?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:258,usability,learn,learn,258,This is the output of `sc.logging.print_versions()`:. ```. scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1. ``` . Probably downgrading of `scikit-learn` might help?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:271,usability,help,help,271,This is the output of `sc.logging.print_versions()`:. ```. scanpy==1.4.3 anndata==0.6.20 umap==0.3.8 numpy==1.16.3 scipy==1.2.1 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1. ``` . Probably downgrading of `scikit-learn` might help?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:0,availability,Down,Downgrading,0,Downgrading `scikit-learn` to `0.20.3` does not resolve the error...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:60,availability,error,error,60,Downgrading `scikit-learn` to `0.20.3` does not resolve the error...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:60,performance,error,error,60,Downgrading `scikit-learn` to `0.20.3` does not resolve the error...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:39,reliability,doe,does,39,Downgrading `scikit-learn` to `0.20.3` does not resolve the error...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:60,safety,error,error,60,Downgrading `scikit-learn` to `0.20.3` does not resolve the error...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:20,usability,learn,learn,20,Downgrading `scikit-learn` to `0.20.3` does not resolve the error...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:60,usability,error,error,60,Downgrading `scikit-learn` to `0.20.3` does not resolve the error...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:348,availability,cluster,clustering,348,"> What I basically do from raw UMI counts:. > 1. total counts normalization / logarithmization. > 2. PCA, bbknn, louvain. > 3. combat, HVG, PCA, UMAP (works well). Just out of curiousity, you use both BBKNN and combat? Does Louvain after ComBat, HVG, and PCA not work as well for you? It's interesting that you go with two different knn graphs for clustering and visualization.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:78,deployability,log,logarithmization,78,"> What I basically do from raw UMI counts:. > 1. total counts normalization / logarithmization. > 2. PCA, bbknn, louvain. > 3. combat, HVG, PCA, UMAP (works well). Just out of curiousity, you use both BBKNN and combat? Does Louvain after ComBat, HVG, and PCA not work as well for you? It's interesting that you go with two different knn graphs for clustering and visualization.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:348,deployability,cluster,clustering,348,"> What I basically do from raw UMI counts:. > 1. total counts normalization / logarithmization. > 2. PCA, bbknn, louvain. > 3. combat, HVG, PCA, UMAP (works well). Just out of curiousity, you use both BBKNN and combat? Does Louvain after ComBat, HVG, and PCA not work as well for you? It's interesting that you go with two different knn graphs for clustering and visualization.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:219,reliability,Doe,Does,219,"> What I basically do from raw UMI counts:. > 1. total counts normalization / logarithmization. > 2. PCA, bbknn, louvain. > 3. combat, HVG, PCA, UMAP (works well). Just out of curiousity, you use both BBKNN and combat? Does Louvain after ComBat, HVG, and PCA not work as well for you? It's interesting that you go with two different knn graphs for clustering and visualization.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:78,safety,log,logarithmization,78,"> What I basically do from raw UMI counts:. > 1. total counts normalization / logarithmization. > 2. PCA, bbknn, louvain. > 3. combat, HVG, PCA, UMAP (works well). Just out of curiousity, you use both BBKNN and combat? Does Louvain after ComBat, HVG, and PCA not work as well for you? It's interesting that you go with two different knn graphs for clustering and visualization.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:78,security,log,logarithmization,78,"> What I basically do from raw UMI counts:. > 1. total counts normalization / logarithmization. > 2. PCA, bbknn, louvain. > 3. combat, HVG, PCA, UMAP (works well). Just out of curiousity, you use both BBKNN and combat? Does Louvain after ComBat, HVG, and PCA not work as well for you? It's interesting that you go with two different knn graphs for clustering and visualization.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:78,testability,log,logarithmization,78,"> What I basically do from raw UMI counts:. > 1. total counts normalization / logarithmization. > 2. PCA, bbknn, louvain. > 3. combat, HVG, PCA, UMAP (works well). Just out of curiousity, you use both BBKNN and combat? Does Louvain after ComBat, HVG, and PCA not work as well for you? It's interesting that you go with two different knn graphs for clustering and visualization.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:363,usability,visual,visualization,363,"> What I basically do from raw UMI counts:. > 1. total counts normalization / logarithmization. > 2. PCA, bbknn, louvain. > 3. combat, HVG, PCA, UMAP (works well). Just out of curiousity, you use both BBKNN and combat? Does Louvain after ComBat, HVG, and PCA not work as well for you? It's interesting that you go with two different knn graphs for clustering and visualization.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:4,availability,error,error,4,The error seems to be that [`rdist`](https://github.com/lmcinnes/umap/blob/0.3.8/umap/umap_.py#L663) is called from [`umap.umap_.optimize_layout`](https://github.com/lmcinnes/umap/blob/0.3.8/umap/umap_.py#L776) with float64 arrays while it can only handle float32 arrays. There seem to have been a few changes in umap [between 0.3.8 and 0.3.9](https://github.com/lmcinnes/umap/compare/0.3.8...0.3.9) maybe you should try 0.3.9.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:4,performance,error,error,4,The error seems to be that [`rdist`](https://github.com/lmcinnes/umap/blob/0.3.8/umap/umap_.py#L663) is called from [`umap.umap_.optimize_layout`](https://github.com/lmcinnes/umap/blob/0.3.8/umap/umap_.py#L776) with float64 arrays while it can only handle float32 arrays. There seem to have been a few changes in umap [between 0.3.8 and 0.3.9](https://github.com/lmcinnes/umap/compare/0.3.8...0.3.9) maybe you should try 0.3.9.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:4,safety,error,error,4,The error seems to be that [`rdist`](https://github.com/lmcinnes/umap/blob/0.3.8/umap/umap_.py#L663) is called from [`umap.umap_.optimize_layout`](https://github.com/lmcinnes/umap/blob/0.3.8/umap/umap_.py#L776) with float64 arrays while it can only handle float32 arrays. There seem to have been a few changes in umap [between 0.3.8 and 0.3.9](https://github.com/lmcinnes/umap/compare/0.3.8...0.3.9) maybe you should try 0.3.9.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:4,usability,error,error,4,The error seems to be that [`rdist`](https://github.com/lmcinnes/umap/blob/0.3.8/umap/umap_.py#L663) is called from [`umap.umap_.optimize_layout`](https://github.com/lmcinnes/umap/blob/0.3.8/umap/umap_.py#L776) with float64 arrays while it can only handle float32 arrays. There seem to have been a few changes in umap [between 0.3.8 and 0.3.9](https://github.com/lmcinnes/umap/compare/0.3.8...0.3.9) maybe you should try 0.3.9.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:186,availability,cluster,clustering,186,"> Just out of curiousity, you use both BBKNN and combat? Does Louvain after ComBat, HVG, and PCA not work as well for you? It's interesting that you go with two different knn graphs for clustering and visualization. @LuckyMD I found that the clustering using the bbknn kNN graph is much cleaner on UMAP, compared to e.g. `sc.pp.neighbors`. From combat, I just obtain the adjusted data, not a kNN graph. . > There seem to have been a few changes in umap between 0.3.8 and 0.3.9 maybe you should try 0.3.9. @flying-sheep Thanks! With `scikit-learn` pinned to `0.20.3` the `umap` version was updated to `0.3.9` (I use a container and rebuilt it). . I guess I will try to create a reproducible example and open an issue in umap. Edit: https://github.com/lmcinnes/umap/issues/179",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:242,availability,cluster,clustering,242,"> Just out of curiousity, you use both BBKNN and combat? Does Louvain after ComBat, HVG, and PCA not work as well for you? It's interesting that you go with two different knn graphs for clustering and visualization. @LuckyMD I found that the clustering using the bbknn kNN graph is much cleaner on UMAP, compared to e.g. `sc.pp.neighbors`. From combat, I just obtain the adjusted data, not a kNN graph. . > There seem to have been a few changes in umap between 0.3.8 and 0.3.9 maybe you should try 0.3.9. @flying-sheep Thanks! With `scikit-learn` pinned to `0.20.3` the `umap` version was updated to `0.3.9` (I use a container and rebuilt it). . I guess I will try to create a reproducible example and open an issue in umap. Edit: https://github.com/lmcinnes/umap/issues/179",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:186,deployability,cluster,clustering,186,"> Just out of curiousity, you use both BBKNN and combat? Does Louvain after ComBat, HVG, and PCA not work as well for you? It's interesting that you go with two different knn graphs for clustering and visualization. @LuckyMD I found that the clustering using the bbknn kNN graph is much cleaner on UMAP, compared to e.g. `sc.pp.neighbors`. From combat, I just obtain the adjusted data, not a kNN graph. . > There seem to have been a few changes in umap between 0.3.8 and 0.3.9 maybe you should try 0.3.9. @flying-sheep Thanks! With `scikit-learn` pinned to `0.20.3` the `umap` version was updated to `0.3.9` (I use a container and rebuilt it). . I guess I will try to create a reproducible example and open an issue in umap. Edit: https://github.com/lmcinnes/umap/issues/179",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:242,deployability,cluster,clustering,242,"> Just out of curiousity, you use both BBKNN and combat? Does Louvain after ComBat, HVG, and PCA not work as well for you? It's interesting that you go with two different knn graphs for clustering and visualization. @LuckyMD I found that the clustering using the bbknn kNN graph is much cleaner on UMAP, compared to e.g. `sc.pp.neighbors`. From combat, I just obtain the adjusted data, not a kNN graph. . > There seem to have been a few changes in umap between 0.3.8 and 0.3.9 maybe you should try 0.3.9. @flying-sheep Thanks! With `scikit-learn` pinned to `0.20.3` the `umap` version was updated to `0.3.9` (I use a container and rebuilt it). . I guess I will try to create a reproducible example and open an issue in umap. Edit: https://github.com/lmcinnes/umap/issues/179",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:577,deployability,version,version,577,"> Just out of curiousity, you use both BBKNN and combat? Does Louvain after ComBat, HVG, and PCA not work as well for you? It's interesting that you go with two different knn graphs for clustering and visualization. @LuckyMD I found that the clustering using the bbknn kNN graph is much cleaner on UMAP, compared to e.g. `sc.pp.neighbors`. From combat, I just obtain the adjusted data, not a kNN graph. . > There seem to have been a few changes in umap between 0.3.8 and 0.3.9 maybe you should try 0.3.9. @flying-sheep Thanks! With `scikit-learn` pinned to `0.20.3` the `umap` version was updated to `0.3.9` (I use a container and rebuilt it). . I guess I will try to create a reproducible example and open an issue in umap. Edit: https://github.com/lmcinnes/umap/issues/179",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:589,deployability,updat,updated,589,"> Just out of curiousity, you use both BBKNN and combat? Does Louvain after ComBat, HVG, and PCA not work as well for you? It's interesting that you go with two different knn graphs for clustering and visualization. @LuckyMD I found that the clustering using the bbknn kNN graph is much cleaner on UMAP, compared to e.g. `sc.pp.neighbors`. From combat, I just obtain the adjusted data, not a kNN graph. . > There seem to have been a few changes in umap between 0.3.8 and 0.3.9 maybe you should try 0.3.9. @flying-sheep Thanks! With `scikit-learn` pinned to `0.20.3` the `umap` version was updated to `0.3.9` (I use a container and rebuilt it). . I guess I will try to create a reproducible example and open an issue in umap. Edit: https://github.com/lmcinnes/umap/issues/179",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:617,deployability,contain,container,617,"> Just out of curiousity, you use both BBKNN and combat? Does Louvain after ComBat, HVG, and PCA not work as well for you? It's interesting that you go with two different knn graphs for clustering and visualization. @LuckyMD I found that the clustering using the bbknn kNN graph is much cleaner on UMAP, compared to e.g. `sc.pp.neighbors`. From combat, I just obtain the adjusted data, not a kNN graph. . > There seem to have been a few changes in umap between 0.3.8 and 0.3.9 maybe you should try 0.3.9. @flying-sheep Thanks! With `scikit-learn` pinned to `0.20.3` the `umap` version was updated to `0.3.9` (I use a container and rebuilt it). . I guess I will try to create a reproducible example and open an issue in umap. Edit: https://github.com/lmcinnes/umap/issues/179",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:577,integrability,version,version,577,"> Just out of curiousity, you use both BBKNN and combat? Does Louvain after ComBat, HVG, and PCA not work as well for you? It's interesting that you go with two different knn graphs for clustering and visualization. @LuckyMD I found that the clustering using the bbknn kNN graph is much cleaner on UMAP, compared to e.g. `sc.pp.neighbors`. From combat, I just obtain the adjusted data, not a kNN graph. . > There seem to have been a few changes in umap between 0.3.8 and 0.3.9 maybe you should try 0.3.9. @flying-sheep Thanks! With `scikit-learn` pinned to `0.20.3` the `umap` version was updated to `0.3.9` (I use a container and rebuilt it). . I guess I will try to create a reproducible example and open an issue in umap. Edit: https://github.com/lmcinnes/umap/issues/179",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:577,modifiability,version,version,577,"> Just out of curiousity, you use both BBKNN and combat? Does Louvain after ComBat, HVG, and PCA not work as well for you? It's interesting that you go with two different knn graphs for clustering and visualization. @LuckyMD I found that the clustering using the bbknn kNN graph is much cleaner on UMAP, compared to e.g. `sc.pp.neighbors`. From combat, I just obtain the adjusted data, not a kNN graph. . > There seem to have been a few changes in umap between 0.3.8 and 0.3.9 maybe you should try 0.3.9. @flying-sheep Thanks! With `scikit-learn` pinned to `0.20.3` the `umap` version was updated to `0.3.9` (I use a container and rebuilt it). . I guess I will try to create a reproducible example and open an issue in umap. Edit: https://github.com/lmcinnes/umap/issues/179",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:57,reliability,Doe,Does,57,"> Just out of curiousity, you use both BBKNN and combat? Does Louvain after ComBat, HVG, and PCA not work as well for you? It's interesting that you go with two different knn graphs for clustering and visualization. @LuckyMD I found that the clustering using the bbknn kNN graph is much cleaner on UMAP, compared to e.g. `sc.pp.neighbors`. From combat, I just obtain the adjusted data, not a kNN graph. . > There seem to have been a few changes in umap between 0.3.8 and 0.3.9 maybe you should try 0.3.9. @flying-sheep Thanks! With `scikit-learn` pinned to `0.20.3` the `umap` version was updated to `0.3.9` (I use a container and rebuilt it). . I guess I will try to create a reproducible example and open an issue in umap. Edit: https://github.com/lmcinnes/umap/issues/179",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:589,safety,updat,updated,589,"> Just out of curiousity, you use both BBKNN and combat? Does Louvain after ComBat, HVG, and PCA not work as well for you? It's interesting that you go with two different knn graphs for clustering and visualization. @LuckyMD I found that the clustering using the bbknn kNN graph is much cleaner on UMAP, compared to e.g. `sc.pp.neighbors`. From combat, I just obtain the adjusted data, not a kNN graph. . > There seem to have been a few changes in umap between 0.3.8 and 0.3.9 maybe you should try 0.3.9. @flying-sheep Thanks! With `scikit-learn` pinned to `0.20.3` the `umap` version was updated to `0.3.9` (I use a container and rebuilt it). . I guess I will try to create a reproducible example and open an issue in umap. Edit: https://github.com/lmcinnes/umap/issues/179",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:589,security,updat,updated,589,"> Just out of curiousity, you use both BBKNN and combat? Does Louvain after ComBat, HVG, and PCA not work as well for you? It's interesting that you go with two different knn graphs for clustering and visualization. @LuckyMD I found that the clustering using the bbknn kNN graph is much cleaner on UMAP, compared to e.g. `sc.pp.neighbors`. From combat, I just obtain the adjusted data, not a kNN graph. . > There seem to have been a few changes in umap between 0.3.8 and 0.3.9 maybe you should try 0.3.9. @flying-sheep Thanks! With `scikit-learn` pinned to `0.20.3` the `umap` version was updated to `0.3.9` (I use a container and rebuilt it). . I guess I will try to create a reproducible example and open an issue in umap. Edit: https://github.com/lmcinnes/umap/issues/179",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:201,usability,visual,visualization,201,"> Just out of curiousity, you use both BBKNN and combat? Does Louvain after ComBat, HVG, and PCA not work as well for you? It's interesting that you go with two different knn graphs for clustering and visualization. @LuckyMD I found that the clustering using the bbknn kNN graph is much cleaner on UMAP, compared to e.g. `sc.pp.neighbors`. From combat, I just obtain the adjusted data, not a kNN graph. . > There seem to have been a few changes in umap between 0.3.8 and 0.3.9 maybe you should try 0.3.9. @flying-sheep Thanks! With `scikit-learn` pinned to `0.20.3` the `umap` version was updated to `0.3.9` (I use a container and rebuilt it). . I guess I will try to create a reproducible example and open an issue in umap. Edit: https://github.com/lmcinnes/umap/issues/179",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:540,usability,learn,learn,540,"> Just out of curiousity, you use both BBKNN and combat? Does Louvain after ComBat, HVG, and PCA not work as well for you? It's interesting that you go with two different knn graphs for clustering and visualization. @LuckyMD I found that the clustering using the bbknn kNN graph is much cleaner on UMAP, compared to e.g. `sc.pp.neighbors`. From combat, I just obtain the adjusted data, not a kNN graph. . > There seem to have been a few changes in umap between 0.3.8 and 0.3.9 maybe you should try 0.3.9. @flying-sheep Thanks! With `scikit-learn` pinned to `0.20.3` the `umap` version was updated to `0.3.9` (I use a container and rebuilt it). . I guess I will try to create a reproducible example and open an issue in umap. Edit: https://github.com/lmcinnes/umap/issues/179",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:28,availability,cluster,clustering,28,"> @LuckyMD I found that the clustering using the bbknn kNN graph is much cleaner on UMAP, compared to e.g. `sc.pp.neighbors`. From combat, I just obtain the adjusted data, not a kNN graph. I see. So you don't recalculate `sc.pp.neighbors` after the combat run? Otherwise your umap is not using the combat corrected counts at all, but instead the KNN graph from bbknn as far as I understand. Then you are basically using bbknn for the embedding, and combat only for the visualization of expression values in your plots. Do your batches have similar cell type compositions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:28,deployability,cluster,clustering,28,"> @LuckyMD I found that the clustering using the bbknn kNN graph is much cleaner on UMAP, compared to e.g. `sc.pp.neighbors`. From combat, I just obtain the adjusted data, not a kNN graph. I see. So you don't recalculate `sc.pp.neighbors` after the combat run? Otherwise your umap is not using the combat corrected counts at all, but instead the KNN graph from bbknn as far as I understand. Then you are basically using bbknn for the embedding, and combat only for the visualization of expression values in your plots. Do your batches have similar cell type compositions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:558,deployability,compos,compositions,558,"> @LuckyMD I found that the clustering using the bbknn kNN graph is much cleaner on UMAP, compared to e.g. `sc.pp.neighbors`. From combat, I just obtain the adjusted data, not a kNN graph. I see. So you don't recalculate `sc.pp.neighbors` after the combat run? Otherwise your umap is not using the combat corrected counts at all, but instead the KNN graph from bbknn as far as I understand. Then you are basically using bbknn for the embedding, and combat only for the visualization of expression values in your plots. Do your batches have similar cell type compositions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:527,integrability,batch,batches,527,"> @LuckyMD I found that the clustering using the bbknn kNN graph is much cleaner on UMAP, compared to e.g. `sc.pp.neighbors`. From combat, I just obtain the adjusted data, not a kNN graph. I see. So you don't recalculate `sc.pp.neighbors` after the combat run? Otherwise your umap is not using the combat corrected counts at all, but instead the KNN graph from bbknn as far as I understand. Then you are basically using bbknn for the embedding, and combat only for the visualization of expression values in your plots. Do your batches have similar cell type compositions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:558,modifiability,compos,compositions,558,"> @LuckyMD I found that the clustering using the bbknn kNN graph is much cleaner on UMAP, compared to e.g. `sc.pp.neighbors`. From combat, I just obtain the adjusted data, not a kNN graph. I see. So you don't recalculate `sc.pp.neighbors` after the combat run? Otherwise your umap is not using the combat corrected counts at all, but instead the KNN graph from bbknn as far as I understand. Then you are basically using bbknn for the embedding, and combat only for the visualization of expression values in your plots. Do your batches have similar cell type compositions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:527,performance,batch,batches,527,"> @LuckyMD I found that the clustering using the bbknn kNN graph is much cleaner on UMAP, compared to e.g. `sc.pp.neighbors`. From combat, I just obtain the adjusted data, not a kNN graph. I see. So you don't recalculate `sc.pp.neighbors` after the combat run? Otherwise your umap is not using the combat corrected counts at all, but instead the KNN graph from bbknn as far as I understand. Then you are basically using bbknn for the embedding, and combat only for the visualization of expression values in your plots. Do your batches have similar cell type compositions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:379,testability,understand,understand,379,"> @LuckyMD I found that the clustering using the bbknn kNN graph is much cleaner on UMAP, compared to e.g. `sc.pp.neighbors`. From combat, I just obtain the adjusted data, not a kNN graph. I see. So you don't recalculate `sc.pp.neighbors` after the combat run? Otherwise your umap is not using the combat corrected counts at all, but instead the KNN graph from bbknn as far as I understand. Then you are basically using bbknn for the embedding, and combat only for the visualization of expression values in your plots. Do your batches have similar cell type compositions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:469,usability,visual,visualization,469,"> @LuckyMD I found that the clustering using the bbknn kNN graph is much cleaner on UMAP, compared to e.g. `sc.pp.neighbors`. From combat, I just obtain the adjusted data, not a kNN graph. I see. So you don't recalculate `sc.pp.neighbors` after the combat run? Otherwise your umap is not using the combat corrected counts at all, but instead the KNN graph from bbknn as far as I understand. Then you are basically using bbknn for the embedding, and combat only for the visualization of expression values in your plots. Do your batches have similar cell type compositions?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:209,deployability,compos,composition,209,"Exactly, I do not recalculate `sc.pp.neighbors` after `bbknn`. In my understanding, `UMAP` uses the corrected counts via the preceding PCA step? . Yes, my batches have very similar (if not the same) cell type composition.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:155,integrability,batch,batches,155,"Exactly, I do not recalculate `sc.pp.neighbors` after `bbknn`. In my understanding, `UMAP` uses the corrected counts via the preceding PCA step? . Yes, my batches have very similar (if not the same) cell type composition.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:209,modifiability,compos,composition,209,"Exactly, I do not recalculate `sc.pp.neighbors` after `bbknn`. In my understanding, `UMAP` uses the corrected counts via the preceding PCA step? . Yes, my batches have very similar (if not the same) cell type composition.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:155,performance,batch,batches,155,"Exactly, I do not recalculate `sc.pp.neighbors` after `bbknn`. In my understanding, `UMAP` uses the corrected counts via the preceding PCA step? . Yes, my batches have very similar (if not the same) cell type composition.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:69,testability,understand,understanding,69,"Exactly, I do not recalculate `sc.pp.neighbors` after `bbknn`. In my understanding, `UMAP` uses the corrected counts via the preceding PCA step? . Yes, my batches have very similar (if not the same) cell type composition.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:289,deployability,compos,composition,289,"I think umap works on the connectivities matrix generated from `sc.pp.neighbors`. I guess you can test this by omitting `sc.pp.neighbors` before running umap. Or just running umap after your step 2 and look at the difference. >Yes, my batches have very similar (if not the same) cell type composition. In that case it would make sense for both bbknn and combat to work quite well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:235,integrability,batch,batches,235,"I think umap works on the connectivities matrix generated from `sc.pp.neighbors`. I guess you can test this by omitting `sc.pp.neighbors` before running umap. Or just running umap after your step 2 and look at the difference. >Yes, my batches have very similar (if not the same) cell type composition. In that case it would make sense for both bbknn and combat to work quite well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:289,modifiability,compos,composition,289,"I think umap works on the connectivities matrix generated from `sc.pp.neighbors`. I guess you can test this by omitting `sc.pp.neighbors` before running umap. Or just running umap after your step 2 and look at the difference. >Yes, my batches have very similar (if not the same) cell type composition. In that case it would make sense for both bbknn and combat to work quite well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:235,performance,batch,batches,235,"I think umap works on the connectivities matrix generated from `sc.pp.neighbors`. I guess you can test this by omitting `sc.pp.neighbors` before running umap. Or just running umap after your step 2 and look at the difference. >Yes, my batches have very similar (if not the same) cell type composition. In that case it would make sense for both bbknn and combat to work quite well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:98,safety,test,test,98,"I think umap works on the connectivities matrix generated from `sc.pp.neighbors`. I guess you can test this by omitting `sc.pp.neighbors` before running umap. Or just running umap after your step 2 and look at the difference. >Yes, my batches have very similar (if not the same) cell type composition. In that case it would make sense for both bbknn and combat to work quite well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:98,testability,test,test,98,"I think umap works on the connectivities matrix generated from `sc.pp.neighbors`. I guess you can test this by omitting `sc.pp.neighbors` before running umap. Or just running umap after your step 2 and look at the difference. >Yes, my batches have very similar (if not the same) cell type composition. In that case it would make sense for both bbknn and combat to work quite well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:404,integrability,batch,batch,404,"@falexwolf @flying-sheep FYI it seems that `sc.pp.combat` returns a `dtype('float64')`. When this ends up in `adata.X`, problems occur during `sc.tl.umap` with `init_pos`. Specifically, because the type of `get_init_pos_from_paga` is set to the type from `adata.X` [here](https://github.com/theislab/scanpy/blob/master/scanpy/tools/_umap.py#L116). . I could work around the problem using. ```. # Correct batch effect. adjusted_data = sc.pp.combat(adata, key = ""Batch"", inplace=False). # Add adjusted data as data. adata.X = adjusted_data.astype(adata.X.dtype). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:461,integrability,Batch,Batch,461,"@falexwolf @flying-sheep FYI it seems that `sc.pp.combat` returns a `dtype('float64')`. When this ends up in `adata.X`, problems occur during `sc.tl.umap` with `init_pos`. Specifically, because the type of `get_init_pos_from_paga` is set to the type from `adata.X` [here](https://github.com/theislab/scanpy/blob/master/scanpy/tools/_umap.py#L116). . I could work around the problem using. ```. # Correct batch effect. adjusted_data = sc.pp.combat(adata, key = ""Batch"", inplace=False). # Add adjusted data as data. adata.X = adjusted_data.astype(adata.X.dtype). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:172,interoperability,Specif,Specifically,172,"@falexwolf @flying-sheep FYI it seems that `sc.pp.combat` returns a `dtype('float64')`. When this ends up in `adata.X`, problems occur during `sc.tl.umap` with `init_pos`. Specifically, because the type of `get_init_pos_from_paga` is set to the type from `adata.X` [here](https://github.com/theislab/scanpy/blob/master/scanpy/tools/_umap.py#L116). . I could work around the problem using. ```. # Correct batch effect. adjusted_data = sc.pp.combat(adata, key = ""Batch"", inplace=False). # Add adjusted data as data. adata.X = adjusted_data.astype(adata.X.dtype). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:404,performance,batch,batch,404,"@falexwolf @flying-sheep FYI it seems that `sc.pp.combat` returns a `dtype('float64')`. When this ends up in `adata.X`, problems occur during `sc.tl.umap` with `init_pos`. Specifically, because the type of `get_init_pos_from_paga` is set to the type from `adata.X` [here](https://github.com/theislab/scanpy/blob/master/scanpy/tools/_umap.py#L116). . I could work around the problem using. ```. # Correct batch effect. adjusted_data = sc.pp.combat(adata, key = ""Batch"", inplace=False). # Add adjusted data as data. adata.X = adjusted_data.astype(adata.X.dtype). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:461,performance,Batch,Batch,461,"@falexwolf @flying-sheep FYI it seems that `sc.pp.combat` returns a `dtype('float64')`. When this ends up in `adata.X`, problems occur during `sc.tl.umap` with `init_pos`. Specifically, because the type of `get_init_pos_from_paga` is set to the type from `adata.X` [here](https://github.com/theislab/scanpy/blob/master/scanpy/tools/_umap.py#L116). . I could work around the problem using. ```. # Correct batch effect. adjusted_data = sc.pp.combat(adata, key = ""Batch"", inplace=False). # Add adjusted data as data. adata.X = adjusted_data.astype(adata.X.dtype). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:326,usability,tool,tools,326,"@falexwolf @flying-sheep FYI it seems that `sc.pp.combat` returns a `dtype('float64')`. When this ends up in `adata.X`, problems occur during `sc.tl.umap` with `init_pos`. Specifically, because the type of `get_init_pos_from_paga` is set to the type from `adata.X` [here](https://github.com/theislab/scanpy/blob/master/scanpy/tools/_umap.py#L116). . I could work around the problem using. ```. # Correct batch effect. adjusted_data = sc.pp.combat(adata, key = ""Batch"", inplace=False). # Add adjusted data as data. adata.X = adjusted_data.astype(adata.X.dtype). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:101,availability,state,states,101,"@LuckyMD Indeed, UMAP does not use information from `X_pca` when `bbknn` was run before, although it states so:. ```. computing UMAP. using 'X_pca' with n_pcs = 50. finished (0:00:19.09) --> added. 'X_umap', UMAP coordinates (adata.obsm). ``` . Any idea why? Then my setup uses `combat` batch correction only for visualization of expression values.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:101,integrability,state,states,101,"@LuckyMD Indeed, UMAP does not use information from `X_pca` when `bbknn` was run before, although it states so:. ```. computing UMAP. using 'X_pca' with n_pcs = 50. finished (0:00:19.09) --> added. 'X_umap', UMAP coordinates (adata.obsm). ``` . Any idea why? Then my setup uses `combat` batch correction only for visualization of expression values.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:287,integrability,batch,batch,287,"@LuckyMD Indeed, UMAP does not use information from `X_pca` when `bbknn` was run before, although it states so:. ```. computing UMAP. using 'X_pca' with n_pcs = 50. finished (0:00:19.09) --> added. 'X_umap', UMAP coordinates (adata.obsm). ``` . Any idea why? Then my setup uses `combat` batch correction only for visualization of expression values.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:213,interoperability,coordinat,coordinates,213,"@LuckyMD Indeed, UMAP does not use information from `X_pca` when `bbknn` was run before, although it states so:. ```. computing UMAP. using 'X_pca' with n_pcs = 50. finished (0:00:19.09) --> added. 'X_umap', UMAP coordinates (adata.obsm). ``` . Any idea why? Then my setup uses `combat` batch correction only for visualization of expression values.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:287,performance,batch,batch,287,"@LuckyMD Indeed, UMAP does not use information from `X_pca` when `bbknn` was run before, although it states so:. ```. computing UMAP. using 'X_pca' with n_pcs = 50. finished (0:00:19.09) --> added. 'X_umap', UMAP coordinates (adata.obsm). ``` . Any idea why? Then my setup uses `combat` batch correction only for visualization of expression values.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:22,reliability,doe,does,22,"@LuckyMD Indeed, UMAP does not use information from `X_pca` when `bbknn` was run before, although it states so:. ```. computing UMAP. using 'X_pca' with n_pcs = 50. finished (0:00:19.09) --> added. 'X_umap', UMAP coordinates (adata.obsm). ``` . Any idea why? Then my setup uses `combat` batch correction only for visualization of expression values.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:313,usability,visual,visualization,313,"@LuckyMD Indeed, UMAP does not use information from `X_pca` when `bbknn` was run before, although it states so:. ```. computing UMAP. using 'X_pca' with n_pcs = 50. finished (0:00:19.09) --> added. 'X_umap', UMAP coordinates (adata.obsm). ``` . Any idea why? Then my setup uses `combat` batch correction only for visualization of expression values.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:367,availability,cluster,clustering,367,"@jenzopr I think this is legacy from when `sc.pp.neighbors` was not a separate function, but `sc.tl.umap` instead had options to recompute the neighborhood graph. This should probably be changed. I don't know where the info about the 50 pcs comes from... is this stored somewhere related to the connectivities matrix? Do you know @flying-sheep? Also, I wonder if the clustering looks better for you only because you do both the umap and louvain with bbknn. It should also look good if both is done via combat in my opinion. Would be curious about this, as I recommend ComBat in my best-practices tutorial (based on several papers, like kBet).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:367,deployability,cluster,clustering,367,"@jenzopr I think this is legacy from when `sc.pp.neighbors` was not a separate function, but `sc.tl.umap` instead had options to recompute the neighborhood graph. This should probably be changed. I don't know where the info about the 50 pcs comes from... is this stored somewhere related to the connectivities matrix? Do you know @flying-sheep? Also, I wonder if the clustering looks better for you only because you do both the umap and louvain with bbknn. It should also look good if both is done via combat in my opinion. Would be curious about this, as I recommend ComBat in my best-practices tutorial (based on several papers, like kBet).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:586,reliability,pra,practices,586,"@jenzopr I think this is legacy from when `sc.pp.neighbors` was not a separate function, but `sc.tl.umap` instead had options to recompute the neighborhood graph. This should probably be changed. I don't know where the info about the 50 pcs comes from... is this stored somewhere related to the connectivities matrix? Do you know @flying-sheep? Also, I wonder if the clustering looks better for you only because you do both the umap and louvain with bbknn. It should also look good if both is done via combat in my opinion. Would be curious about this, as I recommend ComBat in my best-practices tutorial (based on several papers, like kBet).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:33,availability,sli,slides,33,@LuckyMD I can put together some slides for you :-),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:33,reliability,sli,slides,33,@LuckyMD I can put together some slides for you :-),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:14,availability,state,states,14,"> Although it states so:. UMAP only uses the representation of a data matrix for determining the number of connected components of the graph for the init conditions [if these aren't explicitly defined (they are if choosing `init_pos='paga'`): https://github.com/lmcinnes/umap/blob/948f60ff0caf7ccef0ab68626c7b99a11e66f1bb/umap/umap_.py#L958-L965. In fact, the only place where it enters is for the computation of the mean positions of the disconnected components: https://github.com/lmcinnes/umap/blob/948f60ff0caf7ccef0ab68626c7b99a11e66f1bb/umap/spectral.py#L50. Implementation-wise, it's a bit unfortunate that the data matrix is carried through all these functions just for that reason... But it's not a problem for the results. The confusing logging is fixed via. https://github.com/theislab/scanpy/commit/a5bd1ecd8ab04ec79369f60d3656f578a4cde40c",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:747,deployability,log,logging,747,"> Although it states so:. UMAP only uses the representation of a data matrix for determining the number of connected components of the graph for the init conditions [if these aren't explicitly defined (they are if choosing `init_pos='paga'`): https://github.com/lmcinnes/umap/blob/948f60ff0caf7ccef0ab68626c7b99a11e66f1bb/umap/umap_.py#L958-L965. In fact, the only place where it enters is for the computation of the mean positions of the disconnected components: https://github.com/lmcinnes/umap/blob/948f60ff0caf7ccef0ab68626c7b99a11e66f1bb/umap/spectral.py#L50. Implementation-wise, it's a bit unfortunate that the data matrix is carried through all these functions just for that reason... But it's not a problem for the results. The confusing logging is fixed via. https://github.com/theislab/scanpy/commit/a5bd1ecd8ab04ec79369f60d3656f578a4cde40c",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:14,integrability,state,states,14,"> Although it states so:. UMAP only uses the representation of a data matrix for determining the number of connected components of the graph for the init conditions [if these aren't explicitly defined (they are if choosing `init_pos='paga'`): https://github.com/lmcinnes/umap/blob/948f60ff0caf7ccef0ab68626c7b99a11e66f1bb/umap/umap_.py#L958-L965. In fact, the only place where it enters is for the computation of the mean positions of the disconnected components: https://github.com/lmcinnes/umap/blob/948f60ff0caf7ccef0ab68626c7b99a11e66f1bb/umap/spectral.py#L50. Implementation-wise, it's a bit unfortunate that the data matrix is carried through all these functions just for that reason... But it's not a problem for the results. The confusing logging is fixed via. https://github.com/theislab/scanpy/commit/a5bd1ecd8ab04ec79369f60d3656f578a4cde40c",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:117,integrability,compon,components,117,"> Although it states so:. UMAP only uses the representation of a data matrix for determining the number of connected components of the graph for the init conditions [if these aren't explicitly defined (they are if choosing `init_pos='paga'`): https://github.com/lmcinnes/umap/blob/948f60ff0caf7ccef0ab68626c7b99a11e66f1bb/umap/umap_.py#L958-L965. In fact, the only place where it enters is for the computation of the mean positions of the disconnected components: https://github.com/lmcinnes/umap/blob/948f60ff0caf7ccef0ab68626c7b99a11e66f1bb/umap/spectral.py#L50. Implementation-wise, it's a bit unfortunate that the data matrix is carried through all these functions just for that reason... But it's not a problem for the results. The confusing logging is fixed via. https://github.com/theislab/scanpy/commit/a5bd1ecd8ab04ec79369f60d3656f578a4cde40c",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:452,integrability,compon,components,452,"> Although it states so:. UMAP only uses the representation of a data matrix for determining the number of connected components of the graph for the init conditions [if these aren't explicitly defined (they are if choosing `init_pos='paga'`): https://github.com/lmcinnes/umap/blob/948f60ff0caf7ccef0ab68626c7b99a11e66f1bb/umap/umap_.py#L958-L965. In fact, the only place where it enters is for the computation of the mean positions of the disconnected components: https://github.com/lmcinnes/umap/blob/948f60ff0caf7ccef0ab68626c7b99a11e66f1bb/umap/spectral.py#L50. Implementation-wise, it's a bit unfortunate that the data matrix is carried through all these functions just for that reason... But it's not a problem for the results. The confusing logging is fixed via. https://github.com/theislab/scanpy/commit/a5bd1ecd8ab04ec79369f60d3656f578a4cde40c",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:117,interoperability,compon,components,117,"> Although it states so:. UMAP only uses the representation of a data matrix for determining the number of connected components of the graph for the init conditions [if these aren't explicitly defined (they are if choosing `init_pos='paga'`): https://github.com/lmcinnes/umap/blob/948f60ff0caf7ccef0ab68626c7b99a11e66f1bb/umap/umap_.py#L958-L965. In fact, the only place where it enters is for the computation of the mean positions of the disconnected components: https://github.com/lmcinnes/umap/blob/948f60ff0caf7ccef0ab68626c7b99a11e66f1bb/umap/spectral.py#L50. Implementation-wise, it's a bit unfortunate that the data matrix is carried through all these functions just for that reason... But it's not a problem for the results. The confusing logging is fixed via. https://github.com/theislab/scanpy/commit/a5bd1ecd8ab04ec79369f60d3656f578a4cde40c",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:452,interoperability,compon,components,452,"> Although it states so:. UMAP only uses the representation of a data matrix for determining the number of connected components of the graph for the init conditions [if these aren't explicitly defined (they are if choosing `init_pos='paga'`): https://github.com/lmcinnes/umap/blob/948f60ff0caf7ccef0ab68626c7b99a11e66f1bb/umap/umap_.py#L958-L965. In fact, the only place where it enters is for the computation of the mean positions of the disconnected components: https://github.com/lmcinnes/umap/blob/948f60ff0caf7ccef0ab68626c7b99a11e66f1bb/umap/spectral.py#L50. Implementation-wise, it's a bit unfortunate that the data matrix is carried through all these functions just for that reason... But it's not a problem for the results. The confusing logging is fixed via. https://github.com/theislab/scanpy/commit/a5bd1ecd8ab04ec79369f60d3656f578a4cde40c",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:117,modifiability,compon,components,117,"> Although it states so:. UMAP only uses the representation of a data matrix for determining the number of connected components of the graph for the init conditions [if these aren't explicitly defined (they are if choosing `init_pos='paga'`): https://github.com/lmcinnes/umap/blob/948f60ff0caf7ccef0ab68626c7b99a11e66f1bb/umap/umap_.py#L958-L965. In fact, the only place where it enters is for the computation of the mean positions of the disconnected components: https://github.com/lmcinnes/umap/blob/948f60ff0caf7ccef0ab68626c7b99a11e66f1bb/umap/spectral.py#L50. Implementation-wise, it's a bit unfortunate that the data matrix is carried through all these functions just for that reason... But it's not a problem for the results. The confusing logging is fixed via. https://github.com/theislab/scanpy/commit/a5bd1ecd8ab04ec79369f60d3656f578a4cde40c",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:452,modifiability,compon,components,452,"> Although it states so:. UMAP only uses the representation of a data matrix for determining the number of connected components of the graph for the init conditions [if these aren't explicitly defined (they are if choosing `init_pos='paga'`): https://github.com/lmcinnes/umap/blob/948f60ff0caf7ccef0ab68626c7b99a11e66f1bb/umap/umap_.py#L958-L965. In fact, the only place where it enters is for the computation of the mean positions of the disconnected components: https://github.com/lmcinnes/umap/blob/948f60ff0caf7ccef0ab68626c7b99a11e66f1bb/umap/spectral.py#L50. Implementation-wise, it's a bit unfortunate that the data matrix is carried through all these functions just for that reason... But it's not a problem for the results. The confusing logging is fixed via. https://github.com/theislab/scanpy/commit/a5bd1ecd8ab04ec79369f60d3656f578a4cde40c",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:747,safety,log,logging,747,"> Although it states so:. UMAP only uses the representation of a data matrix for determining the number of connected components of the graph for the init conditions [if these aren't explicitly defined (they are if choosing `init_pos='paga'`): https://github.com/lmcinnes/umap/blob/948f60ff0caf7ccef0ab68626c7b99a11e66f1bb/umap/umap_.py#L958-L965. In fact, the only place where it enters is for the computation of the mean positions of the disconnected components: https://github.com/lmcinnes/umap/blob/948f60ff0caf7ccef0ab68626c7b99a11e66f1bb/umap/spectral.py#L50. Implementation-wise, it's a bit unfortunate that the data matrix is carried through all these functions just for that reason... But it's not a problem for the results. The confusing logging is fixed via. https://github.com/theislab/scanpy/commit/a5bd1ecd8ab04ec79369f60d3656f578a4cde40c",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:747,security,log,logging,747,"> Although it states so:. UMAP only uses the representation of a data matrix for determining the number of connected components of the graph for the init conditions [if these aren't explicitly defined (they are if choosing `init_pos='paga'`): https://github.com/lmcinnes/umap/blob/948f60ff0caf7ccef0ab68626c7b99a11e66f1bb/umap/umap_.py#L958-L965. In fact, the only place where it enters is for the computation of the mean positions of the disconnected components: https://github.com/lmcinnes/umap/blob/948f60ff0caf7ccef0ab68626c7b99a11e66f1bb/umap/spectral.py#L50. Implementation-wise, it's a bit unfortunate that the data matrix is carried through all these functions just for that reason... But it's not a problem for the results. The confusing logging is fixed via. https://github.com/theislab/scanpy/commit/a5bd1ecd8ab04ec79369f60d3656f578a4cde40c",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:747,testability,log,logging,747,"> Although it states so:. UMAP only uses the representation of a data matrix for determining the number of connected components of the graph for the init conditions [if these aren't explicitly defined (they are if choosing `init_pos='paga'`): https://github.com/lmcinnes/umap/blob/948f60ff0caf7ccef0ab68626c7b99a11e66f1bb/umap/umap_.py#L958-L965. In fact, the only place where it enters is for the computation of the mean positions of the disconnected components: https://github.com/lmcinnes/umap/blob/948f60ff0caf7ccef0ab68626c7b99a11e66f1bb/umap/spectral.py#L50. Implementation-wise, it's a bit unfortunate that the data matrix is carried through all these functions just for that reason... But it's not a problem for the results. The confusing logging is fixed via. https://github.com/theislab/scanpy/commit/a5bd1ecd8ab04ec79369f60d3656f578a4cde40c",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:35,availability,sli,slides,35,> @LuckyMD I can put together some slides for you :-). that would be awesome :),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/666:35,reliability,sli,slides,35,> @LuckyMD I can put together some slides for you :-). that would be awesome :),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/666
https://github.com/scverse/scanpy/issues/667:164,integrability,sub,subsetting,164,"Hi,. I wonder whether you have a gene with constant expression value in there... that sounds like it might break the regression step. Otherwise, I would argue that subsetting to highly variable genes for regressing out a covariate is completely fine. In the end you are probably regressing out a covariate to improve the embedding. That is anyway only done on the highly variable genes, so other genes won't affect that. The only thing that might not be ideal is that you don't have the ""corrected"" data (data after regressing out your covariate) for plotting gene expression values, as you probably don't want to do any testing on the corrected data anyway. Still... it should be possible to do this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/667
https://github.com/scverse/scanpy/issues/667:185,modifiability,variab,variable,185,"Hi,. I wonder whether you have a gene with constant expression value in there... that sounds like it might break the regression step. Otherwise, I would argue that subsetting to highly variable genes for regressing out a covariate is completely fine. In the end you are probably regressing out a covariate to improve the embedding. That is anyway only done on the highly variable genes, so other genes won't affect that. The only thing that might not be ideal is that you don't have the ""corrected"" data (data after regressing out your covariate) for plotting gene expression values, as you probably don't want to do any testing on the corrected data anyway. Still... it should be possible to do this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/667
https://github.com/scverse/scanpy/issues/667:371,modifiability,variab,variable,371,"Hi,. I wonder whether you have a gene with constant expression value in there... that sounds like it might break the regression step. Otherwise, I would argue that subsetting to highly variable genes for regressing out a covariate is completely fine. In the end you are probably regressing out a covariate to improve the embedding. That is anyway only done on the highly variable genes, so other genes won't affect that. The only thing that might not be ideal is that you don't have the ""corrected"" data (data after regressing out your covariate) for plotting gene expression values, as you probably don't want to do any testing on the corrected data anyway. Still... it should be possible to do this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/667
https://github.com/scverse/scanpy/issues/667:234,safety,compl,completely,234,"Hi,. I wonder whether you have a gene with constant expression value in there... that sounds like it might break the regression step. Otherwise, I would argue that subsetting to highly variable genes for regressing out a covariate is completely fine. In the end you are probably regressing out a covariate to improve the embedding. That is anyway only done on the highly variable genes, so other genes won't affect that. The only thing that might not be ideal is that you don't have the ""corrected"" data (data after regressing out your covariate) for plotting gene expression values, as you probably don't want to do any testing on the corrected data anyway. Still... it should be possible to do this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/667
https://github.com/scverse/scanpy/issues/667:621,safety,test,testing,621,"Hi,. I wonder whether you have a gene with constant expression value in there... that sounds like it might break the regression step. Otherwise, I would argue that subsetting to highly variable genes for regressing out a covariate is completely fine. In the end you are probably regressing out a covariate to improve the embedding. That is anyway only done on the highly variable genes, so other genes won't affect that. The only thing that might not be ideal is that you don't have the ""corrected"" data (data after regressing out your covariate) for plotting gene expression values, as you probably don't want to do any testing on the corrected data anyway. Still... it should be possible to do this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/667
https://github.com/scverse/scanpy/issues/667:234,security,compl,completely,234,"Hi,. I wonder whether you have a gene with constant expression value in there... that sounds like it might break the regression step. Otherwise, I would argue that subsetting to highly variable genes for regressing out a covariate is completely fine. In the end you are probably regressing out a covariate to improve the embedding. That is anyway only done on the highly variable genes, so other genes won't affect that. The only thing that might not be ideal is that you don't have the ""corrected"" data (data after regressing out your covariate) for plotting gene expression values, as you probably don't want to do any testing on the corrected data anyway. Still... it should be possible to do this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/667
https://github.com/scverse/scanpy/issues/667:117,testability,regress,regression,117,"Hi,. I wonder whether you have a gene with constant expression value in there... that sounds like it might break the regression step. Otherwise, I would argue that subsetting to highly variable genes for regressing out a covariate is completely fine. In the end you are probably regressing out a covariate to improve the embedding. That is anyway only done on the highly variable genes, so other genes won't affect that. The only thing that might not be ideal is that you don't have the ""corrected"" data (data after regressing out your covariate) for plotting gene expression values, as you probably don't want to do any testing on the corrected data anyway. Still... it should be possible to do this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/667
https://github.com/scverse/scanpy/issues/667:204,testability,regress,regressing,204,"Hi,. I wonder whether you have a gene with constant expression value in there... that sounds like it might break the regression step. Otherwise, I would argue that subsetting to highly variable genes for regressing out a covariate is completely fine. In the end you are probably regressing out a covariate to improve the embedding. That is anyway only done on the highly variable genes, so other genes won't affect that. The only thing that might not be ideal is that you don't have the ""corrected"" data (data after regressing out your covariate) for plotting gene expression values, as you probably don't want to do any testing on the corrected data anyway. Still... it should be possible to do this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/667
https://github.com/scverse/scanpy/issues/667:279,testability,regress,regressing,279,"Hi,. I wonder whether you have a gene with constant expression value in there... that sounds like it might break the regression step. Otherwise, I would argue that subsetting to highly variable genes for regressing out a covariate is completely fine. In the end you are probably regressing out a covariate to improve the embedding. That is anyway only done on the highly variable genes, so other genes won't affect that. The only thing that might not be ideal is that you don't have the ""corrected"" data (data after regressing out your covariate) for plotting gene expression values, as you probably don't want to do any testing on the corrected data anyway. Still... it should be possible to do this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/667
https://github.com/scverse/scanpy/issues/667:516,testability,regress,regressing,516,"Hi,. I wonder whether you have a gene with constant expression value in there... that sounds like it might break the regression step. Otherwise, I would argue that subsetting to highly variable genes for regressing out a covariate is completely fine. In the end you are probably regressing out a covariate to improve the embedding. That is anyway only done on the highly variable genes, so other genes won't affect that. The only thing that might not be ideal is that you don't have the ""corrected"" data (data after regressing out your covariate) for plotting gene expression values, as you probably don't want to do any testing on the corrected data anyway. Still... it should be possible to do this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/667
https://github.com/scverse/scanpy/issues/667:621,testability,test,testing,621,"Hi,. I wonder whether you have a gene with constant expression value in there... that sounds like it might break the regression step. Otherwise, I would argue that subsetting to highly variable genes for regressing out a covariate is completely fine. In the end you are probably regressing out a covariate to improve the embedding. That is anyway only done on the highly variable genes, so other genes won't affect that. The only thing that might not be ideal is that you don't have the ""corrected"" data (data after regressing out your covariate) for plotting gene expression values, as you probably don't want to do any testing on the corrected data anyway. Still... it should be possible to do this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/667
https://github.com/scverse/scanpy/issues/667:20,availability,error,error,20,"+1, I have the same error (on different data), which also only seems to appear when I don't filter to leave only the highly variable genes. In my case,. ```. np.any(adata.X.sum(axis=0) == 0). np.any(adata.X.sum(axis=1) == 0). ```. both return `False`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/667
https://github.com/scverse/scanpy/issues/667:92,integrability,filter,filter,92,"+1, I have the same error (on different data), which also only seems to appear when I don't filter to leave only the highly variable genes. In my case,. ```. np.any(adata.X.sum(axis=0) == 0). np.any(adata.X.sum(axis=1) == 0). ```. both return `False`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/667
https://github.com/scverse/scanpy/issues/667:124,modifiability,variab,variable,124,"+1, I have the same error (on different data), which also only seems to appear when I don't filter to leave only the highly variable genes. In my case,. ```. np.any(adata.X.sum(axis=0) == 0). np.any(adata.X.sum(axis=1) == 0). ```. both return `False`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/667
https://github.com/scverse/scanpy/issues/667:20,performance,error,error,20,"+1, I have the same error (on different data), which also only seems to appear when I don't filter to leave only the highly variable genes. In my case,. ```. np.any(adata.X.sum(axis=0) == 0). np.any(adata.X.sum(axis=1) == 0). ```. both return `False`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/667
https://github.com/scverse/scanpy/issues/667:20,safety,error,error,20,"+1, I have the same error (on different data), which also only seems to appear when I don't filter to leave only the highly variable genes. In my case,. ```. np.any(adata.X.sum(axis=0) == 0). np.any(adata.X.sum(axis=1) == 0). ```. both return `False`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/667
https://github.com/scverse/scanpy/issues/667:20,usability,error,error,20,"+1, I have the same error (on different data), which also only seems to appear when I don't filter to leave only the highly variable genes. In my case,. ```. np.any(adata.X.sum(axis=0) == 0). np.any(adata.X.sum(axis=1) == 0). ```. both return `False`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/667
https://github.com/scverse/scanpy/issues/667:50,availability,error,error,50,"Hi @chris-rands . It would help to have the whole error traceback here to be able to diagnose this a bit better. @jorvis Did you find the solution to your issue, and if so could you post it here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/667
https://github.com/scverse/scanpy/issues/667:50,performance,error,error,50,"Hi @chris-rands . It would help to have the whole error traceback here to be able to diagnose this a bit better. @jorvis Did you find the solution to your issue, and if so could you post it here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/667
https://github.com/scverse/scanpy/issues/667:85,reliability,diagno,diagnose,85,"Hi @chris-rands . It would help to have the whole error traceback here to be able to diagnose this a bit better. @jorvis Did you find the solution to your issue, and if so could you post it here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/667
https://github.com/scverse/scanpy/issues/667:50,safety,error,error,50,"Hi @chris-rands . It would help to have the whole error traceback here to be able to diagnose this a bit better. @jorvis Did you find the solution to your issue, and if so could you post it here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/667
https://github.com/scverse/scanpy/issues/667:56,testability,trace,traceback,56,"Hi @chris-rands . It would help to have the whole error traceback here to be able to diagnose this a bit better. @jorvis Did you find the solution to your issue, and if so could you post it here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/667
https://github.com/scverse/scanpy/issues/667:85,testability,diagno,diagnose,85,"Hi @chris-rands . It would help to have the whole error traceback here to be able to diagnose this a bit better. @jorvis Did you find the solution to your issue, and if so could you post it here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/667
https://github.com/scverse/scanpy/issues/667:27,usability,help,help,27,"Hi @chris-rands . It would help to have the whole error traceback here to be able to diagnose this a bit better. @jorvis Did you find the solution to your issue, and if so could you post it here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/667
https://github.com/scverse/scanpy/issues/667:50,usability,error,error,50,"Hi @chris-rands . It would help to have the whole error traceback here to be able to diagnose this a bit better. @jorvis Did you find the solution to your issue, and if so could you post it here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/667
https://github.com/scverse/scanpy/issues/667:27,availability,error,error,27,"Right now I do not get the error if I do these steps:. ```py. sc.pp.normalize_per_cell(adata, counts_per_cell_after=norm_counts_per_cell). sc.pp.highly_variable_genes(adata, flavor=flavor, n_top_genes=n_top_genes). sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/667
https://github.com/scverse/scanpy/issues/667:27,performance,error,error,27,"Right now I do not get the error if I do these steps:. ```py. sc.pp.normalize_per_cell(adata, counts_per_cell_after=norm_counts_per_cell). sc.pp.highly_variable_genes(adata, flavor=flavor, n_top_genes=n_top_genes). sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/667
https://github.com/scverse/scanpy/issues/667:27,safety,error,error,27,"Right now I do not get the error if I do these steps:. ```py. sc.pp.normalize_per_cell(adata, counts_per_cell_after=norm_counts_per_cell). sc.pp.highly_variable_genes(adata, flavor=flavor, n_top_genes=n_top_genes). sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/667
https://github.com/scverse/scanpy/issues/667:27,usability,error,error,27,"Right now I do not get the error if I do these steps:. ```py. sc.pp.normalize_per_cell(adata, counts_per_cell_after=norm_counts_per_cell). sc.pp.highly_variable_genes(adata, flavor=flavor, n_top_genes=n_top_genes). sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/667
https://github.com/scverse/scanpy/issues/669:307,deployability,integr,integrated,307,"Hi,. You are correct that DE testing should be performed on raw or normalized data, but not on batch-corrected data. `sc.tl.rank_genes_groups()` doesn't let you include covariates, but there are plenty of methods that do. You could look into `diffxpy` for this, which is also based on AnnData and is easily integrated into a scanpy script. Otherwise, I have a case study for a best practices workflow, which uses MAST. You could reuse code from there as well. You can find the case study [here](https://www.github.com/theislab/single-cell-tutorial).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669
https://github.com/scverse/scanpy/issues/669:95,integrability,batch,batch-corrected,95,"Hi,. You are correct that DE testing should be performed on raw or normalized data, but not on batch-corrected data. `sc.tl.rank_genes_groups()` doesn't let you include covariates, but there are plenty of methods that do. You could look into `diffxpy` for this, which is also based on AnnData and is easily integrated into a scanpy script. Otherwise, I have a case study for a best practices workflow, which uses MAST. You could reuse code from there as well. You can find the case study [here](https://www.github.com/theislab/single-cell-tutorial).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669
https://github.com/scverse/scanpy/issues/669:307,integrability,integr,integrated,307,"Hi,. You are correct that DE testing should be performed on raw or normalized data, but not on batch-corrected data. `sc.tl.rank_genes_groups()` doesn't let you include covariates, but there are plenty of methods that do. You could look into `diffxpy` for this, which is also based on AnnData and is easily integrated into a scanpy script. Otherwise, I have a case study for a best practices workflow, which uses MAST. You could reuse code from there as well. You can find the case study [here](https://www.github.com/theislab/single-cell-tutorial).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669
https://github.com/scverse/scanpy/issues/669:307,interoperability,integr,integrated,307,"Hi,. You are correct that DE testing should be performed on raw or normalized data, but not on batch-corrected data. `sc.tl.rank_genes_groups()` doesn't let you include covariates, but there are plenty of methods that do. You could look into `diffxpy` for this, which is also based on AnnData and is easily integrated into a scanpy script. Otherwise, I have a case study for a best practices workflow, which uses MAST. You could reuse code from there as well. You can find the case study [here](https://www.github.com/theislab/single-cell-tutorial).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669
https://github.com/scverse/scanpy/issues/669:307,modifiability,integr,integrated,307,"Hi,. You are correct that DE testing should be performed on raw or normalized data, but not on batch-corrected data. `sc.tl.rank_genes_groups()` doesn't let you include covariates, but there are plenty of methods that do. You could look into `diffxpy` for this, which is also based on AnnData and is easily integrated into a scanpy script. Otherwise, I have a case study for a best practices workflow, which uses MAST. You could reuse code from there as well. You can find the case study [here](https://www.github.com/theislab/single-cell-tutorial).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669
https://github.com/scverse/scanpy/issues/669:429,modifiability,reu,reuse,429,"Hi,. You are correct that DE testing should be performed on raw or normalized data, but not on batch-corrected data. `sc.tl.rank_genes_groups()` doesn't let you include covariates, but there are plenty of methods that do. You could look into `diffxpy` for this, which is also based on AnnData and is easily integrated into a scanpy script. Otherwise, I have a case study for a best practices workflow, which uses MAST. You could reuse code from there as well. You can find the case study [here](https://www.github.com/theislab/single-cell-tutorial).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669
https://github.com/scverse/scanpy/issues/669:47,performance,perform,performed,47,"Hi,. You are correct that DE testing should be performed on raw or normalized data, but not on batch-corrected data. `sc.tl.rank_genes_groups()` doesn't let you include covariates, but there are plenty of methods that do. You could look into `diffxpy` for this, which is also based on AnnData and is easily integrated into a scanpy script. Otherwise, I have a case study for a best practices workflow, which uses MAST. You could reuse code from there as well. You can find the case study [here](https://www.github.com/theislab/single-cell-tutorial).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669
https://github.com/scverse/scanpy/issues/669:95,performance,batch,batch-corrected,95,"Hi,. You are correct that DE testing should be performed on raw or normalized data, but not on batch-corrected data. `sc.tl.rank_genes_groups()` doesn't let you include covariates, but there are plenty of methods that do. You could look into `diffxpy` for this, which is also based on AnnData and is easily integrated into a scanpy script. Otherwise, I have a case study for a best practices workflow, which uses MAST. You could reuse code from there as well. You can find the case study [here](https://www.github.com/theislab/single-cell-tutorial).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669
https://github.com/scverse/scanpy/issues/669:145,reliability,doe,doesn,145,"Hi,. You are correct that DE testing should be performed on raw or normalized data, but not on batch-corrected data. `sc.tl.rank_genes_groups()` doesn't let you include covariates, but there are plenty of methods that do. You could look into `diffxpy` for this, which is also based on AnnData and is easily integrated into a scanpy script. Otherwise, I have a case study for a best practices workflow, which uses MAST. You could reuse code from there as well. You can find the case study [here](https://www.github.com/theislab/single-cell-tutorial).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669
https://github.com/scverse/scanpy/issues/669:307,reliability,integr,integrated,307,"Hi,. You are correct that DE testing should be performed on raw or normalized data, but not on batch-corrected data. `sc.tl.rank_genes_groups()` doesn't let you include covariates, but there are plenty of methods that do. You could look into `diffxpy` for this, which is also based on AnnData and is easily integrated into a scanpy script. Otherwise, I have a case study for a best practices workflow, which uses MAST. You could reuse code from there as well. You can find the case study [here](https://www.github.com/theislab/single-cell-tutorial).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669
https://github.com/scverse/scanpy/issues/669:382,reliability,pra,practices,382,"Hi,. You are correct that DE testing should be performed on raw or normalized data, but not on batch-corrected data. `sc.tl.rank_genes_groups()` doesn't let you include covariates, but there are plenty of methods that do. You could look into `diffxpy` for this, which is also based on AnnData and is easily integrated into a scanpy script. Otherwise, I have a case study for a best practices workflow, which uses MAST. You could reuse code from there as well. You can find the case study [here](https://www.github.com/theislab/single-cell-tutorial).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669
https://github.com/scverse/scanpy/issues/669:29,safety,test,testing,29,"Hi,. You are correct that DE testing should be performed on raw or normalized data, but not on batch-corrected data. `sc.tl.rank_genes_groups()` doesn't let you include covariates, but there are plenty of methods that do. You could look into `diffxpy` for this, which is also based on AnnData and is easily integrated into a scanpy script. Otherwise, I have a case study for a best practices workflow, which uses MAST. You could reuse code from there as well. You can find the case study [here](https://www.github.com/theislab/single-cell-tutorial).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669
https://github.com/scverse/scanpy/issues/669:307,security,integr,integrated,307,"Hi,. You are correct that DE testing should be performed on raw or normalized data, but not on batch-corrected data. `sc.tl.rank_genes_groups()` doesn't let you include covariates, but there are plenty of methods that do. You could look into `diffxpy` for this, which is also based on AnnData and is easily integrated into a scanpy script. Otherwise, I have a case study for a best practices workflow, which uses MAST. You could reuse code from there as well. You can find the case study [here](https://www.github.com/theislab/single-cell-tutorial).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669
https://github.com/scverse/scanpy/issues/669:29,testability,test,testing,29,"Hi,. You are correct that DE testing should be performed on raw or normalized data, but not on batch-corrected data. `sc.tl.rank_genes_groups()` doesn't let you include covariates, but there are plenty of methods that do. You could look into `diffxpy` for this, which is also based on AnnData and is easily integrated into a scanpy script. Otherwise, I have a case study for a best practices workflow, which uses MAST. You could reuse code from there as well. You can find the case study [here](https://www.github.com/theislab/single-cell-tutorial).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669
https://github.com/scverse/scanpy/issues/669:307,testability,integr,integrated,307,"Hi,. You are correct that DE testing should be performed on raw or normalized data, but not on batch-corrected data. `sc.tl.rank_genes_groups()` doesn't let you include covariates, but there are plenty of methods that do. You could look into `diffxpy` for this, which is also based on AnnData and is easily integrated into a scanpy script. Otherwise, I have a case study for a best practices workflow, which uses MAST. You could reuse code from there as well. You can find the case study [here](https://www.github.com/theislab/single-cell-tutorial).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669
https://github.com/scverse/scanpy/issues/669:47,usability,perform,performed,47,"Hi,. You are correct that DE testing should be performed on raw or normalized data, but not on batch-corrected data. `sc.tl.rank_genes_groups()` doesn't let you include covariates, but there are plenty of methods that do. You could look into `diffxpy` for this, which is also based on AnnData and is easily integrated into a scanpy script. Otherwise, I have a case study for a best practices workflow, which uses MAST. You could reuse code from there as well. You can find the case study [here](https://www.github.com/theislab/single-cell-tutorial).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669
https://github.com/scverse/scanpy/issues/669:392,usability,workflow,workflow,392,"Hi,. You are correct that DE testing should be performed on raw or normalized data, but not on batch-corrected data. `sc.tl.rank_genes_groups()` doesn't let you include covariates, but there are plenty of methods that do. You could look into `diffxpy` for this, which is also based on AnnData and is easily integrated into a scanpy script. Otherwise, I have a case study for a best practices workflow, which uses MAST. You could reuse code from there as well. You can find the case study [here](https://www.github.com/theislab/single-cell-tutorial).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669
https://github.com/scverse/scanpy/issues/669:92,deployability,log,logistic,92,Thanks a lot @LuckyMD! MAST seems to be what I was looking for. An alternative might be the logistic regression with covariates Seurat offers within their FindMarkers function.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669
https://github.com/scverse/scanpy/issues/669:92,safety,log,logistic,92,Thanks a lot @LuckyMD! MAST seems to be what I was looking for. An alternative might be the logistic regression with covariates Seurat offers within their FindMarkers function.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669
https://github.com/scverse/scanpy/issues/669:92,security,log,logistic,92,Thanks a lot @LuckyMD! MAST seems to be what I was looking for. An alternative might be the logistic regression with covariates Seurat offers within their FindMarkers function.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669
https://github.com/scverse/scanpy/issues/669:92,testability,log,logistic,92,Thanks a lot @LuckyMD! MAST seems to be what I was looking for. An alternative might be the logistic regression with covariates Seurat offers within their FindMarkers function.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669
https://github.com/scverse/scanpy/issues/669:101,testability,regress,regression,101,Thanks a lot @LuckyMD! MAST seems to be what I was looking for. An alternative might be the logistic regression with covariates Seurat offers within their FindMarkers function.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669
https://github.com/scverse/scanpy/issues/669:87,deployability,fail,failed,87,"Hi. Thanks a lot for your advices @LuckyMD . However, the link to the case study seems failed. Could you please reattach it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669
https://github.com/scverse/scanpy/issues/669:87,reliability,fail,failed,87,"Hi. Thanks a lot for your advices @LuckyMD . However, the link to the case study seems failed. Could you please reattach it?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/669
https://github.com/scverse/scanpy/issues/670:239,availability,cluster,clustering,239,"I only can advice you on your second part of questions there is no rule of thumb for that. I also don't know what do you exactly mean by best suggestion resolution and how did you assess that. This is a general problem for many supervised clustering methods such as k-mean that user has to provide number of clusters or in this case the resolution which determines the number of clusters. Although there are some indirect ways to assess the clustering quality for example silhouette coefficient which gives you a score between -1 to 1 that tell you how similar your point in each clusters are. The other possibility is that you already expect the number of clusters so you can optimize the resolution based on your previous knowledge. . @falexwolf Out of curiosity, can we integrate such methods like silhouette coefficient inside scanpy? that would be cool!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:308,availability,cluster,clusters,308,"I only can advice you on your second part of questions there is no rule of thumb for that. I also don't know what do you exactly mean by best suggestion resolution and how did you assess that. This is a general problem for many supervised clustering methods such as k-mean that user has to provide number of clusters or in this case the resolution which determines the number of clusters. Although there are some indirect ways to assess the clustering quality for example silhouette coefficient which gives you a score between -1 to 1 that tell you how similar your point in each clusters are. The other possibility is that you already expect the number of clusters so you can optimize the resolution based on your previous knowledge. . @falexwolf Out of curiosity, can we integrate such methods like silhouette coefficient inside scanpy? that would be cool!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:379,availability,cluster,clusters,379,"I only can advice you on your second part of questions there is no rule of thumb for that. I also don't know what do you exactly mean by best suggestion resolution and how did you assess that. This is a general problem for many supervised clustering methods such as k-mean that user has to provide number of clusters or in this case the resolution which determines the number of clusters. Although there are some indirect ways to assess the clustering quality for example silhouette coefficient which gives you a score between -1 to 1 that tell you how similar your point in each clusters are. The other possibility is that you already expect the number of clusters so you can optimize the resolution based on your previous knowledge. . @falexwolf Out of curiosity, can we integrate such methods like silhouette coefficient inside scanpy? that would be cool!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:441,availability,cluster,clustering,441,"I only can advice you on your second part of questions there is no rule of thumb for that. I also don't know what do you exactly mean by best suggestion resolution and how did you assess that. This is a general problem for many supervised clustering methods such as k-mean that user has to provide number of clusters or in this case the resolution which determines the number of clusters. Although there are some indirect ways to assess the clustering quality for example silhouette coefficient which gives you a score between -1 to 1 that tell you how similar your point in each clusters are. The other possibility is that you already expect the number of clusters so you can optimize the resolution based on your previous knowledge. . @falexwolf Out of curiosity, can we integrate such methods like silhouette coefficient inside scanpy? that would be cool!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:580,availability,cluster,clusters,580,"I only can advice you on your second part of questions there is no rule of thumb for that. I also don't know what do you exactly mean by best suggestion resolution and how did you assess that. This is a general problem for many supervised clustering methods such as k-mean that user has to provide number of clusters or in this case the resolution which determines the number of clusters. Although there are some indirect ways to assess the clustering quality for example silhouette coefficient which gives you a score between -1 to 1 that tell you how similar your point in each clusters are. The other possibility is that you already expect the number of clusters so you can optimize the resolution based on your previous knowledge. . @falexwolf Out of curiosity, can we integrate such methods like silhouette coefficient inside scanpy? that would be cool!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:657,availability,cluster,clusters,657,"I only can advice you on your second part of questions there is no rule of thumb for that. I also don't know what do you exactly mean by best suggestion resolution and how did you assess that. This is a general problem for many supervised clustering methods such as k-mean that user has to provide number of clusters or in this case the resolution which determines the number of clusters. Although there are some indirect ways to assess the clustering quality for example silhouette coefficient which gives you a score between -1 to 1 that tell you how similar your point in each clusters are. The other possibility is that you already expect the number of clusters so you can optimize the resolution based on your previous knowledge. . @falexwolf Out of curiosity, can we integrate such methods like silhouette coefficient inside scanpy? that would be cool!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:239,deployability,cluster,clustering,239,"I only can advice you on your second part of questions there is no rule of thumb for that. I also don't know what do you exactly mean by best suggestion resolution and how did you assess that. This is a general problem for many supervised clustering methods such as k-mean that user has to provide number of clusters or in this case the resolution which determines the number of clusters. Although there are some indirect ways to assess the clustering quality for example silhouette coefficient which gives you a score between -1 to 1 that tell you how similar your point in each clusters are. The other possibility is that you already expect the number of clusters so you can optimize the resolution based on your previous knowledge. . @falexwolf Out of curiosity, can we integrate such methods like silhouette coefficient inside scanpy? that would be cool!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:308,deployability,cluster,clusters,308,"I only can advice you on your second part of questions there is no rule of thumb for that. I also don't know what do you exactly mean by best suggestion resolution and how did you assess that. This is a general problem for many supervised clustering methods such as k-mean that user has to provide number of clusters or in this case the resolution which determines the number of clusters. Although there are some indirect ways to assess the clustering quality for example silhouette coefficient which gives you a score between -1 to 1 that tell you how similar your point in each clusters are. The other possibility is that you already expect the number of clusters so you can optimize the resolution based on your previous knowledge. . @falexwolf Out of curiosity, can we integrate such methods like silhouette coefficient inside scanpy? that would be cool!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:379,deployability,cluster,clusters,379,"I only can advice you on your second part of questions there is no rule of thumb for that. I also don't know what do you exactly mean by best suggestion resolution and how did you assess that. This is a general problem for many supervised clustering methods such as k-mean that user has to provide number of clusters or in this case the resolution which determines the number of clusters. Although there are some indirect ways to assess the clustering quality for example silhouette coefficient which gives you a score between -1 to 1 that tell you how similar your point in each clusters are. The other possibility is that you already expect the number of clusters so you can optimize the resolution based on your previous knowledge. . @falexwolf Out of curiosity, can we integrate such methods like silhouette coefficient inside scanpy? that would be cool!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:441,deployability,cluster,clustering,441,"I only can advice you on your second part of questions there is no rule of thumb for that. I also don't know what do you exactly mean by best suggestion resolution and how did you assess that. This is a general problem for many supervised clustering methods such as k-mean that user has to provide number of clusters or in this case the resolution which determines the number of clusters. Although there are some indirect ways to assess the clustering quality for example silhouette coefficient which gives you a score between -1 to 1 that tell you how similar your point in each clusters are. The other possibility is that you already expect the number of clusters so you can optimize the resolution based on your previous knowledge. . @falexwolf Out of curiosity, can we integrate such methods like silhouette coefficient inside scanpy? that would be cool!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:580,deployability,cluster,clusters,580,"I only can advice you on your second part of questions there is no rule of thumb for that. I also don't know what do you exactly mean by best suggestion resolution and how did you assess that. This is a general problem for many supervised clustering methods such as k-mean that user has to provide number of clusters or in this case the resolution which determines the number of clusters. Although there are some indirect ways to assess the clustering quality for example silhouette coefficient which gives you a score between -1 to 1 that tell you how similar your point in each clusters are. The other possibility is that you already expect the number of clusters so you can optimize the resolution based on your previous knowledge. . @falexwolf Out of curiosity, can we integrate such methods like silhouette coefficient inside scanpy? that would be cool!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:657,deployability,cluster,clusters,657,"I only can advice you on your second part of questions there is no rule of thumb for that. I also don't know what do you exactly mean by best suggestion resolution and how did you assess that. This is a general problem for many supervised clustering methods such as k-mean that user has to provide number of clusters or in this case the resolution which determines the number of clusters. Although there are some indirect ways to assess the clustering quality for example silhouette coefficient which gives you a score between -1 to 1 that tell you how similar your point in each clusters are. The other possibility is that you already expect the number of clusters so you can optimize the resolution based on your previous knowledge. . @falexwolf Out of curiosity, can we integrate such methods like silhouette coefficient inside scanpy? that would be cool!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:773,deployability,integr,integrate,773,"I only can advice you on your second part of questions there is no rule of thumb for that. I also don't know what do you exactly mean by best suggestion resolution and how did you assess that. This is a general problem for many supervised clustering methods such as k-mean that user has to provide number of clusters or in this case the resolution which determines the number of clusters. Although there are some indirect ways to assess the clustering quality for example silhouette coefficient which gives you a score between -1 to 1 that tell you how similar your point in each clusters are. The other possibility is that you already expect the number of clusters so you can optimize the resolution based on your previous knowledge. . @falexwolf Out of curiosity, can we integrate such methods like silhouette coefficient inside scanpy? that would be cool!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:677,energy efficiency,optim,optimize,677,"I only can advice you on your second part of questions there is no rule of thumb for that. I also don't know what do you exactly mean by best suggestion resolution and how did you assess that. This is a general problem for many supervised clustering methods such as k-mean that user has to provide number of clusters or in this case the resolution which determines the number of clusters. Although there are some indirect ways to assess the clustering quality for example silhouette coefficient which gives you a score between -1 to 1 that tell you how similar your point in each clusters are. The other possibility is that you already expect the number of clusters so you can optimize the resolution based on your previous knowledge. . @falexwolf Out of curiosity, can we integrate such methods like silhouette coefficient inside scanpy? that would be cool!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:853,energy efficiency,cool,cool,853,"I only can advice you on your second part of questions there is no rule of thumb for that. I also don't know what do you exactly mean by best suggestion resolution and how did you assess that. This is a general problem for many supervised clustering methods such as k-mean that user has to provide number of clusters or in this case the resolution which determines the number of clusters. Although there are some indirect ways to assess the clustering quality for example silhouette coefficient which gives you a score between -1 to 1 that tell you how similar your point in each clusters are. The other possibility is that you already expect the number of clusters so you can optimize the resolution based on your previous knowledge. . @falexwolf Out of curiosity, can we integrate such methods like silhouette coefficient inside scanpy? that would be cool!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:773,integrability,integr,integrate,773,"I only can advice you on your second part of questions there is no rule of thumb for that. I also don't know what do you exactly mean by best suggestion resolution and how did you assess that. This is a general problem for many supervised clustering methods such as k-mean that user has to provide number of clusters or in this case the resolution which determines the number of clusters. Although there are some indirect ways to assess the clustering quality for example silhouette coefficient which gives you a score between -1 to 1 that tell you how similar your point in each clusters are. The other possibility is that you already expect the number of clusters so you can optimize the resolution based on your previous knowledge. . @falexwolf Out of curiosity, can we integrate such methods like silhouette coefficient inside scanpy? that would be cool!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:773,interoperability,integr,integrate,773,"I only can advice you on your second part of questions there is no rule of thumb for that. I also don't know what do you exactly mean by best suggestion resolution and how did you assess that. This is a general problem for many supervised clustering methods such as k-mean that user has to provide number of clusters or in this case the resolution which determines the number of clusters. Although there are some indirect ways to assess the clustering quality for example silhouette coefficient which gives you a score between -1 to 1 that tell you how similar your point in each clusters are. The other possibility is that you already expect the number of clusters so you can optimize the resolution based on your previous knowledge. . @falexwolf Out of curiosity, can we integrate such methods like silhouette coefficient inside scanpy? that would be cool!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:773,modifiability,integr,integrate,773,"I only can advice you on your second part of questions there is no rule of thumb for that. I also don't know what do you exactly mean by best suggestion resolution and how did you assess that. This is a general problem for many supervised clustering methods such as k-mean that user has to provide number of clusters or in this case the resolution which determines the number of clusters. Although there are some indirect ways to assess the clustering quality for example silhouette coefficient which gives you a score between -1 to 1 that tell you how similar your point in each clusters are. The other possibility is that you already expect the number of clusters so you can optimize the resolution based on your previous knowledge. . @falexwolf Out of curiosity, can we integrate such methods like silhouette coefficient inside scanpy? that would be cool!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:677,performance,optimiz,optimize,677,"I only can advice you on your second part of questions there is no rule of thumb for that. I also don't know what do you exactly mean by best suggestion resolution and how did you assess that. This is a general problem for many supervised clustering methods such as k-mean that user has to provide number of clusters or in this case the resolution which determines the number of clusters. Although there are some indirect ways to assess the clustering quality for example silhouette coefficient which gives you a score between -1 to 1 that tell you how similar your point in each clusters are. The other possibility is that you already expect the number of clusters so you can optimize the resolution based on your previous knowledge. . @falexwolf Out of curiosity, can we integrate such methods like silhouette coefficient inside scanpy? that would be cool!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:773,reliability,integr,integrate,773,"I only can advice you on your second part of questions there is no rule of thumb for that. I also don't know what do you exactly mean by best suggestion resolution and how did you assess that. This is a general problem for many supervised clustering methods such as k-mean that user has to provide number of clusters or in this case the resolution which determines the number of clusters. Although there are some indirect ways to assess the clustering quality for example silhouette coefficient which gives you a score between -1 to 1 that tell you how similar your point in each clusters are. The other possibility is that you already expect the number of clusters so you can optimize the resolution based on your previous knowledge. . @falexwolf Out of curiosity, can we integrate such methods like silhouette coefficient inside scanpy? that would be cool!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:180,security,assess,assess,180,"I only can advice you on your second part of questions there is no rule of thumb for that. I also don't know what do you exactly mean by best suggestion resolution and how did you assess that. This is a general problem for many supervised clustering methods such as k-mean that user has to provide number of clusters or in this case the resolution which determines the number of clusters. Although there are some indirect ways to assess the clustering quality for example silhouette coefficient which gives you a score between -1 to 1 that tell you how similar your point in each clusters are. The other possibility is that you already expect the number of clusters so you can optimize the resolution based on your previous knowledge. . @falexwolf Out of curiosity, can we integrate such methods like silhouette coefficient inside scanpy? that would be cool!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:430,security,assess,assess,430,"I only can advice you on your second part of questions there is no rule of thumb for that. I also don't know what do you exactly mean by best suggestion resolution and how did you assess that. This is a general problem for many supervised clustering methods such as k-mean that user has to provide number of clusters or in this case the resolution which determines the number of clusters. Although there are some indirect ways to assess the clustering quality for example silhouette coefficient which gives you a score between -1 to 1 that tell you how similar your point in each clusters are. The other possibility is that you already expect the number of clusters so you can optimize the resolution based on your previous knowledge. . @falexwolf Out of curiosity, can we integrate such methods like silhouette coefficient inside scanpy? that would be cool!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:773,security,integr,integrate,773,"I only can advice you on your second part of questions there is no rule of thumb for that. I also don't know what do you exactly mean by best suggestion resolution and how did you assess that. This is a general problem for many supervised clustering methods such as k-mean that user has to provide number of clusters or in this case the resolution which determines the number of clusters. Although there are some indirect ways to assess the clustering quality for example silhouette coefficient which gives you a score between -1 to 1 that tell you how similar your point in each clusters are. The other possibility is that you already expect the number of clusters so you can optimize the resolution based on your previous knowledge. . @falexwolf Out of curiosity, can we integrate such methods like silhouette coefficient inside scanpy? that would be cool!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:773,testability,integr,integrate,773,"I only can advice you on your second part of questions there is no rule of thumb for that. I also don't know what do you exactly mean by best suggestion resolution and how did you assess that. This is a general problem for many supervised clustering methods such as k-mean that user has to provide number of clusters or in this case the resolution which determines the number of clusters. Although there are some indirect ways to assess the clustering quality for example silhouette coefficient which gives you a score between -1 to 1 that tell you how similar your point in each clusters are. The other possibility is that you already expect the number of clusters so you can optimize the resolution based on your previous knowledge. . @falexwolf Out of curiosity, can we integrate such methods like silhouette coefficient inside scanpy? that would be cool!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:278,usability,user,user,278,"I only can advice you on your second part of questions there is no rule of thumb for that. I also don't know what do you exactly mean by best suggestion resolution and how did you assess that. This is a general problem for many supervised clustering methods such as k-mean that user has to provide number of clusters or in this case the resolution which determines the number of clusters. Although there are some indirect ways to assess the clustering quality for example silhouette coefficient which gives you a score between -1 to 1 that tell you how similar your point in each clusters are. The other possibility is that you already expect the number of clusters so you can optimize the resolution based on your previous knowledge. . @falexwolf Out of curiosity, can we integrate such methods like silhouette coefficient inside scanpy? that would be cool!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:131,availability,cluster,clusters,131,"Thanks @bioguy2018 for your kind reply. Actually I was confused as for Pbmc3k, Scanpy and previous version of Seurat says it has 8 clusters, but in new version of Seurat clusters are 9. I think it's totally based on biological knowledge rather than optimizing paramters, like resolution. It will be great to add something like silhouette coefficient in scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:170,availability,cluster,clusters,170,"Thanks @bioguy2018 for your kind reply. Actually I was confused as for Pbmc3k, Scanpy and previous version of Seurat says it has 8 clusters, but in new version of Seurat clusters are 9. I think it's totally based on biological knowledge rather than optimizing paramters, like resolution. It will be great to add something like silhouette coefficient in scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:99,deployability,version,version,99,"Thanks @bioguy2018 for your kind reply. Actually I was confused as for Pbmc3k, Scanpy and previous version of Seurat says it has 8 clusters, but in new version of Seurat clusters are 9. I think it's totally based on biological knowledge rather than optimizing paramters, like resolution. It will be great to add something like silhouette coefficient in scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:131,deployability,cluster,clusters,131,"Thanks @bioguy2018 for your kind reply. Actually I was confused as for Pbmc3k, Scanpy and previous version of Seurat says it has 8 clusters, but in new version of Seurat clusters are 9. I think it's totally based on biological knowledge rather than optimizing paramters, like resolution. It will be great to add something like silhouette coefficient in scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:152,deployability,version,version,152,"Thanks @bioguy2018 for your kind reply. Actually I was confused as for Pbmc3k, Scanpy and previous version of Seurat says it has 8 clusters, but in new version of Seurat clusters are 9. I think it's totally based on biological knowledge rather than optimizing paramters, like resolution. It will be great to add something like silhouette coefficient in scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:170,deployability,cluster,clusters,170,"Thanks @bioguy2018 for your kind reply. Actually I was confused as for Pbmc3k, Scanpy and previous version of Seurat says it has 8 clusters, but in new version of Seurat clusters are 9. I think it's totally based on biological knowledge rather than optimizing paramters, like resolution. It will be great to add something like silhouette coefficient in scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:249,energy efficiency,optim,optimizing,249,"Thanks @bioguy2018 for your kind reply. Actually I was confused as for Pbmc3k, Scanpy and previous version of Seurat says it has 8 clusters, but in new version of Seurat clusters are 9. I think it's totally based on biological knowledge rather than optimizing paramters, like resolution. It will be great to add something like silhouette coefficient in scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:99,integrability,version,version,99,"Thanks @bioguy2018 for your kind reply. Actually I was confused as for Pbmc3k, Scanpy and previous version of Seurat says it has 8 clusters, but in new version of Seurat clusters are 9. I think it's totally based on biological knowledge rather than optimizing paramters, like resolution. It will be great to add something like silhouette coefficient in scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:152,integrability,version,version,152,"Thanks @bioguy2018 for your kind reply. Actually I was confused as for Pbmc3k, Scanpy and previous version of Seurat says it has 8 clusters, but in new version of Seurat clusters are 9. I think it's totally based on biological knowledge rather than optimizing paramters, like resolution. It will be great to add something like silhouette coefficient in scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:99,modifiability,version,version,99,"Thanks @bioguy2018 for your kind reply. Actually I was confused as for Pbmc3k, Scanpy and previous version of Seurat says it has 8 clusters, but in new version of Seurat clusters are 9. I think it's totally based on biological knowledge rather than optimizing paramters, like resolution. It will be great to add something like silhouette coefficient in scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:152,modifiability,version,version,152,"Thanks @bioguy2018 for your kind reply. Actually I was confused as for Pbmc3k, Scanpy and previous version of Seurat says it has 8 clusters, but in new version of Seurat clusters are 9. I think it's totally based on biological knowledge rather than optimizing paramters, like resolution. It will be great to add something like silhouette coefficient in scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:249,performance,optimiz,optimizing,249,"Thanks @bioguy2018 for your kind reply. Actually I was confused as for Pbmc3k, Scanpy and previous version of Seurat says it has 8 clusters, but in new version of Seurat clusters are 9. I think it's totally based on biological knowledge rather than optimizing paramters, like resolution. It will be great to add something like silhouette coefficient in scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:116,availability,cluster,clusters,116,"You can easily access the silhouette coefficient via scikit-learn. . I would be hesitant to base optimal numbers of clusters on the silhouette coefficient though. The number of clusters is typically dependent on the biological question of interest. There's not really a scale at which all biological questions can be answered. Therefore you have a resolution parameter to check multiple resolutions. For example, T cells could be taken as one cluster or sub-clustered into CD4+ and CD8+ (which is typically done). Here a problem with the silhouette coefficient also shows: often you have one big cluster of T-cells which reluctantly cluster into the CD4+ and CD8+ subtypes (early 10X datasets show this nicely). This will have a lower silhouette coefficient, but it is probably more informative for many people.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:177,availability,cluster,clusters,177,"You can easily access the silhouette coefficient via scikit-learn. . I would be hesitant to base optimal numbers of clusters on the silhouette coefficient though. The number of clusters is typically dependent on the biological question of interest. There's not really a scale at which all biological questions can be answered. Therefore you have a resolution parameter to check multiple resolutions. For example, T cells could be taken as one cluster or sub-clustered into CD4+ and CD8+ (which is typically done). Here a problem with the silhouette coefficient also shows: often you have one big cluster of T-cells which reluctantly cluster into the CD4+ and CD8+ subtypes (early 10X datasets show this nicely). This will have a lower silhouette coefficient, but it is probably more informative for many people.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:443,availability,cluster,cluster,443,"You can easily access the silhouette coefficient via scikit-learn. . I would be hesitant to base optimal numbers of clusters on the silhouette coefficient though. The number of clusters is typically dependent on the biological question of interest. There's not really a scale at which all biological questions can be answered. Therefore you have a resolution parameter to check multiple resolutions. For example, T cells could be taken as one cluster or sub-clustered into CD4+ and CD8+ (which is typically done). Here a problem with the silhouette coefficient also shows: often you have one big cluster of T-cells which reluctantly cluster into the CD4+ and CD8+ subtypes (early 10X datasets show this nicely). This will have a lower silhouette coefficient, but it is probably more informative for many people.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:458,availability,cluster,clustered,458,"You can easily access the silhouette coefficient via scikit-learn. . I would be hesitant to base optimal numbers of clusters on the silhouette coefficient though. The number of clusters is typically dependent on the biological question of interest. There's not really a scale at which all biological questions can be answered. Therefore you have a resolution parameter to check multiple resolutions. For example, T cells could be taken as one cluster or sub-clustered into CD4+ and CD8+ (which is typically done). Here a problem with the silhouette coefficient also shows: often you have one big cluster of T-cells which reluctantly cluster into the CD4+ and CD8+ subtypes (early 10X datasets show this nicely). This will have a lower silhouette coefficient, but it is probably more informative for many people.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:596,availability,cluster,cluster,596,"You can easily access the silhouette coefficient via scikit-learn. . I would be hesitant to base optimal numbers of clusters on the silhouette coefficient though. The number of clusters is typically dependent on the biological question of interest. There's not really a scale at which all biological questions can be answered. Therefore you have a resolution parameter to check multiple resolutions. For example, T cells could be taken as one cluster or sub-clustered into CD4+ and CD8+ (which is typically done). Here a problem with the silhouette coefficient also shows: often you have one big cluster of T-cells which reluctantly cluster into the CD4+ and CD8+ subtypes (early 10X datasets show this nicely). This will have a lower silhouette coefficient, but it is probably more informative for many people.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:633,availability,cluster,cluster,633,"You can easily access the silhouette coefficient via scikit-learn. . I would be hesitant to base optimal numbers of clusters on the silhouette coefficient though. The number of clusters is typically dependent on the biological question of interest. There's not really a scale at which all biological questions can be answered. Therefore you have a resolution parameter to check multiple resolutions. For example, T cells could be taken as one cluster or sub-clustered into CD4+ and CD8+ (which is typically done). Here a problem with the silhouette coefficient also shows: often you have one big cluster of T-cells which reluctantly cluster into the CD4+ and CD8+ subtypes (early 10X datasets show this nicely). This will have a lower silhouette coefficient, but it is probably more informative for many people.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:116,deployability,cluster,clusters,116,"You can easily access the silhouette coefficient via scikit-learn. . I would be hesitant to base optimal numbers of clusters on the silhouette coefficient though. The number of clusters is typically dependent on the biological question of interest. There's not really a scale at which all biological questions can be answered. Therefore you have a resolution parameter to check multiple resolutions. For example, T cells could be taken as one cluster or sub-clustered into CD4+ and CD8+ (which is typically done). Here a problem with the silhouette coefficient also shows: often you have one big cluster of T-cells which reluctantly cluster into the CD4+ and CD8+ subtypes (early 10X datasets show this nicely). This will have a lower silhouette coefficient, but it is probably more informative for many people.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:177,deployability,cluster,clusters,177,"You can easily access the silhouette coefficient via scikit-learn. . I would be hesitant to base optimal numbers of clusters on the silhouette coefficient though. The number of clusters is typically dependent on the biological question of interest. There's not really a scale at which all biological questions can be answered. Therefore you have a resolution parameter to check multiple resolutions. For example, T cells could be taken as one cluster or sub-clustered into CD4+ and CD8+ (which is typically done). Here a problem with the silhouette coefficient also shows: often you have one big cluster of T-cells which reluctantly cluster into the CD4+ and CD8+ subtypes (early 10X datasets show this nicely). This will have a lower silhouette coefficient, but it is probably more informative for many people.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:199,deployability,depend,dependent,199,"You can easily access the silhouette coefficient via scikit-learn. . I would be hesitant to base optimal numbers of clusters on the silhouette coefficient though. The number of clusters is typically dependent on the biological question of interest. There's not really a scale at which all biological questions can be answered. Therefore you have a resolution parameter to check multiple resolutions. For example, T cells could be taken as one cluster or sub-clustered into CD4+ and CD8+ (which is typically done). Here a problem with the silhouette coefficient also shows: often you have one big cluster of T-cells which reluctantly cluster into the CD4+ and CD8+ subtypes (early 10X datasets show this nicely). This will have a lower silhouette coefficient, but it is probably more informative for many people.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:270,deployability,scale,scale,270,"You can easily access the silhouette coefficient via scikit-learn. . I would be hesitant to base optimal numbers of clusters on the silhouette coefficient though. The number of clusters is typically dependent on the biological question of interest. There's not really a scale at which all biological questions can be answered. Therefore you have a resolution parameter to check multiple resolutions. For example, T cells could be taken as one cluster or sub-clustered into CD4+ and CD8+ (which is typically done). Here a problem with the silhouette coefficient also shows: often you have one big cluster of T-cells which reluctantly cluster into the CD4+ and CD8+ subtypes (early 10X datasets show this nicely). This will have a lower silhouette coefficient, but it is probably more informative for many people.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:443,deployability,cluster,cluster,443,"You can easily access the silhouette coefficient via scikit-learn. . I would be hesitant to base optimal numbers of clusters on the silhouette coefficient though. The number of clusters is typically dependent on the biological question of interest. There's not really a scale at which all biological questions can be answered. Therefore you have a resolution parameter to check multiple resolutions. For example, T cells could be taken as one cluster or sub-clustered into CD4+ and CD8+ (which is typically done). Here a problem with the silhouette coefficient also shows: often you have one big cluster of T-cells which reluctantly cluster into the CD4+ and CD8+ subtypes (early 10X datasets show this nicely). This will have a lower silhouette coefficient, but it is probably more informative for many people.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:458,deployability,cluster,clustered,458,"You can easily access the silhouette coefficient via scikit-learn. . I would be hesitant to base optimal numbers of clusters on the silhouette coefficient though. The number of clusters is typically dependent on the biological question of interest. There's not really a scale at which all biological questions can be answered. Therefore you have a resolution parameter to check multiple resolutions. For example, T cells could be taken as one cluster or sub-clustered into CD4+ and CD8+ (which is typically done). Here a problem with the silhouette coefficient also shows: often you have one big cluster of T-cells which reluctantly cluster into the CD4+ and CD8+ subtypes (early 10X datasets show this nicely). This will have a lower silhouette coefficient, but it is probably more informative for many people.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:596,deployability,cluster,cluster,596,"You can easily access the silhouette coefficient via scikit-learn. . I would be hesitant to base optimal numbers of clusters on the silhouette coefficient though. The number of clusters is typically dependent on the biological question of interest. There's not really a scale at which all biological questions can be answered. Therefore you have a resolution parameter to check multiple resolutions. For example, T cells could be taken as one cluster or sub-clustered into CD4+ and CD8+ (which is typically done). Here a problem with the silhouette coefficient also shows: often you have one big cluster of T-cells which reluctantly cluster into the CD4+ and CD8+ subtypes (early 10X datasets show this nicely). This will have a lower silhouette coefficient, but it is probably more informative for many people.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:633,deployability,cluster,cluster,633,"You can easily access the silhouette coefficient via scikit-learn. . I would be hesitant to base optimal numbers of clusters on the silhouette coefficient though. The number of clusters is typically dependent on the biological question of interest. There's not really a scale at which all biological questions can be answered. Therefore you have a resolution parameter to check multiple resolutions. For example, T cells could be taken as one cluster or sub-clustered into CD4+ and CD8+ (which is typically done). Here a problem with the silhouette coefficient also shows: often you have one big cluster of T-cells which reluctantly cluster into the CD4+ and CD8+ subtypes (early 10X datasets show this nicely). This will have a lower silhouette coefficient, but it is probably more informative for many people.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:97,energy efficiency,optim,optimal,97,"You can easily access the silhouette coefficient via scikit-learn. . I would be hesitant to base optimal numbers of clusters on the silhouette coefficient though. The number of clusters is typically dependent on the biological question of interest. There's not really a scale at which all biological questions can be answered. Therefore you have a resolution parameter to check multiple resolutions. For example, T cells could be taken as one cluster or sub-clustered into CD4+ and CD8+ (which is typically done). Here a problem with the silhouette coefficient also shows: often you have one big cluster of T-cells which reluctantly cluster into the CD4+ and CD8+ subtypes (early 10X datasets show this nicely). This will have a lower silhouette coefficient, but it is probably more informative for many people.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:270,energy efficiency,scale,scale,270,"You can easily access the silhouette coefficient via scikit-learn. . I would be hesitant to base optimal numbers of clusters on the silhouette coefficient though. The number of clusters is typically dependent on the biological question of interest. There's not really a scale at which all biological questions can be answered. Therefore you have a resolution parameter to check multiple resolutions. For example, T cells could be taken as one cluster or sub-clustered into CD4+ and CD8+ (which is typically done). Here a problem with the silhouette coefficient also shows: often you have one big cluster of T-cells which reluctantly cluster into the CD4+ and CD8+ subtypes (early 10X datasets show this nicely). This will have a lower silhouette coefficient, but it is probably more informative for many people.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:199,integrability,depend,dependent,199,"You can easily access the silhouette coefficient via scikit-learn. . I would be hesitant to base optimal numbers of clusters on the silhouette coefficient though. The number of clusters is typically dependent on the biological question of interest. There's not really a scale at which all biological questions can be answered. Therefore you have a resolution parameter to check multiple resolutions. For example, T cells could be taken as one cluster or sub-clustered into CD4+ and CD8+ (which is typically done). Here a problem with the silhouette coefficient also shows: often you have one big cluster of T-cells which reluctantly cluster into the CD4+ and CD8+ subtypes (early 10X datasets show this nicely). This will have a lower silhouette coefficient, but it is probably more informative for many people.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:454,integrability,sub,sub-clustered,454,"You can easily access the silhouette coefficient via scikit-learn. . I would be hesitant to base optimal numbers of clusters on the silhouette coefficient though. The number of clusters is typically dependent on the biological question of interest. There's not really a scale at which all biological questions can be answered. Therefore you have a resolution parameter to check multiple resolutions. For example, T cells could be taken as one cluster or sub-clustered into CD4+ and CD8+ (which is typically done). Here a problem with the silhouette coefficient also shows: often you have one big cluster of T-cells which reluctantly cluster into the CD4+ and CD8+ subtypes (early 10X datasets show this nicely). This will have a lower silhouette coefficient, but it is probably more informative for many people.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:664,integrability,sub,subtypes,664,"You can easily access the silhouette coefficient via scikit-learn. . I would be hesitant to base optimal numbers of clusters on the silhouette coefficient though. The number of clusters is typically dependent on the biological question of interest. There's not really a scale at which all biological questions can be answered. Therefore you have a resolution parameter to check multiple resolutions. For example, T cells could be taken as one cluster or sub-clustered into CD4+ and CD8+ (which is typically done). Here a problem with the silhouette coefficient also shows: often you have one big cluster of T-cells which reluctantly cluster into the CD4+ and CD8+ subtypes (early 10X datasets show this nicely). This will have a lower silhouette coefficient, but it is probably more informative for many people.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:199,modifiability,depend,dependent,199,"You can easily access the silhouette coefficient via scikit-learn. . I would be hesitant to base optimal numbers of clusters on the silhouette coefficient though. The number of clusters is typically dependent on the biological question of interest. There's not really a scale at which all biological questions can be answered. Therefore you have a resolution parameter to check multiple resolutions. For example, T cells could be taken as one cluster or sub-clustered into CD4+ and CD8+ (which is typically done). Here a problem with the silhouette coefficient also shows: often you have one big cluster of T-cells which reluctantly cluster into the CD4+ and CD8+ subtypes (early 10X datasets show this nicely). This will have a lower silhouette coefficient, but it is probably more informative for many people.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:270,modifiability,scal,scale,270,"You can easily access the silhouette coefficient via scikit-learn. . I would be hesitant to base optimal numbers of clusters on the silhouette coefficient though. The number of clusters is typically dependent on the biological question of interest. There's not really a scale at which all biological questions can be answered. Therefore you have a resolution parameter to check multiple resolutions. For example, T cells could be taken as one cluster or sub-clustered into CD4+ and CD8+ (which is typically done). Here a problem with the silhouette coefficient also shows: often you have one big cluster of T-cells which reluctantly cluster into the CD4+ and CD8+ subtypes (early 10X datasets show this nicely). This will have a lower silhouette coefficient, but it is probably more informative for many people.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:359,modifiability,paramet,parameter,359,"You can easily access the silhouette coefficient via scikit-learn. . I would be hesitant to base optimal numbers of clusters on the silhouette coefficient though. The number of clusters is typically dependent on the biological question of interest. There's not really a scale at which all biological questions can be answered. Therefore you have a resolution parameter to check multiple resolutions. For example, T cells could be taken as one cluster or sub-clustered into CD4+ and CD8+ (which is typically done). Here a problem with the silhouette coefficient also shows: often you have one big cluster of T-cells which reluctantly cluster into the CD4+ and CD8+ subtypes (early 10X datasets show this nicely). This will have a lower silhouette coefficient, but it is probably more informative for many people.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:270,performance,scale,scale,270,"You can easily access the silhouette coefficient via scikit-learn. . I would be hesitant to base optimal numbers of clusters on the silhouette coefficient though. The number of clusters is typically dependent on the biological question of interest. There's not really a scale at which all biological questions can be answered. Therefore you have a resolution parameter to check multiple resolutions. For example, T cells could be taken as one cluster or sub-clustered into CD4+ and CD8+ (which is typically done). Here a problem with the silhouette coefficient also shows: often you have one big cluster of T-cells which reluctantly cluster into the CD4+ and CD8+ subtypes (early 10X datasets show this nicely). This will have a lower silhouette coefficient, but it is probably more informative for many people.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:199,safety,depend,dependent,199,"You can easily access the silhouette coefficient via scikit-learn. . I would be hesitant to base optimal numbers of clusters on the silhouette coefficient though. The number of clusters is typically dependent on the biological question of interest. There's not really a scale at which all biological questions can be answered. Therefore you have a resolution parameter to check multiple resolutions. For example, T cells could be taken as one cluster or sub-clustered into CD4+ and CD8+ (which is typically done). Here a problem with the silhouette coefficient also shows: often you have one big cluster of T-cells which reluctantly cluster into the CD4+ and CD8+ subtypes (early 10X datasets show this nicely). This will have a lower silhouette coefficient, but it is probably more informative for many people.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:15,security,access,access,15,"You can easily access the silhouette coefficient via scikit-learn. . I would be hesitant to base optimal numbers of clusters on the silhouette coefficient though. The number of clusters is typically dependent on the biological question of interest. There's not really a scale at which all biological questions can be answered. Therefore you have a resolution parameter to check multiple resolutions. For example, T cells could be taken as one cluster or sub-clustered into CD4+ and CD8+ (which is typically done). Here a problem with the silhouette coefficient also shows: often you have one big cluster of T-cells which reluctantly cluster into the CD4+ and CD8+ subtypes (early 10X datasets show this nicely). This will have a lower silhouette coefficient, but it is probably more informative for many people.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:199,testability,depend,dependent,199,"You can easily access the silhouette coefficient via scikit-learn. . I would be hesitant to base optimal numbers of clusters on the silhouette coefficient though. The number of clusters is typically dependent on the biological question of interest. There's not really a scale at which all biological questions can be answered. Therefore you have a resolution parameter to check multiple resolutions. For example, T cells could be taken as one cluster or sub-clustered into CD4+ and CD8+ (which is typically done). Here a problem with the silhouette coefficient also shows: often you have one big cluster of T-cells which reluctantly cluster into the CD4+ and CD8+ subtypes (early 10X datasets show this nicely). This will have a lower silhouette coefficient, but it is probably more informative for many people.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:60,usability,learn,learn,60,"You can easily access the silhouette coefficient via scikit-learn. . I would be hesitant to base optimal numbers of clusters on the silhouette coefficient though. The number of clusters is typically dependent on the biological question of interest. There's not really a scale at which all biological questions can be answered. Therefore you have a resolution parameter to check multiple resolutions. For example, T cells could be taken as one cluster or sub-clustered into CD4+ and CD8+ (which is typically done). Here a problem with the silhouette coefficient also shows: often you have one big cluster of T-cells which reluctantly cluster into the CD4+ and CD8+ subtypes (early 10X datasets show this nicely). This will have a lower silhouette coefficient, but it is probably more informative for many people.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:301,integrability,sub,sub-types,301,"@LuckyMD I am wondering have you tried it before? or did someone try it with the pbmc data with regards to the t-cells that you mentioned? I agree with you that it can not be an option for answering biological questions but in the case that there is no clear biological knowledge like looking for new sub-types or so it can be an option to get some idea (mathematically). . p.s. it just crossed my mind so I am pushing a technical question also here ðŸ˜„ Since we would need to calculate the distance matrix, would the input for the function be adata.X ? should it be the raw file?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:516,safety,input,input,516,"@LuckyMD I am wondering have you tried it before? or did someone try it with the pbmc data with regards to the t-cells that you mentioned? I agree with you that it can not be an option for answering biological questions but in the case that there is no clear biological knowledge like looking for new sub-types or so it can be an option to get some idea (mathematically). . p.s. it just crossed my mind so I am pushing a technical question also here ðŸ˜„ Since we would need to calculate the distance matrix, would the input for the function be adata.X ? should it be the raw file?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:253,usability,clear,clear,253,"@LuckyMD I am wondering have you tried it before? or did someone try it with the pbmc data with regards to the t-cells that you mentioned? I agree with you that it can not be an option for answering biological questions but in the case that there is no clear biological knowledge like looking for new sub-types or so it can be an option to get some idea (mathematically). . p.s. it just crossed my mind so I am pushing a technical question also here ðŸ˜„ Since we would need to calculate the distance matrix, would the input for the function be adata.X ? should it be the raw file?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:516,usability,input,input,516,"@LuckyMD I am wondering have you tried it before? or did someone try it with the pbmc data with regards to the t-cells that you mentioned? I agree with you that it can not be an option for answering biological questions but in the case that there is no clear biological knowledge like looking for new sub-types or so it can be an option to get some idea (mathematically). . p.s. it just crossed my mind so I am pushing a technical question also here ðŸ˜„ Since we would need to calculate the distance matrix, would the input for the function be adata.X ? should it be the raw file?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:194,availability,cluster,clusters,194,"Some notes/observations from my side towards choosing the proper resolution: . - the Leiden algorithm depends on a random seed. With a different random seed, you might get a different number of clusters with the same resolution. - a sensible resolution depends on the input data: when clustering on data processed with `sc.tl.diffmap` a much lower resolution will give the same number of clusters than without. . - I performed a hyperparameter search for the resolution (steps of 0.005) on a large dataset of CD8+ T cells. I observed that at certain resolution ranges, the number of clusters is stable. In my case, I was looking for subtypes of CD8+ T cells and hypothesized that at ~0.1 and ~0.3 I would find something biologically meaningful. Would be interesting to re-do that on the PBMC dataset. I would expect a plateau at a resolution that recovers the well-known cell types CD8+, CD4+, etc. . ![2019-06-03_09:53:34_911x604](https://user-images.githubusercontent.com/7051479/58785259-7ea10e80-85e5-11e9-8e0b-789e2e74754a.png). **Fig:** hyperparameter search for resolution in steps of 0.005. The graph shows the resolution vs. detected number of Leiden-clusters.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:285,availability,cluster,clustering,285,"Some notes/observations from my side towards choosing the proper resolution: . - the Leiden algorithm depends on a random seed. With a different random seed, you might get a different number of clusters with the same resolution. - a sensible resolution depends on the input data: when clustering on data processed with `sc.tl.diffmap` a much lower resolution will give the same number of clusters than without. . - I performed a hyperparameter search for the resolution (steps of 0.005) on a large dataset of CD8+ T cells. I observed that at certain resolution ranges, the number of clusters is stable. In my case, I was looking for subtypes of CD8+ T cells and hypothesized that at ~0.1 and ~0.3 I would find something biologically meaningful. Would be interesting to re-do that on the PBMC dataset. I would expect a plateau at a resolution that recovers the well-known cell types CD8+, CD4+, etc. . ![2019-06-03_09:53:34_911x604](https://user-images.githubusercontent.com/7051479/58785259-7ea10e80-85e5-11e9-8e0b-789e2e74754a.png). **Fig:** hyperparameter search for resolution in steps of 0.005. The graph shows the resolution vs. detected number of Leiden-clusters.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:388,availability,cluster,clusters,388,"Some notes/observations from my side towards choosing the proper resolution: . - the Leiden algorithm depends on a random seed. With a different random seed, you might get a different number of clusters with the same resolution. - a sensible resolution depends on the input data: when clustering on data processed with `sc.tl.diffmap` a much lower resolution will give the same number of clusters than without. . - I performed a hyperparameter search for the resolution (steps of 0.005) on a large dataset of CD8+ T cells. I observed that at certain resolution ranges, the number of clusters is stable. In my case, I was looking for subtypes of CD8+ T cells and hypothesized that at ~0.1 and ~0.3 I would find something biologically meaningful. Would be interesting to re-do that on the PBMC dataset. I would expect a plateau at a resolution that recovers the well-known cell types CD8+, CD4+, etc. . ![2019-06-03_09:53:34_911x604](https://user-images.githubusercontent.com/7051479/58785259-7ea10e80-85e5-11e9-8e0b-789e2e74754a.png). **Fig:** hyperparameter search for resolution in steps of 0.005. The graph shows the resolution vs. detected number of Leiden-clusters.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:583,availability,cluster,clusters,583,"Some notes/observations from my side towards choosing the proper resolution: . - the Leiden algorithm depends on a random seed. With a different random seed, you might get a different number of clusters with the same resolution. - a sensible resolution depends on the input data: when clustering on data processed with `sc.tl.diffmap` a much lower resolution will give the same number of clusters than without. . - I performed a hyperparameter search for the resolution (steps of 0.005) on a large dataset of CD8+ T cells. I observed that at certain resolution ranges, the number of clusters is stable. In my case, I was looking for subtypes of CD8+ T cells and hypothesized that at ~0.1 and ~0.3 I would find something biologically meaningful. Would be interesting to re-do that on the PBMC dataset. I would expect a plateau at a resolution that recovers the well-known cell types CD8+, CD4+, etc. . ![2019-06-03_09:53:34_911x604](https://user-images.githubusercontent.com/7051479/58785259-7ea10e80-85e5-11e9-8e0b-789e2e74754a.png). **Fig:** hyperparameter search for resolution in steps of 0.005. The graph shows the resolution vs. detected number of Leiden-clusters.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:847,availability,recov,recovers,847,"Some notes/observations from my side towards choosing the proper resolution: . - the Leiden algorithm depends on a random seed. With a different random seed, you might get a different number of clusters with the same resolution. - a sensible resolution depends on the input data: when clustering on data processed with `sc.tl.diffmap` a much lower resolution will give the same number of clusters than without. . - I performed a hyperparameter search for the resolution (steps of 0.005) on a large dataset of CD8+ T cells. I observed that at certain resolution ranges, the number of clusters is stable. In my case, I was looking for subtypes of CD8+ T cells and hypothesized that at ~0.1 and ~0.3 I would find something biologically meaningful. Would be interesting to re-do that on the PBMC dataset. I would expect a plateau at a resolution that recovers the well-known cell types CD8+, CD4+, etc. . ![2019-06-03_09:53:34_911x604](https://user-images.githubusercontent.com/7051479/58785259-7ea10e80-85e5-11e9-8e0b-789e2e74754a.png). **Fig:** hyperparameter search for resolution in steps of 0.005. The graph shows the resolution vs. detected number of Leiden-clusters.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
https://github.com/scverse/scanpy/issues/670:1160,availability,cluster,clusters,1160,"Some notes/observations from my side towards choosing the proper resolution: . - the Leiden algorithm depends on a random seed. With a different random seed, you might get a different number of clusters with the same resolution. - a sensible resolution depends on the input data: when clustering on data processed with `sc.tl.diffmap` a much lower resolution will give the same number of clusters than without. . - I performed a hyperparameter search for the resolution (steps of 0.005) on a large dataset of CD8+ T cells. I observed that at certain resolution ranges, the number of clusters is stable. In my case, I was looking for subtypes of CD8+ T cells and hypothesized that at ~0.1 and ~0.3 I would find something biologically meaningful. Would be interesting to re-do that on the PBMC dataset. I would expect a plateau at a resolution that recovers the well-known cell types CD8+, CD4+, etc. . ![2019-06-03_09:53:34_911x604](https://user-images.githubusercontent.com/7051479/58785259-7ea10e80-85e5-11e9-8e0b-789e2e74754a.png). **Fig:** hyperparameter search for resolution in steps of 0.005. The graph shows the resolution vs. detected number of Leiden-clusters.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/670
