id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/scverse/scanpy/issues/262:185,deployability,cluster,cluster,185,"Hi, I'm sorry I forgot about this trend, I just stumble into the same issue. Lets say I have done my analysis in scanpy and everything is good and nice, but now I want to run, say, the cluster 10 from the louvain subset, with Palantir. Palantir can read 10X and 10X_H5 files. Is there a way to plug-and-play this with scanpy? . In another case, if I want to extract the subset expression matrix, where rows are genes (with rownames as gene symbols) and columns are cells (with colnames as cells), so I can use this with SCENIC. How could I get this from the `scanpy adata`? . I apologise in advance if I'm asking something very basic, but it will be really nice to have some sort of interconectivity between tools, since scanpy is so nice to have as a major analysis suite.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:213,integrability,sub,subset,213,"Hi, I'm sorry I forgot about this trend, I just stumble into the same issue. Lets say I have done my analysis in scanpy and everything is good and nice, but now I want to run, say, the cluster 10 from the louvain subset, with Palantir. Palantir can read 10X and 10X_H5 files. Is there a way to plug-and-play this with scanpy? . In another case, if I want to extract the subset expression matrix, where rows are genes (with rownames as gene symbols) and columns are cells (with colnames as cells), so I can use this with SCENIC. How could I get this from the `scanpy adata`? . I apologise in advance if I'm asking something very basic, but it will be really nice to have some sort of interconectivity between tools, since scanpy is so nice to have as a major analysis suite.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:370,integrability,sub,subset,370,"Hi, I'm sorry I forgot about this trend, I just stumble into the same issue. Lets say I have done my analysis in scanpy and everything is good and nice, but now I want to run, say, the cluster 10 from the louvain subset, with Palantir. Palantir can read 10X and 10X_H5 files. Is there a way to plug-and-play this with scanpy? . In another case, if I want to extract the subset expression matrix, where rows are genes (with rownames as gene symbols) and columns are cells (with colnames as cells), so I can use this with SCENIC. How could I get this from the `scanpy adata`? . I apologise in advance if I'm asking something very basic, but it will be really nice to have some sort of interconectivity between tools, since scanpy is so nice to have as a major analysis suite.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:294,interoperability,plug-and-play,plug-and-play,294,"Hi, I'm sorry I forgot about this trend, I just stumble into the same issue. Lets say I have done my analysis in scanpy and everything is good and nice, but now I want to run, say, the cluster 10 from the louvain subset, with Palantir. Palantir can read 10X and 10X_H5 files. Is there a way to plug-and-play this with scanpy? . In another case, if I want to extract the subset expression matrix, where rows are genes (with rownames as gene symbols) and columns are cells (with colnames as cells), so I can use this with SCENIC. How could I get this from the `scanpy adata`? . I apologise in advance if I'm asking something very basic, but it will be really nice to have some sort of interconectivity between tools, since scanpy is so nice to have as a major analysis suite.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:708,usability,tool,tools,708,"Hi, I'm sorry I forgot about this trend, I just stumble into the same issue. Lets say I have done my analysis in scanpy and everything is good and nice, but now I want to run, say, the cluster 10 from the louvain subset, with Palantir. Palantir can read 10X and 10X_H5 files. Is there a way to plug-and-play this with scanpy? . In another case, if I want to extract the subset expression matrix, where rows are genes (with rownames as gene symbols) and columns are cells (with colnames as cells), so I can use this with SCENIC. How could I get this from the `scanpy adata`? . I apologise in advance if I'm asking something very basic, but it will be really nice to have some sort of interconectivity between tools, since scanpy is so nice to have as a major analysis suite.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:64,usability,tool,tools,64,"You can always export as a `.csv` file and read that into other tools using `adata.write_csvs(filename, skip_data=False)`. You can call `adata.T` for transposing before. I can imagine that Palantir would also accept `AnnData` objects, you could make an issue there. Also, have you tried `tl.paga` for trajectory inference, the paper is [here](https://doi.org/10.1101/208819)?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:43,usability,help,help,43,"Hi, I am trying PAGA. Thanks a lot for the help.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:357,usability,help,help,357,"@falexwolf @cartal @LuckyMD I am also trying to export a gene by cell expression file. I tried using adata.write_csvs(filename, skip_data=False) but that wrote the output to multiple files. Is there a way to generate a single file with genes as rows (with gene names as row IDs) and cells as columns (barcodes as column IDs) ? Thank you in advance for your help.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:94,testability,simpl,simply,94,"No, there is no way to produce a single file with data and metadata. Having genes as rows can simply be achieved by transposing the matrix (`adata.T.write_csvs(...)`).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:94,usability,simpl,simply,94,"No, there is no way to produce a single file with data and metadata. Having genes as rows can simply be achieved by transposing the matrix (`adata.T.write_csvs(...)`).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:26,usability,feedback,feedback,26,"@falexwolf thanks for the feedback. As @maximilianh suggested, I was able to export the expression matrix from the cellbrowser export function. Thank you for your help.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:163,usability,help,help,163,"@falexwolf thanks for the feedback. As @maximilianh suggested, I was able to export the expression matrix from the cellbrowser export function. Thank you for your help.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:704,security,auth,auth,704,"Nice to hear that it worked. As a side product, you can now create a cell. browser html directory from the generated directory. On Thu, Feb 7, 2019 at 10:21 AM aditisk <notifications@github.com> wrote:. > @falexwolf <https://github.com/falexwolf> thanks for the feedback. As. > @maximilianh <https://github.com/maximilianh> suggested, I was able to. > export the expression matrix from the cellbrowser export function. Thank. > you for your help. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/262#issuecomment-461540169>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AAS-TXGCkdaQWO8ks_x7uOm-P2_ISArRks5vLG6RgaJpZM4Wne7Z>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:262,usability,feedback,feedback,262,"Nice to hear that it worked. As a side product, you can now create a cell. browser html directory from the generated directory. On Thu, Feb 7, 2019 at 10:21 AM aditisk <notifications@github.com> wrote:. > @falexwolf <https://github.com/falexwolf> thanks for the feedback. As. > @maximilianh <https://github.com/maximilianh> suggested, I was able to. > export the expression matrix from the cellbrowser export function. Thank. > you for your help. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/262#issuecomment-461540169>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AAS-TXGCkdaQWO8ks_x7uOm-P2_ISArRks5vLG6RgaJpZM4Wne7Z>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:441,usability,help,help,441,"Nice to hear that it worked. As a side product, you can now create a cell. browser html directory from the generated directory. On Thu, Feb 7, 2019 at 10:21 AM aditisk <notifications@github.com> wrote:. > @falexwolf <https://github.com/falexwolf> thanks for the feedback. As. > @maximilianh <https://github.com/maximilianh> suggested, I was able to. > export the expression matrix from the cellbrowser export function. Thank. > you for your help. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/262#issuecomment-461540169>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AAS-TXGCkdaQWO8ks_x7uOm-P2_ISArRks5vLG6RgaJpZM4Wne7Z>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:106,availability,error,error,106,"@maximilianh I was able to use the cell browser export function in the past but this time I am getting an error message:. INFO:root:Writing scanpy matrix to adata_cellbrowser_04_01_19_CD8_subclustered/exprMatrix.tsv.gz. INFO:root:Transposing matrix. INFO:root:Writing gene-by-gene, without using pandas. INFO:root:Writing 8068 genes in total. INFO:root:Wrote 0 genes. INFO:root:Wrote 2000 genes. INFO:root:Wrote 4000 genes. INFO:root:Wrote 6000 genes. INFO:root:Wrote 8000 genes. INFO:root:Writing UMAP coords to adata_cellbrowser_04_01_19_CD8_subclustered/umap_coords.tsv. ERROR:root:Couldnt find cluster markers list. I am using an h5ad file to import my ann data object. Is that why there is some issue with finding cluster markers ? I am able to plot the clusters in a UMAP plot so I know that the 'louvain' observation exists. Any thoughts on why this is happening ? Thanks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:574,availability,ERROR,ERROR,574,"@maximilianh I was able to use the cell browser export function in the past but this time I am getting an error message:. INFO:root:Writing scanpy matrix to adata_cellbrowser_04_01_19_CD8_subclustered/exprMatrix.tsv.gz. INFO:root:Transposing matrix. INFO:root:Writing gene-by-gene, without using pandas. INFO:root:Writing 8068 genes in total. INFO:root:Wrote 0 genes. INFO:root:Wrote 2000 genes. INFO:root:Wrote 4000 genes. INFO:root:Wrote 6000 genes. INFO:root:Wrote 8000 genes. INFO:root:Writing UMAP coords to adata_cellbrowser_04_01_19_CD8_subclustered/umap_coords.tsv. ERROR:root:Couldnt find cluster markers list. I am using an h5ad file to import my ann data object. Is that why there is some issue with finding cluster markers ? I am able to plot the clusters in a UMAP plot so I know that the 'louvain' observation exists. Any thoughts on why this is happening ? Thanks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:598,availability,cluster,cluster,598,"@maximilianh I was able to use the cell browser export function in the past but this time I am getting an error message:. INFO:root:Writing scanpy matrix to adata_cellbrowser_04_01_19_CD8_subclustered/exprMatrix.tsv.gz. INFO:root:Transposing matrix. INFO:root:Writing gene-by-gene, without using pandas. INFO:root:Writing 8068 genes in total. INFO:root:Wrote 0 genes. INFO:root:Wrote 2000 genes. INFO:root:Wrote 4000 genes. INFO:root:Wrote 6000 genes. INFO:root:Wrote 8000 genes. INFO:root:Writing UMAP coords to adata_cellbrowser_04_01_19_CD8_subclustered/umap_coords.tsv. ERROR:root:Couldnt find cluster markers list. I am using an h5ad file to import my ann data object. Is that why there is some issue with finding cluster markers ? I am able to plot the clusters in a UMAP plot so I know that the 'louvain' observation exists. Any thoughts on why this is happening ? Thanks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:719,availability,cluster,cluster,719,"@maximilianh I was able to use the cell browser export function in the past but this time I am getting an error message:. INFO:root:Writing scanpy matrix to adata_cellbrowser_04_01_19_CD8_subclustered/exprMatrix.tsv.gz. INFO:root:Transposing matrix. INFO:root:Writing gene-by-gene, without using pandas. INFO:root:Writing 8068 genes in total. INFO:root:Wrote 0 genes. INFO:root:Wrote 2000 genes. INFO:root:Wrote 4000 genes. INFO:root:Wrote 6000 genes. INFO:root:Wrote 8000 genes. INFO:root:Writing UMAP coords to adata_cellbrowser_04_01_19_CD8_subclustered/umap_coords.tsv. ERROR:root:Couldnt find cluster markers list. I am using an h5ad file to import my ann data object. Is that why there is some issue with finding cluster markers ? I am able to plot the clusters in a UMAP plot so I know that the 'louvain' observation exists. Any thoughts on why this is happening ? Thanks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:759,availability,cluster,clusters,759,"@maximilianh I was able to use the cell browser export function in the past but this time I am getting an error message:. INFO:root:Writing scanpy matrix to adata_cellbrowser_04_01_19_CD8_subclustered/exprMatrix.tsv.gz. INFO:root:Transposing matrix. INFO:root:Writing gene-by-gene, without using pandas. INFO:root:Writing 8068 genes in total. INFO:root:Wrote 0 genes. INFO:root:Wrote 2000 genes. INFO:root:Wrote 4000 genes. INFO:root:Wrote 6000 genes. INFO:root:Wrote 8000 genes. INFO:root:Writing UMAP coords to adata_cellbrowser_04_01_19_CD8_subclustered/umap_coords.tsv. ERROR:root:Couldnt find cluster markers list. I am using an h5ad file to import my ann data object. Is that why there is some issue with finding cluster markers ? I am able to plot the clusters in a UMAP plot so I know that the 'louvain' observation exists. Any thoughts on why this is happening ? Thanks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:598,deployability,cluster,cluster,598,"@maximilianh I was able to use the cell browser export function in the past but this time I am getting an error message:. INFO:root:Writing scanpy matrix to adata_cellbrowser_04_01_19_CD8_subclustered/exprMatrix.tsv.gz. INFO:root:Transposing matrix. INFO:root:Writing gene-by-gene, without using pandas. INFO:root:Writing 8068 genes in total. INFO:root:Wrote 0 genes. INFO:root:Wrote 2000 genes. INFO:root:Wrote 4000 genes. INFO:root:Wrote 6000 genes. INFO:root:Wrote 8000 genes. INFO:root:Writing UMAP coords to adata_cellbrowser_04_01_19_CD8_subclustered/umap_coords.tsv. ERROR:root:Couldnt find cluster markers list. I am using an h5ad file to import my ann data object. Is that why there is some issue with finding cluster markers ? I am able to plot the clusters in a UMAP plot so I know that the 'louvain' observation exists. Any thoughts on why this is happening ? Thanks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:719,deployability,cluster,cluster,719,"@maximilianh I was able to use the cell browser export function in the past but this time I am getting an error message:. INFO:root:Writing scanpy matrix to adata_cellbrowser_04_01_19_CD8_subclustered/exprMatrix.tsv.gz. INFO:root:Transposing matrix. INFO:root:Writing gene-by-gene, without using pandas. INFO:root:Writing 8068 genes in total. INFO:root:Wrote 0 genes. INFO:root:Wrote 2000 genes. INFO:root:Wrote 4000 genes. INFO:root:Wrote 6000 genes. INFO:root:Wrote 8000 genes. INFO:root:Writing UMAP coords to adata_cellbrowser_04_01_19_CD8_subclustered/umap_coords.tsv. ERROR:root:Couldnt find cluster markers list. I am using an h5ad file to import my ann data object. Is that why there is some issue with finding cluster markers ? I am able to plot the clusters in a UMAP plot so I know that the 'louvain' observation exists. Any thoughts on why this is happening ? Thanks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:759,deployability,cluster,clusters,759,"@maximilianh I was able to use the cell browser export function in the past but this time I am getting an error message:. INFO:root:Writing scanpy matrix to adata_cellbrowser_04_01_19_CD8_subclustered/exprMatrix.tsv.gz. INFO:root:Transposing matrix. INFO:root:Writing gene-by-gene, without using pandas. INFO:root:Writing 8068 genes in total. INFO:root:Wrote 0 genes. INFO:root:Wrote 2000 genes. INFO:root:Wrote 4000 genes. INFO:root:Wrote 6000 genes. INFO:root:Wrote 8000 genes. INFO:root:Writing UMAP coords to adata_cellbrowser_04_01_19_CD8_subclustered/umap_coords.tsv. ERROR:root:Couldnt find cluster markers list. I am using an h5ad file to import my ann data object. Is that why there is some issue with finding cluster markers ? I am able to plot the clusters in a UMAP plot so I know that the 'louvain' observation exists. Any thoughts on why this is happening ? Thanks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:812,deployability,observ,observation,812,"@maximilianh I was able to use the cell browser export function in the past but this time I am getting an error message:. INFO:root:Writing scanpy matrix to adata_cellbrowser_04_01_19_CD8_subclustered/exprMatrix.tsv.gz. INFO:root:Transposing matrix. INFO:root:Writing gene-by-gene, without using pandas. INFO:root:Writing 8068 genes in total. INFO:root:Wrote 0 genes. INFO:root:Wrote 2000 genes. INFO:root:Wrote 4000 genes. INFO:root:Wrote 6000 genes. INFO:root:Wrote 8000 genes. INFO:root:Writing UMAP coords to adata_cellbrowser_04_01_19_CD8_subclustered/umap_coords.tsv. ERROR:root:Couldnt find cluster markers list. I am using an h5ad file to import my ann data object. Is that why there is some issue with finding cluster markers ? I am able to plot the clusters in a UMAP plot so I know that the 'louvain' observation exists. Any thoughts on why this is happening ? Thanks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:112,integrability,messag,message,112,"@maximilianh I was able to use the cell browser export function in the past but this time I am getting an error message:. INFO:root:Writing scanpy matrix to adata_cellbrowser_04_01_19_CD8_subclustered/exprMatrix.tsv.gz. INFO:root:Transposing matrix. INFO:root:Writing gene-by-gene, without using pandas. INFO:root:Writing 8068 genes in total. INFO:root:Wrote 0 genes. INFO:root:Wrote 2000 genes. INFO:root:Wrote 4000 genes. INFO:root:Wrote 6000 genes. INFO:root:Wrote 8000 genes. INFO:root:Writing UMAP coords to adata_cellbrowser_04_01_19_CD8_subclustered/umap_coords.tsv. ERROR:root:Couldnt find cluster markers list. I am using an h5ad file to import my ann data object. Is that why there is some issue with finding cluster markers ? I am able to plot the clusters in a UMAP plot so I know that the 'louvain' observation exists. Any thoughts on why this is happening ? Thanks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:112,interoperability,messag,message,112,"@maximilianh I was able to use the cell browser export function in the past but this time I am getting an error message:. INFO:root:Writing scanpy matrix to adata_cellbrowser_04_01_19_CD8_subclustered/exprMatrix.tsv.gz. INFO:root:Transposing matrix. INFO:root:Writing gene-by-gene, without using pandas. INFO:root:Writing 8068 genes in total. INFO:root:Wrote 0 genes. INFO:root:Wrote 2000 genes. INFO:root:Wrote 4000 genes. INFO:root:Wrote 6000 genes. INFO:root:Wrote 8000 genes. INFO:root:Writing UMAP coords to adata_cellbrowser_04_01_19_CD8_subclustered/umap_coords.tsv. ERROR:root:Couldnt find cluster markers list. I am using an h5ad file to import my ann data object. Is that why there is some issue with finding cluster markers ? I am able to plot the clusters in a UMAP plot so I know that the 'louvain' observation exists. Any thoughts on why this is happening ? Thanks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:85,performance,time,time,85,"@maximilianh I was able to use the cell browser export function in the past but this time I am getting an error message:. INFO:root:Writing scanpy matrix to adata_cellbrowser_04_01_19_CD8_subclustered/exprMatrix.tsv.gz. INFO:root:Transposing matrix. INFO:root:Writing gene-by-gene, without using pandas. INFO:root:Writing 8068 genes in total. INFO:root:Wrote 0 genes. INFO:root:Wrote 2000 genes. INFO:root:Wrote 4000 genes. INFO:root:Wrote 6000 genes. INFO:root:Wrote 8000 genes. INFO:root:Writing UMAP coords to adata_cellbrowser_04_01_19_CD8_subclustered/umap_coords.tsv. ERROR:root:Couldnt find cluster markers list. I am using an h5ad file to import my ann data object. Is that why there is some issue with finding cluster markers ? I am able to plot the clusters in a UMAP plot so I know that the 'louvain' observation exists. Any thoughts on why this is happening ? Thanks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:106,performance,error,error,106,"@maximilianh I was able to use the cell browser export function in the past but this time I am getting an error message:. INFO:root:Writing scanpy matrix to adata_cellbrowser_04_01_19_CD8_subclustered/exprMatrix.tsv.gz. INFO:root:Transposing matrix. INFO:root:Writing gene-by-gene, without using pandas. INFO:root:Writing 8068 genes in total. INFO:root:Wrote 0 genes. INFO:root:Wrote 2000 genes. INFO:root:Wrote 4000 genes. INFO:root:Wrote 6000 genes. INFO:root:Wrote 8000 genes. INFO:root:Writing UMAP coords to adata_cellbrowser_04_01_19_CD8_subclustered/umap_coords.tsv. ERROR:root:Couldnt find cluster markers list. I am using an h5ad file to import my ann data object. Is that why there is some issue with finding cluster markers ? I am able to plot the clusters in a UMAP plot so I know that the 'louvain' observation exists. Any thoughts on why this is happening ? Thanks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:574,performance,ERROR,ERROR,574,"@maximilianh I was able to use the cell browser export function in the past but this time I am getting an error message:. INFO:root:Writing scanpy matrix to adata_cellbrowser_04_01_19_CD8_subclustered/exprMatrix.tsv.gz. INFO:root:Transposing matrix. INFO:root:Writing gene-by-gene, without using pandas. INFO:root:Writing 8068 genes in total. INFO:root:Wrote 0 genes. INFO:root:Wrote 2000 genes. INFO:root:Wrote 4000 genes. INFO:root:Wrote 6000 genes. INFO:root:Wrote 8000 genes. INFO:root:Writing UMAP coords to adata_cellbrowser_04_01_19_CD8_subclustered/umap_coords.tsv. ERROR:root:Couldnt find cluster markers list. I am using an h5ad file to import my ann data object. Is that why there is some issue with finding cluster markers ? I am able to plot the clusters in a UMAP plot so I know that the 'louvain' observation exists. Any thoughts on why this is happening ? Thanks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:106,safety,error,error,106,"@maximilianh I was able to use the cell browser export function in the past but this time I am getting an error message:. INFO:root:Writing scanpy matrix to adata_cellbrowser_04_01_19_CD8_subclustered/exprMatrix.tsv.gz. INFO:root:Transposing matrix. INFO:root:Writing gene-by-gene, without using pandas. INFO:root:Writing 8068 genes in total. INFO:root:Wrote 0 genes. INFO:root:Wrote 2000 genes. INFO:root:Wrote 4000 genes. INFO:root:Wrote 6000 genes. INFO:root:Wrote 8000 genes. INFO:root:Writing UMAP coords to adata_cellbrowser_04_01_19_CD8_subclustered/umap_coords.tsv. ERROR:root:Couldnt find cluster markers list. I am using an h5ad file to import my ann data object. Is that why there is some issue with finding cluster markers ? I am able to plot the clusters in a UMAP plot so I know that the 'louvain' observation exists. Any thoughts on why this is happening ? Thanks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:574,safety,ERROR,ERROR,574,"@maximilianh I was able to use the cell browser export function in the past but this time I am getting an error message:. INFO:root:Writing scanpy matrix to adata_cellbrowser_04_01_19_CD8_subclustered/exprMatrix.tsv.gz. INFO:root:Transposing matrix. INFO:root:Writing gene-by-gene, without using pandas. INFO:root:Writing 8068 genes in total. INFO:root:Wrote 0 genes. INFO:root:Wrote 2000 genes. INFO:root:Wrote 4000 genes. INFO:root:Wrote 6000 genes. INFO:root:Wrote 8000 genes. INFO:root:Writing UMAP coords to adata_cellbrowser_04_01_19_CD8_subclustered/umap_coords.tsv. ERROR:root:Couldnt find cluster markers list. I am using an h5ad file to import my ann data object. Is that why there is some issue with finding cluster markers ? I am able to plot the clusters in a UMAP plot so I know that the 'louvain' observation exists. Any thoughts on why this is happening ? Thanks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:812,testability,observ,observation,812,"@maximilianh I was able to use the cell browser export function in the past but this time I am getting an error message:. INFO:root:Writing scanpy matrix to adata_cellbrowser_04_01_19_CD8_subclustered/exprMatrix.tsv.gz. INFO:root:Transposing matrix. INFO:root:Writing gene-by-gene, without using pandas. INFO:root:Writing 8068 genes in total. INFO:root:Wrote 0 genes. INFO:root:Wrote 2000 genes. INFO:root:Wrote 4000 genes. INFO:root:Wrote 6000 genes. INFO:root:Wrote 8000 genes. INFO:root:Writing UMAP coords to adata_cellbrowser_04_01_19_CD8_subclustered/umap_coords.tsv. ERROR:root:Couldnt find cluster markers list. I am using an h5ad file to import my ann data object. Is that why there is some issue with finding cluster markers ? I am able to plot the clusters in a UMAP plot so I know that the 'louvain' observation exists. Any thoughts on why this is happening ? Thanks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:106,usability,error,error,106,"@maximilianh I was able to use the cell browser export function in the past but this time I am getting an error message:. INFO:root:Writing scanpy matrix to adata_cellbrowser_04_01_19_CD8_subclustered/exprMatrix.tsv.gz. INFO:root:Transposing matrix. INFO:root:Writing gene-by-gene, without using pandas. INFO:root:Writing 8068 genes in total. INFO:root:Wrote 0 genes. INFO:root:Wrote 2000 genes. INFO:root:Wrote 4000 genes. INFO:root:Wrote 6000 genes. INFO:root:Wrote 8000 genes. INFO:root:Writing UMAP coords to adata_cellbrowser_04_01_19_CD8_subclustered/umap_coords.tsv. ERROR:root:Couldnt find cluster markers list. I am using an h5ad file to import my ann data object. Is that why there is some issue with finding cluster markers ? I am able to plot the clusters in a UMAP plot so I know that the 'louvain' observation exists. Any thoughts on why this is happening ? Thanks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:574,usability,ERROR,ERROR,574,"@maximilianh I was able to use the cell browser export function in the past but this time I am getting an error message:. INFO:root:Writing scanpy matrix to adata_cellbrowser_04_01_19_CD8_subclustered/exprMatrix.tsv.gz. INFO:root:Transposing matrix. INFO:root:Writing gene-by-gene, without using pandas. INFO:root:Writing 8068 genes in total. INFO:root:Wrote 0 genes. INFO:root:Wrote 2000 genes. INFO:root:Wrote 4000 genes. INFO:root:Wrote 6000 genes. INFO:root:Wrote 8000 genes. INFO:root:Writing UMAP coords to adata_cellbrowser_04_01_19_CD8_subclustered/umap_coords.tsv. ERROR:root:Couldnt find cluster markers list. I am using an h5ad file to import my ann data object. Is that why there is some issue with finding cluster markers ? I am able to plot the clusters in a UMAP plot so I know that the 'louvain' observation exists. Any thoughts on why this is happening ? Thanks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:69,availability,cluster,cluster,69,Just a thought... have you run `sc.tl.rank_genes_groups()` to obtain cluster markers?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:69,deployability,cluster,cluster,69,Just a thought... have you run `sc.tl.rank_genes_groups()` to obtain cluster markers?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:75,availability,cluster,cluster-specific,75,"Yes, it seems that I should change this to a warning. People may not want. cluster-specific marker genes in their browser. Do you agree @aditisk ? >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:75,deployability,cluster,cluster-specific,75,"Yes, it seems that I should change this to a warning. People may not want. cluster-specific marker genes in their browser. Do you agree @aditisk ? >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:83,interoperability,specif,specific,83,"Yes, it seems that I should change this to a warning. People may not want. cluster-specific marker genes in their browser. Do you agree @aditisk ? >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:130,availability,cluster,cluster-specific,130,@LuckyMD I did not run sc.tl.rank_genes_groups() which was the problem. @maximilianh I think it should be optional to include the cluster-specific markers so maybe keeping it as a warning might be the best ? That way the user has control on what they want to include.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:130,deployability,cluster,cluster-specific,130,@LuckyMD I did not run sc.tl.rank_genes_groups() which was the problem. @maximilianh I think it should be optional to include the cluster-specific markers so maybe keeping it as a warning might be the best ? That way the user has control on what they want to include.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:138,interoperability,specif,specific,138,@LuckyMD I did not run sc.tl.rank_genes_groups() which was the problem. @maximilianh I think it should be optional to include the cluster-specific markers so maybe keeping it as a warning might be the best ? That way the user has control on what they want to include.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:230,security,control,control,230,@LuckyMD I did not run sc.tl.rank_genes_groups() which was the problem. @maximilianh I think it should be optional to include the cluster-specific markers so maybe keeping it as a warning might be the best ? That way the user has control on what they want to include.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:230,testability,control,control,230,@LuckyMD I did not run sc.tl.rank_genes_groups() which was the problem. @maximilianh I think it should be optional to include the cluster-specific markers so maybe keeping it as a warning might be the best ? That way the user has control on what they want to include.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:221,usability,user,user,221,@LuckyMD I did not run sc.tl.rank_genes_groups() which was the problem. @maximilianh I think it should be optional to include the cluster-specific markers so maybe keeping it as a warning might be the best ? That way the user has control on what they want to include.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:85,availability,error,error,85,@maximilianh I think those messages are from your code? maybe you should improve the error message to include something like. > Try running sc.tl.rank_genes_groups(adata) to create the cluster annotation,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:185,availability,cluster,cluster,185,@maximilianh I think those messages are from your code? maybe you should improve the error message to include something like. > Try running sc.tl.rank_genes_groups(adata) to create the cluster annotation,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:185,deployability,cluster,cluster,185,@maximilianh I think those messages are from your code? maybe you should improve the error message to include something like. > Try running sc.tl.rank_genes_groups(adata) to create the cluster annotation,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:27,integrability,messag,messages,27,@maximilianh I think those messages are from your code? maybe you should improve the error message to include something like. > Try running sc.tl.rank_genes_groups(adata) to create the cluster annotation,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:91,integrability,messag,message,91,@maximilianh I think those messages are from your code? maybe you should improve the error message to include something like. > Try running sc.tl.rank_genes_groups(adata) to create the cluster annotation,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:27,interoperability,messag,messages,27,@maximilianh I think those messages are from your code? maybe you should improve the error message to include something like. > Try running sc.tl.rank_genes_groups(adata) to create the cluster annotation,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:91,interoperability,messag,message,91,@maximilianh I think those messages are from your code? maybe you should improve the error message to include something like. > Try running sc.tl.rank_genes_groups(adata) to create the cluster annotation,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:85,performance,error,error,85,@maximilianh I think those messages are from your code? maybe you should improve the error message to include something like. > Try running sc.tl.rank_genes_groups(adata) to create the cluster annotation,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:85,safety,error,error,85,@maximilianh I think those messages are from your code? maybe you should improve the error message to include something like. > Try running sc.tl.rank_genes_groups(adata) to create the cluster annotation,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:85,usability,error,error,85,@maximilianh I think those messages are from your code? maybe you should improve the error message to include something like. > Try running sc.tl.rank_genes_groups(adata) to create the cluster annotation,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:72,modifiability,variab,variable,72,"Hi, the expression matrix I exported from adata.write only have the top variable genes. Is there a way to output the raw matrix including all genes?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:189,modifiability,variab,variable,189,"The scanpyToCellbrowser function has an option useRaw that will use the. .raw matrix, if present, for the .tsv export. Otherwise, the raw matrix of all genes is stored as ad.raw.X and the. variable names are in ad.raw.var. You can use scanpyToCellbrowser to write. the matrix and all annotations, or anndataToTsv to write just the matrix. Or use code from there to write your own. On Fri, May 31, 2019 at 5:14 PM Jing He <notifications@github.com> wrote:. > Hi, the expression matrix I exported from adata.write only have the top. > variable genes. Is there a way to output the raw matrix including all genes? >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/262?email_source=notifications&email_token=AACL4TNOFS6MLIH44P6J5HDPYE6ENA5CNFSM4FU553M2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWVQBGI#issuecomment-497746073>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACL4TORHPOQ2GTWTUGTAI3PYE6ENANCNFSM4FU553MQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:533,modifiability,variab,variable,533,"The scanpyToCellbrowser function has an option useRaw that will use the. .raw matrix, if present, for the .tsv export. Otherwise, the raw matrix of all genes is stored as ad.raw.X and the. variable names are in ad.raw.var. You can use scanpyToCellbrowser to write. the matrix and all annotations, or anndataToTsv to write just the matrix. Or use code from there to write your own. On Fri, May 31, 2019 at 5:14 PM Jing He <notifications@github.com> wrote:. > Hi, the expression matrix I exported from adata.write only have the top. > variable genes. Is there a way to output the raw matrix including all genes? >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/262?email_source=notifications&email_token=AACL4TNOFS6MLIH44P6J5HDPYE6ENA5CNFSM4FU553M2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWVQBGI#issuecomment-497746073>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACL4TORHPOQ2GTWTUGTAI3PYE6ENANCNFSM4FU553MQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:1018,security,auth,auth,1018,"The scanpyToCellbrowser function has an option useRaw that will use the. .raw matrix, if present, for the .tsv export. Otherwise, the raw matrix of all genes is stored as ad.raw.X and the. variable names are in ad.raw.var. You can use scanpyToCellbrowser to write. the matrix and all annotations, or anndataToTsv to write just the matrix. Or use code from there to write your own. On Fri, May 31, 2019 at 5:14 PM Jing He <notifications@github.com> wrote:. > Hi, the expression matrix I exported from adata.write only have the top. > variable genes. Is there a way to output the raw matrix including all genes? >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/262?email_source=notifications&email_token=AACL4TNOFS6MLIH44P6J5HDPYE6ENA5CNFSM4FU553M2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWVQBGI#issuecomment-497746073>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACL4TORHPOQ2GTWTUGTAI3PYE6ENANCNFSM4FU553MQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:47,usability,useR,useRaw,47,"The scanpyToCellbrowser function has an option useRaw that will use the. .raw matrix, if present, for the .tsv export. Otherwise, the raw matrix of all genes is stored as ad.raw.X and the. variable names are in ad.raw.var. You can use scanpyToCellbrowser to write. the matrix and all annotations, or anndataToTsv to write just the matrix. Or use code from there to write your own. On Fri, May 31, 2019 at 5:14 PM Jing He <notifications@github.com> wrote:. > Hi, the expression matrix I exported from adata.write only have the top. > variable genes. Is there a way to output the raw matrix including all genes? >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/262?email_source=notifications&email_token=AACL4TNOFS6MLIH44P6J5HDPYE6ENA5CNFSM4FU553M2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWVQBGI#issuecomment-497746073>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AACL4TORHPOQ2GTWTUGTAI3PYE6ENANCNFSM4FU553MQ>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:309,availability,error,error,309,"Thanks for the suggestion. Actually, I am using cellxgene which takes the. h5ad file as an input. when using anndata.write() function, it only output. the anndata.X as the expression matrix. And also there is no option of. useRaw here. Also, I tried to re-assign anndata.X = anndata.raw.X, but it returns an. error saying its wrong shape. Do you have any suggestions? Thanks a lot! On Mon, Jun 3, 2019 at 6:03 AM Maximilian Haeussler <. notifications@github.com> wrote:. > The scanpyToCellbrowser function has an option useRaw that will use the. > .raw matrix, if present, for the .tsv export. >. > Otherwise, the raw matrix of all genes is stored as ad.raw.X and the. > variable names are in ad.raw.var. You can use scanpyToCellbrowser to write. > the matrix and all annotations, or anndataToTsv to write just the matrix. > Or use code from there to write your own. >. > On Fri, May 31, 2019 at 5:14 PM Jing He <notifications@github.com> wrote:. >. > > Hi, the expression matrix I exported from adata.write only have the top. > > variable genes. Is there a way to output the raw matrix including all. > genes? > >. > > —. > > You are receiving this because you were mentioned. > > Reply to this email directly, view it on GitHub. > > <. > https://github.com/theislab/scanpy/issues/262?email_source=notifications&email_token=AACL4TNOFS6MLIH44P6J5HDPYE6ENA5CNFSM4FU553M2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWVQBGI#issuecomment-497746073. > >,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AACL4TORHPOQ2GTWTUGTAI3PYE6ENANCNFSM4FU553MQ. > >. > > . > >. >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/262?email_source=notifications&email_token=AAUAIIIOXG5HSDCKTFYS7KLPYTT6BA5CNFSM4FU553M2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWY5RAA#issuecomment-498194560>,. > or mute the thread. > <https://github.com/notificatio",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:671,modifiability,variab,variable,671,"Thanks for the suggestion. Actually, I am using cellxgene which takes the. h5ad file as an input. when using anndata.write() function, it only output. the anndata.X as the expression matrix. And also there is no option of. useRaw here. Also, I tried to re-assign anndata.X = anndata.raw.X, but it returns an. error saying its wrong shape. Do you have any suggestions? Thanks a lot! On Mon, Jun 3, 2019 at 6:03 AM Maximilian Haeussler <. notifications@github.com> wrote:. > The scanpyToCellbrowser function has an option useRaw that will use the. > .raw matrix, if present, for the .tsv export. >. > Otherwise, the raw matrix of all genes is stored as ad.raw.X and the. > variable names are in ad.raw.var. You can use scanpyToCellbrowser to write. > the matrix and all annotations, or anndataToTsv to write just the matrix. > Or use code from there to write your own. >. > On Fri, May 31, 2019 at 5:14 PM Jing He <notifications@github.com> wrote:. >. > > Hi, the expression matrix I exported from adata.write only have the top. > > variable genes. Is there a way to output the raw matrix including all. > genes? > >. > > —. > > You are receiving this because you were mentioned. > > Reply to this email directly, view it on GitHub. > > <. > https://github.com/theislab/scanpy/issues/262?email_source=notifications&email_token=AACL4TNOFS6MLIH44P6J5HDPYE6ENA5CNFSM4FU553M2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWVQBGI#issuecomment-497746073. > >,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AACL4TORHPOQ2GTWTUGTAI3PYE6ENANCNFSM4FU553MQ. > >. > > . > >. >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/262?email_source=notifications&email_token=AAUAIIIOXG5HSDCKTFYS7KLPYTT6BA5CNFSM4FU553M2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWY5RAA#issuecomment-498194560>,. > or mute the thread. > <https://github.com/notificatio",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:1031,modifiability,variab,variable,1031,", I am using cellxgene which takes the. h5ad file as an input. when using anndata.write() function, it only output. the anndata.X as the expression matrix. And also there is no option of. useRaw here. Also, I tried to re-assign anndata.X = anndata.raw.X, but it returns an. error saying its wrong shape. Do you have any suggestions? Thanks a lot! On Mon, Jun 3, 2019 at 6:03 AM Maximilian Haeussler <. notifications@github.com> wrote:. > The scanpyToCellbrowser function has an option useRaw that will use the. > .raw matrix, if present, for the .tsv export. >. > Otherwise, the raw matrix of all genes is stored as ad.raw.X and the. > variable names are in ad.raw.var. You can use scanpyToCellbrowser to write. > the matrix and all annotations, or anndataToTsv to write just the matrix. > Or use code from there to write your own. >. > On Fri, May 31, 2019 at 5:14 PM Jing He <notifications@github.com> wrote:. >. > > Hi, the expression matrix I exported from adata.write only have the top. > > variable genes. Is there a way to output the raw matrix including all. > genes? > >. > > —. > > You are receiving this because you were mentioned. > > Reply to this email directly, view it on GitHub. > > <. > https://github.com/theislab/scanpy/issues/262?email_source=notifications&email_token=AACL4TNOFS6MLIH44P6J5HDPYE6ENA5CNFSM4FU553M2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWVQBGI#issuecomment-497746073. > >,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AACL4TORHPOQ2GTWTUGTAI3PYE6ENANCNFSM4FU553MQ. > >. > > . > >. >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/262?email_source=notifications&email_token=AAUAIIIOXG5HSDCKTFYS7KLPYTT6BA5CNFSM4FU553M2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWY5RAA#issuecomment-498194560>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AAUAIIM2RYJQTSD",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:309,performance,error,error,309,"Thanks for the suggestion. Actually, I am using cellxgene which takes the. h5ad file as an input. when using anndata.write() function, it only output. the anndata.X as the expression matrix. And also there is no option of. useRaw here. Also, I tried to re-assign anndata.X = anndata.raw.X, but it returns an. error saying its wrong shape. Do you have any suggestions? Thanks a lot! On Mon, Jun 3, 2019 at 6:03 AM Maximilian Haeussler <. notifications@github.com> wrote:. > The scanpyToCellbrowser function has an option useRaw that will use the. > .raw matrix, if present, for the .tsv export. >. > Otherwise, the raw matrix of all genes is stored as ad.raw.X and the. > variable names are in ad.raw.var. You can use scanpyToCellbrowser to write. > the matrix and all annotations, or anndataToTsv to write just the matrix. > Or use code from there to write your own. >. > On Fri, May 31, 2019 at 5:14 PM Jing He <notifications@github.com> wrote:. >. > > Hi, the expression matrix I exported from adata.write only have the top. > > variable genes. Is there a way to output the raw matrix including all. > genes? > >. > > —. > > You are receiving this because you were mentioned. > > Reply to this email directly, view it on GitHub. > > <. > https://github.com/theislab/scanpy/issues/262?email_source=notifications&email_token=AACL4TNOFS6MLIH44P6J5HDPYE6ENA5CNFSM4FU553M2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWVQBGI#issuecomment-497746073. > >,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AACL4TORHPOQ2GTWTUGTAI3PYE6ENANCNFSM4FU553MQ. > >. > > . > >. >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/262?email_source=notifications&email_token=AAUAIIIOXG5HSDCKTFYS7KLPYTT6BA5CNFSM4FU553M2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWY5RAA#issuecomment-498194560>,. > or mute the thread. > <https://github.com/notificatio",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:91,safety,input,input,91,"Thanks for the suggestion. Actually, I am using cellxgene which takes the. h5ad file as an input. when using anndata.write() function, it only output. the anndata.X as the expression matrix. And also there is no option of. useRaw here. Also, I tried to re-assign anndata.X = anndata.raw.X, but it returns an. error saying its wrong shape. Do you have any suggestions? Thanks a lot! On Mon, Jun 3, 2019 at 6:03 AM Maximilian Haeussler <. notifications@github.com> wrote:. > The scanpyToCellbrowser function has an option useRaw that will use the. > .raw matrix, if present, for the .tsv export. >. > Otherwise, the raw matrix of all genes is stored as ad.raw.X and the. > variable names are in ad.raw.var. You can use scanpyToCellbrowser to write. > the matrix and all annotations, or anndataToTsv to write just the matrix. > Or use code from there to write your own. >. > On Fri, May 31, 2019 at 5:14 PM Jing He <notifications@github.com> wrote:. >. > > Hi, the expression matrix I exported from adata.write only have the top. > > variable genes. Is there a way to output the raw matrix including all. > genes? > >. > > —. > > You are receiving this because you were mentioned. > > Reply to this email directly, view it on GitHub. > > <. > https://github.com/theislab/scanpy/issues/262?email_source=notifications&email_token=AACL4TNOFS6MLIH44P6J5HDPYE6ENA5CNFSM4FU553M2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWVQBGI#issuecomment-497746073. > >,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AACL4TORHPOQ2GTWTUGTAI3PYE6ENANCNFSM4FU553MQ. > >. > > . > >. >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/262?email_source=notifications&email_token=AAUAIIIOXG5HSDCKTFYS7KLPYTT6BA5CNFSM4FU553M2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWY5RAA#issuecomment-498194560>,. > or mute the thread. > <https://github.com/notificatio",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:309,safety,error,error,309,"Thanks for the suggestion. Actually, I am using cellxgene which takes the. h5ad file as an input. when using anndata.write() function, it only output. the anndata.X as the expression matrix. And also there is no option of. useRaw here. Also, I tried to re-assign anndata.X = anndata.raw.X, but it returns an. error saying its wrong shape. Do you have any suggestions? Thanks a lot! On Mon, Jun 3, 2019 at 6:03 AM Maximilian Haeussler <. notifications@github.com> wrote:. > The scanpyToCellbrowser function has an option useRaw that will use the. > .raw matrix, if present, for the .tsv export. >. > Otherwise, the raw matrix of all genes is stored as ad.raw.X and the. > variable names are in ad.raw.var. You can use scanpyToCellbrowser to write. > the matrix and all annotations, or anndataToTsv to write just the matrix. > Or use code from there to write your own. >. > On Fri, May 31, 2019 at 5:14 PM Jing He <notifications@github.com> wrote:. >. > > Hi, the expression matrix I exported from adata.write only have the top. > > variable genes. Is there a way to output the raw matrix including all. > genes? > >. > > —. > > You are receiving this because you were mentioned. > > Reply to this email directly, view it on GitHub. > > <. > https://github.com/theislab/scanpy/issues/262?email_source=notifications&email_token=AACL4TNOFS6MLIH44P6J5HDPYE6ENA5CNFSM4FU553M2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWVQBGI#issuecomment-497746073. > >,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AACL4TORHPOQ2GTWTUGTAI3PYE6ENANCNFSM4FU553MQ. > >. > > . > >. >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/262?email_source=notifications&email_token=AAUAIIIOXG5HSDCKTFYS7KLPYTT6BA5CNFSM4FU553M2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWY5RAA#issuecomment-498194560>,. > or mute the thread. > <https://github.com/notificatio",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:1545,security,auth,auth,1545,"nput. when using anndata.write() function, it only output. the anndata.X as the expression matrix. And also there is no option of. useRaw here. Also, I tried to re-assign anndata.X = anndata.raw.X, but it returns an. error saying its wrong shape. Do you have any suggestions? Thanks a lot! On Mon, Jun 3, 2019 at 6:03 AM Maximilian Haeussler <. notifications@github.com> wrote:. > The scanpyToCellbrowser function has an option useRaw that will use the. > .raw matrix, if present, for the .tsv export. >. > Otherwise, the raw matrix of all genes is stored as ad.raw.X and the. > variable names are in ad.raw.var. You can use scanpyToCellbrowser to write. > the matrix and all annotations, or anndataToTsv to write just the matrix. > Or use code from there to write your own. >. > On Fri, May 31, 2019 at 5:14 PM Jing He <notifications@github.com> wrote:. >. > > Hi, the expression matrix I exported from adata.write only have the top. > > variable genes. Is there a way to output the raw matrix including all. > genes? > >. > > —. > > You are receiving this because you were mentioned. > > Reply to this email directly, view it on GitHub. > > <. > https://github.com/theislab/scanpy/issues/262?email_source=notifications&email_token=AACL4TNOFS6MLIH44P6J5HDPYE6ENA5CNFSM4FU553M2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWVQBGI#issuecomment-497746073. > >,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AACL4TORHPOQ2GTWTUGTAI3PYE6ENANCNFSM4FU553MQ. > >. > > . > >. >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/262?email_source=notifications&email_token=AAUAIIIOXG5HSDCKTFYS7KLPYTT6BA5CNFSM4FU553M2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWY5RAA#issuecomment-498194560>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AAUAIIM2RYJQTSDTKQ4XZLTPYTT6BANCNFSM4FU553MQ>. > . >. -- . Cheers! Jing.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:2015,security,auth,auth,2015,"nput. when using anndata.write() function, it only output. the anndata.X as the expression matrix. And also there is no option of. useRaw here. Also, I tried to re-assign anndata.X = anndata.raw.X, but it returns an. error saying its wrong shape. Do you have any suggestions? Thanks a lot! On Mon, Jun 3, 2019 at 6:03 AM Maximilian Haeussler <. notifications@github.com> wrote:. > The scanpyToCellbrowser function has an option useRaw that will use the. > .raw matrix, if present, for the .tsv export. >. > Otherwise, the raw matrix of all genes is stored as ad.raw.X and the. > variable names are in ad.raw.var. You can use scanpyToCellbrowser to write. > the matrix and all annotations, or anndataToTsv to write just the matrix. > Or use code from there to write your own. >. > On Fri, May 31, 2019 at 5:14 PM Jing He <notifications@github.com> wrote:. >. > > Hi, the expression matrix I exported from adata.write only have the top. > > variable genes. Is there a way to output the raw matrix including all. > genes? > >. > > —. > > You are receiving this because you were mentioned. > > Reply to this email directly, view it on GitHub. > > <. > https://github.com/theislab/scanpy/issues/262?email_source=notifications&email_token=AACL4TNOFS6MLIH44P6J5HDPYE6ENA5CNFSM4FU553M2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWVQBGI#issuecomment-497746073. > >,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AACL4TORHPOQ2GTWTUGTAI3PYE6ENANCNFSM4FU553MQ. > >. > > . > >. >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/262?email_source=notifications&email_token=AAUAIIIOXG5HSDCKTFYS7KLPYTT6BA5CNFSM4FU553M2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWY5RAA#issuecomment-498194560>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AAUAIIM2RYJQTSDTKQ4XZLTPYTT6BANCNFSM4FU553MQ>. > . >. -- . Cheers! Jing.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:91,usability,input,input,91,"Thanks for the suggestion. Actually, I am using cellxgene which takes the. h5ad file as an input. when using anndata.write() function, it only output. the anndata.X as the expression matrix. And also there is no option of. useRaw here. Also, I tried to re-assign anndata.X = anndata.raw.X, but it returns an. error saying its wrong shape. Do you have any suggestions? Thanks a lot! On Mon, Jun 3, 2019 at 6:03 AM Maximilian Haeussler <. notifications@github.com> wrote:. > The scanpyToCellbrowser function has an option useRaw that will use the. > .raw matrix, if present, for the .tsv export. >. > Otherwise, the raw matrix of all genes is stored as ad.raw.X and the. > variable names are in ad.raw.var. You can use scanpyToCellbrowser to write. > the matrix and all annotations, or anndataToTsv to write just the matrix. > Or use code from there to write your own. >. > On Fri, May 31, 2019 at 5:14 PM Jing He <notifications@github.com> wrote:. >. > > Hi, the expression matrix I exported from adata.write only have the top. > > variable genes. Is there a way to output the raw matrix including all. > genes? > >. > > —. > > You are receiving this because you were mentioned. > > Reply to this email directly, view it on GitHub. > > <. > https://github.com/theislab/scanpy/issues/262?email_source=notifications&email_token=AACL4TNOFS6MLIH44P6J5HDPYE6ENA5CNFSM4FU553M2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWVQBGI#issuecomment-497746073. > >,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AACL4TORHPOQ2GTWTUGTAI3PYE6ENANCNFSM4FU553MQ. > >. > > . > >. >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/262?email_source=notifications&email_token=AAUAIIIOXG5HSDCKTFYS7KLPYTT6BA5CNFSM4FU553M2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWY5RAA#issuecomment-498194560>,. > or mute the thread. > <https://github.com/notificatio",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:223,usability,useR,useRaw,223,"Thanks for the suggestion. Actually, I am using cellxgene which takes the. h5ad file as an input. when using anndata.write() function, it only output. the anndata.X as the expression matrix. And also there is no option of. useRaw here. Also, I tried to re-assign anndata.X = anndata.raw.X, but it returns an. error saying its wrong shape. Do you have any suggestions? Thanks a lot! On Mon, Jun 3, 2019 at 6:03 AM Maximilian Haeussler <. notifications@github.com> wrote:. > The scanpyToCellbrowser function has an option useRaw that will use the. > .raw matrix, if present, for the .tsv export. >. > Otherwise, the raw matrix of all genes is stored as ad.raw.X and the. > variable names are in ad.raw.var. You can use scanpyToCellbrowser to write. > the matrix and all annotations, or anndataToTsv to write just the matrix. > Or use code from there to write your own. >. > On Fri, May 31, 2019 at 5:14 PM Jing He <notifications@github.com> wrote:. >. > > Hi, the expression matrix I exported from adata.write only have the top. > > variable genes. Is there a way to output the raw matrix including all. > genes? > >. > > —. > > You are receiving this because you were mentioned. > > Reply to this email directly, view it on GitHub. > > <. > https://github.com/theislab/scanpy/issues/262?email_source=notifications&email_token=AACL4TNOFS6MLIH44P6J5HDPYE6ENA5CNFSM4FU553M2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWVQBGI#issuecomment-497746073. > >,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AACL4TORHPOQ2GTWTUGTAI3PYE6ENANCNFSM4FU553MQ. > >. > > . > >. >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/262?email_source=notifications&email_token=AAUAIIIOXG5HSDCKTFYS7KLPYTT6BA5CNFSM4FU553M2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWY5RAA#issuecomment-498194560>,. > or mute the thread. > <https://github.com/notificatio",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:309,usability,error,error,309,"Thanks for the suggestion. Actually, I am using cellxgene which takes the. h5ad file as an input. when using anndata.write() function, it only output. the anndata.X as the expression matrix. And also there is no option of. useRaw here. Also, I tried to re-assign anndata.X = anndata.raw.X, but it returns an. error saying its wrong shape. Do you have any suggestions? Thanks a lot! On Mon, Jun 3, 2019 at 6:03 AM Maximilian Haeussler <. notifications@github.com> wrote:. > The scanpyToCellbrowser function has an option useRaw that will use the. > .raw matrix, if present, for the .tsv export. >. > Otherwise, the raw matrix of all genes is stored as ad.raw.X and the. > variable names are in ad.raw.var. You can use scanpyToCellbrowser to write. > the matrix and all annotations, or anndataToTsv to write just the matrix. > Or use code from there to write your own. >. > On Fri, May 31, 2019 at 5:14 PM Jing He <notifications@github.com> wrote:. >. > > Hi, the expression matrix I exported from adata.write only have the top. > > variable genes. Is there a way to output the raw matrix including all. > genes? > >. > > —. > > You are receiving this because you were mentioned. > > Reply to this email directly, view it on GitHub. > > <. > https://github.com/theislab/scanpy/issues/262?email_source=notifications&email_token=AACL4TNOFS6MLIH44P6J5HDPYE6ENA5CNFSM4FU553M2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWVQBGI#issuecomment-497746073. > >,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AACL4TORHPOQ2GTWTUGTAI3PYE6ENANCNFSM4FU553MQ. > >. > > . > >. >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/262?email_source=notifications&email_token=AAUAIIIOXG5HSDCKTFYS7KLPYTT6BA5CNFSM4FU553M2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWY5RAA#issuecomment-498194560>,. > or mute the thread. > <https://github.com/notificatio",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:520,usability,useR,useRaw,520,"Thanks for the suggestion. Actually, I am using cellxgene which takes the. h5ad file as an input. when using anndata.write() function, it only output. the anndata.X as the expression matrix. And also there is no option of. useRaw here. Also, I tried to re-assign anndata.X = anndata.raw.X, but it returns an. error saying its wrong shape. Do you have any suggestions? Thanks a lot! On Mon, Jun 3, 2019 at 6:03 AM Maximilian Haeussler <. notifications@github.com> wrote:. > The scanpyToCellbrowser function has an option useRaw that will use the. > .raw matrix, if present, for the .tsv export. >. > Otherwise, the raw matrix of all genes is stored as ad.raw.X and the. > variable names are in ad.raw.var. You can use scanpyToCellbrowser to write. > the matrix and all annotations, or anndataToTsv to write just the matrix. > Or use code from there to write your own. >. > On Fri, May 31, 2019 at 5:14 PM Jing He <notifications@github.com> wrote:. >. > > Hi, the expression matrix I exported from adata.write only have the top. > > variable genes. Is there a way to output the raw matrix including all. > genes? > >. > > —. > > You are receiving this because you were mentioned. > > Reply to this email directly, view it on GitHub. > > <. > https://github.com/theislab/scanpy/issues/262?email_source=notifications&email_token=AACL4TNOFS6MLIH44P6J5HDPYE6ENA5CNFSM4FU553M2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWVQBGI#issuecomment-497746073. > >,. > > or mute the thread. > > <. > https://github.com/notifications/unsubscribe-auth/AACL4TORHPOQ2GTWTUGTAI3PYE6ENANCNFSM4FU553MQ. > >. > > . > >. >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/262?email_source=notifications&email_token=AAUAIIIOXG5HSDCKTFYS7KLPYTT6BA5CNFSM4FU553M2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWY5RAA#issuecomment-498194560>,. > or mute the thread. > <https://github.com/notificatio",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:620,energy efficiency,load,loaded,620,"Hi @hejing3283,. The wrong shape is probably because you have subsetted `adata.X` to highly variable genes, or did some additional filtering after storing data in `adata.raw`. For a while now scanpy avoids filtering highly variable genes, but instead annotates them in `adata.var['highly_variable']` which is then used in `sc.pp.pca()`. I would suggest you use `subset=False` next time you use `sc.pp.highly_variable()` to avoid different dimensions in `adata.X` and `adata.raw.X`. You can easily proceed by just making a new anndata object from `adata.raw.X`, `adata.raw.var` and `adata.raw.obs` and storing this to be loaded into cellxgene. Just do the following:. ```. adata_raw = sc.AnnData(X=adata.raw.X, obs=adata.raw.obs, var=adata.raw.var). adata_raw.write(my_file). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:62,integrability,sub,subsetted,62,"Hi @hejing3283,. The wrong shape is probably because you have subsetted `adata.X` to highly variable genes, or did some additional filtering after storing data in `adata.raw`. For a while now scanpy avoids filtering highly variable genes, but instead annotates them in `adata.var['highly_variable']` which is then used in `sc.pp.pca()`. I would suggest you use `subset=False` next time you use `sc.pp.highly_variable()` to avoid different dimensions in `adata.X` and `adata.raw.X`. You can easily proceed by just making a new anndata object from `adata.raw.X`, `adata.raw.var` and `adata.raw.obs` and storing this to be loaded into cellxgene. Just do the following:. ```. adata_raw = sc.AnnData(X=adata.raw.X, obs=adata.raw.obs, var=adata.raw.var). adata_raw.write(my_file). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:131,integrability,filter,filtering,131,"Hi @hejing3283,. The wrong shape is probably because you have subsetted `adata.X` to highly variable genes, or did some additional filtering after storing data in `adata.raw`. For a while now scanpy avoids filtering highly variable genes, but instead annotates them in `adata.var['highly_variable']` which is then used in `sc.pp.pca()`. I would suggest you use `subset=False` next time you use `sc.pp.highly_variable()` to avoid different dimensions in `adata.X` and `adata.raw.X`. You can easily proceed by just making a new anndata object from `adata.raw.X`, `adata.raw.var` and `adata.raw.obs` and storing this to be loaded into cellxgene. Just do the following:. ```. adata_raw = sc.AnnData(X=adata.raw.X, obs=adata.raw.obs, var=adata.raw.var). adata_raw.write(my_file). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:206,integrability,filter,filtering,206,"Hi @hejing3283,. The wrong shape is probably because you have subsetted `adata.X` to highly variable genes, or did some additional filtering after storing data in `adata.raw`. For a while now scanpy avoids filtering highly variable genes, but instead annotates them in `adata.var['highly_variable']` which is then used in `sc.pp.pca()`. I would suggest you use `subset=False` next time you use `sc.pp.highly_variable()` to avoid different dimensions in `adata.X` and `adata.raw.X`. You can easily proceed by just making a new anndata object from `adata.raw.X`, `adata.raw.var` and `adata.raw.obs` and storing this to be loaded into cellxgene. Just do the following:. ```. adata_raw = sc.AnnData(X=adata.raw.X, obs=adata.raw.obs, var=adata.raw.var). adata_raw.write(my_file). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:362,integrability,sub,subset,362,"Hi @hejing3283,. The wrong shape is probably because you have subsetted `adata.X` to highly variable genes, or did some additional filtering after storing data in `adata.raw`. For a while now scanpy avoids filtering highly variable genes, but instead annotates them in `adata.var['highly_variable']` which is then used in `sc.pp.pca()`. I would suggest you use `subset=False` next time you use `sc.pp.highly_variable()` to avoid different dimensions in `adata.X` and `adata.raw.X`. You can easily proceed by just making a new anndata object from `adata.raw.X`, `adata.raw.var` and `adata.raw.obs` and storing this to be loaded into cellxgene. Just do the following:. ```. adata_raw = sc.AnnData(X=adata.raw.X, obs=adata.raw.obs, var=adata.raw.var). adata_raw.write(my_file). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:92,modifiability,variab,variable,92,"Hi @hejing3283,. The wrong shape is probably because you have subsetted `adata.X` to highly variable genes, or did some additional filtering after storing data in `adata.raw`. For a while now scanpy avoids filtering highly variable genes, but instead annotates them in `adata.var['highly_variable']` which is then used in `sc.pp.pca()`. I would suggest you use `subset=False` next time you use `sc.pp.highly_variable()` to avoid different dimensions in `adata.X` and `adata.raw.X`. You can easily proceed by just making a new anndata object from `adata.raw.X`, `adata.raw.var` and `adata.raw.obs` and storing this to be loaded into cellxgene. Just do the following:. ```. adata_raw = sc.AnnData(X=adata.raw.X, obs=adata.raw.obs, var=adata.raw.var). adata_raw.write(my_file). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:223,modifiability,variab,variable,223,"Hi @hejing3283,. The wrong shape is probably because you have subsetted `adata.X` to highly variable genes, or did some additional filtering after storing data in `adata.raw`. For a while now scanpy avoids filtering highly variable genes, but instead annotates them in `adata.var['highly_variable']` which is then used in `sc.pp.pca()`. I would suggest you use `subset=False` next time you use `sc.pp.highly_variable()` to avoid different dimensions in `adata.X` and `adata.raw.X`. You can easily proceed by just making a new anndata object from `adata.raw.X`, `adata.raw.var` and `adata.raw.obs` and storing this to be loaded into cellxgene. Just do the following:. ```. adata_raw = sc.AnnData(X=adata.raw.X, obs=adata.raw.obs, var=adata.raw.var). adata_raw.write(my_file). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:381,performance,time,time,381,"Hi @hejing3283,. The wrong shape is probably because you have subsetted `adata.X` to highly variable genes, or did some additional filtering after storing data in `adata.raw`. For a while now scanpy avoids filtering highly variable genes, but instead annotates them in `adata.var['highly_variable']` which is then used in `sc.pp.pca()`. I would suggest you use `subset=False` next time you use `sc.pp.highly_variable()` to avoid different dimensions in `adata.X` and `adata.raw.X`. You can easily proceed by just making a new anndata object from `adata.raw.X`, `adata.raw.var` and `adata.raw.obs` and storing this to be loaded into cellxgene. Just do the following:. ```. adata_raw = sc.AnnData(X=adata.raw.X, obs=adata.raw.obs, var=adata.raw.var). adata_raw.write(my_file). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:620,performance,load,loaded,620,"Hi @hejing3283,. The wrong shape is probably because you have subsetted `adata.X` to highly variable genes, or did some additional filtering after storing data in `adata.raw`. For a while now scanpy avoids filtering highly variable genes, but instead annotates them in `adata.var['highly_variable']` which is then used in `sc.pp.pca()`. I would suggest you use `subset=False` next time you use `sc.pp.highly_variable()` to avoid different dimensions in `adata.X` and `adata.raw.X`. You can easily proceed by just making a new anndata object from `adata.raw.X`, `adata.raw.var` and `adata.raw.obs` and storing this to be loaded into cellxgene. Just do the following:. ```. adata_raw = sc.AnnData(X=adata.raw.X, obs=adata.raw.obs, var=adata.raw.var). adata_raw.write(my_file). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:199,safety,avoid,avoids,199,"Hi @hejing3283,. The wrong shape is probably because you have subsetted `adata.X` to highly variable genes, or did some additional filtering after storing data in `adata.raw`. For a while now scanpy avoids filtering highly variable genes, but instead annotates them in `adata.var['highly_variable']` which is then used in `sc.pp.pca()`. I would suggest you use `subset=False` next time you use `sc.pp.highly_variable()` to avoid different dimensions in `adata.X` and `adata.raw.X`. You can easily proceed by just making a new anndata object from `adata.raw.X`, `adata.raw.var` and `adata.raw.obs` and storing this to be loaded into cellxgene. Just do the following:. ```. adata_raw = sc.AnnData(X=adata.raw.X, obs=adata.raw.obs, var=adata.raw.var). adata_raw.write(my_file). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:423,safety,avoid,avoid,423,"Hi @hejing3283,. The wrong shape is probably because you have subsetted `adata.X` to highly variable genes, or did some additional filtering after storing data in `adata.raw`. For a while now scanpy avoids filtering highly variable genes, but instead annotates them in `adata.var['highly_variable']` which is then used in `sc.pp.pca()`. I would suggest you use `subset=False` next time you use `sc.pp.highly_variable()` to avoid different dimensions in `adata.X` and `adata.raw.X`. You can easily proceed by just making a new anndata object from `adata.raw.X`, `adata.raw.var` and `adata.raw.obs` and storing this to be loaded into cellxgene. Just do the following:. ```. adata_raw = sc.AnnData(X=adata.raw.X, obs=adata.raw.obs, var=adata.raw.var). adata_raw.write(my_file). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:778,energy efficiency,load,loaded,778,"Hi, . Thanks so much for the explanations! Doing it now and it works. . Best,. Jing. > On Jun 5, 2019, at 10:39, MalteDLuecken <notifications@github.com> wrote:. > . > Hi @hejing3283,. > . > The wrong shape is probably because you have subsetted adata.X to highly variable genes, or did some additional filtering after storing data in adata.raw. For a while now scanpy avoids filtering highly variable genes, but instead annotates them in adata.var['highly_variable'] which is then used in sc.pp.pca(). I would suggest you use subset=False next time you use sc.pp.highly_variable() to avoid different dimensions in adata.X and adata.raw.X. > . > You can easily proceed by just making a new anndata object from adata.raw.X, adata.raw.var and adata.raw.obs and storing this to be loaded into cellxgene. Just do the following:. > . > adata_raw.write(my_file). > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:236,integrability,sub,subsetted,236,"Hi, . Thanks so much for the explanations! Doing it now and it works. . Best,. Jing. > On Jun 5, 2019, at 10:39, MalteDLuecken <notifications@github.com> wrote:. > . > Hi @hejing3283,. > . > The wrong shape is probably because you have subsetted adata.X to highly variable genes, or did some additional filtering after storing data in adata.raw. For a while now scanpy avoids filtering highly variable genes, but instead annotates them in adata.var['highly_variable'] which is then used in sc.pp.pca(). I would suggest you use subset=False next time you use sc.pp.highly_variable() to avoid different dimensions in adata.X and adata.raw.X. > . > You can easily proceed by just making a new anndata object from adata.raw.X, adata.raw.var and adata.raw.obs and storing this to be loaded into cellxgene. Just do the following:. > . > adata_raw.write(my_file). > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:303,integrability,filter,filtering,303,"Hi, . Thanks so much for the explanations! Doing it now and it works. . Best,. Jing. > On Jun 5, 2019, at 10:39, MalteDLuecken <notifications@github.com> wrote:. > . > Hi @hejing3283,. > . > The wrong shape is probably because you have subsetted adata.X to highly variable genes, or did some additional filtering after storing data in adata.raw. For a while now scanpy avoids filtering highly variable genes, but instead annotates them in adata.var['highly_variable'] which is then used in sc.pp.pca(). I would suggest you use subset=False next time you use sc.pp.highly_variable() to avoid different dimensions in adata.X and adata.raw.X. > . > You can easily proceed by just making a new anndata object from adata.raw.X, adata.raw.var and adata.raw.obs and storing this to be loaded into cellxgene. Just do the following:. > . > adata_raw.write(my_file). > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:376,integrability,filter,filtering,376,"Hi, . Thanks so much for the explanations! Doing it now and it works. . Best,. Jing. > On Jun 5, 2019, at 10:39, MalteDLuecken <notifications@github.com> wrote:. > . > Hi @hejing3283,. > . > The wrong shape is probably because you have subsetted adata.X to highly variable genes, or did some additional filtering after storing data in adata.raw. For a while now scanpy avoids filtering highly variable genes, but instead annotates them in adata.var['highly_variable'] which is then used in sc.pp.pca(). I would suggest you use subset=False next time you use sc.pp.highly_variable() to avoid different dimensions in adata.X and adata.raw.X. > . > You can easily proceed by just making a new anndata object from adata.raw.X, adata.raw.var and adata.raw.obs and storing this to be loaded into cellxgene. Just do the following:. > . > adata_raw.write(my_file). > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:527,integrability,sub,subset,527,"Hi, . Thanks so much for the explanations! Doing it now and it works. . Best,. Jing. > On Jun 5, 2019, at 10:39, MalteDLuecken <notifications@github.com> wrote:. > . > Hi @hejing3283,. > . > The wrong shape is probably because you have subsetted adata.X to highly variable genes, or did some additional filtering after storing data in adata.raw. For a while now scanpy avoids filtering highly variable genes, but instead annotates them in adata.var['highly_variable'] which is then used in sc.pp.pca(). I would suggest you use subset=False next time you use sc.pp.highly_variable() to avoid different dimensions in adata.X and adata.raw.X. > . > You can easily proceed by just making a new anndata object from adata.raw.X, adata.raw.var and adata.raw.obs and storing this to be loaded into cellxgene. Just do the following:. > . > adata_raw.write(my_file). > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:264,modifiability,variab,variable,264,"Hi, . Thanks so much for the explanations! Doing it now and it works. . Best,. Jing. > On Jun 5, 2019, at 10:39, MalteDLuecken <notifications@github.com> wrote:. > . > Hi @hejing3283,. > . > The wrong shape is probably because you have subsetted adata.X to highly variable genes, or did some additional filtering after storing data in adata.raw. For a while now scanpy avoids filtering highly variable genes, but instead annotates them in adata.var['highly_variable'] which is then used in sc.pp.pca(). I would suggest you use subset=False next time you use sc.pp.highly_variable() to avoid different dimensions in adata.X and adata.raw.X. > . > You can easily proceed by just making a new anndata object from adata.raw.X, adata.raw.var and adata.raw.obs and storing this to be loaded into cellxgene. Just do the following:. > . > adata_raw.write(my_file). > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:393,modifiability,variab,variable,393,"Hi, . Thanks so much for the explanations! Doing it now and it works. . Best,. Jing. > On Jun 5, 2019, at 10:39, MalteDLuecken <notifications@github.com> wrote:. > . > Hi @hejing3283,. > . > The wrong shape is probably because you have subsetted adata.X to highly variable genes, or did some additional filtering after storing data in adata.raw. For a while now scanpy avoids filtering highly variable genes, but instead annotates them in adata.var['highly_variable'] which is then used in sc.pp.pca(). I would suggest you use subset=False next time you use sc.pp.highly_variable() to avoid different dimensions in adata.X and adata.raw.X. > . > You can easily proceed by just making a new anndata object from adata.raw.X, adata.raw.var and adata.raw.obs and storing this to be loaded into cellxgene. Just do the following:. > . > adata_raw.write(my_file). > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:545,performance,time,time,545,"Hi, . Thanks so much for the explanations! Doing it now and it works. . Best,. Jing. > On Jun 5, 2019, at 10:39, MalteDLuecken <notifications@github.com> wrote:. > . > Hi @hejing3283,. > . > The wrong shape is probably because you have subsetted adata.X to highly variable genes, or did some additional filtering after storing data in adata.raw. For a while now scanpy avoids filtering highly variable genes, but instead annotates them in adata.var['highly_variable'] which is then used in sc.pp.pca(). I would suggest you use subset=False next time you use sc.pp.highly_variable() to avoid different dimensions in adata.X and adata.raw.X. > . > You can easily proceed by just making a new anndata object from adata.raw.X, adata.raw.var and adata.raw.obs and storing this to be loaded into cellxgene. Just do the following:. > . > adata_raw.write(my_file). > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:778,performance,load,loaded,778,"Hi, . Thanks so much for the explanations! Doing it now and it works. . Best,. Jing. > On Jun 5, 2019, at 10:39, MalteDLuecken <notifications@github.com> wrote:. > . > Hi @hejing3283,. > . > The wrong shape is probably because you have subsetted adata.X to highly variable genes, or did some additional filtering after storing data in adata.raw. For a while now scanpy avoids filtering highly variable genes, but instead annotates them in adata.var['highly_variable'] which is then used in sc.pp.pca(). I would suggest you use subset=False next time you use sc.pp.highly_variable() to avoid different dimensions in adata.X and adata.raw.X. > . > You can easily proceed by just making a new anndata object from adata.raw.X, adata.raw.var and adata.raw.obs and storing this to be loaded into cellxgene. Just do the following:. > . > adata_raw.write(my_file). > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:369,safety,avoid,avoids,369,"Hi, . Thanks so much for the explanations! Doing it now and it works. . Best,. Jing. > On Jun 5, 2019, at 10:39, MalteDLuecken <notifications@github.com> wrote:. > . > Hi @hejing3283,. > . > The wrong shape is probably because you have subsetted adata.X to highly variable genes, or did some additional filtering after storing data in adata.raw. For a while now scanpy avoids filtering highly variable genes, but instead annotates them in adata.var['highly_variable'] which is then used in sc.pp.pca(). I would suggest you use subset=False next time you use sc.pp.highly_variable() to avoid different dimensions in adata.X and adata.raw.X. > . > You can easily proceed by just making a new anndata object from adata.raw.X, adata.raw.var and adata.raw.obs and storing this to be loaded into cellxgene. Just do the following:. > . > adata_raw.write(my_file). > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:585,safety,avoid,avoid,585,"Hi, . Thanks so much for the explanations! Doing it now and it works. . Best,. Jing. > On Jun 5, 2019, at 10:39, MalteDLuecken <notifications@github.com> wrote:. > . > Hi @hejing3283,. > . > The wrong shape is probably because you have subsetted adata.X to highly variable genes, or did some additional filtering after storing data in adata.raw. For a while now scanpy avoids filtering highly variable genes, but instead annotates them in adata.var['highly_variable'] which is then used in sc.pp.pca(). I would suggest you use subset=False next time you use sc.pp.highly_variable() to avoid different dimensions in adata.X and adata.raw.X. > . > You can easily proceed by just making a new anndata object from adata.raw.X, adata.raw.var and adata.raw.obs and storing this to be loaded into cellxgene. Just do the following:. > . > adata_raw.write(my_file). > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:104,interoperability,format,format,104,"Here's my solution to a similar inter-operability hiccup. This produces files similar to 10X v2 triplet format, plus an extra cell metadata file. ```py. pd.DataFrame(ad.var.index).to_csv(os.path.join(destination, ""genes.tsv"" ), sep = ""\t"", index_col = False). pd.DataFrame(ad.obs.index).to_csv(os.path.join(destination, ""barcodes.tsv""), sep = ""\t"", index_col = False). ad.obs.to_csv(os.path.join(destination, ""metadata.tsv""), sep = ""\t"", index_col = True). scipy.io.mmwrite(os.path.join(destination, ""matrix.mtx""), ad.X). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:23,interoperability,format,format,23,You can convert _h5ad_ format to _Seurat_ object using [sceasy](https://github.com/cellgeni/sceasy).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:138,availability,error,error,138,"> ```python. > scipy.io.mmwrite. > ```. This code doesn't actually work - rows and columns are switched in the matrix, and it produces an error when you try to read in the output using either `Scanpy` or `Seurat` wrapper functions. Perhaps it's a package version thing though..",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:255,deployability,version,version,255,"> ```python. > scipy.io.mmwrite. > ```. This code doesn't actually work - rows and columns are switched in the matrix, and it produces an error when you try to read in the output using either `Scanpy` or `Seurat` wrapper functions. Perhaps it's a package version thing though..",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:213,integrability,wrap,wrapper,213,"> ```python. > scipy.io.mmwrite. > ```. This code doesn't actually work - rows and columns are switched in the matrix, and it produces an error when you try to read in the output using either `Scanpy` or `Seurat` wrapper functions. Perhaps it's a package version thing though..",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:255,integrability,version,version,255,"> ```python. > scipy.io.mmwrite. > ```. This code doesn't actually work - rows and columns are switched in the matrix, and it produces an error when you try to read in the output using either `Scanpy` or `Seurat` wrapper functions. Perhaps it's a package version thing though..",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:213,interoperability,wrapper,wrapper,213,"> ```python. > scipy.io.mmwrite. > ```. This code doesn't actually work - rows and columns are switched in the matrix, and it produces an error when you try to read in the output using either `Scanpy` or `Seurat` wrapper functions. Perhaps it's a package version thing though..",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:247,modifiability,pac,package,247,"> ```python. > scipy.io.mmwrite. > ```. This code doesn't actually work - rows and columns are switched in the matrix, and it produces an error when you try to read in the output using either `Scanpy` or `Seurat` wrapper functions. Perhaps it's a package version thing though..",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:255,modifiability,version,version,255,"> ```python. > scipy.io.mmwrite. > ```. This code doesn't actually work - rows and columns are switched in the matrix, and it produces an error when you try to read in the output using either `Scanpy` or `Seurat` wrapper functions. Perhaps it's a package version thing though..",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:138,performance,error,error,138,"> ```python. > scipy.io.mmwrite. > ```. This code doesn't actually work - rows and columns are switched in the matrix, and it produces an error when you try to read in the output using either `Scanpy` or `Seurat` wrapper functions. Perhaps it's a package version thing though..",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:50,reliability,doe,doesn,50,"> ```python. > scipy.io.mmwrite. > ```. This code doesn't actually work - rows and columns are switched in the matrix, and it produces an error when you try to read in the output using either `Scanpy` or `Seurat` wrapper functions. Perhaps it's a package version thing though..",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:138,safety,error,error,138,"> ```python. > scipy.io.mmwrite. > ```. This code doesn't actually work - rows and columns are switched in the matrix, and it produces an error when you try to read in the output using either `Scanpy` or `Seurat` wrapper functions. Perhaps it's a package version thing though..",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:138,usability,error,error,138,"> ```python. > scipy.io.mmwrite. > ```. This code doesn't actually work - rows and columns are switched in the matrix, and it produces an error when you try to read in the output using either `Scanpy` or `Seurat` wrapper functions. Perhaps it's a package version thing though..",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:150,availability,error,error,150,"> > ```python. > > scipy.io.mmwrite. > > ```. > . > This code doesn't actually work - rows and columns are switched in the matrix, and it produces an error when you try to read in the output using either `Scanpy` or `Seurat` wrapper functions. Perhaps it's a package version thing though.. I was having the same issue as well. I ended up doing what was suggested above:. `adata.T.to_df().to_csv('matrix.csv')`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:267,deployability,version,version,267,"> > ```python. > > scipy.io.mmwrite. > > ```. > . > This code doesn't actually work - rows and columns are switched in the matrix, and it produces an error when you try to read in the output using either `Scanpy` or `Seurat` wrapper functions. Perhaps it's a package version thing though.. I was having the same issue as well. I ended up doing what was suggested above:. `adata.T.to_df().to_csv('matrix.csv')`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:225,integrability,wrap,wrapper,225,"> > ```python. > > scipy.io.mmwrite. > > ```. > . > This code doesn't actually work - rows and columns are switched in the matrix, and it produces an error when you try to read in the output using either `Scanpy` or `Seurat` wrapper functions. Perhaps it's a package version thing though.. I was having the same issue as well. I ended up doing what was suggested above:. `adata.T.to_df().to_csv('matrix.csv')`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:267,integrability,version,version,267,"> > ```python. > > scipy.io.mmwrite. > > ```. > . > This code doesn't actually work - rows and columns are switched in the matrix, and it produces an error when you try to read in the output using either `Scanpy` or `Seurat` wrapper functions. Perhaps it's a package version thing though.. I was having the same issue as well. I ended up doing what was suggested above:. `adata.T.to_df().to_csv('matrix.csv')`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:225,interoperability,wrapper,wrapper,225,"> > ```python. > > scipy.io.mmwrite. > > ```. > . > This code doesn't actually work - rows and columns are switched in the matrix, and it produces an error when you try to read in the output using either `Scanpy` or `Seurat` wrapper functions. Perhaps it's a package version thing though.. I was having the same issue as well. I ended up doing what was suggested above:. `adata.T.to_df().to_csv('matrix.csv')`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:259,modifiability,pac,package,259,"> > ```python. > > scipy.io.mmwrite. > > ```. > . > This code doesn't actually work - rows and columns are switched in the matrix, and it produces an error when you try to read in the output using either `Scanpy` or `Seurat` wrapper functions. Perhaps it's a package version thing though.. I was having the same issue as well. I ended up doing what was suggested above:. `adata.T.to_df().to_csv('matrix.csv')`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:267,modifiability,version,version,267,"> > ```python. > > scipy.io.mmwrite. > > ```. > . > This code doesn't actually work - rows and columns are switched in the matrix, and it produces an error when you try to read in the output using either `Scanpy` or `Seurat` wrapper functions. Perhaps it's a package version thing though.. I was having the same issue as well. I ended up doing what was suggested above:. `adata.T.to_df().to_csv('matrix.csv')`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:150,performance,error,error,150,"> > ```python. > > scipy.io.mmwrite. > > ```. > . > This code doesn't actually work - rows and columns are switched in the matrix, and it produces an error when you try to read in the output using either `Scanpy` or `Seurat` wrapper functions. Perhaps it's a package version thing though.. I was having the same issue as well. I ended up doing what was suggested above:. `adata.T.to_df().to_csv('matrix.csv')`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:62,reliability,doe,doesn,62,"> > ```python. > > scipy.io.mmwrite. > > ```. > . > This code doesn't actually work - rows and columns are switched in the matrix, and it produces an error when you try to read in the output using either `Scanpy` or `Seurat` wrapper functions. Perhaps it's a package version thing though.. I was having the same issue as well. I ended up doing what was suggested above:. `adata.T.to_df().to_csv('matrix.csv')`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:150,safety,error,error,150,"> > ```python. > > scipy.io.mmwrite. > > ```. > . > This code doesn't actually work - rows and columns are switched in the matrix, and it produces an error when you try to read in the output using either `Scanpy` or `Seurat` wrapper functions. Perhaps it's a package version thing though.. I was having the same issue as well. I ended up doing what was suggested above:. `adata.T.to_df().to_csv('matrix.csv')`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:150,usability,error,error,150,"> > ```python. > > scipy.io.mmwrite. > > ```. > . > This code doesn't actually work - rows and columns are switched in the matrix, and it produces an error when you try to read in the output using either `Scanpy` or `Seurat` wrapper functions. Perhaps it's a package version thing though.. I was having the same issue as well. I ended up doing what was suggested above:. `adata.T.to_df().to_csv('matrix.csv')`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:236,deployability,contain,containers,236,"We have this method in scanpy-scripts https://github.com/ebi-gene-expression-group/scanpy-scripts/blob/6297be21119d6964e074fa0b40a3b6fcaec53bbc/scanpy_scripts/cmd_utils.py#L137 - you could as well use it just from there with one of the containers https://quay.io/repository/biocontainers/scanpy-scripts?tab=tags&tag=latest I think it can be used through the filtering CLI call, given numbers that won't filter anything out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:263,integrability,repositor,repository,263,"We have this method in scanpy-scripts https://github.com/ebi-gene-expression-group/scanpy-scripts/blob/6297be21119d6964e074fa0b40a3b6fcaec53bbc/scanpy_scripts/cmd_utils.py#L137 - you could as well use it just from there with one of the containers https://quay.io/repository/biocontainers/scanpy-scripts?tab=tags&tag=latest I think it can be used through the filtering CLI call, given numbers that won't filter anything out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:358,integrability,filter,filtering,358,"We have this method in scanpy-scripts https://github.com/ebi-gene-expression-group/scanpy-scripts/blob/6297be21119d6964e074fa0b40a3b6fcaec53bbc/scanpy_scripts/cmd_utils.py#L137 - you could as well use it just from there with one of the containers https://quay.io/repository/biocontainers/scanpy-scripts?tab=tags&tag=latest I think it can be used through the filtering CLI call, given numbers that won't filter anything out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:403,integrability,filter,filter,403,"We have this method in scanpy-scripts https://github.com/ebi-gene-expression-group/scanpy-scripts/blob/6297be21119d6964e074fa0b40a3b6fcaec53bbc/scanpy_scripts/cmd_utils.py#L137 - you could as well use it just from there with one of the containers https://quay.io/repository/biocontainers/scanpy-scripts?tab=tags&tag=latest I think it can be used through the filtering CLI call, given numbers that won't filter anything out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/262:263,interoperability,repositor,repository,263,"We have this method in scanpy-scripts https://github.com/ebi-gene-expression-group/scanpy-scripts/blob/6297be21119d6964e074fa0b40a3b6fcaec53bbc/scanpy_scripts/cmd_utils.py#L137 - you could as well use it just from there with one of the containers https://quay.io/repository/biocontainers/scanpy-scripts?tab=tags&tag=latest I think it can be used through the filtering CLI call, given numbers that won't filter anything out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/262
https://github.com/scverse/scanpy/issues/263:29,reliability,doe,does,29,Without setting `backed='r'` does it work?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263
https://github.com/scverse/scanpy/issues/263:289,availability,error,errors,289,"This works with a non-backed adata, this works:. ```python. sc.pl.pca(adata[:, :5], color=""0""). ```. I'm not totally sure what exactly the other backed modes are supposed to do (especially when called instantiated by `read`, but none of them work either. `r`, `r+`, and `a` return similar errors, while `x`, `w`, and `w+` don't work, but that's because the file exists.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263
https://github.com/scverse/scanpy/issues/263:289,performance,error,errors,289,"This works with a non-backed adata, this works:. ```python. sc.pl.pca(adata[:, :5], color=""0""). ```. I'm not totally sure what exactly the other backed modes are supposed to do (especially when called instantiated by `read`, but none of them work either. `r`, `r+`, and `a` return similar errors, while `x`, `w`, and `w+` don't work, but that's because the file exists.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263
https://github.com/scverse/scanpy/issues/263:289,safety,error,errors,289,"This works with a non-backed adata, this works:. ```python. sc.pl.pca(adata[:, :5], color=""0""). ```. I'm not totally sure what exactly the other backed modes are supposed to do (especially when called instantiated by `read`, but none of them work either. `r`, `r+`, and `a` return similar errors, while `x`, `w`, and `w+` don't work, but that's because the file exists.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263
https://github.com/scverse/scanpy/issues/263:289,usability,error,errors,289,"This works with a non-backed adata, this works:. ```python. sc.pl.pca(adata[:, :5], color=""0""). ```. I'm not totally sure what exactly the other backed modes are supposed to do (especially when called instantiated by `read`, but none of them work either. `r`, `r+`, and `a` return similar errors, while `x`, `w`, and `w+` don't work, but that's because the file exists.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263
https://github.com/scverse/scanpy/issues/263:168,availability,error,error,168,"Oh, and I just realized it's meant to be `backed=False` and `backed=True`. I'd assumed the documentation was wrong, as `backed=True` and `backed=""True""` both throw the error:. ```python. ValueError: Invalid mode; must be one of r, r+, w, w-, x, a. ```. And I'd never checked `backed=False`. That's probably more for another issue though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263
https://github.com/scverse/scanpy/issues/263:168,performance,error,error,168,"Oh, and I just realized it's meant to be `backed=False` and `backed=True`. I'd assumed the documentation was wrong, as `backed=True` and `backed=""True""` both throw the error:. ```python. ValueError: Invalid mode; must be one of r, r+, w, w-, x, a. ```. And I'd never checked `backed=False`. That's probably more for another issue though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263
https://github.com/scverse/scanpy/issues/263:168,safety,error,error,168,"Oh, and I just realized it's meant to be `backed=False` and `backed=True`. I'd assumed the documentation was wrong, as `backed=True` and `backed=""True""` both throw the error:. ```python. ValueError: Invalid mode; must be one of r, r+, w, w-, x, a. ```. And I'd never checked `backed=False`. That's probably more for another issue though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263
https://github.com/scverse/scanpy/issues/263:91,usability,document,documentation,91,"Oh, and I just realized it's meant to be `backed=False` and `backed=True`. I'd assumed the documentation was wrong, as `backed=True` and `backed=""True""` both throw the error:. ```python. ValueError: Invalid mode; must be one of r, r+, w, w-, x, a. ```. And I'd never checked `backed=False`. That's probably more for another issue though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263
https://github.com/scverse/scanpy/issues/263:168,usability,error,error,168,"Oh, and I just realized it's meant to be `backed=False` and `backed=True`. I'd assumed the documentation was wrong, as `backed=True` and `backed=""True""` both throw the error:. ```python. ValueError: Invalid mode; must be one of r, r+, w, w-, x, a. ```. And I'd never checked `backed=False`. That's probably more for another issue though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263
https://github.com/scverse/scanpy/issues/263:0,availability,Sli,Slicing,0,"Slicing backed AnnData objects is not fully stable, yet. It's a bit tricky as `h5py.datasets` don't support the same general indexing operations as `AnnData`. The sliced AnnData should not be in backed mode [think of it as loading a small portion of the data into memory]. As for https://github.com/theislab/anndata/issues/61, @Sergei, do you have bandwidth? You're still the person who would be supposed to make the backed mode fully functional.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263
https://github.com/scverse/scanpy/issues/263:134,availability,operat,operations,134,"Slicing backed AnnData objects is not fully stable, yet. It's a bit tricky as `h5py.datasets` don't support the same general indexing operations as `AnnData`. The sliced AnnData should not be in backed mode [think of it as loading a small portion of the data into memory]. As for https://github.com/theislab/anndata/issues/61, @Sergei, do you have bandwidth? You're still the person who would be supposed to make the backed mode fully functional.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263
https://github.com/scverse/scanpy/issues/263:163,availability,sli,sliced,163,"Slicing backed AnnData objects is not fully stable, yet. It's a bit tricky as `h5py.datasets` don't support the same general indexing operations as `AnnData`. The sliced AnnData should not be in backed mode [think of it as loading a small portion of the data into memory]. As for https://github.com/theislab/anndata/issues/61, @Sergei, do you have bandwidth? You're still the person who would be supposed to make the backed mode fully functional.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263
https://github.com/scverse/scanpy/issues/263:223,energy efficiency,load,loading,223,"Slicing backed AnnData objects is not fully stable, yet. It's a bit tricky as `h5py.datasets` don't support the same general indexing operations as `AnnData`. The sliced AnnData should not be in backed mode [think of it as loading a small portion of the data into memory]. As for https://github.com/theislab/anndata/issues/61, @Sergei, do you have bandwidth? You're still the person who would be supposed to make the backed mode fully functional.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263
https://github.com/scverse/scanpy/issues/263:223,performance,load,loading,223,"Slicing backed AnnData objects is not fully stable, yet. It's a bit tricky as `h5py.datasets` don't support the same general indexing operations as `AnnData`. The sliced AnnData should not be in backed mode [think of it as loading a small portion of the data into memory]. As for https://github.com/theislab/anndata/issues/61, @Sergei, do you have bandwidth? You're still the person who would be supposed to make the backed mode fully functional.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263
https://github.com/scverse/scanpy/issues/263:264,performance,memor,memory,264,"Slicing backed AnnData objects is not fully stable, yet. It's a bit tricky as `h5py.datasets` don't support the same general indexing operations as `AnnData`. The sliced AnnData should not be in backed mode [think of it as loading a small portion of the data into memory]. As for https://github.com/theislab/anndata/issues/61, @Sergei, do you have bandwidth? You're still the person who would be supposed to make the backed mode fully functional.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263
https://github.com/scverse/scanpy/issues/263:0,reliability,Sli,Slicing,0,"Slicing backed AnnData objects is not fully stable, yet. It's a bit tricky as `h5py.datasets` don't support the same general indexing operations as `AnnData`. The sliced AnnData should not be in backed mode [think of it as loading a small portion of the data into memory]. As for https://github.com/theislab/anndata/issues/61, @Sergei, do you have bandwidth? You're still the person who would be supposed to make the backed mode fully functional.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263
https://github.com/scverse/scanpy/issues/263:163,reliability,sli,sliced,163,"Slicing backed AnnData objects is not fully stable, yet. It's a bit tricky as `h5py.datasets` don't support the same general indexing operations as `AnnData`. The sliced AnnData should not be in backed mode [think of it as loading a small portion of the data into memory]. As for https://github.com/theislab/anndata/issues/61, @Sergei, do you have bandwidth? You're still the person who would be supposed to make the backed mode fully functional.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263
https://github.com/scverse/scanpy/issues/263:100,usability,support,support,100,"Slicing backed AnnData objects is not fully stable, yet. It's a bit tricky as `h5py.datasets` don't support the same general indexing operations as `AnnData`. The sliced AnnData should not be in backed mode [think of it as loading a small portion of the data into memory]. As for https://github.com/theislab/anndata/issues/61, @Sergei, do you have bandwidth? You're still the person who would be supposed to make the backed mode fully functional.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263
https://github.com/scverse/scanpy/issues/263:264,usability,memor,memory,264,"Slicing backed AnnData objects is not fully stable, yet. It's a bit tricky as `h5py.datasets` don't support the same general indexing operations as `AnnData`. The sliced AnnData should not be in backed mode [think of it as loading a small portion of the data into memory]. As for https://github.com/theislab/anndata/issues/61, @Sergei, do you have bandwidth? You're still the person who would be supposed to make the backed mode fully functional.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263
https://github.com/scverse/scanpy/issues/263:376,usability,person,person,376,"Slicing backed AnnData objects is not fully stable, yet. It's a bit tricky as `h5py.datasets` don't support the same general indexing operations as `AnnData`. The sliced AnnData should not be in backed mode [think of it as loading a small portion of the data into memory]. As for https://github.com/theislab/anndata/issues/61, @Sergei, do you have bandwidth? You're still the person who would be supposed to make the backed mode fully functional.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263
https://github.com/scverse/scanpy/issues/263:36,interoperability,standard,standard,36,"The other backed modes are just the standard modes of `h5py`... We'll expand the documentation when the time for backed calculations actually arrived. Right now, everyone working with up to 1M cells should do it in memory; it's just incredibly much faster and a few 10GBs of memory of 1M cells on a server are no problem these days.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263
https://github.com/scverse/scanpy/issues/263:104,performance,time,time,104,"The other backed modes are just the standard modes of `h5py`... We'll expand the documentation when the time for backed calculations actually arrived. Right now, everyone working with up to 1M cells should do it in memory; it's just incredibly much faster and a few 10GBs of memory of 1M cells on a server are no problem these days.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263
https://github.com/scverse/scanpy/issues/263:215,performance,memor,memory,215,"The other backed modes are just the standard modes of `h5py`... We'll expand the documentation when the time for backed calculations actually arrived. Right now, everyone working with up to 1M cells should do it in memory; it's just incredibly much faster and a few 10GBs of memory of 1M cells on a server are no problem these days.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263
https://github.com/scverse/scanpy/issues/263:275,performance,memor,memory,275,"The other backed modes are just the standard modes of `h5py`... We'll expand the documentation when the time for backed calculations actually arrived. Right now, everyone working with up to 1M cells should do it in memory; it's just incredibly much faster and a few 10GBs of memory of 1M cells on a server are no problem these days.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263
https://github.com/scverse/scanpy/issues/263:81,usability,document,documentation,81,"The other backed modes are just the standard modes of `h5py`... We'll expand the documentation when the time for backed calculations actually arrived. Right now, everyone working with up to 1M cells should do it in memory; it's just incredibly much faster and a few 10GBs of memory of 1M cells on a server are no problem these days.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263
https://github.com/scverse/scanpy/issues/263:215,usability,memor,memory,215,"The other backed modes are just the standard modes of `h5py`... We'll expand the documentation when the time for backed calculations actually arrived. Right now, everyone working with up to 1M cells should do it in memory; it's just incredibly much faster and a few 10GBs of memory of 1M cells on a server are no problem these days.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263
https://github.com/scverse/scanpy/issues/263:275,usability,memor,memory,275,"The other backed modes are just the standard modes of `h5py`... We'll expand the documentation when the time for backed calculations actually arrived. Right now, everyone working with up to 1M cells should do it in memory; it's just incredibly much faster and a few 10GBs of memory of 1M cells on a server are no problem these days.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263
https://github.com/scverse/scanpy/issues/263:82,performance,memor,memory,82,"Just a little about my use case for backed mode:. I run all of my computations in memory. However, the HPCs I have access to limit the ways I can use them interactively, so I like to do my visualization locally on a backed object. Personally, I really like this feature of Scanpy. Since I've been playing with more complicated visualization code, I'm starting to run into these issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263
https://github.com/scverse/scanpy/issues/263:315,safety,compl,complicated,315,"Just a little about my use case for backed mode:. I run all of my computations in memory. However, the HPCs I have access to limit the ways I can use them interactively, so I like to do my visualization locally on a backed object. Personally, I really like this feature of Scanpy. Since I've been playing with more complicated visualization code, I'm starting to run into these issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263
https://github.com/scverse/scanpy/issues/263:115,security,access,access,115,"Just a little about my use case for backed mode:. I run all of my computations in memory. However, the HPCs I have access to limit the ways I can use them interactively, so I like to do my visualization locally on a backed object. Personally, I really like this feature of Scanpy. Since I've been playing with more complicated visualization code, I'm starting to run into these issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263
https://github.com/scverse/scanpy/issues/263:315,security,compl,complicated,315,"Just a little about my use case for backed mode:. I run all of my computations in memory. However, the HPCs I have access to limit the ways I can use them interactively, so I like to do my visualization locally on a backed object. Personally, I really like this feature of Scanpy. Since I've been playing with more complicated visualization code, I'm starting to run into these issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263
https://github.com/scverse/scanpy/issues/263:82,usability,memor,memory,82,"Just a little about my use case for backed mode:. I run all of my computations in memory. However, the HPCs I have access to limit the ways I can use them interactively, so I like to do my visualization locally on a backed object. Personally, I really like this feature of Scanpy. Since I've been playing with more complicated visualization code, I'm starting to run into these issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263
https://github.com/scverse/scanpy/issues/263:155,usability,interact,interactively,155,"Just a little about my use case for backed mode:. I run all of my computations in memory. However, the HPCs I have access to limit the ways I can use them interactively, so I like to do my visualization locally on a backed object. Personally, I really like this feature of Scanpy. Since I've been playing with more complicated visualization code, I'm starting to run into these issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263
https://github.com/scverse/scanpy/issues/263:189,usability,visual,visualization,189,"Just a little about my use case for backed mode:. I run all of my computations in memory. However, the HPCs I have access to limit the ways I can use them interactively, so I like to do my visualization locally on a backed object. Personally, I really like this feature of Scanpy. Since I've been playing with more complicated visualization code, I'm starting to run into these issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263
https://github.com/scverse/scanpy/issues/263:231,usability,Person,Personally,231,"Just a little about my use case for backed mode:. I run all of my computations in memory. However, the HPCs I have access to limit the ways I can use them interactively, so I like to do my visualization locally on a backed object. Personally, I really like this feature of Scanpy. Since I've been playing with more complicated visualization code, I'm starting to run into these issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263
https://github.com/scverse/scanpy/issues/263:327,usability,visual,visualization,327,"Just a little about my use case for backed mode:. I run all of my computations in memory. However, the HPCs I have access to limit the ways I can use them interactively, so I like to do my visualization locally on a backed object. Personally, I really like this feature of Scanpy. Since I've been playing with more complicated visualization code, I'm starting to run into these issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263
https://github.com/scverse/scanpy/issues/263:382,deployability,infrastructur,infrastructure,382,"OK, got it! So you're actually moving the data between your HPC and your laptop? Why does the typical forwarding via `ssh -L ...` that I think most people use for accessing the server running jupyter lab, notebooks, tensorboard etc. locally doesn't work for you (e.g. [here](http://benjlindsay.com/blog/running-jupyter-lab-remotely/))? I can do all the visualizations on the remote infrastructure and never have to move around data, which is nice. ;). Of course, though, the backed functionality of Scanpy should become fully stable.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263
https://github.com/scverse/scanpy/issues/263:85,reliability,doe,does,85,"OK, got it! So you're actually moving the data between your HPC and your laptop? Why does the typical forwarding via `ssh -L ...` that I think most people use for accessing the server running jupyter lab, notebooks, tensorboard etc. locally doesn't work for you (e.g. [here](http://benjlindsay.com/blog/running-jupyter-lab-remotely/))? I can do all the visualizations on the remote infrastructure and never have to move around data, which is nice. ;). Of course, though, the backed functionality of Scanpy should become fully stable.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263
https://github.com/scverse/scanpy/issues/263:241,reliability,doe,doesn,241,"OK, got it! So you're actually moving the data between your HPC and your laptop? Why does the typical forwarding via `ssh -L ...` that I think most people use for accessing the server running jupyter lab, notebooks, tensorboard etc. locally doesn't work for you (e.g. [here](http://benjlindsay.com/blog/running-jupyter-lab-remotely/))? I can do all the visualizations on the remote infrastructure and never have to move around data, which is nice. ;). Of course, though, the backed functionality of Scanpy should become fully stable.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263
https://github.com/scverse/scanpy/issues/263:118,security,ssh,ssh,118,"OK, got it! So you're actually moving the data between your HPC and your laptop? Why does the typical forwarding via `ssh -L ...` that I think most people use for accessing the server running jupyter lab, notebooks, tensorboard etc. locally doesn't work for you (e.g. [here](http://benjlindsay.com/blog/running-jupyter-lab-remotely/))? I can do all the visualizations on the remote infrastructure and never have to move around data, which is nice. ;). Of course, though, the backed functionality of Scanpy should become fully stable.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263
https://github.com/scverse/scanpy/issues/263:163,security,access,accessing,163,"OK, got it! So you're actually moving the data between your HPC and your laptop? Why does the typical forwarding via `ssh -L ...` that I think most people use for accessing the server running jupyter lab, notebooks, tensorboard etc. locally doesn't work for you (e.g. [here](http://benjlindsay.com/blog/running-jupyter-lab-remotely/))? I can do all the visualizations on the remote infrastructure and never have to move around data, which is nice. ;). Of course, though, the backed functionality of Scanpy should become fully stable.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263
https://github.com/scverse/scanpy/issues/263:353,usability,visual,visualizations,353,"OK, got it! So you're actually moving the data between your HPC and your laptop? Why does the typical forwarding via `ssh -L ...` that I think most people use for accessing the server running jupyter lab, notebooks, tensorboard etc. locally doesn't work for you (e.g. [here](http://benjlindsay.com/blog/running-jupyter-lab-remotely/))? I can do all the visualizations on the remote infrastructure and never have to move around data, which is nice. ;). Of course, though, the backed functionality of Scanpy should become fully stable.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263
https://github.com/scverse/scanpy/issues/263:561,deployability,infrastructur,infrastructure,561,"I don't know about Isaac, but our HPC nodes don't like long-running. jypyters on them. On Tue, Sep 18, 2018 at 9:32 AM, Alex Wolf <notifications@github.com> wrote:. > OK, got it! So you're actually moving the data between your HPC and your. > laptop? Why does the typical forwarding via ssh -L ... that I think most. > people use for accessing the server running jupyter lab, notebooks,. > tensorboard etc. locally doesn't work for you (e.g. here. > <http://benjlindsay.com/blog/running-jupyter-lab-remotely/>)? I can do. > all the visualizations on the remote infrastructure and never have to move. > around data, which is nice. ;). >. > Of course, though, the backed functionality of Scanpy should become fully. > stable. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/263#issuecomment-422394446>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AAS-TQtLnSFiZOCsL0oqDgJcrzo1VJXJks5ucPXugaJpZM4WokmY>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263
https://github.com/scverse/scanpy/issues/263:773,integrability,sub,subscribed,773,"I don't know about Isaac, but our HPC nodes don't like long-running. jypyters on them. On Tue, Sep 18, 2018 at 9:32 AM, Alex Wolf <notifications@github.com> wrote:. > OK, got it! So you're actually moving the data between your HPC and your. > laptop? Why does the typical forwarding via ssh -L ... that I think most. > people use for accessing the server running jupyter lab, notebooks,. > tensorboard etc. locally doesn't work for you (e.g. here. > <http://benjlindsay.com/blog/running-jupyter-lab-remotely/>)? I can do. > all the visualizations on the remote infrastructure and never have to move. > around data, which is nice. ;). >. > Of course, though, the backed functionality of Scanpy should become fully. > stable. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/263#issuecomment-422394446>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AAS-TQtLnSFiZOCsL0oqDgJcrzo1VJXJks5ucPXugaJpZM4WokmY>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263
https://github.com/scverse/scanpy/issues/263:255,reliability,doe,does,255,"I don't know about Isaac, but our HPC nodes don't like long-running. jypyters on them. On Tue, Sep 18, 2018 at 9:32 AM, Alex Wolf <notifications@github.com> wrote:. > OK, got it! So you're actually moving the data between your HPC and your. > laptop? Why does the typical forwarding via ssh -L ... that I think most. > people use for accessing the server running jupyter lab, notebooks,. > tensorboard etc. locally doesn't work for you (e.g. here. > <http://benjlindsay.com/blog/running-jupyter-lab-remotely/>)? I can do. > all the visualizations on the remote infrastructure and never have to move. > around data, which is nice. ;). >. > Of course, though, the backed functionality of Scanpy should become fully. > stable. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/263#issuecomment-422394446>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AAS-TQtLnSFiZOCsL0oqDgJcrzo1VJXJks5ucPXugaJpZM4WokmY>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263
https://github.com/scverse/scanpy/issues/263:415,reliability,doe,doesn,415,"I don't know about Isaac, but our HPC nodes don't like long-running. jypyters on them. On Tue, Sep 18, 2018 at 9:32 AM, Alex Wolf <notifications@github.com> wrote:. > OK, got it! So you're actually moving the data between your HPC and your. > laptop? Why does the typical forwarding via ssh -L ... that I think most. > people use for accessing the server running jupyter lab, notebooks,. > tensorboard etc. locally doesn't work for you (e.g. here. > <http://benjlindsay.com/blog/running-jupyter-lab-remotely/>)? I can do. > all the visualizations on the remote infrastructure and never have to move. > around data, which is nice. ;). >. > Of course, though, the backed functionality of Scanpy should become fully. > stable. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/263#issuecomment-422394446>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AAS-TQtLnSFiZOCsL0oqDgJcrzo1VJXJks5ucPXugaJpZM4WokmY>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263
https://github.com/scverse/scanpy/issues/263:287,security,ssh,ssh,287,"I don't know about Isaac, but our HPC nodes don't like long-running. jypyters on them. On Tue, Sep 18, 2018 at 9:32 AM, Alex Wolf <notifications@github.com> wrote:. > OK, got it! So you're actually moving the data between your HPC and your. > laptop? Why does the typical forwarding via ssh -L ... that I think most. > people use for accessing the server running jupyter lab, notebooks,. > tensorboard etc. locally doesn't work for you (e.g. here. > <http://benjlindsay.com/blog/running-jupyter-lab-remotely/>)? I can do. > all the visualizations on the remote infrastructure and never have to move. > around data, which is nice. ;). >. > Of course, though, the backed functionality of Scanpy should become fully. > stable. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/263#issuecomment-422394446>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AAS-TQtLnSFiZOCsL0oqDgJcrzo1VJXJks5ucPXugaJpZM4WokmY>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263
https://github.com/scverse/scanpy/issues/263:334,security,access,accessing,334,"I don't know about Isaac, but our HPC nodes don't like long-running. jypyters on them. On Tue, Sep 18, 2018 at 9:32 AM, Alex Wolf <notifications@github.com> wrote:. > OK, got it! So you're actually moving the data between your HPC and your. > laptop? Why does the typical forwarding via ssh -L ... that I think most. > people use for accessing the server running jupyter lab, notebooks,. > tensorboard etc. locally doesn't work for you (e.g. here. > <http://benjlindsay.com/blog/running-jupyter-lab-remotely/>)? I can do. > all the visualizations on the remote infrastructure and never have to move. > around data, which is nice. ;). >. > Of course, though, the backed functionality of Scanpy should become fully. > stable. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/263#issuecomment-422394446>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AAS-TQtLnSFiZOCsL0oqDgJcrzo1VJXJks5ucPXugaJpZM4WokmY>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263
https://github.com/scverse/scanpy/issues/263:996,security,auth,auth,996,"I don't know about Isaac, but our HPC nodes don't like long-running. jypyters on them. On Tue, Sep 18, 2018 at 9:32 AM, Alex Wolf <notifications@github.com> wrote:. > OK, got it! So you're actually moving the data between your HPC and your. > laptop? Why does the typical forwarding via ssh -L ... that I think most. > people use for accessing the server running jupyter lab, notebooks,. > tensorboard etc. locally doesn't work for you (e.g. here. > <http://benjlindsay.com/blog/running-jupyter-lab-remotely/>)? I can do. > all the visualizations on the remote infrastructure and never have to move. > around data, which is nice. ;). >. > Of course, though, the backed functionality of Scanpy should become fully. > stable. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/263#issuecomment-422394446>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AAS-TQtLnSFiZOCsL0oqDgJcrzo1VJXJks5ucPXugaJpZM4WokmY>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263
https://github.com/scverse/scanpy/issues/263:532,usability,visual,visualizations,532,"I don't know about Isaac, but our HPC nodes don't like long-running. jypyters on them. On Tue, Sep 18, 2018 at 9:32 AM, Alex Wolf <notifications@github.com> wrote:. > OK, got it! So you're actually moving the data between your HPC and your. > laptop? Why does the typical forwarding via ssh -L ... that I think most. > people use for accessing the server running jupyter lab, notebooks,. > tensorboard etc. locally doesn't work for you (e.g. here. > <http://benjlindsay.com/blog/running-jupyter-lab-remotely/>)? I can do. > all the visualizations on the remote infrastructure and never have to move. > around data, which is nice. ;). >. > Of course, though, the backed functionality of Scanpy should become fully. > stable. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/263#issuecomment-422394446>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AAS-TQtLnSFiZOCsL0oqDgJcrzo1VJXJks5ucPXugaJpZM4WokmY>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263
https://github.com/scverse/scanpy/issues/263:186,availability,down,down,186,"Yeah, two out of three of the HPCs I have access to don't make using a jupyter server particularly easy (one corporate firewall, one government). I use that with the other one, but it's down this week 😢",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263
https://github.com/scverse/scanpy/issues/263:42,security,access,access,42,"Yeah, two out of three of the HPCs I have access to don't make using a jupyter server particularly easy (one corporate firewall, one government). I use that with the other one, but it's down this week 😢",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263
https://github.com/scverse/scanpy/issues/263:119,security,firewal,firewall,119,"Yeah, two out of three of the HPCs I have access to don't make using a jupyter server particularly easy (one corporate firewall, one government). I use that with the other one, but it's down this week 😢",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263
https://github.com/scverse/scanpy/issues/263:133,security,govern,government,133,"Yeah, two out of three of the HPCs I have access to don't make using a jupyter server particularly easy (one corporate firewall, one government). I use that with the other one, but it's down this week 😢",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/263
https://github.com/scverse/scanpy/issues/264:101,energy efficiency,current,current,101,"Additionally, this only happens when PCA has been computed with `svd_solver='arpack'` parameter. The current default (`svd_solver='auto'`) produces an always non-zero variance explained for the components. Additionally the order of variance explained is wrong with the `svd_solver='arpack'` parameter. I attach some examples:. ```python. >>> sc.tl.pca(adata_h, svd_solver='arpack'). >>> adata_h.uns['pca']. {'variance': array([ 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 2633.797 , 457.86526 , 316.44687 ,. 237.71556 , 143.87927 , 119.6577 , 105.01371 , 91.51559 ,. 66.951355, 61.23979 , 59.957714, 58.998177, 57.82413 ],. dtype=float32),. 'variance_ratio': array([0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0.5971161 , 0.10380401, 0.07174262,. 0.05389321, 0.0326193 , 0.02712796, 0.02380797, 0.02074778,. 0.01517874, 0.01388386, 0.01359319, 0.01337565, 0.01310948],. dtype=float32)}. >>> sc.tl.pca(adata_h). Note that scikit-learn's randomized PCA might not be exactly reproducible across different computational platforms. For exact reproducibility, choose `svd_solver='arpack'.` This will likely become the Scanpy default in the future. >>> adata_h.uns['pca']. {'variance': array([2.63379761e+03, 4.57865112e+02, 3.16446930e+02, 2.37715851e+02,. 1.43879318e+02, 1.19657700e+02, 1.05013855e+02, 9.15156784e+01,. 6.69513855e+01, 6.12398453e+01, 5.99577942e+01, 5.89982376e+01,. 5.78241539e+01, 6.29622976e-09, 9.35712879e-12, 9.35712879e-12,. 9.35712879e-12, 9.35712879e-12, 9.35712879e-12, 9.35712879e-12,. 9.35712879e-12, 9.35712879e-12, 9.35712879e-12, 9.35712879e-12,. 9.35712879e-12, 9.35712879e-12, 9.35712879e-12, 9.35712879e-12,. 9.35712879e-12, 9.35712879e-12, 9.35712879e-12, 9.35712879e-12,",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/264
https://github.com/scverse/scanpy/issues/264:194,integrability,compon,components,194,"Additionally, this only happens when PCA has been computed with `svd_solver='arpack'` parameter. The current default (`svd_solver='auto'`) produces an always non-zero variance explained for the components. Additionally the order of variance explained is wrong with the `svd_solver='arpack'` parameter. I attach some examples:. ```python. >>> sc.tl.pca(adata_h, svd_solver='arpack'). >>> adata_h.uns['pca']. {'variance': array([ 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 2633.797 , 457.86526 , 316.44687 ,. 237.71556 , 143.87927 , 119.6577 , 105.01371 , 91.51559 ,. 66.951355, 61.23979 , 59.957714, 58.998177, 57.82413 ],. dtype=float32),. 'variance_ratio': array([0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0.5971161 , 0.10380401, 0.07174262,. 0.05389321, 0.0326193 , 0.02712796, 0.02380797, 0.02074778,. 0.01517874, 0.01388386, 0.01359319, 0.01337565, 0.01310948],. dtype=float32)}. >>> sc.tl.pca(adata_h). Note that scikit-learn's randomized PCA might not be exactly reproducible across different computational platforms. For exact reproducibility, choose `svd_solver='arpack'.` This will likely become the Scanpy default in the future. >>> adata_h.uns['pca']. {'variance': array([2.63379761e+03, 4.57865112e+02, 3.16446930e+02, 2.37715851e+02,. 1.43879318e+02, 1.19657700e+02, 1.05013855e+02, 9.15156784e+01,. 6.69513855e+01, 6.12398453e+01, 5.99577942e+01, 5.89982376e+01,. 5.78241539e+01, 6.29622976e-09, 9.35712879e-12, 9.35712879e-12,. 9.35712879e-12, 9.35712879e-12, 9.35712879e-12, 9.35712879e-12,. 9.35712879e-12, 9.35712879e-12, 9.35712879e-12, 9.35712879e-12,. 9.35712879e-12, 9.35712879e-12, 9.35712879e-12, 9.35712879e-12,. 9.35712879e-12, 9.35712879e-12, 9.35712879e-12, 9.35712879e-12,",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/264
https://github.com/scverse/scanpy/issues/264:194,interoperability,compon,components,194,"Additionally, this only happens when PCA has been computed with `svd_solver='arpack'` parameter. The current default (`svd_solver='auto'`) produces an always non-zero variance explained for the components. Additionally the order of variance explained is wrong with the `svd_solver='arpack'` parameter. I attach some examples:. ```python. >>> sc.tl.pca(adata_h, svd_solver='arpack'). >>> adata_h.uns['pca']. {'variance': array([ 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 2633.797 , 457.86526 , 316.44687 ,. 237.71556 , 143.87927 , 119.6577 , 105.01371 , 91.51559 ,. 66.951355, 61.23979 , 59.957714, 58.998177, 57.82413 ],. dtype=float32),. 'variance_ratio': array([0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0.5971161 , 0.10380401, 0.07174262,. 0.05389321, 0.0326193 , 0.02712796, 0.02380797, 0.02074778,. 0.01517874, 0.01388386, 0.01359319, 0.01337565, 0.01310948],. dtype=float32)}. >>> sc.tl.pca(adata_h). Note that scikit-learn's randomized PCA might not be exactly reproducible across different computational platforms. For exact reproducibility, choose `svd_solver='arpack'.` This will likely become the Scanpy default in the future. >>> adata_h.uns['pca']. {'variance': array([2.63379761e+03, 4.57865112e+02, 3.16446930e+02, 2.37715851e+02,. 1.43879318e+02, 1.19657700e+02, 1.05013855e+02, 9.15156784e+01,. 6.69513855e+01, 6.12398453e+01, 5.99577942e+01, 5.89982376e+01,. 5.78241539e+01, 6.29622976e-09, 9.35712879e-12, 9.35712879e-12,. 9.35712879e-12, 9.35712879e-12, 9.35712879e-12, 9.35712879e-12,. 9.35712879e-12, 9.35712879e-12, 9.35712879e-12, 9.35712879e-12,. 9.35712879e-12, 9.35712879e-12, 9.35712879e-12, 9.35712879e-12,. 9.35712879e-12, 9.35712879e-12, 9.35712879e-12, 9.35712879e-12,",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/264
https://github.com/scverse/scanpy/issues/264:1312,interoperability,platform,platforms,1312,"xamples:. ```python. >>> sc.tl.pca(adata_h, svd_solver='arpack'). >>> adata_h.uns['pca']. {'variance': array([ 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 2633.797 , 457.86526 , 316.44687 ,. 237.71556 , 143.87927 , 119.6577 , 105.01371 , 91.51559 ,. 66.951355, 61.23979 , 59.957714, 58.998177, 57.82413 ],. dtype=float32),. 'variance_ratio': array([0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0.5971161 , 0.10380401, 0.07174262,. 0.05389321, 0.0326193 , 0.02712796, 0.02380797, 0.02074778,. 0.01517874, 0.01388386, 0.01359319, 0.01337565, 0.01310948],. dtype=float32)}. >>> sc.tl.pca(adata_h). Note that scikit-learn's randomized PCA might not be exactly reproducible across different computational platforms. For exact reproducibility, choose `svd_solver='arpack'.` This will likely become the Scanpy default in the future. >>> adata_h.uns['pca']. {'variance': array([2.63379761e+03, 4.57865112e+02, 3.16446930e+02, 2.37715851e+02,. 1.43879318e+02, 1.19657700e+02, 1.05013855e+02, 9.15156784e+01,. 6.69513855e+01, 6.12398453e+01, 5.99577942e+01, 5.89982376e+01,. 5.78241539e+01, 6.29622976e-09, 9.35712879e-12, 9.35712879e-12,. 9.35712879e-12, 9.35712879e-12, 9.35712879e-12, 9.35712879e-12,. 9.35712879e-12, 9.35712879e-12, 9.35712879e-12, 9.35712879e-12,. 9.35712879e-12, 9.35712879e-12, 9.35712879e-12, 9.35712879e-12,. 9.35712879e-12, 9.35712879e-12, 9.35712879e-12, 9.35712879e-12,. 9.35712879e-12, 9.35712879e-12, 9.35712879e-12, 9.35712879e-12,. 9.35712879e-12, 9.35712879e-12, 9.35712879e-12, 9.35712879e-12,. 9.35712879e-12, 9.35712879e-12, 9.35712879e-12, 9.35712879e-12,. 9.35712879e-12, 9.35712879e-12, 9.35712879e-12, 9.35712879e-12,. 9.35712879e-12, 9.35712879e-12], dtype=float32),. 'vari",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/264
https://github.com/scverse/scanpy/issues/264:86,modifiability,paramet,parameter,86,"Additionally, this only happens when PCA has been computed with `svd_solver='arpack'` parameter. The current default (`svd_solver='auto'`) produces an always non-zero variance explained for the components. Additionally the order of variance explained is wrong with the `svd_solver='arpack'` parameter. I attach some examples:. ```python. >>> sc.tl.pca(adata_h, svd_solver='arpack'). >>> adata_h.uns['pca']. {'variance': array([ 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 2633.797 , 457.86526 , 316.44687 ,. 237.71556 , 143.87927 , 119.6577 , 105.01371 , 91.51559 ,. 66.951355, 61.23979 , 59.957714, 58.998177, 57.82413 ],. dtype=float32),. 'variance_ratio': array([0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0.5971161 , 0.10380401, 0.07174262,. 0.05389321, 0.0326193 , 0.02712796, 0.02380797, 0.02074778,. 0.01517874, 0.01388386, 0.01359319, 0.01337565, 0.01310948],. dtype=float32)}. >>> sc.tl.pca(adata_h). Note that scikit-learn's randomized PCA might not be exactly reproducible across different computational platforms. For exact reproducibility, choose `svd_solver='arpack'.` This will likely become the Scanpy default in the future. >>> adata_h.uns['pca']. {'variance': array([2.63379761e+03, 4.57865112e+02, 3.16446930e+02, 2.37715851e+02,. 1.43879318e+02, 1.19657700e+02, 1.05013855e+02, 9.15156784e+01,. 6.69513855e+01, 6.12398453e+01, 5.99577942e+01, 5.89982376e+01,. 5.78241539e+01, 6.29622976e-09, 9.35712879e-12, 9.35712879e-12,. 9.35712879e-12, 9.35712879e-12, 9.35712879e-12, 9.35712879e-12,. 9.35712879e-12, 9.35712879e-12, 9.35712879e-12, 9.35712879e-12,. 9.35712879e-12, 9.35712879e-12, 9.35712879e-12, 9.35712879e-12,. 9.35712879e-12, 9.35712879e-12, 9.35712879e-12, 9.35712879e-12,",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/264
https://github.com/scverse/scanpy/issues/264:194,modifiability,compon,components,194,"Additionally, this only happens when PCA has been computed with `svd_solver='arpack'` parameter. The current default (`svd_solver='auto'`) produces an always non-zero variance explained for the components. Additionally the order of variance explained is wrong with the `svd_solver='arpack'` parameter. I attach some examples:. ```python. >>> sc.tl.pca(adata_h, svd_solver='arpack'). >>> adata_h.uns['pca']. {'variance': array([ 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 2633.797 , 457.86526 , 316.44687 ,. 237.71556 , 143.87927 , 119.6577 , 105.01371 , 91.51559 ,. 66.951355, 61.23979 , 59.957714, 58.998177, 57.82413 ],. dtype=float32),. 'variance_ratio': array([0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0.5971161 , 0.10380401, 0.07174262,. 0.05389321, 0.0326193 , 0.02712796, 0.02380797, 0.02074778,. 0.01517874, 0.01388386, 0.01359319, 0.01337565, 0.01310948],. dtype=float32)}. >>> sc.tl.pca(adata_h). Note that scikit-learn's randomized PCA might not be exactly reproducible across different computational platforms. For exact reproducibility, choose `svd_solver='arpack'.` This will likely become the Scanpy default in the future. >>> adata_h.uns['pca']. {'variance': array([2.63379761e+03, 4.57865112e+02, 3.16446930e+02, 2.37715851e+02,. 1.43879318e+02, 1.19657700e+02, 1.05013855e+02, 9.15156784e+01,. 6.69513855e+01, 6.12398453e+01, 5.99577942e+01, 5.89982376e+01,. 5.78241539e+01, 6.29622976e-09, 9.35712879e-12, 9.35712879e-12,. 9.35712879e-12, 9.35712879e-12, 9.35712879e-12, 9.35712879e-12,. 9.35712879e-12, 9.35712879e-12, 9.35712879e-12, 9.35712879e-12,. 9.35712879e-12, 9.35712879e-12, 9.35712879e-12, 9.35712879e-12,. 9.35712879e-12, 9.35712879e-12, 9.35712879e-12, 9.35712879e-12,",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/264
https://github.com/scverse/scanpy/issues/264:291,modifiability,paramet,parameter,291,"Additionally, this only happens when PCA has been computed with `svd_solver='arpack'` parameter. The current default (`svd_solver='auto'`) produces an always non-zero variance explained for the components. Additionally the order of variance explained is wrong with the `svd_solver='arpack'` parameter. I attach some examples:. ```python. >>> sc.tl.pca(adata_h, svd_solver='arpack'). >>> adata_h.uns['pca']. {'variance': array([ 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 2633.797 , 457.86526 , 316.44687 ,. 237.71556 , 143.87927 , 119.6577 , 105.01371 , 91.51559 ,. 66.951355, 61.23979 , 59.957714, 58.998177, 57.82413 ],. dtype=float32),. 'variance_ratio': array([0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0.5971161 , 0.10380401, 0.07174262,. 0.05389321, 0.0326193 , 0.02712796, 0.02380797, 0.02074778,. 0.01517874, 0.01388386, 0.01359319, 0.01337565, 0.01310948],. dtype=float32)}. >>> sc.tl.pca(adata_h). Note that scikit-learn's randomized PCA might not be exactly reproducible across different computational platforms. For exact reproducibility, choose `svd_solver='arpack'.` This will likely become the Scanpy default in the future. >>> adata_h.uns['pca']. {'variance': array([2.63379761e+03, 4.57865112e+02, 3.16446930e+02, 2.37715851e+02,. 1.43879318e+02, 1.19657700e+02, 1.05013855e+02, 9.15156784e+01,. 6.69513855e+01, 6.12398453e+01, 5.99577942e+01, 5.89982376e+01,. 5.78241539e+01, 6.29622976e-09, 9.35712879e-12, 9.35712879e-12,. 9.35712879e-12, 9.35712879e-12, 9.35712879e-12, 9.35712879e-12,. 9.35712879e-12, 9.35712879e-12, 9.35712879e-12, 9.35712879e-12,. 9.35712879e-12, 9.35712879e-12, 9.35712879e-12, 9.35712879e-12,. 9.35712879e-12, 9.35712879e-12, 9.35712879e-12, 9.35712879e-12,",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/264
https://github.com/scverse/scanpy/issues/264:1224,usability,learn,learn,1224,"r of variance explained is wrong with the `svd_solver='arpack'` parameter. I attach some examples:. ```python. >>> sc.tl.pca(adata_h, svd_solver='arpack'). >>> adata_h.uns['pca']. {'variance': array([ 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 2633.797 , 457.86526 , 316.44687 ,. 237.71556 , 143.87927 , 119.6577 , 105.01371 , 91.51559 ,. 66.951355, 61.23979 , 59.957714, 58.998177, 57.82413 ],. dtype=float32),. 'variance_ratio': array([0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0. , 0. , 0. ,. 0. , 0. , 0.5971161 , 0.10380401, 0.07174262,. 0.05389321, 0.0326193 , 0.02712796, 0.02380797, 0.02074778,. 0.01517874, 0.01388386, 0.01359319, 0.01337565, 0.01310948],. dtype=float32)}. >>> sc.tl.pca(adata_h). Note that scikit-learn's randomized PCA might not be exactly reproducible across different computational platforms. For exact reproducibility, choose `svd_solver='arpack'.` This will likely become the Scanpy default in the future. >>> adata_h.uns['pca']. {'variance': array([2.63379761e+03, 4.57865112e+02, 3.16446930e+02, 2.37715851e+02,. 1.43879318e+02, 1.19657700e+02, 1.05013855e+02, 9.15156784e+01,. 6.69513855e+01, 6.12398453e+01, 5.99577942e+01, 5.89982376e+01,. 5.78241539e+01, 6.29622976e-09, 9.35712879e-12, 9.35712879e-12,. 9.35712879e-12, 9.35712879e-12, 9.35712879e-12, 9.35712879e-12,. 9.35712879e-12, 9.35712879e-12, 9.35712879e-12, 9.35712879e-12,. 9.35712879e-12, 9.35712879e-12, 9.35712879e-12, 9.35712879e-12,. 9.35712879e-12, 9.35712879e-12, 9.35712879e-12, 9.35712879e-12,. 9.35712879e-12, 9.35712879e-12, 9.35712879e-12, 9.35712879e-12,. 9.35712879e-12, 9.35712879e-12, 9.35712879e-12, 9.35712879e-12,. 9.35712879e-12, 9.35712879e-12, 9.35712879e-12, 9.35712879e-12,. 9.35712879e-12, 9.35712879e-12",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/264
https://github.com/scverse/scanpy/issues/264:53,usability,learn,learn,53,"Arrrgh, this prepending of the 0s is a bug in scikit-learn. Thank you very much for pointing it out. Let me briefly think about solving this elegantly...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/264
https://github.com/scverse/scanpy/issues/264:281,availability,error,error,281,"Still occurring, though it's a really weird case. Here's some code to reproduce:. ```python. a = sc.AnnData(np.ones((100, 100))). sc.pp.pca(a). # RuntimeWarning: invalid value encountered in true_divide. # self.explained_variance_ / total_var.sum(). sc.pl.pca(a) # Throws the same error as above. a.uns[""pca""]. ```. ```. {'params': {'zero_center': True, 'use_highly_variable': False},. 'variance': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,. 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,. 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],. dtype=float32),. 'variance_ratio': array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,. nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,. nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,. nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],. dtype=float32)}. ```. Though I do get the same behaviour with `svd_solver=""randomized""`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/264
https://github.com/scverse/scanpy/issues/264:281,performance,error,error,281,"Still occurring, though it's a really weird case. Here's some code to reproduce:. ```python. a = sc.AnnData(np.ones((100, 100))). sc.pp.pca(a). # RuntimeWarning: invalid value encountered in true_divide. # self.explained_variance_ / total_var.sum(). sc.pl.pca(a) # Throws the same error as above. a.uns[""pca""]. ```. ```. {'params': {'zero_center': True, 'use_highly_variable': False},. 'variance': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,. 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,. 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],. dtype=float32),. 'variance_ratio': array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,. nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,. nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,. nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],. dtype=float32)}. ```. Though I do get the same behaviour with `svd_solver=""randomized""`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/264
https://github.com/scverse/scanpy/issues/264:281,safety,error,error,281,"Still occurring, though it's a really weird case. Here's some code to reproduce:. ```python. a = sc.AnnData(np.ones((100, 100))). sc.pp.pca(a). # RuntimeWarning: invalid value encountered in true_divide. # self.explained_variance_ / total_var.sum(). sc.pl.pca(a) # Throws the same error as above. a.uns[""pca""]. ```. ```. {'params': {'zero_center': True, 'use_highly_variable': False},. 'variance': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,. 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,. 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],. dtype=float32),. 'variance_ratio': array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,. nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,. nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,. nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],. dtype=float32)}. ```. Though I do get the same behaviour with `svd_solver=""randomized""`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/264
https://github.com/scverse/scanpy/issues/264:281,usability,error,error,281,"Still occurring, though it's a really weird case. Here's some code to reproduce:. ```python. a = sc.AnnData(np.ones((100, 100))). sc.pp.pca(a). # RuntimeWarning: invalid value encountered in true_divide. # self.explained_variance_ / total_var.sum(). sc.pl.pca(a) # Throws the same error as above. a.uns[""pca""]. ```. ```. {'params': {'zero_center': True, 'use_highly_variable': False},. 'variance': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,. 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,. 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],. dtype=float32),. 'variance_ratio': array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,. nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,. nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,. nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],. dtype=float32)}. ```. Though I do get the same behaviour with `svd_solver=""randomized""`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/264
https://github.com/scverse/scanpy/issues/264:953,usability,behavi,behaviour,953,"Still occurring, though it's a really weird case. Here's some code to reproduce:. ```python. a = sc.AnnData(np.ones((100, 100))). sc.pp.pca(a). # RuntimeWarning: invalid value encountered in true_divide. # self.explained_variance_ / total_var.sum(). sc.pl.pca(a) # Throws the same error as above. a.uns[""pca""]. ```. ```. {'params': {'zero_center': True, 'use_highly_variable': False},. 'variance': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,. 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,. 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],. dtype=float32),. 'variance_ratio': array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,. nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,. nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,. nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],. dtype=float32)}. ```. Though I do get the same behaviour with `svd_solver=""randomized""`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/264
https://github.com/scverse/scanpy/issues/265:66,deployability,integr,integrate,66,"CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers. @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:420,deployability,continu,continue,420,"CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers. @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:66,integrability,integr,integrate,66,"CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers. @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:314,integrability,discover,discovering-plugins,314,"CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers. @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:66,interoperability,integr,integrate,66,"CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers. @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:202,interoperability,plug,plugins,202,"CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers. @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:314,interoperability,discover,discovering-plugins,314,"CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers. @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:66,modifiability,integr,integrate,66,"CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers. @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:273,modifiability,pac,packaging,273,"CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers. @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:4,reliability,doe,does,4,"CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers. @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:66,reliability,integr,integrate,66,"CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers. @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:66,security,integr,integrate,66,"CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers. @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:66,testability,integr,integrate,66,"CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers. @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:294,usability,guid,guides,294,"CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers. @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:314,usability,discov,discovering-plugins,314,"CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers. @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:409,usability,tool,tools,409,"CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers. @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:564,usability,visual,visualization,564,"CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers. @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:578,usability,tool,tools,578,"CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers. @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:277,availability,consist,consists,277,"I like Seurat's CCA. A pull request using `rpy2` similar to the R wrapper of Haghverdi et al.'s version of [MNN](https://github.com/theislab/scanpy/blob/master/scanpy/rtools/mnn_correct.py) would be welcome. Regarding ""plugins"": I guess a lot of Scanpy's functionality already consists in ""plugins"":. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.dca.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.magic.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.phate.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html. and a lot more are on the way, as far as I know. I guess the strategy of having an optional dependency of the respective and a small wrapper in Scanpy is a scalable strategy. Do you think we need to do more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:96,deployability,version,version,96,"I like Seurat's CCA. A pull request using `rpy2` similar to the R wrapper of Haghverdi et al.'s version of [MNN](https://github.com/theislab/scanpy/blob/master/scanpy/rtools/mnn_correct.py) would be welcome. Regarding ""plugins"": I guess a lot of Scanpy's functionality already consists in ""plugins"":. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.dca.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.magic.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.phate.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html. and a lot more are on the way, as far as I know. I guess the strategy of having an optional dependency of the respective and a small wrapper in Scanpy is a scalable strategy. Do you think we need to do more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:343,deployability,api,api,343,"I like Seurat's CCA. A pull request using `rpy2` similar to the R wrapper of Haghverdi et al.'s version of [MNN](https://github.com/theislab/scanpy/blob/master/scanpy/rtools/mnn_correct.py) would be welcome. Regarding ""plugins"": I guess a lot of Scanpy's functionality already consists in ""plugins"":. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.dca.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.magic.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.phate.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html. and a lot more are on the way, as far as I know. I guess the strategy of having an optional dependency of the respective and a small wrapper in Scanpy is a scalable strategy. Do you think we need to do more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:354,deployability,api,api,354,"I like Seurat's CCA. A pull request using `rpy2` similar to the R wrapper of Haghverdi et al.'s version of [MNN](https://github.com/theislab/scanpy/blob/master/scanpy/rtools/mnn_correct.py) would be welcome. Regarding ""plugins"": I guess a lot of Scanpy's functionality already consists in ""plugins"":. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.dca.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.magic.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.phate.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html. and a lot more are on the way, as far as I know. I guess the strategy of having an optional dependency of the respective and a small wrapper in Scanpy is a scalable strategy. Do you think we need to do more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:421,deployability,api,api,421,"I like Seurat's CCA. A pull request using `rpy2` similar to the R wrapper of Haghverdi et al.'s version of [MNN](https://github.com/theislab/scanpy/blob/master/scanpy/rtools/mnn_correct.py) would be welcome. Regarding ""plugins"": I guess a lot of Scanpy's functionality already consists in ""plugins"":. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.dca.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.magic.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.phate.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html. and a lot more are on the way, as far as I know. I guess the strategy of having an optional dependency of the respective and a small wrapper in Scanpy is a scalable strategy. Do you think we need to do more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:432,deployability,api,api,432,"I like Seurat's CCA. A pull request using `rpy2` similar to the R wrapper of Haghverdi et al.'s version of [MNN](https://github.com/theislab/scanpy/blob/master/scanpy/rtools/mnn_correct.py) would be welcome. Regarding ""plugins"": I guess a lot of Scanpy's functionality already consists in ""plugins"":. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.dca.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.magic.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.phate.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html. and a lot more are on the way, as far as I know. I guess the strategy of having an optional dependency of the respective and a small wrapper in Scanpy is a scalable strategy. Do you think we need to do more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:491,deployability,api,api,491,"I like Seurat's CCA. A pull request using `rpy2` similar to the R wrapper of Haghverdi et al.'s version of [MNN](https://github.com/theislab/scanpy/blob/master/scanpy/rtools/mnn_correct.py) would be welcome. Regarding ""plugins"": I guess a lot of Scanpy's functionality already consists in ""plugins"":. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.dca.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.magic.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.phate.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html. and a lot more are on the way, as far as I know. I guess the strategy of having an optional dependency of the respective and a small wrapper in Scanpy is a scalable strategy. Do you think we need to do more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:502,deployability,api,api,502,"I like Seurat's CCA. A pull request using `rpy2` similar to the R wrapper of Haghverdi et al.'s version of [MNN](https://github.com/theislab/scanpy/blob/master/scanpy/rtools/mnn_correct.py) would be welcome. Regarding ""plugins"": I guess a lot of Scanpy's functionality already consists in ""plugins"":. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.dca.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.magic.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.phate.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html. and a lot more are on the way, as far as I know. I guess the strategy of having an optional dependency of the respective and a small wrapper in Scanpy is a scalable strategy. Do you think we need to do more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:563,deployability,api,api,563,"I like Seurat's CCA. A pull request using `rpy2` similar to the R wrapper of Haghverdi et al.'s version of [MNN](https://github.com/theislab/scanpy/blob/master/scanpy/rtools/mnn_correct.py) would be welcome. Regarding ""plugins"": I guess a lot of Scanpy's functionality already consists in ""plugins"":. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.dca.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.magic.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.phate.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html. and a lot more are on the way, as far as I know. I guess the strategy of having an optional dependency of the respective and a small wrapper in Scanpy is a scalable strategy. Do you think we need to do more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:574,deployability,api,api,574,"I like Seurat's CCA. A pull request using `rpy2` similar to the R wrapper of Haghverdi et al.'s version of [MNN](https://github.com/theislab/scanpy/blob/master/scanpy/rtools/mnn_correct.py) would be welcome. Regarding ""plugins"": I guess a lot of Scanpy's functionality already consists in ""plugins"":. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.dca.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.magic.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.phate.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html. and a lot more are on the way, as far as I know. I guess the strategy of having an optional dependency of the respective and a small wrapper in Scanpy is a scalable strategy. Do you think we need to do more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:635,deployability,api,api,635,"I like Seurat's CCA. A pull request using `rpy2` similar to the R wrapper of Haghverdi et al.'s version of [MNN](https://github.com/theislab/scanpy/blob/master/scanpy/rtools/mnn_correct.py) would be welcome. Regarding ""plugins"": I guess a lot of Scanpy's functionality already consists in ""plugins"":. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.dca.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.magic.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.phate.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html. and a lot more are on the way, as far as I know. I guess the strategy of having an optional dependency of the respective and a small wrapper in Scanpy is a scalable strategy. Do you think we need to do more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:646,deployability,api,api,646,"I like Seurat's CCA. A pull request using `rpy2` similar to the R wrapper of Haghverdi et al.'s version of [MNN](https://github.com/theislab/scanpy/blob/master/scanpy/rtools/mnn_correct.py) would be welcome. Regarding ""plugins"": I guess a lot of Scanpy's functionality already consists in ""plugins"":. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.dca.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.magic.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.phate.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html. and a lot more are on the way, as far as I know. I guess the strategy of having an optional dependency of the respective and a small wrapper in Scanpy is a scalable strategy. Do you think we need to do more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:759,deployability,depend,dependency,759,"I like Seurat's CCA. A pull request using `rpy2` similar to the R wrapper of Haghverdi et al.'s version of [MNN](https://github.com/theislab/scanpy/blob/master/scanpy/rtools/mnn_correct.py) would be welcome. Regarding ""plugins"": I guess a lot of Scanpy's functionality already consists in ""plugins"":. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.dca.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.magic.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.phate.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html. and a lot more are on the way, as far as I know. I guess the strategy of having an optional dependency of the respective and a small wrapper in Scanpy is a scalable strategy. Do you think we need to do more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:66,integrability,wrap,wrapper,66,"I like Seurat's CCA. A pull request using `rpy2` similar to the R wrapper of Haghverdi et al.'s version of [MNN](https://github.com/theislab/scanpy/blob/master/scanpy/rtools/mnn_correct.py) would be welcome. Regarding ""plugins"": I guess a lot of Scanpy's functionality already consists in ""plugins"":. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.dca.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.magic.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.phate.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html. and a lot more are on the way, as far as I know. I guess the strategy of having an optional dependency of the respective and a small wrapper in Scanpy is a scalable strategy. Do you think we need to do more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:96,integrability,version,version,96,"I like Seurat's CCA. A pull request using `rpy2` similar to the R wrapper of Haghverdi et al.'s version of [MNN](https://github.com/theislab/scanpy/blob/master/scanpy/rtools/mnn_correct.py) would be welcome. Regarding ""plugins"": I guess a lot of Scanpy's functionality already consists in ""plugins"":. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.dca.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.magic.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.phate.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html. and a lot more are on the way, as far as I know. I guess the strategy of having an optional dependency of the respective and a small wrapper in Scanpy is a scalable strategy. Do you think we need to do more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:343,integrability,api,api,343,"I like Seurat's CCA. A pull request using `rpy2` similar to the R wrapper of Haghverdi et al.'s version of [MNN](https://github.com/theislab/scanpy/blob/master/scanpy/rtools/mnn_correct.py) would be welcome. Regarding ""plugins"": I guess a lot of Scanpy's functionality already consists in ""plugins"":. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.dca.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.magic.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.phate.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html. and a lot more are on the way, as far as I know. I guess the strategy of having an optional dependency of the respective and a small wrapper in Scanpy is a scalable strategy. Do you think we need to do more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:354,integrability,api,api,354,"I like Seurat's CCA. A pull request using `rpy2` similar to the R wrapper of Haghverdi et al.'s version of [MNN](https://github.com/theislab/scanpy/blob/master/scanpy/rtools/mnn_correct.py) would be welcome. Regarding ""plugins"": I guess a lot of Scanpy's functionality already consists in ""plugins"":. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.dca.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.magic.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.phate.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html. and a lot more are on the way, as far as I know. I guess the strategy of having an optional dependency of the respective and a small wrapper in Scanpy is a scalable strategy. Do you think we need to do more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:421,integrability,api,api,421,"I like Seurat's CCA. A pull request using `rpy2` similar to the R wrapper of Haghverdi et al.'s version of [MNN](https://github.com/theislab/scanpy/blob/master/scanpy/rtools/mnn_correct.py) would be welcome. Regarding ""plugins"": I guess a lot of Scanpy's functionality already consists in ""plugins"":. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.dca.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.magic.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.phate.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html. and a lot more are on the way, as far as I know. I guess the strategy of having an optional dependency of the respective and a small wrapper in Scanpy is a scalable strategy. Do you think we need to do more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:432,integrability,api,api,432,"I like Seurat's CCA. A pull request using `rpy2` similar to the R wrapper of Haghverdi et al.'s version of [MNN](https://github.com/theislab/scanpy/blob/master/scanpy/rtools/mnn_correct.py) would be welcome. Regarding ""plugins"": I guess a lot of Scanpy's functionality already consists in ""plugins"":. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.dca.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.magic.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.phate.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html. and a lot more are on the way, as far as I know. I guess the strategy of having an optional dependency of the respective and a small wrapper in Scanpy is a scalable strategy. Do you think we need to do more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:491,integrability,api,api,491,"I like Seurat's CCA. A pull request using `rpy2` similar to the R wrapper of Haghverdi et al.'s version of [MNN](https://github.com/theislab/scanpy/blob/master/scanpy/rtools/mnn_correct.py) would be welcome. Regarding ""plugins"": I guess a lot of Scanpy's functionality already consists in ""plugins"":. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.dca.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.magic.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.phate.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html. and a lot more are on the way, as far as I know. I guess the strategy of having an optional dependency of the respective and a small wrapper in Scanpy is a scalable strategy. Do you think we need to do more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:502,integrability,api,api,502,"I like Seurat's CCA. A pull request using `rpy2` similar to the R wrapper of Haghverdi et al.'s version of [MNN](https://github.com/theislab/scanpy/blob/master/scanpy/rtools/mnn_correct.py) would be welcome. Regarding ""plugins"": I guess a lot of Scanpy's functionality already consists in ""plugins"":. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.dca.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.magic.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.phate.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html. and a lot more are on the way, as far as I know. I guess the strategy of having an optional dependency of the respective and a small wrapper in Scanpy is a scalable strategy. Do you think we need to do more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:563,integrability,api,api,563,"I like Seurat's CCA. A pull request using `rpy2` similar to the R wrapper of Haghverdi et al.'s version of [MNN](https://github.com/theislab/scanpy/blob/master/scanpy/rtools/mnn_correct.py) would be welcome. Regarding ""plugins"": I guess a lot of Scanpy's functionality already consists in ""plugins"":. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.dca.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.magic.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.phate.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html. and a lot more are on the way, as far as I know. I guess the strategy of having an optional dependency of the respective and a small wrapper in Scanpy is a scalable strategy. Do you think we need to do more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:574,integrability,api,api,574,"I like Seurat's CCA. A pull request using `rpy2` similar to the R wrapper of Haghverdi et al.'s version of [MNN](https://github.com/theislab/scanpy/blob/master/scanpy/rtools/mnn_correct.py) would be welcome. Regarding ""plugins"": I guess a lot of Scanpy's functionality already consists in ""plugins"":. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.dca.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.magic.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.phate.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html. and a lot more are on the way, as far as I know. I guess the strategy of having an optional dependency of the respective and a small wrapper in Scanpy is a scalable strategy. Do you think we need to do more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:635,integrability,api,api,635,"I like Seurat's CCA. A pull request using `rpy2` similar to the R wrapper of Haghverdi et al.'s version of [MNN](https://github.com/theislab/scanpy/blob/master/scanpy/rtools/mnn_correct.py) would be welcome. Regarding ""plugins"": I guess a lot of Scanpy's functionality already consists in ""plugins"":. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.dca.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.magic.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.phate.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html. and a lot more are on the way, as far as I know. I guess the strategy of having an optional dependency of the respective and a small wrapper in Scanpy is a scalable strategy. Do you think we need to do more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:646,integrability,api,api,646,"I like Seurat's CCA. A pull request using `rpy2` similar to the R wrapper of Haghverdi et al.'s version of [MNN](https://github.com/theislab/scanpy/blob/master/scanpy/rtools/mnn_correct.py) would be welcome. Regarding ""plugins"": I guess a lot of Scanpy's functionality already consists in ""plugins"":. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.dca.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.magic.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.phate.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html. and a lot more are on the way, as far as I know. I guess the strategy of having an optional dependency of the respective and a small wrapper in Scanpy is a scalable strategy. Do you think we need to do more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:759,integrability,depend,dependency,759,"I like Seurat's CCA. A pull request using `rpy2` similar to the R wrapper of Haghverdi et al.'s version of [MNN](https://github.com/theislab/scanpy/blob/master/scanpy/rtools/mnn_correct.py) would be welcome. Regarding ""plugins"": I guess a lot of Scanpy's functionality already consists in ""plugins"":. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.dca.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.magic.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.phate.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html. and a lot more are on the way, as far as I know. I guess the strategy of having an optional dependency of the respective and a small wrapper in Scanpy is a scalable strategy. Do you think we need to do more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:800,integrability,wrap,wrapper,800,"I like Seurat's CCA. A pull request using `rpy2` similar to the R wrapper of Haghverdi et al.'s version of [MNN](https://github.com/theislab/scanpy/blob/master/scanpy/rtools/mnn_correct.py) would be welcome. Regarding ""plugins"": I guess a lot of Scanpy's functionality already consists in ""plugins"":. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.dca.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.magic.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.phate.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html. and a lot more are on the way, as far as I know. I guess the strategy of having an optional dependency of the respective and a small wrapper in Scanpy is a scalable strategy. Do you think we need to do more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:66,interoperability,wrapper,wrapper,66,"I like Seurat's CCA. A pull request using `rpy2` similar to the R wrapper of Haghverdi et al.'s version of [MNN](https://github.com/theislab/scanpy/blob/master/scanpy/rtools/mnn_correct.py) would be welcome. Regarding ""plugins"": I guess a lot of Scanpy's functionality already consists in ""plugins"":. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.dca.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.magic.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.phate.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html. and a lot more are on the way, as far as I know. I guess the strategy of having an optional dependency of the respective and a small wrapper in Scanpy is a scalable strategy. Do you think we need to do more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:219,interoperability,plug,plugins,219,"I like Seurat's CCA. A pull request using `rpy2` similar to the R wrapper of Haghverdi et al.'s version of [MNN](https://github.com/theislab/scanpy/blob/master/scanpy/rtools/mnn_correct.py) would be welcome. Regarding ""plugins"": I guess a lot of Scanpy's functionality already consists in ""plugins"":. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.dca.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.magic.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.phate.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html. and a lot more are on the way, as far as I know. I guess the strategy of having an optional dependency of the respective and a small wrapper in Scanpy is a scalable strategy. Do you think we need to do more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:290,interoperability,plug,plugins,290,"I like Seurat's CCA. A pull request using `rpy2` similar to the R wrapper of Haghverdi et al.'s version of [MNN](https://github.com/theislab/scanpy/blob/master/scanpy/rtools/mnn_correct.py) would be welcome. Regarding ""plugins"": I guess a lot of Scanpy's functionality already consists in ""plugins"":. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.dca.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.magic.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.phate.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html. and a lot more are on the way, as far as I know. I guess the strategy of having an optional dependency of the respective and a small wrapper in Scanpy is a scalable strategy. Do you think we need to do more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:343,interoperability,api,api,343,"I like Seurat's CCA. A pull request using `rpy2` similar to the R wrapper of Haghverdi et al.'s version of [MNN](https://github.com/theislab/scanpy/blob/master/scanpy/rtools/mnn_correct.py) would be welcome. Regarding ""plugins"": I guess a lot of Scanpy's functionality already consists in ""plugins"":. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.dca.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.magic.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.phate.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html. and a lot more are on the way, as far as I know. I guess the strategy of having an optional dependency of the respective and a small wrapper in Scanpy is a scalable strategy. Do you think we need to do more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:354,interoperability,api,api,354,"I like Seurat's CCA. A pull request using `rpy2` similar to the R wrapper of Haghverdi et al.'s version of [MNN](https://github.com/theislab/scanpy/blob/master/scanpy/rtools/mnn_correct.py) would be welcome. Regarding ""plugins"": I guess a lot of Scanpy's functionality already consists in ""plugins"":. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.dca.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.magic.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.phate.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html. and a lot more are on the way, as far as I know. I guess the strategy of having an optional dependency of the respective and a small wrapper in Scanpy is a scalable strategy. Do you think we need to do more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:421,interoperability,api,api,421,"I like Seurat's CCA. A pull request using `rpy2` similar to the R wrapper of Haghverdi et al.'s version of [MNN](https://github.com/theislab/scanpy/blob/master/scanpy/rtools/mnn_correct.py) would be welcome. Regarding ""plugins"": I guess a lot of Scanpy's functionality already consists in ""plugins"":. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.dca.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.magic.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.phate.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html. and a lot more are on the way, as far as I know. I guess the strategy of having an optional dependency of the respective and a small wrapper in Scanpy is a scalable strategy. Do you think we need to do more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:432,interoperability,api,api,432,"I like Seurat's CCA. A pull request using `rpy2` similar to the R wrapper of Haghverdi et al.'s version of [MNN](https://github.com/theislab/scanpy/blob/master/scanpy/rtools/mnn_correct.py) would be welcome. Regarding ""plugins"": I guess a lot of Scanpy's functionality already consists in ""plugins"":. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.dca.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.magic.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.phate.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html. and a lot more are on the way, as far as I know. I guess the strategy of having an optional dependency of the respective and a small wrapper in Scanpy is a scalable strategy. Do you think we need to do more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:491,interoperability,api,api,491,"I like Seurat's CCA. A pull request using `rpy2` similar to the R wrapper of Haghverdi et al.'s version of [MNN](https://github.com/theislab/scanpy/blob/master/scanpy/rtools/mnn_correct.py) would be welcome. Regarding ""plugins"": I guess a lot of Scanpy's functionality already consists in ""plugins"":. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.dca.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.magic.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.phate.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html. and a lot more are on the way, as far as I know. I guess the strategy of having an optional dependency of the respective and a small wrapper in Scanpy is a scalable strategy. Do you think we need to do more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:502,interoperability,api,api,502,"I like Seurat's CCA. A pull request using `rpy2` similar to the R wrapper of Haghverdi et al.'s version of [MNN](https://github.com/theislab/scanpy/blob/master/scanpy/rtools/mnn_correct.py) would be welcome. Regarding ""plugins"": I guess a lot of Scanpy's functionality already consists in ""plugins"":. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.dca.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.magic.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.phate.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html. and a lot more are on the way, as far as I know. I guess the strategy of having an optional dependency of the respective and a small wrapper in Scanpy is a scalable strategy. Do you think we need to do more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:563,interoperability,api,api,563,"I like Seurat's CCA. A pull request using `rpy2` similar to the R wrapper of Haghverdi et al.'s version of [MNN](https://github.com/theislab/scanpy/blob/master/scanpy/rtools/mnn_correct.py) would be welcome. Regarding ""plugins"": I guess a lot of Scanpy's functionality already consists in ""plugins"":. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.dca.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.magic.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.phate.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html. and a lot more are on the way, as far as I know. I guess the strategy of having an optional dependency of the respective and a small wrapper in Scanpy is a scalable strategy. Do you think we need to do more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:574,interoperability,api,api,574,"I like Seurat's CCA. A pull request using `rpy2` similar to the R wrapper of Haghverdi et al.'s version of [MNN](https://github.com/theislab/scanpy/blob/master/scanpy/rtools/mnn_correct.py) would be welcome. Regarding ""plugins"": I guess a lot of Scanpy's functionality already consists in ""plugins"":. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.dca.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.magic.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.phate.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html. and a lot more are on the way, as far as I know. I guess the strategy of having an optional dependency of the respective and a small wrapper in Scanpy is a scalable strategy. Do you think we need to do more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:635,interoperability,api,api,635,"I like Seurat's CCA. A pull request using `rpy2` similar to the R wrapper of Haghverdi et al.'s version of [MNN](https://github.com/theislab/scanpy/blob/master/scanpy/rtools/mnn_correct.py) would be welcome. Regarding ""plugins"": I guess a lot of Scanpy's functionality already consists in ""plugins"":. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.dca.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.magic.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.phate.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html. and a lot more are on the way, as far as I know. I guess the strategy of having an optional dependency of the respective and a small wrapper in Scanpy is a scalable strategy. Do you think we need to do more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:646,interoperability,api,api,646,"I like Seurat's CCA. A pull request using `rpy2` similar to the R wrapper of Haghverdi et al.'s version of [MNN](https://github.com/theislab/scanpy/blob/master/scanpy/rtools/mnn_correct.py) would be welcome. Regarding ""plugins"": I guess a lot of Scanpy's functionality already consists in ""plugins"":. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.dca.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.magic.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.phate.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html. and a lot more are on the way, as far as I know. I guess the strategy of having an optional dependency of the respective and a small wrapper in Scanpy is a scalable strategy. Do you think we need to do more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:800,interoperability,wrapper,wrapper,800,"I like Seurat's CCA. A pull request using `rpy2` similar to the R wrapper of Haghverdi et al.'s version of [MNN](https://github.com/theislab/scanpy/blob/master/scanpy/rtools/mnn_correct.py) would be welcome. Regarding ""plugins"": I guess a lot of Scanpy's functionality already consists in ""plugins"":. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.dca.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.magic.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.phate.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html. and a lot more are on the way, as far as I know. I guess the strategy of having an optional dependency of the respective and a small wrapper in Scanpy is a scalable strategy. Do you think we need to do more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:96,modifiability,version,version,96,"I like Seurat's CCA. A pull request using `rpy2` similar to the R wrapper of Haghverdi et al.'s version of [MNN](https://github.com/theislab/scanpy/blob/master/scanpy/rtools/mnn_correct.py) would be welcome. Regarding ""plugins"": I guess a lot of Scanpy's functionality already consists in ""plugins"":. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.dca.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.magic.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.phate.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html. and a lot more are on the way, as far as I know. I guess the strategy of having an optional dependency of the respective and a small wrapper in Scanpy is a scalable strategy. Do you think we need to do more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:759,modifiability,depend,dependency,759,"I like Seurat's CCA. A pull request using `rpy2` similar to the R wrapper of Haghverdi et al.'s version of [MNN](https://github.com/theislab/scanpy/blob/master/scanpy/rtools/mnn_correct.py) would be welcome. Regarding ""plugins"": I guess a lot of Scanpy's functionality already consists in ""plugins"":. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.dca.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.magic.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.phate.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html. and a lot more are on the way, as far as I know. I guess the strategy of having an optional dependency of the respective and a small wrapper in Scanpy is a scalable strategy. Do you think we need to do more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:823,modifiability,scal,scalable,823,"I like Seurat's CCA. A pull request using `rpy2` similar to the R wrapper of Haghverdi et al.'s version of [MNN](https://github.com/theislab/scanpy/blob/master/scanpy/rtools/mnn_correct.py) would be welcome. Regarding ""plugins"": I guess a lot of Scanpy's functionality already consists in ""plugins"":. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.dca.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.magic.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.phate.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html. and a lot more are on the way, as far as I know. I guess the strategy of having an optional dependency of the respective and a small wrapper in Scanpy is a scalable strategy. Do you think we need to do more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:823,performance,scalab,scalable,823,"I like Seurat's CCA. A pull request using `rpy2` similar to the R wrapper of Haghverdi et al.'s version of [MNN](https://github.com/theislab/scanpy/blob/master/scanpy/rtools/mnn_correct.py) would be welcome. Regarding ""plugins"": I guess a lot of Scanpy's functionality already consists in ""plugins"":. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.dca.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.magic.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.phate.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html. and a lot more are on the way, as far as I know. I guess the strategy of having an optional dependency of the respective and a small wrapper in Scanpy is a scalable strategy. Do you think we need to do more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:167,reliability,rto,rtools,167,"I like Seurat's CCA. A pull request using `rpy2` similar to the R wrapper of Haghverdi et al.'s version of [MNN](https://github.com/theislab/scanpy/blob/master/scanpy/rtools/mnn_correct.py) would be welcome. Regarding ""plugins"": I guess a lot of Scanpy's functionality already consists in ""plugins"":. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.dca.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.magic.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.phate.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html. and a lot more are on the way, as far as I know. I guess the strategy of having an optional dependency of the respective and a small wrapper in Scanpy is a scalable strategy. Do you think we need to do more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:759,safety,depend,dependency,759,"I like Seurat's CCA. A pull request using `rpy2` similar to the R wrapper of Haghverdi et al.'s version of [MNN](https://github.com/theislab/scanpy/blob/master/scanpy/rtools/mnn_correct.py) would be welcome. Regarding ""plugins"": I guess a lot of Scanpy's functionality already consists in ""plugins"":. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.dca.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.magic.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.phate.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html. and a lot more are on the way, as far as I know. I guess the strategy of having an optional dependency of the respective and a small wrapper in Scanpy is a scalable strategy. Do you think we need to do more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:759,testability,depend,dependency,759,"I like Seurat's CCA. A pull request using `rpy2` similar to the R wrapper of Haghverdi et al.'s version of [MNN](https://github.com/theislab/scanpy/blob/master/scanpy/rtools/mnn_correct.py) would be welcome. Regarding ""plugins"": I guess a lot of Scanpy's functionality already consists in ""plugins"":. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.dca.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.magic.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.phate.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html. and a lot more are on the way, as far as I know. I guess the strategy of having an optional dependency of the respective and a small wrapper in Scanpy is a scalable strategy. Do you think we need to do more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:277,usability,consist,consists,277,"I like Seurat's CCA. A pull request using `rpy2` similar to the R wrapper of Haghverdi et al.'s version of [MNN](https://github.com/theislab/scanpy/blob/master/scanpy/rtools/mnn_correct.py) would be welcome. Regarding ""plugins"": I guess a lot of Scanpy's functionality already consists in ""plugins"":. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.mnn_correct.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.dca.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.magic.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.phate.html. - https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html. and a lot more are on the way, as far as I know. I guess the strategy of having an optional dependency of the respective and a small wrapper in Scanpy is a scalable strategy. Do you think we need to do more?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:73,deployability,modul,modules,73,Sklearn has its implementation of [CCA](. http://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.CCA.html) but that would allow the alignment of two samples only. Recently a multi sample approach was implemented in [pyrcca](. https://github.com/gallantlab/pyrcca) library for which there is a biorXiv paper.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:73,modifiability,modul,modules,73,Sklearn has its implementation of [CCA](. http://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.CCA.html) but that would allow the alignment of two samples only. Recently a multi sample approach was implemented in [pyrcca](. https://github.com/gallantlab/pyrcca) library for which there is a biorXiv paper.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:73,safety,modul,modules,73,Sklearn has its implementation of [CCA](. http://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.CCA.html) but that would allow the alignment of two samples only. Recently a multi sample approach was implemented in [pyrcca](. https://github.com/gallantlab/pyrcca) library for which there is a biorXiv paper.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:56,usability,learn,learn,56,Sklearn has its implementation of [CCA](. http://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.CCA.html) but that would allow the alignment of two samples only. Recently a multi sample approach was implemented in [pyrcca](. https://github.com/gallantlab/pyrcca) library for which there is a biorXiv paper.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:298,deployability,instal,installed,298,"@falexwolf I think it is worth to have a `scanpy.plugin` or `scanpy.extension` or something shorter like `scanpy.pg` or `scanpy.ext` that aggregates all plugins. First, this clarifies for the user that the tool he/she is using is not directly developed by scanpy. Second, this allows plugins to be installed separately without having to update scanpy's code. The idea is that scanpy will be able to discover any plugins installed. On the side of developers, this could facilitate integration with scanpy. We can get inspired by flask extensions: http://flask.pocoo.org/docs/1.0/extensiondev/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:337,deployability,updat,update,337,"@falexwolf I think it is worth to have a `scanpy.plugin` or `scanpy.extension` or something shorter like `scanpy.pg` or `scanpy.ext` that aggregates all plugins. First, this clarifies for the user that the tool he/she is using is not directly developed by scanpy. Second, this allows plugins to be installed separately without having to update scanpy's code. The idea is that scanpy will be able to discover any plugins installed. On the side of developers, this could facilitate integration with scanpy. We can get inspired by flask extensions: http://flask.pocoo.org/docs/1.0/extensiondev/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:420,deployability,instal,installed,420,"@falexwolf I think it is worth to have a `scanpy.plugin` or `scanpy.extension` or something shorter like `scanpy.pg` or `scanpy.ext` that aggregates all plugins. First, this clarifies for the user that the tool he/she is using is not directly developed by scanpy. Second, this allows plugins to be installed separately without having to update scanpy's code. The idea is that scanpy will be able to discover any plugins installed. On the side of developers, this could facilitate integration with scanpy. We can get inspired by flask extensions: http://flask.pocoo.org/docs/1.0/extensiondev/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:480,deployability,integr,integration,480,"@falexwolf I think it is worth to have a `scanpy.plugin` or `scanpy.extension` or something shorter like `scanpy.pg` or `scanpy.ext` that aggregates all plugins. First, this clarifies for the user that the tool he/she is using is not directly developed by scanpy. Second, this allows plugins to be installed separately without having to update scanpy's code. The idea is that scanpy will be able to discover any plugins installed. On the side of developers, this could facilitate integration with scanpy. We can get inspired by flask extensions: http://flask.pocoo.org/docs/1.0/extensiondev/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:399,integrability,discover,discover,399,"@falexwolf I think it is worth to have a `scanpy.plugin` or `scanpy.extension` or something shorter like `scanpy.pg` or `scanpy.ext` that aggregates all plugins. First, this clarifies for the user that the tool he/she is using is not directly developed by scanpy. Second, this allows plugins to be installed separately without having to update scanpy's code. The idea is that scanpy will be able to discover any plugins installed. On the side of developers, this could facilitate integration with scanpy. We can get inspired by flask extensions: http://flask.pocoo.org/docs/1.0/extensiondev/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:480,integrability,integr,integration,480,"@falexwolf I think it is worth to have a `scanpy.plugin` or `scanpy.extension` or something shorter like `scanpy.pg` or `scanpy.ext` that aggregates all plugins. First, this clarifies for the user that the tool he/she is using is not directly developed by scanpy. Second, this allows plugins to be installed separately without having to update scanpy's code. The idea is that scanpy will be able to discover any plugins installed. On the side of developers, this could facilitate integration with scanpy. We can get inspired by flask extensions: http://flask.pocoo.org/docs/1.0/extensiondev/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:49,interoperability,plug,plugin,49,"@falexwolf I think it is worth to have a `scanpy.plugin` or `scanpy.extension` or something shorter like `scanpy.pg` or `scanpy.ext` that aggregates all plugins. First, this clarifies for the user that the tool he/she is using is not directly developed by scanpy. Second, this allows plugins to be installed separately without having to update scanpy's code. The idea is that scanpy will be able to discover any plugins installed. On the side of developers, this could facilitate integration with scanpy. We can get inspired by flask extensions: http://flask.pocoo.org/docs/1.0/extensiondev/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:153,interoperability,plug,plugins,153,"@falexwolf I think it is worth to have a `scanpy.plugin` or `scanpy.extension` or something shorter like `scanpy.pg` or `scanpy.ext` that aggregates all plugins. First, this clarifies for the user that the tool he/she is using is not directly developed by scanpy. Second, this allows plugins to be installed separately without having to update scanpy's code. The idea is that scanpy will be able to discover any plugins installed. On the side of developers, this could facilitate integration with scanpy. We can get inspired by flask extensions: http://flask.pocoo.org/docs/1.0/extensiondev/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:284,interoperability,plug,plugins,284,"@falexwolf I think it is worth to have a `scanpy.plugin` or `scanpy.extension` or something shorter like `scanpy.pg` or `scanpy.ext` that aggregates all plugins. First, this clarifies for the user that the tool he/she is using is not directly developed by scanpy. Second, this allows plugins to be installed separately without having to update scanpy's code. The idea is that scanpy will be able to discover any plugins installed. On the side of developers, this could facilitate integration with scanpy. We can get inspired by flask extensions: http://flask.pocoo.org/docs/1.0/extensiondev/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:399,interoperability,discover,discover,399,"@falexwolf I think it is worth to have a `scanpy.plugin` or `scanpy.extension` or something shorter like `scanpy.pg` or `scanpy.ext` that aggregates all plugins. First, this clarifies for the user that the tool he/she is using is not directly developed by scanpy. Second, this allows plugins to be installed separately without having to update scanpy's code. The idea is that scanpy will be able to discover any plugins installed. On the side of developers, this could facilitate integration with scanpy. We can get inspired by flask extensions: http://flask.pocoo.org/docs/1.0/extensiondev/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:412,interoperability,plug,plugins,412,"@falexwolf I think it is worth to have a `scanpy.plugin` or `scanpy.extension` or something shorter like `scanpy.pg` or `scanpy.ext` that aggregates all plugins. First, this clarifies for the user that the tool he/she is using is not directly developed by scanpy. Second, this allows plugins to be installed separately without having to update scanpy's code. The idea is that scanpy will be able to discover any plugins installed. On the side of developers, this could facilitate integration with scanpy. We can get inspired by flask extensions: http://flask.pocoo.org/docs/1.0/extensiondev/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:480,interoperability,integr,integration,480,"@falexwolf I think it is worth to have a `scanpy.plugin` or `scanpy.extension` or something shorter like `scanpy.pg` or `scanpy.ext` that aggregates all plugins. First, this clarifies for the user that the tool he/she is using is not directly developed by scanpy. Second, this allows plugins to be installed separately without having to update scanpy's code. The idea is that scanpy will be able to discover any plugins installed. On the side of developers, this could facilitate integration with scanpy. We can get inspired by flask extensions: http://flask.pocoo.org/docs/1.0/extensiondev/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:68,modifiability,extens,extension,68,"@falexwolf I think it is worth to have a `scanpy.plugin` or `scanpy.extension` or something shorter like `scanpy.pg` or `scanpy.ext` that aggregates all plugins. First, this clarifies for the user that the tool he/she is using is not directly developed by scanpy. Second, this allows plugins to be installed separately without having to update scanpy's code. The idea is that scanpy will be able to discover any plugins installed. On the side of developers, this could facilitate integration with scanpy. We can get inspired by flask extensions: http://flask.pocoo.org/docs/1.0/extensiondev/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:480,modifiability,integr,integration,480,"@falexwolf I think it is worth to have a `scanpy.plugin` or `scanpy.extension` or something shorter like `scanpy.pg` or `scanpy.ext` that aggregates all plugins. First, this clarifies for the user that the tool he/she is using is not directly developed by scanpy. Second, this allows plugins to be installed separately without having to update scanpy's code. The idea is that scanpy will be able to discover any plugins installed. On the side of developers, this could facilitate integration with scanpy. We can get inspired by flask extensions: http://flask.pocoo.org/docs/1.0/extensiondev/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:534,modifiability,extens,extensions,534,"@falexwolf I think it is worth to have a `scanpy.plugin` or `scanpy.extension` or something shorter like `scanpy.pg` or `scanpy.ext` that aggregates all plugins. First, this clarifies for the user that the tool he/she is using is not directly developed by scanpy. Second, this allows plugins to be installed separately without having to update scanpy's code. The idea is that scanpy will be able to discover any plugins installed. On the side of developers, this could facilitate integration with scanpy. We can get inspired by flask extensions: http://flask.pocoo.org/docs/1.0/extensiondev/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:578,modifiability,extens,extensiondev,578,"@falexwolf I think it is worth to have a `scanpy.plugin` or `scanpy.extension` or something shorter like `scanpy.pg` or `scanpy.ext` that aggregates all plugins. First, this clarifies for the user that the tool he/she is using is not directly developed by scanpy. Second, this allows plugins to be installed separately without having to update scanpy's code. The idea is that scanpy will be able to discover any plugins installed. On the side of developers, this could facilitate integration with scanpy. We can get inspired by flask extensions: http://flask.pocoo.org/docs/1.0/extensiondev/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:480,reliability,integr,integration,480,"@falexwolf I think it is worth to have a `scanpy.plugin` or `scanpy.extension` or something shorter like `scanpy.pg` or `scanpy.ext` that aggregates all plugins. First, this clarifies for the user that the tool he/she is using is not directly developed by scanpy. Second, this allows plugins to be installed separately without having to update scanpy's code. The idea is that scanpy will be able to discover any plugins installed. On the side of developers, this could facilitate integration with scanpy. We can get inspired by flask extensions: http://flask.pocoo.org/docs/1.0/extensiondev/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:337,safety,updat,update,337,"@falexwolf I think it is worth to have a `scanpy.plugin` or `scanpy.extension` or something shorter like `scanpy.pg` or `scanpy.ext` that aggregates all plugins. First, this clarifies for the user that the tool he/she is using is not directly developed by scanpy. Second, this allows plugins to be installed separately without having to update scanpy's code. The idea is that scanpy will be able to discover any plugins installed. On the side of developers, this could facilitate integration with scanpy. We can get inspired by flask extensions: http://flask.pocoo.org/docs/1.0/extensiondev/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:337,security,updat,update,337,"@falexwolf I think it is worth to have a `scanpy.plugin` or `scanpy.extension` or something shorter like `scanpy.pg` or `scanpy.ext` that aggregates all plugins. First, this clarifies for the user that the tool he/she is using is not directly developed by scanpy. Second, this allows plugins to be installed separately without having to update scanpy's code. The idea is that scanpy will be able to discover any plugins installed. On the side of developers, this could facilitate integration with scanpy. We can get inspired by flask extensions: http://flask.pocoo.org/docs/1.0/extensiondev/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:480,security,integr,integration,480,"@falexwolf I think it is worth to have a `scanpy.plugin` or `scanpy.extension` or something shorter like `scanpy.pg` or `scanpy.ext` that aggregates all plugins. First, this clarifies for the user that the tool he/she is using is not directly developed by scanpy. Second, this allows plugins to be installed separately without having to update scanpy's code. The idea is that scanpy will be able to discover any plugins installed. On the side of developers, this could facilitate integration with scanpy. We can get inspired by flask extensions: http://flask.pocoo.org/docs/1.0/extensiondev/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:480,testability,integr,integration,480,"@falexwolf I think it is worth to have a `scanpy.plugin` or `scanpy.extension` or something shorter like `scanpy.pg` or `scanpy.ext` that aggregates all plugins. First, this clarifies for the user that the tool he/she is using is not directly developed by scanpy. Second, this allows plugins to be installed separately without having to update scanpy's code. The idea is that scanpy will be able to discover any plugins installed. On the side of developers, this could facilitate integration with scanpy. We can get inspired by flask extensions: http://flask.pocoo.org/docs/1.0/extensiondev/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:192,usability,user,user,192,"@falexwolf I think it is worth to have a `scanpy.plugin` or `scanpy.extension` or something shorter like `scanpy.pg` or `scanpy.ext` that aggregates all plugins. First, this clarifies for the user that the tool he/she is using is not directly developed by scanpy. Second, this allows plugins to be installed separately without having to update scanpy's code. The idea is that scanpy will be able to discover any plugins installed. On the side of developers, this could facilitate integration with scanpy. We can get inspired by flask extensions: http://flask.pocoo.org/docs/1.0/extensiondev/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:206,usability,tool,tool,206,"@falexwolf I think it is worth to have a `scanpy.plugin` or `scanpy.extension` or something shorter like `scanpy.pg` or `scanpy.ext` that aggregates all plugins. First, this clarifies for the user that the tool he/she is using is not directly developed by scanpy. Second, this allows plugins to be installed separately without having to update scanpy's code. The idea is that scanpy will be able to discover any plugins installed. On the side of developers, this could facilitate integration with scanpy. We can get inspired by flask extensions: http://flask.pocoo.org/docs/1.0/extensiondev/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:399,usability,discov,discover,399,"@falexwolf I think it is worth to have a `scanpy.plugin` or `scanpy.extension` or something shorter like `scanpy.pg` or `scanpy.ext` that aggregates all plugins. First, this clarifies for the user that the tool he/she is using is not directly developed by scanpy. Second, this allows plugins to be installed separately without having to update scanpy's code. The idea is that scanpy will be able to discover any plugins installed. On the side of developers, this could facilitate integration with scanpy. We can get inspired by flask extensions: http://flask.pocoo.org/docs/1.0/extensiondev/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:73,deployability,modul,modules,73,> Sklearn has its implementation of [CCA](http://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.CCA.html) but that would allow the alignment of two samples only. Recently a multi sample approach was implemented in [pyrcca](https://github.com/gallantlab/pyrcca) library for which there is a biorXiv paper. Is it suitable for single cell data ?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:73,modifiability,modul,modules,73,> Sklearn has its implementation of [CCA](http://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.CCA.html) but that would allow the alignment of two samples only. Recently a multi sample approach was implemented in [pyrcca](https://github.com/gallantlab/pyrcca) library for which there is a biorXiv paper. Is it suitable for single cell data ?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:73,safety,modul,modules,73,> Sklearn has its implementation of [CCA](http://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.CCA.html) but that would allow the alignment of two samples only. Recently a multi sample approach was implemented in [pyrcca](https://github.com/gallantlab/pyrcca) library for which there is a biorXiv paper. Is it suitable for single cell data ?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:56,usability,learn,learn,56,> Sklearn has its implementation of [CCA](http://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.CCA.html) but that would allow the alignment of two samples only. Recently a multi sample approach was implemented in [pyrcca](https://github.com/gallantlab/pyrcca) library for which there is a biorXiv paper. Is it suitable for single cell data ?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:49,deployability,manag,manage,49,"@fidelram Yes, makes sense. Let's see whether we manage to organize it this way. There will be a few plugins coming soon and I'll talk with the one doing it about this. @wangjiawen2013 The Seurat developers did a bit more than simply fitting a standard CCA. So I'd assume that it'd be some work to wrap sklearn's CCA or pyrcca so that it performs similar to Seurat's CCA on single cell data...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:49,energy efficiency,manag,manage,49,"@fidelram Yes, makes sense. Let's see whether we manage to organize it this way. There will be a few plugins coming soon and I'll talk with the one doing it about this. @wangjiawen2013 The Seurat developers did a bit more than simply fitting a standard CCA. So I'd assume that it'd be some work to wrap sklearn's CCA or pyrcca so that it performs similar to Seurat's CCA on single cell data...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:298,integrability,wrap,wrap,298,"@fidelram Yes, makes sense. Let's see whether we manage to organize it this way. There will be a few plugins coming soon and I'll talk with the one doing it about this. @wangjiawen2013 The Seurat developers did a bit more than simply fitting a standard CCA. So I'd assume that it'd be some work to wrap sklearn's CCA or pyrcca so that it performs similar to Seurat's CCA on single cell data...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:101,interoperability,plug,plugins,101,"@fidelram Yes, makes sense. Let's see whether we manage to organize it this way. There will be a few plugins coming soon and I'll talk with the one doing it about this. @wangjiawen2013 The Seurat developers did a bit more than simply fitting a standard CCA. So I'd assume that it'd be some work to wrap sklearn's CCA or pyrcca so that it performs similar to Seurat's CCA on single cell data...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:244,interoperability,standard,standard,244,"@fidelram Yes, makes sense. Let's see whether we manage to organize it this way. There will be a few plugins coming soon and I'll talk with the one doing it about this. @wangjiawen2013 The Seurat developers did a bit more than simply fitting a standard CCA. So I'd assume that it'd be some work to wrap sklearn's CCA or pyrcca so that it performs similar to Seurat's CCA on single cell data...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:338,performance,perform,performs,338,"@fidelram Yes, makes sense. Let's see whether we manage to organize it this way. There will be a few plugins coming soon and I'll talk with the one doing it about this. @wangjiawen2013 The Seurat developers did a bit more than simply fitting a standard CCA. So I'd assume that it'd be some work to wrap sklearn's CCA or pyrcca so that it performs similar to Seurat's CCA on single cell data...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:49,safety,manag,manage,49,"@fidelram Yes, makes sense. Let's see whether we manage to organize it this way. There will be a few plugins coming soon and I'll talk with the one doing it about this. @wangjiawen2013 The Seurat developers did a bit more than simply fitting a standard CCA. So I'd assume that it'd be some work to wrap sklearn's CCA or pyrcca so that it performs similar to Seurat's CCA on single cell data...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:227,testability,simpl,simply,227,"@fidelram Yes, makes sense. Let's see whether we manage to organize it this way. There will be a few plugins coming soon and I'll talk with the one doing it about this. @wangjiawen2013 The Seurat developers did a bit more than simply fitting a standard CCA. So I'd assume that it'd be some work to wrap sklearn's CCA or pyrcca so that it performs similar to Seurat's CCA on single cell data...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:227,usability,simpl,simply,227,"@fidelram Yes, makes sense. Let's see whether we manage to organize it this way. There will be a few plugins coming soon and I'll talk with the one doing it about this. @wangjiawen2013 The Seurat developers did a bit more than simply fitting a standard CCA. So I'd assume that it'd be some work to wrap sklearn's CCA or pyrcca so that it performs similar to Seurat's CCA on single cell data...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:338,usability,perform,performs,338,"@fidelram Yes, makes sense. Let's see whether we manage to organize it this way. There will be a few plugins coming soon and I'll talk with the one doing it about this. @wangjiawen2013 The Seurat developers did a bit more than simply fitting a standard CCA. So I'd assume that it'd be some work to wrap sklearn's CCA or pyrcca so that it performs similar to Seurat's CCA on single cell data...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:41,usability,learn,learning,41,"Given that UMAP can be used for manifold learning, shouldn’t be possibile to align experiments using UMAP? Who wants to join me in this evaluation?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:537,availability,cluster,cluster,537,"UMAP won't do any correction of batch effects for you, like CCA (it looks at the basis that leads to the greatest overlap between the batches, assuming that this captures the common biological variation and projects out everything else, assuming it's nuisance/technical batch effects). Similar for all other ""alignment tools"": you throw away some information in order to align. When you map a new dataset into an existing dataset using UMAP, this will do an _exact_ mapping. If you have pronounced batch effects, the second dataset will cluster as a whole far away from the first. So, I don't think that there will be much to gain. Why not give BBKNN (https://github.com/Teichlab/bbknn) a try? It integrates nicely with Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:537,deployability,cluster,cluster,537,"UMAP won't do any correction of batch effects for you, like CCA (it looks at the basis that leads to the greatest overlap between the batches, assuming that this captures the common biological variation and projects out everything else, assuming it's nuisance/technical batch effects). Similar for all other ""alignment tools"": you throw away some information in order to align. When you map a new dataset into an existing dataset using UMAP, this will do an _exact_ mapping. If you have pronounced batch effects, the second dataset will cluster as a whole far away from the first. So, I don't think that there will be much to gain. Why not give BBKNN (https://github.com/Teichlab/bbknn) a try? It integrates nicely with Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:697,deployability,integr,integrates,697,"UMAP won't do any correction of batch effects for you, like CCA (it looks at the basis that leads to the greatest overlap between the batches, assuming that this captures the common biological variation and projects out everything else, assuming it's nuisance/technical batch effects). Similar for all other ""alignment tools"": you throw away some information in order to align. When you map a new dataset into an existing dataset using UMAP, this will do an _exact_ mapping. If you have pronounced batch effects, the second dataset will cluster as a whole far away from the first. So, I don't think that there will be much to gain. Why not give BBKNN (https://github.com/Teichlab/bbknn) a try? It integrates nicely with Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:32,integrability,batch,batch,32,"UMAP won't do any correction of batch effects for you, like CCA (it looks at the basis that leads to the greatest overlap between the batches, assuming that this captures the common biological variation and projects out everything else, assuming it's nuisance/technical batch effects). Similar for all other ""alignment tools"": you throw away some information in order to align. When you map a new dataset into an existing dataset using UMAP, this will do an _exact_ mapping. If you have pronounced batch effects, the second dataset will cluster as a whole far away from the first. So, I don't think that there will be much to gain. Why not give BBKNN (https://github.com/Teichlab/bbknn) a try? It integrates nicely with Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:134,integrability,batch,batches,134,"UMAP won't do any correction of batch effects for you, like CCA (it looks at the basis that leads to the greatest overlap between the batches, assuming that this captures the common biological variation and projects out everything else, assuming it's nuisance/technical batch effects). Similar for all other ""alignment tools"": you throw away some information in order to align. When you map a new dataset into an existing dataset using UMAP, this will do an _exact_ mapping. If you have pronounced batch effects, the second dataset will cluster as a whole far away from the first. So, I don't think that there will be much to gain. Why not give BBKNN (https://github.com/Teichlab/bbknn) a try? It integrates nicely with Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:270,integrability,batch,batch,270,"UMAP won't do any correction of batch effects for you, like CCA (it looks at the basis that leads to the greatest overlap between the batches, assuming that this captures the common biological variation and projects out everything else, assuming it's nuisance/technical batch effects). Similar for all other ""alignment tools"": you throw away some information in order to align. When you map a new dataset into an existing dataset using UMAP, this will do an _exact_ mapping. If you have pronounced batch effects, the second dataset will cluster as a whole far away from the first. So, I don't think that there will be much to gain. Why not give BBKNN (https://github.com/Teichlab/bbknn) a try? It integrates nicely with Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:498,integrability,batch,batch,498,"UMAP won't do any correction of batch effects for you, like CCA (it looks at the basis that leads to the greatest overlap between the batches, assuming that this captures the common biological variation and projects out everything else, assuming it's nuisance/technical batch effects). Similar for all other ""alignment tools"": you throw away some information in order to align. When you map a new dataset into an existing dataset using UMAP, this will do an _exact_ mapping. If you have pronounced batch effects, the second dataset will cluster as a whole far away from the first. So, I don't think that there will be much to gain. Why not give BBKNN (https://github.com/Teichlab/bbknn) a try? It integrates nicely with Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:697,integrability,integr,integrates,697,"UMAP won't do any correction of batch effects for you, like CCA (it looks at the basis that leads to the greatest overlap between the batches, assuming that this captures the common biological variation and projects out everything else, assuming it's nuisance/technical batch effects). Similar for all other ""alignment tools"": you throw away some information in order to align. When you map a new dataset into an existing dataset using UMAP, this will do an _exact_ mapping. If you have pronounced batch effects, the second dataset will cluster as a whole far away from the first. So, I don't think that there will be much to gain. Why not give BBKNN (https://github.com/Teichlab/bbknn) a try? It integrates nicely with Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:697,interoperability,integr,integrates,697,"UMAP won't do any correction of batch effects for you, like CCA (it looks at the basis that leads to the greatest overlap between the batches, assuming that this captures the common biological variation and projects out everything else, assuming it's nuisance/technical batch effects). Similar for all other ""alignment tools"": you throw away some information in order to align. When you map a new dataset into an existing dataset using UMAP, this will do an _exact_ mapping. If you have pronounced batch effects, the second dataset will cluster as a whole far away from the first. So, I don't think that there will be much to gain. Why not give BBKNN (https://github.com/Teichlab/bbknn) a try? It integrates nicely with Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:697,modifiability,integr,integrates,697,"UMAP won't do any correction of batch effects for you, like CCA (it looks at the basis that leads to the greatest overlap between the batches, assuming that this captures the common biological variation and projects out everything else, assuming it's nuisance/technical batch effects). Similar for all other ""alignment tools"": you throw away some information in order to align. When you map a new dataset into an existing dataset using UMAP, this will do an _exact_ mapping. If you have pronounced batch effects, the second dataset will cluster as a whole far away from the first. So, I don't think that there will be much to gain. Why not give BBKNN (https://github.com/Teichlab/bbknn) a try? It integrates nicely with Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:32,performance,batch,batch,32,"UMAP won't do any correction of batch effects for you, like CCA (it looks at the basis that leads to the greatest overlap between the batches, assuming that this captures the common biological variation and projects out everything else, assuming it's nuisance/technical batch effects). Similar for all other ""alignment tools"": you throw away some information in order to align. When you map a new dataset into an existing dataset using UMAP, this will do an _exact_ mapping. If you have pronounced batch effects, the second dataset will cluster as a whole far away from the first. So, I don't think that there will be much to gain. Why not give BBKNN (https://github.com/Teichlab/bbknn) a try? It integrates nicely with Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:134,performance,batch,batches,134,"UMAP won't do any correction of batch effects for you, like CCA (it looks at the basis that leads to the greatest overlap between the batches, assuming that this captures the common biological variation and projects out everything else, assuming it's nuisance/technical batch effects). Similar for all other ""alignment tools"": you throw away some information in order to align. When you map a new dataset into an existing dataset using UMAP, this will do an _exact_ mapping. If you have pronounced batch effects, the second dataset will cluster as a whole far away from the first. So, I don't think that there will be much to gain. Why not give BBKNN (https://github.com/Teichlab/bbknn) a try? It integrates nicely with Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:270,performance,batch,batch,270,"UMAP won't do any correction of batch effects for you, like CCA (it looks at the basis that leads to the greatest overlap between the batches, assuming that this captures the common biological variation and projects out everything else, assuming it's nuisance/technical batch effects). Similar for all other ""alignment tools"": you throw away some information in order to align. When you map a new dataset into an existing dataset using UMAP, this will do an _exact_ mapping. If you have pronounced batch effects, the second dataset will cluster as a whole far away from the first. So, I don't think that there will be much to gain. Why not give BBKNN (https://github.com/Teichlab/bbknn) a try? It integrates nicely with Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:498,performance,batch,batch,498,"UMAP won't do any correction of batch effects for you, like CCA (it looks at the basis that leads to the greatest overlap between the batches, assuming that this captures the common biological variation and projects out everything else, assuming it's nuisance/technical batch effects). Similar for all other ""alignment tools"": you throw away some information in order to align. When you map a new dataset into an existing dataset using UMAP, this will do an _exact_ mapping. If you have pronounced batch effects, the second dataset will cluster as a whole far away from the first. So, I don't think that there will be much to gain. Why not give BBKNN (https://github.com/Teichlab/bbknn) a try? It integrates nicely with Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:697,reliability,integr,integrates,697,"UMAP won't do any correction of batch effects for you, like CCA (it looks at the basis that leads to the greatest overlap between the batches, assuming that this captures the common biological variation and projects out everything else, assuming it's nuisance/technical batch effects). Similar for all other ""alignment tools"": you throw away some information in order to align. When you map a new dataset into an existing dataset using UMAP, this will do an _exact_ mapping. If you have pronounced batch effects, the second dataset will cluster as a whole far away from the first. So, I don't think that there will be much to gain. Why not give BBKNN (https://github.com/Teichlab/bbknn) a try? It integrates nicely with Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:697,security,integr,integrates,697,"UMAP won't do any correction of batch effects for you, like CCA (it looks at the basis that leads to the greatest overlap between the batches, assuming that this captures the common biological variation and projects out everything else, assuming it's nuisance/technical batch effects). Similar for all other ""alignment tools"": you throw away some information in order to align. When you map a new dataset into an existing dataset using UMAP, this will do an _exact_ mapping. If you have pronounced batch effects, the second dataset will cluster as a whole far away from the first. So, I don't think that there will be much to gain. Why not give BBKNN (https://github.com/Teichlab/bbknn) a try? It integrates nicely with Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:697,testability,integr,integrates,697,"UMAP won't do any correction of batch effects for you, like CCA (it looks at the basis that leads to the greatest overlap between the batches, assuming that this captures the common biological variation and projects out everything else, assuming it's nuisance/technical batch effects). Similar for all other ""alignment tools"": you throw away some information in order to align. When you map a new dataset into an existing dataset using UMAP, this will do an _exact_ mapping. If you have pronounced batch effects, the second dataset will cluster as a whole far away from the first. So, I don't think that there will be much to gain. Why not give BBKNN (https://github.com/Teichlab/bbknn) a try? It integrates nicely with Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:319,usability,tool,tools,319,"UMAP won't do any correction of batch effects for you, like CCA (it looks at the basis that leads to the greatest overlap between the batches, assuming that this captures the common biological variation and projects out everything else, assuming it's nuisance/technical batch effects). Similar for all other ""alignment tools"": you throw away some information in order to align. When you map a new dataset into an existing dataset using UMAP, this will do an _exact_ mapping. If you have pronounced batch effects, the second dataset will cluster as a whole far away from the first. So, I don't think that there will be much to gain. Why not give BBKNN (https://github.com/Teichlab/bbknn) a try? It integrates nicely with Scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:89,integrability,batch,batch,89,"@falexwolf you are right, although I have to say that in my (surely limited) experience, batch effect doesn't seem to play a major role when using UMAP. I don't know BBKNN, I will give it a look for sure. I guess this evaluation may be a side project (for volunteers) which may eventually merge into scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:278,integrability,event,eventually,278,"@falexwolf you are right, although I have to say that in my (surely limited) experience, batch effect doesn't seem to play a major role when using UMAP. I don't know BBKNN, I will give it a look for sure. I guess this evaluation may be a side project (for volunteers) which may eventually merge into scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:89,performance,batch,batch,89,"@falexwolf you are right, although I have to say that in my (surely limited) experience, batch effect doesn't seem to play a major role when using UMAP. I don't know BBKNN, I will give it a look for sure. I guess this evaluation may be a side project (for volunteers) which may eventually merge into scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:102,reliability,doe,doesn,102,"@falexwolf you are right, although I have to say that in my (surely limited) experience, batch effect doesn't seem to play a major role when using UMAP. I don't know BBKNN, I will give it a look for sure. I guess this evaluation may be a side project (for volunteers) which may eventually merge into scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:77,usability,experien,experience,77,"@falexwolf you are right, although I have to say that in my (surely limited) experience, batch effect doesn't seem to play a major role when using UMAP. I don't know BBKNN, I will give it a look for sure. I guess this evaluation may be a side project (for volunteers) which may eventually merge into scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:0,energy efficiency,Cool,Cool,0,"Cool! . You're right, if you don't have strong batch effects across your samples, you don't need any batch correction like CCA. A a simple UMAP of all the samples gives you a reasonable picture of what happens.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:47,integrability,batch,batch,47,"Cool! . You're right, if you don't have strong batch effects across your samples, you don't need any batch correction like CCA. A a simple UMAP of all the samples gives you a reasonable picture of what happens.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:101,integrability,batch,batch,101,"Cool! . You're right, if you don't have strong batch effects across your samples, you don't need any batch correction like CCA. A a simple UMAP of all the samples gives you a reasonable picture of what happens.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:47,performance,batch,batch,47,"Cool! . You're right, if you don't have strong batch effects across your samples, you don't need any batch correction like CCA. A a simple UMAP of all the samples gives you a reasonable picture of what happens.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:101,performance,batch,batch,101,"Cool! . You're right, if you don't have strong batch effects across your samples, you don't need any batch correction like CCA. A a simple UMAP of all the samples gives you a reasonable picture of what happens.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:132,testability,simpl,simple,132,"Cool! . You're right, if you don't have strong batch effects across your samples, you don't need any batch correction like CCA. A a simple UMAP of all the samples gives you a reasonable picture of what happens.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:132,usability,simpl,simple,132,"Cool! . You're right, if you don't have strong batch effects across your samples, you don't need any batch correction like CCA. A a simple UMAP of all the samples gives you a reasonable picture of what happens.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:6,deployability,continu,continue,6,"Let’s continue the discussion about a general plugin mechanism in #271, and this thread for CCA specifically.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:46,interoperability,plug,plugin,46,"Let’s continue the discussion about a general plugin mechanism in #271, and this thread for CCA specifically.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:96,interoperability,specif,specifically,96,"Let’s continue the discussion about a general plugin mechanism in #271, and this thread for CCA specifically.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:198,availability,sli,slight,198,"Just curious about the case of 'batch effect', it looks like to me library construction protocol/chemicals is main source of batch effects. However, if I use same protocols, sequencing platform and slight difference of sequencing depth for some sample, but in different time course, would you call it batch effect? A more specific case is, if I have time-course data1 which has not geneX, however, I time-course data2 will have geneX till days later. In mnn correction, a prerequisite is same genes, will it filter out some genes meaningful?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:32,integrability,batch,batch,32,"Just curious about the case of 'batch effect', it looks like to me library construction protocol/chemicals is main source of batch effects. However, if I use same protocols, sequencing platform and slight difference of sequencing depth for some sample, but in different time course, would you call it batch effect? A more specific case is, if I have time-course data1 which has not geneX, however, I time-course data2 will have geneX till days later. In mnn correction, a prerequisite is same genes, will it filter out some genes meaningful?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:88,integrability,protocol,protocol,88,"Just curious about the case of 'batch effect', it looks like to me library construction protocol/chemicals is main source of batch effects. However, if I use same protocols, sequencing platform and slight difference of sequencing depth for some sample, but in different time course, would you call it batch effect? A more specific case is, if I have time-course data1 which has not geneX, however, I time-course data2 will have geneX till days later. In mnn correction, a prerequisite is same genes, will it filter out some genes meaningful?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:125,integrability,batch,batch,125,"Just curious about the case of 'batch effect', it looks like to me library construction protocol/chemicals is main source of batch effects. However, if I use same protocols, sequencing platform and slight difference of sequencing depth for some sample, but in different time course, would you call it batch effect? A more specific case is, if I have time-course data1 which has not geneX, however, I time-course data2 will have geneX till days later. In mnn correction, a prerequisite is same genes, will it filter out some genes meaningful?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:163,integrability,protocol,protocols,163,"Just curious about the case of 'batch effect', it looks like to me library construction protocol/chemicals is main source of batch effects. However, if I use same protocols, sequencing platform and slight difference of sequencing depth for some sample, but in different time course, would you call it batch effect? A more specific case is, if I have time-course data1 which has not geneX, however, I time-course data2 will have geneX till days later. In mnn correction, a prerequisite is same genes, will it filter out some genes meaningful?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:301,integrability,batch,batch,301,"Just curious about the case of 'batch effect', it looks like to me library construction protocol/chemicals is main source of batch effects. However, if I use same protocols, sequencing platform and slight difference of sequencing depth for some sample, but in different time course, would you call it batch effect? A more specific case is, if I have time-course data1 which has not geneX, however, I time-course data2 will have geneX till days later. In mnn correction, a prerequisite is same genes, will it filter out some genes meaningful?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:508,integrability,filter,filter,508,"Just curious about the case of 'batch effect', it looks like to me library construction protocol/chemicals is main source of batch effects. However, if I use same protocols, sequencing platform and slight difference of sequencing depth for some sample, but in different time course, would you call it batch effect? A more specific case is, if I have time-course data1 which has not geneX, however, I time-course data2 will have geneX till days later. In mnn correction, a prerequisite is same genes, will it filter out some genes meaningful?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:88,interoperability,protocol,protocol,88,"Just curious about the case of 'batch effect', it looks like to me library construction protocol/chemicals is main source of batch effects. However, if I use same protocols, sequencing platform and slight difference of sequencing depth for some sample, but in different time course, would you call it batch effect? A more specific case is, if I have time-course data1 which has not geneX, however, I time-course data2 will have geneX till days later. In mnn correction, a prerequisite is same genes, will it filter out some genes meaningful?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:163,interoperability,protocol,protocols,163,"Just curious about the case of 'batch effect', it looks like to me library construction protocol/chemicals is main source of batch effects. However, if I use same protocols, sequencing platform and slight difference of sequencing depth for some sample, but in different time course, would you call it batch effect? A more specific case is, if I have time-course data1 which has not geneX, however, I time-course data2 will have geneX till days later. In mnn correction, a prerequisite is same genes, will it filter out some genes meaningful?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:185,interoperability,platform,platform,185,"Just curious about the case of 'batch effect', it looks like to me library construction protocol/chemicals is main source of batch effects. However, if I use same protocols, sequencing platform and slight difference of sequencing depth for some sample, but in different time course, would you call it batch effect? A more specific case is, if I have time-course data1 which has not geneX, however, I time-course data2 will have geneX till days later. In mnn correction, a prerequisite is same genes, will it filter out some genes meaningful?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:322,interoperability,specif,specific,322,"Just curious about the case of 'batch effect', it looks like to me library construction protocol/chemicals is main source of batch effects. However, if I use same protocols, sequencing platform and slight difference of sequencing depth for some sample, but in different time course, would you call it batch effect? A more specific case is, if I have time-course data1 which has not geneX, however, I time-course data2 will have geneX till days later. In mnn correction, a prerequisite is same genes, will it filter out some genes meaningful?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:32,performance,batch,batch,32,"Just curious about the case of 'batch effect', it looks like to me library construction protocol/chemicals is main source of batch effects. However, if I use same protocols, sequencing platform and slight difference of sequencing depth for some sample, but in different time course, would you call it batch effect? A more specific case is, if I have time-course data1 which has not geneX, however, I time-course data2 will have geneX till days later. In mnn correction, a prerequisite is same genes, will it filter out some genes meaningful?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:125,performance,batch,batch,125,"Just curious about the case of 'batch effect', it looks like to me library construction protocol/chemicals is main source of batch effects. However, if I use same protocols, sequencing platform and slight difference of sequencing depth for some sample, but in different time course, would you call it batch effect? A more specific case is, if I have time-course data1 which has not geneX, however, I time-course data2 will have geneX till days later. In mnn correction, a prerequisite is same genes, will it filter out some genes meaningful?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:270,performance,time,time,270,"Just curious about the case of 'batch effect', it looks like to me library construction protocol/chemicals is main source of batch effects. However, if I use same protocols, sequencing platform and slight difference of sequencing depth for some sample, but in different time course, would you call it batch effect? A more specific case is, if I have time-course data1 which has not geneX, however, I time-course data2 will have geneX till days later. In mnn correction, a prerequisite is same genes, will it filter out some genes meaningful?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:301,performance,batch,batch,301,"Just curious about the case of 'batch effect', it looks like to me library construction protocol/chemicals is main source of batch effects. However, if I use same protocols, sequencing platform and slight difference of sequencing depth for some sample, but in different time course, would you call it batch effect? A more specific case is, if I have time-course data1 which has not geneX, however, I time-course data2 will have geneX till days later. In mnn correction, a prerequisite is same genes, will it filter out some genes meaningful?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:350,performance,time,time-course,350,"Just curious about the case of 'batch effect', it looks like to me library construction protocol/chemicals is main source of batch effects. However, if I use same protocols, sequencing platform and slight difference of sequencing depth for some sample, but in different time course, would you call it batch effect? A more specific case is, if I have time-course data1 which has not geneX, however, I time-course data2 will have geneX till days later. In mnn correction, a prerequisite is same genes, will it filter out some genes meaningful?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:400,performance,time,time-course,400,"Just curious about the case of 'batch effect', it looks like to me library construction protocol/chemicals is main source of batch effects. However, if I use same protocols, sequencing platform and slight difference of sequencing depth for some sample, but in different time course, would you call it batch effect? A more specific case is, if I have time-course data1 which has not geneX, however, I time-course data2 will have geneX till days later. In mnn correction, a prerequisite is same genes, will it filter out some genes meaningful?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:198,reliability,sli,slight,198,"Just curious about the case of 'batch effect', it looks like to me library construction protocol/chemicals is main source of batch effects. However, if I use same protocols, sequencing platform and slight difference of sequencing depth for some sample, but in different time course, would you call it batch effect? A more specific case is, if I have time-course data1 which has not geneX, however, I time-course data2 will have geneX till days later. In mnn correction, a prerequisite is same genes, will it filter out some genes meaningful?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:407,deployability,integr,integrated,407,"In different time course, the batch effect and true biological variation will be entangled. . Batch effects, which occur because measurements are affected by laboratory conditions,reagent lots and personnel differences. This becomes a major problem when batch effects are correlated with an outcome of interest and lead to incorrect conclusions. However, in single cell RNAseq, different datasets should be integrated with suitable algorithm (such as mnn, CCA, bbknn, harmony, scvi et al.), even no batch effect exists.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:129,energy efficiency,measur,measurements,129,"In different time course, the batch effect and true biological variation will be entangled. . Batch effects, which occur because measurements are affected by laboratory conditions,reagent lots and personnel differences. This becomes a major problem when batch effects are correlated with an outcome of interest and lead to incorrect conclusions. However, in single cell RNAseq, different datasets should be integrated with suitable algorithm (such as mnn, CCA, bbknn, harmony, scvi et al.), even no batch effect exists.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:30,integrability,batch,batch,30,"In different time course, the batch effect and true biological variation will be entangled. . Batch effects, which occur because measurements are affected by laboratory conditions,reagent lots and personnel differences. This becomes a major problem when batch effects are correlated with an outcome of interest and lead to incorrect conclusions. However, in single cell RNAseq, different datasets should be integrated with suitable algorithm (such as mnn, CCA, bbknn, harmony, scvi et al.), even no batch effect exists.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:94,integrability,Batch,Batch,94,"In different time course, the batch effect and true biological variation will be entangled. . Batch effects, which occur because measurements are affected by laboratory conditions,reagent lots and personnel differences. This becomes a major problem when batch effects are correlated with an outcome of interest and lead to incorrect conclusions. However, in single cell RNAseq, different datasets should be integrated with suitable algorithm (such as mnn, CCA, bbknn, harmony, scvi et al.), even no batch effect exists.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:254,integrability,batch,batch,254,"In different time course, the batch effect and true biological variation will be entangled. . Batch effects, which occur because measurements are affected by laboratory conditions,reagent lots and personnel differences. This becomes a major problem when batch effects are correlated with an outcome of interest and lead to incorrect conclusions. However, in single cell RNAseq, different datasets should be integrated with suitable algorithm (such as mnn, CCA, bbknn, harmony, scvi et al.), even no batch effect exists.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:407,integrability,integr,integrated,407,"In different time course, the batch effect and true biological variation will be entangled. . Batch effects, which occur because measurements are affected by laboratory conditions,reagent lots and personnel differences. This becomes a major problem when batch effects are correlated with an outcome of interest and lead to incorrect conclusions. However, in single cell RNAseq, different datasets should be integrated with suitable algorithm (such as mnn, CCA, bbknn, harmony, scvi et al.), even no batch effect exists.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:499,integrability,batch,batch,499,"In different time course, the batch effect and true biological variation will be entangled. . Batch effects, which occur because measurements are affected by laboratory conditions,reagent lots and personnel differences. This becomes a major problem when batch effects are correlated with an outcome of interest and lead to incorrect conclusions. However, in single cell RNAseq, different datasets should be integrated with suitable algorithm (such as mnn, CCA, bbknn, harmony, scvi et al.), even no batch effect exists.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:407,interoperability,integr,integrated,407,"In different time course, the batch effect and true biological variation will be entangled. . Batch effects, which occur because measurements are affected by laboratory conditions,reagent lots and personnel differences. This becomes a major problem when batch effects are correlated with an outcome of interest and lead to incorrect conclusions. However, in single cell RNAseq, different datasets should be integrated with suitable algorithm (such as mnn, CCA, bbknn, harmony, scvi et al.), even no batch effect exists.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:407,modifiability,integr,integrated,407,"In different time course, the batch effect and true biological variation will be entangled. . Batch effects, which occur because measurements are affected by laboratory conditions,reagent lots and personnel differences. This becomes a major problem when batch effects are correlated with an outcome of interest and lead to incorrect conclusions. However, in single cell RNAseq, different datasets should be integrated with suitable algorithm (such as mnn, CCA, bbknn, harmony, scvi et al.), even no batch effect exists.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:13,performance,time,time,13,"In different time course, the batch effect and true biological variation will be entangled. . Batch effects, which occur because measurements are affected by laboratory conditions,reagent lots and personnel differences. This becomes a major problem when batch effects are correlated with an outcome of interest and lead to incorrect conclusions. However, in single cell RNAseq, different datasets should be integrated with suitable algorithm (such as mnn, CCA, bbknn, harmony, scvi et al.), even no batch effect exists.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:30,performance,batch,batch,30,"In different time course, the batch effect and true biological variation will be entangled. . Batch effects, which occur because measurements are affected by laboratory conditions,reagent lots and personnel differences. This becomes a major problem when batch effects are correlated with an outcome of interest and lead to incorrect conclusions. However, in single cell RNAseq, different datasets should be integrated with suitable algorithm (such as mnn, CCA, bbknn, harmony, scvi et al.), even no batch effect exists.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:94,performance,Batch,Batch,94,"In different time course, the batch effect and true biological variation will be entangled. . Batch effects, which occur because measurements are affected by laboratory conditions,reagent lots and personnel differences. This becomes a major problem when batch effects are correlated with an outcome of interest and lead to incorrect conclusions. However, in single cell RNAseq, different datasets should be integrated with suitable algorithm (such as mnn, CCA, bbknn, harmony, scvi et al.), even no batch effect exists.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:254,performance,batch,batch,254,"In different time course, the batch effect and true biological variation will be entangled. . Batch effects, which occur because measurements are affected by laboratory conditions,reagent lots and personnel differences. This becomes a major problem when batch effects are correlated with an outcome of interest and lead to incorrect conclusions. However, in single cell RNAseq, different datasets should be integrated with suitable algorithm (such as mnn, CCA, bbknn, harmony, scvi et al.), even no batch effect exists.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:499,performance,batch,batch,499,"In different time course, the batch effect and true biological variation will be entangled. . Batch effects, which occur because measurements are affected by laboratory conditions,reagent lots and personnel differences. This becomes a major problem when batch effects are correlated with an outcome of interest and lead to incorrect conclusions. However, in single cell RNAseq, different datasets should be integrated with suitable algorithm (such as mnn, CCA, bbknn, harmony, scvi et al.), even no batch effect exists.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:407,reliability,integr,integrated,407,"In different time course, the batch effect and true biological variation will be entangled. . Batch effects, which occur because measurements are affected by laboratory conditions,reagent lots and personnel differences. This becomes a major problem when batch effects are correlated with an outcome of interest and lead to incorrect conclusions. However, in single cell RNAseq, different datasets should be integrated with suitable algorithm (such as mnn, CCA, bbknn, harmony, scvi et al.), even no batch effect exists.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:407,security,integr,integrated,407,"In different time course, the batch effect and true biological variation will be entangled. . Batch effects, which occur because measurements are affected by laboratory conditions,reagent lots and personnel differences. This becomes a major problem when batch effects are correlated with an outcome of interest and lead to incorrect conclusions. However, in single cell RNAseq, different datasets should be integrated with suitable algorithm (such as mnn, CCA, bbknn, harmony, scvi et al.), even no batch effect exists.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:407,testability,integr,integrated,407,"In different time course, the batch effect and true biological variation will be entangled. . Batch effects, which occur because measurements are affected by laboratory conditions,reagent lots and personnel differences. This becomes a major problem when batch effects are correlated with an outcome of interest and lead to incorrect conclusions. However, in single cell RNAseq, different datasets should be integrated with suitable algorithm (such as mnn, CCA, bbknn, harmony, scvi et al.), even no batch effect exists.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:197,usability,person,personnel,197,"In different time course, the batch effect and true biological variation will be entangled. . Batch effects, which occur because measurements are affected by laboratory conditions,reagent lots and personnel differences. This becomes a major problem when batch effects are correlated with an outcome of interest and lead to incorrect conclusions. However, in single cell RNAseq, different datasets should be integrated with suitable algorithm (such as mnn, CCA, bbknn, harmony, scvi et al.), even no batch effect exists.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:68,deployability,integr,integrate,68,"> CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers. > . > @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools. Pyscenic has been integrated into scanpy now! Here is the hyper link:. https://github.com/aertslab/pySCENIC/blob/master/notebooks/pySCENIC%20-%20Integration%20with%20scanpy.ipynb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:428,deployability,continu,continue,428,"> CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers. > . > @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools. Pyscenic has been integrated into scanpy now! Here is the hyper link:. https://github.com/aertslab/pySCENIC/blob/master/notebooks/pySCENIC%20-%20Integration%20with%20scanpy.ipynb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:611,deployability,integr,integrated,611,"> CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers. > . > @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools. Pyscenic has been integrated into scanpy now! Here is the hyper link:. https://github.com/aertslab/pySCENIC/blob/master/notebooks/pySCENIC%20-%20Integration%20with%20scanpy.ipynb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:68,integrability,integr,integrate,68,"> CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers. > . > @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools. Pyscenic has been integrated into scanpy now! Here is the hyper link:. https://github.com/aertslab/pySCENIC/blob/master/notebooks/pySCENIC%20-%20Integration%20with%20scanpy.ipynb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:322,integrability,discover,discovering-plugins,322,"> CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers. > . > @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools. Pyscenic has been integrated into scanpy now! Here is the hyper link:. https://github.com/aertslab/pySCENIC/blob/master/notebooks/pySCENIC%20-%20Integration%20with%20scanpy.ipynb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:611,integrability,integr,integrated,611,"> CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers. > . > @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools. Pyscenic has been integrated into scanpy now! Here is the hyper link:. https://github.com/aertslab/pySCENIC/blob/master/notebooks/pySCENIC%20-%20Integration%20with%20scanpy.ipynb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:68,interoperability,integr,integrate,68,"> CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers. > . > @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools. Pyscenic has been integrated into scanpy now! Here is the hyper link:. https://github.com/aertslab/pySCENIC/blob/master/notebooks/pySCENIC%20-%20Integration%20with%20scanpy.ipynb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:210,interoperability,plug,plugins,210,"> CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers. > . > @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools. Pyscenic has been integrated into scanpy now! Here is the hyper link:. https://github.com/aertslab/pySCENIC/blob/master/notebooks/pySCENIC%20-%20Integration%20with%20scanpy.ipynb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:322,interoperability,discover,discovering-plugins,322,"> CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers. > . > @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools. Pyscenic has been integrated into scanpy now! Here is the hyper link:. https://github.com/aertslab/pySCENIC/blob/master/notebooks/pySCENIC%20-%20Integration%20with%20scanpy.ipynb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:611,interoperability,integr,integrated,611,"> CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers. > . > @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools. Pyscenic has been integrated into scanpy now! Here is the hyper link:. https://github.com/aertslab/pySCENIC/blob/master/notebooks/pySCENIC%20-%20Integration%20with%20scanpy.ipynb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:68,modifiability,integr,integrate,68,"> CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers. > . > @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools. Pyscenic has been integrated into scanpy now! Here is the hyper link:. https://github.com/aertslab/pySCENIC/blob/master/notebooks/pySCENIC%20-%20Integration%20with%20scanpy.ipynb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:281,modifiability,pac,packaging,281,"> CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers. > . > @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools. Pyscenic has been integrated into scanpy now! Here is the hyper link:. https://github.com/aertslab/pySCENIC/blob/master/notebooks/pySCENIC%20-%20Integration%20with%20scanpy.ipynb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:611,modifiability,integr,integrated,611,"> CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers. > . > @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools. Pyscenic has been integrated into scanpy now! Here is the hyper link:. https://github.com/aertslab/pySCENIC/blob/master/notebooks/pySCENIC%20-%20Integration%20with%20scanpy.ipynb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:6,reliability,doe,does,6,"> CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers. > . > @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools. Pyscenic has been integrated into scanpy now! Here is the hyper link:. https://github.com/aertslab/pySCENIC/blob/master/notebooks/pySCENIC%20-%20Integration%20with%20scanpy.ipynb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:68,reliability,integr,integrate,68,"> CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers. > . > @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools. Pyscenic has been integrated into scanpy now! Here is the hyper link:. https://github.com/aertslab/pySCENIC/blob/master/notebooks/pySCENIC%20-%20Integration%20with%20scanpy.ipynb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:611,reliability,integr,integrated,611,"> CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers. > . > @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools. Pyscenic has been integrated into scanpy now! Here is the hyper link:. https://github.com/aertslab/pySCENIC/blob/master/notebooks/pySCENIC%20-%20Integration%20with%20scanpy.ipynb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:68,security,integr,integrate,68,"> CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers. > . > @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools. Pyscenic has been integrated into scanpy now! Here is the hyper link:. https://github.com/aertslab/pySCENIC/blob/master/notebooks/pySCENIC%20-%20Integration%20with%20scanpy.ipynb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:611,security,integr,integrated,611,"> CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers. > . > @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools. Pyscenic has been integrated into scanpy now! Here is the hyper link:. https://github.com/aertslab/pySCENIC/blob/master/notebooks/pySCENIC%20-%20Integration%20with%20scanpy.ipynb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:68,testability,integr,integrate,68,"> CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers. > . > @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools. Pyscenic has been integrated into scanpy now! Here is the hyper link:. https://github.com/aertslab/pySCENIC/blob/master/notebooks/pySCENIC%20-%20Integration%20with%20scanpy.ipynb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:611,testability,integr,integrated,611,"> CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers. > . > @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools. Pyscenic has been integrated into scanpy now! Here is the hyper link:. https://github.com/aertslab/pySCENIC/blob/master/notebooks/pySCENIC%20-%20Integration%20with%20scanpy.ipynb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:302,usability,guid,guides,302,"> CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers. > . > @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools. Pyscenic has been integrated into scanpy now! Here is the hyper link:. https://github.com/aertslab/pySCENIC/blob/master/notebooks/pySCENIC%20-%20Integration%20with%20scanpy.ipynb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:322,usability,discov,discovering-plugins,322,"> CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers. > . > @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools. Pyscenic has been integrated into scanpy now! Here is the hyper link:. https://github.com/aertslab/pySCENIC/blob/master/notebooks/pySCENIC%20-%20Integration%20with%20scanpy.ipynb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:417,usability,tool,tools,417,"> CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers. > . > @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools. Pyscenic has been integrated into scanpy now! Here is the hyper link:. https://github.com/aertslab/pySCENIC/blob/master/notebooks/pySCENIC%20-%20Integration%20with%20scanpy.ipynb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:572,usability,visual,visualization,572,"> CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers. > . > @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools. Pyscenic has been integrated into scanpy now! Here is the hyper link:. https://github.com/aertslab/pySCENIC/blob/master/notebooks/pySCENIC%20-%20Integration%20with%20scanpy.ipynb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:586,usability,tool,tools,586,"> CCA does not have code in python, which will make it difficult to integrate, pySCENIC is probably easier but I would rather ask the developers. > . > @falexwolf We should consider a way to facilitate scanpy 'plugins'. A quick search shows me that this could be possible: https://packaging.python.org/guides/creating-and-discovering-plugins/ but honestly I don't know how it works. Nevertheless, given the number of tools that continue to appear we should consider a scheme that facilitate how developers can take advantage of scanpy preprocessing, storing, analysis and visualization tools. Pyscenic has been integrated into scanpy now! Here is the hyper link:. https://github.com/aertslab/pySCENIC/blob/master/notebooks/pySCENIC%20-%20Integration%20with%20scanpy.ipynb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/issues/265:58,modifiability,pac,packages,58,Maybe we should have a section in the docs for linking to packages that work with scanpy?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/265
https://github.com/scverse/scanpy/pull/266:69,availability,cluster,clusters,69,"Ok, makes sense. Things are infinite when you encounter disconnected clusters. Either increase `n_neighbors`, if it makes sense, or you can only meaningfully run DPT on subsets of the data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/266
https://github.com/scverse/scanpy/pull/266:69,deployability,cluster,clusters,69,"Ok, makes sense. Things are infinite when you encounter disconnected clusters. Either increase `n_neighbors`, if it makes sense, or you can only meaningfully run DPT on subsets of the data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/266
https://github.com/scverse/scanpy/pull/266:169,integrability,sub,subsets,169,"Ok, makes sense. Things are infinite when you encounter disconnected clusters. Either increase `n_neighbors`, if it makes sense, or you can only meaningfully run DPT on subsets of the data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/266
https://github.com/scverse/scanpy/issues/267:221,reliability,Doe,Does,221,"I don't know how they do it Seurat, but I'd simply do. ```. filenames = ['name0.h5', 'name1.h5', 'name2.h5']. adatas = [sc.read_10x_h5(filename) for filename in filenames]. adata = adatas[0].concatenate(adatas[1:]). ```. Does this help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:44,testability,simpl,simply,44,"I don't know how they do it Seurat, but I'd simply do. ```. filenames = ['name0.h5', 'name1.h5', 'name2.h5']. adatas = [sc.read_10x_h5(filename) for filename in filenames]. adata = adatas[0].concatenate(adatas[1:]). ```. Does this help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:44,usability,simpl,simply,44,"I don't know how they do it Seurat, but I'd simply do. ```. filenames = ['name0.h5', 'name1.h5', 'name2.h5']. adatas = [sc.read_10x_h5(filename) for filename in filenames]. adata = adatas[0].concatenate(adatas[1:]). ```. Does this help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:231,usability,help,help,231,"I don't know how they do it Seurat, but I'd simply do. ```. filenames = ['name0.h5', 'name1.h5', 'name2.h5']. adatas = [sc.read_10x_h5(filename) for filename in filenames]. adata = adatas[0].concatenate(adatas[1:]). ```. Does this help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:166,deployability,contain,containing,166,"Hi, thanks for the reply. . This example helps already. Thanks. I was thinking more about importing multiple samples from 10X where for each sample you have a folder containing the three files (matrix, barcodes, genes). But I guess I can do something to convert those into .h5 prior to read them into scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:41,usability,help,helps,41,"Hi, thanks for the reply. . This example helps already. Thanks. I was thinking more about importing multiple samples from 10X where for each sample you have a folder containing the three files (matrix, barcodes, genes). But I guess I can do something to convert those into .h5 prior to read them into scanpy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:72,deployability,releas,release,72,"You can do the same as above using `sc.read_10x_mtx`, which is not in a release yet but on GitHub's Master branch. In `.concatenate()` you have the option to pass how you want to name your batches/samples by passing `batch_categories`. PS: Note that I edited the example above to show `sc.read_10x_h5`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:189,integrability,batch,batches,189,"You can do the same as above using `sc.read_10x_mtx`, which is not in a release yet but on GitHub's Master branch. In `.concatenate()` you have the option to pass how you want to name your batches/samples by passing `batch_categories`. PS: Note that I edited the example above to show `sc.read_10x_h5`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:189,performance,batch,batches,189,"You can do the same as above using `sc.read_10x_mtx`, which is not in a release yet but on GitHub's Master branch. In `.concatenate()` you have the option to pass how you want to name your batches/samples by passing `batch_categories`. PS: Note that I edited the example above to show `sc.read_10x_h5`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:193,availability,slo,slow,193,"Hi falexwolf,. I try to use concatenate to read multiple 10X mtx and put them together. But it seems like if I concatenate more than 15 mtx(already stored and read from cache), it becomes very slow. Do you have any advice? Thanks for any information you may provide.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:169,performance,cach,cache,169,"Hi falexwolf,. I try to use concatenate to read multiple 10X mtx and put them together. But it seems like if I concatenate more than 15 mtx(already stored and read from cache), it becomes very slow. Do you have any advice? Thanks for any information you may provide.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:193,reliability,slo,slow,193,"Hi falexwolf,. I try to use concatenate to read multiple 10X mtx and put them together. But it seems like if I concatenate more than 15 mtx(already stored and read from cache), it becomes very slow. Do you have any advice? Thanks for any information you may provide.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:196,availability,error,error,196,"Hi @falexwolf, thanks for the solution you provided above for reading multiple files. I tried it and it worked when I had just 2 files. I am trying the same code with 23 files and I am getting an error message in the concatenation step. Any idea on how to fix this ? Thanks. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-20-662b857d9182> in <module>. 12 adatas.obs['cell_names'] = pd.read_csv(path + sample + 'barcodes.tsv.gz', header=None)[0].values. 13 . ---> 14 adata = adatas[0].concatenate(adatas[1:]). /Applications/anaconda3/lib/python3.7/site-packages/anndata/core/anndata.py in concatenate(self, join, batch_key, batch_categories, index_unique, *adatas). 1908 . 1909 if any_sparse:. -> 1910 sparse_format = all_adatas[0].X.getformat(). 1911 X = X.asformat(sparse_format). 1912 . AttributeError: 'numpy.ndarray' object has no attribute 'getformat'.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:438,deployability,modul,module,438,"Hi @falexwolf, thanks for the solution you provided above for reading multiple files. I tried it and it worked when I had just 2 files. I am trying the same code with 23 files and I am getting an error message in the concatenation step. Any idea on how to fix this ? Thanks. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-20-662b857d9182> in <module>. 12 adatas.obs['cell_names'] = pd.read_csv(path + sample + 'barcodes.tsv.gz', header=None)[0].values. 13 . ---> 14 adata = adatas[0].concatenate(adatas[1:]). /Applications/anaconda3/lib/python3.7/site-packages/anndata/core/anndata.py in concatenate(self, join, batch_key, batch_categories, index_unique, *adatas). 1908 . 1909 if any_sparse:. -> 1910 sparse_format = all_adatas[0].X.getformat(). 1911 X = X.asformat(sparse_format). 1912 . AttributeError: 'numpy.ndarray' object has no attribute 'getformat'.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:664,energy efficiency,core,core,664,"Hi @falexwolf, thanks for the solution you provided above for reading multiple files. I tried it and it worked when I had just 2 files. I am trying the same code with 23 files and I am getting an error message in the concatenation step. Any idea on how to fix this ? Thanks. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-20-662b857d9182> in <module>. 12 adatas.obs['cell_names'] = pd.read_csv(path + sample + 'barcodes.tsv.gz', header=None)[0].values. 13 . ---> 14 adata = adatas[0].concatenate(adatas[1:]). /Applications/anaconda3/lib/python3.7/site-packages/anndata/core/anndata.py in concatenate(self, join, batch_key, batch_categories, index_unique, *adatas). 1908 . 1909 if any_sparse:. -> 1910 sparse_format = all_adatas[0].X.getformat(). 1911 X = X.asformat(sparse_format). 1912 . AttributeError: 'numpy.ndarray' object has no attribute 'getformat'.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:202,integrability,messag,message,202,"Hi @falexwolf, thanks for the solution you provided above for reading multiple files. I tried it and it worked when I had just 2 files. I am trying the same code with 23 files and I am getting an error message in the concatenation step. Any idea on how to fix this ? Thanks. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-20-662b857d9182> in <module>. 12 adatas.obs['cell_names'] = pd.read_csv(path + sample + 'barcodes.tsv.gz', header=None)[0].values. 13 . ---> 14 adata = adatas[0].concatenate(adatas[1:]). /Applications/anaconda3/lib/python3.7/site-packages/anndata/core/anndata.py in concatenate(self, join, batch_key, batch_categories, index_unique, *adatas). 1908 . 1909 if any_sparse:. -> 1910 sparse_format = all_adatas[0].X.getformat(). 1911 X = X.asformat(sparse_format). 1912 . AttributeError: 'numpy.ndarray' object has no attribute 'getformat'.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:202,interoperability,messag,message,202,"Hi @falexwolf, thanks for the solution you provided above for reading multiple files. I tried it and it worked when I had just 2 files. I am trying the same code with 23 files and I am getting an error message in the concatenation step. Any idea on how to fix this ? Thanks. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-20-662b857d9182> in <module>. 12 adatas.obs['cell_names'] = pd.read_csv(path + sample + 'barcodes.tsv.gz', header=None)[0].values. 13 . ---> 14 adata = adatas[0].concatenate(adatas[1:]). /Applications/anaconda3/lib/python3.7/site-packages/anndata/core/anndata.py in concatenate(self, join, batch_key, batch_categories, index_unique, *adatas). 1908 . 1909 if any_sparse:. -> 1910 sparse_format = all_adatas[0].X.getformat(). 1911 X = X.asformat(sparse_format). 1912 . AttributeError: 'numpy.ndarray' object has no attribute 'getformat'.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:438,modifiability,modul,module,438,"Hi @falexwolf, thanks for the solution you provided above for reading multiple files. I tried it and it worked when I had just 2 files. I am trying the same code with 23 files and I am getting an error message in the concatenation step. Any idea on how to fix this ? Thanks. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-20-662b857d9182> in <module>. 12 adatas.obs['cell_names'] = pd.read_csv(path + sample + 'barcodes.tsv.gz', header=None)[0].values. 13 . ---> 14 adata = adatas[0].concatenate(adatas[1:]). /Applications/anaconda3/lib/python3.7/site-packages/anndata/core/anndata.py in concatenate(self, join, batch_key, batch_categories, index_unique, *adatas). 1908 . 1909 if any_sparse:. -> 1910 sparse_format = all_adatas[0].X.getformat(). 1911 X = X.asformat(sparse_format). 1912 . AttributeError: 'numpy.ndarray' object has no attribute 'getformat'.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:647,modifiability,pac,packages,647,"Hi @falexwolf, thanks for the solution you provided above for reading multiple files. I tried it and it worked when I had just 2 files. I am trying the same code with 23 files and I am getting an error message in the concatenation step. Any idea on how to fix this ? Thanks. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-20-662b857d9182> in <module>. 12 adatas.obs['cell_names'] = pd.read_csv(path + sample + 'barcodes.tsv.gz', header=None)[0].values. 13 . ---> 14 adata = adatas[0].concatenate(adatas[1:]). /Applications/anaconda3/lib/python3.7/site-packages/anndata/core/anndata.py in concatenate(self, join, batch_key, batch_categories, index_unique, *adatas). 1908 . 1909 if any_sparse:. -> 1910 sparse_format = all_adatas[0].X.getformat(). 1911 X = X.asformat(sparse_format). 1912 . AttributeError: 'numpy.ndarray' object has no attribute 'getformat'.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:196,performance,error,error,196,"Hi @falexwolf, thanks for the solution you provided above for reading multiple files. I tried it and it worked when I had just 2 files. I am trying the same code with 23 files and I am getting an error message in the concatenation step. Any idea on how to fix this ? Thanks. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-20-662b857d9182> in <module>. 12 adatas.obs['cell_names'] = pd.read_csv(path + sample + 'barcodes.tsv.gz', header=None)[0].values. 13 . ---> 14 adata = adatas[0].concatenate(adatas[1:]). /Applications/anaconda3/lib/python3.7/site-packages/anndata/core/anndata.py in concatenate(self, join, batch_key, batch_categories, index_unique, *adatas). 1908 . 1909 if any_sparse:. -> 1910 sparse_format = all_adatas[0].X.getformat(). 1911 X = X.asformat(sparse_format). 1912 . AttributeError: 'numpy.ndarray' object has no attribute 'getformat'.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:196,safety,error,error,196,"Hi @falexwolf, thanks for the solution you provided above for reading multiple files. I tried it and it worked when I had just 2 files. I am trying the same code with 23 files and I am getting an error message in the concatenation step. Any idea on how to fix this ? Thanks. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-20-662b857d9182> in <module>. 12 adatas.obs['cell_names'] = pd.read_csv(path + sample + 'barcodes.tsv.gz', header=None)[0].values. 13 . ---> 14 adata = adatas[0].concatenate(adatas[1:]). /Applications/anaconda3/lib/python3.7/site-packages/anndata/core/anndata.py in concatenate(self, join, batch_key, batch_categories, index_unique, *adatas). 1908 . 1909 if any_sparse:. -> 1910 sparse_format = all_adatas[0].X.getformat(). 1911 X = X.asformat(sparse_format). 1912 . AttributeError: 'numpy.ndarray' object has no attribute 'getformat'.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:411,safety,input,input-,411,"Hi @falexwolf, thanks for the solution you provided above for reading multiple files. I tried it and it worked when I had just 2 files. I am trying the same code with 23 files and I am getting an error message in the concatenation step. Any idea on how to fix this ? Thanks. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-20-662b857d9182> in <module>. 12 adatas.obs['cell_names'] = pd.read_csv(path + sample + 'barcodes.tsv.gz', header=None)[0].values. 13 . ---> 14 adata = adatas[0].concatenate(adatas[1:]). /Applications/anaconda3/lib/python3.7/site-packages/anndata/core/anndata.py in concatenate(self, join, batch_key, batch_categories, index_unique, *adatas). 1908 . 1909 if any_sparse:. -> 1910 sparse_format = all_adatas[0].X.getformat(). 1911 X = X.asformat(sparse_format). 1912 . AttributeError: 'numpy.ndarray' object has no attribute 'getformat'.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:438,safety,modul,module,438,"Hi @falexwolf, thanks for the solution you provided above for reading multiple files. I tried it and it worked when I had just 2 files. I am trying the same code with 23 files and I am getting an error message in the concatenation step. Any idea on how to fix this ? Thanks. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-20-662b857d9182> in <module>. 12 adatas.obs['cell_names'] = pd.read_csv(path + sample + 'barcodes.tsv.gz', header=None)[0].values. 13 . ---> 14 adata = adatas[0].concatenate(adatas[1:]). /Applications/anaconda3/lib/python3.7/site-packages/anndata/core/anndata.py in concatenate(self, join, batch_key, batch_categories, index_unique, *adatas). 1908 . 1909 if any_sparse:. -> 1910 sparse_format = all_adatas[0].X.getformat(). 1911 X = X.asformat(sparse_format). 1912 . AttributeError: 'numpy.ndarray' object has no attribute 'getformat'.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:367,testability,Trace,Traceback,367,"Hi @falexwolf, thanks for the solution you provided above for reading multiple files. I tried it and it worked when I had just 2 files. I am trying the same code with 23 files and I am getting an error message in the concatenation step. Any idea on how to fix this ? Thanks. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-20-662b857d9182> in <module>. 12 adatas.obs['cell_names'] = pd.read_csv(path + sample + 'barcodes.tsv.gz', header=None)[0].values. 13 . ---> 14 adata = adatas[0].concatenate(adatas[1:]). /Applications/anaconda3/lib/python3.7/site-packages/anndata/core/anndata.py in concatenate(self, join, batch_key, batch_categories, index_unique, *adatas). 1908 . 1909 if any_sparse:. -> 1910 sparse_format = all_adatas[0].X.getformat(). 1911 X = X.asformat(sparse_format). 1912 . AttributeError: 'numpy.ndarray' object has no attribute 'getformat'.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:196,usability,error,error,196,"Hi @falexwolf, thanks for the solution you provided above for reading multiple files. I tried it and it worked when I had just 2 files. I am trying the same code with 23 files and I am getting an error message in the concatenation step. Any idea on how to fix this ? Thanks. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-20-662b857d9182> in <module>. 12 adatas.obs['cell_names'] = pd.read_csv(path + sample + 'barcodes.tsv.gz', header=None)[0].values. 13 . ---> 14 adata = adatas[0].concatenate(adatas[1:]). /Applications/anaconda3/lib/python3.7/site-packages/anndata/core/anndata.py in concatenate(self, join, batch_key, batch_categories, index_unique, *adatas). 1908 . 1909 if any_sparse:. -> 1910 sparse_format = all_adatas[0].X.getformat(). 1911 X = X.asformat(sparse_format). 1912 . AttributeError: 'numpy.ndarray' object has no attribute 'getformat'.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:411,usability,input,input-,411,"Hi @falexwolf, thanks for the solution you provided above for reading multiple files. I tried it and it worked when I had just 2 files. I am trying the same code with 23 files and I am getting an error message in the concatenation step. Any idea on how to fix this ? Thanks. ---------------------------------------------------------------------------. AttributeError Traceback (most recent call last). <ipython-input-20-662b857d9182> in <module>. 12 adatas.obs['cell_names'] = pd.read_csv(path + sample + 'barcodes.tsv.gz', header=None)[0].values. 13 . ---> 14 adata = adatas[0].concatenate(adatas[1:]). /Applications/anaconda3/lib/python3.7/site-packages/anndata/core/anndata.py in concatenate(self, join, batch_key, batch_categories, index_unique, *adatas). 1908 . 1909 if any_sparse:. -> 1910 sparse_format = all_adatas[0].X.getformat(). 1911 X = X.asformat(sparse_format). 1912 . AttributeError: 'numpy.ndarray' object has no attribute 'getformat'.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:92,interoperability,share,share,92,"Hi @elfore, were you able to concatenate your files successfully ? If yes, could you please share the code you used for concatenation ? Thanks.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:442,availability,error,error,442,"Hello,. I am having problems with reading in multiple h5 files using the code snipped that was posted by falexwolf. I am doing:. ```. filenames = ['./a.h5', './b.h5', './c.h5', './d.h5']. adatas = [sc.read_10x_h5(filename, gex_only = True) for filename in filenames]. adata = adatas[0].concatenate(adatas[1:], batch_key='gene_ids', batch_categories=filenames). ```. With or without the batch_key and batch_categories arguments I get the same error:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-20-e23ba2ca6e37> in <module>. 1 filenames = ['./a.h5', './b.h5', './c.h5', './d.h5']. 2 adatas = [sc.read_10x_h5(filename, gex_only = True) for filename in filenames]. ----> 3 adata = adatas[0].concatenate(adatas[1:], batch_key='gene_ids'). ~/anaconda3/lib/python3.7/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas). 1764 fill_value=fill_value,. 1765 index_unique=index_unique,. -> 1766 pairwise=False,. 1767 ). 1768 . ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise). 817 # Annotation for other axis. 818 alt_annot = merge_dataframes(. --> 819 [getattr(a, alt_dim) for a in adatas], alt_indices, merge. 820 ). 821 . ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in merge_dataframes(dfs, new_index, merge_strategy). 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique. 530 ) -> pd.DataFrame:. --> 531 dfs = [df.reindex(index=new_index) for df in dfs]. 532 # New dataframe with all shared data. 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in <listcomp>(.0). 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique. 530 ) -> pd.DataFrame:. --> 531 dfs = [df.reindex(ind",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:2828,availability,toler,tolerance,2828,"nndata/_core/merge.py in <listcomp>(.0). 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique. 530 ) -> pd.DataFrame:. --> 531 dfs = [df.reindex(index=new_index) for df in dfs]. 532 # New dataframe with all shared data. 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 310 @wraps(func). 311 def wrapper(*args, **kwargs) -> Callable[..., Any]:. --> 312 return func(*args, **kwargs). 313 . 314 kind = inspect.Parameter.POSITIONAL_OR_KEYWORD. ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in reindex(self, *args, **kwargs). 4174 kwargs.pop(""axis"", None). 4175 kwargs.pop(""labels"", None). -> 4176 return super().reindex(**kwargs). 4177 . 4178 def drop(. ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4810 # perform the reindex on the axes. 4811 return self._reindex_axes(. -> 4812 axes, level, limit, tolerance, method, fill_value, copy. 4813 ).__finalize__(self, method=""reindex""). 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4021 if index is not None:. 4022 frame = frame._reindex_index(. -> 4023 index, method, copy, level, fill_value, limit, tolerance. 4024 ). 4025 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_index(self, new_index, method, copy, level, fill_value, limit, tolerance). 4043 copy=copy,. 4044 fill_value=fill_value,. -> 4045 allow_dups=False,. 4046 ). 4047 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in _reindex_with_indexers(self, reindexers, fill_value, copy, allow_dups). 4881 fill_value=fill_value,. 4882 allow_dups=allow_dups,. -> 4883 copy=copy,. 4884 ). 4885 # If we've made a copy once, no need to make another one. ~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py in reindex_indexer(self, new_axi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:3021,availability,toler,tolerance,3021,"s]. 532 # New dataframe with all shared data. 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 310 @wraps(func). 311 def wrapper(*args, **kwargs) -> Callable[..., Any]:. --> 312 return func(*args, **kwargs). 313 . 314 kind = inspect.Parameter.POSITIONAL_OR_KEYWORD. ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in reindex(self, *args, **kwargs). 4174 kwargs.pop(""axis"", None). 4175 kwargs.pop(""labels"", None). -> 4176 return super().reindex(**kwargs). 4177 . 4178 def drop(. ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4810 # perform the reindex on the axes. 4811 return self._reindex_axes(. -> 4812 axes, level, limit, tolerance, method, fill_value, copy. 4813 ).__finalize__(self, method=""reindex""). 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4021 if index is not None:. 4022 frame = frame._reindex_index(. -> 4023 index, method, copy, level, fill_value, limit, tolerance. 4024 ). 4025 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_index(self, new_index, method, copy, level, fill_value, limit, tolerance). 4043 copy=copy,. 4044 fill_value=fill_value,. -> 4045 allow_dups=False,. 4046 ). 4047 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in _reindex_with_indexers(self, reindexers, fill_value, copy, allow_dups). 4881 fill_value=fill_value,. 4882 allow_dups=allow_dups,. -> 4883 copy=copy,. 4884 ). 4885 # If we've made a copy once, no need to make another one. ~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py in reindex_indexer(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate, only_slice). 1299 # some axes don't allow reindexing with dups. 1300 if not allow_dups:. -> 1301 self.axes[axis]._can_reindex(indexe",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:3178,availability,toler,tolerance,3178,"/util/_decorators.py in wrapper(*args, **kwargs). 310 @wraps(func). 311 def wrapper(*args, **kwargs) -> Callable[..., Any]:. --> 312 return func(*args, **kwargs). 313 . 314 kind = inspect.Parameter.POSITIONAL_OR_KEYWORD. ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in reindex(self, *args, **kwargs). 4174 kwargs.pop(""axis"", None). 4175 kwargs.pop(""labels"", None). -> 4176 return super().reindex(**kwargs). 4177 . 4178 def drop(. ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4810 # perform the reindex on the axes. 4811 return self._reindex_axes(. -> 4812 axes, level, limit, tolerance, method, fill_value, copy. 4813 ).__finalize__(self, method=""reindex""). 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4021 if index is not None:. 4022 frame = frame._reindex_index(. -> 4023 index, method, copy, level, fill_value, limit, tolerance. 4024 ). 4025 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_index(self, new_index, method, copy, level, fill_value, limit, tolerance). 4043 copy=copy,. 4044 fill_value=fill_value,. -> 4045 allow_dups=False,. 4046 ). 4047 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in _reindex_with_indexers(self, reindexers, fill_value, copy, allow_dups). 4881 fill_value=fill_value,. 4882 allow_dups=allow_dups,. -> 4883 copy=copy,. 4884 ). 4885 # If we've made a copy once, no need to make another one. ~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py in reindex_indexer(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate, only_slice). 1299 # some axes don't allow reindexing with dups. 1300 if not allow_dups:. -> 1301 self.axes[axis]._can_reindex(indexer). 1302 . 1303 if axis >= self.ndim:. ~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py in _can_reindex(self, indexer). 3475 # trying to r",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:3340,availability,toler,tolerance,3340," 313 . 314 kind = inspect.Parameter.POSITIONAL_OR_KEYWORD. ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in reindex(self, *args, **kwargs). 4174 kwargs.pop(""axis"", None). 4175 kwargs.pop(""labels"", None). -> 4176 return super().reindex(**kwargs). 4177 . 4178 def drop(. ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4810 # perform the reindex on the axes. 4811 return self._reindex_axes(. -> 4812 axes, level, limit, tolerance, method, fill_value, copy. 4813 ).__finalize__(self, method=""reindex""). 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4021 if index is not None:. 4022 frame = frame._reindex_index(. -> 4023 index, method, copy, level, fill_value, limit, tolerance. 4024 ). 4025 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_index(self, new_index, method, copy, level, fill_value, limit, tolerance). 4043 copy=copy,. 4044 fill_value=fill_value,. -> 4045 allow_dups=False,. 4046 ). 4047 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in _reindex_with_indexers(self, reindexers, fill_value, copy, allow_dups). 4881 fill_value=fill_value,. 4882 allow_dups=allow_dups,. -> 4883 copy=copy,. 4884 ). 4885 # If we've made a copy once, no need to make another one. ~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py in reindex_indexer(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate, only_slice). 1299 # some axes don't allow reindexing with dups. 1300 if not allow_dups:. -> 1301 self.axes[axis]._can_reindex(indexer). 1302 . 1303 if axis >= self.ndim:. ~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py in _can_reindex(self, indexer). 3475 # trying to reindex on an axis with duplicates. 3476 if not self._index_as_unique and len(indexer):. -> 3477 raise ValueError(""cannot reindex from a duplicate axis""). 3478 . 3",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:4412,availability,toler,tolerance,4412,", tolerance, method, fill_value, copy. 4813 ).__finalize__(self, method=""reindex""). 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4021 if index is not None:. 4022 frame = frame._reindex_index(. -> 4023 index, method, copy, level, fill_value, limit, tolerance. 4024 ). 4025 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_index(self, new_index, method, copy, level, fill_value, limit, tolerance). 4043 copy=copy,. 4044 fill_value=fill_value,. -> 4045 allow_dups=False,. 4046 ). 4047 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in _reindex_with_indexers(self, reindexers, fill_value, copy, allow_dups). 4881 fill_value=fill_value,. 4882 allow_dups=allow_dups,. -> 4883 copy=copy,. 4884 ). 4885 # If we've made a copy once, no need to make another one. ~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py in reindex_indexer(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate, only_slice). 1299 # some axes don't allow reindexing with dups. 1300 if not allow_dups:. -> 1301 self.axes[axis]._can_reindex(indexer). 1302 . 1303 if axis >= self.ndim:. ~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py in _can_reindex(self, indexer). 3475 # trying to reindex on an axis with duplicates. 3476 if not self._index_as_unique and len(indexer):. -> 3477 raise ValueError(""cannot reindex from a duplicate axis""). 3478 . 3479 def reindex(self, target, method=None, level=None, limit=None, tolerance=None):. ValueError: cannot reindex from a duplicate axis. ```. Loading a single h5 file works and produces expected output:. ```. a = sc.read_10x_h5('./a.h5', gex_only = True). a. AnnData object with n_obs × n_vars = 7474 × 31053. var: 'gene_ids', 'feature_types', 'genome'. ```. So the input files appear to be valid I just can't get them to concatenate to a single object. . Any ideas would be welcome.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:614,deployability,modul,module,614,"Hello,. I am having problems with reading in multiple h5 files using the code snipped that was posted by falexwolf. I am doing:. ```. filenames = ['./a.h5', './b.h5', './c.h5', './d.h5']. adatas = [sc.read_10x_h5(filename, gex_only = True) for filename in filenames]. adata = adatas[0].concatenate(adatas[1:], batch_key='gene_ids', batch_categories=filenames). ```. With or without the batch_key and batch_categories arguments I get the same error:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-20-e23ba2ca6e37> in <module>. 1 filenames = ['./a.h5', './b.h5', './c.h5', './d.h5']. 2 adatas = [sc.read_10x_h5(filename, gex_only = True) for filename in filenames]. ----> 3 adata = adatas[0].concatenate(adatas[1:], batch_key='gene_ids'). ~/anaconda3/lib/python3.7/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas). 1764 fill_value=fill_value,. 1765 index_unique=index_unique,. -> 1766 pairwise=False,. 1767 ). 1768 . ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise). 817 # Annotation for other axis. 818 alt_annot = merge_dataframes(. --> 819 [getattr(a, alt_dim) for a in adatas], alt_indices, merge. 820 ). 821 . ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in merge_dataframes(dfs, new_index, merge_strategy). 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique. 530 ) -> pd.DataFrame:. --> 531 dfs = [df.reindex(index=new_index) for df in dfs]. 532 # New dataframe with all shared data. 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in <listcomp>(.0). 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique. 530 ) -> pd.DataFrame:. --> 531 dfs = [df.reindex(ind",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:3789,deployability,manag,managers,3789,"axes(. -> 4812 axes, level, limit, tolerance, method, fill_value, copy. 4813 ).__finalize__(self, method=""reindex""). 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4021 if index is not None:. 4022 frame = frame._reindex_index(. -> 4023 index, method, copy, level, fill_value, limit, tolerance. 4024 ). 4025 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_index(self, new_index, method, copy, level, fill_value, limit, tolerance). 4043 copy=copy,. 4044 fill_value=fill_value,. -> 4045 allow_dups=False,. 4046 ). 4047 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in _reindex_with_indexers(self, reindexers, fill_value, copy, allow_dups). 4881 fill_value=fill_value,. 4882 allow_dups=allow_dups,. -> 4883 copy=copy,. 4884 ). 4885 # If we've made a copy once, no need to make another one. ~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py in reindex_indexer(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate, only_slice). 1299 # some axes don't allow reindexing with dups. 1300 if not allow_dups:. -> 1301 self.axes[axis]._can_reindex(indexer). 1302 . 1303 if axis >= self.ndim:. ~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py in _can_reindex(self, indexer). 3475 # trying to reindex on an axis with duplicates. 3476 if not self._index_as_unique and len(indexer):. -> 3477 raise ValueError(""cannot reindex from a duplicate axis""). 3478 . 3479 def reindex(self, target, method=None, level=None, limit=None, tolerance=None):. ValueError: cannot reindex from a duplicate axis. ```. Loading a single h5 file works and produces expected output:. ```. a = sc.read_10x_h5('./a.h5', gex_only = True). a. AnnData object with n_obs × n_vars = 7474 × 31053. var: 'gene_ids', 'feature_types', 'genome'. ```. So the input files appear to be valid I just can't get them to concatenate to a single obje",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:2451,energy efficiency,core,core,2451,"anndata/_core/merge.py in merge_dataframes(dfs, new_index, merge_strategy). 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique. 530 ) -> pd.DataFrame:. --> 531 dfs = [df.reindex(index=new_index) for df in dfs]. 532 # New dataframe with all shared data. 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in <listcomp>(.0). 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique. 530 ) -> pd.DataFrame:. --> 531 dfs = [df.reindex(index=new_index) for df in dfs]. 532 # New dataframe with all shared data. 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 310 @wraps(func). 311 def wrapper(*args, **kwargs) -> Callable[..., Any]:. --> 312 return func(*args, **kwargs). 313 . 314 kind = inspect.Parameter.POSITIONAL_OR_KEYWORD. ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in reindex(self, *args, **kwargs). 4174 kwargs.pop(""axis"", None). 4175 kwargs.pop(""labels"", None). -> 4176 return super().reindex(**kwargs). 4177 . 4178 def drop(. ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4810 # perform the reindex on the axes. 4811 return self._reindex_axes(. -> 4812 axes, level, limit, tolerance, method, fill_value, copy. 4813 ).__finalize__(self, method=""reindex""). 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4021 if index is not None:. 4022 frame = frame._reindex_index(. -> 4023 index, method, copy, level, fill_value, limit, tolerance. 4024 ). 4025 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_index(self, new_index, method, copy, level, fill_value, limit, tolerance). 4043 copy=copy,. 4044 fill_value=fill_value,. -> 4045 allow_dups=False,. 4046 ). 4047 . ~/anaconda3/l",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:2676,energy efficiency,core,core,2676," dfs]. 532 # New dataframe with all shared data. 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in <listcomp>(.0). 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique. 530 ) -> pd.DataFrame:. --> 531 dfs = [df.reindex(index=new_index) for df in dfs]. 532 # New dataframe with all shared data. 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 310 @wraps(func). 311 def wrapper(*args, **kwargs) -> Callable[..., Any]:. --> 312 return func(*args, **kwargs). 313 . 314 kind = inspect.Parameter.POSITIONAL_OR_KEYWORD. ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in reindex(self, *args, **kwargs). 4174 kwargs.pop(""axis"", None). 4175 kwargs.pop(""labels"", None). -> 4176 return super().reindex(**kwargs). 4177 . 4178 def drop(. ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4810 # perform the reindex on the axes. 4811 return self._reindex_axes(. -> 4812 axes, level, limit, tolerance, method, fill_value, copy. 4813 ).__finalize__(self, method=""reindex""). 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4021 if index is not None:. 4022 frame = frame._reindex_index(. -> 4023 index, method, copy, level, fill_value, limit, tolerance. 4024 ). 4025 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_index(self, new_index, method, copy, level, fill_value, limit, tolerance). 4043 copy=copy,. 4044 fill_value=fill_value,. -> 4045 allow_dups=False,. 4046 ). 4047 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in _reindex_with_indexers(self, reindexers, fill_value, copy, allow_dups). 4881 fill_value=fill_value,. 4882 allow_dups=allow_dups,. -> 4883 copy=copy,. 4884 ). 4885 # If we'v",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:2964,energy efficiency,core,core,2964,"me:. --> 531 dfs = [df.reindex(index=new_index) for df in dfs]. 532 # New dataframe with all shared data. 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 310 @wraps(func). 311 def wrapper(*args, **kwargs) -> Callable[..., Any]:. --> 312 return func(*args, **kwargs). 313 . 314 kind = inspect.Parameter.POSITIONAL_OR_KEYWORD. ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in reindex(self, *args, **kwargs). 4174 kwargs.pop(""axis"", None). 4175 kwargs.pop(""labels"", None). -> 4176 return super().reindex(**kwargs). 4177 . 4178 def drop(. ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4810 # perform the reindex on the axes. 4811 return self._reindex_axes(. -> 4812 axes, level, limit, tolerance, method, fill_value, copy. 4813 ).__finalize__(self, method=""reindex""). 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4021 if index is not None:. 4022 frame = frame._reindex_index(. -> 4023 index, method, copy, level, fill_value, limit, tolerance. 4024 ). 4025 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_index(self, new_index, method, copy, level, fill_value, limit, tolerance). 4043 copy=copy,. 4044 fill_value=fill_value,. -> 4045 allow_dups=False,. 4046 ). 4047 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in _reindex_with_indexers(self, reindexers, fill_value, copy, allow_dups). 4881 fill_value=fill_value,. 4882 allow_dups=allow_dups,. -> 4883 copy=copy,. 4884 ). 4885 # If we've made a copy once, no need to make another one. ~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py in reindex_indexer(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate, only_slice). 1299 # some axes don't allow reindexing with dups. 1300 if ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:3251,energy efficiency,core,core,3251,"1 def wrapper(*args, **kwargs) -> Callable[..., Any]:. --> 312 return func(*args, **kwargs). 313 . 314 kind = inspect.Parameter.POSITIONAL_OR_KEYWORD. ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in reindex(self, *args, **kwargs). 4174 kwargs.pop(""axis"", None). 4175 kwargs.pop(""labels"", None). -> 4176 return super().reindex(**kwargs). 4177 . 4178 def drop(. ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4810 # perform the reindex on the axes. 4811 return self._reindex_axes(. -> 4812 axes, level, limit, tolerance, method, fill_value, copy. 4813 ).__finalize__(self, method=""reindex""). 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4021 if index is not None:. 4022 frame = frame._reindex_index(. -> 4023 index, method, copy, level, fill_value, limit, tolerance. 4024 ). 4025 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_index(self, new_index, method, copy, level, fill_value, limit, tolerance). 4043 copy=copy,. 4044 fill_value=fill_value,. -> 4045 allow_dups=False,. 4046 ). 4047 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in _reindex_with_indexers(self, reindexers, fill_value, copy, allow_dups). 4881 fill_value=fill_value,. 4882 allow_dups=allow_dups,. -> 4883 copy=copy,. 4884 ). 4885 # If we've made a copy once, no need to make another one. ~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py in reindex_indexer(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate, only_slice). 1299 # some axes don't allow reindexing with dups. 1300 if not allow_dups:. -> 1301 self.axes[axis]._can_reindex(indexer). 1302 . 1303 if axis >= self.ndim:. ~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py in _can_reindex(self, indexer). 3475 # trying to reindex on an axis with duplicates. 3476 if not self._index_as_unique a",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:3487,energy efficiency,core,core,3487,"**kwargs). 4174 kwargs.pop(""axis"", None). 4175 kwargs.pop(""labels"", None). -> 4176 return super().reindex(**kwargs). 4177 . 4178 def drop(. ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4810 # perform the reindex on the axes. 4811 return self._reindex_axes(. -> 4812 axes, level, limit, tolerance, method, fill_value, copy. 4813 ).__finalize__(self, method=""reindex""). 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4021 if index is not None:. 4022 frame = frame._reindex_index(. -> 4023 index, method, copy, level, fill_value, limit, tolerance. 4024 ). 4025 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_index(self, new_index, method, copy, level, fill_value, limit, tolerance). 4043 copy=copy,. 4044 fill_value=fill_value,. -> 4045 allow_dups=False,. 4046 ). 4047 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in _reindex_with_indexers(self, reindexers, fill_value, copy, allow_dups). 4881 fill_value=fill_value,. 4882 allow_dups=allow_dups,. -> 4883 copy=copy,. 4884 ). 4885 # If we've made a copy once, no need to make another one. ~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py in reindex_indexer(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate, only_slice). 1299 # some axes don't allow reindexing with dups. 1300 if not allow_dups:. -> 1301 self.axes[axis]._can_reindex(indexer). 1302 . 1303 if axis >= self.ndim:. ~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py in _can_reindex(self, indexer). 3475 # trying to reindex on an axis with duplicates. 3476 if not self._index_as_unique and len(indexer):. -> 3477 raise ValueError(""cannot reindex from a duplicate axis""). 3478 . 3479 def reindex(self, target, method=None, level=None, limit=None, tolerance=None):. ValueError: cannot reindex from a duplicate axis. ```. Load",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:3774,energy efficiency,core,core,3774,"rn self._reindex_axes(. -> 4812 axes, level, limit, tolerance, method, fill_value, copy. 4813 ).__finalize__(self, method=""reindex""). 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4021 if index is not None:. 4022 frame = frame._reindex_index(. -> 4023 index, method, copy, level, fill_value, limit, tolerance. 4024 ). 4025 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_index(self, new_index, method, copy, level, fill_value, limit, tolerance). 4043 copy=copy,. 4044 fill_value=fill_value,. -> 4045 allow_dups=False,. 4046 ). 4047 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in _reindex_with_indexers(self, reindexers, fill_value, copy, allow_dups). 4881 fill_value=fill_value,. 4882 allow_dups=allow_dups,. -> 4883 copy=copy,. 4884 ). 4885 # If we've made a copy once, no need to make another one. ~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py in reindex_indexer(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate, only_slice). 1299 # some axes don't allow reindexing with dups. 1300 if not allow_dups:. -> 1301 self.axes[axis]._can_reindex(indexer). 1302 . 1303 if axis >= self.ndim:. ~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py in _can_reindex(self, indexer). 3475 # trying to reindex on an axis with duplicates. 3476 if not self._index_as_unique and len(indexer):. -> 3477 raise ValueError(""cannot reindex from a duplicate axis""). 3478 . 3479 def reindex(self, target, method=None, level=None, limit=None, tolerance=None):. ValueError: cannot reindex from a duplicate axis. ```. Loading a single h5 file works and produces expected output:. ```. a = sc.read_10x_h5('./a.h5', gex_only = True). a. AnnData object with n_obs × n_vars = 7474 × 31053. var: 'gene_ids', 'feature_types', 'genome'. ```. So the input files appear to be valid I just can't get them to concatenate",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:3789,energy efficiency,manag,managers,3789,"axes(. -> 4812 axes, level, limit, tolerance, method, fill_value, copy. 4813 ).__finalize__(self, method=""reindex""). 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4021 if index is not None:. 4022 frame = frame._reindex_index(. -> 4023 index, method, copy, level, fill_value, limit, tolerance. 4024 ). 4025 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_index(self, new_index, method, copy, level, fill_value, limit, tolerance). 4043 copy=copy,. 4044 fill_value=fill_value,. -> 4045 allow_dups=False,. 4046 ). 4047 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in _reindex_with_indexers(self, reindexers, fill_value, copy, allow_dups). 4881 fill_value=fill_value,. 4882 allow_dups=allow_dups,. -> 4883 copy=copy,. 4884 ). 4885 # If we've made a copy once, no need to make another one. ~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py in reindex_indexer(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate, only_slice). 1299 # some axes don't allow reindexing with dups. 1300 if not allow_dups:. -> 1301 self.axes[axis]._can_reindex(indexer). 1302 . 1303 if axis >= self.ndim:. ~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py in _can_reindex(self, indexer). 3475 # trying to reindex on an axis with duplicates. 3476 if not self._index_as_unique and len(indexer):. -> 3477 raise ValueError(""cannot reindex from a duplicate axis""). 3478 . 3479 def reindex(self, target, method=None, level=None, limit=None, tolerance=None):. ValueError: cannot reindex from a duplicate axis. ```. Loading a single h5 file works and produces expected output:. ```. a = sc.read_10x_h5('./a.h5', gex_only = True). a. AnnData object with n_obs × n_vars = 7474 × 31053. var: 'gene_ids', 'feature_types', 'genome'. ```. So the input files appear to be valid I just can't get them to concatenate to a single obje",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:4112,energy efficiency,core,core,4112,", tolerance, method, fill_value, copy. 4813 ).__finalize__(self, method=""reindex""). 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4021 if index is not None:. 4022 frame = frame._reindex_index(. -> 4023 index, method, copy, level, fill_value, limit, tolerance. 4024 ). 4025 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_index(self, new_index, method, copy, level, fill_value, limit, tolerance). 4043 copy=copy,. 4044 fill_value=fill_value,. -> 4045 allow_dups=False,. 4046 ). 4047 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in _reindex_with_indexers(self, reindexers, fill_value, copy, allow_dups). 4881 fill_value=fill_value,. 4882 allow_dups=allow_dups,. -> 4883 copy=copy,. 4884 ). 4885 # If we've made a copy once, no need to make another one. ~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py in reindex_indexer(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate, only_slice). 1299 # some axes don't allow reindexing with dups. 1300 if not allow_dups:. -> 1301 self.axes[axis]._can_reindex(indexer). 1302 . 1303 if axis >= self.ndim:. ~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py in _can_reindex(self, indexer). 3475 # trying to reindex on an axis with duplicates. 3476 if not self._index_as_unique and len(indexer):. -> 3477 raise ValueError(""cannot reindex from a duplicate axis""). 3478 . 3479 def reindex(self, target, method=None, level=None, limit=None, tolerance=None):. ValueError: cannot reindex from a duplicate axis. ```. Loading a single h5 file works and produces expected output:. ```. a = sc.read_10x_h5('./a.h5', gex_only = True). a. AnnData object with n_obs × n_vars = 7474 × 31053. var: 'gene_ids', 'feature_types', 'genome'. ```. So the input files appear to be valid I just can't get them to concatenate to a single object. . Any ideas would be welcome.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:4485,energy efficiency,Load,Loading,4485,", tolerance, method, fill_value, copy. 4813 ).__finalize__(self, method=""reindex""). 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4021 if index is not None:. 4022 frame = frame._reindex_index(. -> 4023 index, method, copy, level, fill_value, limit, tolerance. 4024 ). 4025 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_index(self, new_index, method, copy, level, fill_value, limit, tolerance). 4043 copy=copy,. 4044 fill_value=fill_value,. -> 4045 allow_dups=False,. 4046 ). 4047 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in _reindex_with_indexers(self, reindexers, fill_value, copy, allow_dups). 4881 fill_value=fill_value,. 4882 allow_dups=allow_dups,. -> 4883 copy=copy,. 4884 ). 4885 # If we've made a copy once, no need to make another one. ~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py in reindex_indexer(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate, only_slice). 1299 # some axes don't allow reindexing with dups. 1300 if not allow_dups:. -> 1301 self.axes[axis]._can_reindex(indexer). 1302 . 1303 if axis >= self.ndim:. ~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py in _can_reindex(self, indexer). 3475 # trying to reindex on an axis with duplicates. 3476 if not self._index_as_unique and len(indexer):. -> 3477 raise ValueError(""cannot reindex from a duplicate axis""). 3478 . 3479 def reindex(self, target, method=None, level=None, limit=None, tolerance=None):. ValueError: cannot reindex from a duplicate axis. ```. Loading a single h5 file works and produces expected output:. ```. a = sc.read_10x_h5('./a.h5', gex_only = True). a. AnnData object with n_obs × n_vars = 7474 × 31053. var: 'gene_ids', 'feature_types', 'genome'. ```. So the input files appear to be valid I just can't get them to concatenate to a single object. . Any ideas would be welcome.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:2207,integrability,wrap,wrapper,2207,"ge, label, keys, index_unique, fill_value, pairwise). 817 # Annotation for other axis. 818 alt_annot = merge_dataframes(. --> 819 [getattr(a, alt_dim) for a in adatas], alt_indices, merge. 820 ). 821 . ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in merge_dataframes(dfs, new_index, merge_strategy). 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique. 530 ) -> pd.DataFrame:. --> 531 dfs = [df.reindex(index=new_index) for df in dfs]. 532 # New dataframe with all shared data. 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in <listcomp>(.0). 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique. 530 ) -> pd.DataFrame:. --> 531 dfs = [df.reindex(index=new_index) for df in dfs]. 532 # New dataframe with all shared data. 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 310 @wraps(func). 311 def wrapper(*args, **kwargs) -> Callable[..., Any]:. --> 312 return func(*args, **kwargs). 313 . 314 kind = inspect.Parameter.POSITIONAL_OR_KEYWORD. ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in reindex(self, *args, **kwargs). 4174 kwargs.pop(""axis"", None). 4175 kwargs.pop(""labels"", None). -> 4176 return super().reindex(**kwargs). 4177 . 4178 def drop(. ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4810 # perform the reindex on the axes. 4811 return self._reindex_axes(. -> 4812 axes, level, limit, tolerance, method, fill_value, copy. 4813 ).__finalize__(self, method=""reindex""). 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4021 if index is not None:. 4022 frame = frame._reindex_index(. -> 4023 index, method, copy, level, fill_value, limit, tolerance. 4024 ). 4025 . ~/anaco",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:2238,integrability,wrap,wraps,2238," fill_value, pairwise). 817 # Annotation for other axis. 818 alt_annot = merge_dataframes(. --> 819 [getattr(a, alt_dim) for a in adatas], alt_indices, merge. 820 ). 821 . ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in merge_dataframes(dfs, new_index, merge_strategy). 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique. 530 ) -> pd.DataFrame:. --> 531 dfs = [df.reindex(index=new_index) for df in dfs]. 532 # New dataframe with all shared data. 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in <listcomp>(.0). 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique. 530 ) -> pd.DataFrame:. --> 531 dfs = [df.reindex(index=new_index) for df in dfs]. 532 # New dataframe with all shared data. 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 310 @wraps(func). 311 def wrapper(*args, **kwargs) -> Callable[..., Any]:. --> 312 return func(*args, **kwargs). 313 . 314 kind = inspect.Parameter.POSITIONAL_OR_KEYWORD. ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in reindex(self, *args, **kwargs). 4174 kwargs.pop(""axis"", None). 4175 kwargs.pop(""labels"", None). -> 4176 return super().reindex(**kwargs). 4177 . 4178 def drop(. ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4810 # perform the reindex on the axes. 4811 return self._reindex_axes(. -> 4812 axes, level, limit, tolerance, method, fill_value, copy. 4813 ).__finalize__(self, method=""reindex""). 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4021 if index is not None:. 4022 frame = frame._reindex_index(. -> 4023 index, method, copy, level, fill_value, limit, tolerance. 4024 ). 4025 . ~/anaconda3/lib/python3.7/site-packag",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:2259,integrability,wrap,wrapper,2259,". 817 # Annotation for other axis. 818 alt_annot = merge_dataframes(. --> 819 [getattr(a, alt_dim) for a in adatas], alt_indices, merge. 820 ). 821 . ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in merge_dataframes(dfs, new_index, merge_strategy). 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique. 530 ) -> pd.DataFrame:. --> 531 dfs = [df.reindex(index=new_index) for df in dfs]. 532 # New dataframe with all shared data. 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in <listcomp>(.0). 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique. 530 ) -> pd.DataFrame:. --> 531 dfs = [df.reindex(index=new_index) for df in dfs]. 532 # New dataframe with all shared data. 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 310 @wraps(func). 311 def wrapper(*args, **kwargs) -> Callable[..., Any]:. --> 312 return func(*args, **kwargs). 313 . 314 kind = inspect.Parameter.POSITIONAL_OR_KEYWORD. ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in reindex(self, *args, **kwargs). 4174 kwargs.pop(""axis"", None). 4175 kwargs.pop(""labels"", None). -> 4176 return super().reindex(**kwargs). 4177 . 4178 def drop(. ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4810 # perform the reindex on the axes. 4811 return self._reindex_axes(. -> 4812 axes, level, limit, tolerance, method, fill_value, copy. 4813 ).__finalize__(self, method=""reindex""). 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4021 if index is not None:. 4022 frame = frame._reindex_index(. -> 4023 index, method, copy, level, fill_value, limit, tolerance. 4024 ). 4025 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:1714,interoperability,share,shared,1714,"ex_only = True) for filename in filenames]. ----> 3 adata = adatas[0].concatenate(adatas[1:], batch_key='gene_ids'). ~/anaconda3/lib/python3.7/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas). 1764 fill_value=fill_value,. 1765 index_unique=index_unique,. -> 1766 pairwise=False,. 1767 ). 1768 . ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise). 817 # Annotation for other axis. 818 alt_annot = merge_dataframes(. --> 819 [getattr(a, alt_dim) for a in adatas], alt_indices, merge. 820 ). 821 . ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in merge_dataframes(dfs, new_index, merge_strategy). 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique. 530 ) -> pd.DataFrame:. --> 531 dfs = [df.reindex(index=new_index) for df in dfs]. 532 # New dataframe with all shared data. 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in <listcomp>(.0). 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique. 530 ) -> pd.DataFrame:. --> 531 dfs = [df.reindex(index=new_index) for df in dfs]. 532 # New dataframe with all shared data. 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 310 @wraps(func). 311 def wrapper(*args, **kwargs) -> Callable[..., Any]:. --> 312 return func(*args, **kwargs). 313 . 314 kind = inspect.Parameter.POSITIONAL_OR_KEYWORD. ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in reindex(self, *args, **kwargs). 4174 kwargs.pop(""axis"", None). 4175 kwargs.pop(""labels"", None). -> 4176 return super().reindex(**kwargs). 4177 . 4178 def drop(. ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in reindex(self, *args, *",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:2059,interoperability,share,shared,2059,". -> 1766 pairwise=False,. 1767 ). 1768 . ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise). 817 # Annotation for other axis. 818 alt_annot = merge_dataframes(. --> 819 [getattr(a, alt_dim) for a in adatas], alt_indices, merge. 820 ). 821 . ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in merge_dataframes(dfs, new_index, merge_strategy). 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique. 530 ) -> pd.DataFrame:. --> 531 dfs = [df.reindex(index=new_index) for df in dfs]. 532 # New dataframe with all shared data. 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in <listcomp>(.0). 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique. 530 ) -> pd.DataFrame:. --> 531 dfs = [df.reindex(index=new_index) for df in dfs]. 532 # New dataframe with all shared data. 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 310 @wraps(func). 311 def wrapper(*args, **kwargs) -> Callable[..., Any]:. --> 312 return func(*args, **kwargs). 313 . 314 kind = inspect.Parameter.POSITIONAL_OR_KEYWORD. ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in reindex(self, *args, **kwargs). 4174 kwargs.pop(""axis"", None). 4175 kwargs.pop(""labels"", None). -> 4176 return super().reindex(**kwargs). 4177 . 4178 def drop(. ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4810 # perform the reindex on the axes. 4811 return self._reindex_axes(. -> 4812 axes, level, limit, tolerance, method, fill_value, copy. 4813 ).__finalize__(self, method=""reindex""). 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 402",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:2207,interoperability,wrapper,wrapper,2207,"ge, label, keys, index_unique, fill_value, pairwise). 817 # Annotation for other axis. 818 alt_annot = merge_dataframes(. --> 819 [getattr(a, alt_dim) for a in adatas], alt_indices, merge. 820 ). 821 . ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in merge_dataframes(dfs, new_index, merge_strategy). 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique. 530 ) -> pd.DataFrame:. --> 531 dfs = [df.reindex(index=new_index) for df in dfs]. 532 # New dataframe with all shared data. 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in <listcomp>(.0). 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique. 530 ) -> pd.DataFrame:. --> 531 dfs = [df.reindex(index=new_index) for df in dfs]. 532 # New dataframe with all shared data. 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 310 @wraps(func). 311 def wrapper(*args, **kwargs) -> Callable[..., Any]:. --> 312 return func(*args, **kwargs). 313 . 314 kind = inspect.Parameter.POSITIONAL_OR_KEYWORD. ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in reindex(self, *args, **kwargs). 4174 kwargs.pop(""axis"", None). 4175 kwargs.pop(""labels"", None). -> 4176 return super().reindex(**kwargs). 4177 . 4178 def drop(. ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4810 # perform the reindex on the axes. 4811 return self._reindex_axes(. -> 4812 axes, level, limit, tolerance, method, fill_value, copy. 4813 ).__finalize__(self, method=""reindex""). 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4021 if index is not None:. 4022 frame = frame._reindex_index(. -> 4023 index, method, copy, level, fill_value, limit, tolerance. 4024 ). 4025 . ~/anaco",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:2259,interoperability,wrapper,wrapper,2259,". 817 # Annotation for other axis. 818 alt_annot = merge_dataframes(. --> 819 [getattr(a, alt_dim) for a in adatas], alt_indices, merge. 820 ). 821 . ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in merge_dataframes(dfs, new_index, merge_strategy). 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique. 530 ) -> pd.DataFrame:. --> 531 dfs = [df.reindex(index=new_index) for df in dfs]. 532 # New dataframe with all shared data. 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in <listcomp>(.0). 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique. 530 ) -> pd.DataFrame:. --> 531 dfs = [df.reindex(index=new_index) for df in dfs]. 532 # New dataframe with all shared data. 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 310 @wraps(func). 311 def wrapper(*args, **kwargs) -> Callable[..., Any]:. --> 312 return func(*args, **kwargs). 313 . 314 kind = inspect.Parameter.POSITIONAL_OR_KEYWORD. ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in reindex(self, *args, **kwargs). 4174 kwargs.pop(""axis"", None). 4175 kwargs.pop(""labels"", None). -> 4176 return super().reindex(**kwargs). 4177 . 4178 def drop(. ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4810 # perform the reindex on the axes. 4811 return self._reindex_axes(. -> 4812 axes, level, limit, tolerance, method, fill_value, copy. 4813 ).__finalize__(self, method=""reindex""). 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4021 if index is not None:. 4022 frame = frame._reindex_index(. -> 4023 index, method, copy, level, fill_value, limit, tolerance. 4024 ). 4025 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:614,modifiability,modul,module,614,"Hello,. I am having problems with reading in multiple h5 files using the code snipped that was posted by falexwolf. I am doing:. ```. filenames = ['./a.h5', './b.h5', './c.h5', './d.h5']. adatas = [sc.read_10x_h5(filename, gex_only = True) for filename in filenames]. adata = adatas[0].concatenate(adatas[1:], batch_key='gene_ids', batch_categories=filenames). ```. With or without the batch_key and batch_categories arguments I get the same error:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-20-e23ba2ca6e37> in <module>. 1 filenames = ['./a.h5', './b.h5', './c.h5', './d.h5']. 2 adatas = [sc.read_10x_h5(filename, gex_only = True) for filename in filenames]. ----> 3 adata = adatas[0].concatenate(adatas[1:], batch_key='gene_ids'). ~/anaconda3/lib/python3.7/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas). 1764 fill_value=fill_value,. 1765 index_unique=index_unique,. -> 1766 pairwise=False,. 1767 ). 1768 . ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise). 817 # Annotation for other axis. 818 alt_annot = merge_dataframes(. --> 819 [getattr(a, alt_dim) for a in adatas], alt_indices, merge. 820 ). 821 . ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in merge_dataframes(dfs, new_index, merge_strategy). 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique. 530 ) -> pd.DataFrame:. --> 531 dfs = [df.reindex(index=new_index) for df in dfs]. 532 # New dataframe with all shared data. 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in <listcomp>(.0). 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique. 530 ) -> pd.DataFrame:. --> 531 dfs = [df.reindex(ind",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:865,modifiability,pac,packages,865,"Hello,. I am having problems with reading in multiple h5 files using the code snipped that was posted by falexwolf. I am doing:. ```. filenames = ['./a.h5', './b.h5', './c.h5', './d.h5']. adatas = [sc.read_10x_h5(filename, gex_only = True) for filename in filenames]. adata = adatas[0].concatenate(adatas[1:], batch_key='gene_ids', batch_categories=filenames). ```. With or without the batch_key and batch_categories arguments I get the same error:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-20-e23ba2ca6e37> in <module>. 1 filenames = ['./a.h5', './b.h5', './c.h5', './d.h5']. 2 adatas = [sc.read_10x_h5(filename, gex_only = True) for filename in filenames]. ----> 3 adata = adatas[0].concatenate(adatas[1:], batch_key='gene_ids'). ~/anaconda3/lib/python3.7/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas). 1764 fill_value=fill_value,. 1765 index_unique=index_unique,. -> 1766 pairwise=False,. 1767 ). 1768 . ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise). 817 # Annotation for other axis. 818 alt_annot = merge_dataframes(. --> 819 [getattr(a, alt_dim) for a in adatas], alt_indices, merge. 820 ). 821 . ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in merge_dataframes(dfs, new_index, merge_strategy). 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique. 530 ) -> pd.DataFrame:. --> 531 dfs = [df.reindex(index=new_index) for df in dfs]. 532 # New dataframe with all shared data. 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in <listcomp>(.0). 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique. 530 ) -> pd.DataFrame:. --> 531 dfs = [df.reindex(ind",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:1135,modifiability,pac,packages,1135,"ames = ['./a.h5', './b.h5', './c.h5', './d.h5']. adatas = [sc.read_10x_h5(filename, gex_only = True) for filename in filenames]. adata = adatas[0].concatenate(adatas[1:], batch_key='gene_ids', batch_categories=filenames). ```. With or without the batch_key and batch_categories arguments I get the same error:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-20-e23ba2ca6e37> in <module>. 1 filenames = ['./a.h5', './b.h5', './c.h5', './d.h5']. 2 adatas = [sc.read_10x_h5(filename, gex_only = True) for filename in filenames]. ----> 3 adata = adatas[0].concatenate(adatas[1:], batch_key='gene_ids'). ~/anaconda3/lib/python3.7/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas). 1764 fill_value=fill_value,. 1765 index_unique=index_unique,. -> 1766 pairwise=False,. 1767 ). 1768 . ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise). 817 # Annotation for other axis. 818 alt_annot = merge_dataframes(. --> 819 [getattr(a, alt_dim) for a in adatas], alt_indices, merge. 820 ). 821 . ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in merge_dataframes(dfs, new_index, merge_strategy). 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique. 530 ) -> pd.DataFrame:. --> 531 dfs = [df.reindex(index=new_index) for df in dfs]. 532 # New dataframe with all shared data. 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in <listcomp>(.0). 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique. 530 ) -> pd.DataFrame:. --> 531 dfs = [df.reindex(index=new_index) for df in dfs]. 532 # New dataframe with all shared data. 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:1444,modifiability,pac,packages,1444,". ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-20-e23ba2ca6e37> in <module>. 1 filenames = ['./a.h5', './b.h5', './c.h5', './d.h5']. 2 adatas = [sc.read_10x_h5(filename, gex_only = True) for filename in filenames]. ----> 3 adata = adatas[0].concatenate(adatas[1:], batch_key='gene_ids'). ~/anaconda3/lib/python3.7/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas). 1764 fill_value=fill_value,. 1765 index_unique=index_unique,. -> 1766 pairwise=False,. 1767 ). 1768 . ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise). 817 # Annotation for other axis. 818 alt_annot = merge_dataframes(. --> 819 [getattr(a, alt_dim) for a in adatas], alt_indices, merge. 820 ). 821 . ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in merge_dataframes(dfs, new_index, merge_strategy). 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique. 530 ) -> pd.DataFrame:. --> 531 dfs = [df.reindex(index=new_index) for df in dfs]. 532 # New dataframe with all shared data. 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in <listcomp>(.0). 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique. 530 ) -> pd.DataFrame:. --> 531 dfs = [df.reindex(index=new_index) for df in dfs]. 532 # New dataframe with all shared data. 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 310 @wraps(func). 311 def wrapper(*args, **kwargs) -> Callable[..., Any]:. --> 312 return func(*args, **kwargs). 313 . 314 kind = inspect.Parameter.POSITIONAL_OR_KEYWORD. ~/anaconda3/lib/python3.7/site-packages/pand",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:1823,modifiability,pac,packages,1823,"ids'). ~/anaconda3/lib/python3.7/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas). 1764 fill_value=fill_value,. 1765 index_unique=index_unique,. -> 1766 pairwise=False,. 1767 ). 1768 . ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise). 817 # Annotation for other axis. 818 alt_annot = merge_dataframes(. --> 819 [getattr(a, alt_dim) for a in adatas], alt_indices, merge. 820 ). 821 . ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in merge_dataframes(dfs, new_index, merge_strategy). 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique. 530 ) -> pd.DataFrame:. --> 531 dfs = [df.reindex(index=new_index) for df in dfs]. 532 # New dataframe with all shared data. 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in <listcomp>(.0). 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique. 530 ) -> pd.DataFrame:. --> 531 dfs = [df.reindex(index=new_index) for df in dfs]. 532 # New dataframe with all shared data. 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 310 @wraps(func). 311 def wrapper(*args, **kwargs) -> Callable[..., Any]:. --> 312 return func(*args, **kwargs). 313 . 314 kind = inspect.Parameter.POSITIONAL_OR_KEYWORD. ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in reindex(self, *args, **kwargs). 4174 kwargs.pop(""axis"", None). 4175 kwargs.pop(""labels"", None). -> 4176 return super().reindex(**kwargs). 4177 . 4178 def drop(. ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4810 # perform the reindex on the axes. 4811 return self._reindex_axes(. -> 4812 axes, level, limit,",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:2168,modifiability,pac,packages,2168,"ncat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise). 817 # Annotation for other axis. 818 alt_annot = merge_dataframes(. --> 819 [getattr(a, alt_dim) for a in adatas], alt_indices, merge. 820 ). 821 . ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in merge_dataframes(dfs, new_index, merge_strategy). 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique. 530 ) -> pd.DataFrame:. --> 531 dfs = [df.reindex(index=new_index) for df in dfs]. 532 # New dataframe with all shared data. 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in <listcomp>(.0). 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique. 530 ) -> pd.DataFrame:. --> 531 dfs = [df.reindex(index=new_index) for df in dfs]. 532 # New dataframe with all shared data. 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 310 @wraps(func). 311 def wrapper(*args, **kwargs) -> Callable[..., Any]:. --> 312 return func(*args, **kwargs). 313 . 314 kind = inspect.Parameter.POSITIONAL_OR_KEYWORD. ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in reindex(self, *args, **kwargs). 4174 kwargs.pop(""axis"", None). 4175 kwargs.pop(""labels"", None). -> 4176 return super().reindex(**kwargs). 4177 . 4178 def drop(. ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4810 # perform the reindex on the axes. 4811 return self._reindex_axes(. -> 4812 axes, level, limit, tolerance, method, fill_value, copy. 4813 ).__finalize__(self, method=""reindex""). 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4021 if index is not None:. 4022 frame = frame._reindex_index(. -> 4023 index, method, copy, level, fill_value, l",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:2371,modifiability,Paramet,Parameter,2371,"s], alt_indices, merge. 820 ). 821 . ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in merge_dataframes(dfs, new_index, merge_strategy). 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique. 530 ) -> pd.DataFrame:. --> 531 dfs = [df.reindex(index=new_index) for df in dfs]. 532 # New dataframe with all shared data. 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in <listcomp>(.0). 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique. 530 ) -> pd.DataFrame:. --> 531 dfs = [df.reindex(index=new_index) for df in dfs]. 532 # New dataframe with all shared data. 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 310 @wraps(func). 311 def wrapper(*args, **kwargs) -> Callable[..., Any]:. --> 312 return func(*args, **kwargs). 313 . 314 kind = inspect.Parameter.POSITIONAL_OR_KEYWORD. ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in reindex(self, *args, **kwargs). 4174 kwargs.pop(""axis"", None). 4175 kwargs.pop(""labels"", None). -> 4176 return super().reindex(**kwargs). 4177 . 4178 def drop(. ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4810 # perform the reindex on the axes. 4811 return self._reindex_axes(. -> 4812 axes, level, limit, tolerance, method, fill_value, copy. 4813 ).__finalize__(self, method=""reindex""). 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4021 if index is not None:. 4022 frame = frame._reindex_index(. -> 4023 index, method, copy, level, fill_value, limit, tolerance. 4024 ). 4025 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_index(self, new_index, method, copy, level, fill_value, limit, tolerance). 4043 copy=copy,. 4044 fi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:2435,modifiability,pac,packages,2435,"site-packages/anndata/_core/merge.py in merge_dataframes(dfs, new_index, merge_strategy). 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique. 530 ) -> pd.DataFrame:. --> 531 dfs = [df.reindex(index=new_index) for df in dfs]. 532 # New dataframe with all shared data. 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in <listcomp>(.0). 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique. 530 ) -> pd.DataFrame:. --> 531 dfs = [df.reindex(index=new_index) for df in dfs]. 532 # New dataframe with all shared data. 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 310 @wraps(func). 311 def wrapper(*args, **kwargs) -> Callable[..., Any]:. --> 312 return func(*args, **kwargs). 313 . 314 kind = inspect.Parameter.POSITIONAL_OR_KEYWORD. ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in reindex(self, *args, **kwargs). 4174 kwargs.pop(""axis"", None). 4175 kwargs.pop(""labels"", None). -> 4176 return super().reindex(**kwargs). 4177 . 4178 def drop(. ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4810 # perform the reindex on the axes. 4811 return self._reindex_axes(. -> 4812 axes, level, limit, tolerance, method, fill_value, copy. 4813 ).__finalize__(self, method=""reindex""). 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4021 if index is not None:. 4022 frame = frame._reindex_index(. -> 4023 index, method, copy, level, fill_value, limit, tolerance. 4024 ). 4025 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_index(self, new_index, method, copy, level, fill_value, limit, tolerance). 4043 copy=copy,. 4044 fill_value=fill_value,. -> 4045 allow_dups=False,. 4046 ). 4047 .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:2660,modifiability,pac,packages,2660,"dex) for df in dfs]. 532 # New dataframe with all shared data. 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in <listcomp>(.0). 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique. 530 ) -> pd.DataFrame:. --> 531 dfs = [df.reindex(index=new_index) for df in dfs]. 532 # New dataframe with all shared data. 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 310 @wraps(func). 311 def wrapper(*args, **kwargs) -> Callable[..., Any]:. --> 312 return func(*args, **kwargs). 313 . 314 kind = inspect.Parameter.POSITIONAL_OR_KEYWORD. ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in reindex(self, *args, **kwargs). 4174 kwargs.pop(""axis"", None). 4175 kwargs.pop(""labels"", None). -> 4176 return super().reindex(**kwargs). 4177 . 4178 def drop(. ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4810 # perform the reindex on the axes. 4811 return self._reindex_axes(. -> 4812 axes, level, limit, tolerance, method, fill_value, copy. 4813 ).__finalize__(self, method=""reindex""). 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4021 if index is not None:. 4022 frame = frame._reindex_index(. -> 4023 index, method, copy, level, fill_value, limit, tolerance. 4024 ). 4025 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_index(self, new_index, method, copy, level, fill_value, limit, tolerance). 4043 copy=copy,. 4044 fill_value=fill_value,. -> 4045 allow_dups=False,. 4046 ). 4047 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in _reindex_with_indexers(self, reindexers, fill_value, copy, allow_dups). 4881 fill_value=fill_value,. 4882 allow_dups=allow_dups,. -> 4883 copy=copy,. 4884 ). ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:2948,modifiability,pac,packages,2948," -> pd.DataFrame:. --> 531 dfs = [df.reindex(index=new_index) for df in dfs]. 532 # New dataframe with all shared data. 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 310 @wraps(func). 311 def wrapper(*args, **kwargs) -> Callable[..., Any]:. --> 312 return func(*args, **kwargs). 313 . 314 kind = inspect.Parameter.POSITIONAL_OR_KEYWORD. ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in reindex(self, *args, **kwargs). 4174 kwargs.pop(""axis"", None). 4175 kwargs.pop(""labels"", None). -> 4176 return super().reindex(**kwargs). 4177 . 4178 def drop(. ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4810 # perform the reindex on the axes. 4811 return self._reindex_axes(. -> 4812 axes, level, limit, tolerance, method, fill_value, copy. 4813 ).__finalize__(self, method=""reindex""). 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4021 if index is not None:. 4022 frame = frame._reindex_index(. -> 4023 index, method, copy, level, fill_value, limit, tolerance. 4024 ). 4025 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_index(self, new_index, method, copy, level, fill_value, limit, tolerance). 4043 copy=copy,. 4044 fill_value=fill_value,. -> 4045 allow_dups=False,. 4046 ). 4047 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in _reindex_with_indexers(self, reindexers, fill_value, copy, allow_dups). 4881 fill_value=fill_value,. 4882 allow_dups=allow_dups,. -> 4883 copy=copy,. 4884 ). 4885 # If we've made a copy once, no need to make another one. ~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py in reindex_indexer(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate, only_slice). 1299 # some axes don't allow reindexing with ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:3235,modifiability,pac,packages,3235,"raps(func). 311 def wrapper(*args, **kwargs) -> Callable[..., Any]:. --> 312 return func(*args, **kwargs). 313 . 314 kind = inspect.Parameter.POSITIONAL_OR_KEYWORD. ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in reindex(self, *args, **kwargs). 4174 kwargs.pop(""axis"", None). 4175 kwargs.pop(""labels"", None). -> 4176 return super().reindex(**kwargs). 4177 . 4178 def drop(. ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4810 # perform the reindex on the axes. 4811 return self._reindex_axes(. -> 4812 axes, level, limit, tolerance, method, fill_value, copy. 4813 ).__finalize__(self, method=""reindex""). 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4021 if index is not None:. 4022 frame = frame._reindex_index(. -> 4023 index, method, copy, level, fill_value, limit, tolerance. 4024 ). 4025 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_index(self, new_index, method, copy, level, fill_value, limit, tolerance). 4043 copy=copy,. 4044 fill_value=fill_value,. -> 4045 allow_dups=False,. 4046 ). 4047 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in _reindex_with_indexers(self, reindexers, fill_value, copy, allow_dups). 4881 fill_value=fill_value,. 4882 allow_dups=allow_dups,. -> 4883 copy=copy,. 4884 ). 4885 # If we've made a copy once, no need to make another one. ~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py in reindex_indexer(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate, only_slice). 1299 # some axes don't allow reindexing with dups. 1300 if not allow_dups:. -> 1301 self.axes[axis]._can_reindex(indexer). 1302 . 1303 if axis >= self.ndim:. ~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py in _can_reindex(self, indexer). 3475 # trying to reindex on an axis with duplicates. 3476 if not self._ind",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:3471,modifiability,pac,packages,3471,"(self, *args, **kwargs). 4174 kwargs.pop(""axis"", None). 4175 kwargs.pop(""labels"", None). -> 4176 return super().reindex(**kwargs). 4177 . 4178 def drop(. ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4810 # perform the reindex on the axes. 4811 return self._reindex_axes(. -> 4812 axes, level, limit, tolerance, method, fill_value, copy. 4813 ).__finalize__(self, method=""reindex""). 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4021 if index is not None:. 4022 frame = frame._reindex_index(. -> 4023 index, method, copy, level, fill_value, limit, tolerance. 4024 ). 4025 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_index(self, new_index, method, copy, level, fill_value, limit, tolerance). 4043 copy=copy,. 4044 fill_value=fill_value,. -> 4045 allow_dups=False,. 4046 ). 4047 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in _reindex_with_indexers(self, reindexers, fill_value, copy, allow_dups). 4881 fill_value=fill_value,. 4882 allow_dups=allow_dups,. -> 4883 copy=copy,. 4884 ). 4885 # If we've made a copy once, no need to make another one. ~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py in reindex_indexer(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate, only_slice). 1299 # some axes don't allow reindexing with dups. 1300 if not allow_dups:. -> 1301 self.axes[axis]._can_reindex(indexer). 1302 . 1303 if axis >= self.ndim:. ~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py in _can_reindex(self, indexer). 3475 # trying to reindex on an axis with duplicates. 3476 if not self._index_as_unique and len(indexer):. -> 3477 raise ValueError(""cannot reindex from a duplicate axis""). 3478 . 3479 def reindex(self, target, method=None, level=None, limit=None, tolerance=None):. ValueError: cannot reindex from a duplicate a",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:3758,modifiability,pac,packages,3758,"xes. 4811 return self._reindex_axes(. -> 4812 axes, level, limit, tolerance, method, fill_value, copy. 4813 ).__finalize__(self, method=""reindex""). 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4021 if index is not None:. 4022 frame = frame._reindex_index(. -> 4023 index, method, copy, level, fill_value, limit, tolerance. 4024 ). 4025 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_index(self, new_index, method, copy, level, fill_value, limit, tolerance). 4043 copy=copy,. 4044 fill_value=fill_value,. -> 4045 allow_dups=False,. 4046 ). 4047 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in _reindex_with_indexers(self, reindexers, fill_value, copy, allow_dups). 4881 fill_value=fill_value,. 4882 allow_dups=allow_dups,. -> 4883 copy=copy,. 4884 ). 4885 # If we've made a copy once, no need to make another one. ~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py in reindex_indexer(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate, only_slice). 1299 # some axes don't allow reindexing with dups. 1300 if not allow_dups:. -> 1301 self.axes[axis]._can_reindex(indexer). 1302 . 1303 if axis >= self.ndim:. ~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py in _can_reindex(self, indexer). 3475 # trying to reindex on an axis with duplicates. 3476 if not self._index_as_unique and len(indexer):. -> 3477 raise ValueError(""cannot reindex from a duplicate axis""). 3478 . 3479 def reindex(self, target, method=None, level=None, limit=None, tolerance=None):. ValueError: cannot reindex from a duplicate axis. ```. Loading a single h5 file works and produces expected output:. ```. a = sc.read_10x_h5('./a.h5', gex_only = True). a. AnnData object with n_obs × n_vars = 7474 × 31053. var: 'gene_ids', 'feature_types', 'genome'. ```. So the input files appear to be valid I just can't get them ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:4096,modifiability,pac,packages,4096,", tolerance, method, fill_value, copy. 4813 ).__finalize__(self, method=""reindex""). 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4021 if index is not None:. 4022 frame = frame._reindex_index(. -> 4023 index, method, copy, level, fill_value, limit, tolerance. 4024 ). 4025 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_index(self, new_index, method, copy, level, fill_value, limit, tolerance). 4043 copy=copy,. 4044 fill_value=fill_value,. -> 4045 allow_dups=False,. 4046 ). 4047 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in _reindex_with_indexers(self, reindexers, fill_value, copy, allow_dups). 4881 fill_value=fill_value,. 4882 allow_dups=allow_dups,. -> 4883 copy=copy,. 4884 ). 4885 # If we've made a copy once, no need to make another one. ~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py in reindex_indexer(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate, only_slice). 1299 # some axes don't allow reindexing with dups. 1300 if not allow_dups:. -> 1301 self.axes[axis]._can_reindex(indexer). 1302 . 1303 if axis >= self.ndim:. ~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py in _can_reindex(self, indexer). 3475 # trying to reindex on an axis with duplicates. 3476 if not self._index_as_unique and len(indexer):. -> 3477 raise ValueError(""cannot reindex from a duplicate axis""). 3478 . 3479 def reindex(self, target, method=None, level=None, limit=None, tolerance=None):. ValueError: cannot reindex from a duplicate axis. ```. Loading a single h5 file works and produces expected output:. ```. a = sc.read_10x_h5('./a.h5', gex_only = True). a. AnnData object with n_obs × n_vars = 7474 × 31053. var: 'gene_ids', 'feature_types', 'genome'. ```. So the input files appear to be valid I just can't get them to concatenate to a single object. . Any ideas would be welcome.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:442,performance,error,error,442,"Hello,. I am having problems with reading in multiple h5 files using the code snipped that was posted by falexwolf. I am doing:. ```. filenames = ['./a.h5', './b.h5', './c.h5', './d.h5']. adatas = [sc.read_10x_h5(filename, gex_only = True) for filename in filenames]. adata = adatas[0].concatenate(adatas[1:], batch_key='gene_ids', batch_categories=filenames). ```. With or without the batch_key and batch_categories arguments I get the same error:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-20-e23ba2ca6e37> in <module>. 1 filenames = ['./a.h5', './b.h5', './c.h5', './d.h5']. 2 adatas = [sc.read_10x_h5(filename, gex_only = True) for filename in filenames]. ----> 3 adata = adatas[0].concatenate(adatas[1:], batch_key='gene_ids'). ~/anaconda3/lib/python3.7/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas). 1764 fill_value=fill_value,. 1765 index_unique=index_unique,. -> 1766 pairwise=False,. 1767 ). 1768 . ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise). 817 # Annotation for other axis. 818 alt_annot = merge_dataframes(. --> 819 [getattr(a, alt_dim) for a in adatas], alt_indices, merge. 820 ). 821 . ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in merge_dataframes(dfs, new_index, merge_strategy). 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique. 530 ) -> pd.DataFrame:. --> 531 dfs = [df.reindex(index=new_index) for df in dfs]. 532 # New dataframe with all shared data. 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in <listcomp>(.0). 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique. 530 ) -> pd.DataFrame:. --> 531 dfs = [df.reindex(ind",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:1644,performance,reindex,reindex,1644,"'./b.h5', './c.h5', './d.h5']. 2 adatas = [sc.read_10x_h5(filename, gex_only = True) for filename in filenames]. ----> 3 adata = adatas[0].concatenate(adatas[1:], batch_key='gene_ids'). ~/anaconda3/lib/python3.7/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas). 1764 fill_value=fill_value,. 1765 index_unique=index_unique,. -> 1766 pairwise=False,. 1767 ). 1768 . ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise). 817 # Annotation for other axis. 818 alt_annot = merge_dataframes(. --> 819 [getattr(a, alt_dim) for a in adatas], alt_indices, merge. 820 ). 821 . ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in merge_dataframes(dfs, new_index, merge_strategy). 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique. 530 ) -> pd.DataFrame:. --> 531 dfs = [df.reindex(index=new_index) for df in dfs]. 532 # New dataframe with all shared data. 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in <listcomp>(.0). 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique. 530 ) -> pd.DataFrame:. --> 531 dfs = [df.reindex(index=new_index) for df in dfs]. 532 # New dataframe with all shared data. 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 310 @wraps(func). 311 def wrapper(*args, **kwargs) -> Callable[..., Any]:. --> 312 return func(*args, **kwargs). 313 . 314 kind = inspect.Parameter.POSITIONAL_OR_KEYWORD. ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in reindex(self, *args, **kwargs). 4174 kwargs.pop(""axis"", None). 4175 kwargs.pop(""labels"", None). -> 4176 return super().reindex(**kwargs). 4177 . 4178 def drop(. ~/anaconda3/lib/pyt",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:1989,performance,reindex,reindex,1989,"adatas). 1764 fill_value=fill_value,. 1765 index_unique=index_unique,. -> 1766 pairwise=False,. 1767 ). 1768 . ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise). 817 # Annotation for other axis. 818 alt_annot = merge_dataframes(. --> 819 [getattr(a, alt_dim) for a in adatas], alt_indices, merge. 820 ). 821 . ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in merge_dataframes(dfs, new_index, merge_strategy). 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique. 530 ) -> pd.DataFrame:. --> 531 dfs = [df.reindex(index=new_index) for df in dfs]. 532 # New dataframe with all shared data. 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in <listcomp>(.0). 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique. 530 ) -> pd.DataFrame:. --> 531 dfs = [df.reindex(index=new_index) for df in dfs]. 532 # New dataframe with all shared data. 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 310 @wraps(func). 311 def wrapper(*args, **kwargs) -> Callable[..., Any]:. --> 312 return func(*args, **kwargs). 313 . 314 kind = inspect.Parameter.POSITIONAL_OR_KEYWORD. ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in reindex(self, *args, **kwargs). 4174 kwargs.pop(""axis"", None). 4175 kwargs.pop(""labels"", None). -> 4176 return super().reindex(**kwargs). 4177 . 4178 def drop(. ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4810 # perform the reindex on the axes. 4811 return self._reindex_axes(. -> 4812 axes, level, limit, tolerance, method, fill_value, copy. 4813 ).__finalize__(self, method=""reindex""). 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axe",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:2468,performance,reindex,reindex,2468,".py in merge_dataframes(dfs, new_index, merge_strategy). 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique. 530 ) -> pd.DataFrame:. --> 531 dfs = [df.reindex(index=new_index) for df in dfs]. 532 # New dataframe with all shared data. 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in <listcomp>(.0). 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique. 530 ) -> pd.DataFrame:. --> 531 dfs = [df.reindex(index=new_index) for df in dfs]. 532 # New dataframe with all shared data. 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 310 @wraps(func). 311 def wrapper(*args, **kwargs) -> Callable[..., Any]:. --> 312 return func(*args, **kwargs). 313 . 314 kind = inspect.Parameter.POSITIONAL_OR_KEYWORD. ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in reindex(self, *args, **kwargs). 4174 kwargs.pop(""axis"", None). 4175 kwargs.pop(""labels"", None). -> 4176 return super().reindex(**kwargs). 4177 . 4178 def drop(. ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4810 # perform the reindex on the axes. 4811 return self._reindex_axes(. -> 4812 axes, level, limit, tolerance, method, fill_value, copy. 4813 ).__finalize__(self, method=""reindex""). 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4021 if index is not None:. 4022 frame = frame._reindex_index(. -> 4023 index, method, copy, level, fill_value, limit, tolerance. 4024 ). 4025 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_index(self, new_index, method, copy, level, fill_value, limit, tolerance). 4043 copy=copy,. 4044 fill_value=fill_value,. -> 4045 allow_dups=False,. 4046 ). 4047 . ~/anaconda3/lib/python3.7/site-p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:2587,performance,reindex,reindex,2587,"ge_unique. 530 ) -> pd.DataFrame:. --> 531 dfs = [df.reindex(index=new_index) for df in dfs]. 532 # New dataframe with all shared data. 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in <listcomp>(.0). 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique. 530 ) -> pd.DataFrame:. --> 531 dfs = [df.reindex(index=new_index) for df in dfs]. 532 # New dataframe with all shared data. 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 310 @wraps(func). 311 def wrapper(*args, **kwargs) -> Callable[..., Any]:. --> 312 return func(*args, **kwargs). 313 . 314 kind = inspect.Parameter.POSITIONAL_OR_KEYWORD. ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in reindex(self, *args, **kwargs). 4174 kwargs.pop(""axis"", None). 4175 kwargs.pop(""labels"", None). -> 4176 return super().reindex(**kwargs). 4177 . 4178 def drop(. ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4810 # perform the reindex on the axes. 4811 return self._reindex_axes(. -> 4812 axes, level, limit, tolerance, method, fill_value, copy. 4813 ).__finalize__(self, method=""reindex""). 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4021 if index is not None:. 4022 frame = frame._reindex_index(. -> 4023 index, method, copy, level, fill_value, limit, tolerance. 4024 ). 4025 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_index(self, new_index, method, copy, level, fill_value, limit, tolerance). 4043 copy=copy,. 4044 fill_value=fill_value,. -> 4045 allow_dups=False,. 4046 ). 4047 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in _reindex_with_indexers(self, reindexers, fill_value, copy, allow_dups). 4881 fill_val",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:2695,performance,reindex,reindex,2695,"frame with all shared data. 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in <listcomp>(.0). 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique. 530 ) -> pd.DataFrame:. --> 531 dfs = [df.reindex(index=new_index) for df in dfs]. 532 # New dataframe with all shared data. 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 310 @wraps(func). 311 def wrapper(*args, **kwargs) -> Callable[..., Any]:. --> 312 return func(*args, **kwargs). 313 . 314 kind = inspect.Parameter.POSITIONAL_OR_KEYWORD. ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in reindex(self, *args, **kwargs). 4174 kwargs.pop(""axis"", None). 4175 kwargs.pop(""labels"", None). -> 4176 return super().reindex(**kwargs). 4177 . 4178 def drop(. ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4810 # perform the reindex on the axes. 4811 return self._reindex_axes(. -> 4812 axes, level, limit, tolerance, method, fill_value, copy. 4813 ).__finalize__(self, method=""reindex""). 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4021 if index is not None:. 4022 frame = frame._reindex_index(. -> 4023 index, method, copy, level, fill_value, limit, tolerance. 4024 ). 4025 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_index(self, new_index, method, copy, level, fill_value, limit, tolerance). 4043 copy=copy,. 4044 fill_value=fill_value,. -> 4045 allow_dups=False,. 4046 ). 4047 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in _reindex_with_indexers(self, reindexers, fill_value, copy, allow_dups). 4881 fill_value=fill_value,. 4882 allow_dups=allow_dups,. -> 4883 copy=copy,. 4884 ). 4885 # If we've made a copy once, n",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:2734,performance,perform,perform,2734,"= pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in <listcomp>(.0). 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique. 530 ) -> pd.DataFrame:. --> 531 dfs = [df.reindex(index=new_index) for df in dfs]. 532 # New dataframe with all shared data. 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 310 @wraps(func). 311 def wrapper(*args, **kwargs) -> Callable[..., Any]:. --> 312 return func(*args, **kwargs). 313 . 314 kind = inspect.Parameter.POSITIONAL_OR_KEYWORD. ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in reindex(self, *args, **kwargs). 4174 kwargs.pop(""axis"", None). 4175 kwargs.pop(""labels"", None). -> 4176 return super().reindex(**kwargs). 4177 . 4178 def drop(. ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4810 # perform the reindex on the axes. 4811 return self._reindex_axes(. -> 4812 axes, level, limit, tolerance, method, fill_value, copy. 4813 ).__finalize__(self, method=""reindex""). 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4021 if index is not None:. 4022 frame = frame._reindex_index(. -> 4023 index, method, copy, level, fill_value, limit, tolerance. 4024 ). 4025 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_index(self, new_index, method, copy, level, fill_value, limit, tolerance). 4043 copy=copy,. 4044 fill_value=fill_value,. -> 4045 allow_dups=False,. 4046 ). 4047 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in _reindex_with_indexers(self, reindexers, fill_value, copy, allow_dups). 4881 fill_value=fill_value,. 4882 allow_dups=allow_dups,. -> 4883 copy=copy,. 4884 ). 4885 # If we've made a copy once, no need to make another one. ~/anaconda3",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:2746,performance,reindex,reindex,2746,"me(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in <listcomp>(.0). 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique. 530 ) -> pd.DataFrame:. --> 531 dfs = [df.reindex(index=new_index) for df in dfs]. 532 # New dataframe with all shared data. 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 310 @wraps(func). 311 def wrapper(*args, **kwargs) -> Callable[..., Any]:. --> 312 return func(*args, **kwargs). 313 . 314 kind = inspect.Parameter.POSITIONAL_OR_KEYWORD. ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in reindex(self, *args, **kwargs). 4174 kwargs.pop(""axis"", None). 4175 kwargs.pop(""labels"", None). -> 4176 return super().reindex(**kwargs). 4177 . 4178 def drop(. ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4810 # perform the reindex on the axes. 4811 return self._reindex_axes(. -> 4812 axes, level, limit, tolerance, method, fill_value, copy. 4813 ).__finalize__(self, method=""reindex""). 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4021 if index is not None:. 4022 frame = frame._reindex_index(. -> 4023 index, method, copy, level, fill_value, limit, tolerance. 4024 ). 4025 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_index(self, new_index, method, copy, level, fill_value, limit, tolerance). 4043 copy=copy,. 4044 fill_value=fill_value,. -> 4045 allow_dups=False,. 4046 ). 4047 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in _reindex_with_indexers(self, reindexers, fill_value, copy, allow_dups). 4881 fill_value=fill_value,. 4882 allow_dups=allow_dups,. -> 4883 copy=copy,. 4884 ). 4885 # If we've made a copy once, no need to make another one. ~/anaconda3/lib/python3",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:2899,performance,reindex,reindex,2899,"e], new_index, merge_strategy=merge_unique. 530 ) -> pd.DataFrame:. --> 531 dfs = [df.reindex(index=new_index) for df in dfs]. 532 # New dataframe with all shared data. 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 310 @wraps(func). 311 def wrapper(*args, **kwargs) -> Callable[..., Any]:. --> 312 return func(*args, **kwargs). 313 . 314 kind = inspect.Parameter.POSITIONAL_OR_KEYWORD. ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in reindex(self, *args, **kwargs). 4174 kwargs.pop(""axis"", None). 4175 kwargs.pop(""labels"", None). -> 4176 return super().reindex(**kwargs). 4177 . 4178 def drop(. ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4810 # perform the reindex on the axes. 4811 return self._reindex_axes(. -> 4812 axes, level, limit, tolerance, method, fill_value, copy. 4813 ).__finalize__(self, method=""reindex""). 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4021 if index is not None:. 4022 frame = frame._reindex_index(. -> 4023 index, method, copy, level, fill_value, limit, tolerance. 4024 ). 4025 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_index(self, new_index, method, copy, level, fill_value, limit, tolerance). 4043 copy=copy,. 4044 fill_value=fill_value,. -> 4045 allow_dups=False,. 4046 ). 4047 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in _reindex_with_indexers(self, reindexers, fill_value, copy, allow_dups). 4881 fill_value=fill_value,. 4882 allow_dups=allow_dups,. -> 4883 copy=copy,. 4884 ). 4885 # If we've made a copy once, no need to make another one. ~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py in reindex_indexer(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate, only_slic",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:3535,performance,reindex,reindexers,3535,"gs.pop(""labels"", None). -> 4176 return super().reindex(**kwargs). 4177 . 4178 def drop(. ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4810 # perform the reindex on the axes. 4811 return self._reindex_axes(. -> 4812 axes, level, limit, tolerance, method, fill_value, copy. 4813 ).__finalize__(self, method=""reindex""). 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4021 if index is not None:. 4022 frame = frame._reindex_index(. -> 4023 index, method, copy, level, fill_value, limit, tolerance. 4024 ). 4025 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_index(self, new_index, method, copy, level, fill_value, limit, tolerance). 4043 copy=copy,. 4044 fill_value=fill_value,. -> 4045 allow_dups=False,. 4046 ). 4047 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in _reindex_with_indexers(self, reindexers, fill_value, copy, allow_dups). 4881 fill_value=fill_value,. 4882 allow_dups=allow_dups,. -> 4883 copy=copy,. 4884 ). 4885 # If we've made a copy once, no need to make another one. ~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py in reindex_indexer(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate, only_slice). 1299 # some axes don't allow reindexing with dups. 1300 if not allow_dups:. -> 1301 self.axes[axis]._can_reindex(indexer). 1302 . 1303 if axis >= self.ndim:. ~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py in _can_reindex(self, indexer). 3475 # trying to reindex on an axis with duplicates. 3476 if not self._index_as_unique and len(indexer):. -> 3477 raise ValueError(""cannot reindex from a duplicate axis""). 3478 . 3479 def reindex(self, target, method=None, level=None, limit=None, tolerance=None):. ValueError: cannot reindex from a duplicate axis. ```. Loading a single h5 file works and produces expected ou",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:3936,performance,reindex,reindexing,3936,", tolerance, method, fill_value, copy. 4813 ).__finalize__(self, method=""reindex""). 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4021 if index is not None:. 4022 frame = frame._reindex_index(. -> 4023 index, method, copy, level, fill_value, limit, tolerance. 4024 ). 4025 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_index(self, new_index, method, copy, level, fill_value, limit, tolerance). 4043 copy=copy,. 4044 fill_value=fill_value,. -> 4045 allow_dups=False,. 4046 ). 4047 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in _reindex_with_indexers(self, reindexers, fill_value, copy, allow_dups). 4881 fill_value=fill_value,. 4882 allow_dups=allow_dups,. -> 4883 copy=copy,. 4884 ). 4885 # If we've made a copy once, no need to make another one. ~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py in reindex_indexer(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate, only_slice). 1299 # some axes don't allow reindexing with dups. 1300 if not allow_dups:. -> 1301 self.axes[axis]._can_reindex(indexer). 1302 . 1303 if axis >= self.ndim:. ~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py in _can_reindex(self, indexer). 3475 # trying to reindex on an axis with duplicates. 3476 if not self._index_as_unique and len(indexer):. -> 3477 raise ValueError(""cannot reindex from a duplicate axis""). 3478 . 3479 def reindex(self, target, method=None, level=None, limit=None, tolerance=None):. ValueError: cannot reindex from a duplicate axis. ```. Loading a single h5 file works and produces expected output:. ```. a = sc.read_10x_h5('./a.h5', gex_only = True). a. AnnData object with n_obs × n_vars = 7474 × 31053. var: 'gene_ids', 'feature_types', 'genome'. ```. So the input files appear to be valid I just can't get them to concatenate to a single object. . Any ideas would be welcome.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:4182,performance,reindex,reindex,4182,", tolerance, method, fill_value, copy. 4813 ).__finalize__(self, method=""reindex""). 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4021 if index is not None:. 4022 frame = frame._reindex_index(. -> 4023 index, method, copy, level, fill_value, limit, tolerance. 4024 ). 4025 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_index(self, new_index, method, copy, level, fill_value, limit, tolerance). 4043 copy=copy,. 4044 fill_value=fill_value,. -> 4045 allow_dups=False,. 4046 ). 4047 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in _reindex_with_indexers(self, reindexers, fill_value, copy, allow_dups). 4881 fill_value=fill_value,. 4882 allow_dups=allow_dups,. -> 4883 copy=copy,. 4884 ). 4885 # If we've made a copy once, no need to make another one. ~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py in reindex_indexer(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate, only_slice). 1299 # some axes don't allow reindexing with dups. 1300 if not allow_dups:. -> 1301 self.axes[axis]._can_reindex(indexer). 1302 . 1303 if axis >= self.ndim:. ~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py in _can_reindex(self, indexer). 3475 # trying to reindex on an axis with duplicates. 3476 if not self._index_as_unique and len(indexer):. -> 3477 raise ValueError(""cannot reindex from a duplicate axis""). 3478 . 3479 def reindex(self, target, method=None, level=None, limit=None, tolerance=None):. ValueError: cannot reindex from a duplicate axis. ```. Loading a single h5 file works and produces expected output:. ```. a = sc.read_10x_h5('./a.h5', gex_only = True). a. AnnData object with n_obs × n_vars = 7474 × 31053. var: 'gene_ids', 'feature_types', 'genome'. ```. So the input files appear to be valid I just can't get them to concatenate to a single object. . Any ideas would be welcome.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:4304,performance,reindex,reindex,4304,", tolerance, method, fill_value, copy. 4813 ).__finalize__(self, method=""reindex""). 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4021 if index is not None:. 4022 frame = frame._reindex_index(. -> 4023 index, method, copy, level, fill_value, limit, tolerance. 4024 ). 4025 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_index(self, new_index, method, copy, level, fill_value, limit, tolerance). 4043 copy=copy,. 4044 fill_value=fill_value,. -> 4045 allow_dups=False,. 4046 ). 4047 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in _reindex_with_indexers(self, reindexers, fill_value, copy, allow_dups). 4881 fill_value=fill_value,. 4882 allow_dups=allow_dups,. -> 4883 copy=copy,. 4884 ). 4885 # If we've made a copy once, no need to make another one. ~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py in reindex_indexer(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate, only_slice). 1299 # some axes don't allow reindexing with dups. 1300 if not allow_dups:. -> 1301 self.axes[axis]._can_reindex(indexer). 1302 . 1303 if axis >= self.ndim:. ~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py in _can_reindex(self, indexer). 3475 # trying to reindex on an axis with duplicates. 3476 if not self._index_as_unique and len(indexer):. -> 3477 raise ValueError(""cannot reindex from a duplicate axis""). 3478 . 3479 def reindex(self, target, method=None, level=None, limit=None, tolerance=None):. ValueError: cannot reindex from a duplicate axis. ```. Loading a single h5 file works and produces expected output:. ```. a = sc.read_10x_h5('./a.h5', gex_only = True). a. AnnData object with n_obs × n_vars = 7474 × 31053. var: 'gene_ids', 'feature_types', 'genome'. ```. So the input files appear to be valid I just can't get them to concatenate to a single object. . Any ideas would be welcome.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:4353,performance,reindex,reindex,4353,", tolerance, method, fill_value, copy. 4813 ).__finalize__(self, method=""reindex""). 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4021 if index is not None:. 4022 frame = frame._reindex_index(. -> 4023 index, method, copy, level, fill_value, limit, tolerance. 4024 ). 4025 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_index(self, new_index, method, copy, level, fill_value, limit, tolerance). 4043 copy=copy,. 4044 fill_value=fill_value,. -> 4045 allow_dups=False,. 4046 ). 4047 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in _reindex_with_indexers(self, reindexers, fill_value, copy, allow_dups). 4881 fill_value=fill_value,. 4882 allow_dups=allow_dups,. -> 4883 copy=copy,. 4884 ). 4885 # If we've made a copy once, no need to make another one. ~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py in reindex_indexer(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate, only_slice). 1299 # some axes don't allow reindexing with dups. 1300 if not allow_dups:. -> 1301 self.axes[axis]._can_reindex(indexer). 1302 . 1303 if axis >= self.ndim:. ~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py in _can_reindex(self, indexer). 3475 # trying to reindex on an axis with duplicates. 3476 if not self._index_as_unique and len(indexer):. -> 3477 raise ValueError(""cannot reindex from a duplicate axis""). 3478 . 3479 def reindex(self, target, method=None, level=None, limit=None, tolerance=None):. ValueError: cannot reindex from a duplicate axis. ```. Loading a single h5 file works and produces expected output:. ```. a = sc.read_10x_h5('./a.h5', gex_only = True). a. AnnData object with n_obs × n_vars = 7474 × 31053. var: 'gene_ids', 'feature_types', 'genome'. ```. So the input files appear to be valid I just can't get them to concatenate to a single object. . Any ideas would be welcome.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:4449,performance,reindex,reindex,4449,", tolerance, method, fill_value, copy. 4813 ).__finalize__(self, method=""reindex""). 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4021 if index is not None:. 4022 frame = frame._reindex_index(. -> 4023 index, method, copy, level, fill_value, limit, tolerance. 4024 ). 4025 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_index(self, new_index, method, copy, level, fill_value, limit, tolerance). 4043 copy=copy,. 4044 fill_value=fill_value,. -> 4045 allow_dups=False,. 4046 ). 4047 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in _reindex_with_indexers(self, reindexers, fill_value, copy, allow_dups). 4881 fill_value=fill_value,. 4882 allow_dups=allow_dups,. -> 4883 copy=copy,. 4884 ). 4885 # If we've made a copy once, no need to make another one. ~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py in reindex_indexer(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate, only_slice). 1299 # some axes don't allow reindexing with dups. 1300 if not allow_dups:. -> 1301 self.axes[axis]._can_reindex(indexer). 1302 . 1303 if axis >= self.ndim:. ~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py in _can_reindex(self, indexer). 3475 # trying to reindex on an axis with duplicates. 3476 if not self._index_as_unique and len(indexer):. -> 3477 raise ValueError(""cannot reindex from a duplicate axis""). 3478 . 3479 def reindex(self, target, method=None, level=None, limit=None, tolerance=None):. ValueError: cannot reindex from a duplicate axis. ```. Loading a single h5 file works and produces expected output:. ```. a = sc.read_10x_h5('./a.h5', gex_only = True). a. AnnData object with n_obs × n_vars = 7474 × 31053. var: 'gene_ids', 'feature_types', 'genome'. ```. So the input files appear to be valid I just can't get them to concatenate to a single object. . Any ideas would be welcome.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:4485,performance,Load,Loading,4485,", tolerance, method, fill_value, copy. 4813 ).__finalize__(self, method=""reindex""). 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4021 if index is not None:. 4022 frame = frame._reindex_index(. -> 4023 index, method, copy, level, fill_value, limit, tolerance. 4024 ). 4025 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_index(self, new_index, method, copy, level, fill_value, limit, tolerance). 4043 copy=copy,. 4044 fill_value=fill_value,. -> 4045 allow_dups=False,. 4046 ). 4047 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in _reindex_with_indexers(self, reindexers, fill_value, copy, allow_dups). 4881 fill_value=fill_value,. 4882 allow_dups=allow_dups,. -> 4883 copy=copy,. 4884 ). 4885 # If we've made a copy once, no need to make another one. ~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py in reindex_indexer(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate, only_slice). 1299 # some axes don't allow reindexing with dups. 1300 if not allow_dups:. -> 1301 self.axes[axis]._can_reindex(indexer). 1302 . 1303 if axis >= self.ndim:. ~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py in _can_reindex(self, indexer). 3475 # trying to reindex on an axis with duplicates. 3476 if not self._index_as_unique and len(indexer):. -> 3477 raise ValueError(""cannot reindex from a duplicate axis""). 3478 . 3479 def reindex(self, target, method=None, level=None, limit=None, tolerance=None):. ValueError: cannot reindex from a duplicate axis. ```. Loading a single h5 file works and produces expected output:. ```. a = sc.read_10x_h5('./a.h5', gex_only = True). a. AnnData object with n_obs × n_vars = 7474 × 31053. var: 'gene_ids', 'feature_types', 'genome'. ```. So the input files appear to be valid I just can't get them to concatenate to a single object. . Any ideas would be welcome.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:2828,reliability,toleran,tolerance,2828,"nndata/_core/merge.py in <listcomp>(.0). 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique. 530 ) -> pd.DataFrame:. --> 531 dfs = [df.reindex(index=new_index) for df in dfs]. 532 # New dataframe with all shared data. 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 310 @wraps(func). 311 def wrapper(*args, **kwargs) -> Callable[..., Any]:. --> 312 return func(*args, **kwargs). 313 . 314 kind = inspect.Parameter.POSITIONAL_OR_KEYWORD. ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in reindex(self, *args, **kwargs). 4174 kwargs.pop(""axis"", None). 4175 kwargs.pop(""labels"", None). -> 4176 return super().reindex(**kwargs). 4177 . 4178 def drop(. ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4810 # perform the reindex on the axes. 4811 return self._reindex_axes(. -> 4812 axes, level, limit, tolerance, method, fill_value, copy. 4813 ).__finalize__(self, method=""reindex""). 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4021 if index is not None:. 4022 frame = frame._reindex_index(. -> 4023 index, method, copy, level, fill_value, limit, tolerance. 4024 ). 4025 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_index(self, new_index, method, copy, level, fill_value, limit, tolerance). 4043 copy=copy,. 4044 fill_value=fill_value,. -> 4045 allow_dups=False,. 4046 ). 4047 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in _reindex_with_indexers(self, reindexers, fill_value, copy, allow_dups). 4881 fill_value=fill_value,. 4882 allow_dups=allow_dups,. -> 4883 copy=copy,. 4884 ). 4885 # If we've made a copy once, no need to make another one. ~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py in reindex_indexer(self, new_axi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:3021,reliability,toleran,tolerance,3021,"s]. 532 # New dataframe with all shared data. 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 310 @wraps(func). 311 def wrapper(*args, **kwargs) -> Callable[..., Any]:. --> 312 return func(*args, **kwargs). 313 . 314 kind = inspect.Parameter.POSITIONAL_OR_KEYWORD. ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in reindex(self, *args, **kwargs). 4174 kwargs.pop(""axis"", None). 4175 kwargs.pop(""labels"", None). -> 4176 return super().reindex(**kwargs). 4177 . 4178 def drop(. ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4810 # perform the reindex on the axes. 4811 return self._reindex_axes(. -> 4812 axes, level, limit, tolerance, method, fill_value, copy. 4813 ).__finalize__(self, method=""reindex""). 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4021 if index is not None:. 4022 frame = frame._reindex_index(. -> 4023 index, method, copy, level, fill_value, limit, tolerance. 4024 ). 4025 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_index(self, new_index, method, copy, level, fill_value, limit, tolerance). 4043 copy=copy,. 4044 fill_value=fill_value,. -> 4045 allow_dups=False,. 4046 ). 4047 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in _reindex_with_indexers(self, reindexers, fill_value, copy, allow_dups). 4881 fill_value=fill_value,. 4882 allow_dups=allow_dups,. -> 4883 copy=copy,. 4884 ). 4885 # If we've made a copy once, no need to make another one. ~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py in reindex_indexer(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate, only_slice). 1299 # some axes don't allow reindexing with dups. 1300 if not allow_dups:. -> 1301 self.axes[axis]._can_reindex(indexe",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:3178,reliability,toleran,tolerance,3178,"/util/_decorators.py in wrapper(*args, **kwargs). 310 @wraps(func). 311 def wrapper(*args, **kwargs) -> Callable[..., Any]:. --> 312 return func(*args, **kwargs). 313 . 314 kind = inspect.Parameter.POSITIONAL_OR_KEYWORD. ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in reindex(self, *args, **kwargs). 4174 kwargs.pop(""axis"", None). 4175 kwargs.pop(""labels"", None). -> 4176 return super().reindex(**kwargs). 4177 . 4178 def drop(. ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4810 # perform the reindex on the axes. 4811 return self._reindex_axes(. -> 4812 axes, level, limit, tolerance, method, fill_value, copy. 4813 ).__finalize__(self, method=""reindex""). 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4021 if index is not None:. 4022 frame = frame._reindex_index(. -> 4023 index, method, copy, level, fill_value, limit, tolerance. 4024 ). 4025 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_index(self, new_index, method, copy, level, fill_value, limit, tolerance). 4043 copy=copy,. 4044 fill_value=fill_value,. -> 4045 allow_dups=False,. 4046 ). 4047 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in _reindex_with_indexers(self, reindexers, fill_value, copy, allow_dups). 4881 fill_value=fill_value,. 4882 allow_dups=allow_dups,. -> 4883 copy=copy,. 4884 ). 4885 # If we've made a copy once, no need to make another one. ~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py in reindex_indexer(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate, only_slice). 1299 # some axes don't allow reindexing with dups. 1300 if not allow_dups:. -> 1301 self.axes[axis]._can_reindex(indexer). 1302 . 1303 if axis >= self.ndim:. ~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py in _can_reindex(self, indexer). 3475 # trying to r",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:3340,reliability,toleran,tolerance,3340," 313 . 314 kind = inspect.Parameter.POSITIONAL_OR_KEYWORD. ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in reindex(self, *args, **kwargs). 4174 kwargs.pop(""axis"", None). 4175 kwargs.pop(""labels"", None). -> 4176 return super().reindex(**kwargs). 4177 . 4178 def drop(. ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4810 # perform the reindex on the axes. 4811 return self._reindex_axes(. -> 4812 axes, level, limit, tolerance, method, fill_value, copy. 4813 ).__finalize__(self, method=""reindex""). 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4021 if index is not None:. 4022 frame = frame._reindex_index(. -> 4023 index, method, copy, level, fill_value, limit, tolerance. 4024 ). 4025 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_index(self, new_index, method, copy, level, fill_value, limit, tolerance). 4043 copy=copy,. 4044 fill_value=fill_value,. -> 4045 allow_dups=False,. 4046 ). 4047 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in _reindex_with_indexers(self, reindexers, fill_value, copy, allow_dups). 4881 fill_value=fill_value,. 4882 allow_dups=allow_dups,. -> 4883 copy=copy,. 4884 ). 4885 # If we've made a copy once, no need to make another one. ~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py in reindex_indexer(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate, only_slice). 1299 # some axes don't allow reindexing with dups. 1300 if not allow_dups:. -> 1301 self.axes[axis]._can_reindex(indexer). 1302 . 1303 if axis >= self.ndim:. ~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py in _can_reindex(self, indexer). 3475 # trying to reindex on an axis with duplicates. 3476 if not self._index_as_unique and len(indexer):. -> 3477 raise ValueError(""cannot reindex from a duplicate axis""). 3478 . 3",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:4412,reliability,toleran,tolerance,4412,", tolerance, method, fill_value, copy. 4813 ).__finalize__(self, method=""reindex""). 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4021 if index is not None:. 4022 frame = frame._reindex_index(. -> 4023 index, method, copy, level, fill_value, limit, tolerance. 4024 ). 4025 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_index(self, new_index, method, copy, level, fill_value, limit, tolerance). 4043 copy=copy,. 4044 fill_value=fill_value,. -> 4045 allow_dups=False,. 4046 ). 4047 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in _reindex_with_indexers(self, reindexers, fill_value, copy, allow_dups). 4881 fill_value=fill_value,. 4882 allow_dups=allow_dups,. -> 4883 copy=copy,. 4884 ). 4885 # If we've made a copy once, no need to make another one. ~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py in reindex_indexer(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate, only_slice). 1299 # some axes don't allow reindexing with dups. 1300 if not allow_dups:. -> 1301 self.axes[axis]._can_reindex(indexer). 1302 . 1303 if axis >= self.ndim:. ~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py in _can_reindex(self, indexer). 3475 # trying to reindex on an axis with duplicates. 3476 if not self._index_as_unique and len(indexer):. -> 3477 raise ValueError(""cannot reindex from a duplicate axis""). 3478 . 3479 def reindex(self, target, method=None, level=None, limit=None, tolerance=None):. ValueError: cannot reindex from a duplicate axis. ```. Loading a single h5 file works and produces expected output:. ```. a = sc.read_10x_h5('./a.h5', gex_only = True). a. AnnData object with n_obs × n_vars = 7474 × 31053. var: 'gene_ids', 'feature_types', 'genome'. ```. So the input files appear to be valid I just can't get them to concatenate to a single object. . Any ideas would be welcome.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:442,safety,error,error,442,"Hello,. I am having problems with reading in multiple h5 files using the code snipped that was posted by falexwolf. I am doing:. ```. filenames = ['./a.h5', './b.h5', './c.h5', './d.h5']. adatas = [sc.read_10x_h5(filename, gex_only = True) for filename in filenames]. adata = adatas[0].concatenate(adatas[1:], batch_key='gene_ids', batch_categories=filenames). ```. With or without the batch_key and batch_categories arguments I get the same error:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-20-e23ba2ca6e37> in <module>. 1 filenames = ['./a.h5', './b.h5', './c.h5', './d.h5']. 2 adatas = [sc.read_10x_h5(filename, gex_only = True) for filename in filenames]. ----> 3 adata = adatas[0].concatenate(adatas[1:], batch_key='gene_ids'). ~/anaconda3/lib/python3.7/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas). 1764 fill_value=fill_value,. 1765 index_unique=index_unique,. -> 1766 pairwise=False,. 1767 ). 1768 . ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise). 817 # Annotation for other axis. 818 alt_annot = merge_dataframes(. --> 819 [getattr(a, alt_dim) for a in adatas], alt_indices, merge. 820 ). 821 . ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in merge_dataframes(dfs, new_index, merge_strategy). 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique. 530 ) -> pd.DataFrame:. --> 531 dfs = [df.reindex(index=new_index) for df in dfs]. 532 # New dataframe with all shared data. 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in <listcomp>(.0). 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique. 530 ) -> pd.DataFrame:. --> 531 dfs = [df.reindex(ind",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:587,safety,input,input-,587,"Hello,. I am having problems with reading in multiple h5 files using the code snipped that was posted by falexwolf. I am doing:. ```. filenames = ['./a.h5', './b.h5', './c.h5', './d.h5']. adatas = [sc.read_10x_h5(filename, gex_only = True) for filename in filenames]. adata = adatas[0].concatenate(adatas[1:], batch_key='gene_ids', batch_categories=filenames). ```. With or without the batch_key and batch_categories arguments I get the same error:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-20-e23ba2ca6e37> in <module>. 1 filenames = ['./a.h5', './b.h5', './c.h5', './d.h5']. 2 adatas = [sc.read_10x_h5(filename, gex_only = True) for filename in filenames]. ----> 3 adata = adatas[0].concatenate(adatas[1:], batch_key='gene_ids'). ~/anaconda3/lib/python3.7/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas). 1764 fill_value=fill_value,. 1765 index_unique=index_unique,. -> 1766 pairwise=False,. 1767 ). 1768 . ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise). 817 # Annotation for other axis. 818 alt_annot = merge_dataframes(. --> 819 [getattr(a, alt_dim) for a in adatas], alt_indices, merge. 820 ). 821 . ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in merge_dataframes(dfs, new_index, merge_strategy). 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique. 530 ) -> pd.DataFrame:. --> 531 dfs = [df.reindex(index=new_index) for df in dfs]. 532 # New dataframe with all shared data. 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in <listcomp>(.0). 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique. 530 ) -> pd.DataFrame:. --> 531 dfs = [df.reindex(ind",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:614,safety,modul,module,614,"Hello,. I am having problems with reading in multiple h5 files using the code snipped that was posted by falexwolf. I am doing:. ```. filenames = ['./a.h5', './b.h5', './c.h5', './d.h5']. adatas = [sc.read_10x_h5(filename, gex_only = True) for filename in filenames]. adata = adatas[0].concatenate(adatas[1:], batch_key='gene_ids', batch_categories=filenames). ```. With or without the batch_key and batch_categories arguments I get the same error:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-20-e23ba2ca6e37> in <module>. 1 filenames = ['./a.h5', './b.h5', './c.h5', './d.h5']. 2 adatas = [sc.read_10x_h5(filename, gex_only = True) for filename in filenames]. ----> 3 adata = adatas[0].concatenate(adatas[1:], batch_key='gene_ids'). ~/anaconda3/lib/python3.7/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas). 1764 fill_value=fill_value,. 1765 index_unique=index_unique,. -> 1766 pairwise=False,. 1767 ). 1768 . ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise). 817 # Annotation for other axis. 818 alt_annot = merge_dataframes(. --> 819 [getattr(a, alt_dim) for a in adatas], alt_indices, merge. 820 ). 821 . ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in merge_dataframes(dfs, new_index, merge_strategy). 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique. 530 ) -> pd.DataFrame:. --> 531 dfs = [df.reindex(index=new_index) for df in dfs]. 532 # New dataframe with all shared data. 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in <listcomp>(.0). 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique. 530 ) -> pd.DataFrame:. --> 531 dfs = [df.reindex(ind",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:3789,safety,manag,managers,3789,"axes(. -> 4812 axes, level, limit, tolerance, method, fill_value, copy. 4813 ).__finalize__(self, method=""reindex""). 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4021 if index is not None:. 4022 frame = frame._reindex_index(. -> 4023 index, method, copy, level, fill_value, limit, tolerance. 4024 ). 4025 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_index(self, new_index, method, copy, level, fill_value, limit, tolerance). 4043 copy=copy,. 4044 fill_value=fill_value,. -> 4045 allow_dups=False,. 4046 ). 4047 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in _reindex_with_indexers(self, reindexers, fill_value, copy, allow_dups). 4881 fill_value=fill_value,. 4882 allow_dups=allow_dups,. -> 4883 copy=copy,. 4884 ). 4885 # If we've made a copy once, no need to make another one. ~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py in reindex_indexer(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate, only_slice). 1299 # some axes don't allow reindexing with dups. 1300 if not allow_dups:. -> 1301 self.axes[axis]._can_reindex(indexer). 1302 . 1303 if axis >= self.ndim:. ~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py in _can_reindex(self, indexer). 3475 # trying to reindex on an axis with duplicates. 3476 if not self._index_as_unique and len(indexer):. -> 3477 raise ValueError(""cannot reindex from a duplicate axis""). 3478 . 3479 def reindex(self, target, method=None, level=None, limit=None, tolerance=None):. ValueError: cannot reindex from a duplicate axis. ```. Loading a single h5 file works and produces expected output:. ```. a = sc.read_10x_h5('./a.h5', gex_only = True). a. AnnData object with n_obs × n_vars = 7474 × 31053. var: 'gene_ids', 'feature_types', 'genome'. ```. So the input files appear to be valid I just can't get them to concatenate to a single obje",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:4709,safety,input,input,4709,", tolerance, method, fill_value, copy. 4813 ).__finalize__(self, method=""reindex""). 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4021 if index is not None:. 4022 frame = frame._reindex_index(. -> 4023 index, method, copy, level, fill_value, limit, tolerance. 4024 ). 4025 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_index(self, new_index, method, copy, level, fill_value, limit, tolerance). 4043 copy=copy,. 4044 fill_value=fill_value,. -> 4045 allow_dups=False,. 4046 ). 4047 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in _reindex_with_indexers(self, reindexers, fill_value, copy, allow_dups). 4881 fill_value=fill_value,. 4882 allow_dups=allow_dups,. -> 4883 copy=copy,. 4884 ). 4885 # If we've made a copy once, no need to make another one. ~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py in reindex_indexer(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate, only_slice). 1299 # some axes don't allow reindexing with dups. 1300 if not allow_dups:. -> 1301 self.axes[axis]._can_reindex(indexer). 1302 . 1303 if axis >= self.ndim:. ~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py in _can_reindex(self, indexer). 3475 # trying to reindex on an axis with duplicates. 3476 if not self._index_as_unique and len(indexer):. -> 3477 raise ValueError(""cannot reindex from a duplicate axis""). 3478 . 3479 def reindex(self, target, method=None, level=None, limit=None, tolerance=None):. ValueError: cannot reindex from a duplicate axis. ```. Loading a single h5 file works and produces expected output:. ```. a = sc.read_10x_h5('./a.h5', gex_only = True). a. AnnData object with n_obs × n_vars = 7474 × 31053. var: 'gene_ids', 'feature_types', 'genome'. ```. So the input files appear to be valid I just can't get them to concatenate to a single object. . Any ideas would be welcome.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:4734,safety,valid,valid,4734,", tolerance, method, fill_value, copy. 4813 ).__finalize__(self, method=""reindex""). 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4021 if index is not None:. 4022 frame = frame._reindex_index(. -> 4023 index, method, copy, level, fill_value, limit, tolerance. 4024 ). 4025 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_index(self, new_index, method, copy, level, fill_value, limit, tolerance). 4043 copy=copy,. 4044 fill_value=fill_value,. -> 4045 allow_dups=False,. 4046 ). 4047 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in _reindex_with_indexers(self, reindexers, fill_value, copy, allow_dups). 4881 fill_value=fill_value,. 4882 allow_dups=allow_dups,. -> 4883 copy=copy,. 4884 ). 4885 # If we've made a copy once, no need to make another one. ~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py in reindex_indexer(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate, only_slice). 1299 # some axes don't allow reindexing with dups. 1300 if not allow_dups:. -> 1301 self.axes[axis]._can_reindex(indexer). 1302 . 1303 if axis >= self.ndim:. ~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py in _can_reindex(self, indexer). 3475 # trying to reindex on an axis with duplicates. 3476 if not self._index_as_unique and len(indexer):. -> 3477 raise ValueError(""cannot reindex from a duplicate axis""). 3478 . 3479 def reindex(self, target, method=None, level=None, limit=None, tolerance=None):. ValueError: cannot reindex from a duplicate axis. ```. Loading a single h5 file works and produces expected output:. ```. a = sc.read_10x_h5('./a.h5', gex_only = True). a. AnnData object with n_obs × n_vars = 7474 × 31053. var: 'gene_ids', 'feature_types', 'genome'. ```. So the input files appear to be valid I just can't get them to concatenate to a single object. . Any ideas would be welcome.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:543,testability,Trace,Traceback,543,"Hello,. I am having problems with reading in multiple h5 files using the code snipped that was posted by falexwolf. I am doing:. ```. filenames = ['./a.h5', './b.h5', './c.h5', './d.h5']. adatas = [sc.read_10x_h5(filename, gex_only = True) for filename in filenames]. adata = adatas[0].concatenate(adatas[1:], batch_key='gene_ids', batch_categories=filenames). ```. With or without the batch_key and batch_categories arguments I get the same error:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-20-e23ba2ca6e37> in <module>. 1 filenames = ['./a.h5', './b.h5', './c.h5', './d.h5']. 2 adatas = [sc.read_10x_h5(filename, gex_only = True) for filename in filenames]. ----> 3 adata = adatas[0].concatenate(adatas[1:], batch_key='gene_ids'). ~/anaconda3/lib/python3.7/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas). 1764 fill_value=fill_value,. 1765 index_unique=index_unique,. -> 1766 pairwise=False,. 1767 ). 1768 . ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise). 817 # Annotation for other axis. 818 alt_annot = merge_dataframes(. --> 819 [getattr(a, alt_dim) for a in adatas], alt_indices, merge. 820 ). 821 . ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in merge_dataframes(dfs, new_index, merge_strategy). 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique. 530 ) -> pd.DataFrame:. --> 531 dfs = [df.reindex(index=new_index) for df in dfs]. 532 # New dataframe with all shared data. 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in <listcomp>(.0). 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique. 530 ) -> pd.DataFrame:. --> 531 dfs = [df.reindex(ind",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:442,usability,error,error,442,"Hello,. I am having problems with reading in multiple h5 files using the code snipped that was posted by falexwolf. I am doing:. ```. filenames = ['./a.h5', './b.h5', './c.h5', './d.h5']. adatas = [sc.read_10x_h5(filename, gex_only = True) for filename in filenames]. adata = adatas[0].concatenate(adatas[1:], batch_key='gene_ids', batch_categories=filenames). ```. With or without the batch_key and batch_categories arguments I get the same error:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-20-e23ba2ca6e37> in <module>. 1 filenames = ['./a.h5', './b.h5', './c.h5', './d.h5']. 2 adatas = [sc.read_10x_h5(filename, gex_only = True) for filename in filenames]. ----> 3 adata = adatas[0].concatenate(adatas[1:], batch_key='gene_ids'). ~/anaconda3/lib/python3.7/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas). 1764 fill_value=fill_value,. 1765 index_unique=index_unique,. -> 1766 pairwise=False,. 1767 ). 1768 . ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise). 817 # Annotation for other axis. 818 alt_annot = merge_dataframes(. --> 819 [getattr(a, alt_dim) for a in adatas], alt_indices, merge. 820 ). 821 . ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in merge_dataframes(dfs, new_index, merge_strategy). 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique. 530 ) -> pd.DataFrame:. --> 531 dfs = [df.reindex(index=new_index) for df in dfs]. 532 # New dataframe with all shared data. 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in <listcomp>(.0). 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique. 530 ) -> pd.DataFrame:. --> 531 dfs = [df.reindex(ind",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:587,usability,input,input-,587,"Hello,. I am having problems with reading in multiple h5 files using the code snipped that was posted by falexwolf. I am doing:. ```. filenames = ['./a.h5', './b.h5', './c.h5', './d.h5']. adatas = [sc.read_10x_h5(filename, gex_only = True) for filename in filenames]. adata = adatas[0].concatenate(adatas[1:], batch_key='gene_ids', batch_categories=filenames). ```. With or without the batch_key and batch_categories arguments I get the same error:. ```. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-20-e23ba2ca6e37> in <module>. 1 filenames = ['./a.h5', './b.h5', './c.h5', './d.h5']. 2 adatas = [sc.read_10x_h5(filename, gex_only = True) for filename in filenames]. ----> 3 adata = adatas[0].concatenate(adatas[1:], batch_key='gene_ids'). ~/anaconda3/lib/python3.7/site-packages/anndata/_core/anndata.py in concatenate(self, join, batch_key, batch_categories, uns_merge, index_unique, fill_value, *adatas). 1764 fill_value=fill_value,. 1765 index_unique=index_unique,. -> 1766 pairwise=False,. 1767 ). 1768 . ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in concat(adatas, axis, join, merge, uns_merge, label, keys, index_unique, fill_value, pairwise). 817 # Annotation for other axis. 818 alt_annot = merge_dataframes(. --> 819 [getattr(a, alt_dim) for a in adatas], alt_indices, merge. 820 ). 821 . ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in merge_dataframes(dfs, new_index, merge_strategy). 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique. 530 ) -> pd.DataFrame:. --> 531 dfs = [df.reindex(index=new_index) for df in dfs]. 532 # New dataframe with all shared data. 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in <listcomp>(.0). 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique. 530 ) -> pd.DataFrame:. --> 531 dfs = [df.reindex(ind",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:2734,usability,perform,perform,2734,"= pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/anndata/_core/merge.py in <listcomp>(.0). 529 dfs: Iterable[pd.DataFrame], new_index, merge_strategy=merge_unique. 530 ) -> pd.DataFrame:. --> 531 dfs = [df.reindex(index=new_index) for df in dfs]. 532 # New dataframe with all shared data. 533 new_df = pd.DataFrame(merge_strategy(dfs), index=new_index). ~/anaconda3/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 310 @wraps(func). 311 def wrapper(*args, **kwargs) -> Callable[..., Any]:. --> 312 return func(*args, **kwargs). 313 . 314 kind = inspect.Parameter.POSITIONAL_OR_KEYWORD. ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in reindex(self, *args, **kwargs). 4174 kwargs.pop(""axis"", None). 4175 kwargs.pop(""labels"", None). -> 4176 return super().reindex(**kwargs). 4177 . 4178 def drop(. ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs). 4810 # perform the reindex on the axes. 4811 return self._reindex_axes(. -> 4812 axes, level, limit, tolerance, method, fill_value, copy. 4813 ).__finalize__(self, method=""reindex""). 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4021 if index is not None:. 4022 frame = frame._reindex_index(. -> 4023 index, method, copy, level, fill_value, limit, tolerance. 4024 ). 4025 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_index(self, new_index, method, copy, level, fill_value, limit, tolerance). 4043 copy=copy,. 4044 fill_value=fill_value,. -> 4045 allow_dups=False,. 4046 ). 4047 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in _reindex_with_indexers(self, reindexers, fill_value, copy, allow_dups). 4881 fill_value=fill_value,. 4882 allow_dups=allow_dups,. -> 4883 copy=copy,. 4884 ). 4885 # If we've made a copy once, no need to make another one. ~/anaconda3",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:4709,usability,input,input,4709,", tolerance, method, fill_value, copy. 4813 ).__finalize__(self, method=""reindex""). 4814 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy). 4021 if index is not None:. 4022 frame = frame._reindex_index(. -> 4023 index, method, copy, level, fill_value, limit, tolerance. 4024 ). 4025 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in _reindex_index(self, new_index, method, copy, level, fill_value, limit, tolerance). 4043 copy=copy,. 4044 fill_value=fill_value,. -> 4045 allow_dups=False,. 4046 ). 4047 . ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in _reindex_with_indexers(self, reindexers, fill_value, copy, allow_dups). 4881 fill_value=fill_value,. 4882 allow_dups=allow_dups,. -> 4883 copy=copy,. 4884 ). 4885 # If we've made a copy once, no need to make another one. ~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py in reindex_indexer(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate, only_slice). 1299 # some axes don't allow reindexing with dups. 1300 if not allow_dups:. -> 1301 self.axes[axis]._can_reindex(indexer). 1302 . 1303 if axis >= self.ndim:. ~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py in _can_reindex(self, indexer). 3475 # trying to reindex on an axis with duplicates. 3476 if not self._index_as_unique and len(indexer):. -> 3477 raise ValueError(""cannot reindex from a duplicate axis""). 3478 . 3479 def reindex(self, target, method=None, level=None, limit=None, tolerance=None):. ValueError: cannot reindex from a duplicate axis. ```. Loading a single h5 file works and produces expected output:. ```. a = sc.read_10x_h5('./a.h5', gex_only = True). a. AnnData object with n_obs × n_vars = 7474 × 31053. var: 'gene_ids', 'feature_types', 'genome'. ```. So the input files appear to be valid I just can't get them to concatenate to a single object. . Any ideas would be welcome.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:57,usability,help,help,57,"Hi @BrianLohman , I had the same problem, and this might help you. Just run `adata.var_names_make_unique()` before `concatenate`. `. filenames = [""a"",""b"",""c"",""d""]. adatas = []. for filename in filenames:. adata = sc.read_10x_h5(filename). adata.var_names_make_unique(). adatas.append(adata). adata = adatas[0].concatenate(adatas[1:]). `",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:57,availability,error,error,57,"HI, I tried to do what you suggested but I am getting an error saying `ValueError: only one regex group is supported with Index`. I have multiple h5ad files with varying n_obs × n_vars. Here is my code:. ```adatas = [an.read_h5ad(filename) for filename in filenames]. batch_names = []. for i in range(len(adatas)):. adatas[i].var_names_make_unique(). batch_names.append(filenames[i].split('.')[0]). print(i,adatas[i]). adata = adatas[0].concatenate(adatas[1:],. batch_key = 'ID',. uns_merge=""unique"",. index_unique=None,. batch_categories=batch_names). ```. and this produces the above error. Can anyone help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:586,availability,error,error,586,"HI, I tried to do what you suggested but I am getting an error saying `ValueError: only one regex group is supported with Index`. I have multiple h5ad files with varying n_obs × n_vars. Here is my code:. ```adatas = [an.read_h5ad(filename) for filename in filenames]. batch_names = []. for i in range(len(adatas)):. adatas[i].var_names_make_unique(). batch_names.append(filenames[i].split('.')[0]). print(i,adatas[i]). adata = adatas[0].concatenate(adatas[1:],. batch_key = 'ID',. uns_merge=""unique"",. index_unique=None,. batch_categories=batch_names). ```. and this produces the above error. Can anyone help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:57,performance,error,error,57,"HI, I tried to do what you suggested but I am getting an error saying `ValueError: only one regex group is supported with Index`. I have multiple h5ad files with varying n_obs × n_vars. Here is my code:. ```adatas = [an.read_h5ad(filename) for filename in filenames]. batch_names = []. for i in range(len(adatas)):. adatas[i].var_names_make_unique(). batch_names.append(filenames[i].split('.')[0]). print(i,adatas[i]). adata = adatas[0].concatenate(adatas[1:],. batch_key = 'ID',. uns_merge=""unique"",. index_unique=None,. batch_categories=batch_names). ```. and this produces the above error. Can anyone help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:586,performance,error,error,586,"HI, I tried to do what you suggested but I am getting an error saying `ValueError: only one regex group is supported with Index`. I have multiple h5ad files with varying n_obs × n_vars. Here is my code:. ```adatas = [an.read_h5ad(filename) for filename in filenames]. batch_names = []. for i in range(len(adatas)):. adatas[i].var_names_make_unique(). batch_names.append(filenames[i].split('.')[0]). print(i,adatas[i]). adata = adatas[0].concatenate(adatas[1:],. batch_key = 'ID',. uns_merge=""unique"",. index_unique=None,. batch_categories=batch_names). ```. and this produces the above error. Can anyone help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:57,safety,error,error,57,"HI, I tried to do what you suggested but I am getting an error saying `ValueError: only one regex group is supported with Index`. I have multiple h5ad files with varying n_obs × n_vars. Here is my code:. ```adatas = [an.read_h5ad(filename) for filename in filenames]. batch_names = []. for i in range(len(adatas)):. adatas[i].var_names_make_unique(). batch_names.append(filenames[i].split('.')[0]). print(i,adatas[i]). adata = adatas[0].concatenate(adatas[1:],. batch_key = 'ID',. uns_merge=""unique"",. index_unique=None,. batch_categories=batch_names). ```. and this produces the above error. Can anyone help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:586,safety,error,error,586,"HI, I tried to do what you suggested but I am getting an error saying `ValueError: only one regex group is supported with Index`. I have multiple h5ad files with varying n_obs × n_vars. Here is my code:. ```adatas = [an.read_h5ad(filename) for filename in filenames]. batch_names = []. for i in range(len(adatas)):. adatas[i].var_names_make_unique(). batch_names.append(filenames[i].split('.')[0]). print(i,adatas[i]). adata = adatas[0].concatenate(adatas[1:],. batch_key = 'ID',. uns_merge=""unique"",. index_unique=None,. batch_categories=batch_names). ```. and this produces the above error. Can anyone help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:57,usability,error,error,57,"HI, I tried to do what you suggested but I am getting an error saying `ValueError: only one regex group is supported with Index`. I have multiple h5ad files with varying n_obs × n_vars. Here is my code:. ```adatas = [an.read_h5ad(filename) for filename in filenames]. batch_names = []. for i in range(len(adatas)):. adatas[i].var_names_make_unique(). batch_names.append(filenames[i].split('.')[0]). print(i,adatas[i]). adata = adatas[0].concatenate(adatas[1:],. batch_key = 'ID',. uns_merge=""unique"",. index_unique=None,. batch_categories=batch_names). ```. and this produces the above error. Can anyone help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:107,usability,support,supported,107,"HI, I tried to do what you suggested but I am getting an error saying `ValueError: only one regex group is supported with Index`. I have multiple h5ad files with varying n_obs × n_vars. Here is my code:. ```adatas = [an.read_h5ad(filename) for filename in filenames]. batch_names = []. for i in range(len(adatas)):. adatas[i].var_names_make_unique(). batch_names.append(filenames[i].split('.')[0]). print(i,adatas[i]). adata = adatas[0].concatenate(adatas[1:],. batch_key = 'ID',. uns_merge=""unique"",. index_unique=None,. batch_categories=batch_names). ```. and this produces the above error. Can anyone help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:586,usability,error,error,586,"HI, I tried to do what you suggested but I am getting an error saying `ValueError: only one regex group is supported with Index`. I have multiple h5ad files with varying n_obs × n_vars. Here is my code:. ```adatas = [an.read_h5ad(filename) for filename in filenames]. batch_names = []. for i in range(len(adatas)):. adatas[i].var_names_make_unique(). batch_names.append(filenames[i].split('.')[0]). print(i,adatas[i]). adata = adatas[0].concatenate(adatas[1:],. batch_key = 'ID',. uns_merge=""unique"",. index_unique=None,. batch_categories=batch_names). ```. and this produces the above error. Can anyone help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/267:604,usability,help,help,604,"HI, I tried to do what you suggested but I am getting an error saying `ValueError: only one regex group is supported with Index`. I have multiple h5ad files with varying n_obs × n_vars. Here is my code:. ```adatas = [an.read_h5ad(filename) for filename in filenames]. batch_names = []. for i in range(len(adatas)):. adatas[i].var_names_make_unique(). batch_names.append(filenames[i].split('.')[0]). print(i,adatas[i]). adata = adatas[0].concatenate(adatas[1:],. batch_key = 'ID',. uns_merge=""unique"",. index_unique=None,. batch_categories=batch_names). ```. and this produces the above error. Can anyone help?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/267
https://github.com/scverse/scanpy/issues/268:624,interoperability,compatib,compatible,624,"Correct. This is listed on the project page:. ![grafik](https://user-images.githubusercontent.com/291575/45997197-12a5f800-c09f-11e8-925a-262ac41750cb.png). I added `python_requires='>3.5'` in 6d93f91c37b5bba5628a29082b45ab7e65912cc5, now there should be no confusion. If you want a reason why: Python 2 has at the time of writing about [15 months to live](https://pythonclock.org/), after which it will be dead and unsupported. The scientific world is [sunsetting it a bit before that](https://python3statement.org/). This has been known for years and people had more than enough time to prepare. Making a project Python 2 compatible these days is basically self-flagellation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/268
https://github.com/scverse/scanpy/issues/268:315,performance,time,time,315,"Correct. This is listed on the project page:. ![grafik](https://user-images.githubusercontent.com/291575/45997197-12a5f800-c09f-11e8-925a-262ac41750cb.png). I added `python_requires='>3.5'` in 6d93f91c37b5bba5628a29082b45ab7e65912cc5, now there should be no confusion. If you want a reason why: Python 2 has at the time of writing about [15 months to live](https://pythonclock.org/), after which it will be dead and unsupported. The scientific world is [sunsetting it a bit before that](https://python3statement.org/). This has been known for years and people had more than enough time to prepare. Making a project Python 2 compatible these days is basically self-flagellation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/268
https://github.com/scverse/scanpy/issues/268:581,performance,time,time,581,"Correct. This is listed on the project page:. ![grafik](https://user-images.githubusercontent.com/291575/45997197-12a5f800-c09f-11e8-925a-262ac41750cb.png). I added `python_requires='>3.5'` in 6d93f91c37b5bba5628a29082b45ab7e65912cc5, now there should be no confusion. If you want a reason why: Python 2 has at the time of writing about [15 months to live](https://pythonclock.org/), after which it will be dead and unsupported. The scientific world is [sunsetting it a bit before that](https://python3statement.org/). This has been known for years and people had more than enough time to prepare. Making a project Python 2 compatible these days is basically self-flagellation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/268
https://github.com/scverse/scanpy/issues/268:64,usability,user,user-images,64,"Correct. This is listed on the project page:. ![grafik](https://user-images.githubusercontent.com/291575/45997197-12a5f800-c09f-11e8-925a-262ac41750cb.png). I added `python_requires='>3.5'` in 6d93f91c37b5bba5628a29082b45ab7e65912cc5, now there should be no confusion. If you want a reason why: Python 2 has at the time of writing about [15 months to live](https://pythonclock.org/), after which it will be dead and unsupported. The scientific world is [sunsetting it a bit before that](https://python3statement.org/). This has been known for years and people had more than enough time to prepare. Making a project Python 2 compatible these days is basically self-flagellation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/268
https://github.com/scverse/scanpy/issues/269:57,safety,test,tests,57,"Both of these files would need some major clean up, some tests and some documentation, also in notebooks. I'm also happy to merge a pull request adding the new functionality, but I can't do this myself right now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/269
https://github.com/scverse/scanpy/issues/269:57,testability,test,tests,57,"Both of these files would need some major clean up, some tests and some documentation, also in notebooks. I'm also happy to merge a pull request adding the new functionality, but I can't do this myself right now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/269
https://github.com/scverse/scanpy/issues/269:72,usability,document,documentation,72,"Both of these files would need some major clean up, some tests and some documentation, also in notebooks. I'm also happy to merge a pull request adding the new functionality, but I can't do this myself right now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/269
https://github.com/scverse/scanpy/pull/270:249,deployability,log,log,249,"Is it generally a good idea to output P-values for marker gene identification? As the null model is not valid for this setup, P-values are not really a measure of significance, but should only serve to order genes. I can see the point of outputting log fold changes, but outputting P-values may be misleading.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:91,energy efficiency,model,model,91,"Is it generally a good idea to output P-values for marker gene identification? As the null model is not valid for this setup, P-values are not really a measure of significance, but should only serve to order genes. I can see the point of outputting log fold changes, but outputting P-values may be misleading.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:152,energy efficiency,measur,measure,152,"Is it generally a good idea to output P-values for marker gene identification? As the null model is not valid for this setup, P-values are not really a measure of significance, but should only serve to order genes. I can see the point of outputting log fold changes, but outputting P-values may be misleading.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:104,safety,valid,valid,104,"Is it generally a good idea to output P-values for marker gene identification? As the null model is not valid for this setup, P-values are not really a measure of significance, but should only serve to order genes. I can see the point of outputting log fold changes, but outputting P-values may be misleading.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:249,safety,log,log,249,"Is it generally a good idea to output P-values for marker gene identification? As the null model is not valid for this setup, P-values are not really a measure of significance, but should only serve to order genes. I can see the point of outputting log fold changes, but outputting P-values may be misleading.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:63,security,ident,identification,63,"Is it generally a good idea to output P-values for marker gene identification? As the null model is not valid for this setup, P-values are not really a measure of significance, but should only serve to order genes. I can see the point of outputting log fold changes, but outputting P-values may be misleading.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:91,security,model,model,91,"Is it generally a good idea to output P-values for marker gene identification? As the null model is not valid for this setup, P-values are not really a measure of significance, but should only serve to order genes. I can see the point of outputting log fold changes, but outputting P-values may be misleading.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:163,security,sign,significance,163,"Is it generally a good idea to output P-values for marker gene identification? As the null model is not valid for this setup, P-values are not really a measure of significance, but should only serve to order genes. I can see the point of outputting log fold changes, but outputting P-values may be misleading.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:249,security,log,log,249,"Is it generally a good idea to output P-values for marker gene identification? As the null model is not valid for this setup, P-values are not really a measure of significance, but should only serve to order genes. I can see the point of outputting log fold changes, but outputting P-values may be misleading.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:249,testability,log,log,249,"Is it generally a good idea to output P-values for marker gene identification? As the null model is not valid for this setup, P-values are not really a measure of significance, but should only serve to order genes. I can see the point of outputting log fold changes, but outputting P-values may be misleading.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:36,safety,Test,Testing,36,"Why is the null hypothesis invalid? Testing whether two cell types have the same mean for the expression of a given gene seems valid, no? One-vs-rest might be problematic, though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:127,safety,valid,valid,127,"Why is the null hypothesis invalid? Testing whether two cell types have the same mean for the expression of a given gene seems valid, no? One-vs-rest might be problematic, though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:36,testability,Test,Testing,36,"Why is the null hypothesis invalid? Testing whether two cell types have the same mean for the expression of a given gene seems valid, no? One-vs-rest might be problematic, though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:137,availability,cluster,clustering,137,"The null hypothesis assumes no difference in gene expression. However the groups are determined by differences in gene expression due to clustering. In random data, clustering would also separate clusters by 'random' gene expression signatures and thus find marker genes with p-value < 0.05.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:165,availability,cluster,clustering,165,"The null hypothesis assumes no difference in gene expression. However the groups are determined by differences in gene expression due to clustering. In random data, clustering would also separate clusters by 'random' gene expression signatures and thus find marker genes with p-value < 0.05.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:196,availability,cluster,clusters,196,"The null hypothesis assumes no difference in gene expression. However the groups are determined by differences in gene expression due to clustering. In random data, clustering would also separate clusters by 'random' gene expression signatures and thus find marker genes with p-value < 0.05.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:137,deployability,cluster,clustering,137,"The null hypothesis assumes no difference in gene expression. However the groups are determined by differences in gene expression due to clustering. In random data, clustering would also separate clusters by 'random' gene expression signatures and thus find marker genes with p-value < 0.05.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:165,deployability,cluster,clustering,165,"The null hypothesis assumes no difference in gene expression. However the groups are determined by differences in gene expression due to clustering. In random data, clustering would also separate clusters by 'random' gene expression signatures and thus find marker genes with p-value < 0.05.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:196,deployability,cluster,clusters,196,"The null hypothesis assumes no difference in gene expression. However the groups are determined by differences in gene expression due to clustering. In random data, clustering would also separate clusters by 'random' gene expression signatures and thus find marker genes with p-value < 0.05.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:233,security,sign,signatures,233,"The null hypothesis assumes no difference in gene expression. However the groups are determined by differences in gene expression due to clustering. In random data, clustering would also separate clusters by 'random' gene expression signatures and thus find marker genes with p-value < 0.05.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:88,availability,cluster,clustering,88,"Hmm, I think it's more complicated than that. First, cell type labels may not come from clustering. Even if it's the case, the design of Louvain does not rely on univariate mean difference between groups due to the distance metrics used in kNN graph, which take all genes into account at once. . Besides, I believe that how cell types are defined e.g. by biologist's manually annotation of each cell, Louvain, bulk comparison etc. doesn't really make the null hypothesis invalid. t-test just tests if the means are the same or not. Regarding false positives with random clustering, that's why we have Bonferroni, no? But this is now a different story about p-values and multiple testing correction in general. So it's not our duty to solve it in rank_genes_group function. I'd rather prefer reporting a p-value (or maybe t-stat) and letting the user decide how valid/useful it is for his/her research instead of not outputting it with the consideration of the chance that it's not valid.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:570,availability,cluster,clustering,570,"Hmm, I think it's more complicated than that. First, cell type labels may not come from clustering. Even if it's the case, the design of Louvain does not rely on univariate mean difference between groups due to the distance metrics used in kNN graph, which take all genes into account at once. . Besides, I believe that how cell types are defined e.g. by biologist's manually annotation of each cell, Louvain, bulk comparison etc. doesn't really make the null hypothesis invalid. t-test just tests if the means are the same or not. Regarding false positives with random clustering, that's why we have Bonferroni, no? But this is now a different story about p-values and multiple testing correction in general. So it's not our duty to solve it in rank_genes_group function. I'd rather prefer reporting a p-value (or maybe t-stat) and letting the user decide how valid/useful it is for his/her research instead of not outputting it with the consideration of the chance that it's not valid.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:88,deployability,cluster,clustering,88,"Hmm, I think it's more complicated than that. First, cell type labels may not come from clustering. Even if it's the case, the design of Louvain does not rely on univariate mean difference between groups due to the distance metrics used in kNN graph, which take all genes into account at once. . Besides, I believe that how cell types are defined e.g. by biologist's manually annotation of each cell, Louvain, bulk comparison etc. doesn't really make the null hypothesis invalid. t-test just tests if the means are the same or not. Regarding false positives with random clustering, that's why we have Bonferroni, no? But this is now a different story about p-values and multiple testing correction in general. So it's not our duty to solve it in rank_genes_group function. I'd rather prefer reporting a p-value (or maybe t-stat) and letting the user decide how valid/useful it is for his/her research instead of not outputting it with the consideration of the chance that it's not valid.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:570,deployability,cluster,clustering,570,"Hmm, I think it's more complicated than that. First, cell type labels may not come from clustering. Even if it's the case, the design of Louvain does not rely on univariate mean difference between groups due to the distance metrics used in kNN graph, which take all genes into account at once. . Besides, I believe that how cell types are defined e.g. by biologist's manually annotation of each cell, Louvain, bulk comparison etc. doesn't really make the null hypothesis invalid. t-test just tests if the means are the same or not. Regarding false positives with random clustering, that's why we have Bonferroni, no? But this is now a different story about p-values and multiple testing correction in general. So it's not our duty to solve it in rank_genes_group function. I'd rather prefer reporting a p-value (or maybe t-stat) and letting the user decide how valid/useful it is for his/her research instead of not outputting it with the consideration of the chance that it's not valid.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:145,reliability,doe,does,145,"Hmm, I think it's more complicated than that. First, cell type labels may not come from clustering. Even if it's the case, the design of Louvain does not rely on univariate mean difference between groups due to the distance metrics used in kNN graph, which take all genes into account at once. . Besides, I believe that how cell types are defined e.g. by biologist's manually annotation of each cell, Louvain, bulk comparison etc. doesn't really make the null hypothesis invalid. t-test just tests if the means are the same or not. Regarding false positives with random clustering, that's why we have Bonferroni, no? But this is now a different story about p-values and multiple testing correction in general. So it's not our duty to solve it in rank_genes_group function. I'd rather prefer reporting a p-value (or maybe t-stat) and letting the user decide how valid/useful it is for his/her research instead of not outputting it with the consideration of the chance that it's not valid.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:431,reliability,doe,doesn,431,"Hmm, I think it's more complicated than that. First, cell type labels may not come from clustering. Even if it's the case, the design of Louvain does not rely on univariate mean difference between groups due to the distance metrics used in kNN graph, which take all genes into account at once. . Besides, I believe that how cell types are defined e.g. by biologist's manually annotation of each cell, Louvain, bulk comparison etc. doesn't really make the null hypothesis invalid. t-test just tests if the means are the same or not. Regarding false positives with random clustering, that's why we have Bonferroni, no? But this is now a different story about p-values and multiple testing correction in general. So it's not our duty to solve it in rank_genes_group function. I'd rather prefer reporting a p-value (or maybe t-stat) and letting the user decide how valid/useful it is for his/her research instead of not outputting it with the consideration of the chance that it's not valid.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:23,safety,compl,complicated,23,"Hmm, I think it's more complicated than that. First, cell type labels may not come from clustering. Even if it's the case, the design of Louvain does not rely on univariate mean difference between groups due to the distance metrics used in kNN graph, which take all genes into account at once. . Besides, I believe that how cell types are defined e.g. by biologist's manually annotation of each cell, Louvain, bulk comparison etc. doesn't really make the null hypothesis invalid. t-test just tests if the means are the same or not. Regarding false positives with random clustering, that's why we have Bonferroni, no? But this is now a different story about p-values and multiple testing correction in general. So it's not our duty to solve it in rank_genes_group function. I'd rather prefer reporting a p-value (or maybe t-stat) and letting the user decide how valid/useful it is for his/her research instead of not outputting it with the consideration of the chance that it's not valid.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:482,safety,test,test,482,"Hmm, I think it's more complicated than that. First, cell type labels may not come from clustering. Even if it's the case, the design of Louvain does not rely on univariate mean difference between groups due to the distance metrics used in kNN graph, which take all genes into account at once. . Besides, I believe that how cell types are defined e.g. by biologist's manually annotation of each cell, Louvain, bulk comparison etc. doesn't really make the null hypothesis invalid. t-test just tests if the means are the same or not. Regarding false positives with random clustering, that's why we have Bonferroni, no? But this is now a different story about p-values and multiple testing correction in general. So it's not our duty to solve it in rank_genes_group function. I'd rather prefer reporting a p-value (or maybe t-stat) and letting the user decide how valid/useful it is for his/her research instead of not outputting it with the consideration of the chance that it's not valid.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:492,safety,test,tests,492,"Hmm, I think it's more complicated than that. First, cell type labels may not come from clustering. Even if it's the case, the design of Louvain does not rely on univariate mean difference between groups due to the distance metrics used in kNN graph, which take all genes into account at once. . Besides, I believe that how cell types are defined e.g. by biologist's manually annotation of each cell, Louvain, bulk comparison etc. doesn't really make the null hypothesis invalid. t-test just tests if the means are the same or not. Regarding false positives with random clustering, that's why we have Bonferroni, no? But this is now a different story about p-values and multiple testing correction in general. So it's not our duty to solve it in rank_genes_group function. I'd rather prefer reporting a p-value (or maybe t-stat) and letting the user decide how valid/useful it is for his/her research instead of not outputting it with the consideration of the chance that it's not valid.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:679,safety,test,testing,679,"Hmm, I think it's more complicated than that. First, cell type labels may not come from clustering. Even if it's the case, the design of Louvain does not rely on univariate mean difference between groups due to the distance metrics used in kNN graph, which take all genes into account at once. . Besides, I believe that how cell types are defined e.g. by biologist's manually annotation of each cell, Louvain, bulk comparison etc. doesn't really make the null hypothesis invalid. t-test just tests if the means are the same or not. Regarding false positives with random clustering, that's why we have Bonferroni, no? But this is now a different story about p-values and multiple testing correction in general. So it's not our duty to solve it in rank_genes_group function. I'd rather prefer reporting a p-value (or maybe t-stat) and letting the user decide how valid/useful it is for his/her research instead of not outputting it with the consideration of the chance that it's not valid.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:861,safety,valid,valid,861,"Hmm, I think it's more complicated than that. First, cell type labels may not come from clustering. Even if it's the case, the design of Louvain does not rely on univariate mean difference between groups due to the distance metrics used in kNN graph, which take all genes into account at once. . Besides, I believe that how cell types are defined e.g. by biologist's manually annotation of each cell, Louvain, bulk comparison etc. doesn't really make the null hypothesis invalid. t-test just tests if the means are the same or not. Regarding false positives with random clustering, that's why we have Bonferroni, no? But this is now a different story about p-values and multiple testing correction in general. So it's not our duty to solve it in rank_genes_group function. I'd rather prefer reporting a p-value (or maybe t-stat) and letting the user decide how valid/useful it is for his/her research instead of not outputting it with the consideration of the chance that it's not valid.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:981,safety,valid,valid,981,"Hmm, I think it's more complicated than that. First, cell type labels may not come from clustering. Even if it's the case, the design of Louvain does not rely on univariate mean difference between groups due to the distance metrics used in kNN graph, which take all genes into account at once. . Besides, I believe that how cell types are defined e.g. by biologist's manually annotation of each cell, Louvain, bulk comparison etc. doesn't really make the null hypothesis invalid. t-test just tests if the means are the same or not. Regarding false positives with random clustering, that's why we have Bonferroni, no? But this is now a different story about p-values and multiple testing correction in general. So it's not our duty to solve it in rank_genes_group function. I'd rather prefer reporting a p-value (or maybe t-stat) and letting the user decide how valid/useful it is for his/her research instead of not outputting it with the consideration of the chance that it's not valid.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:23,security,compl,complicated,23,"Hmm, I think it's more complicated than that. First, cell type labels may not come from clustering. Even if it's the case, the design of Louvain does not rely on univariate mean difference between groups due to the distance metrics used in kNN graph, which take all genes into account at once. . Besides, I believe that how cell types are defined e.g. by biologist's manually annotation of each cell, Louvain, bulk comparison etc. doesn't really make the null hypothesis invalid. t-test just tests if the means are the same or not. Regarding false positives with random clustering, that's why we have Bonferroni, no? But this is now a different story about p-values and multiple testing correction in general. So it's not our duty to solve it in rank_genes_group function. I'd rather prefer reporting a p-value (or maybe t-stat) and letting the user decide how valid/useful it is for his/her research instead of not outputting it with the consideration of the chance that it's not valid.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:482,testability,test,test,482,"Hmm, I think it's more complicated than that. First, cell type labels may not come from clustering. Even if it's the case, the design of Louvain does not rely on univariate mean difference between groups due to the distance metrics used in kNN graph, which take all genes into account at once. . Besides, I believe that how cell types are defined e.g. by biologist's manually annotation of each cell, Louvain, bulk comparison etc. doesn't really make the null hypothesis invalid. t-test just tests if the means are the same or not. Regarding false positives with random clustering, that's why we have Bonferroni, no? But this is now a different story about p-values and multiple testing correction in general. So it's not our duty to solve it in rank_genes_group function. I'd rather prefer reporting a p-value (or maybe t-stat) and letting the user decide how valid/useful it is for his/her research instead of not outputting it with the consideration of the chance that it's not valid.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:492,testability,test,tests,492,"Hmm, I think it's more complicated than that. First, cell type labels may not come from clustering. Even if it's the case, the design of Louvain does not rely on univariate mean difference between groups due to the distance metrics used in kNN graph, which take all genes into account at once. . Besides, I believe that how cell types are defined e.g. by biologist's manually annotation of each cell, Louvain, bulk comparison etc. doesn't really make the null hypothesis invalid. t-test just tests if the means are the same or not. Regarding false positives with random clustering, that's why we have Bonferroni, no? But this is now a different story about p-values and multiple testing correction in general. So it's not our duty to solve it in rank_genes_group function. I'd rather prefer reporting a p-value (or maybe t-stat) and letting the user decide how valid/useful it is for his/her research instead of not outputting it with the consideration of the chance that it's not valid.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:679,testability,test,testing,679,"Hmm, I think it's more complicated than that. First, cell type labels may not come from clustering. Even if it's the case, the design of Louvain does not rely on univariate mean difference between groups due to the distance metrics used in kNN graph, which take all genes into account at once. . Besides, I believe that how cell types are defined e.g. by biologist's manually annotation of each cell, Louvain, bulk comparison etc. doesn't really make the null hypothesis invalid. t-test just tests if the means are the same or not. Regarding false positives with random clustering, that's why we have Bonferroni, no? But this is now a different story about p-values and multiple testing correction in general. So it's not our duty to solve it in rank_genes_group function. I'd rather prefer reporting a p-value (or maybe t-stat) and letting the user decide how valid/useful it is for his/her research instead of not outputting it with the consideration of the chance that it's not valid.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:784,usability,prefer,prefer,784,"Hmm, I think it's more complicated than that. First, cell type labels may not come from clustering. Even if it's the case, the design of Louvain does not rely on univariate mean difference between groups due to the distance metrics used in kNN graph, which take all genes into account at once. . Besides, I believe that how cell types are defined e.g. by biologist's manually annotation of each cell, Louvain, bulk comparison etc. doesn't really make the null hypothesis invalid. t-test just tests if the means are the same or not. Regarding false positives with random clustering, that's why we have Bonferroni, no? But this is now a different story about p-values and multiple testing correction in general. So it's not our duty to solve it in rank_genes_group function. I'd rather prefer reporting a p-value (or maybe t-stat) and letting the user decide how valid/useful it is for his/her research instead of not outputting it with the consideration of the chance that it's not valid.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:845,usability,user,user,845,"Hmm, I think it's more complicated than that. First, cell type labels may not come from clustering. Even if it's the case, the design of Louvain does not rely on univariate mean difference between groups due to the distance metrics used in kNN graph, which take all genes into account at once. . Besides, I believe that how cell types are defined e.g. by biologist's manually annotation of each cell, Louvain, bulk comparison etc. doesn't really make the null hypothesis invalid. t-test just tests if the means are the same or not. Regarding false positives with random clustering, that's why we have Bonferroni, no? But this is now a different story about p-values and multiple testing correction in general. So it's not our duty to solve it in rank_genes_group function. I'd rather prefer reporting a p-value (or maybe t-stat) and letting the user decide how valid/useful it is for his/her research instead of not outputting it with the consideration of the chance that it's not valid.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:81,interoperability,distribut,distribution,81,"So you should easily be able to see that the null hypothesis is not valid by the distribution of p-values for all genes in one rank_genes_groups test. If it is valid, this distribution should be uniform. In that case it's only a multiple testing problem... I would guarantee you that it's not uniform though. Cell type labels from expression-independent sources should not have the same confounding effect. However, I would bet you can trace back all biological annotations of cell types back to expression.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:172,interoperability,distribut,distribution,172,"So you should easily be able to see that the null hypothesis is not valid by the distribution of p-values for all genes in one rank_genes_groups test. If it is valid, this distribution should be uniform. In that case it's only a multiple testing problem... I would guarantee you that it's not uniform though. Cell type labels from expression-independent sources should not have the same confounding effect. However, I would bet you can trace back all biological annotations of cell types back to expression.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:68,safety,valid,valid,68,"So you should easily be able to see that the null hypothesis is not valid by the distribution of p-values for all genes in one rank_genes_groups test. If it is valid, this distribution should be uniform. In that case it's only a multiple testing problem... I would guarantee you that it's not uniform though. Cell type labels from expression-independent sources should not have the same confounding effect. However, I would bet you can trace back all biological annotations of cell types back to expression.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:145,safety,test,test,145,"So you should easily be able to see that the null hypothesis is not valid by the distribution of p-values for all genes in one rank_genes_groups test. If it is valid, this distribution should be uniform. In that case it's only a multiple testing problem... I would guarantee you that it's not uniform though. Cell type labels from expression-independent sources should not have the same confounding effect. However, I would bet you can trace back all biological annotations of cell types back to expression.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:160,safety,valid,valid,160,"So you should easily be able to see that the null hypothesis is not valid by the distribution of p-values for all genes in one rank_genes_groups test. If it is valid, this distribution should be uniform. In that case it's only a multiple testing problem... I would guarantee you that it's not uniform though. Cell type labels from expression-independent sources should not have the same confounding effect. However, I would bet you can trace back all biological annotations of cell types back to expression.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:238,safety,test,testing,238,"So you should easily be able to see that the null hypothesis is not valid by the distribution of p-values for all genes in one rank_genes_groups test. If it is valid, this distribution should be uniform. In that case it's only a multiple testing problem... I would guarantee you that it's not uniform though. Cell type labels from expression-independent sources should not have the same confounding effect. However, I would bet you can trace back all biological annotations of cell types back to expression.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:145,testability,test,test,145,"So you should easily be able to see that the null hypothesis is not valid by the distribution of p-values for all genes in one rank_genes_groups test. If it is valid, this distribution should be uniform. In that case it's only a multiple testing problem... I would guarantee you that it's not uniform though. Cell type labels from expression-independent sources should not have the same confounding effect. However, I would bet you can trace back all biological annotations of cell types back to expression.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:238,testability,test,testing,238,"So you should easily be able to see that the null hypothesis is not valid by the distribution of p-values for all genes in one rank_genes_groups test. If it is valid, this distribution should be uniform. In that case it's only a multiple testing problem... I would guarantee you that it's not uniform though. Cell type labels from expression-independent sources should not have the same confounding effect. However, I would bet you can trace back all biological annotations of cell types back to expression.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:436,testability,trace,trace,436,"So you should easily be able to see that the null hypothesis is not valid by the distribution of p-values for all genes in one rank_genes_groups test. If it is valid, this distribution should be uniform. In that case it's only a multiple testing problem... I would guarantee you that it's not uniform though. Cell type labels from expression-independent sources should not have the same confounding effect. However, I would bet you can trace back all biological annotations of cell types back to expression.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:100,deployability,depend,depends,100,"Thanks for your comments! This is an interesting discussion. I agree that the validity of a p-value depends on the user's data. From the standpoint of an analysis tool, I think it's better to report a p-value (and a t-statistic, which was already reported in the original function) and let the user decide what to do with that information.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:100,integrability,depend,depends,100,"Thanks for your comments! This is an interesting discussion. I agree that the validity of a p-value depends on the user's data. From the standpoint of an analysis tool, I think it's better to report a p-value (and a t-statistic, which was already reported in the original function) and let the user decide what to do with that information.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:100,modifiability,depend,depends,100,"Thanks for your comments! This is an interesting discussion. I agree that the validity of a p-value depends on the user's data. From the standpoint of an analysis tool, I think it's better to report a p-value (and a t-statistic, which was already reported in the original function) and let the user decide what to do with that information.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:78,safety,valid,validity,78,"Thanks for your comments! This is an interesting discussion. I agree that the validity of a p-value depends on the user's data. From the standpoint of an analysis tool, I think it's better to report a p-value (and a t-statistic, which was already reported in the original function) and let the user decide what to do with that information.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:100,safety,depend,depends,100,"Thanks for your comments! This is an interesting discussion. I agree that the validity of a p-value depends on the user's data. From the standpoint of an analysis tool, I think it's better to report a p-value (and a t-statistic, which was already reported in the original function) and let the user decide what to do with that information.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:100,testability,depend,depends,100,"Thanks for your comments! This is an interesting discussion. I agree that the validity of a p-value depends on the user's data. From the standpoint of an analysis tool, I think it's better to report a p-value (and a t-statistic, which was already reported in the original function) and let the user decide what to do with that information.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:115,usability,user,user,115,"Thanks for your comments! This is an interesting discussion. I agree that the validity of a p-value depends on the user's data. From the standpoint of an analysis tool, I think it's better to report a p-value (and a t-statistic, which was already reported in the original function) and let the user decide what to do with that information.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:163,usability,tool,tool,163,"Thanks for your comments! This is an interesting discussion. I agree that the validity of a p-value depends on the user's data. From the standpoint of an analysis tool, I think it's better to report a p-value (and a t-statistic, which was already reported in the original function) and let the user decide what to do with that information.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:294,usability,user,user,294,"Thanks for your comments! This is an interesting discussion. I agree that the validity of a p-value depends on the user's data. From the standpoint of an analysis tool, I think it's better to report a p-value (and a t-statistic, which was already reported in the original function) and let the user decide what to do with that information.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:530,availability,cluster,clustering,530,"I just noticed I forgot to answer to your point about multiple-testing-correction. Multiple testing correction deals with the effect that obtaining any false positives become more likely with repeated tests. However, it does not deal with bias. Bias is however present in the test given that the correct null hypothesis should assume some genes are different between groups, and not that all genes are equal. This comes back to the observation that some genes will be always be different between groups in random data, given that clustering defines the groups over which we test. And as for louvain using whole transcriptome similarities rather than individual gene similarities... these are related. If you defined groups by differences of whole transcriptomes, this necesitates individual genes having different means. The correct p-value would be the output of a permutation test. . Could discuss this offline if you like @gokceneraslan. I'm preparing a manuscript which makes a point of this... if I am wrong, please convince me of this soon ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:432,deployability,observ,observation,432,"I just noticed I forgot to answer to your point about multiple-testing-correction. Multiple testing correction deals with the effect that obtaining any false positives become more likely with repeated tests. However, it does not deal with bias. Bias is however present in the test given that the correct null hypothesis should assume some genes are different between groups, and not that all genes are equal. This comes back to the observation that some genes will be always be different between groups in random data, given that clustering defines the groups over which we test. And as for louvain using whole transcriptome similarities rather than individual gene similarities... these are related. If you defined groups by differences of whole transcriptomes, this necesitates individual genes having different means. The correct p-value would be the output of a permutation test. . Could discuss this offline if you like @gokceneraslan. I'm preparing a manuscript which makes a point of this... if I am wrong, please convince me of this soon ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:530,deployability,cluster,clustering,530,"I just noticed I forgot to answer to your point about multiple-testing-correction. Multiple testing correction deals with the effect that obtaining any false positives become more likely with repeated tests. However, it does not deal with bias. Bias is however present in the test given that the correct null hypothesis should assume some genes are different between groups, and not that all genes are equal. This comes back to the observation that some genes will be always be different between groups in random data, given that clustering defines the groups over which we test. And as for louvain using whole transcriptome similarities rather than individual gene similarities... these are related. If you defined groups by differences of whole transcriptomes, this necesitates individual genes having different means. The correct p-value would be the output of a permutation test. . Could discuss this offline if you like @gokceneraslan. I'm preparing a manuscript which makes a point of this... if I am wrong, please convince me of this soon ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:220,reliability,doe,does,220,"I just noticed I forgot to answer to your point about multiple-testing-correction. Multiple testing correction deals with the effect that obtaining any false positives become more likely with repeated tests. However, it does not deal with bias. Bias is however present in the test given that the correct null hypothesis should assume some genes are different between groups, and not that all genes are equal. This comes back to the observation that some genes will be always be different between groups in random data, given that clustering defines the groups over which we test. And as for louvain using whole transcriptome similarities rather than individual gene similarities... these are related. If you defined groups by differences of whole transcriptomes, this necesitates individual genes having different means. The correct p-value would be the output of a permutation test. . Could discuss this offline if you like @gokceneraslan. I'm preparing a manuscript which makes a point of this... if I am wrong, please convince me of this soon ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:63,safety,test,testing-correction,63,"I just noticed I forgot to answer to your point about multiple-testing-correction. Multiple testing correction deals with the effect that obtaining any false positives become more likely with repeated tests. However, it does not deal with bias. Bias is however present in the test given that the correct null hypothesis should assume some genes are different between groups, and not that all genes are equal. This comes back to the observation that some genes will be always be different between groups in random data, given that clustering defines the groups over which we test. And as for louvain using whole transcriptome similarities rather than individual gene similarities... these are related. If you defined groups by differences of whole transcriptomes, this necesitates individual genes having different means. The correct p-value would be the output of a permutation test. . Could discuss this offline if you like @gokceneraslan. I'm preparing a manuscript which makes a point of this... if I am wrong, please convince me of this soon ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:92,safety,test,testing,92,"I just noticed I forgot to answer to your point about multiple-testing-correction. Multiple testing correction deals with the effect that obtaining any false positives become more likely with repeated tests. However, it does not deal with bias. Bias is however present in the test given that the correct null hypothesis should assume some genes are different between groups, and not that all genes are equal. This comes back to the observation that some genes will be always be different between groups in random data, given that clustering defines the groups over which we test. And as for louvain using whole transcriptome similarities rather than individual gene similarities... these are related. If you defined groups by differences of whole transcriptomes, this necesitates individual genes having different means. The correct p-value would be the output of a permutation test. . Could discuss this offline if you like @gokceneraslan. I'm preparing a manuscript which makes a point of this... if I am wrong, please convince me of this soon ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:201,safety,test,tests,201,"I just noticed I forgot to answer to your point about multiple-testing-correction. Multiple testing correction deals with the effect that obtaining any false positives become more likely with repeated tests. However, it does not deal with bias. Bias is however present in the test given that the correct null hypothesis should assume some genes are different between groups, and not that all genes are equal. This comes back to the observation that some genes will be always be different between groups in random data, given that clustering defines the groups over which we test. And as for louvain using whole transcriptome similarities rather than individual gene similarities... these are related. If you defined groups by differences of whole transcriptomes, this necesitates individual genes having different means. The correct p-value would be the output of a permutation test. . Could discuss this offline if you like @gokceneraslan. I'm preparing a manuscript which makes a point of this... if I am wrong, please convince me of this soon ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:276,safety,test,test,276,"I just noticed I forgot to answer to your point about multiple-testing-correction. Multiple testing correction deals with the effect that obtaining any false positives become more likely with repeated tests. However, it does not deal with bias. Bias is however present in the test given that the correct null hypothesis should assume some genes are different between groups, and not that all genes are equal. This comes back to the observation that some genes will be always be different between groups in random data, given that clustering defines the groups over which we test. And as for louvain using whole transcriptome similarities rather than individual gene similarities... these are related. If you defined groups by differences of whole transcriptomes, this necesitates individual genes having different means. The correct p-value would be the output of a permutation test. . Could discuss this offline if you like @gokceneraslan. I'm preparing a manuscript which makes a point of this... if I am wrong, please convince me of this soon ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:574,safety,test,test,574,"I just noticed I forgot to answer to your point about multiple-testing-correction. Multiple testing correction deals with the effect that obtaining any false positives become more likely with repeated tests. However, it does not deal with bias. Bias is however present in the test given that the correct null hypothesis should assume some genes are different between groups, and not that all genes are equal. This comes back to the observation that some genes will be always be different between groups in random data, given that clustering defines the groups over which we test. And as for louvain using whole transcriptome similarities rather than individual gene similarities... these are related. If you defined groups by differences of whole transcriptomes, this necesitates individual genes having different means. The correct p-value would be the output of a permutation test. . Could discuss this offline if you like @gokceneraslan. I'm preparing a manuscript which makes a point of this... if I am wrong, please convince me of this soon ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:878,safety,test,test,878,"I just noticed I forgot to answer to your point about multiple-testing-correction. Multiple testing correction deals with the effect that obtaining any false positives become more likely with repeated tests. However, it does not deal with bias. Bias is however present in the test given that the correct null hypothesis should assume some genes are different between groups, and not that all genes are equal. This comes back to the observation that some genes will be always be different between groups in random data, given that clustering defines the groups over which we test. And as for louvain using whole transcriptome similarities rather than individual gene similarities... these are related. If you defined groups by differences of whole transcriptomes, this necesitates individual genes having different means. The correct p-value would be the output of a permutation test. . Could discuss this offline if you like @gokceneraslan. I'm preparing a manuscript which makes a point of this... if I am wrong, please convince me of this soon ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:63,testability,test,testing-correction,63,"I just noticed I forgot to answer to your point about multiple-testing-correction. Multiple testing correction deals with the effect that obtaining any false positives become more likely with repeated tests. However, it does not deal with bias. Bias is however present in the test given that the correct null hypothesis should assume some genes are different between groups, and not that all genes are equal. This comes back to the observation that some genes will be always be different between groups in random data, given that clustering defines the groups over which we test. And as for louvain using whole transcriptome similarities rather than individual gene similarities... these are related. If you defined groups by differences of whole transcriptomes, this necesitates individual genes having different means. The correct p-value would be the output of a permutation test. . Could discuss this offline if you like @gokceneraslan. I'm preparing a manuscript which makes a point of this... if I am wrong, please convince me of this soon ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:92,testability,test,testing,92,"I just noticed I forgot to answer to your point about multiple-testing-correction. Multiple testing correction deals with the effect that obtaining any false positives become more likely with repeated tests. However, it does not deal with bias. Bias is however present in the test given that the correct null hypothesis should assume some genes are different between groups, and not that all genes are equal. This comes back to the observation that some genes will be always be different between groups in random data, given that clustering defines the groups over which we test. And as for louvain using whole transcriptome similarities rather than individual gene similarities... these are related. If you defined groups by differences of whole transcriptomes, this necesitates individual genes having different means. The correct p-value would be the output of a permutation test. . Could discuss this offline if you like @gokceneraslan. I'm preparing a manuscript which makes a point of this... if I am wrong, please convince me of this soon ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:201,testability,test,tests,201,"I just noticed I forgot to answer to your point about multiple-testing-correction. Multiple testing correction deals with the effect that obtaining any false positives become more likely with repeated tests. However, it does not deal with bias. Bias is however present in the test given that the correct null hypothesis should assume some genes are different between groups, and not that all genes are equal. This comes back to the observation that some genes will be always be different between groups in random data, given that clustering defines the groups over which we test. And as for louvain using whole transcriptome similarities rather than individual gene similarities... these are related. If you defined groups by differences of whole transcriptomes, this necesitates individual genes having different means. The correct p-value would be the output of a permutation test. . Could discuss this offline if you like @gokceneraslan. I'm preparing a manuscript which makes a point of this... if I am wrong, please convince me of this soon ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:276,testability,test,test,276,"I just noticed I forgot to answer to your point about multiple-testing-correction. Multiple testing correction deals with the effect that obtaining any false positives become more likely with repeated tests. However, it does not deal with bias. Bias is however present in the test given that the correct null hypothesis should assume some genes are different between groups, and not that all genes are equal. This comes back to the observation that some genes will be always be different between groups in random data, given that clustering defines the groups over which we test. And as for louvain using whole transcriptome similarities rather than individual gene similarities... these are related. If you defined groups by differences of whole transcriptomes, this necesitates individual genes having different means. The correct p-value would be the output of a permutation test. . Could discuss this offline if you like @gokceneraslan. I'm preparing a manuscript which makes a point of this... if I am wrong, please convince me of this soon ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:432,testability,observ,observation,432,"I just noticed I forgot to answer to your point about multiple-testing-correction. Multiple testing correction deals with the effect that obtaining any false positives become more likely with repeated tests. However, it does not deal with bias. Bias is however present in the test given that the correct null hypothesis should assume some genes are different between groups, and not that all genes are equal. This comes back to the observation that some genes will be always be different between groups in random data, given that clustering defines the groups over which we test. And as for louvain using whole transcriptome similarities rather than individual gene similarities... these are related. If you defined groups by differences of whole transcriptomes, this necesitates individual genes having different means. The correct p-value would be the output of a permutation test. . Could discuss this offline if you like @gokceneraslan. I'm preparing a manuscript which makes a point of this... if I am wrong, please convince me of this soon ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:574,testability,test,test,574,"I just noticed I forgot to answer to your point about multiple-testing-correction. Multiple testing correction deals with the effect that obtaining any false positives become more likely with repeated tests. However, it does not deal with bias. Bias is however present in the test given that the correct null hypothesis should assume some genes are different between groups, and not that all genes are equal. This comes back to the observation that some genes will be always be different between groups in random data, given that clustering defines the groups over which we test. And as for louvain using whole transcriptome similarities rather than individual gene similarities... these are related. If you defined groups by differences of whole transcriptomes, this necesitates individual genes having different means. The correct p-value would be the output of a permutation test. . Could discuss this offline if you like @gokceneraslan. I'm preparing a manuscript which makes a point of this... if I am wrong, please convince me of this soon ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:878,testability,test,test,878,"I just noticed I forgot to answer to your point about multiple-testing-correction. Multiple testing correction deals with the effect that obtaining any false positives become more likely with repeated tests. However, it does not deal with bias. Bias is however present in the test given that the correct null hypothesis should assume some genes are different between groups, and not that all genes are equal. This comes back to the observation that some genes will be always be different between groups in random data, given that clustering defines the groups over which we test. And as for louvain using whole transcriptome similarities rather than individual gene similarities... these are related. If you defined groups by differences of whole transcriptomes, this necesitates individual genes having different means. The correct p-value would be the output of a permutation test. . Could discuss this offline if you like @gokceneraslan. I'm preparing a manuscript which makes a point of this... if I am wrong, please convince me of this soon ;).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:162,deployability,log,logFC,162,I can understand your line of reasoning @a-munoz-rojas. But is it not also dangerous to allow a user an interpretation which may be incorrect? I think outputting logFC values is great... I just have issues with calling the other output P-values. It is at the very least contentious whether they are measures of significance.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:299,energy efficiency,measur,measures,299,I can understand your line of reasoning @a-munoz-rojas. But is it not also dangerous to allow a user an interpretation which may be incorrect? I think outputting logFC values is great... I just have issues with calling the other output P-values. It is at the very least contentious whether they are measures of significance.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:270,performance,content,contentious,270,I can understand your line of reasoning @a-munoz-rojas. But is it not also dangerous to allow a user an interpretation which may be incorrect? I think outputting logFC values is great... I just have issues with calling the other output P-values. It is at the very least contentious whether they are measures of significance.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:162,safety,log,logFC,162,I can understand your line of reasoning @a-munoz-rojas. But is it not also dangerous to allow a user an interpretation which may be incorrect? I think outputting logFC values is great... I just have issues with calling the other output P-values. It is at the very least contentious whether they are measures of significance.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:162,security,log,logFC,162,I can understand your line of reasoning @a-munoz-rojas. But is it not also dangerous to allow a user an interpretation which may be incorrect? I think outputting logFC values is great... I just have issues with calling the other output P-values. It is at the very least contentious whether they are measures of significance.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:311,security,sign,significance,311,I can understand your line of reasoning @a-munoz-rojas. But is it not also dangerous to allow a user an interpretation which may be incorrect? I think outputting logFC values is great... I just have issues with calling the other output P-values. It is at the very least contentious whether they are measures of significance.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:6,testability,understand,understand,6,I can understand your line of reasoning @a-munoz-rojas. But is it not also dangerous to allow a user an interpretation which may be incorrect? I think outputting logFC values is great... I just have issues with calling the other output P-values. It is at the very least contentious whether they are measures of significance.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:162,testability,log,logFC,162,I can understand your line of reasoning @a-munoz-rojas. But is it not also dangerous to allow a user an interpretation which may be incorrect? I think outputting logFC values is great... I just have issues with calling the other output P-values. It is at the very least contentious whether they are measures of significance.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:96,usability,user,user,96,I can understand your line of reasoning @a-munoz-rojas. But is it not also dangerous to allow a user an interpretation which may be incorrect? I think outputting logFC values is great... I just have issues with calling the other output P-values. It is at the very least contentious whether they are measures of significance.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:199,deployability,fail,failing,199,"Sorry about the super late response and thank you for the PR, @a-munoz-rojas. I have not read most of the above discussion, yet. Right now, I just wanted to note that (@a-munoz-rojas), the tests are failing as the wilcoxon test seems to yield different results, now. I don't know whether it's just marginal, but it should be clarified; users should still get the same results as before.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:199,reliability,fail,failing,199,"Sorry about the super late response and thank you for the PR, @a-munoz-rojas. I have not read most of the above discussion, yet. Right now, I just wanted to note that (@a-munoz-rojas), the tests are failing as the wilcoxon test seems to yield different results, now. I don't know whether it's just marginal, but it should be clarified; users should still get the same results as before.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:189,safety,test,tests,189,"Sorry about the super late response and thank you for the PR, @a-munoz-rojas. I have not read most of the above discussion, yet. Right now, I just wanted to note that (@a-munoz-rojas), the tests are failing as the wilcoxon test seems to yield different results, now. I don't know whether it's just marginal, but it should be clarified; users should still get the same results as before.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:223,safety,test,test,223,"Sorry about the super late response and thank you for the PR, @a-munoz-rojas. I have not read most of the above discussion, yet. Right now, I just wanted to note that (@a-munoz-rojas), the tests are failing as the wilcoxon test seems to yield different results, now. I don't know whether it's just marginal, but it should be clarified; users should still get the same results as before.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:189,testability,test,tests,189,"Sorry about the super late response and thank you for the PR, @a-munoz-rojas. I have not read most of the above discussion, yet. Right now, I just wanted to note that (@a-munoz-rojas), the tests are failing as the wilcoxon test seems to yield different results, now. I don't know whether it's just marginal, but it should be clarified; users should still get the same results as before.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:223,testability,test,test,223,"Sorry about the super late response and thank you for the PR, @a-munoz-rojas. I have not read most of the above discussion, yet. Right now, I just wanted to note that (@a-munoz-rojas), the tests are failing as the wilcoxon test seems to yield different results, now. I don't know whether it's just marginal, but it should be clarified; users should still get the same results as before.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:336,usability,user,users,336,"Sorry about the super late response and thank you for the PR, @a-munoz-rojas. I have not read most of the above discussion, yet. Right now, I just wanted to note that (@a-munoz-rojas), the tests are failing as the wilcoxon test seems to yield different results, now. I don't know whether it's just marginal, but it should be clarified; users should still get the same results as before.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:66,availability,error,error,66,"Thanks for your reply, @falexwolf. I was looking through the same error and I can't really understand why the results are different - it might be a difference in accuracy levels between the manual wilcoxon method that was used before, and the built-in scipy.stats function I used. I looked at the differences and they indeed look marginal. **Edit:** I actually just went back through the check results, and the comparison between the results before and after are pretty much identical - it could just be a difference in bit-depth. For example, it's tagging (2.292195 , 5.7448500e-01) as different from (2.292195 , 0.574485). . I also compared a before-after with my dataset and I get very similar marker genes, albeit in slightly different order (see attached images). I don't really know what would be the best way to address these differences - I am simply using the built in spicy.stats function and not changing the output it gives me. Could this marginal difference be caused by the estimation in the ""chunk"" approach used in the previous version? Even with this marginal difference, I would assume that using the scipy function is more ""accurate"". Please let me know what you would prefer and what would be the best way to proceed. ![figure_1_newwilcox](https://user-images.githubusercontent.com/37122760/46375973-c93b4700-c662-11e8-8581-b85a28e36dbc.png). ![figure_1_originalwilcox](https://user-images.githubusercontent.com/37122760/46375974-c93b4700-c662-11e8-810b-48238394be1e.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:721,availability,sli,slightly,721,"Thanks for your reply, @falexwolf. I was looking through the same error and I can't really understand why the results are different - it might be a difference in accuracy levels between the manual wilcoxon method that was used before, and the built-in scipy.stats function I used. I looked at the differences and they indeed look marginal. **Edit:** I actually just went back through the check results, and the comparison between the results before and after are pretty much identical - it could just be a difference in bit-depth. For example, it's tagging (2.292195 , 5.7448500e-01) as different from (2.292195 , 0.574485). . I also compared a before-after with my dataset and I get very similar marker genes, albeit in slightly different order (see attached images). I don't really know what would be the best way to address these differences - I am simply using the built in spicy.stats function and not changing the output it gives me. Could this marginal difference be caused by the estimation in the ""chunk"" approach used in the previous version? Even with this marginal difference, I would assume that using the scipy function is more ""accurate"". Please let me know what you would prefer and what would be the best way to proceed. ![figure_1_newwilcox](https://user-images.githubusercontent.com/37122760/46375973-c93b4700-c662-11e8-8581-b85a28e36dbc.png). ![figure_1_originalwilcox](https://user-images.githubusercontent.com/37122760/46375974-c93b4700-c662-11e8-810b-48238394be1e.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:1044,deployability,version,version,1044,"Thanks for your reply, @falexwolf. I was looking through the same error and I can't really understand why the results are different - it might be a difference in accuracy levels between the manual wilcoxon method that was used before, and the built-in scipy.stats function I used. I looked at the differences and they indeed look marginal. **Edit:** I actually just went back through the check results, and the comparison between the results before and after are pretty much identical - it could just be a difference in bit-depth. For example, it's tagging (2.292195 , 5.7448500e-01) as different from (2.292195 , 0.574485). . I also compared a before-after with my dataset and I get very similar marker genes, albeit in slightly different order (see attached images). I don't really know what would be the best way to address these differences - I am simply using the built in spicy.stats function and not changing the output it gives me. Could this marginal difference be caused by the estimation in the ""chunk"" approach used in the previous version? Even with this marginal difference, I would assume that using the scipy function is more ""accurate"". Please let me know what you would prefer and what would be the best way to proceed. ![figure_1_newwilcox](https://user-images.githubusercontent.com/37122760/46375973-c93b4700-c662-11e8-8581-b85a28e36dbc.png). ![figure_1_originalwilcox](https://user-images.githubusercontent.com/37122760/46375974-c93b4700-c662-11e8-810b-48238394be1e.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:988,energy efficiency,estimat,estimation,988,"Thanks for your reply, @falexwolf. I was looking through the same error and I can't really understand why the results are different - it might be a difference in accuracy levels between the manual wilcoxon method that was used before, and the built-in scipy.stats function I used. I looked at the differences and they indeed look marginal. **Edit:** I actually just went back through the check results, and the comparison between the results before and after are pretty much identical - it could just be a difference in bit-depth. For example, it's tagging (2.292195 , 5.7448500e-01) as different from (2.292195 , 0.574485). . I also compared a before-after with my dataset and I get very similar marker genes, albeit in slightly different order (see attached images). I don't really know what would be the best way to address these differences - I am simply using the built in spicy.stats function and not changing the output it gives me. Could this marginal difference be caused by the estimation in the ""chunk"" approach used in the previous version? Even with this marginal difference, I would assume that using the scipy function is more ""accurate"". Please let me know what you would prefer and what would be the best way to proceed. ![figure_1_newwilcox](https://user-images.githubusercontent.com/37122760/46375973-c93b4700-c662-11e8-8581-b85a28e36dbc.png). ![figure_1_originalwilcox](https://user-images.githubusercontent.com/37122760/46375974-c93b4700-c662-11e8-810b-48238394be1e.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:1044,integrability,version,version,1044,"Thanks for your reply, @falexwolf. I was looking through the same error and I can't really understand why the results are different - it might be a difference in accuracy levels between the manual wilcoxon method that was used before, and the built-in scipy.stats function I used. I looked at the differences and they indeed look marginal. **Edit:** I actually just went back through the check results, and the comparison between the results before and after are pretty much identical - it could just be a difference in bit-depth. For example, it's tagging (2.292195 , 5.7448500e-01) as different from (2.292195 , 0.574485). . I also compared a before-after with my dataset and I get very similar marker genes, albeit in slightly different order (see attached images). I don't really know what would be the best way to address these differences - I am simply using the built in spicy.stats function and not changing the output it gives me. Could this marginal difference be caused by the estimation in the ""chunk"" approach used in the previous version? Even with this marginal difference, I would assume that using the scipy function is more ""accurate"". Please let me know what you would prefer and what would be the best way to proceed. ![figure_1_newwilcox](https://user-images.githubusercontent.com/37122760/46375973-c93b4700-c662-11e8-8581-b85a28e36dbc.png). ![figure_1_originalwilcox](https://user-images.githubusercontent.com/37122760/46375974-c93b4700-c662-11e8-810b-48238394be1e.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:1044,modifiability,version,version,1044,"Thanks for your reply, @falexwolf. I was looking through the same error and I can't really understand why the results are different - it might be a difference in accuracy levels between the manual wilcoxon method that was used before, and the built-in scipy.stats function I used. I looked at the differences and they indeed look marginal. **Edit:** I actually just went back through the check results, and the comparison between the results before and after are pretty much identical - it could just be a difference in bit-depth. For example, it's tagging (2.292195 , 5.7448500e-01) as different from (2.292195 , 0.574485). . I also compared a before-after with my dataset and I get very similar marker genes, albeit in slightly different order (see attached images). I don't really know what would be the best way to address these differences - I am simply using the built in spicy.stats function and not changing the output it gives me. Could this marginal difference be caused by the estimation in the ""chunk"" approach used in the previous version? Even with this marginal difference, I would assume that using the scipy function is more ""accurate"". Please let me know what you would prefer and what would be the best way to proceed. ![figure_1_newwilcox](https://user-images.githubusercontent.com/37122760/46375973-c93b4700-c662-11e8-8581-b85a28e36dbc.png). ![figure_1_originalwilcox](https://user-images.githubusercontent.com/37122760/46375974-c93b4700-c662-11e8-810b-48238394be1e.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:66,performance,error,error,66,"Thanks for your reply, @falexwolf. I was looking through the same error and I can't really understand why the results are different - it might be a difference in accuracy levels between the manual wilcoxon method that was used before, and the built-in scipy.stats function I used. I looked at the differences and they indeed look marginal. **Edit:** I actually just went back through the check results, and the comparison between the results before and after are pretty much identical - it could just be a difference in bit-depth. For example, it's tagging (2.292195 , 5.7448500e-01) as different from (2.292195 , 0.574485). . I also compared a before-after with my dataset and I get very similar marker genes, albeit in slightly different order (see attached images). I don't really know what would be the best way to address these differences - I am simply using the built in spicy.stats function and not changing the output it gives me. Could this marginal difference be caused by the estimation in the ""chunk"" approach used in the previous version? Even with this marginal difference, I would assume that using the scipy function is more ""accurate"". Please let me know what you would prefer and what would be the best way to proceed. ![figure_1_newwilcox](https://user-images.githubusercontent.com/37122760/46375973-c93b4700-c662-11e8-8581-b85a28e36dbc.png). ![figure_1_originalwilcox](https://user-images.githubusercontent.com/37122760/46375974-c93b4700-c662-11e8-810b-48238394be1e.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:721,reliability,sli,slightly,721,"Thanks for your reply, @falexwolf. I was looking through the same error and I can't really understand why the results are different - it might be a difference in accuracy levels between the manual wilcoxon method that was used before, and the built-in scipy.stats function I used. I looked at the differences and they indeed look marginal. **Edit:** I actually just went back through the check results, and the comparison between the results before and after are pretty much identical - it could just be a difference in bit-depth. For example, it's tagging (2.292195 , 5.7448500e-01) as different from (2.292195 , 0.574485). . I also compared a before-after with my dataset and I get very similar marker genes, albeit in slightly different order (see attached images). I don't really know what would be the best way to address these differences - I am simply using the built in spicy.stats function and not changing the output it gives me. Could this marginal difference be caused by the estimation in the ""chunk"" approach used in the previous version? Even with this marginal difference, I would assume that using the scipy function is more ""accurate"". Please let me know what you would prefer and what would be the best way to proceed. ![figure_1_newwilcox](https://user-images.githubusercontent.com/37122760/46375973-c93b4700-c662-11e8-8581-b85a28e36dbc.png). ![figure_1_originalwilcox](https://user-images.githubusercontent.com/37122760/46375974-c93b4700-c662-11e8-810b-48238394be1e.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:66,safety,error,error,66,"Thanks for your reply, @falexwolf. I was looking through the same error and I can't really understand why the results are different - it might be a difference in accuracy levels between the manual wilcoxon method that was used before, and the built-in scipy.stats function I used. I looked at the differences and they indeed look marginal. **Edit:** I actually just went back through the check results, and the comparison between the results before and after are pretty much identical - it could just be a difference in bit-depth. For example, it's tagging (2.292195 , 5.7448500e-01) as different from (2.292195 , 0.574485). . I also compared a before-after with my dataset and I get very similar marker genes, albeit in slightly different order (see attached images). I don't really know what would be the best way to address these differences - I am simply using the built in spicy.stats function and not changing the output it gives me. Could this marginal difference be caused by the estimation in the ""chunk"" approach used in the previous version? Even with this marginal difference, I would assume that using the scipy function is more ""accurate"". Please let me know what you would prefer and what would be the best way to proceed. ![figure_1_newwilcox](https://user-images.githubusercontent.com/37122760/46375973-c93b4700-c662-11e8-8581-b85a28e36dbc.png). ![figure_1_originalwilcox](https://user-images.githubusercontent.com/37122760/46375974-c93b4700-c662-11e8-810b-48238394be1e.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:475,security,ident,identical,475,"Thanks for your reply, @falexwolf. I was looking through the same error and I can't really understand why the results are different - it might be a difference in accuracy levels between the manual wilcoxon method that was used before, and the built-in scipy.stats function I used. I looked at the differences and they indeed look marginal. **Edit:** I actually just went back through the check results, and the comparison between the results before and after are pretty much identical - it could just be a difference in bit-depth. For example, it's tagging (2.292195 , 5.7448500e-01) as different from (2.292195 , 0.574485). . I also compared a before-after with my dataset and I get very similar marker genes, albeit in slightly different order (see attached images). I don't really know what would be the best way to address these differences - I am simply using the built in spicy.stats function and not changing the output it gives me. Could this marginal difference be caused by the estimation in the ""chunk"" approach used in the previous version? Even with this marginal difference, I would assume that using the scipy function is more ""accurate"". Please let me know what you would prefer and what would be the best way to proceed. ![figure_1_newwilcox](https://user-images.githubusercontent.com/37122760/46375973-c93b4700-c662-11e8-8581-b85a28e36dbc.png). ![figure_1_originalwilcox](https://user-images.githubusercontent.com/37122760/46375974-c93b4700-c662-11e8-810b-48238394be1e.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:91,testability,understand,understand,91,"Thanks for your reply, @falexwolf. I was looking through the same error and I can't really understand why the results are different - it might be a difference in accuracy levels between the manual wilcoxon method that was used before, and the built-in scipy.stats function I used. I looked at the differences and they indeed look marginal. **Edit:** I actually just went back through the check results, and the comparison between the results before and after are pretty much identical - it could just be a difference in bit-depth. For example, it's tagging (2.292195 , 5.7448500e-01) as different from (2.292195 , 0.574485). . I also compared a before-after with my dataset and I get very similar marker genes, albeit in slightly different order (see attached images). I don't really know what would be the best way to address these differences - I am simply using the built in spicy.stats function and not changing the output it gives me. Could this marginal difference be caused by the estimation in the ""chunk"" approach used in the previous version? Even with this marginal difference, I would assume that using the scipy function is more ""accurate"". Please let me know what you would prefer and what would be the best way to proceed. ![figure_1_newwilcox](https://user-images.githubusercontent.com/37122760/46375973-c93b4700-c662-11e8-8581-b85a28e36dbc.png). ![figure_1_originalwilcox](https://user-images.githubusercontent.com/37122760/46375974-c93b4700-c662-11e8-810b-48238394be1e.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:852,testability,simpl,simply,852,"Thanks for your reply, @falexwolf. I was looking through the same error and I can't really understand why the results are different - it might be a difference in accuracy levels between the manual wilcoxon method that was used before, and the built-in scipy.stats function I used. I looked at the differences and they indeed look marginal. **Edit:** I actually just went back through the check results, and the comparison between the results before and after are pretty much identical - it could just be a difference in bit-depth. For example, it's tagging (2.292195 , 5.7448500e-01) as different from (2.292195 , 0.574485). . I also compared a before-after with my dataset and I get very similar marker genes, albeit in slightly different order (see attached images). I don't really know what would be the best way to address these differences - I am simply using the built in spicy.stats function and not changing the output it gives me. Could this marginal difference be caused by the estimation in the ""chunk"" approach used in the previous version? Even with this marginal difference, I would assume that using the scipy function is more ""accurate"". Please let me know what you would prefer and what would be the best way to proceed. ![figure_1_newwilcox](https://user-images.githubusercontent.com/37122760/46375973-c93b4700-c662-11e8-8581-b85a28e36dbc.png). ![figure_1_originalwilcox](https://user-images.githubusercontent.com/37122760/46375974-c93b4700-c662-11e8-810b-48238394be1e.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:66,usability,error,error,66,"Thanks for your reply, @falexwolf. I was looking through the same error and I can't really understand why the results are different - it might be a difference in accuracy levels between the manual wilcoxon method that was used before, and the built-in scipy.stats function I used. I looked at the differences and they indeed look marginal. **Edit:** I actually just went back through the check results, and the comparison between the results before and after are pretty much identical - it could just be a difference in bit-depth. For example, it's tagging (2.292195 , 5.7448500e-01) as different from (2.292195 , 0.574485). . I also compared a before-after with my dataset and I get very similar marker genes, albeit in slightly different order (see attached images). I don't really know what would be the best way to address these differences - I am simply using the built in spicy.stats function and not changing the output it gives me. Could this marginal difference be caused by the estimation in the ""chunk"" approach used in the previous version? Even with this marginal difference, I would assume that using the scipy function is more ""accurate"". Please let me know what you would prefer and what would be the best way to proceed. ![figure_1_newwilcox](https://user-images.githubusercontent.com/37122760/46375973-c93b4700-c662-11e8-8581-b85a28e36dbc.png). ![figure_1_originalwilcox](https://user-images.githubusercontent.com/37122760/46375974-c93b4700-c662-11e8-810b-48238394be1e.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:852,usability,simpl,simply,852,"Thanks for your reply, @falexwolf. I was looking through the same error and I can't really understand why the results are different - it might be a difference in accuracy levels between the manual wilcoxon method that was used before, and the built-in scipy.stats function I used. I looked at the differences and they indeed look marginal. **Edit:** I actually just went back through the check results, and the comparison between the results before and after are pretty much identical - it could just be a difference in bit-depth. For example, it's tagging (2.292195 , 5.7448500e-01) as different from (2.292195 , 0.574485). . I also compared a before-after with my dataset and I get very similar marker genes, albeit in slightly different order (see attached images). I don't really know what would be the best way to address these differences - I am simply using the built in spicy.stats function and not changing the output it gives me. Could this marginal difference be caused by the estimation in the ""chunk"" approach used in the previous version? Even with this marginal difference, I would assume that using the scipy function is more ""accurate"". Please let me know what you would prefer and what would be the best way to proceed. ![figure_1_newwilcox](https://user-images.githubusercontent.com/37122760/46375973-c93b4700-c662-11e8-8581-b85a28e36dbc.png). ![figure_1_originalwilcox](https://user-images.githubusercontent.com/37122760/46375974-c93b4700-c662-11e8-810b-48238394be1e.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:1188,usability,prefer,prefer,1188,"Thanks for your reply, @falexwolf. I was looking through the same error and I can't really understand why the results are different - it might be a difference in accuracy levels between the manual wilcoxon method that was used before, and the built-in scipy.stats function I used. I looked at the differences and they indeed look marginal. **Edit:** I actually just went back through the check results, and the comparison between the results before and after are pretty much identical - it could just be a difference in bit-depth. For example, it's tagging (2.292195 , 5.7448500e-01) as different from (2.292195 , 0.574485). . I also compared a before-after with my dataset and I get very similar marker genes, albeit in slightly different order (see attached images). I don't really know what would be the best way to address these differences - I am simply using the built in spicy.stats function and not changing the output it gives me. Could this marginal difference be caused by the estimation in the ""chunk"" approach used in the previous version? Even with this marginal difference, I would assume that using the scipy function is more ""accurate"". Please let me know what you would prefer and what would be the best way to proceed. ![figure_1_newwilcox](https://user-images.githubusercontent.com/37122760/46375973-c93b4700-c662-11e8-8581-b85a28e36dbc.png). ![figure_1_originalwilcox](https://user-images.githubusercontent.com/37122760/46375974-c93b4700-c662-11e8-810b-48238394be1e.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:1268,usability,user,user-images,1268,"Thanks for your reply, @falexwolf. I was looking through the same error and I can't really understand why the results are different - it might be a difference in accuracy levels between the manual wilcoxon method that was used before, and the built-in scipy.stats function I used. I looked at the differences and they indeed look marginal. **Edit:** I actually just went back through the check results, and the comparison between the results before and after are pretty much identical - it could just be a difference in bit-depth. For example, it's tagging (2.292195 , 5.7448500e-01) as different from (2.292195 , 0.574485). . I also compared a before-after with my dataset and I get very similar marker genes, albeit in slightly different order (see attached images). I don't really know what would be the best way to address these differences - I am simply using the built in spicy.stats function and not changing the output it gives me. Could this marginal difference be caused by the estimation in the ""chunk"" approach used in the previous version? Even with this marginal difference, I would assume that using the scipy function is more ""accurate"". Please let me know what you would prefer and what would be the best way to proceed. ![figure_1_newwilcox](https://user-images.githubusercontent.com/37122760/46375973-c93b4700-c662-11e8-8581-b85a28e36dbc.png). ![figure_1_originalwilcox](https://user-images.githubusercontent.com/37122760/46375974-c93b4700-c662-11e8-810b-48238394be1e.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:1398,usability,user,user-images,1398,"Thanks for your reply, @falexwolf. I was looking through the same error and I can't really understand why the results are different - it might be a difference in accuracy levels between the manual wilcoxon method that was used before, and the built-in scipy.stats function I used. I looked at the differences and they indeed look marginal. **Edit:** I actually just went back through the check results, and the comparison between the results before and after are pretty much identical - it could just be a difference in bit-depth. For example, it's tagging (2.292195 , 5.7448500e-01) as different from (2.292195 , 0.574485). . I also compared a before-after with my dataset and I get very similar marker genes, albeit in slightly different order (see attached images). I don't really know what would be the best way to address these differences - I am simply using the built in spicy.stats function and not changing the output it gives me. Could this marginal difference be caused by the estimation in the ""chunk"" approach used in the previous version? Even with this marginal difference, I would assume that using the scipy function is more ""accurate"". Please let me know what you would prefer and what would be the best way to proceed. ![figure_1_newwilcox](https://user-images.githubusercontent.com/37122760/46375973-c93b4700-c662-11e8-8581-b85a28e36dbc.png). ![figure_1_originalwilcox](https://user-images.githubusercontent.com/37122760/46375974-c93b4700-c662-11e8-810b-48238394be1e.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:80,availability,replic,replicate,80,"Also, to reply to @LuckyMD, I definitely see your point. I was mostly trying to replicate other single-cell tools that do provide this metric, but I understand why it could be characterized as misleading - maybe we can add a disclaimer in the function description? I'm open to other suggestions!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:149,testability,understand,understand,149,"Also, to reply to @LuckyMD, I definitely see your point. I was mostly trying to replicate other single-cell tools that do provide this metric, but I understand why it could be characterized as misleading - maybe we can add a disclaimer in the function description? I'm open to other suggestions!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:108,usability,tool,tools,108,"Also, to reply to @LuckyMD, I definitely see your point. I was mostly trying to replicate other single-cell tools that do provide this metric, but I understand why it could be characterized as misleading - maybe we can add a disclaimer in the function description? I'm open to other suggestions!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:90,deployability,version,version,90,"@a-munoz-rojas Thanks for checking and thank you very much for the PR :smile! I like your version of the implementation a lot better than the manual one, as the code is much more readable. I don't know why @tcallies added the wilcoxon this way at the time, but I assume for speed and memory reasons. So, I'm very happy to merge this PR; I'll just briefly give this another check today or tomorrow and update the tests so that they don't fail anymore. Regarding the general discussion: Yes, let's just add a disclaimer that several assumptions on how meaningful the null hypotheses are both for wilxocon and t-test for single-cell data, should do the job. Then people will interpret the p-values with care.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:401,deployability,updat,update,401,"@a-munoz-rojas Thanks for checking and thank you very much for the PR :smile! I like your version of the implementation a lot better than the manual one, as the code is much more readable. I don't know why @tcallies added the wilcoxon this way at the time, but I assume for speed and memory reasons. So, I'm very happy to merge this PR; I'll just briefly give this another check today or tomorrow and update the tests so that they don't fail anymore. Regarding the general discussion: Yes, let's just add a disclaimer that several assumptions on how meaningful the null hypotheses are both for wilxocon and t-test for single-cell data, should do the job. Then people will interpret the p-values with care.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:437,deployability,fail,fail,437,"@a-munoz-rojas Thanks for checking and thank you very much for the PR :smile! I like your version of the implementation a lot better than the manual one, as the code is much more readable. I don't know why @tcallies added the wilcoxon this way at the time, but I assume for speed and memory reasons. So, I'm very happy to merge this PR; I'll just briefly give this another check today or tomorrow and update the tests so that they don't fail anymore. Regarding the general discussion: Yes, let's just add a disclaimer that several assumptions on how meaningful the null hypotheses are both for wilxocon and t-test for single-cell data, should do the job. Then people will interpret the p-values with care.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:90,integrability,version,version,90,"@a-munoz-rojas Thanks for checking and thank you very much for the PR :smile! I like your version of the implementation a lot better than the manual one, as the code is much more readable. I don't know why @tcallies added the wilcoxon this way at the time, but I assume for speed and memory reasons. So, I'm very happy to merge this PR; I'll just briefly give this another check today or tomorrow and update the tests so that they don't fail anymore. Regarding the general discussion: Yes, let's just add a disclaimer that several assumptions on how meaningful the null hypotheses are both for wilxocon and t-test for single-cell data, should do the job. Then people will interpret the p-values with care.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:90,modifiability,version,version,90,"@a-munoz-rojas Thanks for checking and thank you very much for the PR :smile! I like your version of the implementation a lot better than the manual one, as the code is much more readable. I don't know why @tcallies added the wilcoxon this way at the time, but I assume for speed and memory reasons. So, I'm very happy to merge this PR; I'll just briefly give this another check today or tomorrow and update the tests so that they don't fail anymore. Regarding the general discussion: Yes, let's just add a disclaimer that several assumptions on how meaningful the null hypotheses are both for wilxocon and t-test for single-cell data, should do the job. Then people will interpret the p-values with care.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:251,performance,time,time,251,"@a-munoz-rojas Thanks for checking and thank you very much for the PR :smile! I like your version of the implementation a lot better than the manual one, as the code is much more readable. I don't know why @tcallies added the wilcoxon this way at the time, but I assume for speed and memory reasons. So, I'm very happy to merge this PR; I'll just briefly give this another check today or tomorrow and update the tests so that they don't fail anymore. Regarding the general discussion: Yes, let's just add a disclaimer that several assumptions on how meaningful the null hypotheses are both for wilxocon and t-test for single-cell data, should do the job. Then people will interpret the p-values with care.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:284,performance,memor,memory,284,"@a-munoz-rojas Thanks for checking and thank you very much for the PR :smile! I like your version of the implementation a lot better than the manual one, as the code is much more readable. I don't know why @tcallies added the wilcoxon this way at the time, but I assume for speed and memory reasons. So, I'm very happy to merge this PR; I'll just briefly give this another check today or tomorrow and update the tests so that they don't fail anymore. Regarding the general discussion: Yes, let's just add a disclaimer that several assumptions on how meaningful the null hypotheses are both for wilxocon and t-test for single-cell data, should do the job. Then people will interpret the p-values with care.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:437,reliability,fail,fail,437,"@a-munoz-rojas Thanks for checking and thank you very much for the PR :smile! I like your version of the implementation a lot better than the manual one, as the code is much more readable. I don't know why @tcallies added the wilcoxon this way at the time, but I assume for speed and memory reasons. So, I'm very happy to merge this PR; I'll just briefly give this another check today or tomorrow and update the tests so that they don't fail anymore. Regarding the general discussion: Yes, let's just add a disclaimer that several assumptions on how meaningful the null hypotheses are both for wilxocon and t-test for single-cell data, should do the job. Then people will interpret the p-values with care.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:401,safety,updat,update,401,"@a-munoz-rojas Thanks for checking and thank you very much for the PR :smile! I like your version of the implementation a lot better than the manual one, as the code is much more readable. I don't know why @tcallies added the wilcoxon this way at the time, but I assume for speed and memory reasons. So, I'm very happy to merge this PR; I'll just briefly give this another check today or tomorrow and update the tests so that they don't fail anymore. Regarding the general discussion: Yes, let's just add a disclaimer that several assumptions on how meaningful the null hypotheses are both for wilxocon and t-test for single-cell data, should do the job. Then people will interpret the p-values with care.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:412,safety,test,tests,412,"@a-munoz-rojas Thanks for checking and thank you very much for the PR :smile! I like your version of the implementation a lot better than the manual one, as the code is much more readable. I don't know why @tcallies added the wilcoxon this way at the time, but I assume for speed and memory reasons. So, I'm very happy to merge this PR; I'll just briefly give this another check today or tomorrow and update the tests so that they don't fail anymore. Regarding the general discussion: Yes, let's just add a disclaimer that several assumptions on how meaningful the null hypotheses are both for wilxocon and t-test for single-cell data, should do the job. Then people will interpret the p-values with care.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:609,safety,test,test,609,"@a-munoz-rojas Thanks for checking and thank you very much for the PR :smile! I like your version of the implementation a lot better than the manual one, as the code is much more readable. I don't know why @tcallies added the wilcoxon this way at the time, but I assume for speed and memory reasons. So, I'm very happy to merge this PR; I'll just briefly give this another check today or tomorrow and update the tests so that they don't fail anymore. Regarding the general discussion: Yes, let's just add a disclaimer that several assumptions on how meaningful the null hypotheses are both for wilxocon and t-test for single-cell data, should do the job. Then people will interpret the p-values with care.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:401,security,updat,update,401,"@a-munoz-rojas Thanks for checking and thank you very much for the PR :smile! I like your version of the implementation a lot better than the manual one, as the code is much more readable. I don't know why @tcallies added the wilcoxon this way at the time, but I assume for speed and memory reasons. So, I'm very happy to merge this PR; I'll just briefly give this another check today or tomorrow and update the tests so that they don't fail anymore. Regarding the general discussion: Yes, let's just add a disclaimer that several assumptions on how meaningful the null hypotheses are both for wilxocon and t-test for single-cell data, should do the job. Then people will interpret the p-values with care.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:412,testability,test,tests,412,"@a-munoz-rojas Thanks for checking and thank you very much for the PR :smile! I like your version of the implementation a lot better than the manual one, as the code is much more readable. I don't know why @tcallies added the wilcoxon this way at the time, but I assume for speed and memory reasons. So, I'm very happy to merge this PR; I'll just briefly give this another check today or tomorrow and update the tests so that they don't fail anymore. Regarding the general discussion: Yes, let's just add a disclaimer that several assumptions on how meaningful the null hypotheses are both for wilxocon and t-test for single-cell data, should do the job. Then people will interpret the p-values with care.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:609,testability,test,test,609,"@a-munoz-rojas Thanks for checking and thank you very much for the PR :smile! I like your version of the implementation a lot better than the manual one, as the code is much more readable. I don't know why @tcallies added the wilcoxon this way at the time, but I assume for speed and memory reasons. So, I'm very happy to merge this PR; I'll just briefly give this another check today or tomorrow and update the tests so that they don't fail anymore. Regarding the general discussion: Yes, let's just add a disclaimer that several assumptions on how meaningful the null hypotheses are both for wilxocon and t-test for single-cell data, should do the job. Then people will interpret the p-values with care.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:284,usability,memor,memory,284,"@a-munoz-rojas Thanks for checking and thank you very much for the PR :smile! I like your version of the implementation a lot better than the manual one, as the code is much more readable. I don't know why @tcallies added the wilcoxon this way at the time, but I assume for speed and memory reasons. So, I'm very happy to merge this PR; I'll just briefly give this another check today or tomorrow and update the tests so that they don't fail anymore. Regarding the general discussion: Yes, let's just add a disclaimer that several assumptions on how meaningful the null hypotheses are both for wilxocon and t-test for single-cell data, should do the job. Then people will interpret the p-values with care.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:88,availability,slo,slower,88,"So, the main problem I see is that the runtime of the new implementation is a factor 20 slower: 37 s vs. 2 s. . ![image](https://user-images.githubusercontent.com/16916678/46554603-cf256800-c8ae-11e8-9607-f67279e80d4f.png). vs. ![image](https://user-images.githubusercontent.com/16916678/46554622-e19fa180-c8ae-11e8-90ef-4a2c4b3839af.png). Let me think about what to do, here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:88,reliability,slo,slower,88,"So, the main problem I see is that the runtime of the new implementation is a factor 20 slower: 37 s vs. 2 s. . ![image](https://user-images.githubusercontent.com/16916678/46554603-cf256800-c8ae-11e8-9607-f67279e80d4f.png). vs. ![image](https://user-images.githubusercontent.com/16916678/46554622-e19fa180-c8ae-11e8-90ef-4a2c4b3839af.png). Let me think about what to do, here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:129,usability,user,user-images,129,"So, the main problem I see is that the runtime of the new implementation is a factor 20 slower: 37 s vs. 2 s. . ![image](https://user-images.githubusercontent.com/16916678/46554603-cf256800-c8ae-11e8-9607-f67279e80d4f.png). vs. ![image](https://user-images.githubusercontent.com/16916678/46554622-e19fa180-c8ae-11e8-90ef-4a2c4b3839af.png). Let me think about what to do, here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:245,usability,user,user-images,245,"So, the main problem I see is that the runtime of the new implementation is a factor 20 slower: 37 s vs. 2 s. . ![image](https://user-images.githubusercontent.com/16916678/46554603-cf256800-c8ae-11e8-9607-f67279e80d4f.png). vs. ![image](https://user-images.githubusercontent.com/16916678/46554622-e19fa180-c8ae-11e8-90ef-4a2c4b3839af.png). Let me think about what to do, here.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:26,availability,restor,restore,26,"I'm merging this but will restore the previous Wilcoxon implementation, for speed reasons. The essential problem is that scipy.stats does not have a multi-dimensional implementation; it should be easy to adapt the previous implementation so that it provides pvalues, too; simply via multi-dimensional adaption of https://github.com/scipy/scipy/blob/v1.1.0/scipy/stats/stats.py#L4931-L4974.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:204,energy efficiency,adapt,adapt,204,"I'm merging this but will restore the previous Wilcoxon implementation, for speed reasons. The essential problem is that scipy.stats does not have a multi-dimensional implementation; it should be easy to adapt the previous implementation so that it provides pvalues, too; simply via multi-dimensional adaption of https://github.com/scipy/scipy/blob/v1.1.0/scipy/stats/stats.py#L4931-L4974.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:301,energy efficiency,adapt,adaption,301,"I'm merging this but will restore the previous Wilcoxon implementation, for speed reasons. The essential problem is that scipy.stats does not have a multi-dimensional implementation; it should be easy to adapt the previous implementation so that it provides pvalues, too; simply via multi-dimensional adaption of https://github.com/scipy/scipy/blob/v1.1.0/scipy/stats/stats.py#L4931-L4974.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:204,integrability,adapt,adapt,204,"I'm merging this but will restore the previous Wilcoxon implementation, for speed reasons. The essential problem is that scipy.stats does not have a multi-dimensional implementation; it should be easy to adapt the previous implementation so that it provides pvalues, too; simply via multi-dimensional adaption of https://github.com/scipy/scipy/blob/v1.1.0/scipy/stats/stats.py#L4931-L4974.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:301,integrability,adapt,adaption,301,"I'm merging this but will restore the previous Wilcoxon implementation, for speed reasons. The essential problem is that scipy.stats does not have a multi-dimensional implementation; it should be easy to adapt the previous implementation so that it provides pvalues, too; simply via multi-dimensional adaption of https://github.com/scipy/scipy/blob/v1.1.0/scipy/stats/stats.py#L4931-L4974.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:204,interoperability,adapt,adapt,204,"I'm merging this but will restore the previous Wilcoxon implementation, for speed reasons. The essential problem is that scipy.stats does not have a multi-dimensional implementation; it should be easy to adapt the previous implementation so that it provides pvalues, too; simply via multi-dimensional adaption of https://github.com/scipy/scipy/blob/v1.1.0/scipy/stats/stats.py#L4931-L4974.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:301,interoperability,adapt,adaption,301,"I'm merging this but will restore the previous Wilcoxon implementation, for speed reasons. The essential problem is that scipy.stats does not have a multi-dimensional implementation; it should be easy to adapt the previous implementation so that it provides pvalues, too; simply via multi-dimensional adaption of https://github.com/scipy/scipy/blob/v1.1.0/scipy/stats/stats.py#L4931-L4974.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:204,modifiability,adapt,adapt,204,"I'm merging this but will restore the previous Wilcoxon implementation, for speed reasons. The essential problem is that scipy.stats does not have a multi-dimensional implementation; it should be easy to adapt the previous implementation so that it provides pvalues, too; simply via multi-dimensional adaption of https://github.com/scipy/scipy/blob/v1.1.0/scipy/stats/stats.py#L4931-L4974.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:301,modifiability,adapt,adaption,301,"I'm merging this but will restore the previous Wilcoxon implementation, for speed reasons. The essential problem is that scipy.stats does not have a multi-dimensional implementation; it should be easy to adapt the previous implementation so that it provides pvalues, too; simply via multi-dimensional adaption of https://github.com/scipy/scipy/blob/v1.1.0/scipy/stats/stats.py#L4931-L4974.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:26,reliability,restor,restore,26,"I'm merging this but will restore the previous Wilcoxon implementation, for speed reasons. The essential problem is that scipy.stats does not have a multi-dimensional implementation; it should be easy to adapt the previous implementation so that it provides pvalues, too; simply via multi-dimensional adaption of https://github.com/scipy/scipy/blob/v1.1.0/scipy/stats/stats.py#L4931-L4974.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:133,reliability,doe,does,133,"I'm merging this but will restore the previous Wilcoxon implementation, for speed reasons. The essential problem is that scipy.stats does not have a multi-dimensional implementation; it should be easy to adapt the previous implementation so that it provides pvalues, too; simply via multi-dimensional adaption of https://github.com/scipy/scipy/blob/v1.1.0/scipy/stats/stats.py#L4931-L4974.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:272,testability,simpl,simply,272,"I'm merging this but will restore the previous Wilcoxon implementation, for speed reasons. The essential problem is that scipy.stats does not have a multi-dimensional implementation; it should be easy to adapt the previous implementation so that it provides pvalues, too; simply via multi-dimensional adaption of https://github.com/scipy/scipy/blob/v1.1.0/scipy/stats/stats.py#L4931-L4974.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:272,usability,simpl,simply,272,"I'm merging this but will restore the previous Wilcoxon implementation, for speed reasons. The essential problem is that scipy.stats does not have a multi-dimensional implementation; it should be easy to adapt the previous implementation so that it provides pvalues, too; simply via multi-dimensional adaption of https://github.com/scipy/scipy/blob/v1.1.0/scipy/stats/stats.py#L4931-L4974.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:43,availability,slo,slow-down,43,"Sounds great, @falexwolf! I did notice the slow-down and agree it's not great. That's a great suggestion, I'll take a look. Thanks, glad it helped!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:43,reliability,slo,slow-down,43,"Sounds great, @falexwolf! I did notice the slow-down and agree it's not great. That's a great suggestion, I'll take a look. Thanks, glad it helped!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:140,usability,help,helped,140,"Sounds great, @falexwolf! I did notice the slow-down and agree it's not great. That's a great suggestion, I'll take a look. Thanks, glad it helped!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:323,deployability,modul,module,323,"If you @a-munoz-rojas would give it another try, using a multi-dimensional chunked implementation along the current implementation and the comment above, then that would be very cool! If you don't have bandwidth for that, could you pass this on to @Koncopd, who might have bandwidth? It would be nice to clean up the whole module (in particular, split up the long code chunks into three functions `_t_test()`, `_wilcoxon()`, `_logreg()`). I also made the tests work again; I guess they failed as the current implementation and scipy are handling ties a little different; the results for scores were exactly the same up to the position of a single gene. . Please make a new PR for any of this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:486,deployability,fail,failed,486,"If you @a-munoz-rojas would give it another try, using a multi-dimensional chunked implementation along the current implementation and the comment above, then that would be very cool! If you don't have bandwidth for that, could you pass this on to @Koncopd, who might have bandwidth? It would be nice to clean up the whole module (in particular, split up the long code chunks into three functions `_t_test()`, `_wilcoxon()`, `_logreg()`). I also made the tests work again; I guess they failed as the current implementation and scipy are handling ties a little different; the results for scores were exactly the same up to the position of a single gene. . Please make a new PR for any of this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:108,energy efficiency,current,current,108,"If you @a-munoz-rojas would give it another try, using a multi-dimensional chunked implementation along the current implementation and the comment above, then that would be very cool! If you don't have bandwidth for that, could you pass this on to @Koncopd, who might have bandwidth? It would be nice to clean up the whole module (in particular, split up the long code chunks into three functions `_t_test()`, `_wilcoxon()`, `_logreg()`). I also made the tests work again; I guess they failed as the current implementation and scipy are handling ties a little different; the results for scores were exactly the same up to the position of a single gene. . Please make a new PR for any of this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:178,energy efficiency,cool,cool,178,"If you @a-munoz-rojas would give it another try, using a multi-dimensional chunked implementation along the current implementation and the comment above, then that would be very cool! If you don't have bandwidth for that, could you pass this on to @Koncopd, who might have bandwidth? It would be nice to clean up the whole module (in particular, split up the long code chunks into three functions `_t_test()`, `_wilcoxon()`, `_logreg()`). I also made the tests work again; I guess they failed as the current implementation and scipy are handling ties a little different; the results for scores were exactly the same up to the position of a single gene. . Please make a new PR for any of this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:500,energy efficiency,current,current,500,"If you @a-munoz-rojas would give it another try, using a multi-dimensional chunked implementation along the current implementation and the comment above, then that would be very cool! If you don't have bandwidth for that, could you pass this on to @Koncopd, who might have bandwidth? It would be nice to clean up the whole module (in particular, split up the long code chunks into three functions `_t_test()`, `_wilcoxon()`, `_logreg()`). I also made the tests work again; I guess they failed as the current implementation and scipy are handling ties a little different; the results for scores were exactly the same up to the position of a single gene. . Please make a new PR for any of this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:323,modifiability,modul,module,323,"If you @a-munoz-rojas would give it another try, using a multi-dimensional chunked implementation along the current implementation and the comment above, then that would be very cool! If you don't have bandwidth for that, could you pass this on to @Koncopd, who might have bandwidth? It would be nice to clean up the whole module (in particular, split up the long code chunks into three functions `_t_test()`, `_wilcoxon()`, `_logreg()`). I also made the tests work again; I guess they failed as the current implementation and scipy are handling ties a little different; the results for scores were exactly the same up to the position of a single gene. . Please make a new PR for any of this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:486,reliability,fail,failed,486,"If you @a-munoz-rojas would give it another try, using a multi-dimensional chunked implementation along the current implementation and the comment above, then that would be very cool! If you don't have bandwidth for that, could you pass this on to @Koncopd, who might have bandwidth? It would be nice to clean up the whole module (in particular, split up the long code chunks into three functions `_t_test()`, `_wilcoxon()`, `_logreg()`). I also made the tests work again; I guess they failed as the current implementation and scipy are handling ties a little different; the results for scores were exactly the same up to the position of a single gene. . Please make a new PR for any of this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:323,safety,modul,module,323,"If you @a-munoz-rojas would give it another try, using a multi-dimensional chunked implementation along the current implementation and the comment above, then that would be very cool! If you don't have bandwidth for that, could you pass this on to @Koncopd, who might have bandwidth? It would be nice to clean up the whole module (in particular, split up the long code chunks into three functions `_t_test()`, `_wilcoxon()`, `_logreg()`). I also made the tests work again; I guess they failed as the current implementation and scipy are handling ties a little different; the results for scores were exactly the same up to the position of a single gene. . Please make a new PR for any of this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:455,safety,test,tests,455,"If you @a-munoz-rojas would give it another try, using a multi-dimensional chunked implementation along the current implementation and the comment above, then that would be very cool! If you don't have bandwidth for that, could you pass this on to @Koncopd, who might have bandwidth? It would be nice to clean up the whole module (in particular, split up the long code chunks into three functions `_t_test()`, `_wilcoxon()`, `_logreg()`). I also made the tests work again; I guess they failed as the current implementation and scipy are handling ties a little different; the results for scores were exactly the same up to the position of a single gene. . Please make a new PR for any of this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:455,testability,test,tests,455,"If you @a-munoz-rojas would give it another try, using a multi-dimensional chunked implementation along the current implementation and the comment above, then that would be very cool! If you don't have bandwidth for that, could you pass this on to @Koncopd, who might have bandwidth? It would be nice to clean up the whole module (in particular, split up the long code chunks into three functions `_t_test()`, `_wilcoxon()`, `_logreg()`). I also made the tests work again; I guess they failed as the current implementation and scipy are handling ties a little different; the results for scores were exactly the same up to the position of a single gene. . Please make a new PR for any of this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:34,performance,time,time,34,"Ah, now we just wrote at the same time. ;) Let me know what you think.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:12,safety,test,tests,12,"Ok, now the tests are actually passing again, everything is in the three commits prior and including this one: https://github.com/theislab/scanpy/commit/d889faf9a58d8981c0783584b3f333680fc161ce",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:12,testability,test,tests,12,"Ok, now the tests are actually passing again, everything is in the three commits prior and including this one: https://github.com/theislab/scanpy/commit/d889faf9a58d8981c0783584b3f333680fc161ce",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:96,performance,memor,memory,96,"And: just as a remark; the chunking along genes is necessary so that everything still fits into memory for really large datasets, which are almost always sparse when fed into `rank_genes_groups`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:96,usability,memor,memory,96,"And: just as a remark; the chunking along genes is necessary so that everything still fits into memory for really large datasets, which are almost always sparse when fed into `rank_genes_groups`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:141,energy efficiency,adapt,adaptation,141,"I was just about to ask about the chunking along genes - you read my mind @falexwolf. I think it might be possible to do a multi-dimensional adaptation of the scipy.stats code you linked to, and still do the math with sparse matrices, similar to how we implemented the t-tests. This way we could possibly avoid the chunking (it might help with readability of the code). Would this be worth pursuing? I'll give this a quick try, but I am a little limited in bandwidth. I'll let you know soon if it would be best to get some help from @Koncopd (if they have time!)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:141,integrability,adapt,adaptation,141,"I was just about to ask about the chunking along genes - you read my mind @falexwolf. I think it might be possible to do a multi-dimensional adaptation of the scipy.stats code you linked to, and still do the math with sparse matrices, similar to how we implemented the t-tests. This way we could possibly avoid the chunking (it might help with readability of the code). Would this be worth pursuing? I'll give this a quick try, but I am a little limited in bandwidth. I'll let you know soon if it would be best to get some help from @Koncopd (if they have time!)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:141,interoperability,adapt,adaptation,141,"I was just about to ask about the chunking along genes - you read my mind @falexwolf. I think it might be possible to do a multi-dimensional adaptation of the scipy.stats code you linked to, and still do the math with sparse matrices, similar to how we implemented the t-tests. This way we could possibly avoid the chunking (it might help with readability of the code). Would this be worth pursuing? I'll give this a quick try, but I am a little limited in bandwidth. I'll let you know soon if it would be best to get some help from @Koncopd (if they have time!)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:141,modifiability,adapt,adaptation,141,"I was just about to ask about the chunking along genes - you read my mind @falexwolf. I think it might be possible to do a multi-dimensional adaptation of the scipy.stats code you linked to, and still do the math with sparse matrices, similar to how we implemented the t-tests. This way we could possibly avoid the chunking (it might help with readability of the code). Would this be worth pursuing? I'll give this a quick try, but I am a little limited in bandwidth. I'll let you know soon if it would be best to get some help from @Koncopd (if they have time!)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:556,performance,time,time,556,"I was just about to ask about the chunking along genes - you read my mind @falexwolf. I think it might be possible to do a multi-dimensional adaptation of the scipy.stats code you linked to, and still do the math with sparse matrices, similar to how we implemented the t-tests. This way we could possibly avoid the chunking (it might help with readability of the code). Would this be worth pursuing? I'll give this a quick try, but I am a little limited in bandwidth. I'll let you know soon if it would be best to get some help from @Koncopd (if they have time!)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:271,safety,test,tests,271,"I was just about to ask about the chunking along genes - you read my mind @falexwolf. I think it might be possible to do a multi-dimensional adaptation of the scipy.stats code you linked to, and still do the math with sparse matrices, similar to how we implemented the t-tests. This way we could possibly avoid the chunking (it might help with readability of the code). Would this be worth pursuing? I'll give this a quick try, but I am a little limited in bandwidth. I'll let you know soon if it would be best to get some help from @Koncopd (if they have time!)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:305,safety,avoid,avoid,305,"I was just about to ask about the chunking along genes - you read my mind @falexwolf. I think it might be possible to do a multi-dimensional adaptation of the scipy.stats code you linked to, and still do the math with sparse matrices, similar to how we implemented the t-tests. This way we could possibly avoid the chunking (it might help with readability of the code). Would this be worth pursuing? I'll give this a quick try, but I am a little limited in bandwidth. I'll let you know soon if it would be best to get some help from @Koncopd (if they have time!)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:271,testability,test,tests,271,"I was just about to ask about the chunking along genes - you read my mind @falexwolf. I think it might be possible to do a multi-dimensional adaptation of the scipy.stats code you linked to, and still do the math with sparse matrices, similar to how we implemented the t-tests. This way we could possibly avoid the chunking (it might help with readability of the code). Would this be worth pursuing? I'll give this a quick try, but I am a little limited in bandwidth. I'll let you know soon if it would be best to get some help from @Koncopd (if they have time!)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:334,usability,help,help,334,"I was just about to ask about the chunking along genes - you read my mind @falexwolf. I think it might be possible to do a multi-dimensional adaptation of the scipy.stats code you linked to, and still do the math with sparse matrices, similar to how we implemented the t-tests. This way we could possibly avoid the chunking (it might help with readability of the code). Would this be worth pursuing? I'll give this a quick try, but I am a little limited in bandwidth. I'll let you know soon if it would be best to get some help from @Koncopd (if they have time!)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/pull/270:523,usability,help,help,523,"I was just about to ask about the chunking along genes - you read my mind @falexwolf. I think it might be possible to do a multi-dimensional adaptation of the scipy.stats code you linked to, and still do the math with sparse matrices, similar to how we implemented the t-tests. This way we could possibly avoid the chunking (it might help with readability of the code). Would this be worth pursuing? I'll give this a quick try, but I am a little limited in bandwidth. I'll let you know soon if it would be best to get some help from @Koncopd (if they have time!)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/270
https://github.com/scverse/scanpy/issues/271:154,integrability,discover,discovering-plugins,154,This page summarizes the approaches mentioned by @flying-sheep together with examples to implement them: https://packaging.python.org/guides/creating-and-discovering-plugins/. My opinion is to implement the option that is easier for the plugin developer to facilitate adoption.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:154,interoperability,discover,discovering-plugins,154,This page summarizes the approaches mentioned by @flying-sheep together with examples to implement them: https://packaging.python.org/guides/creating-and-discovering-plugins/. My opinion is to implement the option that is easier for the plugin developer to facilitate adoption.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:237,interoperability,plug,plugin,237,This page summarizes the approaches mentioned by @flying-sheep together with examples to implement them: https://packaging.python.org/guides/creating-and-discovering-plugins/. My opinion is to implement the option that is easier for the plugin developer to facilitate adoption.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:113,modifiability,pac,packaging,113,This page summarizes the approaches mentioned by @flying-sheep together with examples to implement them: https://packaging.python.org/guides/creating-and-discovering-plugins/. My opinion is to implement the option that is easier for the plugin developer to facilitate adoption.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:134,usability,guid,guides,134,This page summarizes the approaches mentioned by @flying-sheep together with examples to implement them: https://packaging.python.org/guides/creating-and-discovering-plugins/. My opinion is to implement the option that is easier for the plugin developer to facilitate adoption.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:154,usability,discov,discovering-plugins,154,This page summarizes the approaches mentioned by @flying-sheep together with examples to implement them: https://packaging.python.org/guides/creating-and-discovering-plugins/. My opinion is to implement the option that is easier for the plugin developer to facilitate adoption.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:119,integrability,discover,discovering-plugins,119,Thank you! It talks about option 2 there: [#using-namespace-packages](https://packaging.python.org/guides/creating-and-discovering-plugins/#using-namespace-packages) and Option 1 here: [#using-package-metadata](https://packaging.python.org/guides/creating-and-discovering-plugins/#using-package-metadata). The amount of work for plugin devs is completely covered in the first comment: one line added to `setup.py` each.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:260,integrability,discover,discovering-plugins,260,Thank you! It talks about option 2 there: [#using-namespace-packages](https://packaging.python.org/guides/creating-and-discovering-plugins/#using-namespace-packages) and Option 1 here: [#using-package-metadata](https://packaging.python.org/guides/creating-and-discovering-plugins/#using-package-metadata). The amount of work for plugin devs is completely covered in the first comment: one line added to `setup.py` each.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:119,interoperability,discover,discovering-plugins,119,Thank you! It talks about option 2 there: [#using-namespace-packages](https://packaging.python.org/guides/creating-and-discovering-plugins/#using-namespace-packages) and Option 1 here: [#using-package-metadata](https://packaging.python.org/guides/creating-and-discovering-plugins/#using-package-metadata). The amount of work for plugin devs is completely covered in the first comment: one line added to `setup.py` each.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:260,interoperability,discover,discovering-plugins,260,Thank you! It talks about option 2 there: [#using-namespace-packages](https://packaging.python.org/guides/creating-and-discovering-plugins/#using-namespace-packages) and Option 1 here: [#using-package-metadata](https://packaging.python.org/guides/creating-and-discovering-plugins/#using-package-metadata). The amount of work for plugin devs is completely covered in the first comment: one line added to `setup.py` each.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:329,interoperability,plug,plugin,329,Thank you! It talks about option 2 there: [#using-namespace-packages](https://packaging.python.org/guides/creating-and-discovering-plugins/#using-namespace-packages) and Option 1 here: [#using-package-metadata](https://packaging.python.org/guides/creating-and-discovering-plugins/#using-package-metadata). The amount of work for plugin devs is completely covered in the first comment: one line added to `setup.py` each.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:60,modifiability,pac,packages,60,Thank you! It talks about option 2 there: [#using-namespace-packages](https://packaging.python.org/guides/creating-and-discovering-plugins/#using-namespace-packages) and Option 1 here: [#using-package-metadata](https://packaging.python.org/guides/creating-and-discovering-plugins/#using-package-metadata). The amount of work for plugin devs is completely covered in the first comment: one line added to `setup.py` each.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:78,modifiability,pac,packaging,78,Thank you! It talks about option 2 there: [#using-namespace-packages](https://packaging.python.org/guides/creating-and-discovering-plugins/#using-namespace-packages) and Option 1 here: [#using-package-metadata](https://packaging.python.org/guides/creating-and-discovering-plugins/#using-package-metadata). The amount of work for plugin devs is completely covered in the first comment: one line added to `setup.py` each.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:156,modifiability,pac,packages,156,Thank you! It talks about option 2 there: [#using-namespace-packages](https://packaging.python.org/guides/creating-and-discovering-plugins/#using-namespace-packages) and Option 1 here: [#using-package-metadata](https://packaging.python.org/guides/creating-and-discovering-plugins/#using-package-metadata). The amount of work for plugin devs is completely covered in the first comment: one line added to `setup.py` each.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:193,modifiability,pac,package-metadata,193,Thank you! It talks about option 2 there: [#using-namespace-packages](https://packaging.python.org/guides/creating-and-discovering-plugins/#using-namespace-packages) and Option 1 here: [#using-package-metadata](https://packaging.python.org/guides/creating-and-discovering-plugins/#using-package-metadata). The amount of work for plugin devs is completely covered in the first comment: one line added to `setup.py` each.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:219,modifiability,pac,packaging,219,Thank you! It talks about option 2 there: [#using-namespace-packages](https://packaging.python.org/guides/creating-and-discovering-plugins/#using-namespace-packages) and Option 1 here: [#using-package-metadata](https://packaging.python.org/guides/creating-and-discovering-plugins/#using-package-metadata). The amount of work for plugin devs is completely covered in the first comment: one line added to `setup.py` each.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:287,modifiability,pac,package-metadata,287,Thank you! It talks about option 2 there: [#using-namespace-packages](https://packaging.python.org/guides/creating-and-discovering-plugins/#using-namespace-packages) and Option 1 here: [#using-package-metadata](https://packaging.python.org/guides/creating-and-discovering-plugins/#using-package-metadata). The amount of work for plugin devs is completely covered in the first comment: one line added to `setup.py` each.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:344,safety,compl,completely,344,Thank you! It talks about option 2 there: [#using-namespace-packages](https://packaging.python.org/guides/creating-and-discovering-plugins/#using-namespace-packages) and Option 1 here: [#using-package-metadata](https://packaging.python.org/guides/creating-and-discovering-plugins/#using-package-metadata). The amount of work for plugin devs is completely covered in the first comment: one line added to `setup.py` each.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:344,security,compl,completely,344,Thank you! It talks about option 2 there: [#using-namespace-packages](https://packaging.python.org/guides/creating-and-discovering-plugins/#using-namespace-packages) and Option 1 here: [#using-package-metadata](https://packaging.python.org/guides/creating-and-discovering-plugins/#using-package-metadata). The amount of work for plugin devs is completely covered in the first comment: one line added to `setup.py` each.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:99,usability,guid,guides,99,Thank you! It talks about option 2 there: [#using-namespace-packages](https://packaging.python.org/guides/creating-and-discovering-plugins/#using-namespace-packages) and Option 1 here: [#using-package-metadata](https://packaging.python.org/guides/creating-and-discovering-plugins/#using-package-metadata). The amount of work for plugin devs is completely covered in the first comment: one line added to `setup.py` each.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:119,usability,discov,discovering-plugins,119,Thank you! It talks about option 2 there: [#using-namespace-packages](https://packaging.python.org/guides/creating-and-discovering-plugins/#using-namespace-packages) and Option 1 here: [#using-package-metadata](https://packaging.python.org/guides/creating-and-discovering-plugins/#using-package-metadata). The amount of work for plugin devs is completely covered in the first comment: one line added to `setup.py` each.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:240,usability,guid,guides,240,Thank you! It talks about option 2 there: [#using-namespace-packages](https://packaging.python.org/guides/creating-and-discovering-plugins/#using-namespace-packages) and Option 1 here: [#using-package-metadata](https://packaging.python.org/guides/creating-and-discovering-plugins/#using-package-metadata). The amount of work for plugin devs is completely covered in the first comment: one line added to `setup.py` each.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:260,usability,discov,discovering-plugins,260,Thank you! It talks about option 2 there: [#using-namespace-packages](https://packaging.python.org/guides/creating-and-discovering-plugins/#using-namespace-packages) and Option 1 here: [#using-package-metadata](https://packaging.python.org/guides/creating-and-discovering-plugins/#using-package-metadata). The amount of work for plugin devs is completely covered in the first comment: one line added to `setup.py` each.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:379,deployability,api,api,379,"I think the current approach - a very simple interface as in `scanpy/tools/phate.py` and a bunch of others is the easiest way to go for the developer. So, I'd say we make a submodule `.ext` with the `.tools`, `.plotting`, `.preprocessing` substructure in it. We move things like `phate.py` into `scanpy/ext/tools`. We maintain backwards compat by still reexporting it in `scanpy.api`. The canonical way of calling these extension will be by importing `import scanpy.ext as sce` and people can use that extension namespace and call everything in the same way that they are used to. Users can look up extension tools on docs site like [this](https://scanpy.readthedocs.io/en/latest/api/index.html). It will also be clear to users that these extensions will require installing additional packages, which don't come with the default scanpy. Of course, all of this needs none of the ""extension mechanisms"" mentioned above. But people really don't want to write actual ""scanpy extensions""; they want to write their own packages and have them interface with scanpy so that convenient calls are enabled without the need to adapt to new conventions. For the scanpy users, the cool things is that a large number of tools can be quickly tested out. If you don't mind, @fidelram and @flying-sheep, @Koncopd would go along and make this modest change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:680,deployability,api,api,680,"I think the current approach - a very simple interface as in `scanpy/tools/phate.py` and a bunch of others is the easiest way to go for the developer. So, I'd say we make a submodule `.ext` with the `.tools`, `.plotting`, `.preprocessing` substructure in it. We move things like `phate.py` into `scanpy/ext/tools`. We maintain backwards compat by still reexporting it in `scanpy.api`. The canonical way of calling these extension will be by importing `import scanpy.ext as sce` and people can use that extension namespace and call everything in the same way that they are used to. Users can look up extension tools on docs site like [this](https://scanpy.readthedocs.io/en/latest/api/index.html). It will also be clear to users that these extensions will require installing additional packages, which don't come with the default scanpy. Of course, all of this needs none of the ""extension mechanisms"" mentioned above. But people really don't want to write actual ""scanpy extensions""; they want to write their own packages and have them interface with scanpy so that convenient calls are enabled without the need to adapt to new conventions. For the scanpy users, the cool things is that a large number of tools can be quickly tested out. If you don't mind, @fidelram and @flying-sheep, @Koncopd would go along and make this modest change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:763,deployability,instal,installing,763,"I think the current approach - a very simple interface as in `scanpy/tools/phate.py` and a bunch of others is the easiest way to go for the developer. So, I'd say we make a submodule `.ext` with the `.tools`, `.plotting`, `.preprocessing` substructure in it. We move things like `phate.py` into `scanpy/ext/tools`. We maintain backwards compat by still reexporting it in `scanpy.api`. The canonical way of calling these extension will be by importing `import scanpy.ext as sce` and people can use that extension namespace and call everything in the same way that they are used to. Users can look up extension tools on docs site like [this](https://scanpy.readthedocs.io/en/latest/api/index.html). It will also be clear to users that these extensions will require installing additional packages, which don't come with the default scanpy. Of course, all of this needs none of the ""extension mechanisms"" mentioned above. But people really don't want to write actual ""scanpy extensions""; they want to write their own packages and have them interface with scanpy so that convenient calls are enabled without the need to adapt to new conventions. For the scanpy users, the cool things is that a large number of tools can be quickly tested out. If you don't mind, @fidelram and @flying-sheep, @Koncopd would go along and make this modest change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:12,energy efficiency,current,current,12,"I think the current approach - a very simple interface as in `scanpy/tools/phate.py` and a bunch of others is the easiest way to go for the developer. So, I'd say we make a submodule `.ext` with the `.tools`, `.plotting`, `.preprocessing` substructure in it. We move things like `phate.py` into `scanpy/ext/tools`. We maintain backwards compat by still reexporting it in `scanpy.api`. The canonical way of calling these extension will be by importing `import scanpy.ext as sce` and people can use that extension namespace and call everything in the same way that they are used to. Users can look up extension tools on docs site like [this](https://scanpy.readthedocs.io/en/latest/api/index.html). It will also be clear to users that these extensions will require installing additional packages, which don't come with the default scanpy. Of course, all of this needs none of the ""extension mechanisms"" mentioned above. But people really don't want to write actual ""scanpy extensions""; they want to write their own packages and have them interface with scanpy so that convenient calls are enabled without the need to adapt to new conventions. For the scanpy users, the cool things is that a large number of tools can be quickly tested out. If you don't mind, @fidelram and @flying-sheep, @Koncopd would go along and make this modest change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:1115,energy efficiency,adapt,adapt,1115,"I think the current approach - a very simple interface as in `scanpy/tools/phate.py` and a bunch of others is the easiest way to go for the developer. So, I'd say we make a submodule `.ext` with the `.tools`, `.plotting`, `.preprocessing` substructure in it. We move things like `phate.py` into `scanpy/ext/tools`. We maintain backwards compat by still reexporting it in `scanpy.api`. The canonical way of calling these extension will be by importing `import scanpy.ext as sce` and people can use that extension namespace and call everything in the same way that they are used to. Users can look up extension tools on docs site like [this](https://scanpy.readthedocs.io/en/latest/api/index.html). It will also be clear to users that these extensions will require installing additional packages, which don't come with the default scanpy. Of course, all of this needs none of the ""extension mechanisms"" mentioned above. But people really don't want to write actual ""scanpy extensions""; they want to write their own packages and have them interface with scanpy so that convenient calls are enabled without the need to adapt to new conventions. For the scanpy users, the cool things is that a large number of tools can be quickly tested out. If you don't mind, @fidelram and @flying-sheep, @Koncopd would go along and make this modest change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:1167,energy efficiency,cool,cool,1167,"I think the current approach - a very simple interface as in `scanpy/tools/phate.py` and a bunch of others is the easiest way to go for the developer. So, I'd say we make a submodule `.ext` with the `.tools`, `.plotting`, `.preprocessing` substructure in it. We move things like `phate.py` into `scanpy/ext/tools`. We maintain backwards compat by still reexporting it in `scanpy.api`. The canonical way of calling these extension will be by importing `import scanpy.ext as sce` and people can use that extension namespace and call everything in the same way that they are used to. Users can look up extension tools on docs site like [this](https://scanpy.readthedocs.io/en/latest/api/index.html). It will also be clear to users that these extensions will require installing additional packages, which don't come with the default scanpy. Of course, all of this needs none of the ""extension mechanisms"" mentioned above. But people really don't want to write actual ""scanpy extensions""; they want to write their own packages and have them interface with scanpy so that convenient calls are enabled without the need to adapt to new conventions. For the scanpy users, the cool things is that a large number of tools can be quickly tested out. If you don't mind, @fidelram and @flying-sheep, @Koncopd would go along and make this modest change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:45,integrability,interfac,interface,45,"I think the current approach - a very simple interface as in `scanpy/tools/phate.py` and a bunch of others is the easiest way to go for the developer. So, I'd say we make a submodule `.ext` with the `.tools`, `.plotting`, `.preprocessing` substructure in it. We move things like `phate.py` into `scanpy/ext/tools`. We maintain backwards compat by still reexporting it in `scanpy.api`. The canonical way of calling these extension will be by importing `import scanpy.ext as sce` and people can use that extension namespace and call everything in the same way that they are used to. Users can look up extension tools on docs site like [this](https://scanpy.readthedocs.io/en/latest/api/index.html). It will also be clear to users that these extensions will require installing additional packages, which don't come with the default scanpy. Of course, all of this needs none of the ""extension mechanisms"" mentioned above. But people really don't want to write actual ""scanpy extensions""; they want to write their own packages and have them interface with scanpy so that convenient calls are enabled without the need to adapt to new conventions. For the scanpy users, the cool things is that a large number of tools can be quickly tested out. If you don't mind, @fidelram and @flying-sheep, @Koncopd would go along and make this modest change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:173,integrability,sub,submodule,173,"I think the current approach - a very simple interface as in `scanpy/tools/phate.py` and a bunch of others is the easiest way to go for the developer. So, I'd say we make a submodule `.ext` with the `.tools`, `.plotting`, `.preprocessing` substructure in it. We move things like `phate.py` into `scanpy/ext/tools`. We maintain backwards compat by still reexporting it in `scanpy.api`. The canonical way of calling these extension will be by importing `import scanpy.ext as sce` and people can use that extension namespace and call everything in the same way that they are used to. Users can look up extension tools on docs site like [this](https://scanpy.readthedocs.io/en/latest/api/index.html). It will also be clear to users that these extensions will require installing additional packages, which don't come with the default scanpy. Of course, all of this needs none of the ""extension mechanisms"" mentioned above. But people really don't want to write actual ""scanpy extensions""; they want to write their own packages and have them interface with scanpy so that convenient calls are enabled without the need to adapt to new conventions. For the scanpy users, the cool things is that a large number of tools can be quickly tested out. If you don't mind, @fidelram and @flying-sheep, @Koncopd would go along and make this modest change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:239,integrability,sub,substructure,239,"I think the current approach - a very simple interface as in `scanpy/tools/phate.py` and a bunch of others is the easiest way to go for the developer. So, I'd say we make a submodule `.ext` with the `.tools`, `.plotting`, `.preprocessing` substructure in it. We move things like `phate.py` into `scanpy/ext/tools`. We maintain backwards compat by still reexporting it in `scanpy.api`. The canonical way of calling these extension will be by importing `import scanpy.ext as sce` and people can use that extension namespace and call everything in the same way that they are used to. Users can look up extension tools on docs site like [this](https://scanpy.readthedocs.io/en/latest/api/index.html). It will also be clear to users that these extensions will require installing additional packages, which don't come with the default scanpy. Of course, all of this needs none of the ""extension mechanisms"" mentioned above. But people really don't want to write actual ""scanpy extensions""; they want to write their own packages and have them interface with scanpy so that convenient calls are enabled without the need to adapt to new conventions. For the scanpy users, the cool things is that a large number of tools can be quickly tested out. If you don't mind, @fidelram and @flying-sheep, @Koncopd would go along and make this modest change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:379,integrability,api,api,379,"I think the current approach - a very simple interface as in `scanpy/tools/phate.py` and a bunch of others is the easiest way to go for the developer. So, I'd say we make a submodule `.ext` with the `.tools`, `.plotting`, `.preprocessing` substructure in it. We move things like `phate.py` into `scanpy/ext/tools`. We maintain backwards compat by still reexporting it in `scanpy.api`. The canonical way of calling these extension will be by importing `import scanpy.ext as sce` and people can use that extension namespace and call everything in the same way that they are used to. Users can look up extension tools on docs site like [this](https://scanpy.readthedocs.io/en/latest/api/index.html). It will also be clear to users that these extensions will require installing additional packages, which don't come with the default scanpy. Of course, all of this needs none of the ""extension mechanisms"" mentioned above. But people really don't want to write actual ""scanpy extensions""; they want to write their own packages and have them interface with scanpy so that convenient calls are enabled without the need to adapt to new conventions. For the scanpy users, the cool things is that a large number of tools can be quickly tested out. If you don't mind, @fidelram and @flying-sheep, @Koncopd would go along and make this modest change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:680,integrability,api,api,680,"I think the current approach - a very simple interface as in `scanpy/tools/phate.py` and a bunch of others is the easiest way to go for the developer. So, I'd say we make a submodule `.ext` with the `.tools`, `.plotting`, `.preprocessing` substructure in it. We move things like `phate.py` into `scanpy/ext/tools`. We maintain backwards compat by still reexporting it in `scanpy.api`. The canonical way of calling these extension will be by importing `import scanpy.ext as sce` and people can use that extension namespace and call everything in the same way that they are used to. Users can look up extension tools on docs site like [this](https://scanpy.readthedocs.io/en/latest/api/index.html). It will also be clear to users that these extensions will require installing additional packages, which don't come with the default scanpy. Of course, all of this needs none of the ""extension mechanisms"" mentioned above. But people really don't want to write actual ""scanpy extensions""; they want to write their own packages and have them interface with scanpy so that convenient calls are enabled without the need to adapt to new conventions. For the scanpy users, the cool things is that a large number of tools can be quickly tested out. If you don't mind, @fidelram and @flying-sheep, @Koncopd would go along and make this modest change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:1036,integrability,interfac,interface,1036,"I think the current approach - a very simple interface as in `scanpy/tools/phate.py` and a bunch of others is the easiest way to go for the developer. So, I'd say we make a submodule `.ext` with the `.tools`, `.plotting`, `.preprocessing` substructure in it. We move things like `phate.py` into `scanpy/ext/tools`. We maintain backwards compat by still reexporting it in `scanpy.api`. The canonical way of calling these extension will be by importing `import scanpy.ext as sce` and people can use that extension namespace and call everything in the same way that they are used to. Users can look up extension tools on docs site like [this](https://scanpy.readthedocs.io/en/latest/api/index.html). It will also be clear to users that these extensions will require installing additional packages, which don't come with the default scanpy. Of course, all of this needs none of the ""extension mechanisms"" mentioned above. But people really don't want to write actual ""scanpy extensions""; they want to write their own packages and have them interface with scanpy so that convenient calls are enabled without the need to adapt to new conventions. For the scanpy users, the cool things is that a large number of tools can be quickly tested out. If you don't mind, @fidelram and @flying-sheep, @Koncopd would go along and make this modest change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:1115,integrability,adapt,adapt,1115,"I think the current approach - a very simple interface as in `scanpy/tools/phate.py` and a bunch of others is the easiest way to go for the developer. So, I'd say we make a submodule `.ext` with the `.tools`, `.plotting`, `.preprocessing` substructure in it. We move things like `phate.py` into `scanpy/ext/tools`. We maintain backwards compat by still reexporting it in `scanpy.api`. The canonical way of calling these extension will be by importing `import scanpy.ext as sce` and people can use that extension namespace and call everything in the same way that they are used to. Users can look up extension tools on docs site like [this](https://scanpy.readthedocs.io/en/latest/api/index.html). It will also be clear to users that these extensions will require installing additional packages, which don't come with the default scanpy. Of course, all of this needs none of the ""extension mechanisms"" mentioned above. But people really don't want to write actual ""scanpy extensions""; they want to write their own packages and have them interface with scanpy so that convenient calls are enabled without the need to adapt to new conventions. For the scanpy users, the cool things is that a large number of tools can be quickly tested out. If you don't mind, @fidelram and @flying-sheep, @Koncopd would go along and make this modest change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:45,interoperability,interfac,interface,45,"I think the current approach - a very simple interface as in `scanpy/tools/phate.py` and a bunch of others is the easiest way to go for the developer. So, I'd say we make a submodule `.ext` with the `.tools`, `.plotting`, `.preprocessing` substructure in it. We move things like `phate.py` into `scanpy/ext/tools`. We maintain backwards compat by still reexporting it in `scanpy.api`. The canonical way of calling these extension will be by importing `import scanpy.ext as sce` and people can use that extension namespace and call everything in the same way that they are used to. Users can look up extension tools on docs site like [this](https://scanpy.readthedocs.io/en/latest/api/index.html). It will also be clear to users that these extensions will require installing additional packages, which don't come with the default scanpy. Of course, all of this needs none of the ""extension mechanisms"" mentioned above. But people really don't want to write actual ""scanpy extensions""; they want to write their own packages and have them interface with scanpy so that convenient calls are enabled without the need to adapt to new conventions. For the scanpy users, the cool things is that a large number of tools can be quickly tested out. If you don't mind, @fidelram and @flying-sheep, @Koncopd would go along and make this modest change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:379,interoperability,api,api,379,"I think the current approach - a very simple interface as in `scanpy/tools/phate.py` and a bunch of others is the easiest way to go for the developer. So, I'd say we make a submodule `.ext` with the `.tools`, `.plotting`, `.preprocessing` substructure in it. We move things like `phate.py` into `scanpy/ext/tools`. We maintain backwards compat by still reexporting it in `scanpy.api`. The canonical way of calling these extension will be by importing `import scanpy.ext as sce` and people can use that extension namespace and call everything in the same way that they are used to. Users can look up extension tools on docs site like [this](https://scanpy.readthedocs.io/en/latest/api/index.html). It will also be clear to users that these extensions will require installing additional packages, which don't come with the default scanpy. Of course, all of this needs none of the ""extension mechanisms"" mentioned above. But people really don't want to write actual ""scanpy extensions""; they want to write their own packages and have them interface with scanpy so that convenient calls are enabled without the need to adapt to new conventions. For the scanpy users, the cool things is that a large number of tools can be quickly tested out. If you don't mind, @fidelram and @flying-sheep, @Koncopd would go along and make this modest change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:680,interoperability,api,api,680,"I think the current approach - a very simple interface as in `scanpy/tools/phate.py` and a bunch of others is the easiest way to go for the developer. So, I'd say we make a submodule `.ext` with the `.tools`, `.plotting`, `.preprocessing` substructure in it. We move things like `phate.py` into `scanpy/ext/tools`. We maintain backwards compat by still reexporting it in `scanpy.api`. The canonical way of calling these extension will be by importing `import scanpy.ext as sce` and people can use that extension namespace and call everything in the same way that they are used to. Users can look up extension tools on docs site like [this](https://scanpy.readthedocs.io/en/latest/api/index.html). It will also be clear to users that these extensions will require installing additional packages, which don't come with the default scanpy. Of course, all of this needs none of the ""extension mechanisms"" mentioned above. But people really don't want to write actual ""scanpy extensions""; they want to write their own packages and have them interface with scanpy so that convenient calls are enabled without the need to adapt to new conventions. For the scanpy users, the cool things is that a large number of tools can be quickly tested out. If you don't mind, @fidelram and @flying-sheep, @Koncopd would go along and make this modest change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:1036,interoperability,interfac,interface,1036,"I think the current approach - a very simple interface as in `scanpy/tools/phate.py` and a bunch of others is the easiest way to go for the developer. So, I'd say we make a submodule `.ext` with the `.tools`, `.plotting`, `.preprocessing` substructure in it. We move things like `phate.py` into `scanpy/ext/tools`. We maintain backwards compat by still reexporting it in `scanpy.api`. The canonical way of calling these extension will be by importing `import scanpy.ext as sce` and people can use that extension namespace and call everything in the same way that they are used to. Users can look up extension tools on docs site like [this](https://scanpy.readthedocs.io/en/latest/api/index.html). It will also be clear to users that these extensions will require installing additional packages, which don't come with the default scanpy. Of course, all of this needs none of the ""extension mechanisms"" mentioned above. But people really don't want to write actual ""scanpy extensions""; they want to write their own packages and have them interface with scanpy so that convenient calls are enabled without the need to adapt to new conventions. For the scanpy users, the cool things is that a large number of tools can be quickly tested out. If you don't mind, @fidelram and @flying-sheep, @Koncopd would go along and make this modest change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:1115,interoperability,adapt,adapt,1115,"I think the current approach - a very simple interface as in `scanpy/tools/phate.py` and a bunch of others is the easiest way to go for the developer. So, I'd say we make a submodule `.ext` with the `.tools`, `.plotting`, `.preprocessing` substructure in it. We move things like `phate.py` into `scanpy/ext/tools`. We maintain backwards compat by still reexporting it in `scanpy.api`. The canonical way of calling these extension will be by importing `import scanpy.ext as sce` and people can use that extension namespace and call everything in the same way that they are used to. Users can look up extension tools on docs site like [this](https://scanpy.readthedocs.io/en/latest/api/index.html). It will also be clear to users that these extensions will require installing additional packages, which don't come with the default scanpy. Of course, all of this needs none of the ""extension mechanisms"" mentioned above. But people really don't want to write actual ""scanpy extensions""; they want to write their own packages and have them interface with scanpy so that convenient calls are enabled without the need to adapt to new conventions. For the scanpy users, the cool things is that a large number of tools can be quickly tested out. If you don't mind, @fidelram and @flying-sheep, @Koncopd would go along and make this modest change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:45,modifiability,interfac,interface,45,"I think the current approach - a very simple interface as in `scanpy/tools/phate.py` and a bunch of others is the easiest way to go for the developer. So, I'd say we make a submodule `.ext` with the `.tools`, `.plotting`, `.preprocessing` substructure in it. We move things like `phate.py` into `scanpy/ext/tools`. We maintain backwards compat by still reexporting it in `scanpy.api`. The canonical way of calling these extension will be by importing `import scanpy.ext as sce` and people can use that extension namespace and call everything in the same way that they are used to. Users can look up extension tools on docs site like [this](https://scanpy.readthedocs.io/en/latest/api/index.html). It will also be clear to users that these extensions will require installing additional packages, which don't come with the default scanpy. Of course, all of this needs none of the ""extension mechanisms"" mentioned above. But people really don't want to write actual ""scanpy extensions""; they want to write their own packages and have them interface with scanpy so that convenient calls are enabled without the need to adapt to new conventions. For the scanpy users, the cool things is that a large number of tools can be quickly tested out. If you don't mind, @fidelram and @flying-sheep, @Koncopd would go along and make this modest change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:318,modifiability,maintain,maintain,318,"I think the current approach - a very simple interface as in `scanpy/tools/phate.py` and a bunch of others is the easiest way to go for the developer. So, I'd say we make a submodule `.ext` with the `.tools`, `.plotting`, `.preprocessing` substructure in it. We move things like `phate.py` into `scanpy/ext/tools`. We maintain backwards compat by still reexporting it in `scanpy.api`. The canonical way of calling these extension will be by importing `import scanpy.ext as sce` and people can use that extension namespace and call everything in the same way that they are used to. Users can look up extension tools on docs site like [this](https://scanpy.readthedocs.io/en/latest/api/index.html). It will also be clear to users that these extensions will require installing additional packages, which don't come with the default scanpy. Of course, all of this needs none of the ""extension mechanisms"" mentioned above. But people really don't want to write actual ""scanpy extensions""; they want to write their own packages and have them interface with scanpy so that convenient calls are enabled without the need to adapt to new conventions. For the scanpy users, the cool things is that a large number of tools can be quickly tested out. If you don't mind, @fidelram and @flying-sheep, @Koncopd would go along and make this modest change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:420,modifiability,extens,extension,420,"I think the current approach - a very simple interface as in `scanpy/tools/phate.py` and a bunch of others is the easiest way to go for the developer. So, I'd say we make a submodule `.ext` with the `.tools`, `.plotting`, `.preprocessing` substructure in it. We move things like `phate.py` into `scanpy/ext/tools`. We maintain backwards compat by still reexporting it in `scanpy.api`. The canonical way of calling these extension will be by importing `import scanpy.ext as sce` and people can use that extension namespace and call everything in the same way that they are used to. Users can look up extension tools on docs site like [this](https://scanpy.readthedocs.io/en/latest/api/index.html). It will also be clear to users that these extensions will require installing additional packages, which don't come with the default scanpy. Of course, all of this needs none of the ""extension mechanisms"" mentioned above. But people really don't want to write actual ""scanpy extensions""; they want to write their own packages and have them interface with scanpy so that convenient calls are enabled without the need to adapt to new conventions. For the scanpy users, the cool things is that a large number of tools can be quickly tested out. If you don't mind, @fidelram and @flying-sheep, @Koncopd would go along and make this modest change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:502,modifiability,extens,extension,502,"I think the current approach - a very simple interface as in `scanpy/tools/phate.py` and a bunch of others is the easiest way to go for the developer. So, I'd say we make a submodule `.ext` with the `.tools`, `.plotting`, `.preprocessing` substructure in it. We move things like `phate.py` into `scanpy/ext/tools`. We maintain backwards compat by still reexporting it in `scanpy.api`. The canonical way of calling these extension will be by importing `import scanpy.ext as sce` and people can use that extension namespace and call everything in the same way that they are used to. Users can look up extension tools on docs site like [this](https://scanpy.readthedocs.io/en/latest/api/index.html). It will also be clear to users that these extensions will require installing additional packages, which don't come with the default scanpy. Of course, all of this needs none of the ""extension mechanisms"" mentioned above. But people really don't want to write actual ""scanpy extensions""; they want to write their own packages and have them interface with scanpy so that convenient calls are enabled without the need to adapt to new conventions. For the scanpy users, the cool things is that a large number of tools can be quickly tested out. If you don't mind, @fidelram and @flying-sheep, @Koncopd would go along and make this modest change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:599,modifiability,extens,extension,599,"I think the current approach - a very simple interface as in `scanpy/tools/phate.py` and a bunch of others is the easiest way to go for the developer. So, I'd say we make a submodule `.ext` with the `.tools`, `.plotting`, `.preprocessing` substructure in it. We move things like `phate.py` into `scanpy/ext/tools`. We maintain backwards compat by still reexporting it in `scanpy.api`. The canonical way of calling these extension will be by importing `import scanpy.ext as sce` and people can use that extension namespace and call everything in the same way that they are used to. Users can look up extension tools on docs site like [this](https://scanpy.readthedocs.io/en/latest/api/index.html). It will also be clear to users that these extensions will require installing additional packages, which don't come with the default scanpy. Of course, all of this needs none of the ""extension mechanisms"" mentioned above. But people really don't want to write actual ""scanpy extensions""; they want to write their own packages and have them interface with scanpy so that convenient calls are enabled without the need to adapt to new conventions. For the scanpy users, the cool things is that a large number of tools can be quickly tested out. If you don't mind, @fidelram and @flying-sheep, @Koncopd would go along and make this modest change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:739,modifiability,extens,extensions,739,"I think the current approach - a very simple interface as in `scanpy/tools/phate.py` and a bunch of others is the easiest way to go for the developer. So, I'd say we make a submodule `.ext` with the `.tools`, `.plotting`, `.preprocessing` substructure in it. We move things like `phate.py` into `scanpy/ext/tools`. We maintain backwards compat by still reexporting it in `scanpy.api`. The canonical way of calling these extension will be by importing `import scanpy.ext as sce` and people can use that extension namespace and call everything in the same way that they are used to. Users can look up extension tools on docs site like [this](https://scanpy.readthedocs.io/en/latest/api/index.html). It will also be clear to users that these extensions will require installing additional packages, which don't come with the default scanpy. Of course, all of this needs none of the ""extension mechanisms"" mentioned above. But people really don't want to write actual ""scanpy extensions""; they want to write their own packages and have them interface with scanpy so that convenient calls are enabled without the need to adapt to new conventions. For the scanpy users, the cool things is that a large number of tools can be quickly tested out. If you don't mind, @fidelram and @flying-sheep, @Koncopd would go along and make this modest change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:785,modifiability,pac,packages,785,"I think the current approach - a very simple interface as in `scanpy/tools/phate.py` and a bunch of others is the easiest way to go for the developer. So, I'd say we make a submodule `.ext` with the `.tools`, `.plotting`, `.preprocessing` substructure in it. We move things like `phate.py` into `scanpy/ext/tools`. We maintain backwards compat by still reexporting it in `scanpy.api`. The canonical way of calling these extension will be by importing `import scanpy.ext as sce` and people can use that extension namespace and call everything in the same way that they are used to. Users can look up extension tools on docs site like [this](https://scanpy.readthedocs.io/en/latest/api/index.html). It will also be clear to users that these extensions will require installing additional packages, which don't come with the default scanpy. Of course, all of this needs none of the ""extension mechanisms"" mentioned above. But people really don't want to write actual ""scanpy extensions""; they want to write their own packages and have them interface with scanpy so that convenient calls are enabled without the need to adapt to new conventions. For the scanpy users, the cool things is that a large number of tools can be quickly tested out. If you don't mind, @fidelram and @flying-sheep, @Koncopd would go along and make this modest change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:879,modifiability,extens,extension,879,"I think the current approach - a very simple interface as in `scanpy/tools/phate.py` and a bunch of others is the easiest way to go for the developer. So, I'd say we make a submodule `.ext` with the `.tools`, `.plotting`, `.preprocessing` substructure in it. We move things like `phate.py` into `scanpy/ext/tools`. We maintain backwards compat by still reexporting it in `scanpy.api`. The canonical way of calling these extension will be by importing `import scanpy.ext as sce` and people can use that extension namespace and call everything in the same way that they are used to. Users can look up extension tools on docs site like [this](https://scanpy.readthedocs.io/en/latest/api/index.html). It will also be clear to users that these extensions will require installing additional packages, which don't come with the default scanpy. Of course, all of this needs none of the ""extension mechanisms"" mentioned above. But people really don't want to write actual ""scanpy extensions""; they want to write their own packages and have them interface with scanpy so that convenient calls are enabled without the need to adapt to new conventions. For the scanpy users, the cool things is that a large number of tools can be quickly tested out. If you don't mind, @fidelram and @flying-sheep, @Koncopd would go along and make this modest change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:971,modifiability,extens,extensions,971,"I think the current approach - a very simple interface as in `scanpy/tools/phate.py` and a bunch of others is the easiest way to go for the developer. So, I'd say we make a submodule `.ext` with the `.tools`, `.plotting`, `.preprocessing` substructure in it. We move things like `phate.py` into `scanpy/ext/tools`. We maintain backwards compat by still reexporting it in `scanpy.api`. The canonical way of calling these extension will be by importing `import scanpy.ext as sce` and people can use that extension namespace and call everything in the same way that they are used to. Users can look up extension tools on docs site like [this](https://scanpy.readthedocs.io/en/latest/api/index.html). It will also be clear to users that these extensions will require installing additional packages, which don't come with the default scanpy. Of course, all of this needs none of the ""extension mechanisms"" mentioned above. But people really don't want to write actual ""scanpy extensions""; they want to write their own packages and have them interface with scanpy so that convenient calls are enabled without the need to adapt to new conventions. For the scanpy users, the cool things is that a large number of tools can be quickly tested out. If you don't mind, @fidelram and @flying-sheep, @Koncopd would go along and make this modest change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:1013,modifiability,pac,packages,1013,"I think the current approach - a very simple interface as in `scanpy/tools/phate.py` and a bunch of others is the easiest way to go for the developer. So, I'd say we make a submodule `.ext` with the `.tools`, `.plotting`, `.preprocessing` substructure in it. We move things like `phate.py` into `scanpy/ext/tools`. We maintain backwards compat by still reexporting it in `scanpy.api`. The canonical way of calling these extension will be by importing `import scanpy.ext as sce` and people can use that extension namespace and call everything in the same way that they are used to. Users can look up extension tools on docs site like [this](https://scanpy.readthedocs.io/en/latest/api/index.html). It will also be clear to users that these extensions will require installing additional packages, which don't come with the default scanpy. Of course, all of this needs none of the ""extension mechanisms"" mentioned above. But people really don't want to write actual ""scanpy extensions""; they want to write their own packages and have them interface with scanpy so that convenient calls are enabled without the need to adapt to new conventions. For the scanpy users, the cool things is that a large number of tools can be quickly tested out. If you don't mind, @fidelram and @flying-sheep, @Koncopd would go along and make this modest change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:1036,modifiability,interfac,interface,1036,"I think the current approach - a very simple interface as in `scanpy/tools/phate.py` and a bunch of others is the easiest way to go for the developer. So, I'd say we make a submodule `.ext` with the `.tools`, `.plotting`, `.preprocessing` substructure in it. We move things like `phate.py` into `scanpy/ext/tools`. We maintain backwards compat by still reexporting it in `scanpy.api`. The canonical way of calling these extension will be by importing `import scanpy.ext as sce` and people can use that extension namespace and call everything in the same way that they are used to. Users can look up extension tools on docs site like [this](https://scanpy.readthedocs.io/en/latest/api/index.html). It will also be clear to users that these extensions will require installing additional packages, which don't come with the default scanpy. Of course, all of this needs none of the ""extension mechanisms"" mentioned above. But people really don't want to write actual ""scanpy extensions""; they want to write their own packages and have them interface with scanpy so that convenient calls are enabled without the need to adapt to new conventions. For the scanpy users, the cool things is that a large number of tools can be quickly tested out. If you don't mind, @fidelram and @flying-sheep, @Koncopd would go along and make this modest change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:1115,modifiability,adapt,adapt,1115,"I think the current approach - a very simple interface as in `scanpy/tools/phate.py` and a bunch of others is the easiest way to go for the developer. So, I'd say we make a submodule `.ext` with the `.tools`, `.plotting`, `.preprocessing` substructure in it. We move things like `phate.py` into `scanpy/ext/tools`. We maintain backwards compat by still reexporting it in `scanpy.api`. The canonical way of calling these extension will be by importing `import scanpy.ext as sce` and people can use that extension namespace and call everything in the same way that they are used to. Users can look up extension tools on docs site like [this](https://scanpy.readthedocs.io/en/latest/api/index.html). It will also be clear to users that these extensions will require installing additional packages, which don't come with the default scanpy. Of course, all of this needs none of the ""extension mechanisms"" mentioned above. But people really don't want to write actual ""scanpy extensions""; they want to write their own packages and have them interface with scanpy so that convenient calls are enabled without the need to adapt to new conventions. For the scanpy users, the cool things is that a large number of tools can be quickly tested out. If you don't mind, @fidelram and @flying-sheep, @Koncopd would go along and make this modest change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:318,safety,maintain,maintain,318,"I think the current approach - a very simple interface as in `scanpy/tools/phate.py` and a bunch of others is the easiest way to go for the developer. So, I'd say we make a submodule `.ext` with the `.tools`, `.plotting`, `.preprocessing` substructure in it. We move things like `phate.py` into `scanpy/ext/tools`. We maintain backwards compat by still reexporting it in `scanpy.api`. The canonical way of calling these extension will be by importing `import scanpy.ext as sce` and people can use that extension namespace and call everything in the same way that they are used to. Users can look up extension tools on docs site like [this](https://scanpy.readthedocs.io/en/latest/api/index.html). It will also be clear to users that these extensions will require installing additional packages, which don't come with the default scanpy. Of course, all of this needs none of the ""extension mechanisms"" mentioned above. But people really don't want to write actual ""scanpy extensions""; they want to write their own packages and have them interface with scanpy so that convenient calls are enabled without the need to adapt to new conventions. For the scanpy users, the cool things is that a large number of tools can be quickly tested out. If you don't mind, @fidelram and @flying-sheep, @Koncopd would go along and make this modest change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:1226,safety,test,tested,1226,"I think the current approach - a very simple interface as in `scanpy/tools/phate.py` and a bunch of others is the easiest way to go for the developer. So, I'd say we make a submodule `.ext` with the `.tools`, `.plotting`, `.preprocessing` substructure in it. We move things like `phate.py` into `scanpy/ext/tools`. We maintain backwards compat by still reexporting it in `scanpy.api`. The canonical way of calling these extension will be by importing `import scanpy.ext as sce` and people can use that extension namespace and call everything in the same way that they are used to. Users can look up extension tools on docs site like [this](https://scanpy.readthedocs.io/en/latest/api/index.html). It will also be clear to users that these extensions will require installing additional packages, which don't come with the default scanpy. Of course, all of this needs none of the ""extension mechanisms"" mentioned above. But people really don't want to write actual ""scanpy extensions""; they want to write their own packages and have them interface with scanpy so that convenient calls are enabled without the need to adapt to new conventions. For the scanpy users, the cool things is that a large number of tools can be quickly tested out. If you don't mind, @fidelram and @flying-sheep, @Koncopd would go along and make this modest change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:38,testability,simpl,simple,38,"I think the current approach - a very simple interface as in `scanpy/tools/phate.py` and a bunch of others is the easiest way to go for the developer. So, I'd say we make a submodule `.ext` with the `.tools`, `.plotting`, `.preprocessing` substructure in it. We move things like `phate.py` into `scanpy/ext/tools`. We maintain backwards compat by still reexporting it in `scanpy.api`. The canonical way of calling these extension will be by importing `import scanpy.ext as sce` and people can use that extension namespace and call everything in the same way that they are used to. Users can look up extension tools on docs site like [this](https://scanpy.readthedocs.io/en/latest/api/index.html). It will also be clear to users that these extensions will require installing additional packages, which don't come with the default scanpy. Of course, all of this needs none of the ""extension mechanisms"" mentioned above. But people really don't want to write actual ""scanpy extensions""; they want to write their own packages and have them interface with scanpy so that convenient calls are enabled without the need to adapt to new conventions. For the scanpy users, the cool things is that a large number of tools can be quickly tested out. If you don't mind, @fidelram and @flying-sheep, @Koncopd would go along and make this modest change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:1226,testability,test,tested,1226,"I think the current approach - a very simple interface as in `scanpy/tools/phate.py` and a bunch of others is the easiest way to go for the developer. So, I'd say we make a submodule `.ext` with the `.tools`, `.plotting`, `.preprocessing` substructure in it. We move things like `phate.py` into `scanpy/ext/tools`. We maintain backwards compat by still reexporting it in `scanpy.api`. The canonical way of calling these extension will be by importing `import scanpy.ext as sce` and people can use that extension namespace and call everything in the same way that they are used to. Users can look up extension tools on docs site like [this](https://scanpy.readthedocs.io/en/latest/api/index.html). It will also be clear to users that these extensions will require installing additional packages, which don't come with the default scanpy. Of course, all of this needs none of the ""extension mechanisms"" mentioned above. But people really don't want to write actual ""scanpy extensions""; they want to write their own packages and have them interface with scanpy so that convenient calls are enabled without the need to adapt to new conventions. For the scanpy users, the cool things is that a large number of tools can be quickly tested out. If you don't mind, @fidelram and @flying-sheep, @Koncopd would go along and make this modest change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:38,usability,simpl,simple,38,"I think the current approach - a very simple interface as in `scanpy/tools/phate.py` and a bunch of others is the easiest way to go for the developer. So, I'd say we make a submodule `.ext` with the `.tools`, `.plotting`, `.preprocessing` substructure in it. We move things like `phate.py` into `scanpy/ext/tools`. We maintain backwards compat by still reexporting it in `scanpy.api`. The canonical way of calling these extension will be by importing `import scanpy.ext as sce` and people can use that extension namespace and call everything in the same way that they are used to. Users can look up extension tools on docs site like [this](https://scanpy.readthedocs.io/en/latest/api/index.html). It will also be clear to users that these extensions will require installing additional packages, which don't come with the default scanpy. Of course, all of this needs none of the ""extension mechanisms"" mentioned above. But people really don't want to write actual ""scanpy extensions""; they want to write their own packages and have them interface with scanpy so that convenient calls are enabled without the need to adapt to new conventions. For the scanpy users, the cool things is that a large number of tools can be quickly tested out. If you don't mind, @fidelram and @flying-sheep, @Koncopd would go along and make this modest change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:69,usability,tool,tools,69,"I think the current approach - a very simple interface as in `scanpy/tools/phate.py` and a bunch of others is the easiest way to go for the developer. So, I'd say we make a submodule `.ext` with the `.tools`, `.plotting`, `.preprocessing` substructure in it. We move things like `phate.py` into `scanpy/ext/tools`. We maintain backwards compat by still reexporting it in `scanpy.api`. The canonical way of calling these extension will be by importing `import scanpy.ext as sce` and people can use that extension namespace and call everything in the same way that they are used to. Users can look up extension tools on docs site like [this](https://scanpy.readthedocs.io/en/latest/api/index.html). It will also be clear to users that these extensions will require installing additional packages, which don't come with the default scanpy. Of course, all of this needs none of the ""extension mechanisms"" mentioned above. But people really don't want to write actual ""scanpy extensions""; they want to write their own packages and have them interface with scanpy so that convenient calls are enabled without the need to adapt to new conventions. For the scanpy users, the cool things is that a large number of tools can be quickly tested out. If you don't mind, @fidelram and @flying-sheep, @Koncopd would go along and make this modest change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:201,usability,tool,tools,201,"I think the current approach - a very simple interface as in `scanpy/tools/phate.py` and a bunch of others is the easiest way to go for the developer. So, I'd say we make a submodule `.ext` with the `.tools`, `.plotting`, `.preprocessing` substructure in it. We move things like `phate.py` into `scanpy/ext/tools`. We maintain backwards compat by still reexporting it in `scanpy.api`. The canonical way of calling these extension will be by importing `import scanpy.ext as sce` and people can use that extension namespace and call everything in the same way that they are used to. Users can look up extension tools on docs site like [this](https://scanpy.readthedocs.io/en/latest/api/index.html). It will also be clear to users that these extensions will require installing additional packages, which don't come with the default scanpy. Of course, all of this needs none of the ""extension mechanisms"" mentioned above. But people really don't want to write actual ""scanpy extensions""; they want to write their own packages and have them interface with scanpy so that convenient calls are enabled without the need to adapt to new conventions. For the scanpy users, the cool things is that a large number of tools can be quickly tested out. If you don't mind, @fidelram and @flying-sheep, @Koncopd would go along and make this modest change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:307,usability,tool,tools,307,"I think the current approach - a very simple interface as in `scanpy/tools/phate.py` and a bunch of others is the easiest way to go for the developer. So, I'd say we make a submodule `.ext` with the `.tools`, `.plotting`, `.preprocessing` substructure in it. We move things like `phate.py` into `scanpy/ext/tools`. We maintain backwards compat by still reexporting it in `scanpy.api`. The canonical way of calling these extension will be by importing `import scanpy.ext as sce` and people can use that extension namespace and call everything in the same way that they are used to. Users can look up extension tools on docs site like [this](https://scanpy.readthedocs.io/en/latest/api/index.html). It will also be clear to users that these extensions will require installing additional packages, which don't come with the default scanpy. Of course, all of this needs none of the ""extension mechanisms"" mentioned above. But people really don't want to write actual ""scanpy extensions""; they want to write their own packages and have them interface with scanpy so that convenient calls are enabled without the need to adapt to new conventions. For the scanpy users, the cool things is that a large number of tools can be quickly tested out. If you don't mind, @fidelram and @flying-sheep, @Koncopd would go along and make this modest change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:581,usability,User,Users,581,"I think the current approach - a very simple interface as in `scanpy/tools/phate.py` and a bunch of others is the easiest way to go for the developer. So, I'd say we make a submodule `.ext` with the `.tools`, `.plotting`, `.preprocessing` substructure in it. We move things like `phate.py` into `scanpy/ext/tools`. We maintain backwards compat by still reexporting it in `scanpy.api`. The canonical way of calling these extension will be by importing `import scanpy.ext as sce` and people can use that extension namespace and call everything in the same way that they are used to. Users can look up extension tools on docs site like [this](https://scanpy.readthedocs.io/en/latest/api/index.html). It will also be clear to users that these extensions will require installing additional packages, which don't come with the default scanpy. Of course, all of this needs none of the ""extension mechanisms"" mentioned above. But people really don't want to write actual ""scanpy extensions""; they want to write their own packages and have them interface with scanpy so that convenient calls are enabled without the need to adapt to new conventions. For the scanpy users, the cool things is that a large number of tools can be quickly tested out. If you don't mind, @fidelram and @flying-sheep, @Koncopd would go along and make this modest change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:609,usability,tool,tools,609,"I think the current approach - a very simple interface as in `scanpy/tools/phate.py` and a bunch of others is the easiest way to go for the developer. So, I'd say we make a submodule `.ext` with the `.tools`, `.plotting`, `.preprocessing` substructure in it. We move things like `phate.py` into `scanpy/ext/tools`. We maintain backwards compat by still reexporting it in `scanpy.api`. The canonical way of calling these extension will be by importing `import scanpy.ext as sce` and people can use that extension namespace and call everything in the same way that they are used to. Users can look up extension tools on docs site like [this](https://scanpy.readthedocs.io/en/latest/api/index.html). It will also be clear to users that these extensions will require installing additional packages, which don't come with the default scanpy. Of course, all of this needs none of the ""extension mechanisms"" mentioned above. But people really don't want to write actual ""scanpy extensions""; they want to write their own packages and have them interface with scanpy so that convenient calls are enabled without the need to adapt to new conventions. For the scanpy users, the cool things is that a large number of tools can be quickly tested out. If you don't mind, @fidelram and @flying-sheep, @Koncopd would go along and make this modest change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:713,usability,clear,clear,713,"I think the current approach - a very simple interface as in `scanpy/tools/phate.py` and a bunch of others is the easiest way to go for the developer. So, I'd say we make a submodule `.ext` with the `.tools`, `.plotting`, `.preprocessing` substructure in it. We move things like `phate.py` into `scanpy/ext/tools`. We maintain backwards compat by still reexporting it in `scanpy.api`. The canonical way of calling these extension will be by importing `import scanpy.ext as sce` and people can use that extension namespace and call everything in the same way that they are used to. Users can look up extension tools on docs site like [this](https://scanpy.readthedocs.io/en/latest/api/index.html). It will also be clear to users that these extensions will require installing additional packages, which don't come with the default scanpy. Of course, all of this needs none of the ""extension mechanisms"" mentioned above. But people really don't want to write actual ""scanpy extensions""; they want to write their own packages and have them interface with scanpy so that convenient calls are enabled without the need to adapt to new conventions. For the scanpy users, the cool things is that a large number of tools can be quickly tested out. If you don't mind, @fidelram and @flying-sheep, @Koncopd would go along and make this modest change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:722,usability,user,users,722,"I think the current approach - a very simple interface as in `scanpy/tools/phate.py` and a bunch of others is the easiest way to go for the developer. So, I'd say we make a submodule `.ext` with the `.tools`, `.plotting`, `.preprocessing` substructure in it. We move things like `phate.py` into `scanpy/ext/tools`. We maintain backwards compat by still reexporting it in `scanpy.api`. The canonical way of calling these extension will be by importing `import scanpy.ext as sce` and people can use that extension namespace and call everything in the same way that they are used to. Users can look up extension tools on docs site like [this](https://scanpy.readthedocs.io/en/latest/api/index.html). It will also be clear to users that these extensions will require installing additional packages, which don't come with the default scanpy. Of course, all of this needs none of the ""extension mechanisms"" mentioned above. But people really don't want to write actual ""scanpy extensions""; they want to write their own packages and have them interface with scanpy so that convenient calls are enabled without the need to adapt to new conventions. For the scanpy users, the cool things is that a large number of tools can be quickly tested out. If you don't mind, @fidelram and @flying-sheep, @Koncopd would go along and make this modest change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:1156,usability,user,users,1156,"I think the current approach - a very simple interface as in `scanpy/tools/phate.py` and a bunch of others is the easiest way to go for the developer. So, I'd say we make a submodule `.ext` with the `.tools`, `.plotting`, `.preprocessing` substructure in it. We move things like `phate.py` into `scanpy/ext/tools`. We maintain backwards compat by still reexporting it in `scanpy.api`. The canonical way of calling these extension will be by importing `import scanpy.ext as sce` and people can use that extension namespace and call everything in the same way that they are used to. Users can look up extension tools on docs site like [this](https://scanpy.readthedocs.io/en/latest/api/index.html). It will also be clear to users that these extensions will require installing additional packages, which don't come with the default scanpy. Of course, all of this needs none of the ""extension mechanisms"" mentioned above. But people really don't want to write actual ""scanpy extensions""; they want to write their own packages and have them interface with scanpy so that convenient calls are enabled without the need to adapt to new conventions. For the scanpy users, the cool things is that a large number of tools can be quickly tested out. If you don't mind, @fidelram and @flying-sheep, @Koncopd would go along and make this modest change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:1205,usability,tool,tools,1205,"I think the current approach - a very simple interface as in `scanpy/tools/phate.py` and a bunch of others is the easiest way to go for the developer. So, I'd say we make a submodule `.ext` with the `.tools`, `.plotting`, `.preprocessing` substructure in it. We move things like `phate.py` into `scanpy/ext/tools`. We maintain backwards compat by still reexporting it in `scanpy.api`. The canonical way of calling these extension will be by importing `import scanpy.ext as sce` and people can use that extension namespace and call everything in the same way that they are used to. Users can look up extension tools on docs site like [this](https://scanpy.readthedocs.io/en/latest/api/index.html). It will also be clear to users that these extensions will require installing additional packages, which don't come with the default scanpy. Of course, all of this needs none of the ""extension mechanisms"" mentioned above. But people really don't want to write actual ""scanpy extensions""; they want to write their own packages and have them interface with scanpy so that convenient calls are enabled without the need to adapt to new conventions. For the scanpy users, the cool things is that a large number of tools can be quickly tested out. If you don't mind, @fidelram and @flying-sheep, @Koncopd would go along and make this modest change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:86,energy efficiency,cool,cool,86,"sounds good! but of course, having a way to find packages that extend scanpy would be cool. if we could search PyPI for a specific entry point type, and there was a scanpy entry point, that would make finding them a breeze. or we do a `awesome-scanpy` repo that collects tutorials, extensions, and so on!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:122,interoperability,specif,specific,122,"sounds good! but of course, having a way to find packages that extend scanpy would be cool. if we could search PyPI for a specific entry point type, and there was a scanpy entry point, that would make finding them a breeze. or we do a `awesome-scanpy` repo that collects tutorials, extensions, and so on!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:49,modifiability,pac,packages,49,"sounds good! but of course, having a way to find packages that extend scanpy would be cool. if we could search PyPI for a specific entry point type, and there was a scanpy entry point, that would make finding them a breeze. or we do a `awesome-scanpy` repo that collects tutorials, extensions, and so on!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:63,modifiability,exten,extend,63,"sounds good! but of course, having a way to find packages that extend scanpy would be cool. if we could search PyPI for a specific entry point type, and there was a scanpy entry point, that would make finding them a breeze. or we do a `awesome-scanpy` repo that collects tutorials, extensions, and so on!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:282,modifiability,extens,extensions,282,"sounds good! but of course, having a way to find packages that extend scanpy would be cool. if we could search PyPI for a specific entry point type, and there was a scanpy entry point, that would make finding them a breeze. or we do a `awesome-scanpy` repo that collects tutorials, extensions, and so on!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/271:6,energy efficiency,current,currently,6,"We’re currently including many things into `scanpy.external`, which is probably enough …",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/271
https://github.com/scverse/scanpy/issues/273:194,deployability,api,api,194,"The same issue happens when renaming categories with the built-in rename categories function. I think, this definitely should not happen. Example:. ```python. import pandas as pd. import scanpy.api as sc. adata = sc.datasets.blobs(640, 3). sc.tl.pca(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). method='logreg'. sc.tl.rank_genes_groups(adata, 'louvain', method=method). print(pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(1)). adata.rename_categories('louvain', ['Zero', 'One', 'Two']). sc.tl.rank_genes_groups(adata, 'louvain', method=method). print(pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(1)). ```. gives. ```python. 0 1 2. 0 570 63 126. Zero One Two. 0 63 126 570. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/273
https://github.com/scverse/scanpy/issues/273:312,deployability,log,logreg,312,"The same issue happens when renaming categories with the built-in rename categories function. I think, this definitely should not happen. Example:. ```python. import pandas as pd. import scanpy.api as sc. adata = sc.datasets.blobs(640, 3). sc.tl.pca(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). method='logreg'. sc.tl.rank_genes_groups(adata, 'louvain', method=method). print(pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(1)). adata.rename_categories('louvain', ['Zero', 'One', 'Two']). sc.tl.rank_genes_groups(adata, 'louvain', method=method). print(pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(1)). ```. gives. ```python. 0 1 2. 0 570 63 126. Zero One Two. 0 63 126 570. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/273
https://github.com/scverse/scanpy/issues/273:194,integrability,api,api,194,"The same issue happens when renaming categories with the built-in rename categories function. I think, this definitely should not happen. Example:. ```python. import pandas as pd. import scanpy.api as sc. adata = sc.datasets.blobs(640, 3). sc.tl.pca(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). method='logreg'. sc.tl.rank_genes_groups(adata, 'louvain', method=method). print(pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(1)). adata.rename_categories('louvain', ['Zero', 'One', 'Two']). sc.tl.rank_genes_groups(adata, 'louvain', method=method). print(pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(1)). ```. gives. ```python. 0 1 2. 0 570 63 126. Zero One Two. 0 63 126 570. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/273
https://github.com/scverse/scanpy/issues/273:194,interoperability,api,api,194,"The same issue happens when renaming categories with the built-in rename categories function. I think, this definitely should not happen. Example:. ```python. import pandas as pd. import scanpy.api as sc. adata = sc.datasets.blobs(640, 3). sc.tl.pca(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). method='logreg'. sc.tl.rank_genes_groups(adata, 'louvain', method=method). print(pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(1)). adata.rename_categories('louvain', ['Zero', 'One', 'Two']). sc.tl.rank_genes_groups(adata, 'louvain', method=method). print(pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(1)). ```. gives. ```python. 0 1 2. 0 570 63 126. Zero One Two. 0 63 126 570. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/273
https://github.com/scverse/scanpy/issues/273:312,safety,log,logreg,312,"The same issue happens when renaming categories with the built-in rename categories function. I think, this definitely should not happen. Example:. ```python. import pandas as pd. import scanpy.api as sc. adata = sc.datasets.blobs(640, 3). sc.tl.pca(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). method='logreg'. sc.tl.rank_genes_groups(adata, 'louvain', method=method). print(pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(1)). adata.rename_categories('louvain', ['Zero', 'One', 'Two']). sc.tl.rank_genes_groups(adata, 'louvain', method=method). print(pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(1)). ```. gives. ```python. 0 1 2. 0 570 63 126. Zero One Two. 0 63 126 570. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/273
https://github.com/scverse/scanpy/issues/273:312,security,log,logreg,312,"The same issue happens when renaming categories with the built-in rename categories function. I think, this definitely should not happen. Example:. ```python. import pandas as pd. import scanpy.api as sc. adata = sc.datasets.blobs(640, 3). sc.tl.pca(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). method='logreg'. sc.tl.rank_genes_groups(adata, 'louvain', method=method). print(pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(1)). adata.rename_categories('louvain', ['Zero', 'One', 'Two']). sc.tl.rank_genes_groups(adata, 'louvain', method=method). print(pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(1)). ```. gives. ```python. 0 1 2. 0 570 63 126. Zero One Two. 0 63 126 570. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/273
https://github.com/scverse/scanpy/issues/273:312,testability,log,logreg,312,"The same issue happens when renaming categories with the built-in rename categories function. I think, this definitely should not happen. Example:. ```python. import pandas as pd. import scanpy.api as sc. adata = sc.datasets.blobs(640, 3). sc.tl.pca(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). method='logreg'. sc.tl.rank_genes_groups(adata, 'louvain', method=method). print(pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(1)). adata.rename_categories('louvain', ['Zero', 'One', 'Two']). sc.tl.rank_genes_groups(adata, 'louvain', method=method). print(pd.DataFrame(adata.uns['rank_genes_groups']['names']).head(1)). ```. gives. ```python. 0 1 2. 0 570 63 126. Zero One Two. 0 63 126 570. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/273
https://github.com/scverse/scanpy/issues/273:197,usability,learn,learn,197,"Thank you very much for the super clean examples here! Is fixed via https://github.com/theislab/scanpy/commit/0ed304a64038f7d2c11b36fe5883ab9765ffba57 (@yugeji, you fed a pandas series into scikit learn, which is generally a bad idea :wink: ).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/273
https://github.com/scverse/scanpy/issues/275:58,deployability,version,version,58,"Thank you for pointing it out, it is fixed in the current version and will be updated online soon. You're right that it didn't have any effect on the tutorial otherwise.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/275
https://github.com/scverse/scanpy/issues/275:78,deployability,updat,updated,78,"Thank you for pointing it out, it is fixed in the current version and will be updated online soon. You're right that it didn't have any effect on the tutorial otherwise.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/275
https://github.com/scverse/scanpy/issues/275:50,energy efficiency,current,current,50,"Thank you for pointing it out, it is fixed in the current version and will be updated online soon. You're right that it didn't have any effect on the tutorial otherwise.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/275
https://github.com/scverse/scanpy/issues/275:58,integrability,version,version,58,"Thank you for pointing it out, it is fixed in the current version and will be updated online soon. You're right that it didn't have any effect on the tutorial otherwise.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/275
https://github.com/scverse/scanpy/issues/275:58,modifiability,version,version,58,"Thank you for pointing it out, it is fixed in the current version and will be updated online soon. You're right that it didn't have any effect on the tutorial otherwise.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/275
https://github.com/scverse/scanpy/issues/275:78,safety,updat,updated,78,"Thank you for pointing it out, it is fixed in the current version and will be updated online soon. You're right that it didn't have any effect on the tutorial otherwise.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/275
https://github.com/scverse/scanpy/issues/275:78,security,updat,updated,78,"Thank you for pointing it out, it is fixed in the current version and will be updated online soon. You're right that it didn't have any effect on the tutorial otherwise.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/275
https://github.com/scverse/scanpy/issues/277:781,availability,cluster,clustering,781,"From my understanding, Seurat uses the Louvain algorithm to maximize modularity on their KNN where edge weights are Jaccard similarity between the relevant nodes' neighbors. Scanpy is largely similar by default, though the nearest neighbors are found by UMAP's method, and all edge weights are 1. There's a boolean flag to use edge weights if you'd like (`use_weights`), which uses the weights from the adjacency matrix found in: `adata.uns[""neighbors""][""connectivities""]`. If you'd like to know more, I'd recommend looking at the docs for [`sc.pp.neighbors`](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.neighbors.html#scanpy.api.pp.neighbors), [`sc.tl.louvain`](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html#scanpy.api.tl.louvain), and the [clustering tutorials](https://scanpy.readthedocs.io/en/latest/examples.html#clustering).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/277
https://github.com/scverse/scanpy/issues/277:857,availability,cluster,clustering,857,"From my understanding, Seurat uses the Louvain algorithm to maximize modularity on their KNN where edge weights are Jaccard similarity between the relevant nodes' neighbors. Scanpy is largely similar by default, though the nearest neighbors are found by UMAP's method, and all edge weights are 1. There's a boolean flag to use edge weights if you'd like (`use_weights`), which uses the weights from the adjacency matrix found in: `adata.uns[""neighbors""][""connectivities""]`. If you'd like to know more, I'd recommend looking at the docs for [`sc.pp.neighbors`](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.neighbors.html#scanpy.api.pp.neighbors), [`sc.tl.louvain`](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html#scanpy.api.tl.louvain), and the [clustering tutorials](https://scanpy.readthedocs.io/en/latest/examples.html#clustering).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/277
https://github.com/scverse/scanpy/issues/277:69,deployability,modul,modularity,69,"From my understanding, Seurat uses the Louvain algorithm to maximize modularity on their KNN where edge weights are Jaccard similarity between the relevant nodes' neighbors. Scanpy is largely similar by default, though the nearest neighbors are found by UMAP's method, and all edge weights are 1. There's a boolean flag to use edge weights if you'd like (`use_weights`), which uses the weights from the adjacency matrix found in: `adata.uns[""neighbors""][""connectivities""]`. If you'd like to know more, I'd recommend looking at the docs for [`sc.pp.neighbors`](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.neighbors.html#scanpy.api.pp.neighbors), [`sc.tl.louvain`](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html#scanpy.api.tl.louvain), and the [clustering tutorials](https://scanpy.readthedocs.io/en/latest/examples.html#clustering).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/277
https://github.com/scverse/scanpy/issues/277:600,deployability,api,api,600,"From my understanding, Seurat uses the Louvain algorithm to maximize modularity on their KNN where edge weights are Jaccard similarity between the relevant nodes' neighbors. Scanpy is largely similar by default, though the nearest neighbors are found by UMAP's method, and all edge weights are 1. There's a boolean flag to use edge weights if you'd like (`use_weights`), which uses the weights from the adjacency matrix found in: `adata.uns[""neighbors""][""connectivities""]`. If you'd like to know more, I'd recommend looking at the docs for [`sc.pp.neighbors`](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.neighbors.html#scanpy.api.pp.neighbors), [`sc.tl.louvain`](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html#scanpy.api.tl.louvain), and the [clustering tutorials](https://scanpy.readthedocs.io/en/latest/examples.html#clustering).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/277
https://github.com/scverse/scanpy/issues/277:611,deployability,api,api,611,"From my understanding, Seurat uses the Louvain algorithm to maximize modularity on their KNN where edge weights are Jaccard similarity between the relevant nodes' neighbors. Scanpy is largely similar by default, though the nearest neighbors are found by UMAP's method, and all edge weights are 1. There's a boolean flag to use edge weights if you'd like (`use_weights`), which uses the weights from the adjacency matrix found in: `adata.uns[""neighbors""][""connectivities""]`. If you'd like to know more, I'd recommend looking at the docs for [`sc.pp.neighbors`](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.neighbors.html#scanpy.api.pp.neighbors), [`sc.tl.louvain`](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html#scanpy.api.tl.louvain), and the [clustering tutorials](https://scanpy.readthedocs.io/en/latest/examples.html#clustering).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/277
https://github.com/scverse/scanpy/issues/277:640,deployability,api,api,640,"From my understanding, Seurat uses the Louvain algorithm to maximize modularity on their KNN where edge weights are Jaccard similarity between the relevant nodes' neighbors. Scanpy is largely similar by default, though the nearest neighbors are found by UMAP's method, and all edge weights are 1. There's a boolean flag to use edge weights if you'd like (`use_weights`), which uses the weights from the adjacency matrix found in: `adata.uns[""neighbors""][""connectivities""]`. If you'd like to know more, I'd recommend looking at the docs for [`sc.pp.neighbors`](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.neighbors.html#scanpy.api.pp.neighbors), [`sc.tl.louvain`](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html#scanpy.api.tl.louvain), and the [clustering tutorials](https://scanpy.readthedocs.io/en/latest/examples.html#clustering).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/277
https://github.com/scverse/scanpy/issues/277:717,deployability,api,api,717,"From my understanding, Seurat uses the Louvain algorithm to maximize modularity on their KNN where edge weights are Jaccard similarity between the relevant nodes' neighbors. Scanpy is largely similar by default, though the nearest neighbors are found by UMAP's method, and all edge weights are 1. There's a boolean flag to use edge weights if you'd like (`use_weights`), which uses the weights from the adjacency matrix found in: `adata.uns[""neighbors""][""connectivities""]`. If you'd like to know more, I'd recommend looking at the docs for [`sc.pp.neighbors`](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.neighbors.html#scanpy.api.pp.neighbors), [`sc.tl.louvain`](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html#scanpy.api.tl.louvain), and the [clustering tutorials](https://scanpy.readthedocs.io/en/latest/examples.html#clustering).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/277
https://github.com/scverse/scanpy/issues/277:728,deployability,api,api,728,"From my understanding, Seurat uses the Louvain algorithm to maximize modularity on their KNN where edge weights are Jaccard similarity between the relevant nodes' neighbors. Scanpy is largely similar by default, though the nearest neighbors are found by UMAP's method, and all edge weights are 1. There's a boolean flag to use edge weights if you'd like (`use_weights`), which uses the weights from the adjacency matrix found in: `adata.uns[""neighbors""][""connectivities""]`. If you'd like to know more, I'd recommend looking at the docs for [`sc.pp.neighbors`](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.neighbors.html#scanpy.api.pp.neighbors), [`sc.tl.louvain`](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html#scanpy.api.tl.louvain), and the [clustering tutorials](https://scanpy.readthedocs.io/en/latest/examples.html#clustering).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/277
https://github.com/scverse/scanpy/issues/277:755,deployability,api,api,755,"From my understanding, Seurat uses the Louvain algorithm to maximize modularity on their KNN where edge weights are Jaccard similarity between the relevant nodes' neighbors. Scanpy is largely similar by default, though the nearest neighbors are found by UMAP's method, and all edge weights are 1. There's a boolean flag to use edge weights if you'd like (`use_weights`), which uses the weights from the adjacency matrix found in: `adata.uns[""neighbors""][""connectivities""]`. If you'd like to know more, I'd recommend looking at the docs for [`sc.pp.neighbors`](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.neighbors.html#scanpy.api.pp.neighbors), [`sc.tl.louvain`](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html#scanpy.api.tl.louvain), and the [clustering tutorials](https://scanpy.readthedocs.io/en/latest/examples.html#clustering).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/277
https://github.com/scverse/scanpy/issues/277:781,deployability,cluster,clustering,781,"From my understanding, Seurat uses the Louvain algorithm to maximize modularity on their KNN where edge weights are Jaccard similarity between the relevant nodes' neighbors. Scanpy is largely similar by default, though the nearest neighbors are found by UMAP's method, and all edge weights are 1. There's a boolean flag to use edge weights if you'd like (`use_weights`), which uses the weights from the adjacency matrix found in: `adata.uns[""neighbors""][""connectivities""]`. If you'd like to know more, I'd recommend looking at the docs for [`sc.pp.neighbors`](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.neighbors.html#scanpy.api.pp.neighbors), [`sc.tl.louvain`](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html#scanpy.api.tl.louvain), and the [clustering tutorials](https://scanpy.readthedocs.io/en/latest/examples.html#clustering).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/277
https://github.com/scverse/scanpy/issues/277:857,deployability,cluster,clustering,857,"From my understanding, Seurat uses the Louvain algorithm to maximize modularity on their KNN where edge weights are Jaccard similarity between the relevant nodes' neighbors. Scanpy is largely similar by default, though the nearest neighbors are found by UMAP's method, and all edge weights are 1. There's a boolean flag to use edge weights if you'd like (`use_weights`), which uses the weights from the adjacency matrix found in: `adata.uns[""neighbors""][""connectivities""]`. If you'd like to know more, I'd recommend looking at the docs for [`sc.pp.neighbors`](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.neighbors.html#scanpy.api.pp.neighbors), [`sc.tl.louvain`](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html#scanpy.api.tl.louvain), and the [clustering tutorials](https://scanpy.readthedocs.io/en/latest/examples.html#clustering).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/277
https://github.com/scverse/scanpy/issues/277:69,integrability,modular,modularity,69,"From my understanding, Seurat uses the Louvain algorithm to maximize modularity on their KNN where edge weights are Jaccard similarity between the relevant nodes' neighbors. Scanpy is largely similar by default, though the nearest neighbors are found by UMAP's method, and all edge weights are 1. There's a boolean flag to use edge weights if you'd like (`use_weights`), which uses the weights from the adjacency matrix found in: `adata.uns[""neighbors""][""connectivities""]`. If you'd like to know more, I'd recommend looking at the docs for [`sc.pp.neighbors`](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.neighbors.html#scanpy.api.pp.neighbors), [`sc.tl.louvain`](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html#scanpy.api.tl.louvain), and the [clustering tutorials](https://scanpy.readthedocs.io/en/latest/examples.html#clustering).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/277
https://github.com/scverse/scanpy/issues/277:600,integrability,api,api,600,"From my understanding, Seurat uses the Louvain algorithm to maximize modularity on their KNN where edge weights are Jaccard similarity between the relevant nodes' neighbors. Scanpy is largely similar by default, though the nearest neighbors are found by UMAP's method, and all edge weights are 1. There's a boolean flag to use edge weights if you'd like (`use_weights`), which uses the weights from the adjacency matrix found in: `adata.uns[""neighbors""][""connectivities""]`. If you'd like to know more, I'd recommend looking at the docs for [`sc.pp.neighbors`](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.neighbors.html#scanpy.api.pp.neighbors), [`sc.tl.louvain`](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html#scanpy.api.tl.louvain), and the [clustering tutorials](https://scanpy.readthedocs.io/en/latest/examples.html#clustering).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/277
https://github.com/scverse/scanpy/issues/277:611,integrability,api,api,611,"From my understanding, Seurat uses the Louvain algorithm to maximize modularity on their KNN where edge weights are Jaccard similarity between the relevant nodes' neighbors. Scanpy is largely similar by default, though the nearest neighbors are found by UMAP's method, and all edge weights are 1. There's a boolean flag to use edge weights if you'd like (`use_weights`), which uses the weights from the adjacency matrix found in: `adata.uns[""neighbors""][""connectivities""]`. If you'd like to know more, I'd recommend looking at the docs for [`sc.pp.neighbors`](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.neighbors.html#scanpy.api.pp.neighbors), [`sc.tl.louvain`](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html#scanpy.api.tl.louvain), and the [clustering tutorials](https://scanpy.readthedocs.io/en/latest/examples.html#clustering).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/277
https://github.com/scverse/scanpy/issues/277:640,integrability,api,api,640,"From my understanding, Seurat uses the Louvain algorithm to maximize modularity on their KNN where edge weights are Jaccard similarity between the relevant nodes' neighbors. Scanpy is largely similar by default, though the nearest neighbors are found by UMAP's method, and all edge weights are 1. There's a boolean flag to use edge weights if you'd like (`use_weights`), which uses the weights from the adjacency matrix found in: `adata.uns[""neighbors""][""connectivities""]`. If you'd like to know more, I'd recommend looking at the docs for [`sc.pp.neighbors`](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.neighbors.html#scanpy.api.pp.neighbors), [`sc.tl.louvain`](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html#scanpy.api.tl.louvain), and the [clustering tutorials](https://scanpy.readthedocs.io/en/latest/examples.html#clustering).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/277
https://github.com/scverse/scanpy/issues/277:717,integrability,api,api,717,"From my understanding, Seurat uses the Louvain algorithm to maximize modularity on their KNN where edge weights are Jaccard similarity between the relevant nodes' neighbors. Scanpy is largely similar by default, though the nearest neighbors are found by UMAP's method, and all edge weights are 1. There's a boolean flag to use edge weights if you'd like (`use_weights`), which uses the weights from the adjacency matrix found in: `adata.uns[""neighbors""][""connectivities""]`. If you'd like to know more, I'd recommend looking at the docs for [`sc.pp.neighbors`](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.neighbors.html#scanpy.api.pp.neighbors), [`sc.tl.louvain`](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html#scanpy.api.tl.louvain), and the [clustering tutorials](https://scanpy.readthedocs.io/en/latest/examples.html#clustering).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/277
https://github.com/scverse/scanpy/issues/277:728,integrability,api,api,728,"From my understanding, Seurat uses the Louvain algorithm to maximize modularity on their KNN where edge weights are Jaccard similarity between the relevant nodes' neighbors. Scanpy is largely similar by default, though the nearest neighbors are found by UMAP's method, and all edge weights are 1. There's a boolean flag to use edge weights if you'd like (`use_weights`), which uses the weights from the adjacency matrix found in: `adata.uns[""neighbors""][""connectivities""]`. If you'd like to know more, I'd recommend looking at the docs for [`sc.pp.neighbors`](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.neighbors.html#scanpy.api.pp.neighbors), [`sc.tl.louvain`](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html#scanpy.api.tl.louvain), and the [clustering tutorials](https://scanpy.readthedocs.io/en/latest/examples.html#clustering).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/277
https://github.com/scverse/scanpy/issues/277:755,integrability,api,api,755,"From my understanding, Seurat uses the Louvain algorithm to maximize modularity on their KNN where edge weights are Jaccard similarity between the relevant nodes' neighbors. Scanpy is largely similar by default, though the nearest neighbors are found by UMAP's method, and all edge weights are 1. There's a boolean flag to use edge weights if you'd like (`use_weights`), which uses the weights from the adjacency matrix found in: `adata.uns[""neighbors""][""connectivities""]`. If you'd like to know more, I'd recommend looking at the docs for [`sc.pp.neighbors`](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.neighbors.html#scanpy.api.pp.neighbors), [`sc.tl.louvain`](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html#scanpy.api.tl.louvain), and the [clustering tutorials](https://scanpy.readthedocs.io/en/latest/examples.html#clustering).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/277
https://github.com/scverse/scanpy/issues/277:600,interoperability,api,api,600,"From my understanding, Seurat uses the Louvain algorithm to maximize modularity on their KNN where edge weights are Jaccard similarity between the relevant nodes' neighbors. Scanpy is largely similar by default, though the nearest neighbors are found by UMAP's method, and all edge weights are 1. There's a boolean flag to use edge weights if you'd like (`use_weights`), which uses the weights from the adjacency matrix found in: `adata.uns[""neighbors""][""connectivities""]`. If you'd like to know more, I'd recommend looking at the docs for [`sc.pp.neighbors`](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.neighbors.html#scanpy.api.pp.neighbors), [`sc.tl.louvain`](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html#scanpy.api.tl.louvain), and the [clustering tutorials](https://scanpy.readthedocs.io/en/latest/examples.html#clustering).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/277
https://github.com/scverse/scanpy/issues/277:611,interoperability,api,api,611,"From my understanding, Seurat uses the Louvain algorithm to maximize modularity on their KNN where edge weights are Jaccard similarity between the relevant nodes' neighbors. Scanpy is largely similar by default, though the nearest neighbors are found by UMAP's method, and all edge weights are 1. There's a boolean flag to use edge weights if you'd like (`use_weights`), which uses the weights from the adjacency matrix found in: `adata.uns[""neighbors""][""connectivities""]`. If you'd like to know more, I'd recommend looking at the docs for [`sc.pp.neighbors`](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.neighbors.html#scanpy.api.pp.neighbors), [`sc.tl.louvain`](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html#scanpy.api.tl.louvain), and the [clustering tutorials](https://scanpy.readthedocs.io/en/latest/examples.html#clustering).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/277
https://github.com/scverse/scanpy/issues/277:640,interoperability,api,api,640,"From my understanding, Seurat uses the Louvain algorithm to maximize modularity on their KNN where edge weights are Jaccard similarity between the relevant nodes' neighbors. Scanpy is largely similar by default, though the nearest neighbors are found by UMAP's method, and all edge weights are 1. There's a boolean flag to use edge weights if you'd like (`use_weights`), which uses the weights from the adjacency matrix found in: `adata.uns[""neighbors""][""connectivities""]`. If you'd like to know more, I'd recommend looking at the docs for [`sc.pp.neighbors`](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.neighbors.html#scanpy.api.pp.neighbors), [`sc.tl.louvain`](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html#scanpy.api.tl.louvain), and the [clustering tutorials](https://scanpy.readthedocs.io/en/latest/examples.html#clustering).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/277
https://github.com/scverse/scanpy/issues/277:717,interoperability,api,api,717,"From my understanding, Seurat uses the Louvain algorithm to maximize modularity on their KNN where edge weights are Jaccard similarity between the relevant nodes' neighbors. Scanpy is largely similar by default, though the nearest neighbors are found by UMAP's method, and all edge weights are 1. There's a boolean flag to use edge weights if you'd like (`use_weights`), which uses the weights from the adjacency matrix found in: `adata.uns[""neighbors""][""connectivities""]`. If you'd like to know more, I'd recommend looking at the docs for [`sc.pp.neighbors`](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.neighbors.html#scanpy.api.pp.neighbors), [`sc.tl.louvain`](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html#scanpy.api.tl.louvain), and the [clustering tutorials](https://scanpy.readthedocs.io/en/latest/examples.html#clustering).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/277
https://github.com/scverse/scanpy/issues/277:728,interoperability,api,api,728,"From my understanding, Seurat uses the Louvain algorithm to maximize modularity on their KNN where edge weights are Jaccard similarity between the relevant nodes' neighbors. Scanpy is largely similar by default, though the nearest neighbors are found by UMAP's method, and all edge weights are 1. There's a boolean flag to use edge weights if you'd like (`use_weights`), which uses the weights from the adjacency matrix found in: `adata.uns[""neighbors""][""connectivities""]`. If you'd like to know more, I'd recommend looking at the docs for [`sc.pp.neighbors`](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.neighbors.html#scanpy.api.pp.neighbors), [`sc.tl.louvain`](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html#scanpy.api.tl.louvain), and the [clustering tutorials](https://scanpy.readthedocs.io/en/latest/examples.html#clustering).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/277
https://github.com/scverse/scanpy/issues/277:755,interoperability,api,api,755,"From my understanding, Seurat uses the Louvain algorithm to maximize modularity on their KNN where edge weights are Jaccard similarity between the relevant nodes' neighbors. Scanpy is largely similar by default, though the nearest neighbors are found by UMAP's method, and all edge weights are 1. There's a boolean flag to use edge weights if you'd like (`use_weights`), which uses the weights from the adjacency matrix found in: `adata.uns[""neighbors""][""connectivities""]`. If you'd like to know more, I'd recommend looking at the docs for [`sc.pp.neighbors`](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.neighbors.html#scanpy.api.pp.neighbors), [`sc.tl.louvain`](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html#scanpy.api.tl.louvain), and the [clustering tutorials](https://scanpy.readthedocs.io/en/latest/examples.html#clustering).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/277
https://github.com/scverse/scanpy/issues/277:69,modifiability,modul,modularity,69,"From my understanding, Seurat uses the Louvain algorithm to maximize modularity on their KNN where edge weights are Jaccard similarity between the relevant nodes' neighbors. Scanpy is largely similar by default, though the nearest neighbors are found by UMAP's method, and all edge weights are 1. There's a boolean flag to use edge weights if you'd like (`use_weights`), which uses the weights from the adjacency matrix found in: `adata.uns[""neighbors""][""connectivities""]`. If you'd like to know more, I'd recommend looking at the docs for [`sc.pp.neighbors`](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.neighbors.html#scanpy.api.pp.neighbors), [`sc.tl.louvain`](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html#scanpy.api.tl.louvain), and the [clustering tutorials](https://scanpy.readthedocs.io/en/latest/examples.html#clustering).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/277
https://github.com/scverse/scanpy/issues/277:69,safety,modul,modularity,69,"From my understanding, Seurat uses the Louvain algorithm to maximize modularity on their KNN where edge weights are Jaccard similarity between the relevant nodes' neighbors. Scanpy is largely similar by default, though the nearest neighbors are found by UMAP's method, and all edge weights are 1. There's a boolean flag to use edge weights if you'd like (`use_weights`), which uses the weights from the adjacency matrix found in: `adata.uns[""neighbors""][""connectivities""]`. If you'd like to know more, I'd recommend looking at the docs for [`sc.pp.neighbors`](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.neighbors.html#scanpy.api.pp.neighbors), [`sc.tl.louvain`](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html#scanpy.api.tl.louvain), and the [clustering tutorials](https://scanpy.readthedocs.io/en/latest/examples.html#clustering).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/277
https://github.com/scverse/scanpy/issues/277:8,testability,understand,understanding,8,"From my understanding, Seurat uses the Louvain algorithm to maximize modularity on their KNN where edge weights are Jaccard similarity between the relevant nodes' neighbors. Scanpy is largely similar by default, though the nearest neighbors are found by UMAP's method, and all edge weights are 1. There's a boolean flag to use edge weights if you'd like (`use_weights`), which uses the weights from the adjacency matrix found in: `adata.uns[""neighbors""][""connectivities""]`. If you'd like to know more, I'd recommend looking at the docs for [`sc.pp.neighbors`](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.neighbors.html#scanpy.api.pp.neighbors), [`sc.tl.louvain`](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html#scanpy.api.tl.louvain), and the [clustering tutorials](https://scanpy.readthedocs.io/en/latest/examples.html#clustering).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/277
https://github.com/scverse/scanpy/issues/277:69,testability,modula,modularity,69,"From my understanding, Seurat uses the Louvain algorithm to maximize modularity on their KNN where edge weights are Jaccard similarity between the relevant nodes' neighbors. Scanpy is largely similar by default, though the nearest neighbors are found by UMAP's method, and all edge weights are 1. There's a boolean flag to use edge weights if you'd like (`use_weights`), which uses the weights from the adjacency matrix found in: `adata.uns[""neighbors""][""connectivities""]`. If you'd like to know more, I'd recommend looking at the docs for [`sc.pp.neighbors`](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.pp.neighbors.html#scanpy.api.pp.neighbors), [`sc.tl.louvain`](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html#scanpy.api.tl.louvain), and the [clustering tutorials](https://scanpy.readthedocs.io/en/latest/examples.html#clustering).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/277
https://github.com/scverse/scanpy/issues/277:120,integrability,compon,components,120,"I assume UMAP uses a number of top PCs as input as well, no? And do you know if distances are calculated only on 2 UMAP components, or if more are used?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/277
https://github.com/scverse/scanpy/issues/277:120,interoperability,compon,components,120,"I assume UMAP uses a number of top PCs as input as well, no? And do you know if distances are calculated only on 2 UMAP components, or if more are used?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/277
https://github.com/scverse/scanpy/issues/277:120,modifiability,compon,components,120,"I assume UMAP uses a number of top PCs as input as well, no? And do you know if distances are calculated only on 2 UMAP components, or if more are used?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/277
https://github.com/scverse/scanpy/issues/277:42,safety,input,input,42,"I assume UMAP uses a number of top PCs as input as well, no? And do you know if distances are calculated only on 2 UMAP components, or if more are used?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/277
https://github.com/scverse/scanpy/issues/277:42,usability,input,input,42,"I assume UMAP uses a number of top PCs as input as well, no? And do you know if distances are calculated only on 2 UMAP components, or if more are used?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/277
https://github.com/scverse/scanpy/issues/277:206,interoperability,specif,specifies,206,"Part of the UMAP computation is creating an approximate KNN graph. Scanpy uses that part of the UMAP package to create it's neighborhood graph. UMAP uses whatever distance metric and feature space the user specifies for finding neighbors. Scanpy uses euclidean distance in PCA space as the default for finding neighbors, but that can be changed. I hope that's more clear.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/277
https://github.com/scverse/scanpy/issues/277:101,modifiability,pac,package,101,"Part of the UMAP computation is creating an approximate KNN graph. Scanpy uses that part of the UMAP package to create it's neighborhood graph. UMAP uses whatever distance metric and feature space the user specifies for finding neighbors. Scanpy uses euclidean distance in PCA space as the default for finding neighbors, but that can be changed. I hope that's more clear.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/277
https://github.com/scverse/scanpy/issues/277:201,usability,user,user,201,"Part of the UMAP computation is creating an approximate KNN graph. Scanpy uses that part of the UMAP package to create it's neighborhood graph. UMAP uses whatever distance metric and feature space the user specifies for finding neighbors. Scanpy uses euclidean distance in PCA space as the default for finding neighbors, but that can be changed. I hope that's more clear.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/277
https://github.com/scverse/scanpy/issues/277:365,usability,clear,clear,365,"Part of the UMAP computation is creating an approximate KNN graph. Scanpy uses that part of the UMAP package to create it's neighborhood graph. UMAP uses whatever distance metric and feature space the user specifies for finding neighbors. Scanpy uses euclidean distance in PCA space as the default for finding neighbors, but that can be changed. I hope that's more clear.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/277
https://github.com/scverse/scanpy/issues/277:72,energy efficiency,reduc,reduction,72,"So the `method='umap'` has nothing to do with using UMAP dimensionality reduction as a distance metric, but instead only refers to a KNN-graph implementation? It seems to me that making a KNN-graph from the Euclidean distances should be easy.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/277
https://github.com/scverse/scanpy/issues/277:138,performance,memor,memory,138,"Yeah, that sounds right. It's super easy to code, you'd just need to calculate all pairwise distances which is ~*O(n^2)*, and similar for memory. To get around that there's a number of approximate nearest neighbors methods. . I'm not too sure what the assumptions are behind each method though. @falexwolf, any reason in particular you've chosen UMAP's method for the KNN calculation?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/277
https://github.com/scverse/scanpy/issues/277:138,usability,memor,memory,138,"Yeah, that sounds right. It's super easy to code, you'd just need to calculate all pairwise distances which is ~*O(n^2)*, and similar for memory. To get around that there's a number of approximate nearest neighbors methods. . I'm not too sure what the assumptions are behind each method though. @falexwolf, any reason in particular you've chosen UMAP's method for the KNN calculation?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/277
https://github.com/scverse/scanpy/issues/277:319,availability,avail,available,319,"> I'm not to sure what the assumptions are behind each method though. @falexwolf, any reason in particular you've chosen UMAP's method for the KNN calculation? It's highly competitive in terms of speed and accuracy with other libraries (https://github.com/erikbern/ann-benchmarks, pynndescent is what umap uses, wasn't available at the time for Scanpy), it's a lot easier to install than everything else, and the result has been shown to harmonize well with UMAP, which I expected would become the canonical way of visualizing things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/277
https://github.com/scverse/scanpy/issues/277:375,deployability,instal,install,375,"> I'm not to sure what the assumptions are behind each method though. @falexwolf, any reason in particular you've chosen UMAP's method for the KNN calculation? It's highly competitive in terms of speed and accuracy with other libraries (https://github.com/erikbern/ann-benchmarks, pynndescent is what umap uses, wasn't available at the time for Scanpy), it's a lot easier to install than everything else, and the result has been shown to harmonize well with UMAP, which I expected would become the canonical way of visualizing things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/277
https://github.com/scverse/scanpy/issues/277:336,performance,time,time,336,"> I'm not to sure what the assumptions are behind each method though. @falexwolf, any reason in particular you've chosen UMAP's method for the KNN calculation? It's highly competitive in terms of speed and accuracy with other libraries (https://github.com/erikbern/ann-benchmarks, pynndescent is what umap uses, wasn't available at the time for Scanpy), it's a lot easier to install than everything else, and the result has been shown to harmonize well with UMAP, which I expected would become the canonical way of visualizing things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/277
https://github.com/scverse/scanpy/issues/277:319,reliability,availab,available,319,"> I'm not to sure what the assumptions are behind each method though. @falexwolf, any reason in particular you've chosen UMAP's method for the KNN calculation? It's highly competitive in terms of speed and accuracy with other libraries (https://github.com/erikbern/ann-benchmarks, pynndescent is what umap uses, wasn't available at the time for Scanpy), it's a lot easier to install than everything else, and the result has been shown to harmonize well with UMAP, which I expected would become the canonical way of visualizing things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/277
https://github.com/scverse/scanpy/issues/277:319,safety,avail,available,319,"> I'm not to sure what the assumptions are behind each method though. @falexwolf, any reason in particular you've chosen UMAP's method for the KNN calculation? It's highly competitive in terms of speed and accuracy with other libraries (https://github.com/erikbern/ann-benchmarks, pynndescent is what umap uses, wasn't available at the time for Scanpy), it's a lot easier to install than everything else, and the result has been shown to harmonize well with UMAP, which I expected would become the canonical way of visualizing things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/277
https://github.com/scverse/scanpy/issues/277:319,security,availab,available,319,"> I'm not to sure what the assumptions are behind each method though. @falexwolf, any reason in particular you've chosen UMAP's method for the KNN calculation? It's highly competitive in terms of speed and accuracy with other libraries (https://github.com/erikbern/ann-benchmarks, pynndescent is what umap uses, wasn't available at the time for Scanpy), it's a lot easier to install than everything else, and the result has been shown to harmonize well with UMAP, which I expected would become the canonical way of visualizing things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/277
https://github.com/scverse/scanpy/issues/277:515,usability,visual,visualizing,515,"> I'm not to sure what the assumptions are behind each method though. @falexwolf, any reason in particular you've chosen UMAP's method for the KNN calculation? It's highly competitive in terms of speed and accuracy with other libraries (https://github.com/erikbern/ann-benchmarks, pynndescent is what umap uses, wasn't available at the time for Scanpy), it's a lot easier to install than everything else, and the result has been shown to harmonize well with UMAP, which I expected would become the canonical way of visualizing things.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/277
https://github.com/scverse/scanpy/issues/277:161,energy efficiency,optim,optimization,161,"Thanks for the explanation! I had no idea this was an important bottleneck. I always assumed all distances are calculated... that shows how little I think about optimization ^^. On another note, it might be a little confusing to see method='umap' as a parameter. I would have immediately assumed that umap is used as a distance metric. But maybe that's me being too lazy to read as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/277
https://github.com/scverse/scanpy/issues/277:252,modifiability,paramet,parameter,252,"Thanks for the explanation! I had no idea this was an important bottleneck. I always assumed all distances are calculated... that shows how little I think about optimization ^^. On another note, it might be a little confusing to see method='umap' as a parameter. I would have immediately assumed that umap is used as a distance metric. But maybe that's me being too lazy to read as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/277
https://github.com/scverse/scanpy/issues/277:64,performance,bottleneck,bottleneck,64,"Thanks for the explanation! I had no idea this was an important bottleneck. I always assumed all distances are calculated... that shows how little I think about optimization ^^. On another note, it might be a little confusing to see method='umap' as a parameter. I would have immediately assumed that umap is used as a distance metric. But maybe that's me being too lazy to read as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/277
https://github.com/scverse/scanpy/issues/277:161,performance,optimiz,optimization,161,"Thanks for the explanation! I had no idea this was an important bottleneck. I always assumed all distances are calculated... that shows how little I think about optimization ^^. On another note, it might be a little confusing to see method='umap' as a parameter. I would have immediately assumed that umap is used as a distance metric. But maybe that's me being too lazy to read as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/277
https://github.com/scverse/scanpy/issues/277:8,usability,document,documented,8,"Is this documented with this level of clarity anywhere else, other than in this thread?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/277
https://github.com/scverse/scanpy/issues/277:38,usability,clarit,clarity,38,"Is this documented with this level of clarity anywhere else, other than in this thread?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/277
https://github.com/scverse/scanpy/issues/278:176,integrability,sub,subsets,176,"No, each of '1' and '2' is tested against the ""rest"" of the data, that is the union of '2' & '3' in the first, and the union of '1' and '3' in the second case. `groups` merely subsets which groups to look at, the default is to look at all, where 'all' will be equivalent to `['1', '2', '3']`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:27,safety,test,tested,27,"No, each of '1' and '2' is tested against the ""rest"" of the data, that is the union of '2' & '3' in the first, and the union of '1' and '3' in the second case. `groups` merely subsets which groups to look at, the default is to look at all, where 'all' will be equivalent to `['1', '2', '3']`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:27,testability,test,tested,27,"No, each of '1' and '2' is tested against the ""rest"" of the data, that is the union of '2' & '3' in the first, and the union of '1' and '3' in the second case. `groups` merely subsets which groups to look at, the default is to look at all, where 'all' will be equivalent to `['1', '2', '3']`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:6,energy efficiency,cool,cool,6,It be cool if you made a PR to improve the documentation!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:43,usability,document,documentation,43,It be cool if you made a PR to improve the documentation!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:77,deployability,log,logreg,77,"What worries me is that the behaviour you describe might not so for `method='logreg'`. Let's have a look at the following example:. ```python. import scanpy.api as sc. adata = sc.datasets.blobs(). sc.pp.scale(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). for method in ['t-test', 'logreg']:. # first call, without groups. sc.tl.rank_genes_groups(adata, 'louvain', method=method). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). # first call, with groups . sc.tl.rank_genes_groups(adata, 'louvain', method=method, groups=['0', '1', '2']). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). ```. As setting `groups` to `['0', '1', '2']` should not change the reference dataset, exactly the same marker genes should be detected for the first and the second call of `sc.tl.rank_genes_groups`. This is indeed true if I set the method to `t-test`. However, when setting method to `logreg`, I get other marker genes. Visually it appears to me that only the groups `['0', '1', '2']` are used a the reference set when using the method `logreg`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:157,deployability,api,api,157,"What worries me is that the behaviour you describe might not so for `method='logreg'`. Let's have a look at the following example:. ```python. import scanpy.api as sc. adata = sc.datasets.blobs(). sc.pp.scale(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). for method in ['t-test', 'logreg']:. # first call, without groups. sc.tl.rank_genes_groups(adata, 'louvain', method=method). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). # first call, with groups . sc.tl.rank_genes_groups(adata, 'louvain', method=method, groups=['0', '1', '2']). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). ```. As setting `groups` to `['0', '1', '2']` should not change the reference dataset, exactly the same marker genes should be detected for the first and the second call of `sc.tl.rank_genes_groups`. This is indeed true if I set the method to `t-test`. However, when setting method to `logreg`, I get other marker genes. Visually it appears to me that only the groups `['0', '1', '2']` are used a the reference set when using the method `logreg`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:203,deployability,scale,scale,203,"What worries me is that the behaviour you describe might not so for `method='logreg'`. Let's have a look at the following example:. ```python. import scanpy.api as sc. adata = sc.datasets.blobs(). sc.pp.scale(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). for method in ['t-test', 'logreg']:. # first call, without groups. sc.tl.rank_genes_groups(adata, 'louvain', method=method). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). # first call, with groups . sc.tl.rank_genes_groups(adata, 'louvain', method=method, groups=['0', '1', '2']). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). ```. As setting `groups` to `['0', '1', '2']` should not change the reference dataset, exactly the same marker genes should be detected for the first and the second call of `sc.tl.rank_genes_groups`. This is indeed true if I set the method to `t-test`. However, when setting method to `logreg`, I get other marker genes. Visually it appears to me that only the groups `['0', '1', '2']` are used a the reference set when using the method `logreg`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:289,deployability,log,logreg,289,"What worries me is that the behaviour you describe might not so for `method='logreg'`. Let's have a look at the following example:. ```python. import scanpy.api as sc. adata = sc.datasets.blobs(). sc.pp.scale(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). for method in ['t-test', 'logreg']:. # first call, without groups. sc.tl.rank_genes_groups(adata, 'louvain', method=method). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). # first call, with groups . sc.tl.rank_genes_groups(adata, 'louvain', method=method, groups=['0', '1', '2']). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). ```. As setting `groups` to `['0', '1', '2']` should not change the reference dataset, exactly the same marker genes should be detected for the first and the second call of `sc.tl.rank_genes_groups`. This is indeed true if I set the method to `t-test`. However, when setting method to `logreg`, I get other marker genes. Visually it appears to me that only the groups `['0', '1', '2']` are used a the reference set when using the method `logreg`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:892,deployability,log,logreg,892,"What worries me is that the behaviour you describe might not so for `method='logreg'`. Let's have a look at the following example:. ```python. import scanpy.api as sc. adata = sc.datasets.blobs(). sc.pp.scale(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). for method in ['t-test', 'logreg']:. # first call, without groups. sc.tl.rank_genes_groups(adata, 'louvain', method=method). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). # first call, with groups . sc.tl.rank_genes_groups(adata, 'louvain', method=method, groups=['0', '1', '2']). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). ```. As setting `groups` to `['0', '1', '2']` should not change the reference dataset, exactly the same marker genes should be detected for the first and the second call of `sc.tl.rank_genes_groups`. This is indeed true if I set the method to `t-test`. However, when setting method to `logreg`, I get other marker genes. Visually it appears to me that only the groups `['0', '1', '2']` are used a the reference set when using the method `logreg`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:1044,deployability,log,logreg,1044,"What worries me is that the behaviour you describe might not so for `method='logreg'`. Let's have a look at the following example:. ```python. import scanpy.api as sc. adata = sc.datasets.blobs(). sc.pp.scale(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). for method in ['t-test', 'logreg']:. # first call, without groups. sc.tl.rank_genes_groups(adata, 'louvain', method=method). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). # first call, with groups . sc.tl.rank_genes_groups(adata, 'louvain', method=method, groups=['0', '1', '2']). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). ```. As setting `groups` to `['0', '1', '2']` should not change the reference dataset, exactly the same marker genes should be detected for the first and the second call of `sc.tl.rank_genes_groups`. This is indeed true if I set the method to `t-test`. However, when setting method to `logreg`, I get other marker genes. Visually it appears to me that only the groups `['0', '1', '2']` are used a the reference set when using the method `logreg`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:203,energy efficiency,scale,scale,203,"What worries me is that the behaviour you describe might not so for `method='logreg'`. Let's have a look at the following example:. ```python. import scanpy.api as sc. adata = sc.datasets.blobs(). sc.pp.scale(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). for method in ['t-test', 'logreg']:. # first call, without groups. sc.tl.rank_genes_groups(adata, 'louvain', method=method). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). # first call, with groups . sc.tl.rank_genes_groups(adata, 'louvain', method=method, groups=['0', '1', '2']). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). ```. As setting `groups` to `['0', '1', '2']` should not change the reference dataset, exactly the same marker genes should be detected for the first and the second call of `sc.tl.rank_genes_groups`. This is indeed true if I set the method to `t-test`. However, when setting method to `logreg`, I get other marker genes. Visually it appears to me that only the groups `['0', '1', '2']` are used a the reference set when using the method `logreg`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:157,integrability,api,api,157,"What worries me is that the behaviour you describe might not so for `method='logreg'`. Let's have a look at the following example:. ```python. import scanpy.api as sc. adata = sc.datasets.blobs(). sc.pp.scale(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). for method in ['t-test', 'logreg']:. # first call, without groups. sc.tl.rank_genes_groups(adata, 'louvain', method=method). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). # first call, with groups . sc.tl.rank_genes_groups(adata, 'louvain', method=method, groups=['0', '1', '2']). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). ```. As setting `groups` to `['0', '1', '2']` should not change the reference dataset, exactly the same marker genes should be detected for the first and the second call of `sc.tl.rank_genes_groups`. This is indeed true if I set the method to `t-test`. However, when setting method to `logreg`, I get other marker genes. Visually it appears to me that only the groups `['0', '1', '2']` are used a the reference set when using the method `logreg`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:157,interoperability,api,api,157,"What worries me is that the behaviour you describe might not so for `method='logreg'`. Let's have a look at the following example:. ```python. import scanpy.api as sc. adata = sc.datasets.blobs(). sc.pp.scale(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). for method in ['t-test', 'logreg']:. # first call, without groups. sc.tl.rank_genes_groups(adata, 'louvain', method=method). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). # first call, with groups . sc.tl.rank_genes_groups(adata, 'louvain', method=method, groups=['0', '1', '2']). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). ```. As setting `groups` to `['0', '1', '2']` should not change the reference dataset, exactly the same marker genes should be detected for the first and the second call of `sc.tl.rank_genes_groups`. This is indeed true if I set the method to `t-test`. However, when setting method to `logreg`, I get other marker genes. Visually it appears to me that only the groups `['0', '1', '2']` are used a the reference set when using the method `logreg`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:203,modifiability,scal,scale,203,"What worries me is that the behaviour you describe might not so for `method='logreg'`. Let's have a look at the following example:. ```python. import scanpy.api as sc. adata = sc.datasets.blobs(). sc.pp.scale(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). for method in ['t-test', 'logreg']:. # first call, without groups. sc.tl.rank_genes_groups(adata, 'louvain', method=method). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). # first call, with groups . sc.tl.rank_genes_groups(adata, 'louvain', method=method, groups=['0', '1', '2']). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). ```. As setting `groups` to `['0', '1', '2']` should not change the reference dataset, exactly the same marker genes should be detected for the first and the second call of `sc.tl.rank_genes_groups`. This is indeed true if I set the method to `t-test`. However, when setting method to `logreg`, I get other marker genes. Visually it appears to me that only the groups `['0', '1', '2']` are used a the reference set when using the method `logreg`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:203,performance,scale,scale,203,"What worries me is that the behaviour you describe might not so for `method='logreg'`. Let's have a look at the following example:. ```python. import scanpy.api as sc. adata = sc.datasets.blobs(). sc.pp.scale(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). for method in ['t-test', 'logreg']:. # first call, without groups. sc.tl.rank_genes_groups(adata, 'louvain', method=method). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). # first call, with groups . sc.tl.rank_genes_groups(adata, 'louvain', method=method, groups=['0', '1', '2']). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). ```. As setting `groups` to `['0', '1', '2']` should not change the reference dataset, exactly the same marker genes should be detected for the first and the second call of `sc.tl.rank_genes_groups`. This is indeed true if I set the method to `t-test`. However, when setting method to `logreg`, I get other marker genes. Visually it appears to me that only the groups `['0', '1', '2']` are used a the reference set when using the method `logreg`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:77,safety,log,logreg,77,"What worries me is that the behaviour you describe might not so for `method='logreg'`. Let's have a look at the following example:. ```python. import scanpy.api as sc. adata = sc.datasets.blobs(). sc.pp.scale(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). for method in ['t-test', 'logreg']:. # first call, without groups. sc.tl.rank_genes_groups(adata, 'louvain', method=method). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). # first call, with groups . sc.tl.rank_genes_groups(adata, 'louvain', method=method, groups=['0', '1', '2']). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). ```. As setting `groups` to `['0', '1', '2']` should not change the reference dataset, exactly the same marker genes should be detected for the first and the second call of `sc.tl.rank_genes_groups`. This is indeed true if I set the method to `t-test`. However, when setting method to `logreg`, I get other marker genes. Visually it appears to me that only the groups `['0', '1', '2']` are used a the reference set when using the method `logreg`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:281,safety,test,test,281,"What worries me is that the behaviour you describe might not so for `method='logreg'`. Let's have a look at the following example:. ```python. import scanpy.api as sc. adata = sc.datasets.blobs(). sc.pp.scale(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). for method in ['t-test', 'logreg']:. # first call, without groups. sc.tl.rank_genes_groups(adata, 'louvain', method=method). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). # first call, with groups . sc.tl.rank_genes_groups(adata, 'louvain', method=method, groups=['0', '1', '2']). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). ```. As setting `groups` to `['0', '1', '2']` should not change the reference dataset, exactly the same marker genes should be detected for the first and the second call of `sc.tl.rank_genes_groups`. This is indeed true if I set the method to `t-test`. However, when setting method to `logreg`, I get other marker genes. Visually it appears to me that only the groups `['0', '1', '2']` are used a the reference set when using the method `logreg`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:289,safety,log,logreg,289,"What worries me is that the behaviour you describe might not so for `method='logreg'`. Let's have a look at the following example:. ```python. import scanpy.api as sc. adata = sc.datasets.blobs(). sc.pp.scale(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). for method in ['t-test', 'logreg']:. # first call, without groups. sc.tl.rank_genes_groups(adata, 'louvain', method=method). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). # first call, with groups . sc.tl.rank_genes_groups(adata, 'louvain', method=method, groups=['0', '1', '2']). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). ```. As setting `groups` to `['0', '1', '2']` should not change the reference dataset, exactly the same marker genes should be detected for the first and the second call of `sc.tl.rank_genes_groups`. This is indeed true if I set the method to `t-test`. However, when setting method to `logreg`, I get other marker genes. Visually it appears to me that only the groups `['0', '1', '2']` are used a the reference set when using the method `logreg`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:733,safety,detect,detected,733,"What worries me is that the behaviour you describe might not so for `method='logreg'`. Let's have a look at the following example:. ```python. import scanpy.api as sc. adata = sc.datasets.blobs(). sc.pp.scale(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). for method in ['t-test', 'logreg']:. # first call, without groups. sc.tl.rank_genes_groups(adata, 'louvain', method=method). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). # first call, with groups . sc.tl.rank_genes_groups(adata, 'louvain', method=method, groups=['0', '1', '2']). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). ```. As setting `groups` to `['0', '1', '2']` should not change the reference dataset, exactly the same marker genes should be detected for the first and the second call of `sc.tl.rank_genes_groups`. This is indeed true if I set the method to `t-test`. However, when setting method to `logreg`, I get other marker genes. Visually it appears to me that only the groups `['0', '1', '2']` are used a the reference set when using the method `logreg`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:852,safety,test,test,852,"What worries me is that the behaviour you describe might not so for `method='logreg'`. Let's have a look at the following example:. ```python. import scanpy.api as sc. adata = sc.datasets.blobs(). sc.pp.scale(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). for method in ['t-test', 'logreg']:. # first call, without groups. sc.tl.rank_genes_groups(adata, 'louvain', method=method). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). # first call, with groups . sc.tl.rank_genes_groups(adata, 'louvain', method=method, groups=['0', '1', '2']). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). ```. As setting `groups` to `['0', '1', '2']` should not change the reference dataset, exactly the same marker genes should be detected for the first and the second call of `sc.tl.rank_genes_groups`. This is indeed true if I set the method to `t-test`. However, when setting method to `logreg`, I get other marker genes. Visually it appears to me that only the groups `['0', '1', '2']` are used a the reference set when using the method `logreg`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:892,safety,log,logreg,892,"What worries me is that the behaviour you describe might not so for `method='logreg'`. Let's have a look at the following example:. ```python. import scanpy.api as sc. adata = sc.datasets.blobs(). sc.pp.scale(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). for method in ['t-test', 'logreg']:. # first call, without groups. sc.tl.rank_genes_groups(adata, 'louvain', method=method). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). # first call, with groups . sc.tl.rank_genes_groups(adata, 'louvain', method=method, groups=['0', '1', '2']). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). ```. As setting `groups` to `['0', '1', '2']` should not change the reference dataset, exactly the same marker genes should be detected for the first and the second call of `sc.tl.rank_genes_groups`. This is indeed true if I set the method to `t-test`. However, when setting method to `logreg`, I get other marker genes. Visually it appears to me that only the groups `['0', '1', '2']` are used a the reference set when using the method `logreg`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:1044,safety,log,logreg,1044,"What worries me is that the behaviour you describe might not so for `method='logreg'`. Let's have a look at the following example:. ```python. import scanpy.api as sc. adata = sc.datasets.blobs(). sc.pp.scale(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). for method in ['t-test', 'logreg']:. # first call, without groups. sc.tl.rank_genes_groups(adata, 'louvain', method=method). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). # first call, with groups . sc.tl.rank_genes_groups(adata, 'louvain', method=method, groups=['0', '1', '2']). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). ```. As setting `groups` to `['0', '1', '2']` should not change the reference dataset, exactly the same marker genes should be detected for the first and the second call of `sc.tl.rank_genes_groups`. This is indeed true if I set the method to `t-test`. However, when setting method to `logreg`, I get other marker genes. Visually it appears to me that only the groups `['0', '1', '2']` are used a the reference set when using the method `logreg`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:77,security,log,logreg,77,"What worries me is that the behaviour you describe might not so for `method='logreg'`. Let's have a look at the following example:. ```python. import scanpy.api as sc. adata = sc.datasets.blobs(). sc.pp.scale(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). for method in ['t-test', 'logreg']:. # first call, without groups. sc.tl.rank_genes_groups(adata, 'louvain', method=method). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). # first call, with groups . sc.tl.rank_genes_groups(adata, 'louvain', method=method, groups=['0', '1', '2']). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). ```. As setting `groups` to `['0', '1', '2']` should not change the reference dataset, exactly the same marker genes should be detected for the first and the second call of `sc.tl.rank_genes_groups`. This is indeed true if I set the method to `t-test`. However, when setting method to `logreg`, I get other marker genes. Visually it appears to me that only the groups `['0', '1', '2']` are used a the reference set when using the method `logreg`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:289,security,log,logreg,289,"What worries me is that the behaviour you describe might not so for `method='logreg'`. Let's have a look at the following example:. ```python. import scanpy.api as sc. adata = sc.datasets.blobs(). sc.pp.scale(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). for method in ['t-test', 'logreg']:. # first call, without groups. sc.tl.rank_genes_groups(adata, 'louvain', method=method). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). # first call, with groups . sc.tl.rank_genes_groups(adata, 'louvain', method=method, groups=['0', '1', '2']). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). ```. As setting `groups` to `['0', '1', '2']` should not change the reference dataset, exactly the same marker genes should be detected for the first and the second call of `sc.tl.rank_genes_groups`. This is indeed true if I set the method to `t-test`. However, when setting method to `logreg`, I get other marker genes. Visually it appears to me that only the groups `['0', '1', '2']` are used a the reference set when using the method `logreg`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:733,security,detect,detected,733,"What worries me is that the behaviour you describe might not so for `method='logreg'`. Let's have a look at the following example:. ```python. import scanpy.api as sc. adata = sc.datasets.blobs(). sc.pp.scale(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). for method in ['t-test', 'logreg']:. # first call, without groups. sc.tl.rank_genes_groups(adata, 'louvain', method=method). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). # first call, with groups . sc.tl.rank_genes_groups(adata, 'louvain', method=method, groups=['0', '1', '2']). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). ```. As setting `groups` to `['0', '1', '2']` should not change the reference dataset, exactly the same marker genes should be detected for the first and the second call of `sc.tl.rank_genes_groups`. This is indeed true if I set the method to `t-test`. However, when setting method to `logreg`, I get other marker genes. Visually it appears to me that only the groups `['0', '1', '2']` are used a the reference set when using the method `logreg`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:892,security,log,logreg,892,"What worries me is that the behaviour you describe might not so for `method='logreg'`. Let's have a look at the following example:. ```python. import scanpy.api as sc. adata = sc.datasets.blobs(). sc.pp.scale(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). for method in ['t-test', 'logreg']:. # first call, without groups. sc.tl.rank_genes_groups(adata, 'louvain', method=method). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). # first call, with groups . sc.tl.rank_genes_groups(adata, 'louvain', method=method, groups=['0', '1', '2']). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). ```. As setting `groups` to `['0', '1', '2']` should not change the reference dataset, exactly the same marker genes should be detected for the first and the second call of `sc.tl.rank_genes_groups`. This is indeed true if I set the method to `t-test`. However, when setting method to `logreg`, I get other marker genes. Visually it appears to me that only the groups `['0', '1', '2']` are used a the reference set when using the method `logreg`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:1044,security,log,logreg,1044,"What worries me is that the behaviour you describe might not so for `method='logreg'`. Let's have a look at the following example:. ```python. import scanpy.api as sc. adata = sc.datasets.blobs(). sc.pp.scale(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). for method in ['t-test', 'logreg']:. # first call, without groups. sc.tl.rank_genes_groups(adata, 'louvain', method=method). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). # first call, with groups . sc.tl.rank_genes_groups(adata, 'louvain', method=method, groups=['0', '1', '2']). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). ```. As setting `groups` to `['0', '1', '2']` should not change the reference dataset, exactly the same marker genes should be detected for the first and the second call of `sc.tl.rank_genes_groups`. This is indeed true if I set the method to `t-test`. However, when setting method to `logreg`, I get other marker genes. Visually it appears to me that only the groups `['0', '1', '2']` are used a the reference set when using the method `logreg`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:77,testability,log,logreg,77,"What worries me is that the behaviour you describe might not so for `method='logreg'`. Let's have a look at the following example:. ```python. import scanpy.api as sc. adata = sc.datasets.blobs(). sc.pp.scale(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). for method in ['t-test', 'logreg']:. # first call, without groups. sc.tl.rank_genes_groups(adata, 'louvain', method=method). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). # first call, with groups . sc.tl.rank_genes_groups(adata, 'louvain', method=method, groups=['0', '1', '2']). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). ```. As setting `groups` to `['0', '1', '2']` should not change the reference dataset, exactly the same marker genes should be detected for the first and the second call of `sc.tl.rank_genes_groups`. This is indeed true if I set the method to `t-test`. However, when setting method to `logreg`, I get other marker genes. Visually it appears to me that only the groups `['0', '1', '2']` are used a the reference set when using the method `logreg`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:281,testability,test,test,281,"What worries me is that the behaviour you describe might not so for `method='logreg'`. Let's have a look at the following example:. ```python. import scanpy.api as sc. adata = sc.datasets.blobs(). sc.pp.scale(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). for method in ['t-test', 'logreg']:. # first call, without groups. sc.tl.rank_genes_groups(adata, 'louvain', method=method). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). # first call, with groups . sc.tl.rank_genes_groups(adata, 'louvain', method=method, groups=['0', '1', '2']). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). ```. As setting `groups` to `['0', '1', '2']` should not change the reference dataset, exactly the same marker genes should be detected for the first and the second call of `sc.tl.rank_genes_groups`. This is indeed true if I set the method to `t-test`. However, when setting method to `logreg`, I get other marker genes. Visually it appears to me that only the groups `['0', '1', '2']` are used a the reference set when using the method `logreg`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:289,testability,log,logreg,289,"What worries me is that the behaviour you describe might not so for `method='logreg'`. Let's have a look at the following example:. ```python. import scanpy.api as sc. adata = sc.datasets.blobs(). sc.pp.scale(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). for method in ['t-test', 'logreg']:. # first call, without groups. sc.tl.rank_genes_groups(adata, 'louvain', method=method). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). # first call, with groups . sc.tl.rank_genes_groups(adata, 'louvain', method=method, groups=['0', '1', '2']). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). ```. As setting `groups` to `['0', '1', '2']` should not change the reference dataset, exactly the same marker genes should be detected for the first and the second call of `sc.tl.rank_genes_groups`. This is indeed true if I set the method to `t-test`. However, when setting method to `logreg`, I get other marker genes. Visually it appears to me that only the groups `['0', '1', '2']` are used a the reference set when using the method `logreg`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:852,testability,test,test,852,"What worries me is that the behaviour you describe might not so for `method='logreg'`. Let's have a look at the following example:. ```python. import scanpy.api as sc. adata = sc.datasets.blobs(). sc.pp.scale(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). for method in ['t-test', 'logreg']:. # first call, without groups. sc.tl.rank_genes_groups(adata, 'louvain', method=method). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). # first call, with groups . sc.tl.rank_genes_groups(adata, 'louvain', method=method, groups=['0', '1', '2']). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). ```. As setting `groups` to `['0', '1', '2']` should not change the reference dataset, exactly the same marker genes should be detected for the first and the second call of `sc.tl.rank_genes_groups`. This is indeed true if I set the method to `t-test`. However, when setting method to `logreg`, I get other marker genes. Visually it appears to me that only the groups `['0', '1', '2']` are used a the reference set when using the method `logreg`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:892,testability,log,logreg,892,"What worries me is that the behaviour you describe might not so for `method='logreg'`. Let's have a look at the following example:. ```python. import scanpy.api as sc. adata = sc.datasets.blobs(). sc.pp.scale(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). for method in ['t-test', 'logreg']:. # first call, without groups. sc.tl.rank_genes_groups(adata, 'louvain', method=method). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). # first call, with groups . sc.tl.rank_genes_groups(adata, 'louvain', method=method, groups=['0', '1', '2']). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). ```. As setting `groups` to `['0', '1', '2']` should not change the reference dataset, exactly the same marker genes should be detected for the first and the second call of `sc.tl.rank_genes_groups`. This is indeed true if I set the method to `t-test`. However, when setting method to `logreg`, I get other marker genes. Visually it appears to me that only the groups `['0', '1', '2']` are used a the reference set when using the method `logreg`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:1044,testability,log,logreg,1044,"What worries me is that the behaviour you describe might not so for `method='logreg'`. Let's have a look at the following example:. ```python. import scanpy.api as sc. adata = sc.datasets.blobs(). sc.pp.scale(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). for method in ['t-test', 'logreg']:. # first call, without groups. sc.tl.rank_genes_groups(adata, 'louvain', method=method). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). # first call, with groups . sc.tl.rank_genes_groups(adata, 'louvain', method=method, groups=['0', '1', '2']). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). ```. As setting `groups` to `['0', '1', '2']` should not change the reference dataset, exactly the same marker genes should be detected for the first and the second call of `sc.tl.rank_genes_groups`. This is indeed true if I set the method to `t-test`. However, when setting method to `logreg`, I get other marker genes. Visually it appears to me that only the groups `['0', '1', '2']` are used a the reference set when using the method `logreg`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:28,usability,behavi,behaviour,28,"What worries me is that the behaviour you describe might not so for `method='logreg'`. Let's have a look at the following example:. ```python. import scanpy.api as sc. adata = sc.datasets.blobs(). sc.pp.scale(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). for method in ['t-test', 'logreg']:. # first call, without groups. sc.tl.rank_genes_groups(adata, 'louvain', method=method). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). # first call, with groups . sc.tl.rank_genes_groups(adata, 'louvain', method=method, groups=['0', '1', '2']). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). ```. As setting `groups` to `['0', '1', '2']` should not change the reference dataset, exactly the same marker genes should be detected for the first and the second call of `sc.tl.rank_genes_groups`. This is indeed true if I set the method to `t-test`. However, when setting method to `logreg`, I get other marker genes. Visually it appears to me that only the groups `['0', '1', '2']` are used a the reference set when using the method `logreg`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:927,usability,Visual,Visually,927,"What worries me is that the behaviour you describe might not so for `method='logreg'`. Let's have a look at the following example:. ```python. import scanpy.api as sc. adata = sc.datasets.blobs(). sc.pp.scale(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). for method in ['t-test', 'logreg']:. # first call, without groups. sc.tl.rank_genes_groups(adata, 'louvain', method=method). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). # first call, with groups . sc.tl.rank_genes_groups(adata, 'louvain', method=method, groups=['0', '1', '2']). sc.pl.rank_genes_groups_matrixplot(adata, n_genes=1). ```. As setting `groups` to `['0', '1', '2']` should not change the reference dataset, exactly the same marker genes should be detected for the first and the second call of `sc.tl.rank_genes_groups`. This is indeed true if I set the method to `t-test`. However, when setting method to `logreg`, I get other marker genes. Visually it appears to me that only the groups `['0', '1', '2']` are used a the reference set when using the method `logreg`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:790,availability,cluster,cluster,790,"Yes, right, for `logreg` you always regress on data that includes the respective group as this is not a differential test, but a regression. It's a shame that this is not evident from the documentation. It's sort of a recent alternative way of approaching the definition of marker genes and we should give it a bit more of a thought. Regarding the behavior that is inconsistent up there, there is the following in the code by the person who extended this not long ago:. ```. # if reference is not set, then the groups listed will be compared to the rest. # if reference is set, then the groups listed will be compared only to the other groups listed. from sklearn.linear_model import LogisticRegression. if len(groups) == 1:. raise Exception('Cannot perform logistic regression on a single cluster.'). adata_copy = adata[adata.obs[groupby].isin(groups_order)] . adata_comp = adata_copy. if adata.raw is not None and use_raw:. adata_comp = adata_copy.raw. X = adata_comp.X. clf = LogisticRegression(**kwds). clf.fit(X, adata_copy.obs[groupby].cat.codes). ```. You're right that logreg only includes the passed groups, if groups are passed. This should not be the case. I wonder why it's a problem in your specific case as I'd expect that 0, 1, 2 make up the whole data; but maybe the resolution in Louvain is somewhat set to a high value. In any case, I'll change the implementation so that irrespective of whether `groups` is passed or not, one gets the same result.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:17,deployability,log,logreg,17,"Yes, right, for `logreg` you always regress on data that includes the respective group as this is not a differential test, but a regression. It's a shame that this is not evident from the documentation. It's sort of a recent alternative way of approaching the definition of marker genes and we should give it a bit more of a thought. Regarding the behavior that is inconsistent up there, there is the following in the code by the person who extended this not long ago:. ```. # if reference is not set, then the groups listed will be compared to the rest. # if reference is set, then the groups listed will be compared only to the other groups listed. from sklearn.linear_model import LogisticRegression. if len(groups) == 1:. raise Exception('Cannot perform logistic regression on a single cluster.'). adata_copy = adata[adata.obs[groupby].isin(groups_order)] . adata_comp = adata_copy. if adata.raw is not None and use_raw:. adata_comp = adata_copy.raw. X = adata_comp.X. clf = LogisticRegression(**kwds). clf.fit(X, adata_copy.obs[groupby].cat.codes). ```. You're right that logreg only includes the passed groups, if groups are passed. This should not be the case. I wonder why it's a problem in your specific case as I'd expect that 0, 1, 2 make up the whole data; but maybe the resolution in Louvain is somewhat set to a high value. In any case, I'll change the implementation so that irrespective of whether `groups` is passed or not, one gets the same result.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:684,deployability,Log,LogisticRegression,684,"Yes, right, for `logreg` you always regress on data that includes the respective group as this is not a differential test, but a regression. It's a shame that this is not evident from the documentation. It's sort of a recent alternative way of approaching the definition of marker genes and we should give it a bit more of a thought. Regarding the behavior that is inconsistent up there, there is the following in the code by the person who extended this not long ago:. ```. # if reference is not set, then the groups listed will be compared to the rest. # if reference is set, then the groups listed will be compared only to the other groups listed. from sklearn.linear_model import LogisticRegression. if len(groups) == 1:. raise Exception('Cannot perform logistic regression on a single cluster.'). adata_copy = adata[adata.obs[groupby].isin(groups_order)] . adata_comp = adata_copy. if adata.raw is not None and use_raw:. adata_comp = adata_copy.raw. X = adata_comp.X. clf = LogisticRegression(**kwds). clf.fit(X, adata_copy.obs[groupby].cat.codes). ```. You're right that logreg only includes the passed groups, if groups are passed. This should not be the case. I wonder why it's a problem in your specific case as I'd expect that 0, 1, 2 make up the whole data; but maybe the resolution in Louvain is somewhat set to a high value. In any case, I'll change the implementation so that irrespective of whether `groups` is passed or not, one gets the same result.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:758,deployability,log,logistic,758,"Yes, right, for `logreg` you always regress on data that includes the respective group as this is not a differential test, but a regression. It's a shame that this is not evident from the documentation. It's sort of a recent alternative way of approaching the definition of marker genes and we should give it a bit more of a thought. Regarding the behavior that is inconsistent up there, there is the following in the code by the person who extended this not long ago:. ```. # if reference is not set, then the groups listed will be compared to the rest. # if reference is set, then the groups listed will be compared only to the other groups listed. from sklearn.linear_model import LogisticRegression. if len(groups) == 1:. raise Exception('Cannot perform logistic regression on a single cluster.'). adata_copy = adata[adata.obs[groupby].isin(groups_order)] . adata_comp = adata_copy. if adata.raw is not None and use_raw:. adata_comp = adata_copy.raw. X = adata_comp.X. clf = LogisticRegression(**kwds). clf.fit(X, adata_copy.obs[groupby].cat.codes). ```. You're right that logreg only includes the passed groups, if groups are passed. This should not be the case. I wonder why it's a problem in your specific case as I'd expect that 0, 1, 2 make up the whole data; but maybe the resolution in Louvain is somewhat set to a high value. In any case, I'll change the implementation so that irrespective of whether `groups` is passed or not, one gets the same result.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:790,deployability,cluster,cluster,790,"Yes, right, for `logreg` you always regress on data that includes the respective group as this is not a differential test, but a regression. It's a shame that this is not evident from the documentation. It's sort of a recent alternative way of approaching the definition of marker genes and we should give it a bit more of a thought. Regarding the behavior that is inconsistent up there, there is the following in the code by the person who extended this not long ago:. ```. # if reference is not set, then the groups listed will be compared to the rest. # if reference is set, then the groups listed will be compared only to the other groups listed. from sklearn.linear_model import LogisticRegression. if len(groups) == 1:. raise Exception('Cannot perform logistic regression on a single cluster.'). adata_copy = adata[adata.obs[groupby].isin(groups_order)] . adata_comp = adata_copy. if adata.raw is not None and use_raw:. adata_comp = adata_copy.raw. X = adata_comp.X. clf = LogisticRegression(**kwds). clf.fit(X, adata_copy.obs[groupby].cat.codes). ```. You're right that logreg only includes the passed groups, if groups are passed. This should not be the case. I wonder why it's a problem in your specific case as I'd expect that 0, 1, 2 make up the whole data; but maybe the resolution in Louvain is somewhat set to a high value. In any case, I'll change the implementation so that irrespective of whether `groups` is passed or not, one gets the same result.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:979,deployability,Log,LogisticRegression,979,"Yes, right, for `logreg` you always regress on data that includes the respective group as this is not a differential test, but a regression. It's a shame that this is not evident from the documentation. It's sort of a recent alternative way of approaching the definition of marker genes and we should give it a bit more of a thought. Regarding the behavior that is inconsistent up there, there is the following in the code by the person who extended this not long ago:. ```. # if reference is not set, then the groups listed will be compared to the rest. # if reference is set, then the groups listed will be compared only to the other groups listed. from sklearn.linear_model import LogisticRegression. if len(groups) == 1:. raise Exception('Cannot perform logistic regression on a single cluster.'). adata_copy = adata[adata.obs[groupby].isin(groups_order)] . adata_comp = adata_copy. if adata.raw is not None and use_raw:. adata_comp = adata_copy.raw. X = adata_comp.X. clf = LogisticRegression(**kwds). clf.fit(X, adata_copy.obs[groupby].cat.codes). ```. You're right that logreg only includes the passed groups, if groups are passed. This should not be the case. I wonder why it's a problem in your specific case as I'd expect that 0, 1, 2 make up the whole data; but maybe the resolution in Louvain is somewhat set to a high value. In any case, I'll change the implementation so that irrespective of whether `groups` is passed or not, one gets the same result.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:1077,deployability,log,logreg,1077,"Yes, right, for `logreg` you always regress on data that includes the respective group as this is not a differential test, but a regression. It's a shame that this is not evident from the documentation. It's sort of a recent alternative way of approaching the definition of marker genes and we should give it a bit more of a thought. Regarding the behavior that is inconsistent up there, there is the following in the code by the person who extended this not long ago:. ```. # if reference is not set, then the groups listed will be compared to the rest. # if reference is set, then the groups listed will be compared only to the other groups listed. from sklearn.linear_model import LogisticRegression. if len(groups) == 1:. raise Exception('Cannot perform logistic regression on a single cluster.'). adata_copy = adata[adata.obs[groupby].isin(groups_order)] . adata_comp = adata_copy. if adata.raw is not None and use_raw:. adata_comp = adata_copy.raw. X = adata_comp.X. clf = LogisticRegression(**kwds). clf.fit(X, adata_copy.obs[groupby].cat.codes). ```. You're right that logreg only includes the passed groups, if groups are passed. This should not be the case. I wonder why it's a problem in your specific case as I'd expect that 0, 1, 2 make up the whole data; but maybe the resolution in Louvain is somewhat set to a high value. In any case, I'll change the implementation so that irrespective of whether `groups` is passed or not, one gets the same result.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:1204,interoperability,specif,specific,1204,"Yes, right, for `logreg` you always regress on data that includes the respective group as this is not a differential test, but a regression. It's a shame that this is not evident from the documentation. It's sort of a recent alternative way of approaching the definition of marker genes and we should give it a bit more of a thought. Regarding the behavior that is inconsistent up there, there is the following in the code by the person who extended this not long ago:. ```. # if reference is not set, then the groups listed will be compared to the rest. # if reference is set, then the groups listed will be compared only to the other groups listed. from sklearn.linear_model import LogisticRegression. if len(groups) == 1:. raise Exception('Cannot perform logistic regression on a single cluster.'). adata_copy = adata[adata.obs[groupby].isin(groups_order)] . adata_comp = adata_copy. if adata.raw is not None and use_raw:. adata_comp = adata_copy.raw. X = adata_comp.X. clf = LogisticRegression(**kwds). clf.fit(X, adata_copy.obs[groupby].cat.codes). ```. You're right that logreg only includes the passed groups, if groups are passed. This should not be the case. I wonder why it's a problem in your specific case as I'd expect that 0, 1, 2 make up the whole data; but maybe the resolution in Louvain is somewhat set to a high value. In any case, I'll change the implementation so that irrespective of whether `groups` is passed or not, one gets the same result.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:441,modifiability,exten,extended,441,"Yes, right, for `logreg` you always regress on data that includes the respective group as this is not a differential test, but a regression. It's a shame that this is not evident from the documentation. It's sort of a recent alternative way of approaching the definition of marker genes and we should give it a bit more of a thought. Regarding the behavior that is inconsistent up there, there is the following in the code by the person who extended this not long ago:. ```. # if reference is not set, then the groups listed will be compared to the rest. # if reference is set, then the groups listed will be compared only to the other groups listed. from sklearn.linear_model import LogisticRegression. if len(groups) == 1:. raise Exception('Cannot perform logistic regression on a single cluster.'). adata_copy = adata[adata.obs[groupby].isin(groups_order)] . adata_comp = adata_copy. if adata.raw is not None and use_raw:. adata_comp = adata_copy.raw. X = adata_comp.X. clf = LogisticRegression(**kwds). clf.fit(X, adata_copy.obs[groupby].cat.codes). ```. You're right that logreg only includes the passed groups, if groups are passed. This should not be the case. I wonder why it's a problem in your specific case as I'd expect that 0, 1, 2 make up the whole data; but maybe the resolution in Louvain is somewhat set to a high value. In any case, I'll change the implementation so that irrespective of whether `groups` is passed or not, one gets the same result.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:750,performance,perform,perform,750,"Yes, right, for `logreg` you always regress on data that includes the respective group as this is not a differential test, but a regression. It's a shame that this is not evident from the documentation. It's sort of a recent alternative way of approaching the definition of marker genes and we should give it a bit more of a thought. Regarding the behavior that is inconsistent up there, there is the following in the code by the person who extended this not long ago:. ```. # if reference is not set, then the groups listed will be compared to the rest. # if reference is set, then the groups listed will be compared only to the other groups listed. from sklearn.linear_model import LogisticRegression. if len(groups) == 1:. raise Exception('Cannot perform logistic regression on a single cluster.'). adata_copy = adata[adata.obs[groupby].isin(groups_order)] . adata_comp = adata_copy. if adata.raw is not None and use_raw:. adata_comp = adata_copy.raw. X = adata_comp.X. clf = LogisticRegression(**kwds). clf.fit(X, adata_copy.obs[groupby].cat.codes). ```. You're right that logreg only includes the passed groups, if groups are passed. This should not be the case. I wonder why it's a problem in your specific case as I'd expect that 0, 1, 2 make up the whole data; but maybe the resolution in Louvain is somewhat set to a high value. In any case, I'll change the implementation so that irrespective of whether `groups` is passed or not, one gets the same result.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:17,safety,log,logreg,17,"Yes, right, for `logreg` you always regress on data that includes the respective group as this is not a differential test, but a regression. It's a shame that this is not evident from the documentation. It's sort of a recent alternative way of approaching the definition of marker genes and we should give it a bit more of a thought. Regarding the behavior that is inconsistent up there, there is the following in the code by the person who extended this not long ago:. ```. # if reference is not set, then the groups listed will be compared to the rest. # if reference is set, then the groups listed will be compared only to the other groups listed. from sklearn.linear_model import LogisticRegression. if len(groups) == 1:. raise Exception('Cannot perform logistic regression on a single cluster.'). adata_copy = adata[adata.obs[groupby].isin(groups_order)] . adata_comp = adata_copy. if adata.raw is not None and use_raw:. adata_comp = adata_copy.raw. X = adata_comp.X. clf = LogisticRegression(**kwds). clf.fit(X, adata_copy.obs[groupby].cat.codes). ```. You're right that logreg only includes the passed groups, if groups are passed. This should not be the case. I wonder why it's a problem in your specific case as I'd expect that 0, 1, 2 make up the whole data; but maybe the resolution in Louvain is somewhat set to a high value. In any case, I'll change the implementation so that irrespective of whether `groups` is passed or not, one gets the same result.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:117,safety,test,test,117,"Yes, right, for `logreg` you always regress on data that includes the respective group as this is not a differential test, but a regression. It's a shame that this is not evident from the documentation. It's sort of a recent alternative way of approaching the definition of marker genes and we should give it a bit more of a thought. Regarding the behavior that is inconsistent up there, there is the following in the code by the person who extended this not long ago:. ```. # if reference is not set, then the groups listed will be compared to the rest. # if reference is set, then the groups listed will be compared only to the other groups listed. from sklearn.linear_model import LogisticRegression. if len(groups) == 1:. raise Exception('Cannot perform logistic regression on a single cluster.'). adata_copy = adata[adata.obs[groupby].isin(groups_order)] . adata_comp = adata_copy. if adata.raw is not None and use_raw:. adata_comp = adata_copy.raw. X = adata_comp.X. clf = LogisticRegression(**kwds). clf.fit(X, adata_copy.obs[groupby].cat.codes). ```. You're right that logreg only includes the passed groups, if groups are passed. This should not be the case. I wonder why it's a problem in your specific case as I'd expect that 0, 1, 2 make up the whole data; but maybe the resolution in Louvain is somewhat set to a high value. In any case, I'll change the implementation so that irrespective of whether `groups` is passed or not, one gets the same result.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:684,safety,Log,LogisticRegression,684,"Yes, right, for `logreg` you always regress on data that includes the respective group as this is not a differential test, but a regression. It's a shame that this is not evident from the documentation. It's sort of a recent alternative way of approaching the definition of marker genes and we should give it a bit more of a thought. Regarding the behavior that is inconsistent up there, there is the following in the code by the person who extended this not long ago:. ```. # if reference is not set, then the groups listed will be compared to the rest. # if reference is set, then the groups listed will be compared only to the other groups listed. from sklearn.linear_model import LogisticRegression. if len(groups) == 1:. raise Exception('Cannot perform logistic regression on a single cluster.'). adata_copy = adata[adata.obs[groupby].isin(groups_order)] . adata_comp = adata_copy. if adata.raw is not None and use_raw:. adata_comp = adata_copy.raw. X = adata_comp.X. clf = LogisticRegression(**kwds). clf.fit(X, adata_copy.obs[groupby].cat.codes). ```. You're right that logreg only includes the passed groups, if groups are passed. This should not be the case. I wonder why it's a problem in your specific case as I'd expect that 0, 1, 2 make up the whole data; but maybe the resolution in Louvain is somewhat set to a high value. In any case, I'll change the implementation so that irrespective of whether `groups` is passed or not, one gets the same result.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:732,safety,Except,Exception,732,"Yes, right, for `logreg` you always regress on data that includes the respective group as this is not a differential test, but a regression. It's a shame that this is not evident from the documentation. It's sort of a recent alternative way of approaching the definition of marker genes and we should give it a bit more of a thought. Regarding the behavior that is inconsistent up there, there is the following in the code by the person who extended this not long ago:. ```. # if reference is not set, then the groups listed will be compared to the rest. # if reference is set, then the groups listed will be compared only to the other groups listed. from sklearn.linear_model import LogisticRegression. if len(groups) == 1:. raise Exception('Cannot perform logistic regression on a single cluster.'). adata_copy = adata[adata.obs[groupby].isin(groups_order)] . adata_comp = adata_copy. if adata.raw is not None and use_raw:. adata_comp = adata_copy.raw. X = adata_comp.X. clf = LogisticRegression(**kwds). clf.fit(X, adata_copy.obs[groupby].cat.codes). ```. You're right that logreg only includes the passed groups, if groups are passed. This should not be the case. I wonder why it's a problem in your specific case as I'd expect that 0, 1, 2 make up the whole data; but maybe the resolution in Louvain is somewhat set to a high value. In any case, I'll change the implementation so that irrespective of whether `groups` is passed or not, one gets the same result.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:758,safety,log,logistic,758,"Yes, right, for `logreg` you always regress on data that includes the respective group as this is not a differential test, but a regression. It's a shame that this is not evident from the documentation. It's sort of a recent alternative way of approaching the definition of marker genes and we should give it a bit more of a thought. Regarding the behavior that is inconsistent up there, there is the following in the code by the person who extended this not long ago:. ```. # if reference is not set, then the groups listed will be compared to the rest. # if reference is set, then the groups listed will be compared only to the other groups listed. from sklearn.linear_model import LogisticRegression. if len(groups) == 1:. raise Exception('Cannot perform logistic regression on a single cluster.'). adata_copy = adata[adata.obs[groupby].isin(groups_order)] . adata_comp = adata_copy. if adata.raw is not None and use_raw:. adata_comp = adata_copy.raw. X = adata_comp.X. clf = LogisticRegression(**kwds). clf.fit(X, adata_copy.obs[groupby].cat.codes). ```. You're right that logreg only includes the passed groups, if groups are passed. This should not be the case. I wonder why it's a problem in your specific case as I'd expect that 0, 1, 2 make up the whole data; but maybe the resolution in Louvain is somewhat set to a high value. In any case, I'll change the implementation so that irrespective of whether `groups` is passed or not, one gets the same result.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:979,safety,Log,LogisticRegression,979,"Yes, right, for `logreg` you always regress on data that includes the respective group as this is not a differential test, but a regression. It's a shame that this is not evident from the documentation. It's sort of a recent alternative way of approaching the definition of marker genes and we should give it a bit more of a thought. Regarding the behavior that is inconsistent up there, there is the following in the code by the person who extended this not long ago:. ```. # if reference is not set, then the groups listed will be compared to the rest. # if reference is set, then the groups listed will be compared only to the other groups listed. from sklearn.linear_model import LogisticRegression. if len(groups) == 1:. raise Exception('Cannot perform logistic regression on a single cluster.'). adata_copy = adata[adata.obs[groupby].isin(groups_order)] . adata_comp = adata_copy. if adata.raw is not None and use_raw:. adata_comp = adata_copy.raw. X = adata_comp.X. clf = LogisticRegression(**kwds). clf.fit(X, adata_copy.obs[groupby].cat.codes). ```. You're right that logreg only includes the passed groups, if groups are passed. This should not be the case. I wonder why it's a problem in your specific case as I'd expect that 0, 1, 2 make up the whole data; but maybe the resolution in Louvain is somewhat set to a high value. In any case, I'll change the implementation so that irrespective of whether `groups` is passed or not, one gets the same result.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:1077,safety,log,logreg,1077,"Yes, right, for `logreg` you always regress on data that includes the respective group as this is not a differential test, but a regression. It's a shame that this is not evident from the documentation. It's sort of a recent alternative way of approaching the definition of marker genes and we should give it a bit more of a thought. Regarding the behavior that is inconsistent up there, there is the following in the code by the person who extended this not long ago:. ```. # if reference is not set, then the groups listed will be compared to the rest. # if reference is set, then the groups listed will be compared only to the other groups listed. from sklearn.linear_model import LogisticRegression. if len(groups) == 1:. raise Exception('Cannot perform logistic regression on a single cluster.'). adata_copy = adata[adata.obs[groupby].isin(groups_order)] . adata_comp = adata_copy. if adata.raw is not None and use_raw:. adata_comp = adata_copy.raw. X = adata_comp.X. clf = LogisticRegression(**kwds). clf.fit(X, adata_copy.obs[groupby].cat.codes). ```. You're right that logreg only includes the passed groups, if groups are passed. This should not be the case. I wonder why it's a problem in your specific case as I'd expect that 0, 1, 2 make up the whole data; but maybe the resolution in Louvain is somewhat set to a high value. In any case, I'll change the implementation so that irrespective of whether `groups` is passed or not, one gets the same result.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:17,security,log,logreg,17,"Yes, right, for `logreg` you always regress on data that includes the respective group as this is not a differential test, but a regression. It's a shame that this is not evident from the documentation. It's sort of a recent alternative way of approaching the definition of marker genes and we should give it a bit more of a thought. Regarding the behavior that is inconsistent up there, there is the following in the code by the person who extended this not long ago:. ```. # if reference is not set, then the groups listed will be compared to the rest. # if reference is set, then the groups listed will be compared only to the other groups listed. from sklearn.linear_model import LogisticRegression. if len(groups) == 1:. raise Exception('Cannot perform logistic regression on a single cluster.'). adata_copy = adata[adata.obs[groupby].isin(groups_order)] . adata_comp = adata_copy. if adata.raw is not None and use_raw:. adata_comp = adata_copy.raw. X = adata_comp.X. clf = LogisticRegression(**kwds). clf.fit(X, adata_copy.obs[groupby].cat.codes). ```. You're right that logreg only includes the passed groups, if groups are passed. This should not be the case. I wonder why it's a problem in your specific case as I'd expect that 0, 1, 2 make up the whole data; but maybe the resolution in Louvain is somewhat set to a high value. In any case, I'll change the implementation so that irrespective of whether `groups` is passed or not, one gets the same result.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:684,security,Log,LogisticRegression,684,"Yes, right, for `logreg` you always regress on data that includes the respective group as this is not a differential test, but a regression. It's a shame that this is not evident from the documentation. It's sort of a recent alternative way of approaching the definition of marker genes and we should give it a bit more of a thought. Regarding the behavior that is inconsistent up there, there is the following in the code by the person who extended this not long ago:. ```. # if reference is not set, then the groups listed will be compared to the rest. # if reference is set, then the groups listed will be compared only to the other groups listed. from sklearn.linear_model import LogisticRegression. if len(groups) == 1:. raise Exception('Cannot perform logistic regression on a single cluster.'). adata_copy = adata[adata.obs[groupby].isin(groups_order)] . adata_comp = adata_copy. if adata.raw is not None and use_raw:. adata_comp = adata_copy.raw. X = adata_comp.X. clf = LogisticRegression(**kwds). clf.fit(X, adata_copy.obs[groupby].cat.codes). ```. You're right that logreg only includes the passed groups, if groups are passed. This should not be the case. I wonder why it's a problem in your specific case as I'd expect that 0, 1, 2 make up the whole data; but maybe the resolution in Louvain is somewhat set to a high value. In any case, I'll change the implementation so that irrespective of whether `groups` is passed or not, one gets the same result.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:758,security,log,logistic,758,"Yes, right, for `logreg` you always regress on data that includes the respective group as this is not a differential test, but a regression. It's a shame that this is not evident from the documentation. It's sort of a recent alternative way of approaching the definition of marker genes and we should give it a bit more of a thought. Regarding the behavior that is inconsistent up there, there is the following in the code by the person who extended this not long ago:. ```. # if reference is not set, then the groups listed will be compared to the rest. # if reference is set, then the groups listed will be compared only to the other groups listed. from sklearn.linear_model import LogisticRegression. if len(groups) == 1:. raise Exception('Cannot perform logistic regression on a single cluster.'). adata_copy = adata[adata.obs[groupby].isin(groups_order)] . adata_comp = adata_copy. if adata.raw is not None and use_raw:. adata_comp = adata_copy.raw. X = adata_comp.X. clf = LogisticRegression(**kwds). clf.fit(X, adata_copy.obs[groupby].cat.codes). ```. You're right that logreg only includes the passed groups, if groups are passed. This should not be the case. I wonder why it's a problem in your specific case as I'd expect that 0, 1, 2 make up the whole data; but maybe the resolution in Louvain is somewhat set to a high value. In any case, I'll change the implementation so that irrespective of whether `groups` is passed or not, one gets the same result.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:979,security,Log,LogisticRegression,979,"Yes, right, for `logreg` you always regress on data that includes the respective group as this is not a differential test, but a regression. It's a shame that this is not evident from the documentation. It's sort of a recent alternative way of approaching the definition of marker genes and we should give it a bit more of a thought. Regarding the behavior that is inconsistent up there, there is the following in the code by the person who extended this not long ago:. ```. # if reference is not set, then the groups listed will be compared to the rest. # if reference is set, then the groups listed will be compared only to the other groups listed. from sklearn.linear_model import LogisticRegression. if len(groups) == 1:. raise Exception('Cannot perform logistic regression on a single cluster.'). adata_copy = adata[adata.obs[groupby].isin(groups_order)] . adata_comp = adata_copy. if adata.raw is not None and use_raw:. adata_comp = adata_copy.raw. X = adata_comp.X. clf = LogisticRegression(**kwds). clf.fit(X, adata_copy.obs[groupby].cat.codes). ```. You're right that logreg only includes the passed groups, if groups are passed. This should not be the case. I wonder why it's a problem in your specific case as I'd expect that 0, 1, 2 make up the whole data; but maybe the resolution in Louvain is somewhat set to a high value. In any case, I'll change the implementation so that irrespective of whether `groups` is passed or not, one gets the same result.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:1077,security,log,logreg,1077,"Yes, right, for `logreg` you always regress on data that includes the respective group as this is not a differential test, but a regression. It's a shame that this is not evident from the documentation. It's sort of a recent alternative way of approaching the definition of marker genes and we should give it a bit more of a thought. Regarding the behavior that is inconsistent up there, there is the following in the code by the person who extended this not long ago:. ```. # if reference is not set, then the groups listed will be compared to the rest. # if reference is set, then the groups listed will be compared only to the other groups listed. from sklearn.linear_model import LogisticRegression. if len(groups) == 1:. raise Exception('Cannot perform logistic regression on a single cluster.'). adata_copy = adata[adata.obs[groupby].isin(groups_order)] . adata_comp = adata_copy. if adata.raw is not None and use_raw:. adata_comp = adata_copy.raw. X = adata_comp.X. clf = LogisticRegression(**kwds). clf.fit(X, adata_copy.obs[groupby].cat.codes). ```. You're right that logreg only includes the passed groups, if groups are passed. This should not be the case. I wonder why it's a problem in your specific case as I'd expect that 0, 1, 2 make up the whole data; but maybe the resolution in Louvain is somewhat set to a high value. In any case, I'll change the implementation so that irrespective of whether `groups` is passed or not, one gets the same result.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:17,testability,log,logreg,17,"Yes, right, for `logreg` you always regress on data that includes the respective group as this is not a differential test, but a regression. It's a shame that this is not evident from the documentation. It's sort of a recent alternative way of approaching the definition of marker genes and we should give it a bit more of a thought. Regarding the behavior that is inconsistent up there, there is the following in the code by the person who extended this not long ago:. ```. # if reference is not set, then the groups listed will be compared to the rest. # if reference is set, then the groups listed will be compared only to the other groups listed. from sklearn.linear_model import LogisticRegression. if len(groups) == 1:. raise Exception('Cannot perform logistic regression on a single cluster.'). adata_copy = adata[adata.obs[groupby].isin(groups_order)] . adata_comp = adata_copy. if adata.raw is not None and use_raw:. adata_comp = adata_copy.raw. X = adata_comp.X. clf = LogisticRegression(**kwds). clf.fit(X, adata_copy.obs[groupby].cat.codes). ```. You're right that logreg only includes the passed groups, if groups are passed. This should not be the case. I wonder why it's a problem in your specific case as I'd expect that 0, 1, 2 make up the whole data; but maybe the resolution in Louvain is somewhat set to a high value. In any case, I'll change the implementation so that irrespective of whether `groups` is passed or not, one gets the same result.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:36,testability,regress,regress,36,"Yes, right, for `logreg` you always regress on data that includes the respective group as this is not a differential test, but a regression. It's a shame that this is not evident from the documentation. It's sort of a recent alternative way of approaching the definition of marker genes and we should give it a bit more of a thought. Regarding the behavior that is inconsistent up there, there is the following in the code by the person who extended this not long ago:. ```. # if reference is not set, then the groups listed will be compared to the rest. # if reference is set, then the groups listed will be compared only to the other groups listed. from sklearn.linear_model import LogisticRegression. if len(groups) == 1:. raise Exception('Cannot perform logistic regression on a single cluster.'). adata_copy = adata[adata.obs[groupby].isin(groups_order)] . adata_comp = adata_copy. if adata.raw is not None and use_raw:. adata_comp = adata_copy.raw. X = adata_comp.X. clf = LogisticRegression(**kwds). clf.fit(X, adata_copy.obs[groupby].cat.codes). ```. You're right that logreg only includes the passed groups, if groups are passed. This should not be the case. I wonder why it's a problem in your specific case as I'd expect that 0, 1, 2 make up the whole data; but maybe the resolution in Louvain is somewhat set to a high value. In any case, I'll change the implementation so that irrespective of whether `groups` is passed or not, one gets the same result.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:117,testability,test,test,117,"Yes, right, for `logreg` you always regress on data that includes the respective group as this is not a differential test, but a regression. It's a shame that this is not evident from the documentation. It's sort of a recent alternative way of approaching the definition of marker genes and we should give it a bit more of a thought. Regarding the behavior that is inconsistent up there, there is the following in the code by the person who extended this not long ago:. ```. # if reference is not set, then the groups listed will be compared to the rest. # if reference is set, then the groups listed will be compared only to the other groups listed. from sklearn.linear_model import LogisticRegression. if len(groups) == 1:. raise Exception('Cannot perform logistic regression on a single cluster.'). adata_copy = adata[adata.obs[groupby].isin(groups_order)] . adata_comp = adata_copy. if adata.raw is not None and use_raw:. adata_comp = adata_copy.raw. X = adata_comp.X. clf = LogisticRegression(**kwds). clf.fit(X, adata_copy.obs[groupby].cat.codes). ```. You're right that logreg only includes the passed groups, if groups are passed. This should not be the case. I wonder why it's a problem in your specific case as I'd expect that 0, 1, 2 make up the whole data; but maybe the resolution in Louvain is somewhat set to a high value. In any case, I'll change the implementation so that irrespective of whether `groups` is passed or not, one gets the same result.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:129,testability,regress,regression,129,"Yes, right, for `logreg` you always regress on data that includes the respective group as this is not a differential test, but a regression. It's a shame that this is not evident from the documentation. It's sort of a recent alternative way of approaching the definition of marker genes and we should give it a bit more of a thought. Regarding the behavior that is inconsistent up there, there is the following in the code by the person who extended this not long ago:. ```. # if reference is not set, then the groups listed will be compared to the rest. # if reference is set, then the groups listed will be compared only to the other groups listed. from sklearn.linear_model import LogisticRegression. if len(groups) == 1:. raise Exception('Cannot perform logistic regression on a single cluster.'). adata_copy = adata[adata.obs[groupby].isin(groups_order)] . adata_comp = adata_copy. if adata.raw is not None and use_raw:. adata_comp = adata_copy.raw. X = adata_comp.X. clf = LogisticRegression(**kwds). clf.fit(X, adata_copy.obs[groupby].cat.codes). ```. You're right that logreg only includes the passed groups, if groups are passed. This should not be the case. I wonder why it's a problem in your specific case as I'd expect that 0, 1, 2 make up the whole data; but maybe the resolution in Louvain is somewhat set to a high value. In any case, I'll change the implementation so that irrespective of whether `groups` is passed or not, one gets the same result.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:684,testability,Log,LogisticRegression,684,"Yes, right, for `logreg` you always regress on data that includes the respective group as this is not a differential test, but a regression. It's a shame that this is not evident from the documentation. It's sort of a recent alternative way of approaching the definition of marker genes and we should give it a bit more of a thought. Regarding the behavior that is inconsistent up there, there is the following in the code by the person who extended this not long ago:. ```. # if reference is not set, then the groups listed will be compared to the rest. # if reference is set, then the groups listed will be compared only to the other groups listed. from sklearn.linear_model import LogisticRegression. if len(groups) == 1:. raise Exception('Cannot perform logistic regression on a single cluster.'). adata_copy = adata[adata.obs[groupby].isin(groups_order)] . adata_comp = adata_copy. if adata.raw is not None and use_raw:. adata_comp = adata_copy.raw. X = adata_comp.X. clf = LogisticRegression(**kwds). clf.fit(X, adata_copy.obs[groupby].cat.codes). ```. You're right that logreg only includes the passed groups, if groups are passed. This should not be the case. I wonder why it's a problem in your specific case as I'd expect that 0, 1, 2 make up the whole data; but maybe the resolution in Louvain is somewhat set to a high value. In any case, I'll change the implementation so that irrespective of whether `groups` is passed or not, one gets the same result.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:758,testability,log,logistic,758,"Yes, right, for `logreg` you always regress on data that includes the respective group as this is not a differential test, but a regression. It's a shame that this is not evident from the documentation. It's sort of a recent alternative way of approaching the definition of marker genes and we should give it a bit more of a thought. Regarding the behavior that is inconsistent up there, there is the following in the code by the person who extended this not long ago:. ```. # if reference is not set, then the groups listed will be compared to the rest. # if reference is set, then the groups listed will be compared only to the other groups listed. from sklearn.linear_model import LogisticRegression. if len(groups) == 1:. raise Exception('Cannot perform logistic regression on a single cluster.'). adata_copy = adata[adata.obs[groupby].isin(groups_order)] . adata_comp = adata_copy. if adata.raw is not None and use_raw:. adata_comp = adata_copy.raw. X = adata_comp.X. clf = LogisticRegression(**kwds). clf.fit(X, adata_copy.obs[groupby].cat.codes). ```. You're right that logreg only includes the passed groups, if groups are passed. This should not be the case. I wonder why it's a problem in your specific case as I'd expect that 0, 1, 2 make up the whole data; but maybe the resolution in Louvain is somewhat set to a high value. In any case, I'll change the implementation so that irrespective of whether `groups` is passed or not, one gets the same result.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:767,testability,regress,regression,767,"Yes, right, for `logreg` you always regress on data that includes the respective group as this is not a differential test, but a regression. It's a shame that this is not evident from the documentation. It's sort of a recent alternative way of approaching the definition of marker genes and we should give it a bit more of a thought. Regarding the behavior that is inconsistent up there, there is the following in the code by the person who extended this not long ago:. ```. # if reference is not set, then the groups listed will be compared to the rest. # if reference is set, then the groups listed will be compared only to the other groups listed. from sklearn.linear_model import LogisticRegression. if len(groups) == 1:. raise Exception('Cannot perform logistic regression on a single cluster.'). adata_copy = adata[adata.obs[groupby].isin(groups_order)] . adata_comp = adata_copy. if adata.raw is not None and use_raw:. adata_comp = adata_copy.raw. X = adata_comp.X. clf = LogisticRegression(**kwds). clf.fit(X, adata_copy.obs[groupby].cat.codes). ```. You're right that logreg only includes the passed groups, if groups are passed. This should not be the case. I wonder why it's a problem in your specific case as I'd expect that 0, 1, 2 make up the whole data; but maybe the resolution in Louvain is somewhat set to a high value. In any case, I'll change the implementation so that irrespective of whether `groups` is passed or not, one gets the same result.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:979,testability,Log,LogisticRegression,979,"Yes, right, for `logreg` you always regress on data that includes the respective group as this is not a differential test, but a regression. It's a shame that this is not evident from the documentation. It's sort of a recent alternative way of approaching the definition of marker genes and we should give it a bit more of a thought. Regarding the behavior that is inconsistent up there, there is the following in the code by the person who extended this not long ago:. ```. # if reference is not set, then the groups listed will be compared to the rest. # if reference is set, then the groups listed will be compared only to the other groups listed. from sklearn.linear_model import LogisticRegression. if len(groups) == 1:. raise Exception('Cannot perform logistic regression on a single cluster.'). adata_copy = adata[adata.obs[groupby].isin(groups_order)] . adata_comp = adata_copy. if adata.raw is not None and use_raw:. adata_comp = adata_copy.raw. X = adata_comp.X. clf = LogisticRegression(**kwds). clf.fit(X, adata_copy.obs[groupby].cat.codes). ```. You're right that logreg only includes the passed groups, if groups are passed. This should not be the case. I wonder why it's a problem in your specific case as I'd expect that 0, 1, 2 make up the whole data; but maybe the resolution in Louvain is somewhat set to a high value. In any case, I'll change the implementation so that irrespective of whether `groups` is passed or not, one gets the same result.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:1077,testability,log,logreg,1077,"Yes, right, for `logreg` you always regress on data that includes the respective group as this is not a differential test, but a regression. It's a shame that this is not evident from the documentation. It's sort of a recent alternative way of approaching the definition of marker genes and we should give it a bit more of a thought. Regarding the behavior that is inconsistent up there, there is the following in the code by the person who extended this not long ago:. ```. # if reference is not set, then the groups listed will be compared to the rest. # if reference is set, then the groups listed will be compared only to the other groups listed. from sklearn.linear_model import LogisticRegression. if len(groups) == 1:. raise Exception('Cannot perform logistic regression on a single cluster.'). adata_copy = adata[adata.obs[groupby].isin(groups_order)] . adata_comp = adata_copy. if adata.raw is not None and use_raw:. adata_comp = adata_copy.raw. X = adata_comp.X. clf = LogisticRegression(**kwds). clf.fit(X, adata_copy.obs[groupby].cat.codes). ```. You're right that logreg only includes the passed groups, if groups are passed. This should not be the case. I wonder why it's a problem in your specific case as I'd expect that 0, 1, 2 make up the whole data; but maybe the resolution in Louvain is somewhat set to a high value. In any case, I'll change the implementation so that irrespective of whether `groups` is passed or not, one gets the same result.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:188,usability,document,documentation,188,"Yes, right, for `logreg` you always regress on data that includes the respective group as this is not a differential test, but a regression. It's a shame that this is not evident from the documentation. It's sort of a recent alternative way of approaching the definition of marker genes and we should give it a bit more of a thought. Regarding the behavior that is inconsistent up there, there is the following in the code by the person who extended this not long ago:. ```. # if reference is not set, then the groups listed will be compared to the rest. # if reference is set, then the groups listed will be compared only to the other groups listed. from sklearn.linear_model import LogisticRegression. if len(groups) == 1:. raise Exception('Cannot perform logistic regression on a single cluster.'). adata_copy = adata[adata.obs[groupby].isin(groups_order)] . adata_comp = adata_copy. if adata.raw is not None and use_raw:. adata_comp = adata_copy.raw. X = adata_comp.X. clf = LogisticRegression(**kwds). clf.fit(X, adata_copy.obs[groupby].cat.codes). ```. You're right that logreg only includes the passed groups, if groups are passed. This should not be the case. I wonder why it's a problem in your specific case as I'd expect that 0, 1, 2 make up the whole data; but maybe the resolution in Louvain is somewhat set to a high value. In any case, I'll change the implementation so that irrespective of whether `groups` is passed or not, one gets the same result.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:348,usability,behavi,behavior,348,"Yes, right, for `logreg` you always regress on data that includes the respective group as this is not a differential test, but a regression. It's a shame that this is not evident from the documentation. It's sort of a recent alternative way of approaching the definition of marker genes and we should give it a bit more of a thought. Regarding the behavior that is inconsistent up there, there is the following in the code by the person who extended this not long ago:. ```. # if reference is not set, then the groups listed will be compared to the rest. # if reference is set, then the groups listed will be compared only to the other groups listed. from sklearn.linear_model import LogisticRegression. if len(groups) == 1:. raise Exception('Cannot perform logistic regression on a single cluster.'). adata_copy = adata[adata.obs[groupby].isin(groups_order)] . adata_comp = adata_copy. if adata.raw is not None and use_raw:. adata_comp = adata_copy.raw. X = adata_comp.X. clf = LogisticRegression(**kwds). clf.fit(X, adata_copy.obs[groupby].cat.codes). ```. You're right that logreg only includes the passed groups, if groups are passed. This should not be the case. I wonder why it's a problem in your specific case as I'd expect that 0, 1, 2 make up the whole data; but maybe the resolution in Louvain is somewhat set to a high value. In any case, I'll change the implementation so that irrespective of whether `groups` is passed or not, one gets the same result.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:430,usability,person,person,430,"Yes, right, for `logreg` you always regress on data that includes the respective group as this is not a differential test, but a regression. It's a shame that this is not evident from the documentation. It's sort of a recent alternative way of approaching the definition of marker genes and we should give it a bit more of a thought. Regarding the behavior that is inconsistent up there, there is the following in the code by the person who extended this not long ago:. ```. # if reference is not set, then the groups listed will be compared to the rest. # if reference is set, then the groups listed will be compared only to the other groups listed. from sklearn.linear_model import LogisticRegression. if len(groups) == 1:. raise Exception('Cannot perform logistic regression on a single cluster.'). adata_copy = adata[adata.obs[groupby].isin(groups_order)] . adata_comp = adata_copy. if adata.raw is not None and use_raw:. adata_comp = adata_copy.raw. X = adata_comp.X. clf = LogisticRegression(**kwds). clf.fit(X, adata_copy.obs[groupby].cat.codes). ```. You're right that logreg only includes the passed groups, if groups are passed. This should not be the case. I wonder why it's a problem in your specific case as I'd expect that 0, 1, 2 make up the whole data; but maybe the resolution in Louvain is somewhat set to a high value. In any case, I'll change the implementation so that irrespective of whether `groups` is passed or not, one gets the same result.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:750,usability,perform,perform,750,"Yes, right, for `logreg` you always regress on data that includes the respective group as this is not a differential test, but a regression. It's a shame that this is not evident from the documentation. It's sort of a recent alternative way of approaching the definition of marker genes and we should give it a bit more of a thought. Regarding the behavior that is inconsistent up there, there is the following in the code by the person who extended this not long ago:. ```. # if reference is not set, then the groups listed will be compared to the rest. # if reference is set, then the groups listed will be compared only to the other groups listed. from sklearn.linear_model import LogisticRegression. if len(groups) == 1:. raise Exception('Cannot perform logistic regression on a single cluster.'). adata_copy = adata[adata.obs[groupby].isin(groups_order)] . adata_comp = adata_copy. if adata.raw is not None and use_raw:. adata_comp = adata_copy.raw. X = adata_comp.X. clf = LogisticRegression(**kwds). clf.fit(X, adata_copy.obs[groupby].cat.codes). ```. You're right that logreg only includes the passed groups, if groups are passed. This should not be the case. I wonder why it's a problem in your specific case as I'd expect that 0, 1, 2 make up the whole data; but maybe the resolution in Louvain is somewhat set to a high value. In any case, I'll change the implementation so that irrespective of whether `groups` is passed or not, one gets the same result.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:128,availability,cluster,clusters,128,"Hi @falexwolf, thanks a lot for the clarifications. This helps me a lot. In the example I provided yesterday, `louvain` found 5 clusters, so 0, 1, 2 made up only part of the data. I should have provided the output as well to make this clear right away. Concerning a PR for the documentation, I think I would wait until you will update the behaviour of `logreg`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:128,deployability,cluster,clusters,128,"Hi @falexwolf, thanks a lot for the clarifications. This helps me a lot. In the example I provided yesterday, `louvain` found 5 clusters, so 0, 1, 2 made up only part of the data. I should have provided the output as well to make this clear right away. Concerning a PR for the documentation, I think I would wait until you will update the behaviour of `logreg`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:328,deployability,updat,update,328,"Hi @falexwolf, thanks a lot for the clarifications. This helps me a lot. In the example I provided yesterday, `louvain` found 5 clusters, so 0, 1, 2 made up only part of the data. I should have provided the output as well to make this clear right away. Concerning a PR for the documentation, I think I would wait until you will update the behaviour of `logreg`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:353,deployability,log,logreg,353,"Hi @falexwolf, thanks a lot for the clarifications. This helps me a lot. In the example I provided yesterday, `louvain` found 5 clusters, so 0, 1, 2 made up only part of the data. I should have provided the output as well to make this clear right away. Concerning a PR for the documentation, I think I would wait until you will update the behaviour of `logreg`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:253,modifiability,Concern,Concerning,253,"Hi @falexwolf, thanks a lot for the clarifications. This helps me a lot. In the example I provided yesterday, `louvain` found 5 clusters, so 0, 1, 2 made up only part of the data. I should have provided the output as well to make this clear right away. Concerning a PR for the documentation, I think I would wait until you will update the behaviour of `logreg`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:328,safety,updat,update,328,"Hi @falexwolf, thanks a lot for the clarifications. This helps me a lot. In the example I provided yesterday, `louvain` found 5 clusters, so 0, 1, 2 made up only part of the data. I should have provided the output as well to make this clear right away. Concerning a PR for the documentation, I think I would wait until you will update the behaviour of `logreg`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:353,safety,log,logreg,353,"Hi @falexwolf, thanks a lot for the clarifications. This helps me a lot. In the example I provided yesterday, `louvain` found 5 clusters, so 0, 1, 2 made up only part of the data. I should have provided the output as well to make this clear right away. Concerning a PR for the documentation, I think I would wait until you will update the behaviour of `logreg`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:328,security,updat,update,328,"Hi @falexwolf, thanks a lot for the clarifications. This helps me a lot. In the example I provided yesterday, `louvain` found 5 clusters, so 0, 1, 2 made up only part of the data. I should have provided the output as well to make this clear right away. Concerning a PR for the documentation, I think I would wait until you will update the behaviour of `logreg`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:353,security,log,logreg,353,"Hi @falexwolf, thanks a lot for the clarifications. This helps me a lot. In the example I provided yesterday, `louvain` found 5 clusters, so 0, 1, 2 made up only part of the data. I should have provided the output as well to make this clear right away. Concerning a PR for the documentation, I think I would wait until you will update the behaviour of `logreg`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:253,testability,Concern,Concerning,253,"Hi @falexwolf, thanks a lot for the clarifications. This helps me a lot. In the example I provided yesterday, `louvain` found 5 clusters, so 0, 1, 2 made up only part of the data. I should have provided the output as well to make this clear right away. Concerning a PR for the documentation, I think I would wait until you will update the behaviour of `logreg`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:353,testability,log,logreg,353,"Hi @falexwolf, thanks a lot for the clarifications. This helps me a lot. In the example I provided yesterday, `louvain` found 5 clusters, so 0, 1, 2 made up only part of the data. I should have provided the output as well to make this clear right away. Concerning a PR for the documentation, I think I would wait until you will update the behaviour of `logreg`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:57,usability,help,helps,57,"Hi @falexwolf, thanks a lot for the clarifications. This helps me a lot. In the example I provided yesterday, `louvain` found 5 clusters, so 0, 1, 2 made up only part of the data. I should have provided the output as well to make this clear right away. Concerning a PR for the documentation, I think I would wait until you will update the behaviour of `logreg`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:235,usability,clear,clear,235,"Hi @falexwolf, thanks a lot for the clarifications. This helps me a lot. In the example I provided yesterday, `louvain` found 5 clusters, so 0, 1, 2 made up only part of the data. I should have provided the output as well to make this clear right away. Concerning a PR for the documentation, I think I would wait until you will update the behaviour of `logreg`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:277,usability,document,documentation,277,"Hi @falexwolf, thanks a lot for the clarifications. This helps me a lot. In the example I provided yesterday, `louvain` found 5 clusters, so 0, 1, 2 made up only part of the data. I should have provided the output as well to make this clear right away. Concerning a PR for the documentation, I think I would wait until you will update the behaviour of `logreg`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/278:339,usability,behavi,behaviour,339,"Hi @falexwolf, thanks a lot for the clarifications. This helps me a lot. In the example I provided yesterday, `louvain` found 5 clusters, so 0, 1, 2 made up only part of the data. I should have provided the output as well to make this clear right away. Concerning a PR for the documentation, I think I would wait until you will update the behaviour of `logreg`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/278
https://github.com/scverse/scanpy/issues/279:295,availability,cluster,clusters,295,This is really interesting behaviour. Generally your expectation is correct. My guess is that it is related to the random seed. The ordering of nodes for consideration in these runs is probably different. Could you set the `random_state` to a few different values to confirm you always get more clusters in the lower resolution?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/279
https://github.com/scverse/scanpy/issues/279:295,deployability,cluster,clusters,295,This is really interesting behaviour. Generally your expectation is correct. My guess is that it is related to the random seed. The ordering of nodes for consideration in these runs is probably different. Could you set the `random_state` to a few different values to confirm you always get more clusters in the lower resolution?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/279
https://github.com/scverse/scanpy/issues/279:27,usability,behavi,behaviour,27,This is really interesting behaviour. Generally your expectation is correct. My guess is that it is related to the random seed. The ordering of nodes for consideration in these runs is probably different. Could you set the `random_state` to a few different values to confirm you always get more clusters in the lower resolution?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/279
https://github.com/scverse/scanpy/issues/279:267,usability,confirm,confirm,267,This is really interesting behaviour. Generally your expectation is correct. My guess is that it is related to the random seed. The ordering of nodes for consideration in these runs is probably different. Could you set the `random_state` to a few different values to confirm you always get more clusters in the lower resolution?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/279
https://github.com/scverse/scanpy/issues/279:277,availability,state,state,277,"Apparently [louvain](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html#scanpy.api.tl.louvain) has a default value for `random_state` (0). I tried both with 0 and 42 as `random_state` just to be sure that 0 had not a special meaning (i.e. no default random state). Interestingly, with 0 I have the same behavior, and with 42 too - although for different values of resolution. So I think we can rule out the `random_state` initialization. I am not completely familiar with the internals of louvain, but may that be related to tie resolution during aggregation of points?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/279
https://github.com/scverse/scanpy/issues/279:61,deployability,api,api,61,"Apparently [louvain](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html#scanpy.api.tl.louvain) has a default value for `random_state` (0). I tried both with 0 and 42 as `random_state` just to be sure that 0 had not a special meaning (i.e. no default random state). Interestingly, with 0 I have the same behavior, and with 42 too - although for different values of resolution. So I think we can rule out the `random_state` initialization. I am not completely familiar with the internals of louvain, but may that be related to tie resolution during aggregation of points?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/279
https://github.com/scverse/scanpy/issues/279:72,deployability,api,api,72,"Apparently [louvain](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html#scanpy.api.tl.louvain) has a default value for `random_state` (0). I tried both with 0 and 42 as `random_state` just to be sure that 0 had not a special meaning (i.e. no default random state). Interestingly, with 0 I have the same behavior, and with 42 too - although for different values of resolution. So I think we can rule out the `random_state` initialization. I am not completely familiar with the internals of louvain, but may that be related to tie resolution during aggregation of points?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/279
https://github.com/scverse/scanpy/issues/279:99,deployability,api,api,99,"Apparently [louvain](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html#scanpy.api.tl.louvain) has a default value for `random_state` (0). I tried both with 0 and 42 as `random_state` just to be sure that 0 had not a special meaning (i.e. no default random state). Interestingly, with 0 I have the same behavior, and with 42 too - although for different values of resolution. So I think we can rule out the `random_state` initialization. I am not completely familiar with the internals of louvain, but may that be related to tie resolution during aggregation of points?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/279
https://github.com/scverse/scanpy/issues/279:61,integrability,api,api,61,"Apparently [louvain](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html#scanpy.api.tl.louvain) has a default value for `random_state` (0). I tried both with 0 and 42 as `random_state` just to be sure that 0 had not a special meaning (i.e. no default random state). Interestingly, with 0 I have the same behavior, and with 42 too - although for different values of resolution. So I think we can rule out the `random_state` initialization. I am not completely familiar with the internals of louvain, but may that be related to tie resolution during aggregation of points?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/279
https://github.com/scverse/scanpy/issues/279:72,integrability,api,api,72,"Apparently [louvain](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html#scanpy.api.tl.louvain) has a default value for `random_state` (0). I tried both with 0 and 42 as `random_state` just to be sure that 0 had not a special meaning (i.e. no default random state). Interestingly, with 0 I have the same behavior, and with 42 too - although for different values of resolution. So I think we can rule out the `random_state` initialization. I am not completely familiar with the internals of louvain, but may that be related to tie resolution during aggregation of points?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/279
https://github.com/scverse/scanpy/issues/279:99,integrability,api,api,99,"Apparently [louvain](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html#scanpy.api.tl.louvain) has a default value for `random_state` (0). I tried both with 0 and 42 as `random_state` just to be sure that 0 had not a special meaning (i.e. no default random state). Interestingly, with 0 I have the same behavior, and with 42 too - although for different values of resolution. So I think we can rule out the `random_state` initialization. I am not completely familiar with the internals of louvain, but may that be related to tie resolution during aggregation of points?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/279
https://github.com/scverse/scanpy/issues/279:277,integrability,state,state,277,"Apparently [louvain](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html#scanpy.api.tl.louvain) has a default value for `random_state` (0). I tried both with 0 and 42 as `random_state` just to be sure that 0 had not a special meaning (i.e. no default random state). Interestingly, with 0 I have the same behavior, and with 42 too - although for different values of resolution. So I think we can rule out the `random_state` initialization. I am not completely familiar with the internals of louvain, but may that be related to tie resolution during aggregation of points?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/279
https://github.com/scverse/scanpy/issues/279:61,interoperability,api,api,61,"Apparently [louvain](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html#scanpy.api.tl.louvain) has a default value for `random_state` (0). I tried both with 0 and 42 as `random_state` just to be sure that 0 had not a special meaning (i.e. no default random state). Interestingly, with 0 I have the same behavior, and with 42 too - although for different values of resolution. So I think we can rule out the `random_state` initialization. I am not completely familiar with the internals of louvain, but may that be related to tie resolution during aggregation of points?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/279
https://github.com/scverse/scanpy/issues/279:72,interoperability,api,api,72,"Apparently [louvain](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html#scanpy.api.tl.louvain) has a default value for `random_state` (0). I tried both with 0 and 42 as `random_state` just to be sure that 0 had not a special meaning (i.e. no default random state). Interestingly, with 0 I have the same behavior, and with 42 too - although for different values of resolution. So I think we can rule out the `random_state` initialization. I am not completely familiar with the internals of louvain, but may that be related to tie resolution during aggregation of points?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/279
https://github.com/scverse/scanpy/issues/279:99,interoperability,api,api,99,"Apparently [louvain](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html#scanpy.api.tl.louvain) has a default value for `random_state` (0). I tried both with 0 and 42 as `random_state` just to be sure that 0 had not a special meaning (i.e. no default random state). Interestingly, with 0 I have the same behavior, and with 42 too - although for different values of resolution. So I think we can rule out the `random_state` initialization. I am not completely familiar with the internals of louvain, but may that be related to tie resolution during aggregation of points?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/279
https://github.com/scverse/scanpy/issues/279:467,safety,compl,completely,467,"Apparently [louvain](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html#scanpy.api.tl.louvain) has a default value for `random_state` (0). I tried both with 0 and 42 as `random_state` just to be sure that 0 had not a special meaning (i.e. no default random state). Interestingly, with 0 I have the same behavior, and with 42 too - although for different values of resolution. So I think we can rule out the `random_state` initialization. I am not completely familiar with the internals of louvain, but may that be related to tie resolution during aggregation of points?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/279
https://github.com/scverse/scanpy/issues/279:467,security,compl,completely,467,"Apparently [louvain](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html#scanpy.api.tl.louvain) has a default value for `random_state` (0). I tried both with 0 and 42 as `random_state` just to be sure that 0 had not a special meaning (i.e. no default random state). Interestingly, with 0 I have the same behavior, and with 42 too - although for different values of resolution. So I think we can rule out the `random_state` initialization. I am not completely familiar with the internals of louvain, but may that be related to tie resolution during aggregation of points?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/279
https://github.com/scverse/scanpy/issues/279:323,usability,behavi,behavior,323,"Apparently [louvain](https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.louvain.html#scanpy.api.tl.louvain) has a default value for `random_state` (0). I tried both with 0 and 42 as `random_state` just to be sure that 0 had not a special meaning (i.e. no default random state). Interestingly, with 0 I have the same behavior, and with 42 too - although for different values of resolution. So I think we can rule out the `random_state` initialization. I am not completely familiar with the internals of louvain, but may that be related to tie resolution during aggregation of points?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/279
https://github.com/scverse/scanpy/issues/279:390,availability,cluster,clusters,390,"My hunch would be that both runs just find different local minima: one being further away from the global minimum than the other. As Louvain is just a heuristic solution to an NP-hard problem, this is not surprising. Just use a random number generator for the random seed and run it 100 times for each resolution, it would probably show the expected picture again for the average number of clusters detected. . I'm not sure if the tie breaking rule is deterministic or not. But you'd get different ties in both cases anyway I assume.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/279
https://github.com/scverse/scanpy/issues/279:390,deployability,cluster,clusters,390,"My hunch would be that both runs just find different local minima: one being further away from the global minimum than the other. As Louvain is just a heuristic solution to an NP-hard problem, this is not surprising. Just use a random number generator for the random seed and run it 100 times for each resolution, it would probably show the expected picture again for the average number of clusters detected. . I'm not sure if the tie breaking rule is deterministic or not. But you'd get different ties in both cases anyway I assume.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/279
https://github.com/scverse/scanpy/issues/279:287,performance,time,times,287,"My hunch would be that both runs just find different local minima: one being further away from the global minimum than the other. As Louvain is just a heuristic solution to an NP-hard problem, this is not surprising. Just use a random number generator for the random seed and run it 100 times for each resolution, it would probably show the expected picture again for the average number of clusters detected. . I'm not sure if the tie breaking rule is deterministic or not. But you'd get different ties in both cases anyway I assume.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/279
https://github.com/scverse/scanpy/issues/279:399,safety,detect,detected,399,"My hunch would be that both runs just find different local minima: one being further away from the global minimum than the other. As Louvain is just a heuristic solution to an NP-hard problem, this is not surprising. Just use a random number generator for the random seed and run it 100 times for each resolution, it would probably show the expected picture again for the average number of clusters detected. . I'm not sure if the tie breaking rule is deterministic or not. But you'd get different ties in both cases anyway I assume.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/279
https://github.com/scverse/scanpy/issues/279:399,security,detect,detected,399,"My hunch would be that both runs just find different local minima: one being further away from the global minimum than the other. As Louvain is just a heuristic solution to an NP-hard problem, this is not surprising. Just use a random number generator for the random seed and run it 100 times for each resolution, it would probably show the expected picture again for the average number of clusters detected. . I'm not sure if the tie breaking rule is deterministic or not. But you'd get different ties in both cases anyway I assume.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/279
https://github.com/scverse/scanpy/issues/279:59,usability,minim,minima,59,"My hunch would be that both runs just find different local minima: one being further away from the global minimum than the other. As Louvain is just a heuristic solution to an NP-hard problem, this is not surprising. Just use a random number generator for the random seed and run it 100 times for each resolution, it would probably show the expected picture again for the average number of clusters detected. . I'm not sure if the tie breaking rule is deterministic or not. But you'd get different ties in both cases anyway I assume.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/279
https://github.com/scverse/scanpy/issues/279:106,usability,minim,minimum,106,"My hunch would be that both runs just find different local minima: one being further away from the global minimum than the other. As Louvain is just a heuristic solution to an NP-hard problem, this is not surprising. Just use a random number generator for the random seed and run it 100 times for each resolution, it would probably show the expected picture again for the average number of clusters detected. . I'm not sure if the tie breaking rule is deterministic or not. But you'd get different ties in both cases anyway I assume.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/279
https://github.com/scverse/scanpy/issues/279:60,deployability,modul,modularity,60,"AFAIK this is not an issue. Louvain method optimizes global modularity but, as other methods, may miss some “true” communities. Communities in Louvain method are not intended in hierarchical way. I suspect that what you observed applies to many scRNA data at, at least, one resolution value.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/279
https://github.com/scverse/scanpy/issues/279:220,deployability,observ,observed,220,"AFAIK this is not an issue. Louvain method optimizes global modularity but, as other methods, may miss some “true” communities. Communities in Louvain method are not intended in hierarchical way. I suspect that what you observed applies to many scRNA data at, at least, one resolution value.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/279
https://github.com/scverse/scanpy/issues/279:43,energy efficiency,optim,optimizes,43,"AFAIK this is not an issue. Louvain method optimizes global modularity but, as other methods, may miss some “true” communities. Communities in Louvain method are not intended in hierarchical way. I suspect that what you observed applies to many scRNA data at, at least, one resolution value.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/279
https://github.com/scverse/scanpy/issues/279:60,integrability,modular,modularity,60,"AFAIK this is not an issue. Louvain method optimizes global modularity but, as other methods, may miss some “true” communities. Communities in Louvain method are not intended in hierarchical way. I suspect that what you observed applies to many scRNA data at, at least, one resolution value.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/279
https://github.com/scverse/scanpy/issues/279:60,modifiability,modul,modularity,60,"AFAIK this is not an issue. Louvain method optimizes global modularity but, as other methods, may miss some “true” communities. Communities in Louvain method are not intended in hierarchical way. I suspect that what you observed applies to many scRNA data at, at least, one resolution value.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/279
https://github.com/scverse/scanpy/issues/279:43,performance,optimiz,optimizes,43,"AFAIK this is not an issue. Louvain method optimizes global modularity but, as other methods, may miss some “true” communities. Communities in Louvain method are not intended in hierarchical way. I suspect that what you observed applies to many scRNA data at, at least, one resolution value.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/279
https://github.com/scverse/scanpy/issues/279:60,safety,modul,modularity,60,"AFAIK this is not an issue. Louvain method optimizes global modularity but, as other methods, may miss some “true” communities. Communities in Louvain method are not intended in hierarchical way. I suspect that what you observed applies to many scRNA data at, at least, one resolution value.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/279
https://github.com/scverse/scanpy/issues/279:60,testability,modula,modularity,60,"AFAIK this is not an issue. Louvain method optimizes global modularity but, as other methods, may miss some “true” communities. Communities in Louvain method are not intended in hierarchical way. I suspect that what you observed applies to many scRNA data at, at least, one resolution value.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/279
https://github.com/scverse/scanpy/issues/279:220,testability,observ,observed,220,"AFAIK this is not an issue. Louvain method optimizes global modularity but, as other methods, may miss some “true” communities. Communities in Louvain method are not intended in hierarchical way. I suspect that what you observed applies to many scRNA data at, at least, one resolution value.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/279
https://github.com/scverse/scanpy/issues/279:49,deployability,modul,moduliarity,49,"I guess one could calculate the multi-resolution moduliarity score of both partitions with both resolutions and see if the lower number of communities is actually a more optimal modularity score than the higher number that is found. If that's the case, it's just about imperfect optimization, which is expected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/279
https://github.com/scverse/scanpy/issues/279:178,deployability,modul,modularity,178,"I guess one could calculate the multi-resolution moduliarity score of both partitions with both resolutions and see if the lower number of communities is actually a more optimal modularity score than the higher number that is found. If that's the case, it's just about imperfect optimization, which is expected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/279
https://github.com/scverse/scanpy/issues/279:170,energy efficiency,optim,optimal,170,"I guess one could calculate the multi-resolution moduliarity score of both partitions with both resolutions and see if the lower number of communities is actually a more optimal modularity score than the higher number that is found. If that's the case, it's just about imperfect optimization, which is expected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/279
https://github.com/scverse/scanpy/issues/279:279,energy efficiency,optim,optimization,279,"I guess one could calculate the multi-resolution moduliarity score of both partitions with both resolutions and see if the lower number of communities is actually a more optimal modularity score than the higher number that is found. If that's the case, it's just about imperfect optimization, which is expected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/279
https://github.com/scverse/scanpy/issues/279:178,integrability,modular,modularity,178,"I guess one could calculate the multi-resolution moduliarity score of both partitions with both resolutions and see if the lower number of communities is actually a more optimal modularity score than the higher number that is found. If that's the case, it's just about imperfect optimization, which is expected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/279
https://github.com/scverse/scanpy/issues/279:49,modifiability,modul,moduliarity,49,"I guess one could calculate the multi-resolution moduliarity score of both partitions with both resolutions and see if the lower number of communities is actually a more optimal modularity score than the higher number that is found. If that's the case, it's just about imperfect optimization, which is expected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/279
https://github.com/scverse/scanpy/issues/279:178,modifiability,modul,modularity,178,"I guess one could calculate the multi-resolution moduliarity score of both partitions with both resolutions and see if the lower number of communities is actually a more optimal modularity score than the higher number that is found. If that's the case, it's just about imperfect optimization, which is expected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/279
https://github.com/scverse/scanpy/issues/279:279,performance,optimiz,optimization,279,"I guess one could calculate the multi-resolution moduliarity score of both partitions with both resolutions and see if the lower number of communities is actually a more optimal modularity score than the higher number that is found. If that's the case, it's just about imperfect optimization, which is expected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/279
https://github.com/scverse/scanpy/issues/279:49,safety,modul,moduliarity,49,"I guess one could calculate the multi-resolution moduliarity score of both partitions with both resolutions and see if the lower number of communities is actually a more optimal modularity score than the higher number that is found. If that's the case, it's just about imperfect optimization, which is expected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/279
https://github.com/scverse/scanpy/issues/279:178,safety,modul,modularity,178,"I guess one could calculate the multi-resolution moduliarity score of both partitions with both resolutions and see if the lower number of communities is actually a more optimal modularity score than the higher number that is found. If that's the case, it's just about imperfect optimization, which is expected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/279
https://github.com/scverse/scanpy/issues/279:178,testability,modula,modularity,178,"I guess one could calculate the multi-resolution moduliarity score of both partitions with both resolutions and see if the lower number of communities is actually a more optimal modularity score than the higher number that is found. If that's the case, it's just about imperfect optimization, which is expected.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/279
https://github.com/scverse/scanpy/issues/280:420,availability,error,error,420,"How did you installed scanpy? Try:. conda install --file requirements.txt. this may install all the right versions of the packages that you need. On Thu, Oct 4, 2018 at 2:26 AM ar-baya <notifications@github.com> wrote:. > Hi, I am reproducing this tutorial. > https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170503_zheng17/zheng17.ipynb. >. > the line sc.pp.neighbors(adata) produces the following error:. >. > Inconsistency detected by ld.so: dl-version.c: 205:. > _dl_check_map_versions: Assertion `needed != NULL' failed! >. > Ubuntu 18.04. > Python 3.6.6. >. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4. > scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1. >. > Can you help me? Thank You. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Sgm2UxCRL2y2-EGlah7YmtIrmmeks5uhVXGgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:12,deployability,instal,installed,12,"How did you installed scanpy? Try:. conda install --file requirements.txt. this may install all the right versions of the packages that you need. On Thu, Oct 4, 2018 at 2:26 AM ar-baya <notifications@github.com> wrote:. > Hi, I am reproducing this tutorial. > https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170503_zheng17/zheng17.ipynb. >. > the line sc.pp.neighbors(adata) produces the following error:. >. > Inconsistency detected by ld.so: dl-version.c: 205:. > _dl_check_map_versions: Assertion `needed != NULL' failed! >. > Ubuntu 18.04. > Python 3.6.6. >. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4. > scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1. >. > Can you help me? Thank You. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Sgm2UxCRL2y2-EGlah7YmtIrmmeks5uhVXGgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:42,deployability,instal,install,42,"How did you installed scanpy? Try:. conda install --file requirements.txt. this may install all the right versions of the packages that you need. On Thu, Oct 4, 2018 at 2:26 AM ar-baya <notifications@github.com> wrote:. > Hi, I am reproducing this tutorial. > https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170503_zheng17/zheng17.ipynb. >. > the line sc.pp.neighbors(adata) produces the following error:. >. > Inconsistency detected by ld.so: dl-version.c: 205:. > _dl_check_map_versions: Assertion `needed != NULL' failed! >. > Ubuntu 18.04. > Python 3.6.6. >. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4. > scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1. >. > Can you help me? Thank You. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Sgm2UxCRL2y2-EGlah7YmtIrmmeks5uhVXGgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:84,deployability,instal,install,84,"How did you installed scanpy? Try:. conda install --file requirements.txt. this may install all the right versions of the packages that you need. On Thu, Oct 4, 2018 at 2:26 AM ar-baya <notifications@github.com> wrote:. > Hi, I am reproducing this tutorial. > https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170503_zheng17/zheng17.ipynb. >. > the line sc.pp.neighbors(adata) produces the following error:. >. > Inconsistency detected by ld.so: dl-version.c: 205:. > _dl_check_map_versions: Assertion `needed != NULL' failed! >. > Ubuntu 18.04. > Python 3.6.6. >. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4. > scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1. >. > Can you help me? Thank You. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Sgm2UxCRL2y2-EGlah7YmtIrmmeks5uhVXGgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:106,deployability,version,versions,106,"How did you installed scanpy? Try:. conda install --file requirements.txt. this may install all the right versions of the packages that you need. On Thu, Oct 4, 2018 at 2:26 AM ar-baya <notifications@github.com> wrote:. > Hi, I am reproducing this tutorial. > https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170503_zheng17/zheng17.ipynb. >. > the line sc.pp.neighbors(adata) produces the following error:. >. > Inconsistency detected by ld.so: dl-version.c: 205:. > _dl_check_map_versions: Assertion `needed != NULL' failed! >. > Ubuntu 18.04. > Python 3.6.6. >. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4. > scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1. >. > Can you help me? Thank You. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Sgm2UxCRL2y2-EGlah7YmtIrmmeks5uhVXGgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:469,deployability,version,version,469,"How did you installed scanpy? Try:. conda install --file requirements.txt. this may install all the right versions of the packages that you need. On Thu, Oct 4, 2018 at 2:26 AM ar-baya <notifications@github.com> wrote:. > Hi, I am reproducing this tutorial. > https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170503_zheng17/zheng17.ipynb. >. > the line sc.pp.neighbors(adata) produces the following error:. >. > Inconsistency detected by ld.so: dl-version.c: 205:. > _dl_check_map_versions: Assertion `needed != NULL' failed! >. > Ubuntu 18.04. > Python 3.6.6. >. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4. > scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1. >. > Can you help me? Thank You. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Sgm2UxCRL2y2-EGlah7YmtIrmmeks5uhVXGgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:539,deployability,fail,failed,539,"How did you installed scanpy? Try:. conda install --file requirements.txt. this may install all the right versions of the packages that you need. On Thu, Oct 4, 2018 at 2:26 AM ar-baya <notifications@github.com> wrote:. > Hi, I am reproducing this tutorial. > https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170503_zheng17/zheng17.ipynb. >. > the line sc.pp.neighbors(adata) produces the following error:. >. > Inconsistency detected by ld.so: dl-version.c: 205:. > _dl_check_map_versions: Assertion `needed != NULL' failed! >. > Ubuntu 18.04. > Python 3.6.6. >. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4. > scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1. >. > Can you help me? Thank You. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Sgm2UxCRL2y2-EGlah7YmtIrmmeks5uhVXGgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:106,integrability,version,versions,106,"How did you installed scanpy? Try:. conda install --file requirements.txt. this may install all the right versions of the packages that you need. On Thu, Oct 4, 2018 at 2:26 AM ar-baya <notifications@github.com> wrote:. > Hi, I am reproducing this tutorial. > https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170503_zheng17/zheng17.ipynb. >. > the line sc.pp.neighbors(adata) produces the following error:. >. > Inconsistency detected by ld.so: dl-version.c: 205:. > _dl_check_map_versions: Assertion `needed != NULL' failed! >. > Ubuntu 18.04. > Python 3.6.6. >. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4. > scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1. >. > Can you help me? Thank You. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Sgm2UxCRL2y2-EGlah7YmtIrmmeks5uhVXGgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:469,integrability,version,version,469,"How did you installed scanpy? Try:. conda install --file requirements.txt. this may install all the right versions of the packages that you need. On Thu, Oct 4, 2018 at 2:26 AM ar-baya <notifications@github.com> wrote:. > Hi, I am reproducing this tutorial. > https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170503_zheng17/zheng17.ipynb. >. > the line sc.pp.neighbors(adata) produces the following error:. >. > Inconsistency detected by ld.so: dl-version.c: 205:. > _dl_check_map_versions: Assertion `needed != NULL' failed! >. > Ubuntu 18.04. > Python 3.6.6. >. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4. > scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1. >. > Can you help me? Thank You. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Sgm2UxCRL2y2-EGlah7YmtIrmmeks5uhVXGgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:821,integrability,sub,subscribed,821,"How did you installed scanpy? Try:. conda install --file requirements.txt. this may install all the right versions of the packages that you need. On Thu, Oct 4, 2018 at 2:26 AM ar-baya <notifications@github.com> wrote:. > Hi, I am reproducing this tutorial. > https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170503_zheng17/zheng17.ipynb. >. > the line sc.pp.neighbors(adata) produces the following error:. >. > Inconsistency detected by ld.so: dl-version.c: 205:. > _dl_check_map_versions: Assertion `needed != NULL' failed! >. > Ubuntu 18.04. > Python 3.6.6. >. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4. > scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1. >. > Can you help me? Thank You. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Sgm2UxCRL2y2-EGlah7YmtIrmmeks5uhVXGgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:106,modifiability,version,versions,106,"How did you installed scanpy? Try:. conda install --file requirements.txt. this may install all the right versions of the packages that you need. On Thu, Oct 4, 2018 at 2:26 AM ar-baya <notifications@github.com> wrote:. > Hi, I am reproducing this tutorial. > https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170503_zheng17/zheng17.ipynb. >. > the line sc.pp.neighbors(adata) produces the following error:. >. > Inconsistency detected by ld.so: dl-version.c: 205:. > _dl_check_map_versions: Assertion `needed != NULL' failed! >. > Ubuntu 18.04. > Python 3.6.6. >. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4. > scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1. >. > Can you help me? Thank You. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Sgm2UxCRL2y2-EGlah7YmtIrmmeks5uhVXGgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:122,modifiability,pac,packages,122,"How did you installed scanpy? Try:. conda install --file requirements.txt. this may install all the right versions of the packages that you need. On Thu, Oct 4, 2018 at 2:26 AM ar-baya <notifications@github.com> wrote:. > Hi, I am reproducing this tutorial. > https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170503_zheng17/zheng17.ipynb. >. > the line sc.pp.neighbors(adata) produces the following error:. >. > Inconsistency detected by ld.so: dl-version.c: 205:. > _dl_check_map_versions: Assertion `needed != NULL' failed! >. > Ubuntu 18.04. > Python 3.6.6. >. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4. > scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1. >. > Can you help me? Thank You. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Sgm2UxCRL2y2-EGlah7YmtIrmmeks5uhVXGgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:469,modifiability,version,version,469,"How did you installed scanpy? Try:. conda install --file requirements.txt. this may install all the right versions of the packages that you need. On Thu, Oct 4, 2018 at 2:26 AM ar-baya <notifications@github.com> wrote:. > Hi, I am reproducing this tutorial. > https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170503_zheng17/zheng17.ipynb. >. > the line sc.pp.neighbors(adata) produces the following error:. >. > Inconsistency detected by ld.so: dl-version.c: 205:. > _dl_check_map_versions: Assertion `needed != NULL' failed! >. > Ubuntu 18.04. > Python 3.6.6. >. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4. > scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1. >. > Can you help me? Thank You. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Sgm2UxCRL2y2-EGlah7YmtIrmmeks5uhVXGgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:420,performance,error,error,420,"How did you installed scanpy? Try:. conda install --file requirements.txt. this may install all the right versions of the packages that you need. On Thu, Oct 4, 2018 at 2:26 AM ar-baya <notifications@github.com> wrote:. > Hi, I am reproducing this tutorial. > https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170503_zheng17/zheng17.ipynb. >. > the line sc.pp.neighbors(adata) produces the following error:. >. > Inconsistency detected by ld.so: dl-version.c: 205:. > _dl_check_map_versions: Assertion `needed != NULL' failed! >. > Ubuntu 18.04. > Python 3.6.6. >. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4. > scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1. >. > Can you help me? Thank You. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Sgm2UxCRL2y2-EGlah7YmtIrmmeks5uhVXGgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:539,reliability,fail,failed,539,"How did you installed scanpy? Try:. conda install --file requirements.txt. this may install all the right versions of the packages that you need. On Thu, Oct 4, 2018 at 2:26 AM ar-baya <notifications@github.com> wrote:. > Hi, I am reproducing this tutorial. > https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170503_zheng17/zheng17.ipynb. >. > the line sc.pp.neighbors(adata) produces the following error:. >. > Inconsistency detected by ld.so: dl-version.c: 205:. > _dl_check_map_versions: Assertion `needed != NULL' failed! >. > Ubuntu 18.04. > Python 3.6.6. >. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4. > scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1. >. > Can you help me? Thank You. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Sgm2UxCRL2y2-EGlah7YmtIrmmeks5uhVXGgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:420,safety,error,error,420,"How did you installed scanpy? Try:. conda install --file requirements.txt. this may install all the right versions of the packages that you need. On Thu, Oct 4, 2018 at 2:26 AM ar-baya <notifications@github.com> wrote:. > Hi, I am reproducing this tutorial. > https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170503_zheng17/zheng17.ipynb. >. > the line sc.pp.neighbors(adata) produces the following error:. >. > Inconsistency detected by ld.so: dl-version.c: 205:. > _dl_check_map_versions: Assertion `needed != NULL' failed! >. > Ubuntu 18.04. > Python 3.6.6. >. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4. > scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1. >. > Can you help me? Thank You. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Sgm2UxCRL2y2-EGlah7YmtIrmmeks5uhVXGgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:447,safety,detect,detected,447,"How did you installed scanpy? Try:. conda install --file requirements.txt. this may install all the right versions of the packages that you need. On Thu, Oct 4, 2018 at 2:26 AM ar-baya <notifications@github.com> wrote:. > Hi, I am reproducing this tutorial. > https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170503_zheng17/zheng17.ipynb. >. > the line sc.pp.neighbors(adata) produces the following error:. >. > Inconsistency detected by ld.so: dl-version.c: 205:. > _dl_check_map_versions: Assertion `needed != NULL' failed! >. > Ubuntu 18.04. > Python 3.6.6. >. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4. > scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1. >. > Can you help me? Thank You. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Sgm2UxCRL2y2-EGlah7YmtIrmmeks5uhVXGgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:447,security,detect,detected,447,"How did you installed scanpy? Try:. conda install --file requirements.txt. this may install all the right versions of the packages that you need. On Thu, Oct 4, 2018 at 2:26 AM ar-baya <notifications@github.com> wrote:. > Hi, I am reproducing this tutorial. > https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170503_zheng17/zheng17.ipynb. >. > the line sc.pp.neighbors(adata) produces the following error:. >. > Inconsistency detected by ld.so: dl-version.c: 205:. > _dl_check_map_versions: Assertion `needed != NULL' failed! >. > Ubuntu 18.04. > Python 3.6.6. >. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4. > scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1. >. > Can you help me? Thank You. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Sgm2UxCRL2y2-EGlah7YmtIrmmeks5uhVXGgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:1018,security,auth,auth,1018,"How did you installed scanpy? Try:. conda install --file requirements.txt. this may install all the right versions of the packages that you need. On Thu, Oct 4, 2018 at 2:26 AM ar-baya <notifications@github.com> wrote:. > Hi, I am reproducing this tutorial. > https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170503_zheng17/zheng17.ipynb. >. > the line sc.pp.neighbors(adata) produces the following error:. >. > Inconsistency detected by ld.so: dl-version.c: 205:. > _dl_check_map_versions: Assertion `needed != NULL' failed! >. > Ubuntu 18.04. > Python 3.6.6. >. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4. > scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1. >. > Can you help me? Thank You. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Sgm2UxCRL2y2-EGlah7YmtIrmmeks5uhVXGgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:512,testability,Assert,Assertion,512,"How did you installed scanpy? Try:. conda install --file requirements.txt. this may install all the right versions of the packages that you need. On Thu, Oct 4, 2018 at 2:26 AM ar-baya <notifications@github.com> wrote:. > Hi, I am reproducing this tutorial. > https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170503_zheng17/zheng17.ipynb. >. > the line sc.pp.neighbors(adata) produces the following error:. >. > Inconsistency detected by ld.so: dl-version.c: 205:. > _dl_check_map_versions: Assertion `needed != NULL' failed! >. > Ubuntu 18.04. > Python 3.6.6. >. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4. > scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1. >. > Can you help me? Thank You. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Sgm2UxCRL2y2-EGlah7YmtIrmmeks5uhVXGgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:420,usability,error,error,420,"How did you installed scanpy? Try:. conda install --file requirements.txt. this may install all the right versions of the packages that you need. On Thu, Oct 4, 2018 at 2:26 AM ar-baya <notifications@github.com> wrote:. > Hi, I am reproducing this tutorial. > https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170503_zheng17/zheng17.ipynb. >. > the line sc.pp.neighbors(adata) produces the following error:. >. > Inconsistency detected by ld.so: dl-version.c: 205:. > _dl_check_map_versions: Assertion `needed != NULL' failed! >. > Ubuntu 18.04. > Python 3.6.6. >. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4. > scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1. >. > Can you help me? Thank You. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Sgm2UxCRL2y2-EGlah7YmtIrmmeks5uhVXGgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:669,usability,learn,learn,669,"How did you installed scanpy? Try:. conda install --file requirements.txt. this may install all the right versions of the packages that you need. On Thu, Oct 4, 2018 at 2:26 AM ar-baya <notifications@github.com> wrote:. > Hi, I am reproducing this tutorial. > https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170503_zheng17/zheng17.ipynb. >. > the line sc.pp.neighbors(adata) produces the following error:. >. > Inconsistency detected by ld.so: dl-version.c: 205:. > _dl_check_map_versions: Assertion `needed != NULL' failed! >. > Ubuntu 18.04. > Python 3.6.6. >. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4. > scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1. >. > Can you help me? Thank You. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Sgm2UxCRL2y2-EGlah7YmtIrmmeks5uhVXGgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:752,usability,help,help,752,"How did you installed scanpy? Try:. conda install --file requirements.txt. this may install all the right versions of the packages that you need. On Thu, Oct 4, 2018 at 2:26 AM ar-baya <notifications@github.com> wrote:. > Hi, I am reproducing this tutorial. > https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/170503_zheng17/zheng17.ipynb. >. > the line sc.pp.neighbors(adata) produces the following error:. >. > Inconsistency detected by ld.so: dl-version.c: 205:. > _dl_check_map_versions: Assertion `needed != NULL' failed! >. > Ubuntu 18.04. > Python 3.6.6. >. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4. > scikit-learn==0.19.2 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1. >. > Can you help me? Thank You. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Sgm2UxCRL2y2-EGlah7YmtIrmmeks5uhVXGgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:8,deployability,instal,installed,8,"First I installed scanpy using sudo and pip as any python package. But, now I followed your advice and installed it using Bioconda, and it solved the issue. Thank you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:103,deployability,instal,installed,103,"First I installed scanpy using sudo and pip as any python package. But, now I followed your advice and installed it using Bioconda, and it solved the issue. Thank you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:58,modifiability,pac,package,58,"First I installed scanpy using sudo and pip as any python package. But, now I followed your advice and installed it using Bioconda, and it solved the issue. Thank you.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:43,usability,help,helpful,43,@ar-baya Great to hear that the advice was helpful.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:605,availability,cluster,clusters,605,Same issue here. Using `pip` +pyhton3.7 and not conda to install from pypi. Is there a way to resolve it without installing using conda? Logs:. ```. [dilawars@chamcham scanpy_exp]$ python planaria.py . /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. import imp. scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:02:53.98). saving figure to file ./figures/tsne_full.pdf. computing neighbors. using data matrix X directly. Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! ```.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:57,deployability,instal,install,57,Same issue here. Using `pip` +pyhton3.7 and not conda to install from pypi. Is there a way to resolve it without installing using conda? Logs:. ```. [dilawars@chamcham scanpy_exp]$ python planaria.py . /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. import imp. scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:02:53.98). saving figure to file ./figures/tsne_full.pdf. computing neighbors. using data matrix X directly. Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! ```.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:113,deployability,instal,installing,113,Same issue here. Using `pip` +pyhton3.7 and not conda to install from pypi. Is there a way to resolve it without installing using conda? Logs:. ```. [dilawars@chamcham scanpy_exp]$ python planaria.py . /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. import imp. scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:02:53.98). saving figure to file ./figures/tsne_full.pdf. computing neighbors. using data matrix X directly. Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! ```.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:137,deployability,Log,Logs,137,Same issue here. Using `pip` +pyhton3.7 and not conda to install from pypi. Is there a way to resolve it without installing using conda? Logs:. ```. [dilawars@chamcham scanpy_exp]$ python planaria.py . /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. import imp. scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:02:53.98). saving figure to file ./figures/tsne_full.pdf. computing neighbors. using data matrix X directly. Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! ```.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:347,deployability,modul,module,347,Same issue here. Using `pip` +pyhton3.7 and not conda to install from pypi. Is there a way to resolve it without installing using conda? Logs:. ```. [dilawars@chamcham scanpy_exp]$ python planaria.py . /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. import imp. scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:02:53.98). saving figure to file ./figures/tsne_full.pdf. computing neighbors. using data matrix X directly. Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! ```.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:400,deployability,modul,module,400,Same issue here. Using `pip` +pyhton3.7 and not conda to install from pypi. Is there a way to resolve it without installing using conda? Logs:. ```. [dilawars@chamcham scanpy_exp]$ python planaria.py . /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. import imp. scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:02:53.98). saving figure to file ./figures/tsne_full.pdf. computing neighbors. using data matrix X directly. Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! ```.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:605,deployability,cluster,clusters,605,Same issue here. Using `pip` +pyhton3.7 and not conda to install from pypi. Is there a way to resolve it without installing using conda? Logs:. ```. [dilawars@chamcham scanpy_exp]$ python planaria.py . /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. import imp. scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:02:53.98). saving figure to file ./figures/tsne_full.pdf. computing neighbors. using data matrix X directly. Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! ```.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:887,deployability,version,version,887,Same issue here. Using `pip` +pyhton3.7 and not conda to install from pypi. Is there a way to resolve it without installing using conda? Logs:. ```. [dilawars@chamcham scanpy_exp]$ python planaria.py . /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. import imp. scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:02:53.98). saving figure to file ./figures/tsne_full.pdf. computing neighbors. using data matrix X directly. Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! ```.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:954,deployability,fail,failed,954,Same issue here. Using `pip` +pyhton3.7 and not conda to install from pypi. Is there a way to resolve it without installing using conda? Logs:. ```. [dilawars@chamcham scanpy_exp]$ python planaria.py . /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. import imp. scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:02:53.98). saving figure to file ./figures/tsne_full.pdf. computing neighbors. using data matrix X directly. Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! ```.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:288,energy efficiency,cloud,cloudpickle,288,Same issue here. Using `pip` +pyhton3.7 and not conda to install from pypi. Is there a way to resolve it without installing using conda? Logs:. ```. [dilawars@chamcham scanpy_exp]$ python planaria.py . /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. import imp. scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:02:53.98). saving figure to file ./figures/tsne_full.pdf. computing neighbors. using data matrix X directly. Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! ```.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:300,energy efficiency,cloud,cloudpickle,300,Same issue here. Using `pip` +pyhton3.7 and not conda to install from pypi. Is there a way to resolve it without installing using conda? Logs:. ```. [dilawars@chamcham scanpy_exp]$ python planaria.py . /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. import imp. scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:02:53.98). saving figure to file ./figures/tsne_full.pdf. computing neighbors. using data matrix X directly. Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! ```.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:887,integrability,version,version,887,Same issue here. Using `pip` +pyhton3.7 and not conda to install from pypi. Is there a way to resolve it without installing using conda? Logs:. ```. [dilawars@chamcham scanpy_exp]$ python planaria.py . /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. import imp. scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:02:53.98). saving figure to file ./figures/tsne_full.pdf. computing neighbors. using data matrix X directly. Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! ```.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:244,modifiability,pac,packages,244,Same issue here. Using `pip` +pyhton3.7 and not conda to install from pypi. Is there a way to resolve it without installing using conda? Logs:. ```. [dilawars@chamcham scanpy_exp]$ python planaria.py . /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. import imp. scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:02:53.98). saving figure to file ./figures/tsne_full.pdf. computing neighbors. using data matrix X directly. Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! ```.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:347,modifiability,modul,module,347,Same issue here. Using `pip` +pyhton3.7 and not conda to install from pypi. Is there a way to resolve it without installing using conda? Logs:. ```. [dilawars@chamcham scanpy_exp]$ python planaria.py . /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. import imp. scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:02:53.98). saving figure to file ./figures/tsne_full.pdf. computing neighbors. using data matrix X directly. Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! ```.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:400,modifiability,modul,module,400,Same issue here. Using `pip` +pyhton3.7 and not conda to install from pypi. Is there a way to resolve it without installing using conda? Logs:. ```. [dilawars@chamcham scanpy_exp]$ python planaria.py . /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. import imp. scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:02:53.98). saving figure to file ./figures/tsne_full.pdf. computing neighbors. using data matrix X directly. Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! ```.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:703,modifiability,pac,package,703,Same issue here. Using `pip` +pyhton3.7 and not conda to install from pypi. Is there a way to resolve it without installing using conda? Logs:. ```. [dilawars@chamcham scanpy_exp]$ python planaria.py . /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. import imp. scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:02:53.98). saving figure to file ./figures/tsne_full.pdf. computing neighbors. using data matrix X directly. Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! ```.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:887,modifiability,version,version,887,Same issue here. Using `pip` +pyhton3.7 and not conda to install from pypi. Is there a way to resolve it without installing using conda? Logs:. ```. [dilawars@chamcham scanpy_exp]$ python planaria.py . /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. import imp. scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:02:53.98). saving figure to file ./figures/tsne_full.pdf. computing neighbors. using data matrix X directly. Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! ```.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:954,reliability,fail,failed,954,Same issue here. Using `pip` +pyhton3.7 and not conda to install from pypi. Is there a way to resolve it without installing using conda? Logs:. ```. [dilawars@chamcham scanpy_exp]$ python planaria.py . /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. import imp. scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:02:53.98). saving figure to file ./figures/tsne_full.pdf. computing neighbors. using data matrix X directly. Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! ```.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:137,safety,Log,Logs,137,Same issue here. Using `pip` +pyhton3.7 and not conda to install from pypi. Is there a way to resolve it without installing using conda? Logs:. ```. [dilawars@chamcham scanpy_exp]$ python planaria.py . /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. import imp. scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:02:53.98). saving figure to file ./figures/tsne_full.pdf. computing neighbors. using data matrix X directly. Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! ```.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:347,safety,modul,module,347,Same issue here. Using `pip` +pyhton3.7 and not conda to install from pypi. Is there a way to resolve it without installing using conda? Logs:. ```. [dilawars@chamcham scanpy_exp]$ python planaria.py . /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. import imp. scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:02:53.98). saving figure to file ./figures/tsne_full.pdf. computing neighbors. using data matrix X directly. Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! ```.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:400,safety,modul,module,400,Same issue here. Using `pip` +pyhton3.7 and not conda to install from pypi. Is there a way to resolve it without installing using conda? Logs:. ```. [dilawars@chamcham scanpy_exp]$ python planaria.py . /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. import imp. scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:02:53.98). saving figure to file ./figures/tsne_full.pdf. computing neighbors. using data matrix X directly. Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! ```.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:865,safety,detect,detected,865,Same issue here. Using `pip` +pyhton3.7 and not conda to install from pypi. Is there a way to resolve it without installing using conda? Logs:. ```. [dilawars@chamcham scanpy_exp]$ python planaria.py . /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. import imp. scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:02:53.98). saving figure to file ./figures/tsne_full.pdf. computing neighbors. using data matrix X directly. Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! ```.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:137,security,Log,Logs,137,Same issue here. Using `pip` +pyhton3.7 and not conda to install from pypi. Is there a way to resolve it without installing using conda? Logs:. ```. [dilawars@chamcham scanpy_exp]$ python planaria.py . /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. import imp. scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:02:53.98). saving figure to file ./figures/tsne_full.pdf. computing neighbors. using data matrix X directly. Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! ```.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:865,security,detect,detected,865,Same issue here. Using `pip` +pyhton3.7 and not conda to install from pypi. Is there a way to resolve it without installing using conda? Logs:. ```. [dilawars@chamcham scanpy_exp]$ python planaria.py . /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. import imp. scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:02:53.98). saving figure to file ./figures/tsne_full.pdf. computing neighbors. using data matrix X directly. Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! ```.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:137,testability,Log,Logs,137,Same issue here. Using `pip` +pyhton3.7 and not conda to install from pypi. Is there a way to resolve it without installing using conda? Logs:. ```. [dilawars@chamcham scanpy_exp]$ python planaria.py . /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. import imp. scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:02:53.98). saving figure to file ./figures/tsne_full.pdf. computing neighbors. using data matrix X directly. Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! ```.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:188,testability,plan,planaria,188,Same issue here. Using `pip` +pyhton3.7 and not conda to install from pypi. Is there a way to resolve it without installing using conda? Logs:. ```. [dilawars@chamcham scanpy_exp]$ python planaria.py . /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. import imp. scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:02:53.98). saving figure to file ./figures/tsne_full.pdf. computing neighbors. using data matrix X directly. Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! ```.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:927,testability,Assert,Assertion,927,Same issue here. Using `pip` +pyhton3.7 and not conda to install from pypi. Is there a way to resolve it without installing using conda? Logs:. ```. [dilawars@chamcham scanpy_exp]$ python planaria.py . /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. import imp. scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:02:53.98). saving figure to file ./figures/tsne_full.pdf. computing neighbors. using data matrix X directly. Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! ```.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:409,usability,document,documentation,409,Same issue here. Using `pip` +pyhton3.7 and not conda to install from pypi. Is there a way to resolve it without installing using conda? Logs:. ```. [dilawars@chamcham scanpy_exp]$ python planaria.py . /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. import imp. scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:02:53.98). saving figure to file ./figures/tsne_full.pdf. computing neighbors. using data matrix X directly. Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! ```.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:536,usability,learn,learn,536,Same issue here. Using `pip` +pyhton3.7 and not conda to install from pypi. Is there a way to resolve it without installing using conda? Logs:. ```. [dilawars@chamcham scanpy_exp]$ python planaria.py . /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. import imp. scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:02:53.98). saving figure to file ./figures/tsne_full.pdf. computing neighbors. using data matrix X directly. Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! ```.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:5,availability,error,error,5,"That error is not specific to scanpy. It would be good to know which. library is causing the problem such that it can be updated but most likely. is either numpy, scipy, matplotlib or sklearn. Maybe try to update those. packages and see if the error goes away or try to google the error to find. some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>. wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi. > Is there a way to resolve it without installing using conda? >. > Logs:. >. > [dilawars@chamcham scanpy_exp]$ python planaria.py. > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. > import imp. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1. > ... storing 'clusters' as categorical. > computing tSNE. > using data matrix X directly. > using the 'MulticoreTSNE' package by Ulyanov (2017). > finished (0:02:53.98). > saving figure to file ./figures/tsne_full.pdf. > computing neighbors. > using data matrix X directly. > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:244,availability,error,error,244,"That error is not specific to scanpy. It would be good to know which. library is causing the problem such that it can be updated but most likely. is either numpy, scipy, matplotlib or sklearn. Maybe try to update those. packages and see if the error goes away or try to google the error to find. some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>. wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi. > Is there a way to resolve it without installing using conda? >. > Logs:. >. > [dilawars@chamcham scanpy_exp]$ python planaria.py. > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. > import imp. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1. > ... storing 'clusters' as categorical. > computing tSNE. > using data matrix X directly. > using the 'MulticoreTSNE' package by Ulyanov (2017). > finished (0:02:53.98). > saving figure to file ./figures/tsne_full.pdf. > computing neighbors. > using data matrix X directly. > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:281,availability,error,error,281,"That error is not specific to scanpy. It would be good to know which. library is causing the problem such that it can be updated but most likely. is either numpy, scipy, matplotlib or sklearn. Maybe try to update those. packages and see if the error goes away or try to google the error to find. some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>. wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi. > Is there a way to resolve it without installing using conda? >. > Logs:. >. > [dilawars@chamcham scanpy_exp]$ python planaria.py. > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. > import imp. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1. > ... storing 'clusters' as categorical. > computing tSNE. > using data matrix X directly. > using the 'MulticoreTSNE' package by Ulyanov (2017). > finished (0:02:53.98). > saving figure to file ./figures/tsne_full.pdf. > computing neighbors. > using data matrix X directly. > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:1010,availability,cluster,clusters,1010,"That error is not specific to scanpy. It would be good to know which. library is causing the problem such that it can be updated but most likely. is either numpy, scipy, matplotlib or sklearn. Maybe try to update those. packages and see if the error goes away or try to google the error to find. some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>. wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi. > Is there a way to resolve it without installing using conda? >. > Logs:. >. > [dilawars@chamcham scanpy_exp]$ python planaria.py. > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. > import imp. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1. > ... storing 'clusters' as categorical. > computing tSNE. > using data matrix X directly. > using the 'MulticoreTSNE' package by Ulyanov (2017). > finished (0:02:53.98). > saving figure to file ./figures/tsne_full.pdf. > computing neighbors. > using data matrix X directly. > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:121,deployability,updat,updated,121,"That error is not specific to scanpy. It would be good to know which. library is causing the problem such that it can be updated but most likely. is either numpy, scipy, matplotlib or sklearn. Maybe try to update those. packages and see if the error goes away or try to google the error to find. some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>. wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi. > Is there a way to resolve it without installing using conda? >. > Logs:. >. > [dilawars@chamcham scanpy_exp]$ python planaria.py. > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. > import imp. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1. > ... storing 'clusters' as categorical. > computing tSNE. > using data matrix X directly. > using the 'MulticoreTSNE' package by Ulyanov (2017). > finished (0:02:53.98). > saving figure to file ./figures/tsne_full.pdf. > computing neighbors. > using data matrix X directly. > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:206,deployability,updat,update,206,"That error is not specific to scanpy. It would be good to know which. library is causing the problem such that it can be updated but most likely. is either numpy, scipy, matplotlib or sklearn. Maybe try to update those. packages and see if the error goes away or try to google the error to find. some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>. wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi. > Is there a way to resolve it without installing using conda? >. > Logs:. >. > [dilawars@chamcham scanpy_exp]$ python planaria.py. > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. > import imp. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1. > ... storing 'clusters' as categorical. > computing tSNE. > using data matrix X directly. > using the 'MulticoreTSNE' package by Ulyanov (2017). > finished (0:02:53.98). > saving figure to file ./figures/tsne_full.pdf. > computing neighbors. > using data matrix X directly. > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:449,deployability,instal,install,449,"That error is not specific to scanpy. It would be good to know which. library is causing the problem such that it can be updated but most likely. is either numpy, scipy, matplotlib or sklearn. Maybe try to update those. packages and see if the error goes away or try to google the error to find. some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>. wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi. > Is there a way to resolve it without installing using conda? >. > Logs:. >. > [dilawars@chamcham scanpy_exp]$ python planaria.py. > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. > import imp. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1. > ... storing 'clusters' as categorical. > computing tSNE. > using data matrix X directly. > using the 'MulticoreTSNE' package by Ulyanov (2017). > finished (0:02:53.98). > saving figure to file ./figures/tsne_full.pdf. > computing neighbors. > using data matrix X directly. > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:507,deployability,instal,installing,507,"That error is not specific to scanpy. It would be good to know which. library is causing the problem such that it can be updated but most likely. is either numpy, scipy, matplotlib or sklearn. Maybe try to update those. packages and see if the error goes away or try to google the error to find. some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>. wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi. > Is there a way to resolve it without installing using conda? >. > Logs:. >. > [dilawars@chamcham scanpy_exp]$ python planaria.py. > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. > import imp. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1. > ... storing 'clusters' as categorical. > computing tSNE. > using data matrix X directly. > using the 'MulticoreTSNE' package by Ulyanov (2017). > finished (0:02:53.98). > saving figure to file ./figures/tsne_full.pdf. > computing neighbors. > using data matrix X directly. > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:536,deployability,Log,Logs,536,"That error is not specific to scanpy. It would be good to know which. library is causing the problem such that it can be updated but most likely. is either numpy, scipy, matplotlib or sklearn. Maybe try to update those. packages and see if the error goes away or try to google the error to find. some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>. wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi. > Is there a way to resolve it without installing using conda? >. > Logs:. >. > [dilawars@chamcham scanpy_exp]$ python planaria.py. > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. > import imp. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1. > ... storing 'clusters' as categorical. > computing tSNE. > using data matrix X directly. > using the 'MulticoreTSNE' package by Ulyanov (2017). > finished (0:02:53.98). > saving figure to file ./figures/tsne_full.pdf. > computing neighbors. > using data matrix X directly. > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:747,deployability,modul,module,747,"That error is not specific to scanpy. It would be good to know which. library is causing the problem such that it can be updated but most likely. is either numpy, scipy, matplotlib or sklearn. Maybe try to update those. packages and see if the error goes away or try to google the error to find. some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>. wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi. > Is there a way to resolve it without installing using conda? >. > Logs:. >. > [dilawars@chamcham scanpy_exp]$ python planaria.py. > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. > import imp. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1. > ... storing 'clusters' as categorical. > computing tSNE. > using data matrix X directly. > using the 'MulticoreTSNE' package by Ulyanov (2017). > finished (0:02:53.98). > saving figure to file ./figures/tsne_full.pdf. > computing neighbors. > using data matrix X directly. > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:800,deployability,modul,module,800,"That error is not specific to scanpy. It would be good to know which. library is causing the problem such that it can be updated but most likely. is either numpy, scipy, matplotlib or sklearn. Maybe try to update those. packages and see if the error goes away or try to google the error to find. some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>. wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi. > Is there a way to resolve it without installing using conda? >. > Logs:. >. > [dilawars@chamcham scanpy_exp]$ python planaria.py. > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. > import imp. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1. > ... storing 'clusters' as categorical. > computing tSNE. > using data matrix X directly. > using the 'MulticoreTSNE' package by Ulyanov (2017). > finished (0:02:53.98). > saving figure to file ./figures/tsne_full.pdf. > computing neighbors. > using data matrix X directly. > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:1010,deployability,cluster,clusters,1010,"That error is not specific to scanpy. It would be good to know which. library is causing the problem such that it can be updated but most likely. is either numpy, scipy, matplotlib or sklearn. Maybe try to update those. packages and see if the error goes away or try to google the error to find. some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>. wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi. > Is there a way to resolve it without installing using conda? >. > Logs:. >. > [dilawars@chamcham scanpy_exp]$ python planaria.py. > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. > import imp. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1. > ... storing 'clusters' as categorical. > computing tSNE. > using data matrix X directly. > using the 'MulticoreTSNE' package by Ulyanov (2017). > finished (0:02:53.98). > saving figure to file ./figures/tsne_full.pdf. > computing neighbors. > using data matrix X directly. > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:1308,deployability,version,version,1308,"That error is not specific to scanpy. It would be good to know which. library is causing the problem such that it can be updated but most likely. is either numpy, scipy, matplotlib or sklearn. Maybe try to update those. packages and see if the error goes away or try to google the error to find. some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>. wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi. > Is there a way to resolve it without installing using conda? >. > Logs:. >. > [dilawars@chamcham scanpy_exp]$ python planaria.py. > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. > import imp. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1. > ... storing 'clusters' as categorical. > computing tSNE. > using data matrix X directly. > using the 'MulticoreTSNE' package by Ulyanov (2017). > finished (0:02:53.98). > saving figure to file ./figures/tsne_full.pdf. > computing neighbors. > using data matrix X directly. > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:1375,deployability,fail,failed,1375,"That error is not specific to scanpy. It would be good to know which. library is causing the problem such that it can be updated but most likely. is either numpy, scipy, matplotlib or sklearn. Maybe try to update those. packages and see if the error goes away or try to google the error to find. some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>. wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi. > Is there a way to resolve it without installing using conda? >. > Logs:. >. > [dilawars@chamcham scanpy_exp]$ python planaria.py. > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. > import imp. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1. > ... storing 'clusters' as categorical. > computing tSNE. > using data matrix X directly. > using the 'MulticoreTSNE' package by Ulyanov (2017). > finished (0:02:53.98). > saving figure to file ./figures/tsne_full.pdf. > computing neighbors. > using data matrix X directly. > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:688,energy efficiency,cloud,cloudpickle,688,"That error is not specific to scanpy. It would be good to know which. library is causing the problem such that it can be updated but most likely. is either numpy, scipy, matplotlib or sklearn. Maybe try to update those. packages and see if the error goes away or try to google the error to find. some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>. wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi. > Is there a way to resolve it without installing using conda? >. > Logs:. >. > [dilawars@chamcham scanpy_exp]$ python planaria.py. > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. > import imp. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1. > ... storing 'clusters' as categorical. > computing tSNE. > using data matrix X directly. > using the 'MulticoreTSNE' package by Ulyanov (2017). > finished (0:02:53.98). > saving figure to file ./figures/tsne_full.pdf. > computing neighbors. > using data matrix X directly. > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:700,energy efficiency,cloud,cloudpickle,700,"That error is not specific to scanpy. It would be good to know which. library is causing the problem such that it can be updated but most likely. is either numpy, scipy, matplotlib or sklearn. Maybe try to update those. packages and see if the error goes away or try to google the error to find. some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>. wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi. > Is there a way to resolve it without installing using conda? >. > Logs:. >. > [dilawars@chamcham scanpy_exp]$ python planaria.py. > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. > import imp. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1. > ... storing 'clusters' as categorical. > computing tSNE. > using data matrix X directly. > using the 'MulticoreTSNE' package by Ulyanov (2017). > finished (0:02:53.98). > saving figure to file ./figures/tsne_full.pdf. > computing neighbors. > using data matrix X directly. > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:1308,integrability,version,version,1308,"That error is not specific to scanpy. It would be good to know which. library is causing the problem such that it can be updated but most likely. is either numpy, scipy, matplotlib or sklearn. Maybe try to update those. packages and see if the error goes away or try to google the error to find. some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>. wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi. > Is there a way to resolve it without installing using conda? >. > Logs:. >. > [dilawars@chamcham scanpy_exp]$ python planaria.py. > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. > import imp. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1. > ... storing 'clusters' as categorical. > computing tSNE. > using data matrix X directly. > using the 'MulticoreTSNE' package by Ulyanov (2017). > finished (0:02:53.98). > saving figure to file ./figures/tsne_full.pdf. > computing neighbors. > using data matrix X directly. > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:18,interoperability,specif,specific,18,"That error is not specific to scanpy. It would be good to know which. library is causing the problem such that it can be updated but most likely. is either numpy, scipy, matplotlib or sklearn. Maybe try to update those. packages and see if the error goes away or try to google the error to find. some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>. wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi. > Is there a way to resolve it without installing using conda? >. > Logs:. >. > [dilawars@chamcham scanpy_exp]$ python planaria.py. > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. > import imp. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1. > ... storing 'clusters' as categorical. > computing tSNE. > using data matrix X directly. > using the 'MulticoreTSNE' package by Ulyanov (2017). > finished (0:02:53.98). > saving figure to file ./figures/tsne_full.pdf. > computing neighbors. > using data matrix X directly. > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:220,modifiability,pac,packages,220,"That error is not specific to scanpy. It would be good to know which. library is causing the problem such that it can be updated but most likely. is either numpy, scipy, matplotlib or sklearn. Maybe try to update those. packages and see if the error goes away or try to google the error to find. some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>. wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi. > Is there a way to resolve it without installing using conda? >. > Logs:. >. > [dilawars@chamcham scanpy_exp]$ python planaria.py. > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. > import imp. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1. > ... storing 'clusters' as categorical. > computing tSNE. > using data matrix X directly. > using the 'MulticoreTSNE' package by Ulyanov (2017). > finished (0:02:53.98). > saving figure to file ./figures/tsne_full.pdf. > computing neighbors. > using data matrix X directly. > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:644,modifiability,pac,packages,644,"That error is not specific to scanpy. It would be good to know which. library is causing the problem such that it can be updated but most likely. is either numpy, scipy, matplotlib or sklearn. Maybe try to update those. packages and see if the error goes away or try to google the error to find. some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>. wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi. > Is there a way to resolve it without installing using conda? >. > Logs:. >. > [dilawars@chamcham scanpy_exp]$ python planaria.py. > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. > import imp. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1. > ... storing 'clusters' as categorical. > computing tSNE. > using data matrix X directly. > using the 'MulticoreTSNE' package by Ulyanov (2017). > finished (0:02:53.98). > saving figure to file ./figures/tsne_full.pdf. > computing neighbors. > using data matrix X directly. > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:747,modifiability,modul,module,747,"That error is not specific to scanpy. It would be good to know which. library is causing the problem such that it can be updated but most likely. is either numpy, scipy, matplotlib or sklearn. Maybe try to update those. packages and see if the error goes away or try to google the error to find. some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>. wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi. > Is there a way to resolve it without installing using conda? >. > Logs:. >. > [dilawars@chamcham scanpy_exp]$ python planaria.py. > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. > import imp. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1. > ... storing 'clusters' as categorical. > computing tSNE. > using data matrix X directly. > using the 'MulticoreTSNE' package by Ulyanov (2017). > finished (0:02:53.98). > saving figure to file ./figures/tsne_full.pdf. > computing neighbors. > using data matrix X directly. > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:800,modifiability,modul,module,800,"That error is not specific to scanpy. It would be good to know which. library is causing the problem such that it can be updated but most likely. is either numpy, scipy, matplotlib or sklearn. Maybe try to update those. packages and see if the error goes away or try to google the error to find. some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>. wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi. > Is there a way to resolve it without installing using conda? >. > Logs:. >. > [dilawars@chamcham scanpy_exp]$ python planaria.py. > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. > import imp. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1. > ... storing 'clusters' as categorical. > computing tSNE. > using data matrix X directly. > using the 'MulticoreTSNE' package by Ulyanov (2017). > finished (0:02:53.98). > saving figure to file ./figures/tsne_full.pdf. > computing neighbors. > using data matrix X directly. > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:1114,modifiability,pac,package,1114,"That error is not specific to scanpy. It would be good to know which. library is causing the problem such that it can be updated but most likely. is either numpy, scipy, matplotlib or sklearn. Maybe try to update those. packages and see if the error goes away or try to google the error to find. some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>. wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi. > Is there a way to resolve it without installing using conda? >. > Logs:. >. > [dilawars@chamcham scanpy_exp]$ python planaria.py. > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. > import imp. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1. > ... storing 'clusters' as categorical. > computing tSNE. > using data matrix X directly. > using the 'MulticoreTSNE' package by Ulyanov (2017). > finished (0:02:53.98). > saving figure to file ./figures/tsne_full.pdf. > computing neighbors. > using data matrix X directly. > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:1308,modifiability,version,version,1308,"That error is not specific to scanpy. It would be good to know which. library is causing the problem such that it can be updated but most likely. is either numpy, scipy, matplotlib or sklearn. Maybe try to update those. packages and see if the error goes away or try to google the error to find. some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>. wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi. > Is there a way to resolve it without installing using conda? >. > Logs:. >. > [dilawars@chamcham scanpy_exp]$ python planaria.py. > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. > import imp. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1. > ... storing 'clusters' as categorical. > computing tSNE. > using data matrix X directly. > using the 'MulticoreTSNE' package by Ulyanov (2017). > finished (0:02:53.98). > saving figure to file ./figures/tsne_full.pdf. > computing neighbors. > using data matrix X directly. > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:5,performance,error,error,5,"That error is not specific to scanpy. It would be good to know which. library is causing the problem such that it can be updated but most likely. is either numpy, scipy, matplotlib or sklearn. Maybe try to update those. packages and see if the error goes away or try to google the error to find. some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>. wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi. > Is there a way to resolve it without installing using conda? >. > Logs:. >. > [dilawars@chamcham scanpy_exp]$ python planaria.py. > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. > import imp. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1. > ... storing 'clusters' as categorical. > computing tSNE. > using data matrix X directly. > using the 'MulticoreTSNE' package by Ulyanov (2017). > finished (0:02:53.98). > saving figure to file ./figures/tsne_full.pdf. > computing neighbors. > using data matrix X directly. > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:244,performance,error,error,244,"That error is not specific to scanpy. It would be good to know which. library is causing the problem such that it can be updated but most likely. is either numpy, scipy, matplotlib or sklearn. Maybe try to update those. packages and see if the error goes away or try to google the error to find. some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>. wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi. > Is there a way to resolve it without installing using conda? >. > Logs:. >. > [dilawars@chamcham scanpy_exp]$ python planaria.py. > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. > import imp. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1. > ... storing 'clusters' as categorical. > computing tSNE. > using data matrix X directly. > using the 'MulticoreTSNE' package by Ulyanov (2017). > finished (0:02:53.98). > saving figure to file ./figures/tsne_full.pdf. > computing neighbors. > using data matrix X directly. > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:281,performance,error,error,281,"That error is not specific to scanpy. It would be good to know which. library is causing the problem such that it can be updated but most likely. is either numpy, scipy, matplotlib or sklearn. Maybe try to update those. packages and see if the error goes away or try to google the error to find. some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>. wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi. > Is there a way to resolve it without installing using conda? >. > Logs:. >. > [dilawars@chamcham scanpy_exp]$ python planaria.py. > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. > import imp. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1. > ... storing 'clusters' as categorical. > computing tSNE. > using data matrix X directly. > using the 'MulticoreTSNE' package by Ulyanov (2017). > finished (0:02:53.98). > saving figure to file ./figures/tsne_full.pdf. > computing neighbors. > using data matrix X directly. > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:1375,reliability,fail,failed,1375,"That error is not specific to scanpy. It would be good to know which. library is causing the problem such that it can be updated but most likely. is either numpy, scipy, matplotlib or sklearn. Maybe try to update those. packages and see if the error goes away or try to google the error to find. some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>. wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi. > Is there a way to resolve it without installing using conda? >. > Logs:. >. > [dilawars@chamcham scanpy_exp]$ python planaria.py. > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. > import imp. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1. > ... storing 'clusters' as categorical. > computing tSNE. > using data matrix X directly. > using the 'MulticoreTSNE' package by Ulyanov (2017). > finished (0:02:53.98). > saving figure to file ./figures/tsne_full.pdf. > computing neighbors. > using data matrix X directly. > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:5,safety,error,error,5,"That error is not specific to scanpy. It would be good to know which. library is causing the problem such that it can be updated but most likely. is either numpy, scipy, matplotlib or sklearn. Maybe try to update those. packages and see if the error goes away or try to google the error to find. some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>. wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi. > Is there a way to resolve it without installing using conda? >. > Logs:. >. > [dilawars@chamcham scanpy_exp]$ python planaria.py. > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. > import imp. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1. > ... storing 'clusters' as categorical. > computing tSNE. > using data matrix X directly. > using the 'MulticoreTSNE' package by Ulyanov (2017). > finished (0:02:53.98). > saving figure to file ./figures/tsne_full.pdf. > computing neighbors. > using data matrix X directly. > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:121,safety,updat,updated,121,"That error is not specific to scanpy. It would be good to know which. library is causing the problem such that it can be updated but most likely. is either numpy, scipy, matplotlib or sklearn. Maybe try to update those. packages and see if the error goes away or try to google the error to find. some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>. wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi. > Is there a way to resolve it without installing using conda? >. > Logs:. >. > [dilawars@chamcham scanpy_exp]$ python planaria.py. > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. > import imp. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1. > ... storing 'clusters' as categorical. > computing tSNE. > using data matrix X directly. > using the 'MulticoreTSNE' package by Ulyanov (2017). > finished (0:02:53.98). > saving figure to file ./figures/tsne_full.pdf. > computing neighbors. > using data matrix X directly. > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:206,safety,updat,update,206,"That error is not specific to scanpy. It would be good to know which. library is causing the problem such that it can be updated but most likely. is either numpy, scipy, matplotlib or sklearn. Maybe try to update those. packages and see if the error goes away or try to google the error to find. some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>. wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi. > Is there a way to resolve it without installing using conda? >. > Logs:. >. > [dilawars@chamcham scanpy_exp]$ python planaria.py. > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. > import imp. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1. > ... storing 'clusters' as categorical. > computing tSNE. > using data matrix X directly. > using the 'MulticoreTSNE' package by Ulyanov (2017). > finished (0:02:53.98). > saving figure to file ./figures/tsne_full.pdf. > computing neighbors. > using data matrix X directly. > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:244,safety,error,error,244,"That error is not specific to scanpy. It would be good to know which. library is causing the problem such that it can be updated but most likely. is either numpy, scipy, matplotlib or sklearn. Maybe try to update those. packages and see if the error goes away or try to google the error to find. some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>. wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi. > Is there a way to resolve it without installing using conda? >. > Logs:. >. > [dilawars@chamcham scanpy_exp]$ python planaria.py. > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. > import imp. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1. > ... storing 'clusters' as categorical. > computing tSNE. > using data matrix X directly. > using the 'MulticoreTSNE' package by Ulyanov (2017). > finished (0:02:53.98). > saving figure to file ./figures/tsne_full.pdf. > computing neighbors. > using data matrix X directly. > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:281,safety,error,error,281,"That error is not specific to scanpy. It would be good to know which. library is causing the problem such that it can be updated but most likely. is either numpy, scipy, matplotlib or sklearn. Maybe try to update those. packages and see if the error goes away or try to google the error to find. some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>. wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi. > Is there a way to resolve it without installing using conda? >. > Logs:. >. > [dilawars@chamcham scanpy_exp]$ python planaria.py. > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. > import imp. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1. > ... storing 'clusters' as categorical. > computing tSNE. > using data matrix X directly. > using the 'MulticoreTSNE' package by Ulyanov (2017). > finished (0:02:53.98). > saving figure to file ./figures/tsne_full.pdf. > computing neighbors. > using data matrix X directly. > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:536,safety,Log,Logs,536,"That error is not specific to scanpy. It would be good to know which. library is causing the problem such that it can be updated but most likely. is either numpy, scipy, matplotlib or sklearn. Maybe try to update those. packages and see if the error goes away or try to google the error to find. some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>. wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi. > Is there a way to resolve it without installing using conda? >. > Logs:. >. > [dilawars@chamcham scanpy_exp]$ python planaria.py. > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. > import imp. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1. > ... storing 'clusters' as categorical. > computing tSNE. > using data matrix X directly. > using the 'MulticoreTSNE' package by Ulyanov (2017). > finished (0:02:53.98). > saving figure to file ./figures/tsne_full.pdf. > computing neighbors. > using data matrix X directly. > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:747,safety,modul,module,747,"That error is not specific to scanpy. It would be good to know which. library is causing the problem such that it can be updated but most likely. is either numpy, scipy, matplotlib or sklearn. Maybe try to update those. packages and see if the error goes away or try to google the error to find. some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>. wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi. > Is there a way to resolve it without installing using conda? >. > Logs:. >. > [dilawars@chamcham scanpy_exp]$ python planaria.py. > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. > import imp. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1. > ... storing 'clusters' as categorical. > computing tSNE. > using data matrix X directly. > using the 'MulticoreTSNE' package by Ulyanov (2017). > finished (0:02:53.98). > saving figure to file ./figures/tsne_full.pdf. > computing neighbors. > using data matrix X directly. > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:800,safety,modul,module,800,"That error is not specific to scanpy. It would be good to know which. library is causing the problem such that it can be updated but most likely. is either numpy, scipy, matplotlib or sklearn. Maybe try to update those. packages and see if the error goes away or try to google the error to find. some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>. wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi. > Is there a way to resolve it without installing using conda? >. > Logs:. >. > [dilawars@chamcham scanpy_exp]$ python planaria.py. > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. > import imp. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1. > ... storing 'clusters' as categorical. > computing tSNE. > using data matrix X directly. > using the 'MulticoreTSNE' package by Ulyanov (2017). > finished (0:02:53.98). > saving figure to file ./figures/tsne_full.pdf. > computing neighbors. > using data matrix X directly. > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:1286,safety,detect,detected,1286,"That error is not specific to scanpy. It would be good to know which. library is causing the problem such that it can be updated but most likely. is either numpy, scipy, matplotlib or sklearn. Maybe try to update those. packages and see if the error goes away or try to google the error to find. some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>. wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi. > Is there a way to resolve it without installing using conda? >. > Logs:. >. > [dilawars@chamcham scanpy_exp]$ python planaria.py. > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. > import imp. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1. > ... storing 'clusters' as categorical. > computing tSNE. > using data matrix X directly. > using the 'MulticoreTSNE' package by Ulyanov (2017). > finished (0:02:53.98). > saving figure to file ./figures/tsne_full.pdf. > computing neighbors. > using data matrix X directly. > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:121,security,updat,updated,121,"That error is not specific to scanpy. It would be good to know which. library is causing the problem such that it can be updated but most likely. is either numpy, scipy, matplotlib or sklearn. Maybe try to update those. packages and see if the error goes away or try to google the error to find. some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>. wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi. > Is there a way to resolve it without installing using conda? >. > Logs:. >. > [dilawars@chamcham scanpy_exp]$ python planaria.py. > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. > import imp. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1. > ... storing 'clusters' as categorical. > computing tSNE. > using data matrix X directly. > using the 'MulticoreTSNE' package by Ulyanov (2017). > finished (0:02:53.98). > saving figure to file ./figures/tsne_full.pdf. > computing neighbors. > using data matrix X directly. > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:206,security,updat,update,206,"That error is not specific to scanpy. It would be good to know which. library is causing the problem such that it can be updated but most likely. is either numpy, scipy, matplotlib or sklearn. Maybe try to update those. packages and see if the error goes away or try to google the error to find. some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>. wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi. > Is there a way to resolve it without installing using conda? >. > Logs:. >. > [dilawars@chamcham scanpy_exp]$ python planaria.py. > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. > import imp. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1. > ... storing 'clusters' as categorical. > computing tSNE. > using data matrix X directly. > using the 'MulticoreTSNE' package by Ulyanov (2017). > finished (0:02:53.98). > saving figure to file ./figures/tsne_full.pdf. > computing neighbors. > using data matrix X directly. > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:536,security,Log,Logs,536,"That error is not specific to scanpy. It would be good to know which. library is causing the problem such that it can be updated but most likely. is either numpy, scipy, matplotlib or sklearn. Maybe try to update those. packages and see if the error goes away or try to google the error to find. some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>. wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi. > Is there a way to resolve it without installing using conda? >. > Logs:. >. > [dilawars@chamcham scanpy_exp]$ python planaria.py. > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. > import imp. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1. > ... storing 'clusters' as categorical. > computing tSNE. > using data matrix X directly. > using the 'MulticoreTSNE' package by Ulyanov (2017). > finished (0:02:53.98). > saving figure to file ./figures/tsne_full.pdf. > computing neighbors. > using data matrix X directly. > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:1286,security,detect,detected,1286,"That error is not specific to scanpy. It would be good to know which. library is causing the problem such that it can be updated but most likely. is either numpy, scipy, matplotlib or sklearn. Maybe try to update those. packages and see if the error goes away or try to google the error to find. some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>. wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi. > Is there a way to resolve it without installing using conda? >. > Logs:. >. > [dilawars@chamcham scanpy_exp]$ python planaria.py. > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. > import imp. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1. > ... storing 'clusters' as categorical. > computing tSNE. > using data matrix X directly. > using the 'MulticoreTSNE' package by Ulyanov (2017). > finished (0:02:53.98). > saving figure to file ./figures/tsne_full.pdf. > computing neighbors. > using data matrix X directly. > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:1635,security,auth,auth,1635,"That error is not specific to scanpy. It would be good to know which. library is causing the problem such that it can be updated but most likely. is either numpy, scipy, matplotlib or sklearn. Maybe try to update those. packages and see if the error goes away or try to google the error to find. some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>. wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi. > Is there a way to resolve it without installing using conda? >. > Logs:. >. > [dilawars@chamcham scanpy_exp]$ python planaria.py. > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. > import imp. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1. > ... storing 'clusters' as categorical. > computing tSNE. > using data matrix X directly. > using the 'MulticoreTSNE' package by Ulyanov (2017). > finished (0:02:53.98). > saving figure to file ./figures/tsne_full.pdf. > computing neighbors. > using data matrix X directly. > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:536,testability,Log,Logs,536,"That error is not specific to scanpy. It would be good to know which. library is causing the problem such that it can be updated but most likely. is either numpy, scipy, matplotlib or sklearn. Maybe try to update those. packages and see if the error goes away or try to google the error to find. some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>. wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi. > Is there a way to resolve it without installing using conda? >. > Logs:. >. > [dilawars@chamcham scanpy_exp]$ python planaria.py. > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. > import imp. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1. > ... storing 'clusters' as categorical. > computing tSNE. > using data matrix X directly. > using the 'MulticoreTSNE' package by Ulyanov (2017). > finished (0:02:53.98). > saving figure to file ./figures/tsne_full.pdf. > computing neighbors. > using data matrix X directly. > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:587,testability,plan,planaria,587,"That error is not specific to scanpy. It would be good to know which. library is causing the problem such that it can be updated but most likely. is either numpy, scipy, matplotlib or sklearn. Maybe try to update those. packages and see if the error goes away or try to google the error to find. some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>. wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi. > Is there a way to resolve it without installing using conda? >. > Logs:. >. > [dilawars@chamcham scanpy_exp]$ python planaria.py. > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. > import imp. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1. > ... storing 'clusters' as categorical. > computing tSNE. > using data matrix X directly. > using the 'MulticoreTSNE' package by Ulyanov (2017). > finished (0:02:53.98). > saving figure to file ./figures/tsne_full.pdf. > computing neighbors. > using data matrix X directly. > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:1348,testability,Assert,Assertion,1348,"That error is not specific to scanpy. It would be good to know which. library is causing the problem such that it can be updated but most likely. is either numpy, scipy, matplotlib or sklearn. Maybe try to update those. packages and see if the error goes away or try to google the error to find. some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>. wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi. > Is there a way to resolve it without installing using conda? >. > Logs:. >. > [dilawars@chamcham scanpy_exp]$ python planaria.py. > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. > import imp. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1. > ... storing 'clusters' as categorical. > computing tSNE. > using data matrix X directly. > using the 'MulticoreTSNE' package by Ulyanov (2017). > finished (0:02:53.98). > saving figure to file ./figures/tsne_full.pdf. > computing neighbors. > using data matrix X directly. > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:5,usability,error,error,5,"That error is not specific to scanpy. It would be good to know which. library is causing the problem such that it can be updated but most likely. is either numpy, scipy, matplotlib or sklearn. Maybe try to update those. packages and see if the error goes away or try to google the error to find. some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>. wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi. > Is there a way to resolve it without installing using conda? >. > Logs:. >. > [dilawars@chamcham scanpy_exp]$ python planaria.py. > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. > import imp. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1. > ... storing 'clusters' as categorical. > computing tSNE. > using data matrix X directly. > using the 'MulticoreTSNE' package by Ulyanov (2017). > finished (0:02:53.98). > saving figure to file ./figures/tsne_full.pdf. > computing neighbors. > using data matrix X directly. > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:244,usability,error,error,244,"That error is not specific to scanpy. It would be good to know which. library is causing the problem such that it can be updated but most likely. is either numpy, scipy, matplotlib or sklearn. Maybe try to update those. packages and see if the error goes away or try to google the error to find. some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>. wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi. > Is there a way to resolve it without installing using conda? >. > Logs:. >. > [dilawars@chamcham scanpy_exp]$ python planaria.py. > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. > import imp. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1. > ... storing 'clusters' as categorical. > computing tSNE. > using data matrix X directly. > using the 'MulticoreTSNE' package by Ulyanov (2017). > finished (0:02:53.98). > saving figure to file ./figures/tsne_full.pdf. > computing neighbors. > using data matrix X directly. > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:281,usability,error,error,281,"That error is not specific to scanpy. It would be good to know which. library is causing the problem such that it can be updated but most likely. is either numpy, scipy, matplotlib or sklearn. Maybe try to update those. packages and see if the error goes away or try to google the error to find. some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>. wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi. > Is there a way to resolve it without installing using conda? >. > Logs:. >. > [dilawars@chamcham scanpy_exp]$ python planaria.py. > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. > import imp. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1. > ... storing 'clusters' as categorical. > computing tSNE. > using data matrix X directly. > using the 'MulticoreTSNE' package by Ulyanov (2017). > finished (0:02:53.98). > saving figure to file ./figures/tsne_full.pdf. > computing neighbors. > using data matrix X directly. > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:809,usability,document,documentation,809,"That error is not specific to scanpy. It would be good to know which. library is causing the problem such that it can be updated but most likely. is either numpy, scipy, matplotlib or sklearn. Maybe try to update those. packages and see if the error goes away or try to google the error to find. some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>. wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi. > Is there a way to resolve it without installing using conda? >. > Logs:. >. > [dilawars@chamcham scanpy_exp]$ python planaria.py. > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. > import imp. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1. > ... storing 'clusters' as categorical. > computing tSNE. > using data matrix X directly. > using the 'MulticoreTSNE' package by Ulyanov (2017). > finished (0:02:53.98). > saving figure to file ./figures/tsne_full.pdf. > computing neighbors. > using data matrix X directly. > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:940,usability,learn,learn,940,"That error is not specific to scanpy. It would be good to know which. library is causing the problem such that it can be updated but most likely. is either numpy, scipy, matplotlib or sklearn. Maybe try to update those. packages and see if the error goes away or try to google the error to find. some solution. On Fri, Oct 5, 2018 at 2:59 PM Dilawar Singh <notifications@github.com>. wrote:. > Same issue here. Using pip +pyhton3.7 and not conda to install from pypi. > Is there a way to resolve it without installing using conda? >. > Logs:. >. > [dilawars@chamcham scanpy_exp]$ python planaria.py. > /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. > import imp. > scanpy==1.3.1 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1. > ... storing 'clusters' as categorical. > computing tSNE. > using data matrix X directly. > using the 'MulticoreTSNE' package by Ulyanov (2017). > finished (0:02:53.98). > saving figure to file ./figures/tsne_full.pdf. > computing neighbors. > using data matrix X directly. > Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed! >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/280#issuecomment-427357518>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1WzuXR5Mhpb3jNte9UkVDqzQjb1pks5uh1eZgaJpZM4XHKo6>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:182,availability,error,error,182,"Its an issue with numba. See here https://github.com/jmschrei/apricot/blob/98693788ca315ceceeb2eb0f4ce8526f40e0049b/README.md. *Update* Quoting from the REAME above. > If you get an error that looks like. > . > Inconsistency detected by ld.so: dl-version.c: 224: _dl_check_map_versions: Assertion `needed != NULL' failed! > . > or a segmentation fault when importing apricot for the first time then you should try reinstalling numba through conda using. > . > conda install numba. > . > or. > . > pip install numba==0.39.0. > . > The issue appears to be with the most recent verson of numba, v0.40.0. Downgrading to numba v0.39.0 should solve the issue. . > . That means numba should be frozen to v0.39.0 .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:346,availability,fault,fault,346,"Its an issue with numba. See here https://github.com/jmschrei/apricot/blob/98693788ca315ceceeb2eb0f4ce8526f40e0049b/README.md. *Update* Quoting from the REAME above. > If you get an error that looks like. > . > Inconsistency detected by ld.so: dl-version.c: 224: _dl_check_map_versions: Assertion `needed != NULL' failed! > . > or a segmentation fault when importing apricot for the first time then you should try reinstalling numba through conda using. > . > conda install numba. > . > or. > . > pip install numba==0.39.0. > . > The issue appears to be with the most recent verson of numba, v0.40.0. Downgrading to numba v0.39.0 should solve the issue. . > . That means numba should be frozen to v0.39.0 .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:601,availability,Down,Downgrading,601,"Its an issue with numba. See here https://github.com/jmschrei/apricot/blob/98693788ca315ceceeb2eb0f4ce8526f40e0049b/README.md. *Update* Quoting from the REAME above. > If you get an error that looks like. > . > Inconsistency detected by ld.so: dl-version.c: 224: _dl_check_map_versions: Assertion `needed != NULL' failed! > . > or a segmentation fault when importing apricot for the first time then you should try reinstalling numba through conda using. > . > conda install numba. > . > or. > . > pip install numba==0.39.0. > . > The issue appears to be with the most recent verson of numba, v0.40.0. Downgrading to numba v0.39.0 should solve the issue. . > . That means numba should be frozen to v0.39.0 .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:128,deployability,Updat,Update,128,"Its an issue with numba. See here https://github.com/jmschrei/apricot/blob/98693788ca315ceceeb2eb0f4ce8526f40e0049b/README.md. *Update* Quoting from the REAME above. > If you get an error that looks like. > . > Inconsistency detected by ld.so: dl-version.c: 224: _dl_check_map_versions: Assertion `needed != NULL' failed! > . > or a segmentation fault when importing apricot for the first time then you should try reinstalling numba through conda using. > . > conda install numba. > . > or. > . > pip install numba==0.39.0. > . > The issue appears to be with the most recent verson of numba, v0.40.0. Downgrading to numba v0.39.0 should solve the issue. . > . That means numba should be frozen to v0.39.0 .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:247,deployability,version,version,247,"Its an issue with numba. See here https://github.com/jmschrei/apricot/blob/98693788ca315ceceeb2eb0f4ce8526f40e0049b/README.md. *Update* Quoting from the REAME above. > If you get an error that looks like. > . > Inconsistency detected by ld.so: dl-version.c: 224: _dl_check_map_versions: Assertion `needed != NULL' failed! > . > or a segmentation fault when importing apricot for the first time then you should try reinstalling numba through conda using. > . > conda install numba. > . > or. > . > pip install numba==0.39.0. > . > The issue appears to be with the most recent verson of numba, v0.40.0. Downgrading to numba v0.39.0 should solve the issue. . > . That means numba should be frozen to v0.39.0 .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:314,deployability,fail,failed,314,"Its an issue with numba. See here https://github.com/jmschrei/apricot/blob/98693788ca315ceceeb2eb0f4ce8526f40e0049b/README.md. *Update* Quoting from the REAME above. > If you get an error that looks like. > . > Inconsistency detected by ld.so: dl-version.c: 224: _dl_check_map_versions: Assertion `needed != NULL' failed! > . > or a segmentation fault when importing apricot for the first time then you should try reinstalling numba through conda using. > . > conda install numba. > . > or. > . > pip install numba==0.39.0. > . > The issue appears to be with the most recent verson of numba, v0.40.0. Downgrading to numba v0.39.0 should solve the issue. . > . That means numba should be frozen to v0.39.0 .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:466,deployability,instal,install,466,"Its an issue with numba. See here https://github.com/jmschrei/apricot/blob/98693788ca315ceceeb2eb0f4ce8526f40e0049b/README.md. *Update* Quoting from the REAME above. > If you get an error that looks like. > . > Inconsistency detected by ld.so: dl-version.c: 224: _dl_check_map_versions: Assertion `needed != NULL' failed! > . > or a segmentation fault when importing apricot for the first time then you should try reinstalling numba through conda using. > . > conda install numba. > . > or. > . > pip install numba==0.39.0. > . > The issue appears to be with the most recent verson of numba, v0.40.0. Downgrading to numba v0.39.0 should solve the issue. . > . That means numba should be frozen to v0.39.0 .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:501,deployability,instal,install,501,"Its an issue with numba. See here https://github.com/jmschrei/apricot/blob/98693788ca315ceceeb2eb0f4ce8526f40e0049b/README.md. *Update* Quoting from the REAME above. > If you get an error that looks like. > . > Inconsistency detected by ld.so: dl-version.c: 224: _dl_check_map_versions: Assertion `needed != NULL' failed! > . > or a segmentation fault when importing apricot for the first time then you should try reinstalling numba through conda using. > . > conda install numba. > . > or. > . > pip install numba==0.39.0. > . > The issue appears to be with the most recent verson of numba, v0.40.0. Downgrading to numba v0.39.0 should solve the issue. . > . That means numba should be frozen to v0.39.0 .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:346,energy efficiency,fault,fault,346,"Its an issue with numba. See here https://github.com/jmschrei/apricot/blob/98693788ca315ceceeb2eb0f4ce8526f40e0049b/README.md. *Update* Quoting from the REAME above. > If you get an error that looks like. > . > Inconsistency detected by ld.so: dl-version.c: 224: _dl_check_map_versions: Assertion `needed != NULL' failed! > . > or a segmentation fault when importing apricot for the first time then you should try reinstalling numba through conda using. > . > conda install numba. > . > or. > . > pip install numba==0.39.0. > . > The issue appears to be with the most recent verson of numba, v0.40.0. Downgrading to numba v0.39.0 should solve the issue. . > . That means numba should be frozen to v0.39.0 .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:247,integrability,version,version,247,"Its an issue with numba. See here https://github.com/jmschrei/apricot/blob/98693788ca315ceceeb2eb0f4ce8526f40e0049b/README.md. *Update* Quoting from the REAME above. > If you get an error that looks like. > . > Inconsistency detected by ld.so: dl-version.c: 224: _dl_check_map_versions: Assertion `needed != NULL' failed! > . > or a segmentation fault when importing apricot for the first time then you should try reinstalling numba through conda using. > . > conda install numba. > . > or. > . > pip install numba==0.39.0. > . > The issue appears to be with the most recent verson of numba, v0.40.0. Downgrading to numba v0.39.0 should solve the issue. . > . That means numba should be frozen to v0.39.0 .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:247,modifiability,version,version,247,"Its an issue with numba. See here https://github.com/jmschrei/apricot/blob/98693788ca315ceceeb2eb0f4ce8526f40e0049b/README.md. *Update* Quoting from the REAME above. > If you get an error that looks like. > . > Inconsistency detected by ld.so: dl-version.c: 224: _dl_check_map_versions: Assertion `needed != NULL' failed! > . > or a segmentation fault when importing apricot for the first time then you should try reinstalling numba through conda using. > . > conda install numba. > . > or. > . > pip install numba==0.39.0. > . > The issue appears to be with the most recent verson of numba, v0.40.0. Downgrading to numba v0.39.0 should solve the issue. . > . That means numba should be frozen to v0.39.0 .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:182,performance,error,error,182,"Its an issue with numba. See here https://github.com/jmschrei/apricot/blob/98693788ca315ceceeb2eb0f4ce8526f40e0049b/README.md. *Update* Quoting from the REAME above. > If you get an error that looks like. > . > Inconsistency detected by ld.so: dl-version.c: 224: _dl_check_map_versions: Assertion `needed != NULL' failed! > . > or a segmentation fault when importing apricot for the first time then you should try reinstalling numba through conda using. > . > conda install numba. > . > or. > . > pip install numba==0.39.0. > . > The issue appears to be with the most recent verson of numba, v0.40.0. Downgrading to numba v0.39.0 should solve the issue. . > . That means numba should be frozen to v0.39.0 .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:346,performance,fault,fault,346,"Its an issue with numba. See here https://github.com/jmschrei/apricot/blob/98693788ca315ceceeb2eb0f4ce8526f40e0049b/README.md. *Update* Quoting from the REAME above. > If you get an error that looks like. > . > Inconsistency detected by ld.so: dl-version.c: 224: _dl_check_map_versions: Assertion `needed != NULL' failed! > . > or a segmentation fault when importing apricot for the first time then you should try reinstalling numba through conda using. > . > conda install numba. > . > or. > . > pip install numba==0.39.0. > . > The issue appears to be with the most recent verson of numba, v0.40.0. Downgrading to numba v0.39.0 should solve the issue. . > . That means numba should be frozen to v0.39.0 .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:389,performance,time,time,389,"Its an issue with numba. See here https://github.com/jmschrei/apricot/blob/98693788ca315ceceeb2eb0f4ce8526f40e0049b/README.md. *Update* Quoting from the REAME above. > If you get an error that looks like. > . > Inconsistency detected by ld.so: dl-version.c: 224: _dl_check_map_versions: Assertion `needed != NULL' failed! > . > or a segmentation fault when importing apricot for the first time then you should try reinstalling numba through conda using. > . > conda install numba. > . > or. > . > pip install numba==0.39.0. > . > The issue appears to be with the most recent verson of numba, v0.40.0. Downgrading to numba v0.39.0 should solve the issue. . > . That means numba should be frozen to v0.39.0 .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:314,reliability,fail,failed,314,"Its an issue with numba. See here https://github.com/jmschrei/apricot/blob/98693788ca315ceceeb2eb0f4ce8526f40e0049b/README.md. *Update* Quoting from the REAME above. > If you get an error that looks like. > . > Inconsistency detected by ld.so: dl-version.c: 224: _dl_check_map_versions: Assertion `needed != NULL' failed! > . > or a segmentation fault when importing apricot for the first time then you should try reinstalling numba through conda using. > . > conda install numba. > . > or. > . > pip install numba==0.39.0. > . > The issue appears to be with the most recent verson of numba, v0.40.0. Downgrading to numba v0.39.0 should solve the issue. . > . That means numba should be frozen to v0.39.0 .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:346,reliability,fault,fault,346,"Its an issue with numba. See here https://github.com/jmschrei/apricot/blob/98693788ca315ceceeb2eb0f4ce8526f40e0049b/README.md. *Update* Quoting from the REAME above. > If you get an error that looks like. > . > Inconsistency detected by ld.so: dl-version.c: 224: _dl_check_map_versions: Assertion `needed != NULL' failed! > . > or a segmentation fault when importing apricot for the first time then you should try reinstalling numba through conda using. > . > conda install numba. > . > or. > . > pip install numba==0.39.0. > . > The issue appears to be with the most recent verson of numba, v0.40.0. Downgrading to numba v0.39.0 should solve the issue. . > . That means numba should be frozen to v0.39.0 .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:128,safety,Updat,Update,128,"Its an issue with numba. See here https://github.com/jmschrei/apricot/blob/98693788ca315ceceeb2eb0f4ce8526f40e0049b/README.md. *Update* Quoting from the REAME above. > If you get an error that looks like. > . > Inconsistency detected by ld.so: dl-version.c: 224: _dl_check_map_versions: Assertion `needed != NULL' failed! > . > or a segmentation fault when importing apricot for the first time then you should try reinstalling numba through conda using. > . > conda install numba. > . > or. > . > pip install numba==0.39.0. > . > The issue appears to be with the most recent verson of numba, v0.40.0. Downgrading to numba v0.39.0 should solve the issue. . > . That means numba should be frozen to v0.39.0 .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:182,safety,error,error,182,"Its an issue with numba. See here https://github.com/jmschrei/apricot/blob/98693788ca315ceceeb2eb0f4ce8526f40e0049b/README.md. *Update* Quoting from the REAME above. > If you get an error that looks like. > . > Inconsistency detected by ld.so: dl-version.c: 224: _dl_check_map_versions: Assertion `needed != NULL' failed! > . > or a segmentation fault when importing apricot for the first time then you should try reinstalling numba through conda using. > . > conda install numba. > . > or. > . > pip install numba==0.39.0. > . > The issue appears to be with the most recent verson of numba, v0.40.0. Downgrading to numba v0.39.0 should solve the issue. . > . That means numba should be frozen to v0.39.0 .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:225,safety,detect,detected,225,"Its an issue with numba. See here https://github.com/jmschrei/apricot/blob/98693788ca315ceceeb2eb0f4ce8526f40e0049b/README.md. *Update* Quoting from the REAME above. > If you get an error that looks like. > . > Inconsistency detected by ld.so: dl-version.c: 224: _dl_check_map_versions: Assertion `needed != NULL' failed! > . > or a segmentation fault when importing apricot for the first time then you should try reinstalling numba through conda using. > . > conda install numba. > . > or. > . > pip install numba==0.39.0. > . > The issue appears to be with the most recent verson of numba, v0.40.0. Downgrading to numba v0.39.0 should solve the issue. . > . That means numba should be frozen to v0.39.0 .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:346,safety,fault,fault,346,"Its an issue with numba. See here https://github.com/jmschrei/apricot/blob/98693788ca315ceceeb2eb0f4ce8526f40e0049b/README.md. *Update* Quoting from the REAME above. > If you get an error that looks like. > . > Inconsistency detected by ld.so: dl-version.c: 224: _dl_check_map_versions: Assertion `needed != NULL' failed! > . > or a segmentation fault when importing apricot for the first time then you should try reinstalling numba through conda using. > . > conda install numba. > . > or. > . > pip install numba==0.39.0. > . > The issue appears to be with the most recent verson of numba, v0.40.0. Downgrading to numba v0.39.0 should solve the issue. . > . That means numba should be frozen to v0.39.0 .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:128,security,Updat,Update,128,"Its an issue with numba. See here https://github.com/jmschrei/apricot/blob/98693788ca315ceceeb2eb0f4ce8526f40e0049b/README.md. *Update* Quoting from the REAME above. > If you get an error that looks like. > . > Inconsistency detected by ld.so: dl-version.c: 224: _dl_check_map_versions: Assertion `needed != NULL' failed! > . > or a segmentation fault when importing apricot for the first time then you should try reinstalling numba through conda using. > . > conda install numba. > . > or. > . > pip install numba==0.39.0. > . > The issue appears to be with the most recent verson of numba, v0.40.0. Downgrading to numba v0.39.0 should solve the issue. . > . That means numba should be frozen to v0.39.0 .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:225,security,detect,detected,225,"Its an issue with numba. See here https://github.com/jmschrei/apricot/blob/98693788ca315ceceeb2eb0f4ce8526f40e0049b/README.md. *Update* Quoting from the REAME above. > If you get an error that looks like. > . > Inconsistency detected by ld.so: dl-version.c: 224: _dl_check_map_versions: Assertion `needed != NULL' failed! > . > or a segmentation fault when importing apricot for the first time then you should try reinstalling numba through conda using. > . > conda install numba. > . > or. > . > pip install numba==0.39.0. > . > The issue appears to be with the most recent verson of numba, v0.40.0. Downgrading to numba v0.39.0 should solve the issue. . > . That means numba should be frozen to v0.39.0 .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:287,testability,Assert,Assertion,287,"Its an issue with numba. See here https://github.com/jmschrei/apricot/blob/98693788ca315ceceeb2eb0f4ce8526f40e0049b/README.md. *Update* Quoting from the REAME above. > If you get an error that looks like. > . > Inconsistency detected by ld.so: dl-version.c: 224: _dl_check_map_versions: Assertion `needed != NULL' failed! > . > or a segmentation fault when importing apricot for the first time then you should try reinstalling numba through conda using. > . > conda install numba. > . > or. > . > pip install numba==0.39.0. > . > The issue appears to be with the most recent verson of numba, v0.40.0. Downgrading to numba v0.39.0 should solve the issue. . > . That means numba should be frozen to v0.39.0 .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:182,usability,error,error,182,"Its an issue with numba. See here https://github.com/jmschrei/apricot/blob/98693788ca315ceceeb2eb0f4ce8526f40e0049b/README.md. *Update* Quoting from the REAME above. > If you get an error that looks like. > . > Inconsistency detected by ld.so: dl-version.c: 224: _dl_check_map_versions: Assertion `needed != NULL' failed! > . > or a segmentation fault when importing apricot for the first time then you should try reinstalling numba through conda using. > . > conda install numba. > . > or. > . > pip install numba==0.39.0. > . > The issue appears to be with the most recent verson of numba, v0.40.0. Downgrading to numba v0.39.0 should solve the issue. . > . That means numba should be frozen to v0.39.0 .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/280:2,usability,confirm,confirm,2,I confirm that with `numba==0.39.0' the problem goes away. .,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/280
https://github.com/scverse/scanpy/issues/281:0,energy efficiency,Cool,Cool,0,"Cool! Initially, Scanpy came with a built-in command-line interface. But I threw it out after a couple of months as it was hard to maintain both the interface and the library at the same time, I also expected a bit too much from the command-line interface at the time, I guess. I'd really like to support your much better efforts for generating command line interfaces. But I'm a bit hesitant to maintain it here in the main repo - already the library is already so much to take care of and other libraries typically don't involve a ""script-wrapper"", too. What about a PyPI package `scanpy-scripts`, which directly advertise from the Scanpy main docs, we could also have a little tutorial there. Similar to that, there will be interactive visual interfaces to scanpy, again as separate packages. I tend to think that this would be the better option. But if I miss something fundamental, please convince me of the opposite.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:58,integrability,interfac,interface,58,"Cool! Initially, Scanpy came with a built-in command-line interface. But I threw it out after a couple of months as it was hard to maintain both the interface and the library at the same time, I also expected a bit too much from the command-line interface at the time, I guess. I'd really like to support your much better efforts for generating command line interfaces. But I'm a bit hesitant to maintain it here in the main repo - already the library is already so much to take care of and other libraries typically don't involve a ""script-wrapper"", too. What about a PyPI package `scanpy-scripts`, which directly advertise from the Scanpy main docs, we could also have a little tutorial there. Similar to that, there will be interactive visual interfaces to scanpy, again as separate packages. I tend to think that this would be the better option. But if I miss something fundamental, please convince me of the opposite.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:96,integrability,coupl,couple,96,"Cool! Initially, Scanpy came with a built-in command-line interface. But I threw it out after a couple of months as it was hard to maintain both the interface and the library at the same time, I also expected a bit too much from the command-line interface at the time, I guess. I'd really like to support your much better efforts for generating command line interfaces. But I'm a bit hesitant to maintain it here in the main repo - already the library is already so much to take care of and other libraries typically don't involve a ""script-wrapper"", too. What about a PyPI package `scanpy-scripts`, which directly advertise from the Scanpy main docs, we could also have a little tutorial there. Similar to that, there will be interactive visual interfaces to scanpy, again as separate packages. I tend to think that this would be the better option. But if I miss something fundamental, please convince me of the opposite.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:149,integrability,interfac,interface,149,"Cool! Initially, Scanpy came with a built-in command-line interface. But I threw it out after a couple of months as it was hard to maintain both the interface and the library at the same time, I also expected a bit too much from the command-line interface at the time, I guess. I'd really like to support your much better efforts for generating command line interfaces. But I'm a bit hesitant to maintain it here in the main repo - already the library is already so much to take care of and other libraries typically don't involve a ""script-wrapper"", too. What about a PyPI package `scanpy-scripts`, which directly advertise from the Scanpy main docs, we could also have a little tutorial there. Similar to that, there will be interactive visual interfaces to scanpy, again as separate packages. I tend to think that this would be the better option. But if I miss something fundamental, please convince me of the opposite.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:246,integrability,interfac,interface,246,"Cool! Initially, Scanpy came with a built-in command-line interface. But I threw it out after a couple of months as it was hard to maintain both the interface and the library at the same time, I also expected a bit too much from the command-line interface at the time, I guess. I'd really like to support your much better efforts for generating command line interfaces. But I'm a bit hesitant to maintain it here in the main repo - already the library is already so much to take care of and other libraries typically don't involve a ""script-wrapper"", too. What about a PyPI package `scanpy-scripts`, which directly advertise from the Scanpy main docs, we could also have a little tutorial there. Similar to that, there will be interactive visual interfaces to scanpy, again as separate packages. I tend to think that this would be the better option. But if I miss something fundamental, please convince me of the opposite.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:358,integrability,interfac,interfaces,358,"Cool! Initially, Scanpy came with a built-in command-line interface. But I threw it out after a couple of months as it was hard to maintain both the interface and the library at the same time, I also expected a bit too much from the command-line interface at the time, I guess. I'd really like to support your much better efforts for generating command line interfaces. But I'm a bit hesitant to maintain it here in the main repo - already the library is already so much to take care of and other libraries typically don't involve a ""script-wrapper"", too. What about a PyPI package `scanpy-scripts`, which directly advertise from the Scanpy main docs, we could also have a little tutorial there. Similar to that, there will be interactive visual interfaces to scanpy, again as separate packages. I tend to think that this would be the better option. But if I miss something fundamental, please convince me of the opposite.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:541,integrability,wrap,wrapper,541,"Cool! Initially, Scanpy came with a built-in command-line interface. But I threw it out after a couple of months as it was hard to maintain both the interface and the library at the same time, I also expected a bit too much from the command-line interface at the time, I guess. I'd really like to support your much better efforts for generating command line interfaces. But I'm a bit hesitant to maintain it here in the main repo - already the library is already so much to take care of and other libraries typically don't involve a ""script-wrapper"", too. What about a PyPI package `scanpy-scripts`, which directly advertise from the Scanpy main docs, we could also have a little tutorial there. Similar to that, there will be interactive visual interfaces to scanpy, again as separate packages. I tend to think that this would be the better option. But if I miss something fundamental, please convince me of the opposite.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:746,integrability,interfac,interfaces,746,"Cool! Initially, Scanpy came with a built-in command-line interface. But I threw it out after a couple of months as it was hard to maintain both the interface and the library at the same time, I also expected a bit too much from the command-line interface at the time, I guess. I'd really like to support your much better efforts for generating command line interfaces. But I'm a bit hesitant to maintain it here in the main repo - already the library is already so much to take care of and other libraries typically don't involve a ""script-wrapper"", too. What about a PyPI package `scanpy-scripts`, which directly advertise from the Scanpy main docs, we could also have a little tutorial there. Similar to that, there will be interactive visual interfaces to scanpy, again as separate packages. I tend to think that this would be the better option. But if I miss something fundamental, please convince me of the opposite.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:58,interoperability,interfac,interface,58,"Cool! Initially, Scanpy came with a built-in command-line interface. But I threw it out after a couple of months as it was hard to maintain both the interface and the library at the same time, I also expected a bit too much from the command-line interface at the time, I guess. I'd really like to support your much better efforts for generating command line interfaces. But I'm a bit hesitant to maintain it here in the main repo - already the library is already so much to take care of and other libraries typically don't involve a ""script-wrapper"", too. What about a PyPI package `scanpy-scripts`, which directly advertise from the Scanpy main docs, we could also have a little tutorial there. Similar to that, there will be interactive visual interfaces to scanpy, again as separate packages. I tend to think that this would be the better option. But if I miss something fundamental, please convince me of the opposite.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:149,interoperability,interfac,interface,149,"Cool! Initially, Scanpy came with a built-in command-line interface. But I threw it out after a couple of months as it was hard to maintain both the interface and the library at the same time, I also expected a bit too much from the command-line interface at the time, I guess. I'd really like to support your much better efforts for generating command line interfaces. But I'm a bit hesitant to maintain it here in the main repo - already the library is already so much to take care of and other libraries typically don't involve a ""script-wrapper"", too. What about a PyPI package `scanpy-scripts`, which directly advertise from the Scanpy main docs, we could also have a little tutorial there. Similar to that, there will be interactive visual interfaces to scanpy, again as separate packages. I tend to think that this would be the better option. But if I miss something fundamental, please convince me of the opposite.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:246,interoperability,interfac,interface,246,"Cool! Initially, Scanpy came with a built-in command-line interface. But I threw it out after a couple of months as it was hard to maintain both the interface and the library at the same time, I also expected a bit too much from the command-line interface at the time, I guess. I'd really like to support your much better efforts for generating command line interfaces. But I'm a bit hesitant to maintain it here in the main repo - already the library is already so much to take care of and other libraries typically don't involve a ""script-wrapper"", too. What about a PyPI package `scanpy-scripts`, which directly advertise from the Scanpy main docs, we could also have a little tutorial there. Similar to that, there will be interactive visual interfaces to scanpy, again as separate packages. I tend to think that this would be the better option. But if I miss something fundamental, please convince me of the opposite.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:358,interoperability,interfac,interfaces,358,"Cool! Initially, Scanpy came with a built-in command-line interface. But I threw it out after a couple of months as it was hard to maintain both the interface and the library at the same time, I also expected a bit too much from the command-line interface at the time, I guess. I'd really like to support your much better efforts for generating command line interfaces. But I'm a bit hesitant to maintain it here in the main repo - already the library is already so much to take care of and other libraries typically don't involve a ""script-wrapper"", too. What about a PyPI package `scanpy-scripts`, which directly advertise from the Scanpy main docs, we could also have a little tutorial there. Similar to that, there will be interactive visual interfaces to scanpy, again as separate packages. I tend to think that this would be the better option. But if I miss something fundamental, please convince me of the opposite.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:541,interoperability,wrapper,wrapper,541,"Cool! Initially, Scanpy came with a built-in command-line interface. But I threw it out after a couple of months as it was hard to maintain both the interface and the library at the same time, I also expected a bit too much from the command-line interface at the time, I guess. I'd really like to support your much better efforts for generating command line interfaces. But I'm a bit hesitant to maintain it here in the main repo - already the library is already so much to take care of and other libraries typically don't involve a ""script-wrapper"", too. What about a PyPI package `scanpy-scripts`, which directly advertise from the Scanpy main docs, we could also have a little tutorial there. Similar to that, there will be interactive visual interfaces to scanpy, again as separate packages. I tend to think that this would be the better option. But if I miss something fundamental, please convince me of the opposite.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:746,interoperability,interfac,interfaces,746,"Cool! Initially, Scanpy came with a built-in command-line interface. But I threw it out after a couple of months as it was hard to maintain both the interface and the library at the same time, I also expected a bit too much from the command-line interface at the time, I guess. I'd really like to support your much better efforts for generating command line interfaces. But I'm a bit hesitant to maintain it here in the main repo - already the library is already so much to take care of and other libraries typically don't involve a ""script-wrapper"", too. What about a PyPI package `scanpy-scripts`, which directly advertise from the Scanpy main docs, we could also have a little tutorial there. Similar to that, there will be interactive visual interfaces to scanpy, again as separate packages. I tend to think that this would be the better option. But if I miss something fundamental, please convince me of the opposite.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:58,modifiability,interfac,interface,58,"Cool! Initially, Scanpy came with a built-in command-line interface. But I threw it out after a couple of months as it was hard to maintain both the interface and the library at the same time, I also expected a bit too much from the command-line interface at the time, I guess. I'd really like to support your much better efforts for generating command line interfaces. But I'm a bit hesitant to maintain it here in the main repo - already the library is already so much to take care of and other libraries typically don't involve a ""script-wrapper"", too. What about a PyPI package `scanpy-scripts`, which directly advertise from the Scanpy main docs, we could also have a little tutorial there. Similar to that, there will be interactive visual interfaces to scanpy, again as separate packages. I tend to think that this would be the better option. But if I miss something fundamental, please convince me of the opposite.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:96,modifiability,coupl,couple,96,"Cool! Initially, Scanpy came with a built-in command-line interface. But I threw it out after a couple of months as it was hard to maintain both the interface and the library at the same time, I also expected a bit too much from the command-line interface at the time, I guess. I'd really like to support your much better efforts for generating command line interfaces. But I'm a bit hesitant to maintain it here in the main repo - already the library is already so much to take care of and other libraries typically don't involve a ""script-wrapper"", too. What about a PyPI package `scanpy-scripts`, which directly advertise from the Scanpy main docs, we could also have a little tutorial there. Similar to that, there will be interactive visual interfaces to scanpy, again as separate packages. I tend to think that this would be the better option. But if I miss something fundamental, please convince me of the opposite.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:131,modifiability,maintain,maintain,131,"Cool! Initially, Scanpy came with a built-in command-line interface. But I threw it out after a couple of months as it was hard to maintain both the interface and the library at the same time, I also expected a bit too much from the command-line interface at the time, I guess. I'd really like to support your much better efforts for generating command line interfaces. But I'm a bit hesitant to maintain it here in the main repo - already the library is already so much to take care of and other libraries typically don't involve a ""script-wrapper"", too. What about a PyPI package `scanpy-scripts`, which directly advertise from the Scanpy main docs, we could also have a little tutorial there. Similar to that, there will be interactive visual interfaces to scanpy, again as separate packages. I tend to think that this would be the better option. But if I miss something fundamental, please convince me of the opposite.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:149,modifiability,interfac,interface,149,"Cool! Initially, Scanpy came with a built-in command-line interface. But I threw it out after a couple of months as it was hard to maintain both the interface and the library at the same time, I also expected a bit too much from the command-line interface at the time, I guess. I'd really like to support your much better efforts for generating command line interfaces. But I'm a bit hesitant to maintain it here in the main repo - already the library is already so much to take care of and other libraries typically don't involve a ""script-wrapper"", too. What about a PyPI package `scanpy-scripts`, which directly advertise from the Scanpy main docs, we could also have a little tutorial there. Similar to that, there will be interactive visual interfaces to scanpy, again as separate packages. I tend to think that this would be the better option. But if I miss something fundamental, please convince me of the opposite.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:246,modifiability,interfac,interface,246,"Cool! Initially, Scanpy came with a built-in command-line interface. But I threw it out after a couple of months as it was hard to maintain both the interface and the library at the same time, I also expected a bit too much from the command-line interface at the time, I guess. I'd really like to support your much better efforts for generating command line interfaces. But I'm a bit hesitant to maintain it here in the main repo - already the library is already so much to take care of and other libraries typically don't involve a ""script-wrapper"", too. What about a PyPI package `scanpy-scripts`, which directly advertise from the Scanpy main docs, we could also have a little tutorial there. Similar to that, there will be interactive visual interfaces to scanpy, again as separate packages. I tend to think that this would be the better option. But if I miss something fundamental, please convince me of the opposite.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:358,modifiability,interfac,interfaces,358,"Cool! Initially, Scanpy came with a built-in command-line interface. But I threw it out after a couple of months as it was hard to maintain both the interface and the library at the same time, I also expected a bit too much from the command-line interface at the time, I guess. I'd really like to support your much better efforts for generating command line interfaces. But I'm a bit hesitant to maintain it here in the main repo - already the library is already so much to take care of and other libraries typically don't involve a ""script-wrapper"", too. What about a PyPI package `scanpy-scripts`, which directly advertise from the Scanpy main docs, we could also have a little tutorial there. Similar to that, there will be interactive visual interfaces to scanpy, again as separate packages. I tend to think that this would be the better option. But if I miss something fundamental, please convince me of the opposite.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:396,modifiability,maintain,maintain,396,"Cool! Initially, Scanpy came with a built-in command-line interface. But I threw it out after a couple of months as it was hard to maintain both the interface and the library at the same time, I also expected a bit too much from the command-line interface at the time, I guess. I'd really like to support your much better efforts for generating command line interfaces. But I'm a bit hesitant to maintain it here in the main repo - already the library is already so much to take care of and other libraries typically don't involve a ""script-wrapper"", too. What about a PyPI package `scanpy-scripts`, which directly advertise from the Scanpy main docs, we could also have a little tutorial there. Similar to that, there will be interactive visual interfaces to scanpy, again as separate packages. I tend to think that this would be the better option. But if I miss something fundamental, please convince me of the opposite.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:574,modifiability,pac,package,574,"Cool! Initially, Scanpy came with a built-in command-line interface. But I threw it out after a couple of months as it was hard to maintain both the interface and the library at the same time, I also expected a bit too much from the command-line interface at the time, I guess. I'd really like to support your much better efforts for generating command line interfaces. But I'm a bit hesitant to maintain it here in the main repo - already the library is already so much to take care of and other libraries typically don't involve a ""script-wrapper"", too. What about a PyPI package `scanpy-scripts`, which directly advertise from the Scanpy main docs, we could also have a little tutorial there. Similar to that, there will be interactive visual interfaces to scanpy, again as separate packages. I tend to think that this would be the better option. But if I miss something fundamental, please convince me of the opposite.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:746,modifiability,interfac,interfaces,746,"Cool! Initially, Scanpy came with a built-in command-line interface. But I threw it out after a couple of months as it was hard to maintain both the interface and the library at the same time, I also expected a bit too much from the command-line interface at the time, I guess. I'd really like to support your much better efforts for generating command line interfaces. But I'm a bit hesitant to maintain it here in the main repo - already the library is already so much to take care of and other libraries typically don't involve a ""script-wrapper"", too. What about a PyPI package `scanpy-scripts`, which directly advertise from the Scanpy main docs, we could also have a little tutorial there. Similar to that, there will be interactive visual interfaces to scanpy, again as separate packages. I tend to think that this would be the better option. But if I miss something fundamental, please convince me of the opposite.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:786,modifiability,pac,packages,786,"Cool! Initially, Scanpy came with a built-in command-line interface. But I threw it out after a couple of months as it was hard to maintain both the interface and the library at the same time, I also expected a bit too much from the command-line interface at the time, I guess. I'd really like to support your much better efforts for generating command line interfaces. But I'm a bit hesitant to maintain it here in the main repo - already the library is already so much to take care of and other libraries typically don't involve a ""script-wrapper"", too. What about a PyPI package `scanpy-scripts`, which directly advertise from the Scanpy main docs, we could also have a little tutorial there. Similar to that, there will be interactive visual interfaces to scanpy, again as separate packages. I tend to think that this would be the better option. But if I miss something fundamental, please convince me of the opposite.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:187,performance,time,time,187,"Cool! Initially, Scanpy came with a built-in command-line interface. But I threw it out after a couple of months as it was hard to maintain both the interface and the library at the same time, I also expected a bit too much from the command-line interface at the time, I guess. I'd really like to support your much better efforts for generating command line interfaces. But I'm a bit hesitant to maintain it here in the main repo - already the library is already so much to take care of and other libraries typically don't involve a ""script-wrapper"", too. What about a PyPI package `scanpy-scripts`, which directly advertise from the Scanpy main docs, we could also have a little tutorial there. Similar to that, there will be interactive visual interfaces to scanpy, again as separate packages. I tend to think that this would be the better option. But if I miss something fundamental, please convince me of the opposite.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:263,performance,time,time,263,"Cool! Initially, Scanpy came with a built-in command-line interface. But I threw it out after a couple of months as it was hard to maintain both the interface and the library at the same time, I also expected a bit too much from the command-line interface at the time, I guess. I'd really like to support your much better efforts for generating command line interfaces. But I'm a bit hesitant to maintain it here in the main repo - already the library is already so much to take care of and other libraries typically don't involve a ""script-wrapper"", too. What about a PyPI package `scanpy-scripts`, which directly advertise from the Scanpy main docs, we could also have a little tutorial there. Similar to that, there will be interactive visual interfaces to scanpy, again as separate packages. I tend to think that this would be the better option. But if I miss something fundamental, please convince me of the opposite.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:131,safety,maintain,maintain,131,"Cool! Initially, Scanpy came with a built-in command-line interface. But I threw it out after a couple of months as it was hard to maintain both the interface and the library at the same time, I also expected a bit too much from the command-line interface at the time, I guess. I'd really like to support your much better efforts for generating command line interfaces. But I'm a bit hesitant to maintain it here in the main repo - already the library is already so much to take care of and other libraries typically don't involve a ""script-wrapper"", too. What about a PyPI package `scanpy-scripts`, which directly advertise from the Scanpy main docs, we could also have a little tutorial there. Similar to that, there will be interactive visual interfaces to scanpy, again as separate packages. I tend to think that this would be the better option. But if I miss something fundamental, please convince me of the opposite.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:396,safety,maintain,maintain,396,"Cool! Initially, Scanpy came with a built-in command-line interface. But I threw it out after a couple of months as it was hard to maintain both the interface and the library at the same time, I also expected a bit too much from the command-line interface at the time, I guess. I'd really like to support your much better efforts for generating command line interfaces. But I'm a bit hesitant to maintain it here in the main repo - already the library is already so much to take care of and other libraries typically don't involve a ""script-wrapper"", too. What about a PyPI package `scanpy-scripts`, which directly advertise from the Scanpy main docs, we could also have a little tutorial there. Similar to that, there will be interactive visual interfaces to scanpy, again as separate packages. I tend to think that this would be the better option. But if I miss something fundamental, please convince me of the opposite.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:45,security,command-lin,command-line,45,"Cool! Initially, Scanpy came with a built-in command-line interface. But I threw it out after a couple of months as it was hard to maintain both the interface and the library at the same time, I also expected a bit too much from the command-line interface at the time, I guess. I'd really like to support your much better efforts for generating command line interfaces. But I'm a bit hesitant to maintain it here in the main repo - already the library is already so much to take care of and other libraries typically don't involve a ""script-wrapper"", too. What about a PyPI package `scanpy-scripts`, which directly advertise from the Scanpy main docs, we could also have a little tutorial there. Similar to that, there will be interactive visual interfaces to scanpy, again as separate packages. I tend to think that this would be the better option. But if I miss something fundamental, please convince me of the opposite.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:233,security,command-lin,command-line,233,"Cool! Initially, Scanpy came with a built-in command-line interface. But I threw it out after a couple of months as it was hard to maintain both the interface and the library at the same time, I also expected a bit too much from the command-line interface at the time, I guess. I'd really like to support your much better efforts for generating command line interfaces. But I'm a bit hesitant to maintain it here in the main repo - already the library is already so much to take care of and other libraries typically don't involve a ""script-wrapper"", too. What about a PyPI package `scanpy-scripts`, which directly advertise from the Scanpy main docs, we could also have a little tutorial there. Similar to that, there will be interactive visual interfaces to scanpy, again as separate packages. I tend to think that this would be the better option. But if I miss something fundamental, please convince me of the opposite.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:96,testability,coupl,couple,96,"Cool! Initially, Scanpy came with a built-in command-line interface. But I threw it out after a couple of months as it was hard to maintain both the interface and the library at the same time, I also expected a bit too much from the command-line interface at the time, I guess. I'd really like to support your much better efforts for generating command line interfaces. But I'm a bit hesitant to maintain it here in the main repo - already the library is already so much to take care of and other libraries typically don't involve a ""script-wrapper"", too. What about a PyPI package `scanpy-scripts`, which directly advertise from the Scanpy main docs, we could also have a little tutorial there. Similar to that, there will be interactive visual interfaces to scanpy, again as separate packages. I tend to think that this would be the better option. But if I miss something fundamental, please convince me of the opposite.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:45,usability,command,command-line,45,"Cool! Initially, Scanpy came with a built-in command-line interface. But I threw it out after a couple of months as it was hard to maintain both the interface and the library at the same time, I also expected a bit too much from the command-line interface at the time, I guess. I'd really like to support your much better efforts for generating command line interfaces. But I'm a bit hesitant to maintain it here in the main repo - already the library is already so much to take care of and other libraries typically don't involve a ""script-wrapper"", too. What about a PyPI package `scanpy-scripts`, which directly advertise from the Scanpy main docs, we could also have a little tutorial there. Similar to that, there will be interactive visual interfaces to scanpy, again as separate packages. I tend to think that this would be the better option. But if I miss something fundamental, please convince me of the opposite.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:233,usability,command,command-line,233,"Cool! Initially, Scanpy came with a built-in command-line interface. But I threw it out after a couple of months as it was hard to maintain both the interface and the library at the same time, I also expected a bit too much from the command-line interface at the time, I guess. I'd really like to support your much better efforts for generating command line interfaces. But I'm a bit hesitant to maintain it here in the main repo - already the library is already so much to take care of and other libraries typically don't involve a ""script-wrapper"", too. What about a PyPI package `scanpy-scripts`, which directly advertise from the Scanpy main docs, we could also have a little tutorial there. Similar to that, there will be interactive visual interfaces to scanpy, again as separate packages. I tend to think that this would be the better option. But if I miss something fundamental, please convince me of the opposite.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:297,usability,support,support,297,"Cool! Initially, Scanpy came with a built-in command-line interface. But I threw it out after a couple of months as it was hard to maintain both the interface and the library at the same time, I also expected a bit too much from the command-line interface at the time, I guess. I'd really like to support your much better efforts for generating command line interfaces. But I'm a bit hesitant to maintain it here in the main repo - already the library is already so much to take care of and other libraries typically don't involve a ""script-wrapper"", too. What about a PyPI package `scanpy-scripts`, which directly advertise from the Scanpy main docs, we could also have a little tutorial there. Similar to that, there will be interactive visual interfaces to scanpy, again as separate packages. I tend to think that this would be the better option. But if I miss something fundamental, please convince me of the opposite.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:345,usability,command,command,345,"Cool! Initially, Scanpy came with a built-in command-line interface. But I threw it out after a couple of months as it was hard to maintain both the interface and the library at the same time, I also expected a bit too much from the command-line interface at the time, I guess. I'd really like to support your much better efforts for generating command line interfaces. But I'm a bit hesitant to maintain it here in the main repo - already the library is already so much to take care of and other libraries typically don't involve a ""script-wrapper"", too. What about a PyPI package `scanpy-scripts`, which directly advertise from the Scanpy main docs, we could also have a little tutorial there. Similar to that, there will be interactive visual interfaces to scanpy, again as separate packages. I tend to think that this would be the better option. But if I miss something fundamental, please convince me of the opposite.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:727,usability,interact,interactive,727,"Cool! Initially, Scanpy came with a built-in command-line interface. But I threw it out after a couple of months as it was hard to maintain both the interface and the library at the same time, I also expected a bit too much from the command-line interface at the time, I guess. I'd really like to support your much better efforts for generating command line interfaces. But I'm a bit hesitant to maintain it here in the main repo - already the library is already so much to take care of and other libraries typically don't involve a ""script-wrapper"", too. What about a PyPI package `scanpy-scripts`, which directly advertise from the Scanpy main docs, we could also have a little tutorial there. Similar to that, there will be interactive visual interfaces to scanpy, again as separate packages. I tend to think that this would be the better option. But if I miss something fundamental, please convince me of the opposite.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:739,usability,visual,visual,739,"Cool! Initially, Scanpy came with a built-in command-line interface. But I threw it out after a couple of months as it was hard to maintain both the interface and the library at the same time, I also expected a bit too much from the command-line interface at the time, I guess. I'd really like to support your much better efforts for generating command line interfaces. But I'm a bit hesitant to maintain it here in the main repo - already the library is already so much to take care of and other libraries typically don't involve a ""script-wrapper"", too. What about a PyPI package `scanpy-scripts`, which directly advertise from the Scanpy main docs, we could also have a little tutorial there. Similar to that, there will be interactive visual interfaces to scanpy, again as separate packages. I tend to think that this would be the better option. But if I miss something fundamental, please convince me of the opposite.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:12,deployability,continu,continue,12,"Yes, we can continue with the scanpy-scripts separate package if you prefer, and make sure that things are properly linked. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:54,modifiability,pac,package,54,"Yes, we can continue with the scanpy-scripts separate package if you prefer, and make sure that things are properly linked. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:69,usability,prefer,prefer,69,"Yes, we can continue with the scanpy-scripts separate package if you prefer, and make sure that things are properly linked. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:90,usability,prefer,prefer,90,If you think this is a viable option without great disadvantages I have to admit that I'd prefer that. 🙂,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:267,energy efficiency,current,current,267,"hey all! fun discussion. as an FYI, over on the `cellxgene` project we're proposing a little standalone CLI that does some basic preprocessing of data with `scanpy` for the purpose of then being visualized (includes some basic filtering, pca, neighbors, umap, etc.). current PR is here https://github.com/chanzuckerberg/cellxgene/pull/358. in general, really like @falexwolf 's idea of a `scanpy-scripts` CLI, and would be happy to contribute to that discussion, and potentially use / reuse some of what we're doing if useful",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:227,integrability,filter,filtering,227,"hey all! fun discussion. as an FYI, over on the `cellxgene` project we're proposing a little standalone CLI that does some basic preprocessing of data with `scanpy` for the purpose of then being visualized (includes some basic filtering, pca, neighbors, umap, etc.). current PR is here https://github.com/chanzuckerberg/cellxgene/pull/358. in general, really like @falexwolf 's idea of a `scanpy-scripts` CLI, and would be happy to contribute to that discussion, and potentially use / reuse some of what we're doing if useful",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:485,modifiability,reu,reuse,485,"hey all! fun discussion. as an FYI, over on the `cellxgene` project we're proposing a little standalone CLI that does some basic preprocessing of data with `scanpy` for the purpose of then being visualized (includes some basic filtering, pca, neighbors, umap, etc.). current PR is here https://github.com/chanzuckerberg/cellxgene/pull/358. in general, really like @falexwolf 's idea of a `scanpy-scripts` CLI, and would be happy to contribute to that discussion, and potentially use / reuse some of what we're doing if useful",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:113,reliability,doe,does,113,"hey all! fun discussion. as an FYI, over on the `cellxgene` project we're proposing a little standalone CLI that does some basic preprocessing of data with `scanpy` for the purpose of then being visualized (includes some basic filtering, pca, neighbors, umap, etc.). current PR is here https://github.com/chanzuckerberg/cellxgene/pull/358. in general, really like @falexwolf 's idea of a `scanpy-scripts` CLI, and would be happy to contribute to that discussion, and potentially use / reuse some of what we're doing if useful",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:195,usability,visual,visualized,195,"hey all! fun discussion. as an FYI, over on the `cellxgene` project we're proposing a little standalone CLI that does some basic preprocessing of data with `scanpy` for the purpose of then being visualized (includes some basic filtering, pca, neighbors, umap, etc.). current PR is here https://github.com/chanzuckerberg/cellxgene/pull/358. in general, really like @falexwolf 's idea of a `scanpy-scripts` CLI, and would be happy to contribute to that discussion, and potentially use / reuse some of what we're doing if useful",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:47,deployability,modul,modules,47,"Hi @freeman-lab, we have now quite a number of modules:. https://github.com/ebi-gene-expression-group/scanpy-scripts. And, as our other seurat-scripts, sc3-scripts and scater-scripts, it is bioconda installable (or in the way to be). We would be happy to accept your module, although it would be good to see how much it overlaps or not with existing parts already there, to find the best way to integrate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:199,deployability,instal,installable,199,"Hi @freeman-lab, we have now quite a number of modules:. https://github.com/ebi-gene-expression-group/scanpy-scripts. And, as our other seurat-scripts, sc3-scripts and scater-scripts, it is bioconda installable (or in the way to be). We would be happy to accept your module, although it would be good to see how much it overlaps or not with existing parts already there, to find the best way to integrate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:267,deployability,modul,module,267,"Hi @freeman-lab, we have now quite a number of modules:. https://github.com/ebi-gene-expression-group/scanpy-scripts. And, as our other seurat-scripts, sc3-scripts and scater-scripts, it is bioconda installable (or in the way to be). We would be happy to accept your module, although it would be good to see how much it overlaps or not with existing parts already there, to find the best way to integrate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:395,deployability,integr,integrate,395,"Hi @freeman-lab, we have now quite a number of modules:. https://github.com/ebi-gene-expression-group/scanpy-scripts. And, as our other seurat-scripts, sc3-scripts and scater-scripts, it is bioconda installable (or in the way to be). We would be happy to accept your module, although it would be good to see how much it overlaps or not with existing parts already there, to find the best way to integrate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:395,integrability,integr,integrate,395,"Hi @freeman-lab, we have now quite a number of modules:. https://github.com/ebi-gene-expression-group/scanpy-scripts. And, as our other seurat-scripts, sc3-scripts and scater-scripts, it is bioconda installable (or in the way to be). We would be happy to accept your module, although it would be good to see how much it overlaps or not with existing parts already there, to find the best way to integrate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:395,interoperability,integr,integrate,395,"Hi @freeman-lab, we have now quite a number of modules:. https://github.com/ebi-gene-expression-group/scanpy-scripts. And, as our other seurat-scripts, sc3-scripts and scater-scripts, it is bioconda installable (or in the way to be). We would be happy to accept your module, although it would be good to see how much it overlaps or not with existing parts already there, to find the best way to integrate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:47,modifiability,modul,modules,47,"Hi @freeman-lab, we have now quite a number of modules:. https://github.com/ebi-gene-expression-group/scanpy-scripts. And, as our other seurat-scripts, sc3-scripts and scater-scripts, it is bioconda installable (or in the way to be). We would be happy to accept your module, although it would be good to see how much it overlaps or not with existing parts already there, to find the best way to integrate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:267,modifiability,modul,module,267,"Hi @freeman-lab, we have now quite a number of modules:. https://github.com/ebi-gene-expression-group/scanpy-scripts. And, as our other seurat-scripts, sc3-scripts and scater-scripts, it is bioconda installable (or in the way to be). We would be happy to accept your module, although it would be good to see how much it overlaps or not with existing parts already there, to find the best way to integrate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:395,modifiability,integr,integrate,395,"Hi @freeman-lab, we have now quite a number of modules:. https://github.com/ebi-gene-expression-group/scanpy-scripts. And, as our other seurat-scripts, sc3-scripts and scater-scripts, it is bioconda installable (or in the way to be). We would be happy to accept your module, although it would be good to see how much it overlaps or not with existing parts already there, to find the best way to integrate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:395,reliability,integr,integrate,395,"Hi @freeman-lab, we have now quite a number of modules:. https://github.com/ebi-gene-expression-group/scanpy-scripts. And, as our other seurat-scripts, sc3-scripts and scater-scripts, it is bioconda installable (or in the way to be). We would be happy to accept your module, although it would be good to see how much it overlaps or not with existing parts already there, to find the best way to integrate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:47,safety,modul,modules,47,"Hi @freeman-lab, we have now quite a number of modules:. https://github.com/ebi-gene-expression-group/scanpy-scripts. And, as our other seurat-scripts, sc3-scripts and scater-scripts, it is bioconda installable (or in the way to be). We would be happy to accept your module, although it would be good to see how much it overlaps or not with existing parts already there, to find the best way to integrate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:267,safety,modul,module,267,"Hi @freeman-lab, we have now quite a number of modules:. https://github.com/ebi-gene-expression-group/scanpy-scripts. And, as our other seurat-scripts, sc3-scripts and scater-scripts, it is bioconda installable (or in the way to be). We would be happy to accept your module, although it would be good to see how much it overlaps or not with existing parts already there, to find the best way to integrate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:395,security,integr,integrate,395,"Hi @freeman-lab, we have now quite a number of modules:. https://github.com/ebi-gene-expression-group/scanpy-scripts. And, as our other seurat-scripts, sc3-scripts and scater-scripts, it is bioconda installable (or in the way to be). We would be happy to accept your module, although it would be good to see how much it overlaps or not with existing parts already there, to find the best way to integrate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:395,testability,integr,integrate,395,"Hi @freeman-lab, we have now quite a number of modules:. https://github.com/ebi-gene-expression-group/scanpy-scripts. And, as our other seurat-scripts, sc3-scripts and scater-scripts, it is bioconda installable (or in the way to be). We would be happy to accept your module, although it would be good to see how much it overlaps or not with existing parts already there, to find the best way to integrate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:180,deployability,instal,installs,180,"I have an idea. We provide a base `scanpy` command here. It works the same as the `jupyter` binary, i.e. it searches for commands named `scanpy-something` in the `$PATH`. Once one installs `scanpy-scripts`, there will be many commands like `scanpy-filter-genes` (without `.py`, I can assist in making that happen via entry-points and publishing it to PyPI). Calling `scanpy` without arguments will then list all those commands, and `scanpy filter-genes` (note the space) will call the respective command.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:248,integrability,filter,filter-genes,248,"I have an idea. We provide a base `scanpy` command here. It works the same as the `jupyter` binary, i.e. it searches for commands named `scanpy-something` in the `$PATH`. Once one installs `scanpy-scripts`, there will be many commands like `scanpy-filter-genes` (without `.py`, I can assist in making that happen via entry-points and publishing it to PyPI). Calling `scanpy` without arguments will then list all those commands, and `scanpy filter-genes` (note the space) will call the respective command.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:334,integrability,pub,publishing,334,"I have an idea. We provide a base `scanpy` command here. It works the same as the `jupyter` binary, i.e. it searches for commands named `scanpy-something` in the `$PATH`. Once one installs `scanpy-scripts`, there will be many commands like `scanpy-filter-genes` (without `.py`, I can assist in making that happen via entry-points and publishing it to PyPI). Calling `scanpy` without arguments will then list all those commands, and `scanpy filter-genes` (note the space) will call the respective command.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:440,integrability,filter,filter-genes,440,"I have an idea. We provide a base `scanpy` command here. It works the same as the `jupyter` binary, i.e. it searches for commands named `scanpy-something` in the `$PATH`. Once one installs `scanpy-scripts`, there will be many commands like `scanpy-filter-genes` (without `.py`, I can assist in making that happen via entry-points and publishing it to PyPI). Calling `scanpy` without arguments will then list all those commands, and `scanpy filter-genes` (note the space) will call the respective command.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:43,usability,command,command,43,"I have an idea. We provide a base `scanpy` command here. It works the same as the `jupyter` binary, i.e. it searches for commands named `scanpy-something` in the `$PATH`. Once one installs `scanpy-scripts`, there will be many commands like `scanpy-filter-genes` (without `.py`, I can assist in making that happen via entry-points and publishing it to PyPI). Calling `scanpy` without arguments will then list all those commands, and `scanpy filter-genes` (note the space) will call the respective command.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:121,usability,command,commands,121,"I have an idea. We provide a base `scanpy` command here. It works the same as the `jupyter` binary, i.e. it searches for commands named `scanpy-something` in the `$PATH`. Once one installs `scanpy-scripts`, there will be many commands like `scanpy-filter-genes` (without `.py`, I can assist in making that happen via entry-points and publishing it to PyPI). Calling `scanpy` without arguments will then list all those commands, and `scanpy filter-genes` (note the space) will call the respective command.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:226,usability,command,commands,226,"I have an idea. We provide a base `scanpy` command here. It works the same as the `jupyter` binary, i.e. it searches for commands named `scanpy-something` in the `$PATH`. Once one installs `scanpy-scripts`, there will be many commands like `scanpy-filter-genes` (without `.py`, I can assist in making that happen via entry-points and publishing it to PyPI). Calling `scanpy` without arguments will then list all those commands, and `scanpy filter-genes` (note the space) will call the respective command.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:418,usability,command,commands,418,"I have an idea. We provide a base `scanpy` command here. It works the same as the `jupyter` binary, i.e. it searches for commands named `scanpy-something` in the `$PATH`. Once one installs `scanpy-scripts`, there will be many commands like `scanpy-filter-genes` (without `.py`, I can assist in making that happen via entry-points and publishing it to PyPI). Calling `scanpy` without arguments will then list all those commands, and `scanpy filter-genes` (note the space) will call the respective command.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:496,usability,command,command,496,"I have an idea. We provide a base `scanpy` command here. It works the same as the `jupyter` binary, i.e. it searches for commands named `scanpy-something` in the `$PATH`. Once one installs `scanpy-scripts`, there will be many commands like `scanpy-filter-genes` (without `.py`, I can assist in making that happen via entry-points and publishing it to PyPI). Calling `scanpy` without arguments will then list all those commands, and `scanpy filter-genes` (note the space) will call the respective command.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:437,availability,mainten,maintenance,437,"Sounds good, we are currently hosting the whole scanpy-scripts as a bioconda package, but we could look into having it as pip installable as well. Does this means that scanpy administrators are happy to have the scanpy-scripts code poured here to make them pip installable, or that you want to contribute the pip packaging to the repo where we currently have scanpy-scripts? We have travis testing for our scripts layer, that would make maintenance easier on your side, as it would detect any changes on scanpy that break the scripts layers (taking you to only break API if extremely necessary - in turn making your tool more stable for external users).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:126,deployability,instal,installable,126,"Sounds good, we are currently hosting the whole scanpy-scripts as a bioconda package, but we could look into having it as pip installable as well. Does this means that scanpy administrators are happy to have the scanpy-scripts code poured here to make them pip installable, or that you want to contribute the pip packaging to the repo where we currently have scanpy-scripts? We have travis testing for our scripts layer, that would make maintenance easier on your side, as it would detect any changes on scanpy that break the scripts layers (taking you to only break API if extremely necessary - in turn making your tool more stable for external users).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:261,deployability,instal,installable,261,"Sounds good, we are currently hosting the whole scanpy-scripts as a bioconda package, but we could look into having it as pip installable as well. Does this means that scanpy administrators are happy to have the scanpy-scripts code poured here to make them pip installable, or that you want to contribute the pip packaging to the repo where we currently have scanpy-scripts? We have travis testing for our scripts layer, that would make maintenance easier on your side, as it would detect any changes on scanpy that break the scripts layers (taking you to only break API if extremely necessary - in turn making your tool more stable for external users).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:567,deployability,API,API,567,"Sounds good, we are currently hosting the whole scanpy-scripts as a bioconda package, but we could look into having it as pip installable as well. Does this means that scanpy administrators are happy to have the scanpy-scripts code poured here to make them pip installable, or that you want to contribute the pip packaging to the repo where we currently have scanpy-scripts? We have travis testing for our scripts layer, that would make maintenance easier on your side, as it would detect any changes on scanpy that break the scripts layers (taking you to only break API if extremely necessary - in turn making your tool more stable for external users).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:20,energy efficiency,current,currently,20,"Sounds good, we are currently hosting the whole scanpy-scripts as a bioconda package, but we could look into having it as pip installable as well. Does this means that scanpy administrators are happy to have the scanpy-scripts code poured here to make them pip installable, or that you want to contribute the pip packaging to the repo where we currently have scanpy-scripts? We have travis testing for our scripts layer, that would make maintenance easier on your side, as it would detect any changes on scanpy that break the scripts layers (taking you to only break API if extremely necessary - in turn making your tool more stable for external users).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:344,energy efficiency,current,currently,344,"Sounds good, we are currently hosting the whole scanpy-scripts as a bioconda package, but we could look into having it as pip installable as well. Does this means that scanpy administrators are happy to have the scanpy-scripts code poured here to make them pip installable, or that you want to contribute the pip packaging to the repo where we currently have scanpy-scripts? We have travis testing for our scripts layer, that would make maintenance easier on your side, as it would detect any changes on scanpy that break the scripts layers (taking you to only break API if extremely necessary - in turn making your tool more stable for external users).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:567,integrability,API,API,567,"Sounds good, we are currently hosting the whole scanpy-scripts as a bioconda package, but we could look into having it as pip installable as well. Does this means that scanpy administrators are happy to have the scanpy-scripts code poured here to make them pip installable, or that you want to contribute the pip packaging to the repo where we currently have scanpy-scripts? We have travis testing for our scripts layer, that would make maintenance easier on your side, as it would detect any changes on scanpy that break the scripts layers (taking you to only break API if extremely necessary - in turn making your tool more stable for external users).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:567,interoperability,API,API,567,"Sounds good, we are currently hosting the whole scanpy-scripts as a bioconda package, but we could look into having it as pip installable as well. Does this means that scanpy administrators are happy to have the scanpy-scripts code poured here to make them pip installable, or that you want to contribute the pip packaging to the repo where we currently have scanpy-scripts? We have travis testing for our scripts layer, that would make maintenance easier on your side, as it would detect any changes on scanpy that break the scripts layers (taking you to only break API if extremely necessary - in turn making your tool more stable for external users).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:77,modifiability,pac,package,77,"Sounds good, we are currently hosting the whole scanpy-scripts as a bioconda package, but we could look into having it as pip installable as well. Does this means that scanpy administrators are happy to have the scanpy-scripts code poured here to make them pip installable, or that you want to contribute the pip packaging to the repo where we currently have scanpy-scripts? We have travis testing for our scripts layer, that would make maintenance easier on your side, as it would detect any changes on scanpy that break the scripts layers (taking you to only break API if extremely necessary - in turn making your tool more stable for external users).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:313,modifiability,pac,packaging,313,"Sounds good, we are currently hosting the whole scanpy-scripts as a bioconda package, but we could look into having it as pip installable as well. Does this means that scanpy administrators are happy to have the scanpy-scripts code poured here to make them pip installable, or that you want to contribute the pip packaging to the repo where we currently have scanpy-scripts? We have travis testing for our scripts layer, that would make maintenance easier on your side, as it would detect any changes on scanpy that break the scripts layers (taking you to only break API if extremely necessary - in turn making your tool more stable for external users).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:414,modifiability,layer,layer,414,"Sounds good, we are currently hosting the whole scanpy-scripts as a bioconda package, but we could look into having it as pip installable as well. Does this means that scanpy administrators are happy to have the scanpy-scripts code poured here to make them pip installable, or that you want to contribute the pip packaging to the repo where we currently have scanpy-scripts? We have travis testing for our scripts layer, that would make maintenance easier on your side, as it would detect any changes on scanpy that break the scripts layers (taking you to only break API if extremely necessary - in turn making your tool more stable for external users).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:534,modifiability,layer,layers,534,"Sounds good, we are currently hosting the whole scanpy-scripts as a bioconda package, but we could look into having it as pip installable as well. Does this means that scanpy administrators are happy to have the scanpy-scripts code poured here to make them pip installable, or that you want to contribute the pip packaging to the repo where we currently have scanpy-scripts? We have travis testing for our scripts layer, that would make maintenance easier on your side, as it would detect any changes on scanpy that break the scripts layers (taking you to only break API if extremely necessary - in turn making your tool more stable for external users).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:147,reliability,Doe,Does,147,"Sounds good, we are currently hosting the whole scanpy-scripts as a bioconda package, but we could look into having it as pip installable as well. Does this means that scanpy administrators are happy to have the scanpy-scripts code poured here to make them pip installable, or that you want to contribute the pip packaging to the repo where we currently have scanpy-scripts? We have travis testing for our scripts layer, that would make maintenance easier on your side, as it would detect any changes on scanpy that break the scripts layers (taking you to only break API if extremely necessary - in turn making your tool more stable for external users).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:437,reliability,mainten,maintenance,437,"Sounds good, we are currently hosting the whole scanpy-scripts as a bioconda package, but we could look into having it as pip installable as well. Does this means that scanpy administrators are happy to have the scanpy-scripts code poured here to make them pip installable, or that you want to contribute the pip packaging to the repo where we currently have scanpy-scripts? We have travis testing for our scripts layer, that would make maintenance easier on your side, as it would detect any changes on scanpy that break the scripts layers (taking you to only break API if extremely necessary - in turn making your tool more stable for external users).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:390,safety,test,testing,390,"Sounds good, we are currently hosting the whole scanpy-scripts as a bioconda package, but we could look into having it as pip installable as well. Does this means that scanpy administrators are happy to have the scanpy-scripts code poured here to make them pip installable, or that you want to contribute the pip packaging to the repo where we currently have scanpy-scripts? We have travis testing for our scripts layer, that would make maintenance easier on your side, as it would detect any changes on scanpy that break the scripts layers (taking you to only break API if extremely necessary - in turn making your tool more stable for external users).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:482,safety,detect,detect,482,"Sounds good, we are currently hosting the whole scanpy-scripts as a bioconda package, but we could look into having it as pip installable as well. Does this means that scanpy administrators are happy to have the scanpy-scripts code poured here to make them pip installable, or that you want to contribute the pip packaging to the repo where we currently have scanpy-scripts? We have travis testing for our scripts layer, that would make maintenance easier on your side, as it would detect any changes on scanpy that break the scripts layers (taking you to only break API if extremely necessary - in turn making your tool more stable for external users).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:482,security,detect,detect,482,"Sounds good, we are currently hosting the whole scanpy-scripts as a bioconda package, but we could look into having it as pip installable as well. Does this means that scanpy administrators are happy to have the scanpy-scripts code poured here to make them pip installable, or that you want to contribute the pip packaging to the repo where we currently have scanpy-scripts? We have travis testing for our scripts layer, that would make maintenance easier on your side, as it would detect any changes on scanpy that break the scripts layers (taking you to only break API if extremely necessary - in turn making your tool more stable for external users).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:390,testability,test,testing,390,"Sounds good, we are currently hosting the whole scanpy-scripts as a bioconda package, but we could look into having it as pip installable as well. Does this means that scanpy administrators are happy to have the scanpy-scripts code poured here to make them pip installable, or that you want to contribute the pip packaging to the repo where we currently have scanpy-scripts? We have travis testing for our scripts layer, that would make maintenance easier on your side, as it would detect any changes on scanpy that break the scripts layers (taking you to only break API if extremely necessary - in turn making your tool more stable for external users).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:616,usability,tool,tool,616,"Sounds good, we are currently hosting the whole scanpy-scripts as a bioconda package, but we could look into having it as pip installable as well. Does this means that scanpy administrators are happy to have the scanpy-scripts code poured here to make them pip installable, or that you want to contribute the pip packaging to the repo where we currently have scanpy-scripts? We have travis testing for our scripts layer, that would make maintenance easier on your side, as it would detect any changes on scanpy that break the scripts layers (taking you to only break API if extremely necessary - in turn making your tool more stable for external users).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:646,usability,user,users,646,"Sounds good, we are currently hosting the whole scanpy-scripts as a bioconda package, but we could look into having it as pip installable as well. Does this means that scanpy administrators are happy to have the scanpy-scripts code poured here to make them pip installable, or that you want to contribute the pip packaging to the repo where we currently have scanpy-scripts? We have travis testing for our scripts layer, that would make maintenance easier on your side, as it would detect any changes on scanpy that break the scripts layers (taking you to only break API if extremely necessary - in turn making your tool more stable for external users).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:94,energy efficiency,current,currently,94,"> Does this means that […] that you want to contribute the pip packaging to the repo where we currently have scanpy-scripts? I meant the quoted alternative, alex was against the other alternative if I understand him correctly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:63,modifiability,pac,packaging,63,"> Does this means that […] that you want to contribute the pip packaging to the repo where we currently have scanpy-scripts? I meant the quoted alternative, alex was against the other alternative if I understand him correctly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:2,reliability,Doe,Does,2,"> Does this means that […] that you want to contribute the pip packaging to the repo where we currently have scanpy-scripts? I meant the quoted alternative, alex was against the other alternative if I understand him correctly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:201,testability,understand,understand,201,"> Does this means that […] that you want to contribute the pip packaging to the repo where we currently have scanpy-scripts? I meant the quoted alternative, alex was against the other alternative if I understand him correctly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:159,deployability,instal,installation,159,"Sure, by all means, open a PR at https://github.com/ebi-gene-expression-group/scanpy-scripts with the directory reformatting and needed metadata files for pip installation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:261,availability,mainten,maintenance,261,@flying-sheep Thanks for fielding all this! You never wrote what thought about having the CLI layer in the scanpy repo... my main reason is that I simply think that I cannot maintain a layer that I'm not actively using (at least right now) and that the library maintenance and development is already quite some work...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:94,modifiability,layer,layer,94,@flying-sheep Thanks for fielding all this! You never wrote what thought about having the CLI layer in the scanpy repo... my main reason is that I simply think that I cannot maintain a layer that I'm not actively using (at least right now) and that the library maintenance and development is already quite some work...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:174,modifiability,maintain,maintain,174,@flying-sheep Thanks for fielding all this! You never wrote what thought about having the CLI layer in the scanpy repo... my main reason is that I simply think that I cannot maintain a layer that I'm not actively using (at least right now) and that the library maintenance and development is already quite some work...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:185,modifiability,layer,layer,185,@flying-sheep Thanks for fielding all this! You never wrote what thought about having the CLI layer in the scanpy repo... my main reason is that I simply think that I cannot maintain a layer that I'm not actively using (at least right now) and that the library maintenance and development is already quite some work...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:261,reliability,mainten,maintenance,261,@flying-sheep Thanks for fielding all this! You never wrote what thought about having the CLI layer in the scanpy repo... my main reason is that I simply think that I cannot maintain a layer that I'm not actively using (at least right now) and that the library maintenance and development is already quite some work...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:174,safety,maintain,maintain,174,@flying-sheep Thanks for fielding all this! You never wrote what thought about having the CLI layer in the scanpy repo... my main reason is that I simply think that I cannot maintain a layer that I'm not actively using (at least right now) and that the library maintenance and development is already quite some work...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:147,testability,simpl,simply,147,@flying-sheep Thanks for fielding all this! You never wrote what thought about having the CLI layer in the scanpy repo... my main reason is that I simply think that I cannot maintain a layer that I'm not actively using (at least right now) and that the library maintenance and development is already quite some work...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:147,usability,simpl,simply,147,@flying-sheep Thanks for fielding all this! You never wrote what thought about having the CLI layer in the scanpy repo... my main reason is that I simply think that I cannot maintain a layer that I'm not actively using (at least right now) and that the library maintenance and development is already quite some work...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:119,integrability,wrap,wrapper,119,"Exactly, this is why I think it’s a great solution to advertise that repo in the scanpy docs and make scanpy provide a wrapper binary. I added ebi-gene-expression-group/scanpy-scripts#24, let me hear what you think",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:119,interoperability,wrapper,wrapper,119,"Exactly, this is why I think it’s a great solution to advertise that repo in the scanpy docs and make scanpy provide a wrapper binary. I added ebi-gene-expression-group/scanpy-scripts#24, let me hear what you think",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:59,deployability,version,version,59,"Hi @pcm32 I finally got around to this. In the next scanpy version, you should be able to invoke your scripts via e.g `scanpy filter-cells`, and `scanpy -h` and `scanpy settings` will do something too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:59,integrability,version,version,59,"Hi @pcm32 I finally got around to this. In the next scanpy version, you should be able to invoke your scripts via e.g `scanpy filter-cells`, and `scanpy -h` and `scanpy settings` will do something too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:126,integrability,filter,filter-cells,126,"Hi @pcm32 I finally got around to this. In the next scanpy version, you should be able to invoke your scripts via e.g `scanpy filter-cells`, and `scanpy -h` and `scanpy settings` will do something too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:59,modifiability,version,version,59,"Hi @pcm32 I finally got around to this. In the next scanpy version, you should be able to invoke your scripts via e.g `scanpy filter-cells`, and `scanpy -h` and `scanpy settings` will do something too.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:52,availability,slo,slowly,52,"Hi @flying-sheep, just saw your updates here. I was slowly working on a re-structured scanpy-scripts that has a sub-command interface i.e `scanpy-cli read`, `scanpy-cli filter` etc and with some added functionality for convenience (https://github.com/ebi-gene-expression-group/scanpy-scripts/pull/40). I'll try change the interface back to make it compatible with yours.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:32,deployability,updat,updates,32,"Hi @flying-sheep, just saw your updates here. I was slowly working on a re-structured scanpy-scripts that has a sub-command interface i.e `scanpy-cli read`, `scanpy-cli filter` etc and with some added functionality for convenience (https://github.com/ebi-gene-expression-group/scanpy-scripts/pull/40). I'll try change the interface back to make it compatible with yours.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:112,integrability,sub,sub-command,112,"Hi @flying-sheep, just saw your updates here. I was slowly working on a re-structured scanpy-scripts that has a sub-command interface i.e `scanpy-cli read`, `scanpy-cli filter` etc and with some added functionality for convenience (https://github.com/ebi-gene-expression-group/scanpy-scripts/pull/40). I'll try change the interface back to make it compatible with yours.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:124,integrability,interfac,interface,124,"Hi @flying-sheep, just saw your updates here. I was slowly working on a re-structured scanpy-scripts that has a sub-command interface i.e `scanpy-cli read`, `scanpy-cli filter` etc and with some added functionality for convenience (https://github.com/ebi-gene-expression-group/scanpy-scripts/pull/40). I'll try change the interface back to make it compatible with yours.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:169,integrability,filter,filter,169,"Hi @flying-sheep, just saw your updates here. I was slowly working on a re-structured scanpy-scripts that has a sub-command interface i.e `scanpy-cli read`, `scanpy-cli filter` etc and with some added functionality for convenience (https://github.com/ebi-gene-expression-group/scanpy-scripts/pull/40). I'll try change the interface back to make it compatible with yours.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:322,integrability,interfac,interface,322,"Hi @flying-sheep, just saw your updates here. I was slowly working on a re-structured scanpy-scripts that has a sub-command interface i.e `scanpy-cli read`, `scanpy-cli filter` etc and with some added functionality for convenience (https://github.com/ebi-gene-expression-group/scanpy-scripts/pull/40). I'll try change the interface back to make it compatible with yours.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:124,interoperability,interfac,interface,124,"Hi @flying-sheep, just saw your updates here. I was slowly working on a re-structured scanpy-scripts that has a sub-command interface i.e `scanpy-cli read`, `scanpy-cli filter` etc and with some added functionality for convenience (https://github.com/ebi-gene-expression-group/scanpy-scripts/pull/40). I'll try change the interface back to make it compatible with yours.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:322,interoperability,interfac,interface,322,"Hi @flying-sheep, just saw your updates here. I was slowly working on a re-structured scanpy-scripts that has a sub-command interface i.e `scanpy-cli read`, `scanpy-cli filter` etc and with some added functionality for convenience (https://github.com/ebi-gene-expression-group/scanpy-scripts/pull/40). I'll try change the interface back to make it compatible with yours.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:348,interoperability,compatib,compatible,348,"Hi @flying-sheep, just saw your updates here. I was slowly working on a re-structured scanpy-scripts that has a sub-command interface i.e `scanpy-cli read`, `scanpy-cli filter` etc and with some added functionality for convenience (https://github.com/ebi-gene-expression-group/scanpy-scripts/pull/40). I'll try change the interface back to make it compatible with yours.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:124,modifiability,interfac,interface,124,"Hi @flying-sheep, just saw your updates here. I was slowly working on a re-structured scanpy-scripts that has a sub-command interface i.e `scanpy-cli read`, `scanpy-cli filter` etc and with some added functionality for convenience (https://github.com/ebi-gene-expression-group/scanpy-scripts/pull/40). I'll try change the interface back to make it compatible with yours.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:322,modifiability,interfac,interface,322,"Hi @flying-sheep, just saw your updates here. I was slowly working on a re-structured scanpy-scripts that has a sub-command interface i.e `scanpy-cli read`, `scanpy-cli filter` etc and with some added functionality for convenience (https://github.com/ebi-gene-expression-group/scanpy-scripts/pull/40). I'll try change the interface back to make it compatible with yours.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:52,reliability,slo,slowly,52,"Hi @flying-sheep, just saw your updates here. I was slowly working on a re-structured scanpy-scripts that has a sub-command interface i.e `scanpy-cli read`, `scanpy-cli filter` etc and with some added functionality for convenience (https://github.com/ebi-gene-expression-group/scanpy-scripts/pull/40). I'll try change the interface back to make it compatible with yours.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:32,safety,updat,updates,32,"Hi @flying-sheep, just saw your updates here. I was slowly working on a re-structured scanpy-scripts that has a sub-command interface i.e `scanpy-cli read`, `scanpy-cli filter` etc and with some added functionality for convenience (https://github.com/ebi-gene-expression-group/scanpy-scripts/pull/40). I'll try change the interface back to make it compatible with yours.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:32,security,updat,updates,32,"Hi @flying-sheep, just saw your updates here. I was slowly working on a re-structured scanpy-scripts that has a sub-command interface i.e `scanpy-cli read`, `scanpy-cli filter` etc and with some added functionality for convenience (https://github.com/ebi-gene-expression-group/scanpy-scripts/pull/40). I'll try change the interface back to make it compatible with yours.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:116,usability,command,command,116,"Hi @flying-sheep, just saw your updates here. I was slowly working on a re-structured scanpy-scripts that has a sub-command interface i.e `scanpy-cli read`, `scanpy-cli filter` etc and with some added functionality for convenience (https://github.com/ebi-gene-expression-group/scanpy-scripts/pull/40). I'll try change the interface back to make it compatible with yours.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:151,interoperability,share,share,151,"damn, sorry I took so long that you now made all this effort for nothing! if you have any ideas on how the main scanpy command can be improved, please share them!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:119,usability,command,command,119,"damn, sorry I took so long that you now made all this effort for nothing! if you have any ideas on how the main scanpy command can be improved, please share them!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:5,safety,test,tested,5,"Just tested, works perfectly with the main command. Brilliant job @flying-sheep!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:5,testability,test,tested,5,"Just tested, works perfectly with the main command. Brilliant job @flying-sheep!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/issues/281:43,usability,command,command,43,"Just tested, works perfectly with the main command. Brilliant job @flying-sheep!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/281
https://github.com/scverse/scanpy/pull/284:114,availability,down,downstream,114,"This is great, thanks! Now just for the neighbours `use_hvgs=` parameter and then we're sorted for using HVGs for downstream analysis without filtering the whole dataset.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/284
https://github.com/scverse/scanpy/pull/284:142,integrability,filter,filtering,142,"This is great, thanks! Now just for the neighbours `use_hvgs=` parameter and then we're sorted for using HVGs for downstream analysis without filtering the whole dataset.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/284
https://github.com/scverse/scanpy/pull/284:63,modifiability,paramet,parameter,63,"This is great, thanks! Now just for the neighbours `use_hvgs=` parameter and then we're sorted for using HVGs for downstream analysis without filtering the whole dataset.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/284
https://github.com/scverse/scanpy/pull/284:36,interoperability,convers,conversation,36,"This is great, thanks! I forgot our conversation the other week... was it `sc.pp.pca()` and not `sc.pp.neighbors()` that needed the `use_hvgs` parameter?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/284
https://github.com/scverse/scanpy/pull/284:143,modifiability,paramet,parameter,143,"This is great, thanks! I forgot our conversation the other week... was it `sc.pp.pca()` and not `sc.pp.neighbors()` that needed the `use_hvgs` parameter?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/284
https://github.com/scverse/scanpy/pull/284:29,testability,simpl,simple,29,@VolkerBergen can you type a simple example on how to use this new functionality. I think that I want to use this.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/284
https://github.com/scverse/scanpy/pull/284:29,usability,simpl,simple,29,@VolkerBergen can you type a simple example on how to use this new functionality. I think that I want to use this.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/284
https://github.com/scverse/scanpy/pull/284:360,availability,operat,operations,360,"Highly variable genes (hvg) can now be used without removing the non-hvg from your data. That's simply `sc.pp.filter_genes_dispersion(adata, subset=False, **params)`, which then does not do the actual filtering but just stores the result in `.var['highly_variable']`. . `sc.pp.pca(adata, **params)` is then performed on the those hvg per default. As all other operations such as neighbors, embeddings etc. are usually performed on PCA space, they implicitly use hvg as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/284
https://github.com/scverse/scanpy/pull/284:141,integrability,sub,subset,141,"Highly variable genes (hvg) can now be used without removing the non-hvg from your data. That's simply `sc.pp.filter_genes_dispersion(adata, subset=False, **params)`, which then does not do the actual filtering but just stores the result in `.var['highly_variable']`. . `sc.pp.pca(adata, **params)` is then performed on the those hvg per default. As all other operations such as neighbors, embeddings etc. are usually performed on PCA space, they implicitly use hvg as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/284
https://github.com/scverse/scanpy/pull/284:201,integrability,filter,filtering,201,"Highly variable genes (hvg) can now be used without removing the non-hvg from your data. That's simply `sc.pp.filter_genes_dispersion(adata, subset=False, **params)`, which then does not do the actual filtering but just stores the result in `.var['highly_variable']`. . `sc.pp.pca(adata, **params)` is then performed on the those hvg per default. As all other operations such as neighbors, embeddings etc. are usually performed on PCA space, they implicitly use hvg as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/284
https://github.com/scverse/scanpy/pull/284:7,modifiability,variab,variable,7,"Highly variable genes (hvg) can now be used without removing the non-hvg from your data. That's simply `sc.pp.filter_genes_dispersion(adata, subset=False, **params)`, which then does not do the actual filtering but just stores the result in `.var['highly_variable']`. . `sc.pp.pca(adata, **params)` is then performed on the those hvg per default. As all other operations such as neighbors, embeddings etc. are usually performed on PCA space, they implicitly use hvg as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/284
https://github.com/scverse/scanpy/pull/284:307,performance,perform,performed,307,"Highly variable genes (hvg) can now be used without removing the non-hvg from your data. That's simply `sc.pp.filter_genes_dispersion(adata, subset=False, **params)`, which then does not do the actual filtering but just stores the result in `.var['highly_variable']`. . `sc.pp.pca(adata, **params)` is then performed on the those hvg per default. As all other operations such as neighbors, embeddings etc. are usually performed on PCA space, they implicitly use hvg as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/284
https://github.com/scverse/scanpy/pull/284:418,performance,perform,performed,418,"Highly variable genes (hvg) can now be used without removing the non-hvg from your data. That's simply `sc.pp.filter_genes_dispersion(adata, subset=False, **params)`, which then does not do the actual filtering but just stores the result in `.var['highly_variable']`. . `sc.pp.pca(adata, **params)` is then performed on the those hvg per default. As all other operations such as neighbors, embeddings etc. are usually performed on PCA space, they implicitly use hvg as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/284
https://github.com/scverse/scanpy/pull/284:178,reliability,doe,does,178,"Highly variable genes (hvg) can now be used without removing the non-hvg from your data. That's simply `sc.pp.filter_genes_dispersion(adata, subset=False, **params)`, which then does not do the actual filtering but just stores the result in `.var['highly_variable']`. . `sc.pp.pca(adata, **params)` is then performed on the those hvg per default. As all other operations such as neighbors, embeddings etc. are usually performed on PCA space, they implicitly use hvg as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/284
https://github.com/scverse/scanpy/pull/284:96,testability,simpl,simply,96,"Highly variable genes (hvg) can now be used without removing the non-hvg from your data. That's simply `sc.pp.filter_genes_dispersion(adata, subset=False, **params)`, which then does not do the actual filtering but just stores the result in `.var['highly_variable']`. . `sc.pp.pca(adata, **params)` is then performed on the those hvg per default. As all other operations such as neighbors, embeddings etc. are usually performed on PCA space, they implicitly use hvg as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/284
https://github.com/scverse/scanpy/pull/284:96,usability,simpl,simply,96,"Highly variable genes (hvg) can now be used without removing the non-hvg from your data. That's simply `sc.pp.filter_genes_dispersion(adata, subset=False, **params)`, which then does not do the actual filtering but just stores the result in `.var['highly_variable']`. . `sc.pp.pca(adata, **params)` is then performed on the those hvg per default. As all other operations such as neighbors, embeddings etc. are usually performed on PCA space, they implicitly use hvg as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/284
https://github.com/scverse/scanpy/pull/284:307,usability,perform,performed,307,"Highly variable genes (hvg) can now be used without removing the non-hvg from your data. That's simply `sc.pp.filter_genes_dispersion(adata, subset=False, **params)`, which then does not do the actual filtering but just stores the result in `.var['highly_variable']`. . `sc.pp.pca(adata, **params)` is then performed on the those hvg per default. As all other operations such as neighbors, embeddings etc. are usually performed on PCA space, they implicitly use hvg as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/284
https://github.com/scverse/scanpy/pull/284:418,usability,perform,performed,418,"Highly variable genes (hvg) can now be used without removing the non-hvg from your data. That's simply `sc.pp.filter_genes_dispersion(adata, subset=False, **params)`, which then does not do the actual filtering but just stores the result in `.var['highly_variable']`. . `sc.pp.pca(adata, **params)` is then performed on the those hvg per default. As all other operations such as neighbors, embeddings etc. are usually performed on PCA space, they implicitly use hvg as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/284
https://github.com/scverse/scanpy/pull/284:545,availability,sli,slightly,545,"I just tested this out... The idea was that if `sc.pp.pca()` has the parameter `use_highly_variable` that be extension `sc.tl.umap()`, `sc.tl.tsne()`, and `sc.tl.draw_graph()` would also be based only on highly variable genes. That however doesn't seem to be the case. When I subset my anndata object to only highly variable genes I get a different result than when I just run it with `sc.pp.pca(adata, use_highly_variable=True)`. The `sc.pl.pca()` is the same, but `sc.pl.diffmap` seems somehow inverted, and umap, tsne, and draw_graph are all slightly different.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/284
https://github.com/scverse/scanpy/pull/284:276,integrability,sub,subset,276,"I just tested this out... The idea was that if `sc.pp.pca()` has the parameter `use_highly_variable` that be extension `sc.tl.umap()`, `sc.tl.tsne()`, and `sc.tl.draw_graph()` would also be based only on highly variable genes. That however doesn't seem to be the case. When I subset my anndata object to only highly variable genes I get a different result than when I just run it with `sc.pp.pca(adata, use_highly_variable=True)`. The `sc.pl.pca()` is the same, but `sc.pl.diffmap` seems somehow inverted, and umap, tsne, and draw_graph are all slightly different.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/284
https://github.com/scverse/scanpy/pull/284:69,modifiability,paramet,parameter,69,"I just tested this out... The idea was that if `sc.pp.pca()` has the parameter `use_highly_variable` that be extension `sc.tl.umap()`, `sc.tl.tsne()`, and `sc.tl.draw_graph()` would also be based only on highly variable genes. That however doesn't seem to be the case. When I subset my anndata object to only highly variable genes I get a different result than when I just run it with `sc.pp.pca(adata, use_highly_variable=True)`. The `sc.pl.pca()` is the same, but `sc.pl.diffmap` seems somehow inverted, and umap, tsne, and draw_graph are all slightly different.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/284
https://github.com/scverse/scanpy/pull/284:109,modifiability,extens,extension,109,"I just tested this out... The idea was that if `sc.pp.pca()` has the parameter `use_highly_variable` that be extension `sc.tl.umap()`, `sc.tl.tsne()`, and `sc.tl.draw_graph()` would also be based only on highly variable genes. That however doesn't seem to be the case. When I subset my anndata object to only highly variable genes I get a different result than when I just run it with `sc.pp.pca(adata, use_highly_variable=True)`. The `sc.pl.pca()` is the same, but `sc.pl.diffmap` seems somehow inverted, and umap, tsne, and draw_graph are all slightly different.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/284
https://github.com/scverse/scanpy/pull/284:211,modifiability,variab,variable,211,"I just tested this out... The idea was that if `sc.pp.pca()` has the parameter `use_highly_variable` that be extension `sc.tl.umap()`, `sc.tl.tsne()`, and `sc.tl.draw_graph()` would also be based only on highly variable genes. That however doesn't seem to be the case. When I subset my anndata object to only highly variable genes I get a different result than when I just run it with `sc.pp.pca(adata, use_highly_variable=True)`. The `sc.pl.pca()` is the same, but `sc.pl.diffmap` seems somehow inverted, and umap, tsne, and draw_graph are all slightly different.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/284
https://github.com/scverse/scanpy/pull/284:316,modifiability,variab,variable,316,"I just tested this out... The idea was that if `sc.pp.pca()` has the parameter `use_highly_variable` that be extension `sc.tl.umap()`, `sc.tl.tsne()`, and `sc.tl.draw_graph()` would also be based only on highly variable genes. That however doesn't seem to be the case. When I subset my anndata object to only highly variable genes I get a different result than when I just run it with `sc.pp.pca(adata, use_highly_variable=True)`. The `sc.pl.pca()` is the same, but `sc.pl.diffmap` seems somehow inverted, and umap, tsne, and draw_graph are all slightly different.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/284
https://github.com/scverse/scanpy/pull/284:240,reliability,doe,doesn,240,"I just tested this out... The idea was that if `sc.pp.pca()` has the parameter `use_highly_variable` that be extension `sc.tl.umap()`, `sc.tl.tsne()`, and `sc.tl.draw_graph()` would also be based only on highly variable genes. That however doesn't seem to be the case. When I subset my anndata object to only highly variable genes I get a different result than when I just run it with `sc.pp.pca(adata, use_highly_variable=True)`. The `sc.pl.pca()` is the same, but `sc.pl.diffmap` seems somehow inverted, and umap, tsne, and draw_graph are all slightly different.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/284
https://github.com/scverse/scanpy/pull/284:545,reliability,sli,slightly,545,"I just tested this out... The idea was that if `sc.pp.pca()` has the parameter `use_highly_variable` that be extension `sc.tl.umap()`, `sc.tl.tsne()`, and `sc.tl.draw_graph()` would also be based only on highly variable genes. That however doesn't seem to be the case. When I subset my anndata object to only highly variable genes I get a different result than when I just run it with `sc.pp.pca(adata, use_highly_variable=True)`. The `sc.pl.pca()` is the same, but `sc.pl.diffmap` seems somehow inverted, and umap, tsne, and draw_graph are all slightly different.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/284
https://github.com/scverse/scanpy/pull/284:7,safety,test,tested,7,"I just tested this out... The idea was that if `sc.pp.pca()` has the parameter `use_highly_variable` that be extension `sc.tl.umap()`, `sc.tl.tsne()`, and `sc.tl.draw_graph()` would also be based only on highly variable genes. That however doesn't seem to be the case. When I subset my anndata object to only highly variable genes I get a different result than when I just run it with `sc.pp.pca(adata, use_highly_variable=True)`. The `sc.pl.pca()` is the same, but `sc.pl.diffmap` seems somehow inverted, and umap, tsne, and draw_graph are all slightly different.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/284
https://github.com/scverse/scanpy/pull/284:7,testability,test,tested,7,"I just tested this out... The idea was that if `sc.pp.pca()` has the parameter `use_highly_variable` that be extension `sc.tl.umap()`, `sc.tl.tsne()`, and `sc.tl.draw_graph()` would also be based only on highly variable genes. That however doesn't seem to be the case. When I subset my anndata object to only highly variable genes I get a different result than when I just run it with `sc.pp.pca(adata, use_highly_variable=True)`. The `sc.pl.pca()` is the same, but `sc.pl.diffmap` seems somehow inverted, and umap, tsne, and draw_graph are all slightly different.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/284
https://github.com/scverse/scanpy/issues/285:16,usability,support,supported,16,python 2 is not supported as evident from the docs and pypi.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/285
https://github.com/scverse/scanpy/issues/285:103,deployability,resourc,resources,103,"The main reason is that Python 2 is getting obsolete really soon, therefore we do not think allocating resources for maintaining it is worthwhile. See https://python3statement.org/ for more details. Another reason is that there are many nice Python3-only features that are being used in Scanpy such as function annotations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/285
https://github.com/scverse/scanpy/issues/285:92,energy efficiency,alloc,allocating,92,"The main reason is that Python 2 is getting obsolete really soon, therefore we do not think allocating resources for maintaining it is worthwhile. See https://python3statement.org/ for more details. Another reason is that there are many nice Python3-only features that are being used in Scanpy such as function annotations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/285
https://github.com/scverse/scanpy/issues/285:103,energy efficiency,resourc,resources,103,"The main reason is that Python 2 is getting obsolete really soon, therefore we do not think allocating resources for maintaining it is worthwhile. See https://python3statement.org/ for more details. Another reason is that there are many nice Python3-only features that are being used in Scanpy such as function annotations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/285
https://github.com/scverse/scanpy/issues/285:117,modifiability,maintain,maintaining,117,"The main reason is that Python 2 is getting obsolete really soon, therefore we do not think allocating resources for maintaining it is worthwhile. See https://python3statement.org/ for more details. Another reason is that there are many nice Python3-only features that are being used in Scanpy such as function annotations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/285
https://github.com/scverse/scanpy/issues/285:103,performance,resourc,resources,103,"The main reason is that Python 2 is getting obsolete really soon, therefore we do not think allocating resources for maintaining it is worthwhile. See https://python3statement.org/ for more details. Another reason is that there are many nice Python3-only features that are being used in Scanpy such as function annotations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/285
https://github.com/scverse/scanpy/issues/285:103,safety,resourc,resources,103,"The main reason is that Python 2 is getting obsolete really soon, therefore we do not think allocating resources for maintaining it is worthwhile. See https://python3statement.org/ for more details. Another reason is that there are many nice Python3-only features that are being used in Scanpy such as function annotations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/285
https://github.com/scverse/scanpy/issues/285:117,safety,maintain,maintaining,117,"The main reason is that Python 2 is getting obsolete really soon, therefore we do not think allocating resources for maintaining it is worthwhile. See https://python3statement.org/ for more details. Another reason is that there are many nice Python3-only features that are being used in Scanpy such as function annotations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/285
https://github.com/scverse/scanpy/issues/285:103,testability,resourc,resources,103,"The main reason is that Python 2 is getting obsolete really soon, therefore we do not think allocating resources for maintaining it is worthwhile. See https://python3statement.org/ for more details. Another reason is that there are many nice Python3-only features that are being used in Scanpy such as function annotations.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/285
https://github.com/scverse/scanpy/issues/285:105,deployability,resourc,resources,105,"> The main reason is that Python 2 is getting obsolete really soon, therefore we do not think allocating resources for maintaining it is worthwhile. See https://python3statement.org/ for more details. Another reason is that there are many nice Python3-only features that are being used in Scanpy such as function annotations. thanks a lot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/285
https://github.com/scverse/scanpy/issues/285:94,energy efficiency,alloc,allocating,94,"> The main reason is that Python 2 is getting obsolete really soon, therefore we do not think allocating resources for maintaining it is worthwhile. See https://python3statement.org/ for more details. Another reason is that there are many nice Python3-only features that are being used in Scanpy such as function annotations. thanks a lot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/285
https://github.com/scverse/scanpy/issues/285:105,energy efficiency,resourc,resources,105,"> The main reason is that Python 2 is getting obsolete really soon, therefore we do not think allocating resources for maintaining it is worthwhile. See https://python3statement.org/ for more details. Another reason is that there are many nice Python3-only features that are being used in Scanpy such as function annotations. thanks a lot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/285
https://github.com/scverse/scanpy/issues/285:119,modifiability,maintain,maintaining,119,"> The main reason is that Python 2 is getting obsolete really soon, therefore we do not think allocating resources for maintaining it is worthwhile. See https://python3statement.org/ for more details. Another reason is that there are many nice Python3-only features that are being used in Scanpy such as function annotations. thanks a lot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/285
https://github.com/scverse/scanpy/issues/285:105,performance,resourc,resources,105,"> The main reason is that Python 2 is getting obsolete really soon, therefore we do not think allocating resources for maintaining it is worthwhile. See https://python3statement.org/ for more details. Another reason is that there are many nice Python3-only features that are being used in Scanpy such as function annotations. thanks a lot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/285
https://github.com/scverse/scanpy/issues/285:105,safety,resourc,resources,105,"> The main reason is that Python 2 is getting obsolete really soon, therefore we do not think allocating resources for maintaining it is worthwhile. See https://python3statement.org/ for more details. Another reason is that there are many nice Python3-only features that are being used in Scanpy such as function annotations. thanks a lot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/285
https://github.com/scverse/scanpy/issues/285:119,safety,maintain,maintaining,119,"> The main reason is that Python 2 is getting obsolete really soon, therefore we do not think allocating resources for maintaining it is worthwhile. See https://python3statement.org/ for more details. Another reason is that there are many nice Python3-only features that are being used in Scanpy such as function annotations. thanks a lot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/285
https://github.com/scverse/scanpy/issues/285:105,testability,resourc,resources,105,"> The main reason is that Python 2 is getting obsolete really soon, therefore we do not think allocating resources for maintaining it is worthwhile. See https://python3statement.org/ for more details. Another reason is that there are many nice Python3-only features that are being used in Scanpy such as function annotations. thanks a lot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/285
https://github.com/scverse/scanpy/issues/285:18,usability,support,supported,18,> python 2 is not supported as evident from the docs and pypi. thanks a lot.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/285
https://github.com/scverse/scanpy/issues/286:256,availability,cluster,clusters,256,"`grey80` is not a valid color name at least in matplotlib 3 although is referenced internally. I will do a quick PR to change this. Meanwhile, you can modify the function that causes the problem by adding a `palette`: . ```PYTHON. sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full', palette='Set2'). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:256,deployability,cluster,clusters,256,"`grey80` is not a valid color name at least in matplotlib 3 although is referenced internally. I will do a quick PR to change this. Meanwhile, you can modify the function that causes the problem by adding a `palette`: . ```PYTHON. sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full', palette='Set2'). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:18,safety,valid,valid,18,"`grey80` is not a valid color name at least in matplotlib 3 although is referenced internally. I will do a quick PR to change this. Meanwhile, you can modify the function that causes the problem by adding a `palette`: . ```PYTHON. sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full', palette='Set2'). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:151,security,modif,modify,151,"`grey80` is not a valid color name at least in matplotlib 3 although is referenced internally. I will do a quick PR to change this. Meanwhile, you can modify the function that causes the problem by adding a `palette`: . ```PYTHON. sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full', palette='Set2'). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:81,deployability,version,version,81,"Could it be that you are using an annData object that you saved using a previous version of scanpy, which may include non-standard colors?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:81,integrability,version,version,81,"Could it be that you are using an annData object that you saved using a previous version of scanpy, which may include non-standard colors?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:122,interoperability,standard,standard,122,"Could it be that you are using an annData object that you saved using a previous version of scanpy, which may include non-standard colors?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:81,modifiability,version,version,81,"Could it be that you are using an annData object that you saved using a previous version of scanpy, which may include non-standard colors?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:52,availability,error,error,52,I am attaching reduced files. I could reproduce the error with this dataset. Color names are `colors_dataset.txt` file. Note that python script is renamed to `.py.txt` . There was an error in `paga` related plotting function as well. . [R_pca_seurat.txt](https://github.com/theislab/scanpy/files/2455948/R_pca_seurat.txt). [R_annotation.txt](https://github.com/theislab/scanpy/files/2455949/R_annotation.txt). [colors_dataset.txt](https://github.com/theislab/scanpy/files/2455950/colors_dataset.txt). [planaria.py.txt](https://github.com/theislab/scanpy/files/2455953/planaria.py.txt).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:183,availability,error,error,183,I am attaching reduced files. I could reproduce the error with this dataset. Color names are `colors_dataset.txt` file. Note that python script is renamed to `.py.txt` . There was an error in `paga` related plotting function as well. . [R_pca_seurat.txt](https://github.com/theislab/scanpy/files/2455948/R_pca_seurat.txt). [R_annotation.txt](https://github.com/theislab/scanpy/files/2455949/R_annotation.txt). [colors_dataset.txt](https://github.com/theislab/scanpy/files/2455950/colors_dataset.txt). [planaria.py.txt](https://github.com/theislab/scanpy/files/2455953/planaria.py.txt).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:15,energy efficiency,reduc,reduced,15,I am attaching reduced files. I could reproduce the error with this dataset. Color names are `colors_dataset.txt` file. Note that python script is renamed to `.py.txt` . There was an error in `paga` related plotting function as well. . [R_pca_seurat.txt](https://github.com/theislab/scanpy/files/2455948/R_pca_seurat.txt). [R_annotation.txt](https://github.com/theislab/scanpy/files/2455949/R_annotation.txt). [colors_dataset.txt](https://github.com/theislab/scanpy/files/2455950/colors_dataset.txt). [planaria.py.txt](https://github.com/theislab/scanpy/files/2455953/planaria.py.txt).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:52,performance,error,error,52,I am attaching reduced files. I could reproduce the error with this dataset. Color names are `colors_dataset.txt` file. Note that python script is renamed to `.py.txt` . There was an error in `paga` related plotting function as well. . [R_pca_seurat.txt](https://github.com/theislab/scanpy/files/2455948/R_pca_seurat.txt). [R_annotation.txt](https://github.com/theislab/scanpy/files/2455949/R_annotation.txt). [colors_dataset.txt](https://github.com/theislab/scanpy/files/2455950/colors_dataset.txt). [planaria.py.txt](https://github.com/theislab/scanpy/files/2455953/planaria.py.txt).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:183,performance,error,error,183,I am attaching reduced files. I could reproduce the error with this dataset. Color names are `colors_dataset.txt` file. Note that python script is renamed to `.py.txt` . There was an error in `paga` related plotting function as well. . [R_pca_seurat.txt](https://github.com/theislab/scanpy/files/2455948/R_pca_seurat.txt). [R_annotation.txt](https://github.com/theislab/scanpy/files/2455949/R_annotation.txt). [colors_dataset.txt](https://github.com/theislab/scanpy/files/2455950/colors_dataset.txt). [planaria.py.txt](https://github.com/theislab/scanpy/files/2455953/planaria.py.txt).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:52,safety,error,error,52,I am attaching reduced files. I could reproduce the error with this dataset. Color names are `colors_dataset.txt` file. Note that python script is renamed to `.py.txt` . There was an error in `paga` related plotting function as well. . [R_pca_seurat.txt](https://github.com/theislab/scanpy/files/2455948/R_pca_seurat.txt). [R_annotation.txt](https://github.com/theislab/scanpy/files/2455949/R_annotation.txt). [colors_dataset.txt](https://github.com/theislab/scanpy/files/2455950/colors_dataset.txt). [planaria.py.txt](https://github.com/theislab/scanpy/files/2455953/planaria.py.txt).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:183,safety,error,error,183,I am attaching reduced files. I could reproduce the error with this dataset. Color names are `colors_dataset.txt` file. Note that python script is renamed to `.py.txt` . There was an error in `paga` related plotting function as well. . [R_pca_seurat.txt](https://github.com/theislab/scanpy/files/2455948/R_pca_seurat.txt). [R_annotation.txt](https://github.com/theislab/scanpy/files/2455949/R_annotation.txt). [colors_dataset.txt](https://github.com/theislab/scanpy/files/2455950/colors_dataset.txt). [planaria.py.txt](https://github.com/theislab/scanpy/files/2455953/planaria.py.txt).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:502,testability,plan,planaria,502,I am attaching reduced files. I could reproduce the error with this dataset. Color names are `colors_dataset.txt` file. Note that python script is renamed to `.py.txt` . There was an error in `paga` related plotting function as well. . [R_pca_seurat.txt](https://github.com/theislab/scanpy/files/2455948/R_pca_seurat.txt). [R_annotation.txt](https://github.com/theislab/scanpy/files/2455949/R_annotation.txt). [colors_dataset.txt](https://github.com/theislab/scanpy/files/2455950/colors_dataset.txt). [planaria.py.txt](https://github.com/theislab/scanpy/files/2455953/planaria.py.txt).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:568,testability,plan,planaria,568,I am attaching reduced files. I could reproduce the error with this dataset. Color names are `colors_dataset.txt` file. Note that python script is renamed to `.py.txt` . There was an error in `paga` related plotting function as well. . [R_pca_seurat.txt](https://github.com/theislab/scanpy/files/2455948/R_pca_seurat.txt). [R_annotation.txt](https://github.com/theislab/scanpy/files/2455949/R_annotation.txt). [colors_dataset.txt](https://github.com/theislab/scanpy/files/2455950/colors_dataset.txt). [planaria.py.txt](https://github.com/theislab/scanpy/files/2455953/planaria.py.txt).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:52,usability,error,error,52,I am attaching reduced files. I could reproduce the error with this dataset. Color names are `colors_dataset.txt` file. Note that python script is renamed to `.py.txt` . There was an error in `paga` related plotting function as well. . [R_pca_seurat.txt](https://github.com/theislab/scanpy/files/2455948/R_pca_seurat.txt). [R_annotation.txt](https://github.com/theislab/scanpy/files/2455949/R_annotation.txt). [colors_dataset.txt](https://github.com/theislab/scanpy/files/2455950/colors_dataset.txt). [planaria.py.txt](https://github.com/theislab/scanpy/files/2455953/planaria.py.txt).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:183,usability,error,error,183,I am attaching reduced files. I could reproduce the error with this dataset. Color names are `colors_dataset.txt` file. Note that python script is renamed to `.py.txt` . There was an error in `paga` related plotting function as well. . [R_pca_seurat.txt](https://github.com/theislab/scanpy/files/2455948/R_pca_seurat.txt). [R_annotation.txt](https://github.com/theislab/scanpy/files/2455949/R_annotation.txt). [colors_dataset.txt](https://github.com/theislab/scanpy/files/2455950/colors_dataset.txt). [planaria.py.txt](https://github.com/theislab/scanpy/files/2455953/planaria.py.txt).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:319,deployability,integr,integrate,319,"@fidelram It might be that the new plotting backend doesn't support the ""additional colors"" ([here](https://github.com/theislab/scanpy/blob/7c9fb1a5f2293956adda0673d47e7dec1b32ddfb/scanpy/plotting/utils.py#L166-L186)) anymore. These are colors that are standard in R and used for the Planaria example. We should try to integrate them for the sake of easily moving between python and R.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:319,integrability,integr,integrate,319,"@fidelram It might be that the new plotting backend doesn't support the ""additional colors"" ([here](https://github.com/theislab/scanpy/blob/7c9fb1a5f2293956adda0673d47e7dec1b32ddfb/scanpy/plotting/utils.py#L166-L186)) anymore. These are colors that are standard in R and used for the Planaria example. We should try to integrate them for the sake of easily moving between python and R.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:253,interoperability,standard,standard,253,"@fidelram It might be that the new plotting backend doesn't support the ""additional colors"" ([here](https://github.com/theislab/scanpy/blob/7c9fb1a5f2293956adda0673d47e7dec1b32ddfb/scanpy/plotting/utils.py#L166-L186)) anymore. These are colors that are standard in R and used for the Planaria example. We should try to integrate them for the sake of easily moving between python and R.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:319,interoperability,integr,integrate,319,"@fidelram It might be that the new plotting backend doesn't support the ""additional colors"" ([here](https://github.com/theislab/scanpy/blob/7c9fb1a5f2293956adda0673d47e7dec1b32ddfb/scanpy/plotting/utils.py#L166-L186)) anymore. These are colors that are standard in R and used for the Planaria example. We should try to integrate them for the sake of easily moving between python and R.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:319,modifiability,integr,integrate,319,"@fidelram It might be that the new plotting backend doesn't support the ""additional colors"" ([here](https://github.com/theislab/scanpy/blob/7c9fb1a5f2293956adda0673d47e7dec1b32ddfb/scanpy/plotting/utils.py#L166-L186)) anymore. These are colors that are standard in R and used for the Planaria example. We should try to integrate them for the sake of easily moving between python and R.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:52,reliability,doe,doesn,52,"@fidelram It might be that the new plotting backend doesn't support the ""additional colors"" ([here](https://github.com/theislab/scanpy/blob/7c9fb1a5f2293956adda0673d47e7dec1b32ddfb/scanpy/plotting/utils.py#L166-L186)) anymore. These are colors that are standard in R and used for the Planaria example. We should try to integrate them for the sake of easily moving between python and R.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:319,reliability,integr,integrate,319,"@fidelram It might be that the new plotting backend doesn't support the ""additional colors"" ([here](https://github.com/theislab/scanpy/blob/7c9fb1a5f2293956adda0673d47e7dec1b32ddfb/scanpy/plotting/utils.py#L166-L186)) anymore. These are colors that are standard in R and used for the Planaria example. We should try to integrate them for the sake of easily moving between python and R.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:319,security,integr,integrate,319,"@fidelram It might be that the new plotting backend doesn't support the ""additional colors"" ([here](https://github.com/theislab/scanpy/blob/7c9fb1a5f2293956adda0673d47e7dec1b32ddfb/scanpy/plotting/utils.py#L166-L186)) anymore. These are colors that are standard in R and used for the Planaria example. We should try to integrate them for the sake of easily moving between python and R.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:284,testability,Plan,Planaria,284,"@fidelram It might be that the new plotting backend doesn't support the ""additional colors"" ([here](https://github.com/theislab/scanpy/blob/7c9fb1a5f2293956adda0673d47e7dec1b32ddfb/scanpy/plotting/utils.py#L166-L186)) anymore. These are colors that are standard in R and used for the Planaria example. We should try to integrate them for the sake of easily moving between python and R.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:319,testability,integr,integrate,319,"@fidelram It might be that the new plotting backend doesn't support the ""additional colors"" ([here](https://github.com/theislab/scanpy/blob/7c9fb1a5f2293956adda0673d47e7dec1b32ddfb/scanpy/plotting/utils.py#L166-L186)) anymore. These are colors that are standard in R and used for the Planaria example. We should try to integrate them for the sake of easily moving between python and R.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:60,usability,support,support,60,"@fidelram It might be that the new plotting backend doesn't support the ""additional colors"" ([here](https://github.com/theislab/scanpy/blob/7c9fb1a5f2293956adda0673d47e7dec1b32ddfb/scanpy/plotting/utils.py#L166-L186)) anymore. These are colors that are standard in R and used for the Planaria example. We should try to integrate them for the sake of easily moving between python and R.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:34,performance,time,time,34,I will check that once I get some time.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:17,usability,confirm,confirm,17,@dilawar Can you confirm that the changes solved your problem?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:624,availability,cluster,clusters,624,"No. There is still some issue with colors. Note that now I am on python3.7 (which is default on ArchLinux). . ```. $ pip install git+https://github.com/theislab/scanpy --upgrade --user. $ python planaria.py . /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. import imp. scanpy==1.3.2+19.g94c3dc5 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:01:09.28). Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 166, in to_rgba. rgba = _colors_full_map.cache[c, alpha]. KeyError: ('mediumpurple3', None). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4288, in scatter. colors = mcolors.to_rgba_array(c). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 267, in to_rgba_array. result[i] = to_rgba(cc, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 168, in to_rgba. rgba = _to_rgba_no_colorcycle(c, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 212, in _to_rgba_no_colorcycle. raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c)). ValueError: Invalid RGBA argument: 'mediumpurple3'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""planaria.py"", line 47, in <module>. sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full'). File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:1831,availability,cluster,clusters,1831,"das==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:01:09.28). Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 166, in to_rgba. rgba = _colors_full_map.cache[c, alpha]. KeyError: ('mediumpurple3', None). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4288, in scatter. colors = mcolors.to_rgba_array(c). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 267, in to_rgba_array. result[i] = to_rgba(cc, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 168, in to_rgba. rgba = _to_rgba_no_colorcycle(c, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 212, in _to_rgba_no_colorcycle. raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c)). ValueError: Invalid RGBA argument: 'mediumpurple3'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""planaria.py"", line 47, in <module>. sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full'). File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line 47, in tsne. return plot_scatter(adata, basis='tsne', **kwargs). File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line 301, in plot_scatter. **kwargs). File ""/usr/lib/python3.7/site-packages/matplotlib/__init__.py"", line 1867, in inner. return func(ax, *args, **kwargs). File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4293, in scatter. .format(c.shape, x.size, y.size)). AttributeError: 'list' object has no attribute 'shape'. [dilawars@chamcham scanpy_exp]$ . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:121,deployability,instal,install,121,"No. There is still some issue with colors. Note that now I am on python3.7 (which is default on ArchLinux). . ```. $ pip install git+https://github.com/theislab/scanpy --upgrade --user. $ python planaria.py . /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. import imp. scanpy==1.3.2+19.g94c3dc5 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:01:09.28). Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 166, in to_rgba. rgba = _colors_full_map.cache[c, alpha]. KeyError: ('mediumpurple3', None). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4288, in scatter. colors = mcolors.to_rgba_array(c). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 267, in to_rgba_array. result[i] = to_rgba(cc, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 168, in to_rgba. rgba = _to_rgba_no_colorcycle(c, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 212, in _to_rgba_no_colorcycle. raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c)). ValueError: Invalid RGBA argument: 'mediumpurple3'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""planaria.py"", line 47, in <module>. sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full'). File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:170,deployability,upgrad,upgrade,170,"No. There is still some issue with colors. Note that now I am on python3.7 (which is default on ArchLinux). . ```. $ pip install git+https://github.com/theislab/scanpy --upgrade --user. $ python planaria.py . /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. import imp. scanpy==1.3.2+19.g94c3dc5 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:01:09.28). Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 166, in to_rgba. rgba = _colors_full_map.cache[c, alpha]. KeyError: ('mediumpurple3', None). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4288, in scatter. colors = mcolors.to_rgba_array(c). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 267, in to_rgba_array. result[i] = to_rgba(cc, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 168, in to_rgba. rgba = _to_rgba_no_colorcycle(c, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 212, in _to_rgba_no_colorcycle. raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c)). ValueError: Invalid RGBA argument: 'mediumpurple3'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""planaria.py"", line 47, in <module>. sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full'). File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:354,deployability,modul,module,354,"No. There is still some issue with colors. Note that now I am on python3.7 (which is default on ArchLinux). . ```. $ pip install git+https://github.com/theislab/scanpy --upgrade --user. $ python planaria.py . /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. import imp. scanpy==1.3.2+19.g94c3dc5 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:01:09.28). Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 166, in to_rgba. rgba = _colors_full_map.cache[c, alpha]. KeyError: ('mediumpurple3', None). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4288, in scatter. colors = mcolors.to_rgba_array(c). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 267, in to_rgba_array. result[i] = to_rgba(cc, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 168, in to_rgba. rgba = _to_rgba_no_colorcycle(c, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 212, in _to_rgba_no_colorcycle. raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c)). ValueError: Invalid RGBA argument: 'mediumpurple3'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""planaria.py"", line 47, in <module>. sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full'). File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:407,deployability,modul,module,407,"No. There is still some issue with colors. Note that now I am on python3.7 (which is default on ArchLinux). . ```. $ pip install git+https://github.com/theislab/scanpy --upgrade --user. $ python planaria.py . /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. import imp. scanpy==1.3.2+19.g94c3dc5 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:01:09.28). Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 166, in to_rgba. rgba = _colors_full_map.cache[c, alpha]. KeyError: ('mediumpurple3', None). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4288, in scatter. colors = mcolors.to_rgba_array(c). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 267, in to_rgba_array. result[i] = to_rgba(cc, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 168, in to_rgba. rgba = _to_rgba_no_colorcycle(c, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 212, in _to_rgba_no_colorcycle. raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c)). ValueError: Invalid RGBA argument: 'mediumpurple3'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""planaria.py"", line 47, in <module>. sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full'). File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:624,deployability,cluster,clusters,624,"No. There is still some issue with colors. Note that now I am on python3.7 (which is default on ArchLinux). . ```. $ pip install git+https://github.com/theislab/scanpy --upgrade --user. $ python planaria.py . /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. import imp. scanpy==1.3.2+19.g94c3dc5 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:01:09.28). Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 166, in to_rgba. rgba = _colors_full_map.cache[c, alpha]. KeyError: ('mediumpurple3', None). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4288, in scatter. colors = mcolors.to_rgba_array(c). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 267, in to_rgba_array. result[i] = to_rgba(cc, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 168, in to_rgba. rgba = _to_rgba_no_colorcycle(c, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 212, in _to_rgba_no_colorcycle. raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c)). ValueError: Invalid RGBA argument: 'mediumpurple3'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""planaria.py"", line 47, in <module>. sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full'). File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:1797,deployability,modul,module,1797,"das==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:01:09.28). Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 166, in to_rgba. rgba = _colors_full_map.cache[c, alpha]. KeyError: ('mediumpurple3', None). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4288, in scatter. colors = mcolors.to_rgba_array(c). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 267, in to_rgba_array. result[i] = to_rgba(cc, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 168, in to_rgba. rgba = _to_rgba_no_colorcycle(c, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 212, in _to_rgba_no_colorcycle. raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c)). ValueError: Invalid RGBA argument: 'mediumpurple3'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""planaria.py"", line 47, in <module>. sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full'). File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line 47, in tsne. return plot_scatter(adata, basis='tsne', **kwargs). File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line 301, in plot_scatter. **kwargs). File ""/usr/lib/python3.7/site-packages/matplotlib/__init__.py"", line 1867, in inner. return func(ax, *args, **kwargs). File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4293, in scatter. .format(c.shape, x.size, y.size)). AttributeError: 'list' object has no attribute 'shape'. [dilawars@chamcham scanpy_exp]$ . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:1831,deployability,cluster,clusters,1831,"das==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:01:09.28). Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 166, in to_rgba. rgba = _colors_full_map.cache[c, alpha]. KeyError: ('mediumpurple3', None). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4288, in scatter. colors = mcolors.to_rgba_array(c). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 267, in to_rgba_array. result[i] = to_rgba(cc, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 168, in to_rgba. rgba = _to_rgba_no_colorcycle(c, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 212, in _to_rgba_no_colorcycle. raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c)). ValueError: Invalid RGBA argument: 'mediumpurple3'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""planaria.py"", line 47, in <module>. sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full'). File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line 47, in tsne. return plot_scatter(adata, basis='tsne', **kwargs). File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line 301, in plot_scatter. **kwargs). File ""/usr/lib/python3.7/site-packages/matplotlib/__init__.py"", line 1867, in inner. return func(ax, *args, **kwargs). File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4293, in scatter. .format(c.shape, x.size, y.size)). AttributeError: 'list' object has no attribute 'shape'. [dilawars@chamcham scanpy_exp]$ . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:295,energy efficiency,cloud,cloudpickle,295,"No. There is still some issue with colors. Note that now I am on python3.7 (which is default on ArchLinux). . ```. $ pip install git+https://github.com/theislab/scanpy --upgrade --user. $ python planaria.py . /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. import imp. scanpy==1.3.2+19.g94c3dc5 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:01:09.28). Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 166, in to_rgba. rgba = _colors_full_map.cache[c, alpha]. KeyError: ('mediumpurple3', None). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4288, in scatter. colors = mcolors.to_rgba_array(c). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 267, in to_rgba_array. result[i] = to_rgba(cc, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 168, in to_rgba. rgba = _to_rgba_no_colorcycle(c, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 212, in _to_rgba_no_colorcycle. raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c)). ValueError: Invalid RGBA argument: 'mediumpurple3'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""planaria.py"", line 47, in <module>. sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full'). File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:307,energy efficiency,cloud,cloudpickle,307,"No. There is still some issue with colors. Note that now I am on python3.7 (which is default on ArchLinux). . ```. $ pip install git+https://github.com/theislab/scanpy --upgrade --user. $ python planaria.py . /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. import imp. scanpy==1.3.2+19.g94c3dc5 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:01:09.28). Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 166, in to_rgba. rgba = _colors_full_map.cache[c, alpha]. KeyError: ('mediumpurple3', None). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4288, in scatter. colors = mcolors.to_rgba_array(c). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 267, in to_rgba_array. result[i] = to_rgba(cc, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 168, in to_rgba. rgba = _to_rgba_no_colorcycle(c, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 212, in _to_rgba_no_colorcycle. raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c)). ValueError: Invalid RGBA argument: 'mediumpurple3'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""planaria.py"", line 47, in <module>. sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full'). File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:1590,interoperability,format,format,1590,"das==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:01:09.28). Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 166, in to_rgba. rgba = _colors_full_map.cache[c, alpha]. KeyError: ('mediumpurple3', None). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4288, in scatter. colors = mcolors.to_rgba_array(c). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 267, in to_rgba_array. result[i] = to_rgba(cc, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 168, in to_rgba. rgba = _to_rgba_no_colorcycle(c, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 212, in _to_rgba_no_colorcycle. raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c)). ValueError: Invalid RGBA argument: 'mediumpurple3'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""planaria.py"", line 47, in <module>. sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full'). File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line 47, in tsne. return plot_scatter(adata, basis='tsne', **kwargs). File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line 301, in plot_scatter. **kwargs). File ""/usr/lib/python3.7/site-packages/matplotlib/__init__.py"", line 1867, in inner. return func(ax, *args, **kwargs). File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4293, in scatter. .format(c.shape, x.size, y.size)). AttributeError: 'list' object has no attribute 'shape'. [dilawars@chamcham scanpy_exp]$ . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:2409,interoperability,format,format,2409,"das==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:01:09.28). Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 166, in to_rgba. rgba = _colors_full_map.cache[c, alpha]. KeyError: ('mediumpurple3', None). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4288, in scatter. colors = mcolors.to_rgba_array(c). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 267, in to_rgba_array. result[i] = to_rgba(cc, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 168, in to_rgba. rgba = _to_rgba_no_colorcycle(c, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 212, in _to_rgba_no_colorcycle. raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c)). ValueError: Invalid RGBA argument: 'mediumpurple3'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""planaria.py"", line 47, in <module>. sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full'). File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line 47, in tsne. return plot_scatter(adata, basis='tsne', **kwargs). File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line 301, in plot_scatter. **kwargs). File ""/usr/lib/python3.7/site-packages/matplotlib/__init__.py"", line 1867, in inner. return func(ax, *args, **kwargs). File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4293, in scatter. .format(c.shape, x.size, y.size)). AttributeError: 'list' object has no attribute 'shape'. [dilawars@chamcham scanpy_exp]$ . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:170,modifiability,upgrad,upgrade,170,"No. There is still some issue with colors. Note that now I am on python3.7 (which is default on ArchLinux). . ```. $ pip install git+https://github.com/theislab/scanpy --upgrade --user. $ python planaria.py . /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. import imp. scanpy==1.3.2+19.g94c3dc5 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:01:09.28). Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 166, in to_rgba. rgba = _colors_full_map.cache[c, alpha]. KeyError: ('mediumpurple3', None). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4288, in scatter. colors = mcolors.to_rgba_array(c). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 267, in to_rgba_array. result[i] = to_rgba(cc, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 168, in to_rgba. rgba = _to_rgba_no_colorcycle(c, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 212, in _to_rgba_no_colorcycle. raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c)). ValueError: Invalid RGBA argument: 'mediumpurple3'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""planaria.py"", line 47, in <module>. sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full'). File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:251,modifiability,pac,packages,251,"No. There is still some issue with colors. Note that now I am on python3.7 (which is default on ArchLinux). . ```. $ pip install git+https://github.com/theislab/scanpy --upgrade --user. $ python planaria.py . /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. import imp. scanpy==1.3.2+19.g94c3dc5 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:01:09.28). Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 166, in to_rgba. rgba = _colors_full_map.cache[c, alpha]. KeyError: ('mediumpurple3', None). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4288, in scatter. colors = mcolors.to_rgba_array(c). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 267, in to_rgba_array. result[i] = to_rgba(cc, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 168, in to_rgba. rgba = _to_rgba_no_colorcycle(c, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 212, in _to_rgba_no_colorcycle. raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c)). ValueError: Invalid RGBA argument: 'mediumpurple3'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""planaria.py"", line 47, in <module>. sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full'). File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:354,modifiability,modul,module,354,"No. There is still some issue with colors. Note that now I am on python3.7 (which is default on ArchLinux). . ```. $ pip install git+https://github.com/theislab/scanpy --upgrade --user. $ python planaria.py . /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. import imp. scanpy==1.3.2+19.g94c3dc5 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:01:09.28). Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 166, in to_rgba. rgba = _colors_full_map.cache[c, alpha]. KeyError: ('mediumpurple3', None). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4288, in scatter. colors = mcolors.to_rgba_array(c). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 267, in to_rgba_array. result[i] = to_rgba(cc, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 168, in to_rgba. rgba = _to_rgba_no_colorcycle(c, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 212, in _to_rgba_no_colorcycle. raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c)). ValueError: Invalid RGBA argument: 'mediumpurple3'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""planaria.py"", line 47, in <module>. sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full'). File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:407,modifiability,modul,module,407,"No. There is still some issue with colors. Note that now I am on python3.7 (which is default on ArchLinux). . ```. $ pip install git+https://github.com/theislab/scanpy --upgrade --user. $ python planaria.py . /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. import imp. scanpy==1.3.2+19.g94c3dc5 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:01:09.28). Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 166, in to_rgba. rgba = _colors_full_map.cache[c, alpha]. KeyError: ('mediumpurple3', None). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4288, in scatter. colors = mcolors.to_rgba_array(c). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 267, in to_rgba_array. result[i] = to_rgba(cc, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 168, in to_rgba. rgba = _to_rgba_no_colorcycle(c, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 212, in _to_rgba_no_colorcycle. raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c)). ValueError: Invalid RGBA argument: 'mediumpurple3'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""planaria.py"", line 47, in <module>. sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full'). File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:722,modifiability,pac,package,722,"No. There is still some issue with colors. Note that now I am on python3.7 (which is default on ArchLinux). . ```. $ pip install git+https://github.com/theislab/scanpy --upgrade --user. $ python planaria.py . /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. import imp. scanpy==1.3.2+19.g94c3dc5 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:01:09.28). Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 166, in to_rgba. rgba = _colors_full_map.cache[c, alpha]. KeyError: ('mediumpurple3', None). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4288, in scatter. colors = mcolors.to_rgba_array(c). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 267, in to_rgba_array. result[i] = to_rgba(cc, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 168, in to_rgba. rgba = _to_rgba_no_colorcycle(c, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 212, in _to_rgba_no_colorcycle. raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c)). ValueError: Invalid RGBA argument: 'mediumpurple3'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""planaria.py"", line 47, in <module>. sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full'). File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:838,modifiability,pac,packages,838,"No. There is still some issue with colors. Note that now I am on python3.7 (which is default on ArchLinux). . ```. $ pip install git+https://github.com/theislab/scanpy --upgrade --user. $ python planaria.py . /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. import imp. scanpy==1.3.2+19.g94c3dc5 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:01:09.28). Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 166, in to_rgba. rgba = _colors_full_map.cache[c, alpha]. KeyError: ('mediumpurple3', None). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4288, in scatter. colors = mcolors.to_rgba_array(c). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 267, in to_rgba_array. result[i] = to_rgba(cc, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 168, in to_rgba. rgba = _to_rgba_no_colorcycle(c, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 212, in _to_rgba_no_colorcycle. raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c)). ValueError: Invalid RGBA argument: 'mediumpurple3'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""planaria.py"", line 47, in <module>. sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full'). File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:1103,modifiability,pac,packages,1103," . ```. $ pip install git+https://github.com/theislab/scanpy --upgrade --user. $ python planaria.py . /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. import imp. scanpy==1.3.2+19.g94c3dc5 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:01:09.28). Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 166, in to_rgba. rgba = _colors_full_map.cache[c, alpha]. KeyError: ('mediumpurple3', None). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4288, in scatter. colors = mcolors.to_rgba_array(c). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 267, in to_rgba_array. result[i] = to_rgba(cc, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 168, in to_rgba. rgba = _to_rgba_no_colorcycle(c, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 212, in _to_rgba_no_colorcycle. raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c)). ValueError: Invalid RGBA argument: 'mediumpurple3'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""planaria.py"", line 47, in <module>. sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full'). File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line 47, in tsne. return plot_scatter(adata, basis='tsne', **kwargs). File ""/home1/dilawars/.local/lib/python3.7",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:1227,modifiability,pac,packages,1227,"/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. import imp. scanpy==1.3.2+19.g94c3dc5 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:01:09.28). Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 166, in to_rgba. rgba = _colors_full_map.cache[c, alpha]. KeyError: ('mediumpurple3', None). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4288, in scatter. colors = mcolors.to_rgba_array(c). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 267, in to_rgba_array. result[i] = to_rgba(cc, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 168, in to_rgba. rgba = _to_rgba_no_colorcycle(c, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 212, in _to_rgba_no_colorcycle. raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c)). ValueError: Invalid RGBA argument: 'mediumpurple3'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""planaria.py"", line 47, in <module>. sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full'). File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line 47, in tsne. return plot_scatter(adata, basis='tsne', **kwargs). File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line 301, in plot_scatter. **kwargs). File ""/usr/lib/python3.7/site-p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:1349,modifiability,pac,packages,1349," module is deprecated in favour of importlib; see the module's documentation for alternative uses. import imp. scanpy==1.3.2+19.g94c3dc5 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:01:09.28). Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 166, in to_rgba. rgba = _colors_full_map.cache[c, alpha]. KeyError: ('mediumpurple3', None). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4288, in scatter. colors = mcolors.to_rgba_array(c). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 267, in to_rgba_array. result[i] = to_rgba(cc, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 168, in to_rgba. rgba = _to_rgba_no_colorcycle(c, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 212, in _to_rgba_no_colorcycle. raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c)). ValueError: Invalid RGBA argument: 'mediumpurple3'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""planaria.py"", line 47, in <module>. sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full'). File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line 47, in tsne. return plot_scatter(adata, basis='tsne', **kwargs). File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line 301, in plot_scatter. **kwargs). File ""/usr/lib/python3.7/site-packages/matplotlib/__init__.py"", line 1867, in inner. return func(ax, *args, **kwargs). File ""/usr/lib/python3.7/site-pack",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:1474,modifiability,pac,packages,1474,"19.g94c3dc5 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:01:09.28). Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 166, in to_rgba. rgba = _colors_full_map.cache[c, alpha]. KeyError: ('mediumpurple3', None). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4288, in scatter. colors = mcolors.to_rgba_array(c). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 267, in to_rgba_array. result[i] = to_rgba(cc, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 168, in to_rgba. rgba = _to_rgba_no_colorcycle(c, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 212, in _to_rgba_no_colorcycle. raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c)). ValueError: Invalid RGBA argument: 'mediumpurple3'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""planaria.py"", line 47, in <module>. sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full'). File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line 47, in tsne. return plot_scatter(adata, basis='tsne', **kwargs). File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line 301, in plot_scatter. **kwargs). File ""/usr/lib/python3.7/site-packages/matplotlib/__init__.py"", line 1867, in inner. return func(ax, *args, **kwargs). File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4293, in scatter. .format(c.shape, x.size, y.size)). AttributeError: 'list' object has n",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:1797,modifiability,modul,module,1797,"das==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:01:09.28). Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 166, in to_rgba. rgba = _colors_full_map.cache[c, alpha]. KeyError: ('mediumpurple3', None). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4288, in scatter. colors = mcolors.to_rgba_array(c). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 267, in to_rgba_array. result[i] = to_rgba(cc, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 168, in to_rgba. rgba = _to_rgba_no_colorcycle(c, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 212, in _to_rgba_no_colorcycle. raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c)). ValueError: Invalid RGBA argument: 'mediumpurple3'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""planaria.py"", line 47, in <module>. sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full'). File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line 47, in tsne. return plot_scatter(adata, basis='tsne', **kwargs). File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line 301, in plot_scatter. **kwargs). File ""/usr/lib/python3.7/site-packages/matplotlib/__init__.py"", line 1867, in inner. return func(ax, *args, **kwargs). File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4293, in scatter. .format(c.shape, x.size, y.size)). AttributeError: 'list' object has no attribute 'shape'. [dilawars@chamcham scanpy_exp]$ . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:1946,modifiability,pac,packages,1946,"das==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:01:09.28). Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 166, in to_rgba. rgba = _colors_full_map.cache[c, alpha]. KeyError: ('mediumpurple3', None). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4288, in scatter. colors = mcolors.to_rgba_array(c). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 267, in to_rgba_array. result[i] = to_rgba(cc, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 168, in to_rgba. rgba = _to_rgba_no_colorcycle(c, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 212, in _to_rgba_no_colorcycle. raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c)). ValueError: Invalid RGBA argument: 'mediumpurple3'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""planaria.py"", line 47, in <module>. sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full'). File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line 47, in tsne. return plot_scatter(adata, basis='tsne', **kwargs). File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line 301, in plot_scatter. **kwargs). File ""/usr/lib/python3.7/site-packages/matplotlib/__init__.py"", line 1867, in inner. return func(ax, *args, **kwargs). File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4293, in scatter. .format(c.shape, x.size, y.size)). AttributeError: 'list' object has no attribute 'shape'. [dilawars@chamcham scanpy_exp]$ . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:2113,modifiability,pac,packages,2113,"das==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:01:09.28). Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 166, in to_rgba. rgba = _colors_full_map.cache[c, alpha]. KeyError: ('mediumpurple3', None). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4288, in scatter. colors = mcolors.to_rgba_array(c). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 267, in to_rgba_array. result[i] = to_rgba(cc, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 168, in to_rgba. rgba = _to_rgba_no_colorcycle(c, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 212, in _to_rgba_no_colorcycle. raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c)). ValueError: Invalid RGBA argument: 'mediumpurple3'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""planaria.py"", line 47, in <module>. sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full'). File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line 47, in tsne. return plot_scatter(adata, basis='tsne', **kwargs). File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line 301, in plot_scatter. **kwargs). File ""/usr/lib/python3.7/site-packages/matplotlib/__init__.py"", line 1867, in inner. return func(ax, *args, **kwargs). File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4293, in scatter. .format(c.shape, x.size, y.size)). AttributeError: 'list' object has no attribute 'shape'. [dilawars@chamcham scanpy_exp]$ . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:2230,modifiability,pac,packages,2230,"das==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:01:09.28). Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 166, in to_rgba. rgba = _colors_full_map.cache[c, alpha]. KeyError: ('mediumpurple3', None). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4288, in scatter. colors = mcolors.to_rgba_array(c). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 267, in to_rgba_array. result[i] = to_rgba(cc, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 168, in to_rgba. rgba = _to_rgba_no_colorcycle(c, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 212, in _to_rgba_no_colorcycle. raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c)). ValueError: Invalid RGBA argument: 'mediumpurple3'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""planaria.py"", line 47, in <module>. sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full'). File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line 47, in tsne. return plot_scatter(adata, basis='tsne', **kwargs). File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line 301, in plot_scatter. **kwargs). File ""/usr/lib/python3.7/site-packages/matplotlib/__init__.py"", line 1867, in inner. return func(ax, *args, **kwargs). File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4293, in scatter. .format(c.shape, x.size, y.size)). AttributeError: 'list' object has no attribute 'shape'. [dilawars@chamcham scanpy_exp]$ . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:2349,modifiability,pac,packages,2349,"das==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:01:09.28). Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 166, in to_rgba. rgba = _colors_full_map.cache[c, alpha]. KeyError: ('mediumpurple3', None). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4288, in scatter. colors = mcolors.to_rgba_array(c). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 267, in to_rgba_array. result[i] = to_rgba(cc, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 168, in to_rgba. rgba = _to_rgba_no_colorcycle(c, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 212, in _to_rgba_no_colorcycle. raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c)). ValueError: Invalid RGBA argument: 'mediumpurple3'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""planaria.py"", line 47, in <module>. sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full'). File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line 47, in tsne. return plot_scatter(adata, basis='tsne', **kwargs). File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line 301, in plot_scatter. **kwargs). File ""/usr/lib/python3.7/site-packages/matplotlib/__init__.py"", line 1867, in inner. return func(ax, *args, **kwargs). File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4293, in scatter. .format(c.shape, x.size, y.size)). AttributeError: 'list' object has no attribute 'shape'. [dilawars@chamcham scanpy_exp]$ . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:916,performance,cach,cache,916,"No. There is still some issue with colors. Note that now I am on python3.7 (which is default on ArchLinux). . ```. $ pip install git+https://github.com/theislab/scanpy --upgrade --user. $ python planaria.py . /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. import imp. scanpy==1.3.2+19.g94c3dc5 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:01:09.28). Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 166, in to_rgba. rgba = _colors_full_map.cache[c, alpha]. KeyError: ('mediumpurple3', None). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4288, in scatter. colors = mcolors.to_rgba_array(c). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 267, in to_rgba_array. result[i] = to_rgba(cc, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 168, in to_rgba. rgba = _to_rgba_no_colorcycle(c, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 212, in _to_rgba_no_colorcycle. raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c)). ValueError: Invalid RGBA argument: 'mediumpurple3'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""planaria.py"", line 47, in <module>. sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full'). File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:354,safety,modul,module,354,"No. There is still some issue with colors. Note that now I am on python3.7 (which is default on ArchLinux). . ```. $ pip install git+https://github.com/theislab/scanpy --upgrade --user. $ python planaria.py . /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. import imp. scanpy==1.3.2+19.g94c3dc5 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:01:09.28). Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 166, in to_rgba. rgba = _colors_full_map.cache[c, alpha]. KeyError: ('mediumpurple3', None). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4288, in scatter. colors = mcolors.to_rgba_array(c). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 267, in to_rgba_array. result[i] = to_rgba(cc, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 168, in to_rgba. rgba = _to_rgba_no_colorcycle(c, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 212, in _to_rgba_no_colorcycle. raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c)). ValueError: Invalid RGBA argument: 'mediumpurple3'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""planaria.py"", line 47, in <module>. sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full'). File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:407,safety,modul,module,407,"No. There is still some issue with colors. Note that now I am on python3.7 (which is default on ArchLinux). . ```. $ pip install git+https://github.com/theislab/scanpy --upgrade --user. $ python planaria.py . /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. import imp. scanpy==1.3.2+19.g94c3dc5 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:01:09.28). Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 166, in to_rgba. rgba = _colors_full_map.cache[c, alpha]. KeyError: ('mediumpurple3', None). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4288, in scatter. colors = mcolors.to_rgba_array(c). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 267, in to_rgba_array. result[i] = to_rgba(cc, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 168, in to_rgba. rgba = _to_rgba_no_colorcycle(c, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 212, in _to_rgba_no_colorcycle. raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c)). ValueError: Invalid RGBA argument: 'mediumpurple3'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""planaria.py"", line 47, in <module>. sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full'). File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:997,safety,except,exception,997,". There is still some issue with colors. Note that now I am on python3.7 (which is default on ArchLinux). . ```. $ pip install git+https://github.com/theislab/scanpy --upgrade --user. $ python planaria.py . /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. import imp. scanpy==1.3.2+19.g94c3dc5 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:01:09.28). Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 166, in to_rgba. rgba = _colors_full_map.cache[c, alpha]. KeyError: ('mediumpurple3', None). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4288, in scatter. colors = mcolors.to_rgba_array(c). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 267, in to_rgba_array. result[i] = to_rgba(cc, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 168, in to_rgba. rgba = _to_rgba_no_colorcycle(c, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 212, in _to_rgba_no_colorcycle. raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c)). ValueError: Invalid RGBA argument: 'mediumpurple3'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""planaria.py"", line 47, in <module>. sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full'). File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line 47",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:1016,safety,except,exception,1016,"me issue with colors. Note that now I am on python3.7 (which is default on ArchLinux). . ```. $ pip install git+https://github.com/theislab/scanpy --upgrade --user. $ python planaria.py . /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. import imp. scanpy==1.3.2+19.g94c3dc5 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:01:09.28). Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 166, in to_rgba. rgba = _colors_full_map.cache[c, alpha]. KeyError: ('mediumpurple3', None). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4288, in scatter. colors = mcolors.to_rgba_array(c). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 267, in to_rgba_array. result[i] = to_rgba(cc, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 168, in to_rgba. rgba = _to_rgba_no_colorcycle(c, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 212, in _to_rgba_no_colorcycle. raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c)). ValueError: Invalid RGBA argument: 'mediumpurple3'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""planaria.py"", line 47, in <module>. sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full'). File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line 47, in tsne. return p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:1688,safety,except,exception,1688,"das==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:01:09.28). Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 166, in to_rgba. rgba = _colors_full_map.cache[c, alpha]. KeyError: ('mediumpurple3', None). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4288, in scatter. colors = mcolors.to_rgba_array(c). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 267, in to_rgba_array. result[i] = to_rgba(cc, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 168, in to_rgba. rgba = _to_rgba_no_colorcycle(c, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 212, in _to_rgba_no_colorcycle. raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c)). ValueError: Invalid RGBA argument: 'mediumpurple3'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""planaria.py"", line 47, in <module>. sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full'). File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line 47, in tsne. return plot_scatter(adata, basis='tsne', **kwargs). File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line 301, in plot_scatter. **kwargs). File ""/usr/lib/python3.7/site-packages/matplotlib/__init__.py"", line 1867, in inner. return func(ax, *args, **kwargs). File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4293, in scatter. .format(c.shape, x.size, y.size)). AttributeError: 'list' object has no attribute 'shape'. [dilawars@chamcham scanpy_exp]$ . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:1707,safety,except,exception,1707,"das==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:01:09.28). Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 166, in to_rgba. rgba = _colors_full_map.cache[c, alpha]. KeyError: ('mediumpurple3', None). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4288, in scatter. colors = mcolors.to_rgba_array(c). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 267, in to_rgba_array. result[i] = to_rgba(cc, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 168, in to_rgba. rgba = _to_rgba_no_colorcycle(c, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 212, in _to_rgba_no_colorcycle. raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c)). ValueError: Invalid RGBA argument: 'mediumpurple3'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""planaria.py"", line 47, in <module>. sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full'). File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line 47, in tsne. return plot_scatter(adata, basis='tsne', **kwargs). File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line 301, in plot_scatter. **kwargs). File ""/usr/lib/python3.7/site-packages/matplotlib/__init__.py"", line 1867, in inner. return func(ax, *args, **kwargs). File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4293, in scatter. .format(c.shape, x.size, y.size)). AttributeError: 'list' object has no attribute 'shape'. [dilawars@chamcham scanpy_exp]$ . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:1797,safety,modul,module,1797,"das==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:01:09.28). Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 166, in to_rgba. rgba = _colors_full_map.cache[c, alpha]. KeyError: ('mediumpurple3', None). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4288, in scatter. colors = mcolors.to_rgba_array(c). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 267, in to_rgba_array. result[i] = to_rgba(cc, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 168, in to_rgba. rgba = _to_rgba_no_colorcycle(c, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 212, in _to_rgba_no_colorcycle. raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c)). ValueError: Invalid RGBA argument: 'mediumpurple3'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""planaria.py"", line 47, in <module>. sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full'). File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line 47, in tsne. return plot_scatter(adata, basis='tsne', **kwargs). File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line 301, in plot_scatter. **kwargs). File ""/usr/lib/python3.7/site-packages/matplotlib/__init__.py"", line 1867, in inner. return func(ax, *args, **kwargs). File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4293, in scatter. .format(c.shape, x.size, y.size)). AttributeError: 'list' object has no attribute 'shape'. [dilawars@chamcham scanpy_exp]$ . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:195,testability,plan,planaria,195,"No. There is still some issue with colors. Note that now I am on python3.7 (which is default on ArchLinux). . ```. $ pip install git+https://github.com/theislab/scanpy --upgrade --user. $ python planaria.py . /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. import imp. scanpy==1.3.2+19.g94c3dc5 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:01:09.28). Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 166, in to_rgba. rgba = _colors_full_map.cache[c, alpha]. KeyError: ('mediumpurple3', None). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4288, in scatter. colors = mcolors.to_rgba_array(c). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 267, in to_rgba_array. result[i] = to_rgba(cc, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 168, in to_rgba. rgba = _to_rgba_no_colorcycle(c, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 212, in _to_rgba_no_colorcycle. raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c)). ValueError: Invalid RGBA argument: 'mediumpurple3'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""planaria.py"", line 47, in <module>. sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full'). File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:772,testability,Trace,Traceback,772,"No. There is still some issue with colors. Note that now I am on python3.7 (which is default on ArchLinux). . ```. $ pip install git+https://github.com/theislab/scanpy --upgrade --user. $ python planaria.py . /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. import imp. scanpy==1.3.2+19.g94c3dc5 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:01:09.28). Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 166, in to_rgba. rgba = _colors_full_map.cache[c, alpha]. KeyError: ('mediumpurple3', None). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4288, in scatter. colors = mcolors.to_rgba_array(c). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 267, in to_rgba_array. result[i] = to_rgba(cc, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 168, in to_rgba. rgba = _to_rgba_no_colorcycle(c, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 212, in _to_rgba_no_colorcycle. raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c)). ValueError: Invalid RGBA argument: 'mediumpurple3'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""planaria.py"", line 47, in <module>. sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full'). File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:1037,testability,Trace,Traceback,1037," Note that now I am on python3.7 (which is default on ArchLinux). . ```. $ pip install git+https://github.com/theislab/scanpy --upgrade --user. $ python planaria.py . /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. import imp. scanpy==1.3.2+19.g94c3dc5 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:01:09.28). Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 166, in to_rgba. rgba = _colors_full_map.cache[c, alpha]. KeyError: ('mediumpurple3', None). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4288, in scatter. colors = mcolors.to_rgba_array(c). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 267, in to_rgba_array. result[i] = to_rgba(cc, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 168, in to_rgba. rgba = _to_rgba_no_colorcycle(c, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 212, in _to_rgba_no_colorcycle. raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c)). ValueError: Invalid RGBA argument: 'mediumpurple3'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""planaria.py"", line 47, in <module>. sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full'). File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line 47, in tsne. return plot_scatter(adata, ba",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:1728,testability,Trace,Traceback,1728,"das==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:01:09.28). Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 166, in to_rgba. rgba = _colors_full_map.cache[c, alpha]. KeyError: ('mediumpurple3', None). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4288, in scatter. colors = mcolors.to_rgba_array(c). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 267, in to_rgba_array. result[i] = to_rgba(cc, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 168, in to_rgba. rgba = _to_rgba_no_colorcycle(c, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 212, in _to_rgba_no_colorcycle. raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c)). ValueError: Invalid RGBA argument: 'mediumpurple3'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""planaria.py"", line 47, in <module>. sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full'). File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line 47, in tsne. return plot_scatter(adata, basis='tsne', **kwargs). File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line 301, in plot_scatter. **kwargs). File ""/usr/lib/python3.7/site-packages/matplotlib/__init__.py"", line 1867, in inner. return func(ax, *args, **kwargs). File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4293, in scatter. .format(c.shape, x.size, y.size)). AttributeError: 'list' object has no attribute 'shape'. [dilawars@chamcham scanpy_exp]$ . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:1770,testability,plan,planaria,1770,"das==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:01:09.28). Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 166, in to_rgba. rgba = _colors_full_map.cache[c, alpha]. KeyError: ('mediumpurple3', None). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4288, in scatter. colors = mcolors.to_rgba_array(c). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 267, in to_rgba_array. result[i] = to_rgba(cc, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 168, in to_rgba. rgba = _to_rgba_no_colorcycle(c, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 212, in _to_rgba_no_colorcycle. raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c)). ValueError: Invalid RGBA argument: 'mediumpurple3'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""planaria.py"", line 47, in <module>. sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full'). File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line 47, in tsne. return plot_scatter(adata, basis='tsne', **kwargs). File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line 301, in plot_scatter. **kwargs). File ""/usr/lib/python3.7/site-packages/matplotlib/__init__.py"", line 1867, in inner. return func(ax, *args, **kwargs). File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4293, in scatter. .format(c.shape, x.size, y.size)). AttributeError: 'list' object has no attribute 'shape'. [dilawars@chamcham scanpy_exp]$ . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:180,usability,user,user,180,"No. There is still some issue with colors. Note that now I am on python3.7 (which is default on ArchLinux). . ```. $ pip install git+https://github.com/theislab/scanpy --upgrade --user. $ python planaria.py . /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. import imp. scanpy==1.3.2+19.g94c3dc5 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:01:09.28). Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 166, in to_rgba. rgba = _colors_full_map.cache[c, alpha]. KeyError: ('mediumpurple3', None). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4288, in scatter. colors = mcolors.to_rgba_array(c). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 267, in to_rgba_array. result[i] = to_rgba(cc, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 168, in to_rgba. rgba = _to_rgba_no_colorcycle(c, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 212, in _to_rgba_no_colorcycle. raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c)). ValueError: Invalid RGBA argument: 'mediumpurple3'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""planaria.py"", line 47, in <module>. sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full'). File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:416,usability,document,documentation,416,"No. There is still some issue with colors. Note that now I am on python3.7 (which is default on ArchLinux). . ```. $ pip install git+https://github.com/theislab/scanpy --upgrade --user. $ python planaria.py . /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. import imp. scanpy==1.3.2+19.g94c3dc5 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:01:09.28). Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 166, in to_rgba. rgba = _colors_full_map.cache[c, alpha]. KeyError: ('mediumpurple3', None). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4288, in scatter. colors = mcolors.to_rgba_array(c). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 267, in to_rgba_array. result[i] = to_rgba(cc, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 168, in to_rgba. rgba = _to_rgba_no_colorcycle(c, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 212, in _to_rgba_no_colorcycle. raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c)). ValueError: Invalid RGBA argument: 'mediumpurple3'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""planaria.py"", line 47, in <module>. sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full'). File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:555,usability,learn,learn,555,"No. There is still some issue with colors. Note that now I am on python3.7 (which is default on ArchLinux). . ```. $ pip install git+https://github.com/theislab/scanpy --upgrade --user. $ python planaria.py . /home1/dilawars/.local/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses. import imp. scanpy==1.3.2+19.g94c3dc5 anndata==0.6.10 numpy==1.15.2 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:01:09.28). Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 166, in to_rgba. rgba = _colors_full_map.cache[c, alpha]. KeyError: ('mediumpurple3', None). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4288, in scatter. colors = mcolors.to_rgba_array(c). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 267, in to_rgba_array. result[i] = to_rgba(cc, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 168, in to_rgba. rgba = _to_rgba_no_colorcycle(c, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 212, in _to_rgba_no_colorcycle. raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c)). ValueError: Invalid RGBA argument: 'mediumpurple3'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""planaria.py"", line 47, in <module>. sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full'). File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:1971,usability,tool,tools,1971,"das==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:01:09.28). Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 166, in to_rgba. rgba = _colors_full_map.cache[c, alpha]. KeyError: ('mediumpurple3', None). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4288, in scatter. colors = mcolors.to_rgba_array(c). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 267, in to_rgba_array. result[i] = to_rgba(cc, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 168, in to_rgba. rgba = _to_rgba_no_colorcycle(c, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 212, in _to_rgba_no_colorcycle. raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c)). ValueError: Invalid RGBA argument: 'mediumpurple3'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""planaria.py"", line 47, in <module>. sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full'). File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line 47, in tsne. return plot_scatter(adata, basis='tsne', **kwargs). File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line 301, in plot_scatter. **kwargs). File ""/usr/lib/python3.7/site-packages/matplotlib/__init__.py"", line 1867, in inner. return func(ax, *args, **kwargs). File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4293, in scatter. .format(c.shape, x.size, y.size)). AttributeError: 'list' object has no attribute 'shape'. [dilawars@chamcham scanpy_exp]$ . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:2138,usability,tool,tools,2138,"das==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 . ... storing 'clusters' as categorical. computing tSNE. using data matrix X directly. using the 'MulticoreTSNE' package by Ulyanov (2017). finished (0:01:09.28). Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 166, in to_rgba. rgba = _colors_full_map.cache[c, alpha]. KeyError: ('mediumpurple3', None). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4288, in scatter. colors = mcolors.to_rgba_array(c). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 267, in to_rgba_array. result[i] = to_rgba(cc, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 168, in to_rgba. rgba = _to_rgba_no_colorcycle(c, alpha). File ""/usr/lib/python3.7/site-packages/matplotlib/colors.py"", line 212, in _to_rgba_no_colorcycle. raise ValueError(""Invalid RGBA argument: {!r}"".format(orig_c)). ValueError: Invalid RGBA argument: 'mediumpurple3'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""planaria.py"", line 47, in <module>. sc.pl.tsne(adata, color='clusters', legend_loc='on data', legend_fontsize=5, save='_full'). File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line 47, in tsne. return plot_scatter(adata, basis='tsne', **kwargs). File ""/home1/dilawars/.local/lib/python3.7/site-packages/scanpy/plotting/tools/scatterplots.py"", line 301, in plot_scatter. **kwargs). File ""/usr/lib/python3.7/site-packages/matplotlib/__init__.py"", line 1867, in inner. return func(ax, *args, **kwargs). File ""/usr/lib/python3.7/site-packages/matplotlib/axes/_axes.py"", line 4293, in scatter. .format(c.shape, x.size, y.size)). AttributeError: 'list' object has no attribute 'shape'. [dilawars@chamcham scanpy_exp]$ . ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:67,deployability,contain,contain,67,I think the problem is that some of the color lists in `adata.uns` contain these matplotlib invalid color names. The current code translate those colors before setting `adata.uns` but not after. I will add a check for that.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:117,energy efficiency,current,current,117,I think the problem is that some of the color lists in `adata.uns` contain these matplotlib invalid color names. The current code translate those colors before setting `adata.uns` but not after. I will add a check for that.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:130,integrability,translat,translate,130,I think the problem is that some of the color lists in `adata.uns` contain these matplotlib invalid color names. The current code translate those colors before setting `adata.uns` but not after. I will add a check for that.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:130,interoperability,translat,translate,130,I think the problem is that some of the color lists in `adata.uns` contain these matplotlib invalid color names. The current code translate those colors before setting `adata.uns` but not after. I will add a check for that.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:9,energy efficiency,current,currently,9,I'm also currently having trouble with sc.pl.scatter. The palette keyword doesn't seem to be working? The colors always seem to be magma no matter what I set as the palette.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:74,reliability,doe,doesn,74,I'm also currently having trouble with sc.pl.scatter. The palette keyword doesn't seem to be working? The colors always seem to be magma no matter what I set as the palette.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:59,safety,test,test,59,Can you type the command you are using? Or better set up a test case. See the example on #293 maybe you can reproduce your problem with that set up.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:59,testability,test,test,59,Can you type the command you are using? Or better set up a test case. See the example on #293 maybe you can reproduce your problem with that set up.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:17,usability,command,command,17,Can you type the command you are using? Or better set up a test case. See the example on #293 maybe you can reproduce your problem with that set up.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:295,availability,cluster,clusters,295,"Hi, sorry for not giving more of a description of the issue I was having. I tried to recreate a minimal example today using the PBMC_68k dataset and the cmap argument seemed to be working fine when using a gene as the color, but I'm still having problems with categorical variables like louvain clusters or user-defined cluster names. ```. fig, ax = plt.subplots(2,2,figsize=(12,8)). sc.pl.umap(adata, color='louvain', ax = ax[0,0], show=False). sc.pl.umap(adata, color='louvain', ax = ax[0,1], cmap=""tab10"", show=False). ax[1,0].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.obs['louvain'], cmap=""tab10"", s=0.1). ax[1,1].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.obs['louvain'], cmap=""tab20b"", s=0.1). ```. ![image](https://user-images.githubusercontent.com/7407663/47044553-8008ee00-d15e-11e8-8791-65ccb0fc7769.png). ```. fig, ax = plt.subplots(2,2,figsize=(12,8)). sc.pl.umap(adata, color=[""CD74""], ax=ax[0,0], show=False). sc.pl.umap(adata, color=[""CD74""], cmap=""viridis"", ax=ax[0,1], show=False). ax[1,0].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.X[:,adata.var_names==""CD74""].flatten(), cmap=""magma"", s=0.1). ax[1,1].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.X[:,adata.var_names==""CD74""].flatten(), cmap=""viridis"",. s=0.1, vmin=-0.6, vmax=3.5). ```. ![image](https://user-images.githubusercontent.com/7407663/47044843-45538580-d15f-11e8-8b05-89a1f75d3cee.png). These are the versions I'm using:. scanpy==1.3.2 anndata==0.6.11 numpy==1.14.6 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . My matplotlib version is 3.0.0.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:320,availability,cluster,cluster,320,"Hi, sorry for not giving more of a description of the issue I was having. I tried to recreate a minimal example today using the PBMC_68k dataset and the cmap argument seemed to be working fine when using a gene as the color, but I'm still having problems with categorical variables like louvain clusters or user-defined cluster names. ```. fig, ax = plt.subplots(2,2,figsize=(12,8)). sc.pl.umap(adata, color='louvain', ax = ax[0,0], show=False). sc.pl.umap(adata, color='louvain', ax = ax[0,1], cmap=""tab10"", show=False). ax[1,0].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.obs['louvain'], cmap=""tab10"", s=0.1). ax[1,1].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.obs['louvain'], cmap=""tab20b"", s=0.1). ```. ![image](https://user-images.githubusercontent.com/7407663/47044553-8008ee00-d15e-11e8-8791-65ccb0fc7769.png). ```. fig, ax = plt.subplots(2,2,figsize=(12,8)). sc.pl.umap(adata, color=[""CD74""], ax=ax[0,0], show=False). sc.pl.umap(adata, color=[""CD74""], cmap=""viridis"", ax=ax[0,1], show=False). ax[1,0].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.X[:,adata.var_names==""CD74""].flatten(), cmap=""magma"", s=0.1). ax[1,1].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.X[:,adata.var_names==""CD74""].flatten(), cmap=""viridis"",. s=0.1, vmin=-0.6, vmax=3.5). ```. ![image](https://user-images.githubusercontent.com/7407663/47044843-45538580-d15f-11e8-8b05-89a1f75d3cee.png). These are the versions I'm using:. scanpy==1.3.2 anndata==0.6.11 numpy==1.14.6 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . My matplotlib version is 3.0.0.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:295,deployability,cluster,clusters,295,"Hi, sorry for not giving more of a description of the issue I was having. I tried to recreate a minimal example today using the PBMC_68k dataset and the cmap argument seemed to be working fine when using a gene as the color, but I'm still having problems with categorical variables like louvain clusters or user-defined cluster names. ```. fig, ax = plt.subplots(2,2,figsize=(12,8)). sc.pl.umap(adata, color='louvain', ax = ax[0,0], show=False). sc.pl.umap(adata, color='louvain', ax = ax[0,1], cmap=""tab10"", show=False). ax[1,0].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.obs['louvain'], cmap=""tab10"", s=0.1). ax[1,1].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.obs['louvain'], cmap=""tab20b"", s=0.1). ```. ![image](https://user-images.githubusercontent.com/7407663/47044553-8008ee00-d15e-11e8-8791-65ccb0fc7769.png). ```. fig, ax = plt.subplots(2,2,figsize=(12,8)). sc.pl.umap(adata, color=[""CD74""], ax=ax[0,0], show=False). sc.pl.umap(adata, color=[""CD74""], cmap=""viridis"", ax=ax[0,1], show=False). ax[1,0].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.X[:,adata.var_names==""CD74""].flatten(), cmap=""magma"", s=0.1). ax[1,1].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.X[:,adata.var_names==""CD74""].flatten(), cmap=""viridis"",. s=0.1, vmin=-0.6, vmax=3.5). ```. ![image](https://user-images.githubusercontent.com/7407663/47044843-45538580-d15f-11e8-8b05-89a1f75d3cee.png). These are the versions I'm using:. scanpy==1.3.2 anndata==0.6.11 numpy==1.14.6 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . My matplotlib version is 3.0.0.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:320,deployability,cluster,cluster,320,"Hi, sorry for not giving more of a description of the issue I was having. I tried to recreate a minimal example today using the PBMC_68k dataset and the cmap argument seemed to be working fine when using a gene as the color, but I'm still having problems with categorical variables like louvain clusters or user-defined cluster names. ```. fig, ax = plt.subplots(2,2,figsize=(12,8)). sc.pl.umap(adata, color='louvain', ax = ax[0,0], show=False). sc.pl.umap(adata, color='louvain', ax = ax[0,1], cmap=""tab10"", show=False). ax[1,0].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.obs['louvain'], cmap=""tab10"", s=0.1). ax[1,1].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.obs['louvain'], cmap=""tab20b"", s=0.1). ```. ![image](https://user-images.githubusercontent.com/7407663/47044553-8008ee00-d15e-11e8-8791-65ccb0fc7769.png). ```. fig, ax = plt.subplots(2,2,figsize=(12,8)). sc.pl.umap(adata, color=[""CD74""], ax=ax[0,0], show=False). sc.pl.umap(adata, color=[""CD74""], cmap=""viridis"", ax=ax[0,1], show=False). ax[1,0].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.X[:,adata.var_names==""CD74""].flatten(), cmap=""magma"", s=0.1). ax[1,1].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.X[:,adata.var_names==""CD74""].flatten(), cmap=""viridis"",. s=0.1, vmin=-0.6, vmax=3.5). ```. ![image](https://user-images.githubusercontent.com/7407663/47044843-45538580-d15f-11e8-8b05-89a1f75d3cee.png). These are the versions I'm using:. scanpy==1.3.2 anndata==0.6.11 numpy==1.14.6 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . My matplotlib version is 3.0.0.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:1492,deployability,version,versions,1492,"Hi, sorry for not giving more of a description of the issue I was having. I tried to recreate a minimal example today using the PBMC_68k dataset and the cmap argument seemed to be working fine when using a gene as the color, but I'm still having problems with categorical variables like louvain clusters or user-defined cluster names. ```. fig, ax = plt.subplots(2,2,figsize=(12,8)). sc.pl.umap(adata, color='louvain', ax = ax[0,0], show=False). sc.pl.umap(adata, color='louvain', ax = ax[0,1], cmap=""tab10"", show=False). ax[1,0].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.obs['louvain'], cmap=""tab10"", s=0.1). ax[1,1].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.obs['louvain'], cmap=""tab20b"", s=0.1). ```. ![image](https://user-images.githubusercontent.com/7407663/47044553-8008ee00-d15e-11e8-8791-65ccb0fc7769.png). ```. fig, ax = plt.subplots(2,2,figsize=(12,8)). sc.pl.umap(adata, color=[""CD74""], ax=ax[0,0], show=False). sc.pl.umap(adata, color=[""CD74""], cmap=""viridis"", ax=ax[0,1], show=False). ax[1,0].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.X[:,adata.var_names==""CD74""].flatten(), cmap=""magma"", s=0.1). ax[1,1].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.X[:,adata.var_names==""CD74""].flatten(), cmap=""viridis"",. s=0.1, vmin=-0.6, vmax=3.5). ```. ![image](https://user-images.githubusercontent.com/7407663/47044843-45538580-d15f-11e8-8b05-89a1f75d3cee.png). These are the versions I'm using:. scanpy==1.3.2 anndata==0.6.11 numpy==1.14.6 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . My matplotlib version is 3.0.0.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:1677,deployability,version,version,1677,"Hi, sorry for not giving more of a description of the issue I was having. I tried to recreate a minimal example today using the PBMC_68k dataset and the cmap argument seemed to be working fine when using a gene as the color, but I'm still having problems with categorical variables like louvain clusters or user-defined cluster names. ```. fig, ax = plt.subplots(2,2,figsize=(12,8)). sc.pl.umap(adata, color='louvain', ax = ax[0,0], show=False). sc.pl.umap(adata, color='louvain', ax = ax[0,1], cmap=""tab10"", show=False). ax[1,0].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.obs['louvain'], cmap=""tab10"", s=0.1). ax[1,1].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.obs['louvain'], cmap=""tab20b"", s=0.1). ```. ![image](https://user-images.githubusercontent.com/7407663/47044553-8008ee00-d15e-11e8-8791-65ccb0fc7769.png). ```. fig, ax = plt.subplots(2,2,figsize=(12,8)). sc.pl.umap(adata, color=[""CD74""], ax=ax[0,0], show=False). sc.pl.umap(adata, color=[""CD74""], cmap=""viridis"", ax=ax[0,1], show=False). ax[1,0].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.X[:,adata.var_names==""CD74""].flatten(), cmap=""magma"", s=0.1). ax[1,1].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.X[:,adata.var_names==""CD74""].flatten(), cmap=""viridis"",. s=0.1, vmin=-0.6, vmax=3.5). ```. ![image](https://user-images.githubusercontent.com/7407663/47044843-45538580-d15f-11e8-8b05-89a1f75d3cee.png). These are the versions I'm using:. scanpy==1.3.2 anndata==0.6.11 numpy==1.14.6 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . My matplotlib version is 3.0.0.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:354,integrability,sub,subplots,354,"Hi, sorry for not giving more of a description of the issue I was having. I tried to recreate a minimal example today using the PBMC_68k dataset and the cmap argument seemed to be working fine when using a gene as the color, but I'm still having problems with categorical variables like louvain clusters or user-defined cluster names. ```. fig, ax = plt.subplots(2,2,figsize=(12,8)). sc.pl.umap(adata, color='louvain', ax = ax[0,0], show=False). sc.pl.umap(adata, color='louvain', ax = ax[0,1], cmap=""tab10"", show=False). ax[1,0].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.obs['louvain'], cmap=""tab10"", s=0.1). ax[1,1].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.obs['louvain'], cmap=""tab20b"", s=0.1). ```. ![image](https://user-images.githubusercontent.com/7407663/47044553-8008ee00-d15e-11e8-8791-65ccb0fc7769.png). ```. fig, ax = plt.subplots(2,2,figsize=(12,8)). sc.pl.umap(adata, color=[""CD74""], ax=ax[0,0], show=False). sc.pl.umap(adata, color=[""CD74""], cmap=""viridis"", ax=ax[0,1], show=False). ax[1,0].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.X[:,adata.var_names==""CD74""].flatten(), cmap=""magma"", s=0.1). ax[1,1].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.X[:,adata.var_names==""CD74""].flatten(), cmap=""viridis"",. s=0.1, vmin=-0.6, vmax=3.5). ```. ![image](https://user-images.githubusercontent.com/7407663/47044843-45538580-d15f-11e8-8b05-89a1f75d3cee.png). These are the versions I'm using:. scanpy==1.3.2 anndata==0.6.11 numpy==1.14.6 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . My matplotlib version is 3.0.0.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:892,integrability,sub,subplots,892,"Hi, sorry for not giving more of a description of the issue I was having. I tried to recreate a minimal example today using the PBMC_68k dataset and the cmap argument seemed to be working fine when using a gene as the color, but I'm still having problems with categorical variables like louvain clusters or user-defined cluster names. ```. fig, ax = plt.subplots(2,2,figsize=(12,8)). sc.pl.umap(adata, color='louvain', ax = ax[0,0], show=False). sc.pl.umap(adata, color='louvain', ax = ax[0,1], cmap=""tab10"", show=False). ax[1,0].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.obs['louvain'], cmap=""tab10"", s=0.1). ax[1,1].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.obs['louvain'], cmap=""tab20b"", s=0.1). ```. ![image](https://user-images.githubusercontent.com/7407663/47044553-8008ee00-d15e-11e8-8791-65ccb0fc7769.png). ```. fig, ax = plt.subplots(2,2,figsize=(12,8)). sc.pl.umap(adata, color=[""CD74""], ax=ax[0,0], show=False). sc.pl.umap(adata, color=[""CD74""], cmap=""viridis"", ax=ax[0,1], show=False). ax[1,0].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.X[:,adata.var_names==""CD74""].flatten(), cmap=""magma"", s=0.1). ax[1,1].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.X[:,adata.var_names==""CD74""].flatten(), cmap=""viridis"",. s=0.1, vmin=-0.6, vmax=3.5). ```. ![image](https://user-images.githubusercontent.com/7407663/47044843-45538580-d15f-11e8-8b05-89a1f75d3cee.png). These are the versions I'm using:. scanpy==1.3.2 anndata==0.6.11 numpy==1.14.6 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . My matplotlib version is 3.0.0.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:1492,integrability,version,versions,1492,"Hi, sorry for not giving more of a description of the issue I was having. I tried to recreate a minimal example today using the PBMC_68k dataset and the cmap argument seemed to be working fine when using a gene as the color, but I'm still having problems with categorical variables like louvain clusters or user-defined cluster names. ```. fig, ax = plt.subplots(2,2,figsize=(12,8)). sc.pl.umap(adata, color='louvain', ax = ax[0,0], show=False). sc.pl.umap(adata, color='louvain', ax = ax[0,1], cmap=""tab10"", show=False). ax[1,0].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.obs['louvain'], cmap=""tab10"", s=0.1). ax[1,1].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.obs['louvain'], cmap=""tab20b"", s=0.1). ```. ![image](https://user-images.githubusercontent.com/7407663/47044553-8008ee00-d15e-11e8-8791-65ccb0fc7769.png). ```. fig, ax = plt.subplots(2,2,figsize=(12,8)). sc.pl.umap(adata, color=[""CD74""], ax=ax[0,0], show=False). sc.pl.umap(adata, color=[""CD74""], cmap=""viridis"", ax=ax[0,1], show=False). ax[1,0].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.X[:,adata.var_names==""CD74""].flatten(), cmap=""magma"", s=0.1). ax[1,1].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.X[:,adata.var_names==""CD74""].flatten(), cmap=""viridis"",. s=0.1, vmin=-0.6, vmax=3.5). ```. ![image](https://user-images.githubusercontent.com/7407663/47044843-45538580-d15f-11e8-8b05-89a1f75d3cee.png). These are the versions I'm using:. scanpy==1.3.2 anndata==0.6.11 numpy==1.14.6 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . My matplotlib version is 3.0.0.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:1677,integrability,version,version,1677,"Hi, sorry for not giving more of a description of the issue I was having. I tried to recreate a minimal example today using the PBMC_68k dataset and the cmap argument seemed to be working fine when using a gene as the color, but I'm still having problems with categorical variables like louvain clusters or user-defined cluster names. ```. fig, ax = plt.subplots(2,2,figsize=(12,8)). sc.pl.umap(adata, color='louvain', ax = ax[0,0], show=False). sc.pl.umap(adata, color='louvain', ax = ax[0,1], cmap=""tab10"", show=False). ax[1,0].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.obs['louvain'], cmap=""tab10"", s=0.1). ax[1,1].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.obs['louvain'], cmap=""tab20b"", s=0.1). ```. ![image](https://user-images.githubusercontent.com/7407663/47044553-8008ee00-d15e-11e8-8791-65ccb0fc7769.png). ```. fig, ax = plt.subplots(2,2,figsize=(12,8)). sc.pl.umap(adata, color=[""CD74""], ax=ax[0,0], show=False). sc.pl.umap(adata, color=[""CD74""], cmap=""viridis"", ax=ax[0,1], show=False). ax[1,0].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.X[:,adata.var_names==""CD74""].flatten(), cmap=""magma"", s=0.1). ax[1,1].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.X[:,adata.var_names==""CD74""].flatten(), cmap=""viridis"",. s=0.1, vmin=-0.6, vmax=3.5). ```. ![image](https://user-images.githubusercontent.com/7407663/47044843-45538580-d15f-11e8-8b05-89a1f75d3cee.png). These are the versions I'm using:. scanpy==1.3.2 anndata==0.6.11 numpy==1.14.6 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . My matplotlib version is 3.0.0.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:272,modifiability,variab,variables,272,"Hi, sorry for not giving more of a description of the issue I was having. I tried to recreate a minimal example today using the PBMC_68k dataset and the cmap argument seemed to be working fine when using a gene as the color, but I'm still having problems with categorical variables like louvain clusters or user-defined cluster names. ```. fig, ax = plt.subplots(2,2,figsize=(12,8)). sc.pl.umap(adata, color='louvain', ax = ax[0,0], show=False). sc.pl.umap(adata, color='louvain', ax = ax[0,1], cmap=""tab10"", show=False). ax[1,0].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.obs['louvain'], cmap=""tab10"", s=0.1). ax[1,1].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.obs['louvain'], cmap=""tab20b"", s=0.1). ```. ![image](https://user-images.githubusercontent.com/7407663/47044553-8008ee00-d15e-11e8-8791-65ccb0fc7769.png). ```. fig, ax = plt.subplots(2,2,figsize=(12,8)). sc.pl.umap(adata, color=[""CD74""], ax=ax[0,0], show=False). sc.pl.umap(adata, color=[""CD74""], cmap=""viridis"", ax=ax[0,1], show=False). ax[1,0].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.X[:,adata.var_names==""CD74""].flatten(), cmap=""magma"", s=0.1). ax[1,1].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.X[:,adata.var_names==""CD74""].flatten(), cmap=""viridis"",. s=0.1, vmin=-0.6, vmax=3.5). ```. ![image](https://user-images.githubusercontent.com/7407663/47044843-45538580-d15f-11e8-8b05-89a1f75d3cee.png). These are the versions I'm using:. scanpy==1.3.2 anndata==0.6.11 numpy==1.14.6 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . My matplotlib version is 3.0.0.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:1492,modifiability,version,versions,1492,"Hi, sorry for not giving more of a description of the issue I was having. I tried to recreate a minimal example today using the PBMC_68k dataset and the cmap argument seemed to be working fine when using a gene as the color, but I'm still having problems with categorical variables like louvain clusters or user-defined cluster names. ```. fig, ax = plt.subplots(2,2,figsize=(12,8)). sc.pl.umap(adata, color='louvain', ax = ax[0,0], show=False). sc.pl.umap(adata, color='louvain', ax = ax[0,1], cmap=""tab10"", show=False). ax[1,0].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.obs['louvain'], cmap=""tab10"", s=0.1). ax[1,1].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.obs['louvain'], cmap=""tab20b"", s=0.1). ```. ![image](https://user-images.githubusercontent.com/7407663/47044553-8008ee00-d15e-11e8-8791-65ccb0fc7769.png). ```. fig, ax = plt.subplots(2,2,figsize=(12,8)). sc.pl.umap(adata, color=[""CD74""], ax=ax[0,0], show=False). sc.pl.umap(adata, color=[""CD74""], cmap=""viridis"", ax=ax[0,1], show=False). ax[1,0].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.X[:,adata.var_names==""CD74""].flatten(), cmap=""magma"", s=0.1). ax[1,1].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.X[:,adata.var_names==""CD74""].flatten(), cmap=""viridis"",. s=0.1, vmin=-0.6, vmax=3.5). ```. ![image](https://user-images.githubusercontent.com/7407663/47044843-45538580-d15f-11e8-8b05-89a1f75d3cee.png). These are the versions I'm using:. scanpy==1.3.2 anndata==0.6.11 numpy==1.14.6 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . My matplotlib version is 3.0.0.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:1677,modifiability,version,version,1677,"Hi, sorry for not giving more of a description of the issue I was having. I tried to recreate a minimal example today using the PBMC_68k dataset and the cmap argument seemed to be working fine when using a gene as the color, but I'm still having problems with categorical variables like louvain clusters or user-defined cluster names. ```. fig, ax = plt.subplots(2,2,figsize=(12,8)). sc.pl.umap(adata, color='louvain', ax = ax[0,0], show=False). sc.pl.umap(adata, color='louvain', ax = ax[0,1], cmap=""tab10"", show=False). ax[1,0].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.obs['louvain'], cmap=""tab10"", s=0.1). ax[1,1].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.obs['louvain'], cmap=""tab20b"", s=0.1). ```. ![image](https://user-images.githubusercontent.com/7407663/47044553-8008ee00-d15e-11e8-8791-65ccb0fc7769.png). ```. fig, ax = plt.subplots(2,2,figsize=(12,8)). sc.pl.umap(adata, color=[""CD74""], ax=ax[0,0], show=False). sc.pl.umap(adata, color=[""CD74""], cmap=""viridis"", ax=ax[0,1], show=False). ax[1,0].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.X[:,adata.var_names==""CD74""].flatten(), cmap=""magma"", s=0.1). ax[1,1].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.X[:,adata.var_names==""CD74""].flatten(), cmap=""viridis"",. s=0.1, vmin=-0.6, vmax=3.5). ```. ![image](https://user-images.githubusercontent.com/7407663/47044843-45538580-d15f-11e8-8b05-89a1f75d3cee.png). These are the versions I'm using:. scanpy==1.3.2 anndata==0.6.11 numpy==1.14.6 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . My matplotlib version is 3.0.0.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:96,usability,minim,minimal,96,"Hi, sorry for not giving more of a description of the issue I was having. I tried to recreate a minimal example today using the PBMC_68k dataset and the cmap argument seemed to be working fine when using a gene as the color, but I'm still having problems with categorical variables like louvain clusters or user-defined cluster names. ```. fig, ax = plt.subplots(2,2,figsize=(12,8)). sc.pl.umap(adata, color='louvain', ax = ax[0,0], show=False). sc.pl.umap(adata, color='louvain', ax = ax[0,1], cmap=""tab10"", show=False). ax[1,0].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.obs['louvain'], cmap=""tab10"", s=0.1). ax[1,1].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.obs['louvain'], cmap=""tab20b"", s=0.1). ```. ![image](https://user-images.githubusercontent.com/7407663/47044553-8008ee00-d15e-11e8-8791-65ccb0fc7769.png). ```. fig, ax = plt.subplots(2,2,figsize=(12,8)). sc.pl.umap(adata, color=[""CD74""], ax=ax[0,0], show=False). sc.pl.umap(adata, color=[""CD74""], cmap=""viridis"", ax=ax[0,1], show=False). ax[1,0].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.X[:,adata.var_names==""CD74""].flatten(), cmap=""magma"", s=0.1). ax[1,1].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.X[:,adata.var_names==""CD74""].flatten(), cmap=""viridis"",. s=0.1, vmin=-0.6, vmax=3.5). ```. ![image](https://user-images.githubusercontent.com/7407663/47044843-45538580-d15f-11e8-8b05-89a1f75d3cee.png). These are the versions I'm using:. scanpy==1.3.2 anndata==0.6.11 numpy==1.14.6 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . My matplotlib version is 3.0.0.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:307,usability,user,user-defined,307,"Hi, sorry for not giving more of a description of the issue I was having. I tried to recreate a minimal example today using the PBMC_68k dataset and the cmap argument seemed to be working fine when using a gene as the color, but I'm still having problems with categorical variables like louvain clusters or user-defined cluster names. ```. fig, ax = plt.subplots(2,2,figsize=(12,8)). sc.pl.umap(adata, color='louvain', ax = ax[0,0], show=False). sc.pl.umap(adata, color='louvain', ax = ax[0,1], cmap=""tab10"", show=False). ax[1,0].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.obs['louvain'], cmap=""tab10"", s=0.1). ax[1,1].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.obs['louvain'], cmap=""tab20b"", s=0.1). ```. ![image](https://user-images.githubusercontent.com/7407663/47044553-8008ee00-d15e-11e8-8791-65ccb0fc7769.png). ```. fig, ax = plt.subplots(2,2,figsize=(12,8)). sc.pl.umap(adata, color=[""CD74""], ax=ax[0,0], show=False). sc.pl.umap(adata, color=[""CD74""], cmap=""viridis"", ax=ax[0,1], show=False). ax[1,0].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.X[:,adata.var_names==""CD74""].flatten(), cmap=""magma"", s=0.1). ax[1,1].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.X[:,adata.var_names==""CD74""].flatten(), cmap=""viridis"",. s=0.1, vmin=-0.6, vmax=3.5). ```. ![image](https://user-images.githubusercontent.com/7407663/47044843-45538580-d15f-11e8-8b05-89a1f75d3cee.png). These are the versions I'm using:. scanpy==1.3.2 anndata==0.6.11 numpy==1.14.6 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . My matplotlib version is 3.0.0.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:779,usability,user,user-images,779,"Hi, sorry for not giving more of a description of the issue I was having. I tried to recreate a minimal example today using the PBMC_68k dataset and the cmap argument seemed to be working fine when using a gene as the color, but I'm still having problems with categorical variables like louvain clusters or user-defined cluster names. ```. fig, ax = plt.subplots(2,2,figsize=(12,8)). sc.pl.umap(adata, color='louvain', ax = ax[0,0], show=False). sc.pl.umap(adata, color='louvain', ax = ax[0,1], cmap=""tab10"", show=False). ax[1,0].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.obs['louvain'], cmap=""tab10"", s=0.1). ax[1,1].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.obs['louvain'], cmap=""tab20b"", s=0.1). ```. ![image](https://user-images.githubusercontent.com/7407663/47044553-8008ee00-d15e-11e8-8791-65ccb0fc7769.png). ```. fig, ax = plt.subplots(2,2,figsize=(12,8)). sc.pl.umap(adata, color=[""CD74""], ax=ax[0,0], show=False). sc.pl.umap(adata, color=[""CD74""], cmap=""viridis"", ax=ax[0,1], show=False). ax[1,0].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.X[:,adata.var_names==""CD74""].flatten(), cmap=""magma"", s=0.1). ax[1,1].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.X[:,adata.var_names==""CD74""].flatten(), cmap=""viridis"",. s=0.1, vmin=-0.6, vmax=3.5). ```. ![image](https://user-images.githubusercontent.com/7407663/47044843-45538580-d15f-11e8-8b05-89a1f75d3cee.png). These are the versions I'm using:. scanpy==1.3.2 anndata==0.6.11 numpy==1.14.6 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . My matplotlib version is 3.0.0.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:1384,usability,user,user-images,1384,"Hi, sorry for not giving more of a description of the issue I was having. I tried to recreate a minimal example today using the PBMC_68k dataset and the cmap argument seemed to be working fine when using a gene as the color, but I'm still having problems with categorical variables like louvain clusters or user-defined cluster names. ```. fig, ax = plt.subplots(2,2,figsize=(12,8)). sc.pl.umap(adata, color='louvain', ax = ax[0,0], show=False). sc.pl.umap(adata, color='louvain', ax = ax[0,1], cmap=""tab10"", show=False). ax[1,0].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.obs['louvain'], cmap=""tab10"", s=0.1). ax[1,1].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.obs['louvain'], cmap=""tab20b"", s=0.1). ```. ![image](https://user-images.githubusercontent.com/7407663/47044553-8008ee00-d15e-11e8-8791-65ccb0fc7769.png). ```. fig, ax = plt.subplots(2,2,figsize=(12,8)). sc.pl.umap(adata, color=[""CD74""], ax=ax[0,0], show=False). sc.pl.umap(adata, color=[""CD74""], cmap=""viridis"", ax=ax[0,1], show=False). ax[1,0].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.X[:,adata.var_names==""CD74""].flatten(), cmap=""magma"", s=0.1). ax[1,1].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.X[:,adata.var_names==""CD74""].flatten(), cmap=""viridis"",. s=0.1, vmin=-0.6, vmax=3.5). ```. ![image](https://user-images.githubusercontent.com/7407663/47044843-45538580-d15f-11e8-8b05-89a1f75d3cee.png). These are the versions I'm using:. scanpy==1.3.2 anndata==0.6.11 numpy==1.14.6 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . My matplotlib version is 3.0.0.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:1592,usability,learn,learn,1592,"Hi, sorry for not giving more of a description of the issue I was having. I tried to recreate a minimal example today using the PBMC_68k dataset and the cmap argument seemed to be working fine when using a gene as the color, but I'm still having problems with categorical variables like louvain clusters or user-defined cluster names. ```. fig, ax = plt.subplots(2,2,figsize=(12,8)). sc.pl.umap(adata, color='louvain', ax = ax[0,0], show=False). sc.pl.umap(adata, color='louvain', ax = ax[0,1], cmap=""tab10"", show=False). ax[1,0].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.obs['louvain'], cmap=""tab10"", s=0.1). ax[1,1].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.obs['louvain'], cmap=""tab20b"", s=0.1). ```. ![image](https://user-images.githubusercontent.com/7407663/47044553-8008ee00-d15e-11e8-8791-65ccb0fc7769.png). ```. fig, ax = plt.subplots(2,2,figsize=(12,8)). sc.pl.umap(adata, color=[""CD74""], ax=ax[0,0], show=False). sc.pl.umap(adata, color=[""CD74""], cmap=""viridis"", ax=ax[0,1], show=False). ax[1,0].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.X[:,adata.var_names==""CD74""].flatten(), cmap=""magma"", s=0.1). ax[1,1].scatter(adata.obsm['X_umap'][:,0], adata.obsm['X_umap'][:,1],. c=adata.X[:,adata.var_names==""CD74""].flatten(), cmap=""viridis"",. s=0.1, vmin=-0.6, vmax=3.5). ```. ![image](https://user-images.githubusercontent.com/7407663/47044843-45538580-d15f-11e8-8b05-89a1f75d3cee.png). These are the versions I'm using:. scanpy==1.3.2 anndata==0.6.11 numpy==1.14.6 scipy==1.1.0 pandas==0.23.4 scikit-learn==0.20.0 statsmodels==0.9.0 python-igraph==0.7.1 louvain==0.6.1 . My matplotlib version is 3.0.0.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:296,deployability,continu,continuous,296,"Can you try using the `palette` argument? After 1.3.1, Scanpy's plotting underwent quite some fundamental changes due to @fidelram. The code base improved a lot, there might be a few small issues, though. I had both `cmap` and `palette` as argument as I wanted users to choose a default for both continuous and categorical annotation. So if someone passes a `cmap` this only affects the continuous annotation, but for categoricals the `rcParams` default is used. Does this make sense? It might stop making sense when you provide lists to `cmap` and/or `palette`; in order to plot two different categoricals with two different palettes (which should be the default behavior at some point). Happy to discuss, whether we should depricate the `palette` argument and have the default access via `cmap`, that can be provided as a list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:387,deployability,continu,continuous,387,"Can you try using the `palette` argument? After 1.3.1, Scanpy's plotting underwent quite some fundamental changes due to @fidelram. The code base improved a lot, there might be a few small issues, though. I had both `cmap` and `palette` as argument as I wanted users to choose a default for both continuous and categorical annotation. So if someone passes a `cmap` this only affects the continuous annotation, but for categoricals the `rcParams` default is used. Does this make sense? It might stop making sense when you provide lists to `cmap` and/or `palette`; in order to plot two different categoricals with two different palettes (which should be the default behavior at some point). Happy to discuss, whether we should depricate the `palette` argument and have the default access via `cmap`, that can be provided as a list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:463,reliability,Doe,Does,463,"Can you try using the `palette` argument? After 1.3.1, Scanpy's plotting underwent quite some fundamental changes due to @fidelram. The code base improved a lot, there might be a few small issues, though. I had both `cmap` and `palette` as argument as I wanted users to choose a default for both continuous and categorical annotation. So if someone passes a `cmap` this only affects the continuous annotation, but for categoricals the `rcParams` default is used. Does this make sense? It might stop making sense when you provide lists to `cmap` and/or `palette`; in order to plot two different categoricals with two different palettes (which should be the default behavior at some point). Happy to discuss, whether we should depricate the `palette` argument and have the default access via `cmap`, that can be provided as a list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:779,security,access,access,779,"Can you try using the `palette` argument? After 1.3.1, Scanpy's plotting underwent quite some fundamental changes due to @fidelram. The code base improved a lot, there might be a few small issues, though. I had both `cmap` and `palette` as argument as I wanted users to choose a default for both continuous and categorical annotation. So if someone passes a `cmap` this only affects the continuous annotation, but for categoricals the `rcParams` default is used. Does this make sense? It might stop making sense when you provide lists to `cmap` and/or `palette`; in order to plot two different categoricals with two different palettes (which should be the default behavior at some point). Happy to discuss, whether we should depricate the `palette` argument and have the default access via `cmap`, that can be provided as a list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:261,usability,user,users,261,"Can you try using the `palette` argument? After 1.3.1, Scanpy's plotting underwent quite some fundamental changes due to @fidelram. The code base improved a lot, there might be a few small issues, though. I had both `cmap` and `palette` as argument as I wanted users to choose a default for both continuous and categorical annotation. So if someone passes a `cmap` this only affects the continuous annotation, but for categoricals the `rcParams` default is used. Does this make sense? It might stop making sense when you provide lists to `cmap` and/or `palette`; in order to plot two different categoricals with two different palettes (which should be the default behavior at some point). Happy to discuss, whether we should depricate the `palette` argument and have the default access via `cmap`, that can be provided as a list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:494,usability,stop,stop,494,"Can you try using the `palette` argument? After 1.3.1, Scanpy's plotting underwent quite some fundamental changes due to @fidelram. The code base improved a lot, there might be a few small issues, though. I had both `cmap` and `palette` as argument as I wanted users to choose a default for both continuous and categorical annotation. So if someone passes a `cmap` this only affects the continuous annotation, but for categoricals the `rcParams` default is used. Does this make sense? It might stop making sense when you provide lists to `cmap` and/or `palette`; in order to plot two different categoricals with two different palettes (which should be the default behavior at some point). Happy to discuss, whether we should depricate the `palette` argument and have the default access via `cmap`, that can be provided as a list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:664,usability,behavi,behavior,664,"Can you try using the `palette` argument? After 1.3.1, Scanpy's plotting underwent quite some fundamental changes due to @fidelram. The code base improved a lot, there might be a few small issues, though. I had both `cmap` and `palette` as argument as I wanted users to choose a default for both continuous and categorical annotation. So if someone passes a `cmap` this only affects the continuous annotation, but for categoricals the `rcParams` default is used. Does this make sense? It might stop making sense when you provide lists to `cmap` and/or `palette`; in order to plot two different categoricals with two different palettes (which should be the default behavior at some point). Happy to discuss, whether we should depricate the `palette` argument and have the default access via `cmap`, that can be provided as a list.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:25,availability,error,error,25,"I'm also seeing the same error when using `sc.ppl.scatter`:. `sc.pl.scatter(adata, color='louvain', basis=""umap"", palette=""tab20"")`. ![image](https://user-images.githubusercontent.com/7407663/47046149-80a38380-d162-11e8-9865-6548c7e9454d.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:25,performance,error,error,25,"I'm also seeing the same error when using `sc.ppl.scatter`:. `sc.pl.scatter(adata, color='louvain', basis=""umap"", palette=""tab20"")`. ![image](https://user-images.githubusercontent.com/7407663/47046149-80a38380-d162-11e8-9865-6548c7e9454d.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:25,safety,error,error,25,"I'm also seeing the same error when using `sc.ppl.scatter`:. `sc.pl.scatter(adata, color='louvain', basis=""umap"", palette=""tab20"")`. ![image](https://user-images.githubusercontent.com/7407663/47046149-80a38380-d162-11e8-9865-6548c7e9454d.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:25,usability,error,error,25,"I'm also seeing the same error when using `sc.ppl.scatter`:. `sc.pl.scatter(adata, color='louvain', basis=""umap"", palette=""tab20"")`. ![image](https://user-images.githubusercontent.com/7407663/47046149-80a38380-d162-11e8-9865-6548c7e9454d.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:150,usability,user,user-images,150,"I'm also seeing the same error when using `sc.ppl.scatter`:. `sc.pl.scatter(adata, color='louvain', basis=""umap"", palette=""tab20"")`. ![image](https://user-images.githubusercontent.com/7407663/47046149-80a38380-d162-11e8-9865-6548c7e9454d.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:10,availability,replic,replicate,10,I can not replicate the problem. Maybe a lot of clusters are needed to replicate it? ![image](https://user-images.githubusercontent.com/4964309/47082937-87d49b00-d20f-11e8-8725-848cc2ac627a.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:48,availability,cluster,clusters,48,I can not replicate the problem. Maybe a lot of clusters are needed to replicate it? ![image](https://user-images.githubusercontent.com/4964309/47082937-87d49b00-d20f-11e8-8725-848cc2ac627a.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:71,availability,replic,replicate,71,I can not replicate the problem. Maybe a lot of clusters are needed to replicate it? ![image](https://user-images.githubusercontent.com/4964309/47082937-87d49b00-d20f-11e8-8725-848cc2ac627a.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:48,deployability,cluster,clusters,48,I can not replicate the problem. Maybe a lot of clusters are needed to replicate it? ![image](https://user-images.githubusercontent.com/4964309/47082937-87d49b00-d20f-11e8-8725-848cc2ac627a.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:102,usability,user,user-images,102,I can not replicate the problem. Maybe a lot of clusters are needed to replicate it? ![image](https://user-images.githubusercontent.com/4964309/47082937-87d49b00-d20f-11e8-8725-848cc2ac627a.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:11,deployability,releas,release,11,"Maybe it's release 1.3.2 - I wouldn't have made that release if I hadn't been asked to, I expected the new plotting backend to still have several bugs. The current master has several fixes. Do you think we should move forward with another release, @fidelram, @ivirshup; or are there still a few striking bugs in the scatter plots that I'm not aware of? It seems like a lot has been fixed in the past week.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:53,deployability,releas,release,53,"Maybe it's release 1.3.2 - I wouldn't have made that release if I hadn't been asked to, I expected the new plotting backend to still have several bugs. The current master has several fixes. Do you think we should move forward with another release, @fidelram, @ivirshup; or are there still a few striking bugs in the scatter plots that I'm not aware of? It seems like a lot has been fixed in the past week.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:239,deployability,releas,release,239,"Maybe it's release 1.3.2 - I wouldn't have made that release if I hadn't been asked to, I expected the new plotting backend to still have several bugs. The current master has several fixes. Do you think we should move forward with another release, @fidelram, @ivirshup; or are there still a few striking bugs in the scatter plots that I'm not aware of? It seems like a lot has been fixed in the past week.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:156,energy efficiency,current,current,156,"Maybe it's release 1.3.2 - I wouldn't have made that release if I hadn't been asked to, I expected the new plotting backend to still have several bugs. The current master has several fixes. Do you think we should move forward with another release, @fidelram, @ivirshup; or are there still a few striking bugs in the scatter plots that I'm not aware of? It seems like a lot has been fixed in the past week.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:84,deployability,observ,observations,84,I was experiencing the same issues and what worked for me in v 1.4.3 is casting the observations as categorical. adata.obs['sample'] = adata.obs['sample'].astype('category'). In previous versions of scanpy I had a bunch of warnings when I was saving as an h5ad with non-categorical data and now as I can see problems with the plotting. .,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:187,deployability,version,versions,187,I was experiencing the same issues and what worked for me in v 1.4.3 is casting the observations as categorical. adata.obs['sample'] = adata.obs['sample'].astype('category'). In previous versions of scanpy I had a bunch of warnings when I was saving as an h5ad with non-categorical data and now as I can see problems with the plotting. .,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:187,integrability,version,versions,187,I was experiencing the same issues and what worked for me in v 1.4.3 is casting the observations as categorical. adata.obs['sample'] = adata.obs['sample'].astype('category'). In previous versions of scanpy I had a bunch of warnings when I was saving as an h5ad with non-categorical data and now as I can see problems with the plotting. .,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:187,modifiability,version,versions,187,I was experiencing the same issues and what worked for me in v 1.4.3 is casting the observations as categorical. adata.obs['sample'] = adata.obs['sample'].astype('category'). In previous versions of scanpy I had a bunch of warnings when I was saving as an h5ad with non-categorical data and now as I can see problems with the plotting. .,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:84,testability,observ,observations,84,I was experiencing the same issues and what worked for me in v 1.4.3 is casting the observations as categorical. adata.obs['sample'] = adata.obs['sample'].astype('category'). In previous versions of scanpy I had a bunch of warnings when I was saving as an h5ad with non-categorical data and now as I can see problems with the plotting. .,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/286:6,usability,experien,experiencing,6,I was experiencing the same issues and what worked for me in v 1.4.3 is casting the observations as categorical. adata.obs['sample'] = adata.obs['sample'].astype('category'). In previous versions of scanpy I had a bunch of warnings when I was saving as an h5ad with non-categorical data and now as I can see problems with the plotting. .,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/286
https://github.com/scverse/scanpy/issues/287:49,deployability,api,api,49,"Yes, see https://scanpy.readthedocs.io/en/latest/api/scanpy.api.set_figure_params.html#scanpy.api.set_figure_params.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/287
https://github.com/scverse/scanpy/issues/287:60,deployability,api,api,60,"Yes, see https://scanpy.readthedocs.io/en/latest/api/scanpy.api.set_figure_params.html#scanpy.api.set_figure_params.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/287
https://github.com/scverse/scanpy/issues/287:94,deployability,api,api,94,"Yes, see https://scanpy.readthedocs.io/en/latest/api/scanpy.api.set_figure_params.html#scanpy.api.set_figure_params.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/287
https://github.com/scverse/scanpy/issues/287:49,integrability,api,api,49,"Yes, see https://scanpy.readthedocs.io/en/latest/api/scanpy.api.set_figure_params.html#scanpy.api.set_figure_params.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/287
https://github.com/scverse/scanpy/issues/287:60,integrability,api,api,60,"Yes, see https://scanpy.readthedocs.io/en/latest/api/scanpy.api.set_figure_params.html#scanpy.api.set_figure_params.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/287
https://github.com/scverse/scanpy/issues/287:94,integrability,api,api,94,"Yes, see https://scanpy.readthedocs.io/en/latest/api/scanpy.api.set_figure_params.html#scanpy.api.set_figure_params.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/287
https://github.com/scverse/scanpy/issues/287:49,interoperability,api,api,49,"Yes, see https://scanpy.readthedocs.io/en/latest/api/scanpy.api.set_figure_params.html#scanpy.api.set_figure_params.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/287
https://github.com/scverse/scanpy/issues/287:60,interoperability,api,api,60,"Yes, see https://scanpy.readthedocs.io/en/latest/api/scanpy.api.set_figure_params.html#scanpy.api.set_figure_params.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/287
https://github.com/scverse/scanpy/issues/287:94,interoperability,api,api,94,"Yes, see https://scanpy.readthedocs.io/en/latest/api/scanpy.api.set_figure_params.html#scanpy.api.set_figure_params.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/287
https://github.com/scverse/scanpy/issues/287:51,deployability,api,api,51,"> Yes, see https://scanpy.readthedocs.io/en/latest/api/scanpy.api.set_figure_params.html#scanpy.api.set_figure_params. The link is broken",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/287
https://github.com/scverse/scanpy/issues/287:62,deployability,api,api,62,"> Yes, see https://scanpy.readthedocs.io/en/latest/api/scanpy.api.set_figure_params.html#scanpy.api.set_figure_params. The link is broken",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/287
https://github.com/scverse/scanpy/issues/287:96,deployability,api,api,96,"> Yes, see https://scanpy.readthedocs.io/en/latest/api/scanpy.api.set_figure_params.html#scanpy.api.set_figure_params. The link is broken",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/287
https://github.com/scverse/scanpy/issues/287:51,integrability,api,api,51,"> Yes, see https://scanpy.readthedocs.io/en/latest/api/scanpy.api.set_figure_params.html#scanpy.api.set_figure_params. The link is broken",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/287
https://github.com/scverse/scanpy/issues/287:62,integrability,api,api,62,"> Yes, see https://scanpy.readthedocs.io/en/latest/api/scanpy.api.set_figure_params.html#scanpy.api.set_figure_params. The link is broken",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/287
https://github.com/scverse/scanpy/issues/287:96,integrability,api,api,96,"> Yes, see https://scanpy.readthedocs.io/en/latest/api/scanpy.api.set_figure_params.html#scanpy.api.set_figure_params. The link is broken",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/287
https://github.com/scverse/scanpy/issues/287:51,interoperability,api,api,51,"> Yes, see https://scanpy.readthedocs.io/en/latest/api/scanpy.api.set_figure_params.html#scanpy.api.set_figure_params. The link is broken",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/287
https://github.com/scverse/scanpy/issues/287:62,interoperability,api,api,62,"> Yes, see https://scanpy.readthedocs.io/en/latest/api/scanpy.api.set_figure_params.html#scanpy.api.set_figure_params. The link is broken",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/287
https://github.com/scverse/scanpy/issues/287:96,interoperability,api,api,96,"> Yes, see https://scanpy.readthedocs.io/en/latest/api/scanpy.api.set_figure_params.html#scanpy.api.set_figure_params. The link is broken",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/287
https://github.com/scverse/scanpy/issues/287:68,deployability,api,api,68,Suggest this one is correct https://scanpy.readthedocs.io/en/stable/api/scanpy.set_figure_params.html,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/287
https://github.com/scverse/scanpy/issues/287:68,integrability,api,api,68,Suggest this one is correct https://scanpy.readthedocs.io/en/stable/api/scanpy.set_figure_params.html,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/287
https://github.com/scverse/scanpy/issues/287:68,interoperability,api,api,68,Suggest this one is correct https://scanpy.readthedocs.io/en/stable/api/scanpy.set_figure_params.html,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/287
https://github.com/scverse/scanpy/pull/289:69,interoperability,standard,standard,69,"Just a quick comment... Benjamini-Hochberg correction is usually the standard for multiple-testing correction in differential expression testing. Not sure if you want to take it into account, but I thought I should mention it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289
https://github.com/scverse/scanpy/pull/289:91,safety,test,testing,91,"Just a quick comment... Benjamini-Hochberg correction is usually the standard for multiple-testing correction in differential expression testing. Not sure if you want to take it into account, but I thought I should mention it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289
https://github.com/scverse/scanpy/pull/289:137,safety,test,testing,137,"Just a quick comment... Benjamini-Hochberg correction is usually the standard for multiple-testing correction in differential expression testing. Not sure if you want to take it into account, but I thought I should mention it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289
https://github.com/scverse/scanpy/pull/289:91,testability,test,testing,91,"Just a quick comment... Benjamini-Hochberg correction is usually the standard for multiple-testing correction in differential expression testing. Not sure if you want to take it into account, but I thought I should mention it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289
https://github.com/scverse/scanpy/pull/289:137,testability,test,testing,137,"Just a quick comment... Benjamini-Hochberg correction is usually the standard for multiple-testing correction in differential expression testing. Not sure if you want to take it into account, but I thought I should mention it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289
https://github.com/scverse/scanpy/pull/289:109,energy efficiency,power,power,109,"That's a good point, @LuckyMD. I chose Bonferroni to have a more stringent correction (albeit with a loss in power), particularly due to the increased power inherent in the large sample sizes of single cell data. I might be wrong, but I think the Benjamini-Hochberg standard was established with bulk RNAseq, where limited sample sizes required an approach with more power. . However, I'm happy to change it to Benjamini-Hochberg if that's the consensus! It's a simple one-liner - we can even provide both and let the user choose by passing a parameter. Whatever is preferred!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289
https://github.com/scverse/scanpy/pull/289:151,energy efficiency,power,power,151,"That's a good point, @LuckyMD. I chose Bonferroni to have a more stringent correction (albeit with a loss in power), particularly due to the increased power inherent in the large sample sizes of single cell data. I might be wrong, but I think the Benjamini-Hochberg standard was established with bulk RNAseq, where limited sample sizes required an approach with more power. . However, I'm happy to change it to Benjamini-Hochberg if that's the consensus! It's a simple one-liner - we can even provide both and let the user choose by passing a parameter. Whatever is preferred!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289
https://github.com/scverse/scanpy/pull/289:367,energy efficiency,power,power,367,"That's a good point, @LuckyMD. I chose Bonferroni to have a more stringent correction (albeit with a loss in power), particularly due to the increased power inherent in the large sample sizes of single cell data. I might be wrong, but I think the Benjamini-Hochberg standard was established with bulk RNAseq, where limited sample sizes required an approach with more power. . However, I'm happy to change it to Benjamini-Hochberg if that's the consensus! It's a simple one-liner - we can even provide both and let the user choose by passing a parameter. Whatever is preferred!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289
https://github.com/scverse/scanpy/pull/289:266,interoperability,standard,standard,266,"That's a good point, @LuckyMD. I chose Bonferroni to have a more stringent correction (albeit with a loss in power), particularly due to the increased power inherent in the large sample sizes of single cell data. I might be wrong, but I think the Benjamini-Hochberg standard was established with bulk RNAseq, where limited sample sizes required an approach with more power. . However, I'm happy to change it to Benjamini-Hochberg if that's the consensus! It's a simple one-liner - we can even provide both and let the user choose by passing a parameter. Whatever is preferred!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289
https://github.com/scverse/scanpy/pull/289:543,modifiability,paramet,parameter,543,"That's a good point, @LuckyMD. I chose Bonferroni to have a more stringent correction (albeit with a loss in power), particularly due to the increased power inherent in the large sample sizes of single cell data. I might be wrong, but I think the Benjamini-Hochberg standard was established with bulk RNAseq, where limited sample sizes required an approach with more power. . However, I'm happy to change it to Benjamini-Hochberg if that's the consensus! It's a simple one-liner - we can even provide both and let the user choose by passing a parameter. Whatever is preferred!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289
https://github.com/scverse/scanpy/pull/289:101,security,loss,loss,101,"That's a good point, @LuckyMD. I chose Bonferroni to have a more stringent correction (albeit with a loss in power), particularly due to the increased power inherent in the large sample sizes of single cell data. I might be wrong, but I think the Benjamini-Hochberg standard was established with bulk RNAseq, where limited sample sizes required an approach with more power. . However, I'm happy to change it to Benjamini-Hochberg if that's the consensus! It's a simple one-liner - we can even provide both and let the user choose by passing a parameter. Whatever is preferred!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289
https://github.com/scverse/scanpy/pull/289:462,testability,simpl,simple,462,"That's a good point, @LuckyMD. I chose Bonferroni to have a more stringent correction (albeit with a loss in power), particularly due to the increased power inherent in the large sample sizes of single cell data. I might be wrong, but I think the Benjamini-Hochberg standard was established with bulk RNAseq, where limited sample sizes required an approach with more power. . However, I'm happy to change it to Benjamini-Hochberg if that's the consensus! It's a simple one-liner - we can even provide both and let the user choose by passing a parameter. Whatever is preferred!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289
https://github.com/scverse/scanpy/pull/289:462,usability,simpl,simple,462,"That's a good point, @LuckyMD. I chose Bonferroni to have a more stringent correction (albeit with a loss in power), particularly due to the increased power inherent in the large sample sizes of single cell data. I might be wrong, but I think the Benjamini-Hochberg standard was established with bulk RNAseq, where limited sample sizes required an approach with more power. . However, I'm happy to change it to Benjamini-Hochberg if that's the consensus! It's a simple one-liner - we can even provide both and let the user choose by passing a parameter. Whatever is preferred!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289
https://github.com/scverse/scanpy/pull/289:518,usability,user,user,518,"That's a good point, @LuckyMD. I chose Bonferroni to have a more stringent correction (albeit with a loss in power), particularly due to the increased power inherent in the large sample sizes of single cell data. I might be wrong, but I think the Benjamini-Hochberg standard was established with bulk RNAseq, where limited sample sizes required an approach with more power. . However, I'm happy to change it to Benjamini-Hochberg if that's the consensus! It's a simple one-liner - we can even provide both and let the user choose by passing a parameter. Whatever is preferred!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289
https://github.com/scverse/scanpy/pull/289:566,usability,prefer,preferred,566,"That's a good point, @LuckyMD. I chose Bonferroni to have a more stringent correction (albeit with a loss in power), particularly due to the increased power inherent in the large sample sizes of single cell data. I might be wrong, but I think the Benjamini-Hochberg standard was established with bulk RNAseq, where limited sample sizes required an approach with more power. . However, I'm happy to change it to Benjamini-Hochberg if that's the consensus! It's a simple one-liner - we can even provide both and let the user choose by passing a parameter. Whatever is preferred!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289
https://github.com/scverse/scanpy/pull/289:175,availability,error,error,175,"I definitely don't define the consensus, but I normally prefer FDR correction. It makes a bit more sense to me to correct for a false discovery rate, rather than a test-based error, if you are only interested in the rejected null hypotheses. . They also test for FDR control in a [recent comparison of differential testing methods](http://www.nature.com/doifinder/10.1038/nmeth.4612).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289
https://github.com/scverse/scanpy/pull/289:134,integrability,discover,discovery,134,"I definitely don't define the consensus, but I normally prefer FDR correction. It makes a bit more sense to me to correct for a false discovery rate, rather than a test-based error, if you are only interested in the rejected null hypotheses. . They also test for FDR control in a [recent comparison of differential testing methods](http://www.nature.com/doifinder/10.1038/nmeth.4612).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289
https://github.com/scverse/scanpy/pull/289:134,interoperability,discover,discovery,134,"I definitely don't define the consensus, but I normally prefer FDR correction. It makes a bit more sense to me to correct for a false discovery rate, rather than a test-based error, if you are only interested in the rejected null hypotheses. . They also test for FDR control in a [recent comparison of differential testing methods](http://www.nature.com/doifinder/10.1038/nmeth.4612).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289
https://github.com/scverse/scanpy/pull/289:175,performance,error,error,175,"I definitely don't define the consensus, but I normally prefer FDR correction. It makes a bit more sense to me to correct for a false discovery rate, rather than a test-based error, if you are only interested in the rejected null hypotheses. . They also test for FDR control in a [recent comparison of differential testing methods](http://www.nature.com/doifinder/10.1038/nmeth.4612).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289
https://github.com/scverse/scanpy/pull/289:164,safety,test,test-based,164,"I definitely don't define the consensus, but I normally prefer FDR correction. It makes a bit more sense to me to correct for a false discovery rate, rather than a test-based error, if you are only interested in the rejected null hypotheses. . They also test for FDR control in a [recent comparison of differential testing methods](http://www.nature.com/doifinder/10.1038/nmeth.4612).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289
https://github.com/scverse/scanpy/pull/289:175,safety,error,error,175,"I definitely don't define the consensus, but I normally prefer FDR correction. It makes a bit more sense to me to correct for a false discovery rate, rather than a test-based error, if you are only interested in the rejected null hypotheses. . They also test for FDR control in a [recent comparison of differential testing methods](http://www.nature.com/doifinder/10.1038/nmeth.4612).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289
https://github.com/scverse/scanpy/pull/289:254,safety,test,test,254,"I definitely don't define the consensus, but I normally prefer FDR correction. It makes a bit more sense to me to correct for a false discovery rate, rather than a test-based error, if you are only interested in the rejected null hypotheses. . They also test for FDR control in a [recent comparison of differential testing methods](http://www.nature.com/doifinder/10.1038/nmeth.4612).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289
https://github.com/scverse/scanpy/pull/289:315,safety,test,testing,315,"I definitely don't define the consensus, but I normally prefer FDR correction. It makes a bit more sense to me to correct for a false discovery rate, rather than a test-based error, if you are only interested in the rejected null hypotheses. . They also test for FDR control in a [recent comparison of differential testing methods](http://www.nature.com/doifinder/10.1038/nmeth.4612).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289
https://github.com/scverse/scanpy/pull/289:267,security,control,control,267,"I definitely don't define the consensus, but I normally prefer FDR correction. It makes a bit more sense to me to correct for a false discovery rate, rather than a test-based error, if you are only interested in the rejected null hypotheses. . They also test for FDR control in a [recent comparison of differential testing methods](http://www.nature.com/doifinder/10.1038/nmeth.4612).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289
https://github.com/scverse/scanpy/pull/289:164,testability,test,test-based,164,"I definitely don't define the consensus, but I normally prefer FDR correction. It makes a bit more sense to me to correct for a false discovery rate, rather than a test-based error, if you are only interested in the rejected null hypotheses. . They also test for FDR control in a [recent comparison of differential testing methods](http://www.nature.com/doifinder/10.1038/nmeth.4612).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289
https://github.com/scverse/scanpy/pull/289:254,testability,test,test,254,"I definitely don't define the consensus, but I normally prefer FDR correction. It makes a bit more sense to me to correct for a false discovery rate, rather than a test-based error, if you are only interested in the rejected null hypotheses. . They also test for FDR control in a [recent comparison of differential testing methods](http://www.nature.com/doifinder/10.1038/nmeth.4612).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289
https://github.com/scverse/scanpy/pull/289:267,testability,control,control,267,"I definitely don't define the consensus, but I normally prefer FDR correction. It makes a bit more sense to me to correct for a false discovery rate, rather than a test-based error, if you are only interested in the rejected null hypotheses. . They also test for FDR control in a [recent comparison of differential testing methods](http://www.nature.com/doifinder/10.1038/nmeth.4612).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289
https://github.com/scverse/scanpy/pull/289:315,testability,test,testing,315,"I definitely don't define the consensus, but I normally prefer FDR correction. It makes a bit more sense to me to correct for a false discovery rate, rather than a test-based error, if you are only interested in the rejected null hypotheses. . They also test for FDR control in a [recent comparison of differential testing methods](http://www.nature.com/doifinder/10.1038/nmeth.4612).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289
https://github.com/scverse/scanpy/pull/289:56,usability,prefer,prefer,56,"I definitely don't define the consensus, but I normally prefer FDR correction. It makes a bit more sense to me to correct for a false discovery rate, rather than a test-based error, if you are only interested in the rejected null hypotheses. . They also test for FDR control in a [recent comparison of differential testing methods](http://www.nature.com/doifinder/10.1038/nmeth.4612).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289
https://github.com/scverse/scanpy/pull/289:134,usability,discov,discovery,134,"I definitely don't define the consensus, but I normally prefer FDR correction. It makes a bit more sense to me to correct for a false discovery rate, rather than a test-based error, if you are only interested in the rejected null hypotheses. . They also test for FDR control in a [recent comparison of differential testing methods](http://www.nature.com/doifinder/10.1038/nmeth.4612).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289
https://github.com/scverse/scanpy/pull/289:175,usability,error,error,175,"I definitely don't define the consensus, but I normally prefer FDR correction. It makes a bit more sense to me to correct for a false discovery rate, rather than a test-based error, if you are only interested in the rejected null hypotheses. . They also test for FDR control in a [recent comparison of differential testing methods](http://www.nature.com/doifinder/10.1038/nmeth.4612).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289
https://github.com/scverse/scanpy/pull/289:435,availability,down,downstream,435,"That's a good point! I just added the Benjamini-Hochberg correction to the Wilcoxon code (as well as to the t-tests) and left that as the default. I left the Bonferroni code in there and marked the place with comments. If and when the pull request is complete, they can choose what to keep and remove the extra comments. Either way, since the uncorrected p-values are also outputted, the user can choose whichever correction they want downstream.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289
https://github.com/scverse/scanpy/pull/289:110,safety,test,tests,110,"That's a good point! I just added the Benjamini-Hochberg correction to the Wilcoxon code (as well as to the t-tests) and left that as the default. I left the Bonferroni code in there and marked the place with comments. If and when the pull request is complete, they can choose what to keep and remove the extra comments. Either way, since the uncorrected p-values are also outputted, the user can choose whichever correction they want downstream.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289
https://github.com/scverse/scanpy/pull/289:251,safety,compl,complete,251,"That's a good point! I just added the Benjamini-Hochberg correction to the Wilcoxon code (as well as to the t-tests) and left that as the default. I left the Bonferroni code in there and marked the place with comments. If and when the pull request is complete, they can choose what to keep and remove the extra comments. Either way, since the uncorrected p-values are also outputted, the user can choose whichever correction they want downstream.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289
https://github.com/scverse/scanpy/pull/289:251,security,compl,complete,251,"That's a good point! I just added the Benjamini-Hochberg correction to the Wilcoxon code (as well as to the t-tests) and left that as the default. I left the Bonferroni code in there and marked the place with comments. If and when the pull request is complete, they can choose what to keep and remove the extra comments. Either way, since the uncorrected p-values are also outputted, the user can choose whichever correction they want downstream.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289
https://github.com/scverse/scanpy/pull/289:110,testability,test,tests,110,"That's a good point! I just added the Benjamini-Hochberg correction to the Wilcoxon code (as well as to the t-tests) and left that as the default. I left the Bonferroni code in there and marked the place with comments. If and when the pull request is complete, they can choose what to keep and remove the extra comments. Either way, since the uncorrected p-values are also outputted, the user can choose whichever correction they want downstream.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289
https://github.com/scverse/scanpy/pull/289:388,usability,user,user,388,"That's a good point! I just added the Benjamini-Hochberg correction to the Wilcoxon code (as well as to the t-tests) and left that as the default. I left the Bonferroni code in there and marked the place with comments. If and when the pull request is complete, they can choose what to keep and remove the extra comments. Either way, since the uncorrected p-values are also outputted, the user can choose whichever correction they want downstream.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289
https://github.com/scverse/scanpy/pull/289:414,deployability,modul,module,414,"Looks very good to me, thank you very much! Would you mind adding an option to select for the correction type that defaults to 'benjamini-hochberg' and can be set to 'bonferroni'? In the best of all world's, you'd also extend the tests for rank_genes_groups so that the p values are tested and not messed up by pull requests in the future. We want people to get the same p values again and again. And as the whole module sort of involves a lot of custom code as the scipy alternatives are not there for mult-dimensional and sparse data, it's easy to mess this up in the future. Thank you so much for the awesome addition @a-munoz-rojas , I'll add you both to the Scanpy author list and to the release notes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289
https://github.com/scverse/scanpy/pull/289:693,deployability,releas,release,693,"Looks very good to me, thank you very much! Would you mind adding an option to select for the correction type that defaults to 'benjamini-hochberg' and can be set to 'bonferroni'? In the best of all world's, you'd also extend the tests for rank_genes_groups so that the p values are tested and not messed up by pull requests in the future. We want people to get the same p values again and again. And as the whole module sort of involves a lot of custom code as the scipy alternatives are not there for mult-dimensional and sparse data, it's easy to mess this up in the future. Thank you so much for the awesome addition @a-munoz-rojas , I'll add you both to the Scanpy author list and to the release notes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289
https://github.com/scverse/scanpy/pull/289:219,modifiability,exten,extend,219,"Looks very good to me, thank you very much! Would you mind adding an option to select for the correction type that defaults to 'benjamini-hochberg' and can be set to 'bonferroni'? In the best of all world's, you'd also extend the tests for rank_genes_groups so that the p values are tested and not messed up by pull requests in the future. We want people to get the same p values again and again. And as the whole module sort of involves a lot of custom code as the scipy alternatives are not there for mult-dimensional and sparse data, it's easy to mess this up in the future. Thank you so much for the awesome addition @a-munoz-rojas , I'll add you both to the Scanpy author list and to the release notes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289
https://github.com/scverse/scanpy/pull/289:414,modifiability,modul,module,414,"Looks very good to me, thank you very much! Would you mind adding an option to select for the correction type that defaults to 'benjamini-hochberg' and can be set to 'bonferroni'? In the best of all world's, you'd also extend the tests for rank_genes_groups so that the p values are tested and not messed up by pull requests in the future. We want people to get the same p values again and again. And as the whole module sort of involves a lot of custom code as the scipy alternatives are not there for mult-dimensional and sparse data, it's easy to mess this up in the future. Thank you so much for the awesome addition @a-munoz-rojas , I'll add you both to the Scanpy author list and to the release notes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289
https://github.com/scverse/scanpy/pull/289:230,safety,test,tests,230,"Looks very good to me, thank you very much! Would you mind adding an option to select for the correction type that defaults to 'benjamini-hochberg' and can be set to 'bonferroni'? In the best of all world's, you'd also extend the tests for rank_genes_groups so that the p values are tested and not messed up by pull requests in the future. We want people to get the same p values again and again. And as the whole module sort of involves a lot of custom code as the scipy alternatives are not there for mult-dimensional and sparse data, it's easy to mess this up in the future. Thank you so much for the awesome addition @a-munoz-rojas , I'll add you both to the Scanpy author list and to the release notes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289
https://github.com/scverse/scanpy/pull/289:283,safety,test,tested,283,"Looks very good to me, thank you very much! Would you mind adding an option to select for the correction type that defaults to 'benjamini-hochberg' and can be set to 'bonferroni'? In the best of all world's, you'd also extend the tests for rank_genes_groups so that the p values are tested and not messed up by pull requests in the future. We want people to get the same p values again and again. And as the whole module sort of involves a lot of custom code as the scipy alternatives are not there for mult-dimensional and sparse data, it's easy to mess this up in the future. Thank you so much for the awesome addition @a-munoz-rojas , I'll add you both to the Scanpy author list and to the release notes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289
https://github.com/scverse/scanpy/pull/289:414,safety,modul,module,414,"Looks very good to me, thank you very much! Would you mind adding an option to select for the correction type that defaults to 'benjamini-hochberg' and can be set to 'bonferroni'? In the best of all world's, you'd also extend the tests for rank_genes_groups so that the p values are tested and not messed up by pull requests in the future. We want people to get the same p values again and again. And as the whole module sort of involves a lot of custom code as the scipy alternatives are not there for mult-dimensional and sparse data, it's easy to mess this up in the future. Thank you so much for the awesome addition @a-munoz-rojas , I'll add you both to the Scanpy author list and to the release notes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289
https://github.com/scverse/scanpy/pull/289:670,security,auth,author,670,"Looks very good to me, thank you very much! Would you mind adding an option to select for the correction type that defaults to 'benjamini-hochberg' and can be set to 'bonferroni'? In the best of all world's, you'd also extend the tests for rank_genes_groups so that the p values are tested and not messed up by pull requests in the future. We want people to get the same p values again and again. And as the whole module sort of involves a lot of custom code as the scipy alternatives are not there for mult-dimensional and sparse data, it's easy to mess this up in the future. Thank you so much for the awesome addition @a-munoz-rojas , I'll add you both to the Scanpy author list and to the release notes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289
https://github.com/scverse/scanpy/pull/289:230,testability,test,tests,230,"Looks very good to me, thank you very much! Would you mind adding an option to select for the correction type that defaults to 'benjamini-hochberg' and can be set to 'bonferroni'? In the best of all world's, you'd also extend the tests for rank_genes_groups so that the p values are tested and not messed up by pull requests in the future. We want people to get the same p values again and again. And as the whole module sort of involves a lot of custom code as the scipy alternatives are not there for mult-dimensional and sparse data, it's easy to mess this up in the future. Thank you so much for the awesome addition @a-munoz-rojas , I'll add you both to the Scanpy author list and to the release notes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289
https://github.com/scverse/scanpy/pull/289:283,testability,test,tested,283,"Looks very good to me, thank you very much! Would you mind adding an option to select for the correction type that defaults to 'benjamini-hochberg' and can be set to 'bonferroni'? In the best of all world's, you'd also extend the tests for rank_genes_groups so that the p values are tested and not messed up by pull requests in the future. We want people to get the same p values again and again. And as the whole module sort of involves a lot of custom code as the scipy alternatives are not there for mult-dimensional and sparse data, it's easy to mess this up in the future. Thank you so much for the awesome addition @a-munoz-rojas , I'll add you both to the Scanpy author list and to the release notes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289
https://github.com/scverse/scanpy/pull/289:447,usability,custom,custom,447,"Looks very good to me, thank you very much! Would you mind adding an option to select for the correction type that defaults to 'benjamini-hochberg' and can be set to 'bonferroni'? In the best of all world's, you'd also extend the tests for rank_genes_groups so that the p values are tested and not messed up by pull requests in the future. We want people to get the same p values again and again. And as the whole module sort of involves a lot of custom code as the scipy alternatives are not there for mult-dimensional and sparse data, it's easy to mess this up in the future. Thank you so much for the awesome addition @a-munoz-rojas , I'll add you both to the Scanpy author list and to the release notes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289
https://github.com/scverse/scanpy/pull/289:315,deployability,build,building,315,"Thank you very much, @falexwolf! I really appreciate the addition to the author list! I'm glad this is useful. I just now added the option to choose which correction method to use, and set benjamini-hochberg as the default, so that should be all set. With regards to the test, I unfortunately don't have experience building those tests, and have limited bandwidth at the moment. So it would probably be best if someone else could extend those tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289
https://github.com/scverse/scanpy/pull/289:430,modifiability,exten,extend,430,"Thank you very much, @falexwolf! I really appreciate the addition to the author list! I'm glad this is useful. I just now added the option to choose which correction method to use, and set benjamini-hochberg as the default, so that should be all set. With regards to the test, I unfortunately don't have experience building those tests, and have limited bandwidth at the moment. So it would probably be best if someone else could extend those tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289
https://github.com/scverse/scanpy/pull/289:271,safety,test,test,271,"Thank you very much, @falexwolf! I really appreciate the addition to the author list! I'm glad this is useful. I just now added the option to choose which correction method to use, and set benjamini-hochberg as the default, so that should be all set. With regards to the test, I unfortunately don't have experience building those tests, and have limited bandwidth at the moment. So it would probably be best if someone else could extend those tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289
https://github.com/scverse/scanpy/pull/289:330,safety,test,tests,330,"Thank you very much, @falexwolf! I really appreciate the addition to the author list! I'm glad this is useful. I just now added the option to choose which correction method to use, and set benjamini-hochberg as the default, so that should be all set. With regards to the test, I unfortunately don't have experience building those tests, and have limited bandwidth at the moment. So it would probably be best if someone else could extend those tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289
https://github.com/scverse/scanpy/pull/289:443,safety,test,tests,443,"Thank you very much, @falexwolf! I really appreciate the addition to the author list! I'm glad this is useful. I just now added the option to choose which correction method to use, and set benjamini-hochberg as the default, so that should be all set. With regards to the test, I unfortunately don't have experience building those tests, and have limited bandwidth at the moment. So it would probably be best if someone else could extend those tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289
https://github.com/scverse/scanpy/pull/289:73,security,auth,author,73,"Thank you very much, @falexwolf! I really appreciate the addition to the author list! I'm glad this is useful. I just now added the option to choose which correction method to use, and set benjamini-hochberg as the default, so that should be all set. With regards to the test, I unfortunately don't have experience building those tests, and have limited bandwidth at the moment. So it would probably be best if someone else could extend those tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289
https://github.com/scverse/scanpy/pull/289:271,testability,test,test,271,"Thank you very much, @falexwolf! I really appreciate the addition to the author list! I'm glad this is useful. I just now added the option to choose which correction method to use, and set benjamini-hochberg as the default, so that should be all set. With regards to the test, I unfortunately don't have experience building those tests, and have limited bandwidth at the moment. So it would probably be best if someone else could extend those tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289
https://github.com/scverse/scanpy/pull/289:330,testability,test,tests,330,"Thank you very much, @falexwolf! I really appreciate the addition to the author list! I'm glad this is useful. I just now added the option to choose which correction method to use, and set benjamini-hochberg as the default, so that should be all set. With regards to the test, I unfortunately don't have experience building those tests, and have limited bandwidth at the moment. So it would probably be best if someone else could extend those tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289
https://github.com/scverse/scanpy/pull/289:443,testability,test,tests,443,"Thank you very much, @falexwolf! I really appreciate the addition to the author list! I'm glad this is useful. I just now added the option to choose which correction method to use, and set benjamini-hochberg as the default, so that should be all set. With regards to the test, I unfortunately don't have experience building those tests, and have limited bandwidth at the moment. So it would probably be best if someone else could extend those tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289
https://github.com/scverse/scanpy/pull/289:304,usability,experien,experience,304,"Thank you very much, @falexwolf! I really appreciate the addition to the author list! I'm glad this is useful. I just now added the option to choose which correction method to use, and set benjamini-hochberg as the default, so that should be all set. With regards to the test, I unfortunately don't have experience building those tests, and have limited bandwidth at the moment. So it would probably be best if someone else could extend those tests.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289
https://github.com/scverse/scanpy/pull/289:17,safety,test,tests,17,"OK, I'll add the tests myself. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289
https://github.com/scverse/scanpy/pull/289:17,testability,test,tests,17,"OK, I'll add the tests myself. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289
https://github.com/scverse/scanpy/pull/289:31,security,auth,author,31,Which name should I add to the author list?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/289
https://github.com/scverse/scanpy/issues/290:46,availability,cluster,clusters,46,"I have written a couple of functions to match clusters and marker genes. The simplest case is just a table of overlap score. Alternatively, I know someone who has used the Jaccard Index and enrichment tests. The other functions I wrote calculate average z-scores of marker genes in clusters (not sure if this is similar to `score_genes` or not. I could paste the functions in here if you like.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:282,availability,cluster,clusters,282,"I have written a couple of functions to match clusters and marker genes. The simplest case is just a table of overlap score. Alternatively, I know someone who has used the Jaccard Index and enrichment tests. The other functions I wrote calculate average z-scores of marker genes in clusters (not sure if this is similar to `score_genes` or not. I could paste the functions in here if you like.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:46,deployability,cluster,clusters,46,"I have written a couple of functions to match clusters and marker genes. The simplest case is just a table of overlap score. Alternatively, I know someone who has used the Jaccard Index and enrichment tests. The other functions I wrote calculate average z-scores of marker genes in clusters (not sure if this is similar to `score_genes` or not. I could paste the functions in here if you like.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:282,deployability,cluster,clusters,282,"I have written a couple of functions to match clusters and marker genes. The simplest case is just a table of overlap score. Alternatively, I know someone who has used the Jaccard Index and enrichment tests. The other functions I wrote calculate average z-scores of marker genes in clusters (not sure if this is similar to `score_genes` or not. I could paste the functions in here if you like.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:17,integrability,coupl,couple,17,"I have written a couple of functions to match clusters and marker genes. The simplest case is just a table of overlap score. Alternatively, I know someone who has used the Jaccard Index and enrichment tests. The other functions I wrote calculate average z-scores of marker genes in clusters (not sure if this is similar to `score_genes` or not. I could paste the functions in here if you like.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:17,modifiability,coupl,couple,17,"I have written a couple of functions to match clusters and marker genes. The simplest case is just a table of overlap score. Alternatively, I know someone who has used the Jaccard Index and enrichment tests. The other functions I wrote calculate average z-scores of marker genes in clusters (not sure if this is similar to `score_genes` or not. I could paste the functions in here if you like.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:201,safety,test,tests,201,"I have written a couple of functions to match clusters and marker genes. The simplest case is just a table of overlap score. Alternatively, I know someone who has used the Jaccard Index and enrichment tests. The other functions I wrote calculate average z-scores of marker genes in clusters (not sure if this is similar to `score_genes` or not. I could paste the functions in here if you like.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:17,testability,coupl,couple,17,"I have written a couple of functions to match clusters and marker genes. The simplest case is just a table of overlap score. Alternatively, I know someone who has used the Jaccard Index and enrichment tests. The other functions I wrote calculate average z-scores of marker genes in clusters (not sure if this is similar to `score_genes` or not. I could paste the functions in here if you like.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:77,testability,simpl,simplest,77,"I have written a couple of functions to match clusters and marker genes. The simplest case is just a table of overlap score. Alternatively, I know someone who has used the Jaccard Index and enrichment tests. The other functions I wrote calculate average z-scores of marker genes in clusters (not sure if this is similar to `score_genes` or not. I could paste the functions in here if you like.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:201,testability,test,tests,201,"I have written a couple of functions to match clusters and marker genes. The simplest case is just a table of overlap score. Alternatively, I know someone who has used the Jaccard Index and enrichment tests. The other functions I wrote calculate average z-scores of marker genes in clusters (not sure if this is similar to `score_genes` or not. I could paste the functions in here if you like.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:77,usability,simpl,simplest,77,"I have written a couple of functions to match clusters and marker genes. The simplest case is just a table of overlap score. Alternatively, I know someone who has used the Jaccard Index and enrichment tests. The other functions I wrote calculate average z-scores of marker genes in clusters (not sure if this is similar to `score_genes` or not. I could paste the functions in here if you like.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:238,availability,cluster,cluster,238,@LuckyMD I would be interested into looking at your method. It is different than that of `score_genes`. . I was considering to use the same tests from scanpy to identify marker genes but with a given set of markers as I want to know if a cluster could be annotated with a marker (which is different than to annotate a single cell). Any thoughts on this idea?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:238,deployability,cluster,cluster,238,@LuckyMD I would be interested into looking at your method. It is different than that of `score_genes`. . I was considering to use the same tests from scanpy to identify marker genes but with a given set of markers as I want to know if a cluster could be annotated with a marker (which is different than to annotate a single cell). Any thoughts on this idea?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:140,safety,test,tests,140,@LuckyMD I would be interested into looking at your method. It is different than that of `score_genes`. . I was considering to use the same tests from scanpy to identify marker genes but with a given set of markers as I want to know if a cluster could be annotated with a marker (which is different than to annotate a single cell). Any thoughts on this idea?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:161,security,ident,identify,161,@LuckyMD I would be interested into looking at your method. It is different than that of `score_genes`. . I was considering to use the same tests from scanpy to identify marker genes but with a given set of markers as I want to know if a cluster could be annotated with a marker (which is different than to annotate a single cell). Any thoughts on this idea?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:140,testability,test,tests,140,@LuckyMD I would be interested into looking at your method. It is different than that of `score_genes`. . I was considering to use the same tests from scanpy to identify marker genes but with a given set of markers as I want to know if a cluster could be annotated with a marker (which is different than to annotate a single cell). Any thoughts on this idea?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:98,availability,cluster,clusters,98,"Not sure what the best way of posting this is, but I'll just paste it for now:. Function to score clusters using multiple cell-type markers. ```. #Define cluster score for all markers. def evaluate_partition(anndata, marker_dict, gene_symbol_key=None, partition_key='louvain_r1'):. # Inputs:. # anndata - An AnnData object containing the data set and a partition. # marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' . #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = np.unique(anndata.obs[partition_key]). n_clust = len(clusters). n_groups = len(marker_dict). . marker_res = np.zeros((n_groups, n_clust)). z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. j = 0. for clust in clusters:. cluster_cells = np.in1d(z_scores.obs[partition_key], clust). marker_genes = np.in1d(gene_ids, marker_dict[group]). marker_res[i,j] = z_scores.X[np.ix_(cluster_cells,marker_genes)].mean(). j += 1. i+=1. variances = np.nanvar(marker_r",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:154,availability,cluster,cluster,154,"Not sure what the best way of posting this is, but I'll just paste it for now:. Function to score clusters using multiple cell-type markers. ```. #Define cluster score for all markers. def evaluate_partition(anndata, marker_dict, gene_symbol_key=None, partition_key='louvain_r1'):. # Inputs:. # anndata - An AnnData object containing the data set and a partition. # marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' . #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = np.unique(anndata.obs[partition_key]). n_clust = len(clusters). n_groups = len(marker_dict). . marker_res = np.zeros((n_groups, n_clust)). z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. j = 0. for clust in clusters:. cluster_cells = np.in1d(z_scores.obs[partition_key], clust). marker_genes = np.in1d(gene_ids, marker_dict[group]). marker_res[i,j] = z_scores.X[np.ix_(cluster_cells,marker_genes)].mean(). j += 1. i+=1. variances = np.nanvar(marker_r",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:724,availability,cluster,cluster,724,"Not sure what the best way of posting this is, but I'll just paste it for now:. Function to score clusters using multiple cell-type markers. ```. #Define cluster score for all markers. def evaluate_partition(anndata, marker_dict, gene_symbol_key=None, partition_key='louvain_r1'):. # Inputs:. # anndata - An AnnData object containing the data set and a partition. # marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' . #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = np.unique(anndata.obs[partition_key]). n_clust = len(clusters). n_groups = len(marker_dict). . marker_res = np.zeros((n_groups, n_clust)). z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. j = 0. for clust in clusters:. cluster_cells = np.in1d(z_scores.obs[partition_key], clust). marker_genes = np.in1d(gene_ids, marker_dict[group]). marker_res[i,j] = z_scores.X[np.ix_(cluster_cells,marker_genes)].mean(). j += 1. i+=1. variances = np.nanvar(marker_r",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:956,availability,cluster,clustering,956,"Not sure what the best way of posting this is, but I'll just paste it for now:. Function to score clusters using multiple cell-type markers. ```. #Define cluster score for all markers. def evaluate_partition(anndata, marker_dict, gene_symbol_key=None, partition_key='louvain_r1'):. # Inputs:. # anndata - An AnnData object containing the data set and a partition. # marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' . #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = np.unique(anndata.obs[partition_key]). n_clust = len(clusters). n_groups = len(marker_dict). . marker_res = np.zeros((n_groups, n_clust)). z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. j = 0. for clust in clusters:. cluster_cells = np.in1d(z_scores.obs[partition_key], clust). marker_genes = np.in1d(gene_ids, marker_dict[group]). marker_res[i,j] = z_scores.X[np.ix_(cluster_cells,marker_genes)].mean(). j += 1. i+=1. variances = np.nanvar(marker_r",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:996,availability,cluster,cluster,996,"Not sure what the best way of posting this is, but I'll just paste it for now:. Function to score clusters using multiple cell-type markers. ```. #Define cluster score for all markers. def evaluate_partition(anndata, marker_dict, gene_symbol_key=None, partition_key='louvain_r1'):. # Inputs:. # anndata - An AnnData object containing the data set and a partition. # marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' . #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = np.unique(anndata.obs[partition_key]). n_clust = len(clusters). n_groups = len(marker_dict). . marker_res = np.zeros((n_groups, n_clust)). z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. j = 0. for clust in clusters:. cluster_cells = np.in1d(z_scores.obs[partition_key], clust). marker_genes = np.in1d(gene_ids, marker_dict[group]). marker_res[i,j] = z_scores.X[np.ix_(cluster_cells,marker_genes)].mean(). j += 1. i+=1. variances = np.nanvar(marker_r",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:1430,availability,cluster,clusters,1430,"d be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' . #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = np.unique(anndata.obs[partition_key]). n_clust = len(clusters). n_groups = len(marker_dict). . marker_res = np.zeros((n_groups, n_clust)). z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. j = 0. for clust in clusters:. cluster_cells = np.in1d(z_scores.obs[partition_key], clust). marker_genes = np.in1d(gene_ids, marker_dict[group]). marker_res[i,j] = z_scores.X[np.ix_(cluster_cells,marker_genes)].mean(). j += 1. i+=1. variances = np.nanvar(marker_res, axis=0). if np.all(np.isnan(variances)):. print(""No variances could be computed, check if your cell markers are in the data set.""). print(""Maybe the cell marker IDs do not correspond to your gene_symbol_key input or the var_names""). raise. marker_res_df = pd.DataFrame(marker_res, columns=clusters, index=marker_dict.keys()). #Return the median of the variances over the clusters. return([np.median(variances), marker_res_df]). ``",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:1494,availability,cluster,clusters,1494,"h the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' . #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = np.unique(anndata.obs[partition_key]). n_clust = len(clusters). n_groups = len(marker_dict). . marker_res = np.zeros((n_groups, n_clust)). z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. j = 0. for clust in clusters:. cluster_cells = np.in1d(z_scores.obs[partition_key], clust). marker_genes = np.in1d(gene_ids, marker_dict[group]). marker_res[i,j] = z_scores.X[np.ix_(cluster_cells,marker_genes)].mean(). j += 1. i+=1. variances = np.nanvar(marker_res, axis=0). if np.all(np.isnan(variances)):. print(""No variances could be computed, check if your cell markers are in the data set.""). print(""Maybe the cell marker IDs do not correspond to your gene_symbol_key input or the var_names""). raise. marker_res_df = pd.DataFrame(marker_res, columns=clusters, index=marker_dict.keys()). #Return the median of the variances over the clusters. return([np.median(variances), marker_res_df]). ```. Function to obtain scores for each marker individually per cl",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:1728,availability,cluster,cluster,1728,"IDs are stored. The default is. # 'louvain_r1' . #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = np.unique(anndata.obs[partition_key]). n_clust = len(clusters). n_groups = len(marker_dict). . marker_res = np.zeros((n_groups, n_clust)). z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. j = 0. for clust in clusters:. cluster_cells = np.in1d(z_scores.obs[partition_key], clust). marker_genes = np.in1d(gene_ids, marker_dict[group]). marker_res[i,j] = z_scores.X[np.ix_(cluster_cells,marker_genes)].mean(). j += 1. i+=1. variances = np.nanvar(marker_res, axis=0). if np.all(np.isnan(variances)):. print(""No variances could be computed, check if your cell markers are in the data set.""). print(""Maybe the cell marker IDs do not correspond to your gene_symbol_key input or the var_names""). raise. marker_res_df = pd.DataFrame(marker_res, columns=clusters, index=marker_dict.keys()). #Return the median of the variances over the clusters. return([np.median(variances), marker_res_df]). ```. Function to obtain scores for each marker individually per cluster. ```. #Define cluster score for individual genes. def marker_gene_expression(anndata, marker_dict, gene_symbol_key=None, partition_key='louvain_r1'):. """""". A function to get mean z-score expressions of marker genes. # . # Inputs",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:1757,availability,cluster,clusters,1757,"s. # 'louvain_r1' . #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = np.unique(anndata.obs[partition_key]). n_clust = len(clusters). n_groups = len(marker_dict). . marker_res = np.zeros((n_groups, n_clust)). z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. j = 0. for clust in clusters:. cluster_cells = np.in1d(z_scores.obs[partition_key], clust). marker_genes = np.in1d(gene_ids, marker_dict[group]). marker_res[i,j] = z_scores.X[np.ix_(cluster_cells,marker_genes)].mean(). j += 1. i+=1. variances = np.nanvar(marker_res, axis=0). if np.all(np.isnan(variances)):. print(""No variances could be computed, check if your cell markers are in the data set.""). print(""Maybe the cell marker IDs do not correspond to your gene_symbol_key input or the var_names""). raise. marker_res_df = pd.DataFrame(marker_res, columns=clusters, index=marker_dict.keys()). #Return the median of the variances over the clusters. return([np.median(variances), marker_res_df]). ```. Function to obtain scores for each marker individually per cluster. ```. #Define cluster score for individual genes. def marker_gene_expression(anndata, marker_dict, gene_symbol_key=None, partition_key='louvain_r1'):. """""". A function to get mean z-score expressions of marker genes. # . # Inputs:. # anndata - An AnnData obj",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:2293,availability,cluster,clusters,2293,"anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = np.unique(anndata.obs[partition_key]). n_clust = len(clusters). n_groups = len(marker_dict). . marker_res = np.zeros((n_groups, n_clust)). z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. j = 0. for clust in clusters:. cluster_cells = np.in1d(z_scores.obs[partition_key], clust). marker_genes = np.in1d(gene_ids, marker_dict[group]). marker_res[i,j] = z_scores.X[np.ix_(cluster_cells,marker_genes)].mean(). j += 1. i+=1. variances = np.nanvar(marker_res, axis=0). if np.all(np.isnan(variances)):. print(""No variances could be computed, check if your cell markers are in the data set.""). print(""Maybe the cell marker IDs do not correspond to your gene_symbol_key input or the var_names""). raise. marker_res_df = pd.DataFrame(marker_res, columns=clusters, index=marker_dict.keys()). #Return the median of the variances over the clusters. return([np.median(variances), marker_res_df]). ```. Function to obtain scores for each marker individually per cluster. ```. #Define cluster score for individual genes. def marker_gene_expression(anndata, marker_dict, gene_symbol_key=None, partition_key='louvain_r1'):. """""". A function to get mean z-score expressions of marker genes. # . # Inputs:. # anndata - An AnnData object containing the data set and a partition. # marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' . """""". #Test inputs. if partition_key not in anndata.obs.columns.values:. pr",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:2375,availability,cluster,clusters,2375,"_symbol_key]. else:. gene_ids = anndata.var_names. clusters = np.unique(anndata.obs[partition_key]). n_clust = len(clusters). n_groups = len(marker_dict). . marker_res = np.zeros((n_groups, n_clust)). z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. j = 0. for clust in clusters:. cluster_cells = np.in1d(z_scores.obs[partition_key], clust). marker_genes = np.in1d(gene_ids, marker_dict[group]). marker_res[i,j] = z_scores.X[np.ix_(cluster_cells,marker_genes)].mean(). j += 1. i+=1. variances = np.nanvar(marker_res, axis=0). if np.all(np.isnan(variances)):. print(""No variances could be computed, check if your cell markers are in the data set.""). print(""Maybe the cell marker IDs do not correspond to your gene_symbol_key input or the var_names""). raise. marker_res_df = pd.DataFrame(marker_res, columns=clusters, index=marker_dict.keys()). #Return the median of the variances over the clusters. return([np.median(variances), marker_res_df]). ```. Function to obtain scores for each marker individually per cluster. ```. #Define cluster score for individual genes. def marker_gene_expression(anndata, marker_dict, gene_symbol_key=None, partition_key='louvain_r1'):. """""". A function to get mean z-score expressions of marker genes. # . # Inputs:. # anndata - An AnnData object containing the data set and a partition. # marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' . """""". #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). pr",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:2496,availability,cluster,cluster,2496,"rs). n_groups = len(marker_dict). . marker_res = np.zeros((n_groups, n_clust)). z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. j = 0. for clust in clusters:. cluster_cells = np.in1d(z_scores.obs[partition_key], clust). marker_genes = np.in1d(gene_ids, marker_dict[group]). marker_res[i,j] = z_scores.X[np.ix_(cluster_cells,marker_genes)].mean(). j += 1. i+=1. variances = np.nanvar(marker_res, axis=0). if np.all(np.isnan(variances)):. print(""No variances could be computed, check if your cell markers are in the data set.""). print(""Maybe the cell marker IDs do not correspond to your gene_symbol_key input or the var_names""). raise. marker_res_df = pd.DataFrame(marker_res, columns=clusters, index=marker_dict.keys()). #Return the median of the variances over the clusters. return([np.median(variances), marker_res_df]). ```. Function to obtain scores for each marker individually per cluster. ```. #Define cluster score for individual genes. def marker_gene_expression(anndata, marker_dict, gene_symbol_key=None, partition_key='louvain_r1'):. """""". A function to get mean z-score expressions of marker genes. # . # Inputs:. # anndata - An AnnData object containing the data set and a partition. # marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' . """""". #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:2518,availability,cluster,cluster,2518,"rker_dict). . marker_res = np.zeros((n_groups, n_clust)). z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. j = 0. for clust in clusters:. cluster_cells = np.in1d(z_scores.obs[partition_key], clust). marker_genes = np.in1d(gene_ids, marker_dict[group]). marker_res[i,j] = z_scores.X[np.ix_(cluster_cells,marker_genes)].mean(). j += 1. i+=1. variances = np.nanvar(marker_res, axis=0). if np.all(np.isnan(variances)):. print(""No variances could be computed, check if your cell markers are in the data set.""). print(""Maybe the cell marker IDs do not correspond to your gene_symbol_key input or the var_names""). raise. marker_res_df = pd.DataFrame(marker_res, columns=clusters, index=marker_dict.keys()). #Return the median of the variances over the clusters. return([np.median(variances), marker_res_df]). ```. Function to obtain scores for each marker individually per cluster. ```. #Define cluster score for individual genes. def marker_gene_expression(anndata, marker_dict, gene_symbol_key=None, partition_key='louvain_r1'):. """""". A function to get mean z-score expressions of marker genes. # . # Inputs:. # anndata - An AnnData object containing the data set and a partition. # marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' . """""". #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:3166,availability,cluster,cluster,3166,"o not correspond to your gene_symbol_key input or the var_names""). raise. marker_res_df = pd.DataFrame(marker_res, columns=clusters, index=marker_dict.keys()). #Return the median of the variances over the clusters. return([np.median(variances), marker_res_df]). ```. Function to obtain scores for each marker individually per cluster. ```. #Define cluster score for individual genes. def marker_gene_expression(anndata, marker_dict, gene_symbol_key=None, partition_key='louvain_r1'):. """""". A function to get mean z-score expressions of marker genes. # . # Inputs:. # anndata - An AnnData object containing the data set and a partition. # marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' . """""". #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = anndata.obs[partition_key].cat.categories. n_clust = len(clusters). marker_exp = pd.DataFrame(columns=clusters). marker_exp['cell_type'] = pd.Series({}, dtype='str'). marker_names = []. . z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corre",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:3403,availability,cluster,clustering,3403,"nces), marker_res_df]). ```. Function to obtain scores for each marker individually per cluster. ```. #Define cluster score for individual genes. def marker_gene_expression(anndata, marker_dict, gene_symbol_key=None, partition_key='louvain_r1'):. """""". A function to get mean z-score expressions of marker genes. # . # Inputs:. # anndata - An AnnData object containing the data set and a partition. # marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' . """""". #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = anndata.obs[partition_key].cat.categories. n_clust = len(clusters). marker_exp = pd.DataFrame(columns=clusters). marker_exp['cell_type'] = pd.Series({}, dtype='str'). marker_names = []. . z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. for gene in marker_dict[group]:. ens_idx = np.in1d(gene_ids, gene) #Note there may be multiple mappings. if np.sum(ens_idx) == 0:. continue. else:. z_scores.obs[ens_idx[0]] = ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:3443,availability,cluster,cluster,3443,"o obtain scores for each marker individually per cluster. ```. #Define cluster score for individual genes. def marker_gene_expression(anndata, marker_dict, gene_symbol_key=None, partition_key='louvain_r1'):. """""". A function to get mean z-score expressions of marker genes. # . # Inputs:. # anndata - An AnnData object containing the data set and a partition. # marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' . """""". #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = anndata.obs[partition_key].cat.categories. n_clust = len(clusters). marker_exp = pd.DataFrame(columns=clusters). marker_exp['cell_type'] = pd.Series({}, dtype='str'). marker_names = []. . z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. for gene in marker_dict[group]:. ens_idx = np.in1d(gene_ids, gene) #Note there may be multiple mappings. if np.sum(ens_idx) == 0:. continue. else:. z_scores.obs[ens_idx[0]] = z_scores.X[:,ens_idx].mean(1) #works fo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:3877,availability,cluster,clusters,3877,"# marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' . """""". #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = anndata.obs[partition_key].cat.categories. n_clust = len(clusters). marker_exp = pd.DataFrame(columns=clusters). marker_exp['cell_type'] = pd.Series({}, dtype='str'). marker_names = []. . z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. for gene in marker_dict[group]:. ens_idx = np.in1d(gene_ids, gene) #Note there may be multiple mappings. if np.sum(ens_idx) == 0:. continue. else:. z_scores.obs[ens_idx[0]] = z_scores.X[:,ens_idx].mean(1) #works for both single and multiple mapping. ens_idx = ens_idx[0]. clust_marker_exp = z_scores.obs.groupby(partition_key)[ens_idx].apply(np.mean).tolist(). clust_marker_exp.append(group). marker_exp.loc[i] = clust_marker_exp. marker_names.append(gene). i+=1. #Replace the rownames with informative gene symbols. marker_exp.index = marker_names. return(marker_exp). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:3945,availability,cluster,clusters,3945,"# marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' . """""". #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = anndata.obs[partition_key].cat.categories. n_clust = len(clusters). marker_exp = pd.DataFrame(columns=clusters). marker_exp['cell_type'] = pd.Series({}, dtype='str'). marker_names = []. . z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. for gene in marker_dict[group]:. ens_idx = np.in1d(gene_ids, gene) #Note there may be multiple mappings. if np.sum(ens_idx) == 0:. continue. else:. z_scores.obs[ens_idx[0]] = z_scores.X[:,ens_idx].mean(1) #works for both single and multiple mapping. ens_idx = ens_idx[0]. clust_marker_exp = z_scores.obs.groupby(partition_key)[ens_idx].apply(np.mean).tolist(). clust_marker_exp.append(group). marker_exp.loc[i] = clust_marker_exp. marker_names.append(gene). i+=1. #Replace the rownames with informative gene symbols. marker_exp.index = marker_names. return(marker_exp). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:3990,availability,cluster,clusters,3990,"# marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' . """""". #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = anndata.obs[partition_key].cat.categories. n_clust = len(clusters). marker_exp = pd.DataFrame(columns=clusters). marker_exp['cell_type'] = pd.Series({}, dtype='str'). marker_names = []. . z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. for gene in marker_dict[group]:. ens_idx = np.in1d(gene_ids, gene) #Note there may be multiple mappings. if np.sum(ens_idx) == 0:. continue. else:. z_scores.obs[ens_idx[0]] = z_scores.X[:,ens_idx].mean(1) #works for both single and multiple mapping. ens_idx = ens_idx[0]. clust_marker_exp = z_scores.obs.groupby(partition_key)[ens_idx].apply(np.mean).tolist(). clust_marker_exp.append(group). marker_exp.loc[i] = clust_marker_exp. marker_names.append(gene). i+=1. #Replace the rownames with informative gene symbols. marker_exp.index = marker_names. return(marker_exp). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:4224,availability,cluster,cluster,4224,"# marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' . """""". #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = anndata.obs[partition_key].cat.categories. n_clust = len(clusters). marker_exp = pd.DataFrame(columns=clusters). marker_exp['cell_type'] = pd.Series({}, dtype='str'). marker_names = []. . z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. for gene in marker_dict[group]:. ens_idx = np.in1d(gene_ids, gene) #Note there may be multiple mappings. if np.sum(ens_idx) == 0:. continue. else:. z_scores.obs[ens_idx[0]] = z_scores.X[:,ens_idx].mean(1) #works for both single and multiple mapping. ens_idx = ens_idx[0]. clust_marker_exp = z_scores.obs.groupby(partition_key)[ens_idx].apply(np.mean).tolist(). clust_marker_exp.append(group). marker_exp.loc[i] = clust_marker_exp. marker_names.append(gene). i+=1. #Replace the rownames with informative gene symbols. marker_exp.index = marker_names. return(marker_exp). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:98,deployability,cluster,clusters,98,"Not sure what the best way of posting this is, but I'll just paste it for now:. Function to score clusters using multiple cell-type markers. ```. #Define cluster score for all markers. def evaluate_partition(anndata, marker_dict, gene_symbol_key=None, partition_key='louvain_r1'):. # Inputs:. # anndata - An AnnData object containing the data set and a partition. # marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' . #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = np.unique(anndata.obs[partition_key]). n_clust = len(clusters). n_groups = len(marker_dict). . marker_res = np.zeros((n_groups, n_clust)). z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. j = 0. for clust in clusters:. cluster_cells = np.in1d(z_scores.obs[partition_key], clust). marker_genes = np.in1d(gene_ids, marker_dict[group]). marker_res[i,j] = z_scores.X[np.ix_(cluster_cells,marker_genes)].mean(). j += 1. i+=1. variances = np.nanvar(marker_r",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:154,deployability,cluster,cluster,154,"Not sure what the best way of posting this is, but I'll just paste it for now:. Function to score clusters using multiple cell-type markers. ```. #Define cluster score for all markers. def evaluate_partition(anndata, marker_dict, gene_symbol_key=None, partition_key='louvain_r1'):. # Inputs:. # anndata - An AnnData object containing the data set and a partition. # marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' . #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = np.unique(anndata.obs[partition_key]). n_clust = len(clusters). n_groups = len(marker_dict). . marker_res = np.zeros((n_groups, n_clust)). z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. j = 0. for clust in clusters:. cluster_cells = np.in1d(z_scores.obs[partition_key], clust). marker_genes = np.in1d(gene_ids, marker_dict[group]). marker_res[i,j] = z_scores.X[np.ix_(cluster_cells,marker_genes)].mean(). j += 1. i+=1. variances = np.nanvar(marker_r",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:323,deployability,contain,containing,323,"Not sure what the best way of posting this is, but I'll just paste it for now:. Function to score clusters using multiple cell-type markers. ```. #Define cluster score for all markers. def evaluate_partition(anndata, marker_dict, gene_symbol_key=None, partition_key='louvain_r1'):. # Inputs:. # anndata - An AnnData object containing the data set and a partition. # marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' . #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = np.unique(anndata.obs[partition_key]). n_clust = len(clusters). n_groups = len(marker_dict). . marker_res = np.zeros((n_groups, n_clust)). z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. j = 0. for clust in clusters:. cluster_cells = np.in1d(z_scores.obs[partition_key], clust). marker_genes = np.in1d(gene_ids, marker_dict[group]). marker_res[i,j] = z_scores.X[np.ix_(cluster_cells,marker_genes)].mean(). j += 1. i+=1. variances = np.nanvar(marker_r",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:724,deployability,cluster,cluster,724,"Not sure what the best way of posting this is, but I'll just paste it for now:. Function to score clusters using multiple cell-type markers. ```. #Define cluster score for all markers. def evaluate_partition(anndata, marker_dict, gene_symbol_key=None, partition_key='louvain_r1'):. # Inputs:. # anndata - An AnnData object containing the data set and a partition. # marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' . #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = np.unique(anndata.obs[partition_key]). n_clust = len(clusters). n_groups = len(marker_dict). . marker_res = np.zeros((n_groups, n_clust)). z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. j = 0. for clust in clusters:. cluster_cells = np.in1d(z_scores.obs[partition_key], clust). marker_genes = np.in1d(gene_ids, marker_dict[group]). marker_res[i,j] = z_scores.X[np.ix_(cluster_cells,marker_genes)].mean(). j += 1. i+=1. variances = np.nanvar(marker_r",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:956,deployability,cluster,clustering,956,"Not sure what the best way of posting this is, but I'll just paste it for now:. Function to score clusters using multiple cell-type markers. ```. #Define cluster score for all markers. def evaluate_partition(anndata, marker_dict, gene_symbol_key=None, partition_key='louvain_r1'):. # Inputs:. # anndata - An AnnData object containing the data set and a partition. # marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' . #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = np.unique(anndata.obs[partition_key]). n_clust = len(clusters). n_groups = len(marker_dict). . marker_res = np.zeros((n_groups, n_clust)). z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. j = 0. for clust in clusters:. cluster_cells = np.in1d(z_scores.obs[partition_key], clust). marker_genes = np.in1d(gene_ids, marker_dict[group]). marker_res[i,j] = z_scores.X[np.ix_(cluster_cells,marker_genes)].mean(). j += 1. i+=1. variances = np.nanvar(marker_r",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:996,deployability,cluster,cluster,996,"Not sure what the best way of posting this is, but I'll just paste it for now:. Function to score clusters using multiple cell-type markers. ```. #Define cluster score for all markers. def evaluate_partition(anndata, marker_dict, gene_symbol_key=None, partition_key='louvain_r1'):. # Inputs:. # anndata - An AnnData object containing the data set and a partition. # marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' . #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = np.unique(anndata.obs[partition_key]). n_clust = len(clusters). n_groups = len(marker_dict). . marker_res = np.zeros((n_groups, n_clust)). z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. j = 0. for clust in clusters:. cluster_cells = np.in1d(z_scores.obs[partition_key], clust). marker_genes = np.in1d(gene_ids, marker_dict[group]). marker_res[i,j] = z_scores.X[np.ix_(cluster_cells,marker_genes)].mean(). j += 1. i+=1. variances = np.nanvar(marker_r",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:1430,deployability,cluster,clusters,1430,"d be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' . #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = np.unique(anndata.obs[partition_key]). n_clust = len(clusters). n_groups = len(marker_dict). . marker_res = np.zeros((n_groups, n_clust)). z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. j = 0. for clust in clusters:. cluster_cells = np.in1d(z_scores.obs[partition_key], clust). marker_genes = np.in1d(gene_ids, marker_dict[group]). marker_res[i,j] = z_scores.X[np.ix_(cluster_cells,marker_genes)].mean(). j += 1. i+=1. variances = np.nanvar(marker_res, axis=0). if np.all(np.isnan(variances)):. print(""No variances could be computed, check if your cell markers are in the data set.""). print(""Maybe the cell marker IDs do not correspond to your gene_symbol_key input or the var_names""). raise. marker_res_df = pd.DataFrame(marker_res, columns=clusters, index=marker_dict.keys()). #Return the median of the variances over the clusters. return([np.median(variances), marker_res_df]). ``",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:1494,deployability,cluster,clusters,1494,"h the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' . #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = np.unique(anndata.obs[partition_key]). n_clust = len(clusters). n_groups = len(marker_dict). . marker_res = np.zeros((n_groups, n_clust)). z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. j = 0. for clust in clusters:. cluster_cells = np.in1d(z_scores.obs[partition_key], clust). marker_genes = np.in1d(gene_ids, marker_dict[group]). marker_res[i,j] = z_scores.X[np.ix_(cluster_cells,marker_genes)].mean(). j += 1. i+=1. variances = np.nanvar(marker_res, axis=0). if np.all(np.isnan(variances)):. print(""No variances could be computed, check if your cell markers are in the data set.""). print(""Maybe the cell marker IDs do not correspond to your gene_symbol_key input or the var_names""). raise. marker_res_df = pd.DataFrame(marker_res, columns=clusters, index=marker_dict.keys()). #Return the median of the variances over the clusters. return([np.median(variances), marker_res_df]). ```. Function to obtain scores for each marker individually per cl",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:1597,deployability,scale,scale,1597,"th gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' . #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = np.unique(anndata.obs[partition_key]). n_clust = len(clusters). n_groups = len(marker_dict). . marker_res = np.zeros((n_groups, n_clust)). z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. j = 0. for clust in clusters:. cluster_cells = np.in1d(z_scores.obs[partition_key], clust). marker_genes = np.in1d(gene_ids, marker_dict[group]). marker_res[i,j] = z_scores.X[np.ix_(cluster_cells,marker_genes)].mean(). j += 1. i+=1. variances = np.nanvar(marker_res, axis=0). if np.all(np.isnan(variances)):. print(""No variances could be computed, check if your cell markers are in the data set.""). print(""Maybe the cell marker IDs do not correspond to your gene_symbol_key input or the var_names""). raise. marker_res_df = pd.DataFrame(marker_res, columns=clusters, index=marker_dict.keys()). #Return the median of the variances over the clusters. return([np.median(variances), marker_res_df]). ```. Function to obtain scores for each marker individually per cluster. ```. #Define cluster score for individual genes. def marker_gene_expression(anndata, marker_dic",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:1728,deployability,cluster,cluster,1728,"IDs are stored. The default is. # 'louvain_r1' . #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = np.unique(anndata.obs[partition_key]). n_clust = len(clusters). n_groups = len(marker_dict). . marker_res = np.zeros((n_groups, n_clust)). z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. j = 0. for clust in clusters:. cluster_cells = np.in1d(z_scores.obs[partition_key], clust). marker_genes = np.in1d(gene_ids, marker_dict[group]). marker_res[i,j] = z_scores.X[np.ix_(cluster_cells,marker_genes)].mean(). j += 1. i+=1. variances = np.nanvar(marker_res, axis=0). if np.all(np.isnan(variances)):. print(""No variances could be computed, check if your cell markers are in the data set.""). print(""Maybe the cell marker IDs do not correspond to your gene_symbol_key input or the var_names""). raise. marker_res_df = pd.DataFrame(marker_res, columns=clusters, index=marker_dict.keys()). #Return the median of the variances over the clusters. return([np.median(variances), marker_res_df]). ```. Function to obtain scores for each marker individually per cluster. ```. #Define cluster score for individual genes. def marker_gene_expression(anndata, marker_dict, gene_symbol_key=None, partition_key='louvain_r1'):. """""". A function to get mean z-score expressions of marker genes. # . # Inputs",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:1757,deployability,cluster,clusters,1757,"s. # 'louvain_r1' . #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = np.unique(anndata.obs[partition_key]). n_clust = len(clusters). n_groups = len(marker_dict). . marker_res = np.zeros((n_groups, n_clust)). z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. j = 0. for clust in clusters:. cluster_cells = np.in1d(z_scores.obs[partition_key], clust). marker_genes = np.in1d(gene_ids, marker_dict[group]). marker_res[i,j] = z_scores.X[np.ix_(cluster_cells,marker_genes)].mean(). j += 1. i+=1. variances = np.nanvar(marker_res, axis=0). if np.all(np.isnan(variances)):. print(""No variances could be computed, check if your cell markers are in the data set.""). print(""Maybe the cell marker IDs do not correspond to your gene_symbol_key input or the var_names""). raise. marker_res_df = pd.DataFrame(marker_res, columns=clusters, index=marker_dict.keys()). #Return the median of the variances over the clusters. return([np.median(variances), marker_res_df]). ```. Function to obtain scores for each marker individually per cluster. ```. #Define cluster score for individual genes. def marker_gene_expression(anndata, marker_dict, gene_symbol_key=None, partition_key='louvain_r1'):. """""". A function to get mean z-score expressions of marker genes. # . # Inputs:. # anndata - An AnnData obj",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:2293,deployability,cluster,clusters,2293,"anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = np.unique(anndata.obs[partition_key]). n_clust = len(clusters). n_groups = len(marker_dict). . marker_res = np.zeros((n_groups, n_clust)). z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. j = 0. for clust in clusters:. cluster_cells = np.in1d(z_scores.obs[partition_key], clust). marker_genes = np.in1d(gene_ids, marker_dict[group]). marker_res[i,j] = z_scores.X[np.ix_(cluster_cells,marker_genes)].mean(). j += 1. i+=1. variances = np.nanvar(marker_res, axis=0). if np.all(np.isnan(variances)):. print(""No variances could be computed, check if your cell markers are in the data set.""). print(""Maybe the cell marker IDs do not correspond to your gene_symbol_key input or the var_names""). raise. marker_res_df = pd.DataFrame(marker_res, columns=clusters, index=marker_dict.keys()). #Return the median of the variances over the clusters. return([np.median(variances), marker_res_df]). ```. Function to obtain scores for each marker individually per cluster. ```. #Define cluster score for individual genes. def marker_gene_expression(anndata, marker_dict, gene_symbol_key=None, partition_key='louvain_r1'):. """""". A function to get mean z-score expressions of marker genes. # . # Inputs:. # anndata - An AnnData object containing the data set and a partition. # marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' . """""". #Test inputs. if partition_key not in anndata.obs.columns.values:. pr",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:2375,deployability,cluster,clusters,2375,"_symbol_key]. else:. gene_ids = anndata.var_names. clusters = np.unique(anndata.obs[partition_key]). n_clust = len(clusters). n_groups = len(marker_dict). . marker_res = np.zeros((n_groups, n_clust)). z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. j = 0. for clust in clusters:. cluster_cells = np.in1d(z_scores.obs[partition_key], clust). marker_genes = np.in1d(gene_ids, marker_dict[group]). marker_res[i,j] = z_scores.X[np.ix_(cluster_cells,marker_genes)].mean(). j += 1. i+=1. variances = np.nanvar(marker_res, axis=0). if np.all(np.isnan(variances)):. print(""No variances could be computed, check if your cell markers are in the data set.""). print(""Maybe the cell marker IDs do not correspond to your gene_symbol_key input or the var_names""). raise. marker_res_df = pd.DataFrame(marker_res, columns=clusters, index=marker_dict.keys()). #Return the median of the variances over the clusters. return([np.median(variances), marker_res_df]). ```. Function to obtain scores for each marker individually per cluster. ```. #Define cluster score for individual genes. def marker_gene_expression(anndata, marker_dict, gene_symbol_key=None, partition_key='louvain_r1'):. """""". A function to get mean z-score expressions of marker genes. # . # Inputs:. # anndata - An AnnData object containing the data set and a partition. # marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' . """""". #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). pr",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:2496,deployability,cluster,cluster,2496,"rs). n_groups = len(marker_dict). . marker_res = np.zeros((n_groups, n_clust)). z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. j = 0. for clust in clusters:. cluster_cells = np.in1d(z_scores.obs[partition_key], clust). marker_genes = np.in1d(gene_ids, marker_dict[group]). marker_res[i,j] = z_scores.X[np.ix_(cluster_cells,marker_genes)].mean(). j += 1. i+=1. variances = np.nanvar(marker_res, axis=0). if np.all(np.isnan(variances)):. print(""No variances could be computed, check if your cell markers are in the data set.""). print(""Maybe the cell marker IDs do not correspond to your gene_symbol_key input or the var_names""). raise. marker_res_df = pd.DataFrame(marker_res, columns=clusters, index=marker_dict.keys()). #Return the median of the variances over the clusters. return([np.median(variances), marker_res_df]). ```. Function to obtain scores for each marker individually per cluster. ```. #Define cluster score for individual genes. def marker_gene_expression(anndata, marker_dict, gene_symbol_key=None, partition_key='louvain_r1'):. """""". A function to get mean z-score expressions of marker genes. # . # Inputs:. # anndata - An AnnData object containing the data set and a partition. # marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' . """""". #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:2518,deployability,cluster,cluster,2518,"rker_dict). . marker_res = np.zeros((n_groups, n_clust)). z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. j = 0. for clust in clusters:. cluster_cells = np.in1d(z_scores.obs[partition_key], clust). marker_genes = np.in1d(gene_ids, marker_dict[group]). marker_res[i,j] = z_scores.X[np.ix_(cluster_cells,marker_genes)].mean(). j += 1. i+=1. variances = np.nanvar(marker_res, axis=0). if np.all(np.isnan(variances)):. print(""No variances could be computed, check if your cell markers are in the data set.""). print(""Maybe the cell marker IDs do not correspond to your gene_symbol_key input or the var_names""). raise. marker_res_df = pd.DataFrame(marker_res, columns=clusters, index=marker_dict.keys()). #Return the median of the variances over the clusters. return([np.median(variances), marker_res_df]). ```. Function to obtain scores for each marker individually per cluster. ```. #Define cluster score for individual genes. def marker_gene_expression(anndata, marker_dict, gene_symbol_key=None, partition_key='louvain_r1'):. """""". A function to get mean z-score expressions of marker genes. # . # Inputs:. # anndata - An AnnData object containing the data set and a partition. # marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' . """""". #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:2765,deployability,contain,containing,2765,"uster_cells = np.in1d(z_scores.obs[partition_key], clust). marker_genes = np.in1d(gene_ids, marker_dict[group]). marker_res[i,j] = z_scores.X[np.ix_(cluster_cells,marker_genes)].mean(). j += 1. i+=1. variances = np.nanvar(marker_res, axis=0). if np.all(np.isnan(variances)):. print(""No variances could be computed, check if your cell markers are in the data set.""). print(""Maybe the cell marker IDs do not correspond to your gene_symbol_key input or the var_names""). raise. marker_res_df = pd.DataFrame(marker_res, columns=clusters, index=marker_dict.keys()). #Return the median of the variances over the clusters. return([np.median(variances), marker_res_df]). ```. Function to obtain scores for each marker individually per cluster. ```. #Define cluster score for individual genes. def marker_gene_expression(anndata, marker_dict, gene_symbol_key=None, partition_key='louvain_r1'):. """""". A function to get mean z-score expressions of marker genes. # . # Inputs:. # anndata - An AnnData object containing the data set and a partition. # marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' . """""". #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). r",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:3166,deployability,cluster,cluster,3166,"o not correspond to your gene_symbol_key input or the var_names""). raise. marker_res_df = pd.DataFrame(marker_res, columns=clusters, index=marker_dict.keys()). #Return the median of the variances over the clusters. return([np.median(variances), marker_res_df]). ```. Function to obtain scores for each marker individually per cluster. ```. #Define cluster score for individual genes. def marker_gene_expression(anndata, marker_dict, gene_symbol_key=None, partition_key='louvain_r1'):. """""". A function to get mean z-score expressions of marker genes. # . # Inputs:. # anndata - An AnnData object containing the data set and a partition. # marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' . """""". #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = anndata.obs[partition_key].cat.categories. n_clust = len(clusters). marker_exp = pd.DataFrame(columns=clusters). marker_exp['cell_type'] = pd.Series({}, dtype='str'). marker_names = []. . z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corre",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:3403,deployability,cluster,clustering,3403,"nces), marker_res_df]). ```. Function to obtain scores for each marker individually per cluster. ```. #Define cluster score for individual genes. def marker_gene_expression(anndata, marker_dict, gene_symbol_key=None, partition_key='louvain_r1'):. """""". A function to get mean z-score expressions of marker genes. # . # Inputs:. # anndata - An AnnData object containing the data set and a partition. # marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' . """""". #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = anndata.obs[partition_key].cat.categories. n_clust = len(clusters). marker_exp = pd.DataFrame(columns=clusters). marker_exp['cell_type'] = pd.Series({}, dtype='str'). marker_names = []. . z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. for gene in marker_dict[group]:. ens_idx = np.in1d(gene_ids, gene) #Note there may be multiple mappings. if np.sum(ens_idx) == 0:. continue. else:. z_scores.obs[ens_idx[0]] = ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:3443,deployability,cluster,cluster,3443,"o obtain scores for each marker individually per cluster. ```. #Define cluster score for individual genes. def marker_gene_expression(anndata, marker_dict, gene_symbol_key=None, partition_key='louvain_r1'):. """""". A function to get mean z-score expressions of marker genes. # . # Inputs:. # anndata - An AnnData object containing the data set and a partition. # marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' . """""". #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = anndata.obs[partition_key].cat.categories. n_clust = len(clusters). marker_exp = pd.DataFrame(columns=clusters). marker_exp['cell_type'] = pd.Series({}, dtype='str'). marker_names = []. . z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. for gene in marker_dict[group]:. ens_idx = np.in1d(gene_ids, gene) #Note there may be multiple mappings. if np.sum(ens_idx) == 0:. continue. else:. z_scores.obs[ens_idx[0]] = z_scores.X[:,ens_idx].mean(1) #works fo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:3877,deployability,cluster,clusters,3877,"# marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' . """""". #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = anndata.obs[partition_key].cat.categories. n_clust = len(clusters). marker_exp = pd.DataFrame(columns=clusters). marker_exp['cell_type'] = pd.Series({}, dtype='str'). marker_names = []. . z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. for gene in marker_dict[group]:. ens_idx = np.in1d(gene_ids, gene) #Note there may be multiple mappings. if np.sum(ens_idx) == 0:. continue. else:. z_scores.obs[ens_idx[0]] = z_scores.X[:,ens_idx].mean(1) #works for both single and multiple mapping. ens_idx = ens_idx[0]. clust_marker_exp = z_scores.obs.groupby(partition_key)[ens_idx].apply(np.mean).tolist(). clust_marker_exp.append(group). marker_exp.loc[i] = clust_marker_exp. marker_names.append(gene). i+=1. #Replace the rownames with informative gene symbols. marker_exp.index = marker_names. return(marker_exp). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:3945,deployability,cluster,clusters,3945,"# marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' . """""". #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = anndata.obs[partition_key].cat.categories. n_clust = len(clusters). marker_exp = pd.DataFrame(columns=clusters). marker_exp['cell_type'] = pd.Series({}, dtype='str'). marker_names = []. . z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. for gene in marker_dict[group]:. ens_idx = np.in1d(gene_ids, gene) #Note there may be multiple mappings. if np.sum(ens_idx) == 0:. continue. else:. z_scores.obs[ens_idx[0]] = z_scores.X[:,ens_idx].mean(1) #works for both single and multiple mapping. ens_idx = ens_idx[0]. clust_marker_exp = z_scores.obs.groupby(partition_key)[ens_idx].apply(np.mean).tolist(). clust_marker_exp.append(group). marker_exp.loc[i] = clust_marker_exp. marker_names.append(gene). i+=1. #Replace the rownames with informative gene symbols. marker_exp.index = marker_names. return(marker_exp). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:3990,deployability,cluster,clusters,3990,"# marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' . """""". #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = anndata.obs[partition_key].cat.categories. n_clust = len(clusters). marker_exp = pd.DataFrame(columns=clusters). marker_exp['cell_type'] = pd.Series({}, dtype='str'). marker_names = []. . z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. for gene in marker_dict[group]:. ens_idx = np.in1d(gene_ids, gene) #Note there may be multiple mappings. if np.sum(ens_idx) == 0:. continue. else:. z_scores.obs[ens_idx[0]] = z_scores.X[:,ens_idx].mean(1) #works for both single and multiple mapping. ens_idx = ens_idx[0]. clust_marker_exp = z_scores.obs.groupby(partition_key)[ens_idx].apply(np.mean).tolist(). clust_marker_exp.append(group). marker_exp.loc[i] = clust_marker_exp. marker_names.append(gene). i+=1. #Replace the rownames with informative gene symbols. marker_exp.index = marker_names. return(marker_exp). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:4093,deployability,scale,scale,4093,"# marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' . """""". #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = anndata.obs[partition_key].cat.categories. n_clust = len(clusters). marker_exp = pd.DataFrame(columns=clusters). marker_exp['cell_type'] = pd.Series({}, dtype='str'). marker_names = []. . z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. for gene in marker_dict[group]:. ens_idx = np.in1d(gene_ids, gene) #Note there may be multiple mappings. if np.sum(ens_idx) == 0:. continue. else:. z_scores.obs[ens_idx[0]] = z_scores.X[:,ens_idx].mean(1) #works for both single and multiple mapping. ens_idx = ens_idx[0]. clust_marker_exp = z_scores.obs.groupby(partition_key)[ens_idx].apply(np.mean).tolist(). clust_marker_exp.append(group). marker_exp.loc[i] = clust_marker_exp. marker_names.append(gene). i+=1. #Replace the rownames with informative gene symbols. marker_exp.index = marker_names. return(marker_exp). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:4224,deployability,cluster,cluster,4224,"# marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' . """""". #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = anndata.obs[partition_key].cat.categories. n_clust = len(clusters). marker_exp = pd.DataFrame(columns=clusters). marker_exp['cell_type'] = pd.Series({}, dtype='str'). marker_names = []. . z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. for gene in marker_dict[group]:. ens_idx = np.in1d(gene_ids, gene) #Note there may be multiple mappings. if np.sum(ens_idx) == 0:. continue. else:. z_scores.obs[ens_idx[0]] = z_scores.X[:,ens_idx].mean(1) #works for both single and multiple mapping. ens_idx = ens_idx[0]. clust_marker_exp = z_scores.obs.groupby(partition_key)[ens_idx].apply(np.mean).tolist(). clust_marker_exp.append(group). marker_exp.loc[i] = clust_marker_exp. marker_names.append(gene). i+=1. #Replace the rownames with informative gene symbols. marker_exp.index = marker_names. return(marker_exp). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:4364,deployability,continu,continue,4364,"# marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' . """""". #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = anndata.obs[partition_key].cat.categories. n_clust = len(clusters). marker_exp = pd.DataFrame(columns=clusters). marker_exp['cell_type'] = pd.Series({}, dtype='str'). marker_names = []. . z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. for gene in marker_dict[group]:. ens_idx = np.in1d(gene_ids, gene) #Note there may be multiple mappings. if np.sum(ens_idx) == 0:. continue. else:. z_scores.obs[ens_idx[0]] = z_scores.X[:,ens_idx].mean(1) #works for both single and multiple mapping. ens_idx = ens_idx[0]. clust_marker_exp = z_scores.obs.groupby(partition_key)[ens_idx].apply(np.mean).tolist(). clust_marker_exp.append(group). marker_exp.loc[i] = clust_marker_exp. marker_names.append(gene). i+=1. #Replace the rownames with informative gene symbols. marker_exp.index = marker_names. return(marker_exp). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:1597,energy efficiency,scale,scale,1597,"th gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' . #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = np.unique(anndata.obs[partition_key]). n_clust = len(clusters). n_groups = len(marker_dict). . marker_res = np.zeros((n_groups, n_clust)). z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. j = 0. for clust in clusters:. cluster_cells = np.in1d(z_scores.obs[partition_key], clust). marker_genes = np.in1d(gene_ids, marker_dict[group]). marker_res[i,j] = z_scores.X[np.ix_(cluster_cells,marker_genes)].mean(). j += 1. i+=1. variances = np.nanvar(marker_res, axis=0). if np.all(np.isnan(variances)):. print(""No variances could be computed, check if your cell markers are in the data set.""). print(""Maybe the cell marker IDs do not correspond to your gene_symbol_key input or the var_names""). raise. marker_res_df = pd.DataFrame(marker_res, columns=clusters, index=marker_dict.keys()). #Return the median of the variances over the clusters. return([np.median(variances), marker_res_df]). ```. Function to obtain scores for each marker individually per cluster. ```. #Define cluster score for individual genes. def marker_gene_expression(anndata, marker_dic",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:4093,energy efficiency,scale,scale,4093,"# marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' . """""". #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = anndata.obs[partition_key].cat.categories. n_clust = len(clusters). marker_exp = pd.DataFrame(columns=clusters). marker_exp['cell_type'] = pd.Series({}, dtype='str'). marker_names = []. . z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. for gene in marker_dict[group]:. ens_idx = np.in1d(gene_ids, gene) #Note there may be multiple mappings. if np.sum(ens_idx) == 0:. continue. else:. z_scores.obs[ens_idx[0]] = z_scores.X[:,ens_idx].mean(1) #works for both single and multiple mapping. ens_idx = ens_idx[0]. clust_marker_exp = z_scores.obs.groupby(partition_key)[ens_idx].apply(np.mean).tolist(). clust_marker_exp.append(group). marker_exp.loc[i] = clust_marker_exp. marker_names.append(gene). i+=1. #Replace the rownames with informative gene symbols. marker_exp.index = marker_names. return(marker_exp). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:1280,interoperability,format,format,1280," Inputs:. # anndata - An AnnData object containing the data set and a partition. # marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' . #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = np.unique(anndata.obs[partition_key]). n_clust = len(clusters). n_groups = len(marker_dict). . marker_res = np.zeros((n_groups, n_clust)). z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. j = 0. for clust in clusters:. cluster_cells = np.in1d(z_scores.obs[partition_key], clust). marker_genes = np.in1d(gene_ids, marker_dict[group]). marker_res[i,j] = z_scores.X[np.ix_(cluster_cells,marker_genes)].mean(). j += 1. i+=1. variances = np.nanvar(marker_res, axis=0). if np.all(np.isnan(variances)):. print(""No variances could be computed, check if your cell markers are in the data set.""). print(""Maybe the cell marker IDs do not correspond to your gene_symbol_key input or the var_names""). raise. marker_res_df = pd.DataFrame(marker_res",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:3727,interoperability,format,format,3727,"ts:. # anndata - An AnnData object containing the data set and a partition. # marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' . """""". #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = anndata.obs[partition_key].cat.categories. n_clust = len(clusters). marker_exp = pd.DataFrame(columns=clusters). marker_exp['cell_type'] = pd.Series({}, dtype='str'). marker_names = []. . z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. for gene in marker_dict[group]:. ens_idx = np.in1d(gene_ids, gene) #Note there may be multiple mappings. if np.sum(ens_idx) == 0:. continue. else:. z_scores.obs[ens_idx[0]] = z_scores.X[:,ens_idx].mean(1) #works for both single and multiple mapping. ens_idx = ens_idx[0]. clust_marker_exp = z_scores.obs.groupby(partition_key)[ens_idx].apply(np.mean).tolist(). clust_marker_exp.append(group). marker_exp.loc[i] = clust_marker_exp. marker_names.append(gene). i+=1. #Replace the rownames with inform",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:1597,modifiability,scal,scale,1597,"th gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' . #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = np.unique(anndata.obs[partition_key]). n_clust = len(clusters). n_groups = len(marker_dict). . marker_res = np.zeros((n_groups, n_clust)). z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. j = 0. for clust in clusters:. cluster_cells = np.in1d(z_scores.obs[partition_key], clust). marker_genes = np.in1d(gene_ids, marker_dict[group]). marker_res[i,j] = z_scores.X[np.ix_(cluster_cells,marker_genes)].mean(). j += 1. i+=1. variances = np.nanvar(marker_res, axis=0). if np.all(np.isnan(variances)):. print(""No variances could be computed, check if your cell markers are in the data set.""). print(""Maybe the cell marker IDs do not correspond to your gene_symbol_key input or the var_names""). raise. marker_res_df = pd.DataFrame(marker_res, columns=clusters, index=marker_dict.keys()). #Return the median of the variances over the clusters. return([np.median(variances), marker_res_df]). ```. Function to obtain scores for each marker individually per cluster. ```. #Define cluster score for individual genes. def marker_gene_expression(anndata, marker_dic",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:4093,modifiability,scal,scale,4093,"# marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' . """""". #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = anndata.obs[partition_key].cat.categories. n_clust = len(clusters). marker_exp = pd.DataFrame(columns=clusters). marker_exp['cell_type'] = pd.Series({}, dtype='str'). marker_names = []. . z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. for gene in marker_dict[group]:. ens_idx = np.in1d(gene_ids, gene) #Note there may be multiple mappings. if np.sum(ens_idx) == 0:. continue. else:. z_scores.obs[ens_idx[0]] = z_scores.X[:,ens_idx].mean(1) #works for both single and multiple mapping. ens_idx = ens_idx[0]. clust_marker_exp = z_scores.obs.groupby(partition_key)[ens_idx].apply(np.mean).tolist(). clust_marker_exp.append(group). marker_exp.loc[i] = clust_marker_exp. marker_names.append(gene). i+=1. #Replace the rownames with informative gene symbols. marker_exp.index = marker_names. return(marker_exp). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:1597,performance,scale,scale,1597,"th gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' . #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = np.unique(anndata.obs[partition_key]). n_clust = len(clusters). n_groups = len(marker_dict). . marker_res = np.zeros((n_groups, n_clust)). z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. j = 0. for clust in clusters:. cluster_cells = np.in1d(z_scores.obs[partition_key], clust). marker_genes = np.in1d(gene_ids, marker_dict[group]). marker_res[i,j] = z_scores.X[np.ix_(cluster_cells,marker_genes)].mean(). j += 1. i+=1. variances = np.nanvar(marker_res, axis=0). if np.all(np.isnan(variances)):. print(""No variances could be computed, check if your cell markers are in the data set.""). print(""Maybe the cell marker IDs do not correspond to your gene_symbol_key input or the var_names""). raise. marker_res_df = pd.DataFrame(marker_res, columns=clusters, index=marker_dict.keys()). #Return the median of the variances over the clusters. return([np.median(variances), marker_res_df]). ```. Function to obtain scores for each marker individually per cluster. ```. #Define cluster score for individual genes. def marker_gene_expression(anndata, marker_dic",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:4093,performance,scale,scale,4093,"# marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' . """""". #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = anndata.obs[partition_key].cat.categories. n_clust = len(clusters). marker_exp = pd.DataFrame(columns=clusters). marker_exp['cell_type'] = pd.Series({}, dtype='str'). marker_names = []. . z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. for gene in marker_dict[group]:. ens_idx = np.in1d(gene_ids, gene) #Note there may be multiple mappings. if np.sum(ens_idx) == 0:. continue. else:. z_scores.obs[ens_idx[0]] = z_scores.X[:,ens_idx].mean(1) #works for both single and multiple mapping. ens_idx = ens_idx[0]. clust_marker_exp = z_scores.obs.groupby(partition_key)[ens_idx].apply(np.mean).tolist(). clust_marker_exp.append(group). marker_exp.loc[i] = clust_marker_exp. marker_names.append(gene). i+=1. #Replace the rownames with informative gene symbols. marker_exp.index = marker_names. return(marker_exp). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:284,safety,Input,Inputs,284,"Not sure what the best way of posting this is, but I'll just paste it for now:. Function to score clusters using multiple cell-type markers. ```. #Define cluster score for all markers. def evaluate_partition(anndata, marker_dict, gene_symbol_key=None, partition_key='louvain_r1'):. # Inputs:. # anndata - An AnnData object containing the data set and a partition. # marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' . #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = np.unique(anndata.obs[partition_key]). n_clust = len(clusters). n_groups = len(marker_dict). . marker_res = np.zeros((n_groups, n_clust)). z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. j = 0. for clust in clusters:. cluster_cells = np.in1d(z_scores.obs[partition_key], clust). marker_genes = np.in1d(gene_ids, marker_dict[group]). marker_res[i,j] = z_scores.X[np.ix_(cluster_cells,marker_genes)].mean(). j += 1. i+=1. variances = np.nanvar(marker_r",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:537,safety,input,input,537,"Not sure what the best way of posting this is, but I'll just paste it for now:. Function to score clusters using multiple cell-type markers. ```. #Define cluster score for all markers. def evaluate_partition(anndata, marker_dict, gene_symbol_key=None, partition_key='louvain_r1'):. # Inputs:. # anndata - An AnnData object containing the data set and a partition. # marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' . #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = np.unique(anndata.obs[partition_key]). n_clust = len(clusters). n_groups = len(marker_dict). . marker_res = np.zeros((n_groups, n_clust)). z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. j = 0. for clust in clusters:. cluster_cells = np.in1d(z_scores.obs[partition_key], clust). marker_genes = np.in1d(gene_ids, marker_dict[group]). marker_res[i,j] = z_scores.X[np.ix_(cluster_cells,marker_genes)].mean(). j += 1. i+=1. variances = np.nanvar(marker_r",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:782,safety,Test,Test,782,"Not sure what the best way of posting this is, but I'll just paste it for now:. Function to score clusters using multiple cell-type markers. ```. #Define cluster score for all markers. def evaluate_partition(anndata, marker_dict, gene_symbol_key=None, partition_key='louvain_r1'):. # Inputs:. # anndata - An AnnData object containing the data set and a partition. # marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' . #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = np.unique(anndata.obs[partition_key]). n_clust = len(clusters). n_groups = len(marker_dict). . marker_res = np.zeros((n_groups, n_clust)). z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. j = 0. for clust in clusters:. cluster_cells = np.in1d(z_scores.obs[partition_key], clust). marker_genes = np.in1d(gene_ids, marker_dict[group]). marker_res[i,j] = z_scores.X[np.ix_(cluster_cells,marker_genes)].mean(). j += 1. i+=1. variances = np.nanvar(marker_r",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:787,safety,input,inputs,787,"Not sure what the best way of posting this is, but I'll just paste it for now:. Function to score clusters using multiple cell-type markers. ```. #Define cluster score for all markers. def evaluate_partition(anndata, marker_dict, gene_symbol_key=None, partition_key='louvain_r1'):. # Inputs:. # anndata - An AnnData object containing the data set and a partition. # marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' . #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = np.unique(anndata.obs[partition_key]). n_clust = len(clusters). n_groups = len(marker_dict). . marker_res = np.zeros((n_groups, n_clust)). z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. j = 0. for clust in clusters:. cluster_cells = np.in1d(z_scores.obs[partition_key], clust). marker_genes = np.in1d(gene_ids, marker_dict[group]). marker_res[i,j] = z_scores.X[np.ix_(cluster_cells,marker_genes)].mean(). j += 1. i+=1. variances = np.nanvar(marker_r",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:2211,safety,input,input,2211,"ject.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = np.unique(anndata.obs[partition_key]). n_clust = len(clusters). n_groups = len(marker_dict). . marker_res = np.zeros((n_groups, n_clust)). z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. j = 0. for clust in clusters:. cluster_cells = np.in1d(z_scores.obs[partition_key], clust). marker_genes = np.in1d(gene_ids, marker_dict[group]). marker_res[i,j] = z_scores.X[np.ix_(cluster_cells,marker_genes)].mean(). j += 1. i+=1. variances = np.nanvar(marker_res, axis=0). if np.all(np.isnan(variances)):. print(""No variances could be computed, check if your cell markers are in the data set.""). print(""Maybe the cell marker IDs do not correspond to your gene_symbol_key input or the var_names""). raise. marker_res_df = pd.DataFrame(marker_res, columns=clusters, index=marker_dict.keys()). #Return the median of the variances over the clusters. return([np.median(variances), marker_res_df]). ```. Function to obtain scores for each marker individually per cluster. ```. #Define cluster score for individual genes. def marker_gene_expression(anndata, marker_dict, gene_symbol_key=None, partition_key='louvain_r1'):. """""". A function to get mean z-score expressions of marker genes. # . # Inputs:. # anndata - An AnnData object containing the data set and a partition. # marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louva",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:2726,safety,Input,Inputs,2726,"luster. j = 0. for clust in clusters:. cluster_cells = np.in1d(z_scores.obs[partition_key], clust). marker_genes = np.in1d(gene_ids, marker_dict[group]). marker_res[i,j] = z_scores.X[np.ix_(cluster_cells,marker_genes)].mean(). j += 1. i+=1. variances = np.nanvar(marker_res, axis=0). if np.all(np.isnan(variances)):. print(""No variances could be computed, check if your cell markers are in the data set.""). print(""Maybe the cell marker IDs do not correspond to your gene_symbol_key input or the var_names""). raise. marker_res_df = pd.DataFrame(marker_res, columns=clusters, index=marker_dict.keys()). #Return the median of the variances over the clusters. return([np.median(variances), marker_res_df]). ```. Function to obtain scores for each marker individually per cluster. ```. #Define cluster score for individual genes. def marker_gene_expression(anndata, marker_dict, gene_symbol_key=None, partition_key='louvain_r1'):. """""". A function to get mean z-score expressions of marker genes. # . # Inputs:. # anndata - An AnnData object containing the data set and a partition. # marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' . """""". #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a fo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:2979,safety,input,input,2979,"np.nanvar(marker_res, axis=0). if np.all(np.isnan(variances)):. print(""No variances could be computed, check if your cell markers are in the data set.""). print(""Maybe the cell marker IDs do not correspond to your gene_symbol_key input or the var_names""). raise. marker_res_df = pd.DataFrame(marker_res, columns=clusters, index=marker_dict.keys()). #Return the median of the variances over the clusters. return([np.median(variances), marker_res_df]). ```. Function to obtain scores for each marker individually per cluster. ```. #Define cluster score for individual genes. def marker_gene_expression(anndata, marker_dict, gene_symbol_key=None, partition_key='louvain_r1'):. """""". A function to get mean z-score expressions of marker genes. # . # Inputs:. # anndata - An AnnData object containing the data set and a partition. # marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' . """""". #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = anndata.obs[partition_key].cat.categories. n_clust = len(clusters). marker_exp = pd.DataFrame(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:3229,safety,Test,Test,3229,"es""). raise. marker_res_df = pd.DataFrame(marker_res, columns=clusters, index=marker_dict.keys()). #Return the median of the variances over the clusters. return([np.median(variances), marker_res_df]). ```. Function to obtain scores for each marker individually per cluster. ```. #Define cluster score for individual genes. def marker_gene_expression(anndata, marker_dict, gene_symbol_key=None, partition_key='louvain_r1'):. """""". A function to get mean z-score expressions of marker genes. # . # Inputs:. # anndata - An AnnData object containing the data set and a partition. # marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' . """""". #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = anndata.obs[partition_key].cat.categories. n_clust = len(clusters). marker_exp = pd.DataFrame(columns=clusters). marker_exp['cell_type'] = pd.Series({}, dtype='str'). marker_names = []. . z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:3234,safety,input,inputs,3234,"raise. marker_res_df = pd.DataFrame(marker_res, columns=clusters, index=marker_dict.keys()). #Return the median of the variances over the clusters. return([np.median(variances), marker_res_df]). ```. Function to obtain scores for each marker individually per cluster. ```. #Define cluster score for individual genes. def marker_gene_expression(anndata, marker_dict, gene_symbol_key=None, partition_key='louvain_r1'):. """""". A function to get mean z-score expressions of marker genes. # . # Inputs:. # anndata - An AnnData object containing the data set and a partition. # marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' . """""". #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = anndata.obs[partition_key].cat.categories. n_clust = len(clusters). marker_exp = pd.DataFrame(columns=clusters). marker_exp['cell_type'] = pd.Series({}, dtype='str'). marker_names = []. . z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. for ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:782,testability,Test,Test,782,"Not sure what the best way of posting this is, but I'll just paste it for now:. Function to score clusters using multiple cell-type markers. ```. #Define cluster score for all markers. def evaluate_partition(anndata, marker_dict, gene_symbol_key=None, partition_key='louvain_r1'):. # Inputs:. # anndata - An AnnData object containing the data set and a partition. # marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' . #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = np.unique(anndata.obs[partition_key]). n_clust = len(clusters). n_groups = len(marker_dict). . marker_res = np.zeros((n_groups, n_clust)). z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. j = 0. for clust in clusters:. cluster_cells = np.in1d(z_scores.obs[partition_key], clust). marker_genes = np.in1d(gene_ids, marker_dict[group]). marker_res[i,j] = z_scores.X[np.ix_(cluster_cells,marker_genes)].mean(). j += 1. i+=1. variances = np.nanvar(marker_r",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:3229,testability,Test,Test,3229,"es""). raise. marker_res_df = pd.DataFrame(marker_res, columns=clusters, index=marker_dict.keys()). #Return the median of the variances over the clusters. return([np.median(variances), marker_res_df]). ```. Function to obtain scores for each marker individually per cluster. ```. #Define cluster score for individual genes. def marker_gene_expression(anndata, marker_dict, gene_symbol_key=None, partition_key='louvain_r1'):. """""". A function to get mean z-score expressions of marker genes. # . # Inputs:. # anndata - An AnnData object containing the data set and a partition. # marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' . """""". #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = anndata.obs[partition_key].cat.categories. n_clust = len(clusters). marker_exp = pd.DataFrame(columns=clusters). marker_exp['cell_type'] = pd.Series({}, dtype='str'). marker_names = []. . z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:284,usability,Input,Inputs,284,"Not sure what the best way of posting this is, but I'll just paste it for now:. Function to score clusters using multiple cell-type markers. ```. #Define cluster score for all markers. def evaluate_partition(anndata, marker_dict, gene_symbol_key=None, partition_key='louvain_r1'):. # Inputs:. # anndata - An AnnData object containing the data set and a partition. # marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' . #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = np.unique(anndata.obs[partition_key]). n_clust = len(clusters). n_groups = len(marker_dict). . marker_res = np.zeros((n_groups, n_clust)). z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. j = 0. for clust in clusters:. cluster_cells = np.in1d(z_scores.obs[partition_key], clust). marker_genes = np.in1d(gene_ids, marker_dict[group]). marker_res[i,j] = z_scores.X[np.ix_(cluster_cells,marker_genes)].mean(). j += 1. i+=1. variances = np.nanvar(marker_r",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:537,usability,input,input,537,"Not sure what the best way of posting this is, but I'll just paste it for now:. Function to score clusters using multiple cell-type markers. ```. #Define cluster score for all markers. def evaluate_partition(anndata, marker_dict, gene_symbol_key=None, partition_key='louvain_r1'):. # Inputs:. # anndata - An AnnData object containing the data set and a partition. # marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' . #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = np.unique(anndata.obs[partition_key]). n_clust = len(clusters). n_groups = len(marker_dict). . marker_res = np.zeros((n_groups, n_clust)). z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. j = 0. for clust in clusters:. cluster_cells = np.in1d(z_scores.obs[partition_key], clust). marker_genes = np.in1d(gene_ids, marker_dict[group]). marker_res[i,j] = z_scores.X[np.ix_(cluster_cells,marker_genes)].mean(). j += 1. i+=1. variances = np.nanvar(marker_r",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:787,usability,input,inputs,787,"Not sure what the best way of posting this is, but I'll just paste it for now:. Function to score clusters using multiple cell-type markers. ```. #Define cluster score for all markers. def evaluate_partition(anndata, marker_dict, gene_symbol_key=None, partition_key='louvain_r1'):. # Inputs:. # anndata - An AnnData object containing the data set and a partition. # marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' . #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = np.unique(anndata.obs[partition_key]). n_clust = len(clusters). n_groups = len(marker_dict). . marker_res = np.zeros((n_groups, n_clust)). z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. j = 0. for clust in clusters:. cluster_cells = np.in1d(z_scores.obs[partition_key], clust). marker_genes = np.in1d(gene_ids, marker_dict[group]). marker_res[i,j] = z_scores.X[np.ix_(cluster_cells,marker_genes)].mean(). j += 1. i+=1. variances = np.nanvar(marker_r",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:2211,usability,input,input,2211,"ject.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = np.unique(anndata.obs[partition_key]). n_clust = len(clusters). n_groups = len(marker_dict). . marker_res = np.zeros((n_groups, n_clust)). z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. j = 0. for clust in clusters:. cluster_cells = np.in1d(z_scores.obs[partition_key], clust). marker_genes = np.in1d(gene_ids, marker_dict[group]). marker_res[i,j] = z_scores.X[np.ix_(cluster_cells,marker_genes)].mean(). j += 1. i+=1. variances = np.nanvar(marker_res, axis=0). if np.all(np.isnan(variances)):. print(""No variances could be computed, check if your cell markers are in the data set.""). print(""Maybe the cell marker IDs do not correspond to your gene_symbol_key input or the var_names""). raise. marker_res_df = pd.DataFrame(marker_res, columns=clusters, index=marker_dict.keys()). #Return the median of the variances over the clusters. return([np.median(variances), marker_res_df]). ```. Function to obtain scores for each marker individually per cluster. ```. #Define cluster score for individual genes. def marker_gene_expression(anndata, marker_dict, gene_symbol_key=None, partition_key='louvain_r1'):. """""". A function to get mean z-score expressions of marker genes. # . # Inputs:. # anndata - An AnnData object containing the data set and a partition. # marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louva",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:2726,usability,Input,Inputs,2726,"luster. j = 0. for clust in clusters:. cluster_cells = np.in1d(z_scores.obs[partition_key], clust). marker_genes = np.in1d(gene_ids, marker_dict[group]). marker_res[i,j] = z_scores.X[np.ix_(cluster_cells,marker_genes)].mean(). j += 1. i+=1. variances = np.nanvar(marker_res, axis=0). if np.all(np.isnan(variances)):. print(""No variances could be computed, check if your cell markers are in the data set.""). print(""Maybe the cell marker IDs do not correspond to your gene_symbol_key input or the var_names""). raise. marker_res_df = pd.DataFrame(marker_res, columns=clusters, index=marker_dict.keys()). #Return the median of the variances over the clusters. return([np.median(variances), marker_res_df]). ```. Function to obtain scores for each marker individually per cluster. ```. #Define cluster score for individual genes. def marker_gene_expression(anndata, marker_dict, gene_symbol_key=None, partition_key='louvain_r1'):. """""". A function to get mean z-score expressions of marker genes. # . # Inputs:. # anndata - An AnnData object containing the data set and a partition. # marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' . """""". #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a fo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:2979,usability,input,input,2979,"np.nanvar(marker_res, axis=0). if np.all(np.isnan(variances)):. print(""No variances could be computed, check if your cell markers are in the data set.""). print(""Maybe the cell marker IDs do not correspond to your gene_symbol_key input or the var_names""). raise. marker_res_df = pd.DataFrame(marker_res, columns=clusters, index=marker_dict.keys()). #Return the median of the variances over the clusters. return([np.median(variances), marker_res_df]). ```. Function to obtain scores for each marker individually per cluster. ```. #Define cluster score for individual genes. def marker_gene_expression(anndata, marker_dict, gene_symbol_key=None, partition_key='louvain_r1'):. """""". A function to get mean z-score expressions of marker genes. # . # Inputs:. # anndata - An AnnData object containing the data set and a partition. # marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' . """""". #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = anndata.obs[partition_key].cat.categories. n_clust = len(clusters). marker_exp = pd.DataFrame(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:3234,usability,input,inputs,3234,"raise. marker_res_df = pd.DataFrame(marker_res, columns=clusters, index=marker_dict.keys()). #Return the median of the variances over the clusters. return([np.median(variances), marker_res_df]). ```. Function to obtain scores for each marker individually per cluster. ```. #Define cluster score for individual genes. def marker_gene_expression(anndata, marker_dict, gene_symbol_key=None, partition_key='louvain_r1'):. """""". A function to get mean z-score expressions of marker genes. # . # Inputs:. # anndata - An AnnData object containing the data set and a partition. # marker_dict - A dictionary with cell-type markers. The markers should be stores as anndata.var_names or . # an anndata.var field with the key given by the gene_symbol_key input. # gene_symbol_key - The key for the anndata.var field with gene IDs or names that correspond to the marker . # genes. # partition_key - The key for the anndata.obs field where the cluster IDs are stored. The default is. # 'louvain_r1' . """""". #Test inputs. if partition_key not in anndata.obs.columns.values:. print('KeyError: The partition key was not found in the passed AnnData object.'). print(' Have you done the clustering? If so, please tell pass the cluster IDs with the AnnData object!'). raise. if (gene_symbol_key != None) and (gene_symbol_key not in anndata.var.columns.values):. print('KeyError: The provided gene symbol key was not found in the passed AnnData object.'). print(' Check that your cell type markers are given in a format that your anndata object knows!'). raise. . if gene_symbol_key:. gene_ids = anndata.var[gene_symbol_key]. else:. gene_ids = anndata.var_names. clusters = anndata.obs[partition_key].cat.categories. n_clust = len(clusters). marker_exp = pd.DataFrame(columns=clusters). marker_exp['cell_type'] = pd.Series({}, dtype='str'). marker_names = []. . z_scores = sc.pp.scale(anndata, copy=True). i = 0. for group in marker_dict:. # Find the corresponding columns and get their mean expression in the cluster. for ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:134,availability,cluster,cluster,134,"> I was considering to use the same tests from scanpy to identify marker genes but with a given set of markers as I want to know if a cluster could be annotated with a marker (which is different than to annotate a single cell). Any thoughts on this idea? I think that makes sense too. Without multiple testing correction, I feel like that should be equivalent to just filtering the results for the marker genes you have. And as the scores/p-values of that test are not really measures of significance (#270), it would be difficult to evaluate whether the score/p-value is sufficient to assign the cluster annotation. However, this approach is no worse than mine... I wonder how you can evaluate these approaches? Is there a dataset with very similar cells? Maybe gut with goblet and tuft cells appearing annoyingly similar (@mbuttner)?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:597,availability,cluster,cluster,597,"> I was considering to use the same tests from scanpy to identify marker genes but with a given set of markers as I want to know if a cluster could be annotated with a marker (which is different than to annotate a single cell). Any thoughts on this idea? I think that makes sense too. Without multiple testing correction, I feel like that should be equivalent to just filtering the results for the marker genes you have. And as the scores/p-values of that test are not really measures of significance (#270), it would be difficult to evaluate whether the score/p-value is sufficient to assign the cluster annotation. However, this approach is no worse than mine... I wonder how you can evaluate these approaches? Is there a dataset with very similar cells? Maybe gut with goblet and tuft cells appearing annoyingly similar (@mbuttner)?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:134,deployability,cluster,cluster,134,"> I was considering to use the same tests from scanpy to identify marker genes but with a given set of markers as I want to know if a cluster could be annotated with a marker (which is different than to annotate a single cell). Any thoughts on this idea? I think that makes sense too. Without multiple testing correction, I feel like that should be equivalent to just filtering the results for the marker genes you have. And as the scores/p-values of that test are not really measures of significance (#270), it would be difficult to evaluate whether the score/p-value is sufficient to assign the cluster annotation. However, this approach is no worse than mine... I wonder how you can evaluate these approaches? Is there a dataset with very similar cells? Maybe gut with goblet and tuft cells appearing annoyingly similar (@mbuttner)?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:597,deployability,cluster,cluster,597,"> I was considering to use the same tests from scanpy to identify marker genes but with a given set of markers as I want to know if a cluster could be annotated with a marker (which is different than to annotate a single cell). Any thoughts on this idea? I think that makes sense too. Without multiple testing correction, I feel like that should be equivalent to just filtering the results for the marker genes you have. And as the scores/p-values of that test are not really measures of significance (#270), it would be difficult to evaluate whether the score/p-value is sufficient to assign the cluster annotation. However, this approach is no worse than mine... I wonder how you can evaluate these approaches? Is there a dataset with very similar cells? Maybe gut with goblet and tuft cells appearing annoyingly similar (@mbuttner)?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:476,energy efficiency,measur,measures,476,"> I was considering to use the same tests from scanpy to identify marker genes but with a given set of markers as I want to know if a cluster could be annotated with a marker (which is different than to annotate a single cell). Any thoughts on this idea? I think that makes sense too. Without multiple testing correction, I feel like that should be equivalent to just filtering the results for the marker genes you have. And as the scores/p-values of that test are not really measures of significance (#270), it would be difficult to evaluate whether the score/p-value is sufficient to assign the cluster annotation. However, this approach is no worse than mine... I wonder how you can evaluate these approaches? Is there a dataset with very similar cells? Maybe gut with goblet and tuft cells appearing annoyingly similar (@mbuttner)?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:368,integrability,filter,filtering,368,"> I was considering to use the same tests from scanpy to identify marker genes but with a given set of markers as I want to know if a cluster could be annotated with a marker (which is different than to annotate a single cell). Any thoughts on this idea? I think that makes sense too. Without multiple testing correction, I feel like that should be equivalent to just filtering the results for the marker genes you have. And as the scores/p-values of that test are not really measures of significance (#270), it would be difficult to evaluate whether the score/p-value is sufficient to assign the cluster annotation. However, this approach is no worse than mine... I wonder how you can evaluate these approaches? Is there a dataset with very similar cells? Maybe gut with goblet and tuft cells appearing annoyingly similar (@mbuttner)?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:36,safety,test,tests,36,"> I was considering to use the same tests from scanpy to identify marker genes but with a given set of markers as I want to know if a cluster could be annotated with a marker (which is different than to annotate a single cell). Any thoughts on this idea? I think that makes sense too. Without multiple testing correction, I feel like that should be equivalent to just filtering the results for the marker genes you have. And as the scores/p-values of that test are not really measures of significance (#270), it would be difficult to evaluate whether the score/p-value is sufficient to assign the cluster annotation. However, this approach is no worse than mine... I wonder how you can evaluate these approaches? Is there a dataset with very similar cells? Maybe gut with goblet and tuft cells appearing annoyingly similar (@mbuttner)?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:302,safety,test,testing,302,"> I was considering to use the same tests from scanpy to identify marker genes but with a given set of markers as I want to know if a cluster could be annotated with a marker (which is different than to annotate a single cell). Any thoughts on this idea? I think that makes sense too. Without multiple testing correction, I feel like that should be equivalent to just filtering the results for the marker genes you have. And as the scores/p-values of that test are not really measures of significance (#270), it would be difficult to evaluate whether the score/p-value is sufficient to assign the cluster annotation. However, this approach is no worse than mine... I wonder how you can evaluate these approaches? Is there a dataset with very similar cells? Maybe gut with goblet and tuft cells appearing annoyingly similar (@mbuttner)?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:456,safety,test,test,456,"> I was considering to use the same tests from scanpy to identify marker genes but with a given set of markers as I want to know if a cluster could be annotated with a marker (which is different than to annotate a single cell). Any thoughts on this idea? I think that makes sense too. Without multiple testing correction, I feel like that should be equivalent to just filtering the results for the marker genes you have. And as the scores/p-values of that test are not really measures of significance (#270), it would be difficult to evaluate whether the score/p-value is sufficient to assign the cluster annotation. However, this approach is no worse than mine... I wonder how you can evaluate these approaches? Is there a dataset with very similar cells? Maybe gut with goblet and tuft cells appearing annoyingly similar (@mbuttner)?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:57,security,ident,identify,57,"> I was considering to use the same tests from scanpy to identify marker genes but with a given set of markers as I want to know if a cluster could be annotated with a marker (which is different than to annotate a single cell). Any thoughts on this idea? I think that makes sense too. Without multiple testing correction, I feel like that should be equivalent to just filtering the results for the marker genes you have. And as the scores/p-values of that test are not really measures of significance (#270), it would be difficult to evaluate whether the score/p-value is sufficient to assign the cluster annotation. However, this approach is no worse than mine... I wonder how you can evaluate these approaches? Is there a dataset with very similar cells? Maybe gut with goblet and tuft cells appearing annoyingly similar (@mbuttner)?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:488,security,sign,significance,488,"> I was considering to use the same tests from scanpy to identify marker genes but with a given set of markers as I want to know if a cluster could be annotated with a marker (which is different than to annotate a single cell). Any thoughts on this idea? I think that makes sense too. Without multiple testing correction, I feel like that should be equivalent to just filtering the results for the marker genes you have. And as the scores/p-values of that test are not really measures of significance (#270), it would be difficult to evaluate whether the score/p-value is sufficient to assign the cluster annotation. However, this approach is no worse than mine... I wonder how you can evaluate these approaches? Is there a dataset with very similar cells? Maybe gut with goblet and tuft cells appearing annoyingly similar (@mbuttner)?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:36,testability,test,tests,36,"> I was considering to use the same tests from scanpy to identify marker genes but with a given set of markers as I want to know if a cluster could be annotated with a marker (which is different than to annotate a single cell). Any thoughts on this idea? I think that makes sense too. Without multiple testing correction, I feel like that should be equivalent to just filtering the results for the marker genes you have. And as the scores/p-values of that test are not really measures of significance (#270), it would be difficult to evaluate whether the score/p-value is sufficient to assign the cluster annotation. However, this approach is no worse than mine... I wonder how you can evaluate these approaches? Is there a dataset with very similar cells? Maybe gut with goblet and tuft cells appearing annoyingly similar (@mbuttner)?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:302,testability,test,testing,302,"> I was considering to use the same tests from scanpy to identify marker genes but with a given set of markers as I want to know if a cluster could be annotated with a marker (which is different than to annotate a single cell). Any thoughts on this idea? I think that makes sense too. Without multiple testing correction, I feel like that should be equivalent to just filtering the results for the marker genes you have. And as the scores/p-values of that test are not really measures of significance (#270), it would be difficult to evaluate whether the score/p-value is sufficient to assign the cluster annotation. However, this approach is no worse than mine... I wonder how you can evaluate these approaches? Is there a dataset with very similar cells? Maybe gut with goblet and tuft cells appearing annoyingly similar (@mbuttner)?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:456,testability,test,test,456,"> I was considering to use the same tests from scanpy to identify marker genes but with a given set of markers as I want to know if a cluster could be annotated with a marker (which is different than to annotate a single cell). Any thoughts on this idea? I think that makes sense too. Without multiple testing correction, I feel like that should be equivalent to just filtering the results for the marker genes you have. And as the scores/p-values of that test are not really measures of significance (#270), it would be difficult to evaluate whether the score/p-value is sufficient to assign the cluster annotation. However, this approach is no worse than mine... I wonder how you can evaluate these approaches? Is there a dataset with very similar cells? Maybe gut with goblet and tuft cells appearing annoyingly similar (@mbuttner)?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/issues/290:196,energy efficiency,reduc,reduced,196,"@LuckyMD Thanks for sharing your code, I will try it. As for a dataset with very similar cells, I think the pbmc68k has T-cells that are very similar to each other. You can take a quick look at a reduced datase by doing:. ```PYTHON. adata = sc.datasets.pbmc68k_reduced(). sc.pl.umap(adata, color='bulk_labels'). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/290
https://github.com/scverse/scanpy/pull/291:329,integrability,event,event-,329,"One bug less... On Tue, Oct 9, 2018 at 8:03 PM Alex Wolf <notifications@github.com> wrote:. > Merged #291 <https://github.com/theislab/scanpy/pull/291> into master. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/291#event-1893554964>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1e9MrVDv71IFmUI6wXkWV7tII6Xiks5ujOTjgaJpZM4XTKyN>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/291
https://github.com/scverse/scanpy/pull/291:210,security,auth,authored,210,"One bug less... On Tue, Oct 9, 2018 at 8:03 PM Alex Wolf <notifications@github.com> wrote:. > Merged #291 <https://github.com/theislab/scanpy/pull/291> into master. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/291#event-1893554964>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1e9MrVDv71IFmUI6wXkWV7tII6Xiks5ujOTjgaJpZM4XTKyN>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/291
https://github.com/scverse/scanpy/pull/291:419,security,auth,auth,419,"One bug less... On Tue, Oct 9, 2018 at 8:03 PM Alex Wolf <notifications@github.com> wrote:. > Merged #291 <https://github.com/theislab/scanpy/pull/291> into master. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/291#event-1893554964>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1e9MrVDv71IFmUI6wXkWV7tII6Xiks5ujOTjgaJpZM4XTKyN>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/291
https://github.com/scverse/scanpy/pull/292:19,energy efficiency,adapt,adapt,19,Thank you! Can you adapt the doc string so that it matches the other tools. We need numpydoc style documentation for it to render properly. You can also check whether it looks good by running `make html` in the docs folder.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/292
https://github.com/scverse/scanpy/pull/292:19,integrability,adapt,adapt,19,Thank you! Can you adapt the doc string so that it matches the other tools. We need numpydoc style documentation for it to render properly. You can also check whether it looks good by running `make html` in the docs folder.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/292
https://github.com/scverse/scanpy/pull/292:19,interoperability,adapt,adapt,19,Thank you! Can you adapt the doc string so that it matches the other tools. We need numpydoc style documentation for it to render properly. You can also check whether it looks good by running `make html` in the docs folder.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/292
https://github.com/scverse/scanpy/pull/292:19,modifiability,adapt,adapt,19,Thank you! Can you adapt the doc string so that it matches the other tools. We need numpydoc style documentation for it to render properly. You can also check whether it looks good by running `make html` in the docs folder.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/292
https://github.com/scverse/scanpy/pull/292:69,usability,tool,tools,69,Thank you! Can you adapt the doc string so that it matches the other tools. We need numpydoc style documentation for it to render properly. You can also check whether it looks good by running `make html` in the docs folder.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/292
https://github.com/scverse/scanpy/pull/292:99,usability,document,documentation,99,Thank you! Can you adapt the doc string so that it matches the other tools. We need numpydoc style documentation for it to render properly. You can also check whether it looks good by running `make html` in the docs folder.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/292
https://github.com/scverse/scanpy/pull/292:185,energy efficiency,adapt,adapted,185,"Sorry about this taking so long, I'm waiting until we have the new way of handling extensions in place... It will only be another couple of days and then this is going to be merged and adapted to that... There's nothing to do from your end on this... Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/292
https://github.com/scverse/scanpy/pull/292:130,integrability,coupl,couple,130,"Sorry about this taking so long, I'm waiting until we have the new way of handling extensions in place... It will only be another couple of days and then this is going to be merged and adapted to that... There's nothing to do from your end on this... Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/292
https://github.com/scverse/scanpy/pull/292:185,integrability,adapt,adapted,185,"Sorry about this taking so long, I'm waiting until we have the new way of handling extensions in place... It will only be another couple of days and then this is going to be merged and adapted to that... There's nothing to do from your end on this... Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/292
https://github.com/scverse/scanpy/pull/292:185,interoperability,adapt,adapted,185,"Sorry about this taking so long, I'm waiting until we have the new way of handling extensions in place... It will only be another couple of days and then this is going to be merged and adapted to that... There's nothing to do from your end on this... Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/292
https://github.com/scverse/scanpy/pull/292:83,modifiability,extens,extensions,83,"Sorry about this taking so long, I'm waiting until we have the new way of handling extensions in place... It will only be another couple of days and then this is going to be merged and adapted to that... There's nothing to do from your end on this... Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/292
https://github.com/scverse/scanpy/pull/292:130,modifiability,coupl,couple,130,"Sorry about this taking so long, I'm waiting until we have the new way of handling extensions in place... It will only be another couple of days and then this is going to be merged and adapted to that... There's nothing to do from your end on this... Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/292
https://github.com/scverse/scanpy/pull/292:185,modifiability,adapt,adapted,185,"Sorry about this taking so long, I'm waiting until we have the new way of handling extensions in place... It will only be another couple of days and then this is going to be merged and adapted to that... There's nothing to do from your end on this... Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/292
https://github.com/scverse/scanpy/pull/292:130,testability,coupl,couple,130,"Sorry about this taking so long, I'm waiting until we have the new way of handling extensions in place... It will only be another couple of days and then this is going to be merged and adapted to that... There's nothing to do from your end on this... Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/292
https://github.com/scverse/scanpy/pull/292:68,modifiability,extens,extensions,68,@falexwolf what is that new way? isn’t there a tracking issue where extensions should be discussed and you said we don’t need them at all?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/292
https://github.com/scverse/scanpy/pull/292:2,testability,simpl,simply,2,I simply meant to do what I described in that issue: https://github.com/theislab/scanpy/issues/271#issuecomment-431634492,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/292
https://github.com/scverse/scanpy/pull/292:2,usability,simpl,simply,2,I simply meant to do what I described in that issue: https://github.com/theislab/scanpy/issues/271#issuecomment-431634492,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/292
https://github.com/scverse/scanpy/pull/292:63,integrability,sub,submodule,63,"I'm finally merging this as we know have the `scanpy.external` submodule, where this can be properly linked - along with PHATE and all other wrappers to external single-cell packages. Sorry for that this took so long, I only could do this during the past calm couple of days. Thank you for the PR! A very happy new year to you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/292
https://github.com/scverse/scanpy/pull/292:141,integrability,wrap,wrappers,141,"I'm finally merging this as we know have the `scanpy.external` submodule, where this can be properly linked - along with PHATE and all other wrappers to external single-cell packages. Sorry for that this took so long, I only could do this during the past calm couple of days. Thank you for the PR! A very happy new year to you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/292
https://github.com/scverse/scanpy/pull/292:260,integrability,coupl,couple,260,"I'm finally merging this as we know have the `scanpy.external` submodule, where this can be properly linked - along with PHATE and all other wrappers to external single-cell packages. Sorry for that this took so long, I only could do this during the past calm couple of days. Thank you for the PR! A very happy new year to you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/292
https://github.com/scverse/scanpy/pull/292:141,interoperability,wrapper,wrappers,141,"I'm finally merging this as we know have the `scanpy.external` submodule, where this can be properly linked - along with PHATE and all other wrappers to external single-cell packages. Sorry for that this took so long, I only could do this during the past calm couple of days. Thank you for the PR! A very happy new year to you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/292
https://github.com/scverse/scanpy/pull/292:174,modifiability,pac,packages,174,"I'm finally merging this as we know have the `scanpy.external` submodule, where this can be properly linked - along with PHATE and all other wrappers to external single-cell packages. Sorry for that this took so long, I only could do this during the past calm couple of days. Thank you for the PR! A very happy new year to you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/292
https://github.com/scverse/scanpy/pull/292:260,modifiability,coupl,couple,260,"I'm finally merging this as we know have the `scanpy.external` submodule, where this can be properly linked - along with PHATE and all other wrappers to external single-cell packages. Sorry for that this took so long, I only could do this during the past calm couple of days. Thank you for the PR! A very happy new year to you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/292
https://github.com/scverse/scanpy/pull/292:260,testability,coupl,couple,260,"I'm finally merging this as we know have the `scanpy.external` submodule, where this can be properly linked - along with PHATE and all other wrappers to external single-cell packages. Sorry for that this took so long, I only could do this during the past calm couple of days. Thank you for the PR! A very happy new year to you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/292
https://github.com/scverse/scanpy/issues/293:193,deployability,version,version,193,"I just checked and I am getting solid circles:. ![image](https://user-images.githubusercontent.com/4964309/46802863-ee187400-cd5e-11e8-9d05-282f577df70c.png). Are you using the same matplotlib version in both cases? From your images it looks like if the alpha value is set to almost completely transparent, but since the edges in matplotlib do not become transparent you end up seeing circles. Another reason that I can think of for these results is that your marker type is set to `'o'`. Can you try to reset your rcParams? (`sc.pl.set_rcParams_defaults()`)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/293
https://github.com/scverse/scanpy/issues/293:193,integrability,version,version,193,"I just checked and I am getting solid circles:. ![image](https://user-images.githubusercontent.com/4964309/46802863-ee187400-cd5e-11e8-9d05-282f577df70c.png). Are you using the same matplotlib version in both cases? From your images it looks like if the alpha value is set to almost completely transparent, but since the edges in matplotlib do not become transparent you end up seeing circles. Another reason that I can think of for these results is that your marker type is set to `'o'`. Can you try to reset your rcParams? (`sc.pl.set_rcParams_defaults()`)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/293
https://github.com/scverse/scanpy/issues/293:193,modifiability,version,version,193,"I just checked and I am getting solid circles:. ![image](https://user-images.githubusercontent.com/4964309/46802863-ee187400-cd5e-11e8-9d05-282f577df70c.png). Are you using the same matplotlib version in both cases? From your images it looks like if the alpha value is set to almost completely transparent, but since the edges in matplotlib do not become transparent you end up seeing circles. Another reason that I can think of for these results is that your marker type is set to `'o'`. Can you try to reset your rcParams? (`sc.pl.set_rcParams_defaults()`)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/293
https://github.com/scverse/scanpy/issues/293:283,safety,compl,completely,283,"I just checked and I am getting solid circles:. ![image](https://user-images.githubusercontent.com/4964309/46802863-ee187400-cd5e-11e8-9d05-282f577df70c.png). Are you using the same matplotlib version in both cases? From your images it looks like if the alpha value is set to almost completely transparent, but since the edges in matplotlib do not become transparent you end up seeing circles. Another reason that I can think of for these results is that your marker type is set to `'o'`. Can you try to reset your rcParams? (`sc.pl.set_rcParams_defaults()`)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/293
https://github.com/scverse/scanpy/issues/293:283,security,compl,completely,283,"I just checked and I am getting solid circles:. ![image](https://user-images.githubusercontent.com/4964309/46802863-ee187400-cd5e-11e8-9d05-282f577df70c.png). Are you using the same matplotlib version in both cases? From your images it looks like if the alpha value is set to almost completely transparent, but since the edges in matplotlib do not become transparent you end up seeing circles. Another reason that I can think of for these results is that your marker type is set to `'o'`. Can you try to reset your rcParams? (`sc.pl.set_rcParams_defaults()`)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/293
https://github.com/scverse/scanpy/issues/293:65,usability,user,user-images,65,"I just checked and I am getting solid circles:. ![image](https://user-images.githubusercontent.com/4964309/46802863-ee187400-cd5e-11e8-9d05-282f577df70c.png). Are you using the same matplotlib version in both cases? From your images it looks like if the alpha value is set to almost completely transparent, but since the edges in matplotlib do not become transparent you end up seeing circles. Another reason that I can think of for these results is that your marker type is set to `'o'`. Can you try to reset your rcParams? (`sc.pl.set_rcParams_defaults()`)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/293
https://github.com/scverse/scanpy/issues/293:395,reliability,doe,doesn,395,"`edgecolor='none'` seems to work:. ![edgecolor_none](https://user-images.githubusercontent.com/8238804/46846796-fd65f300-ce2c-11e8-8273-f2cd53a41389.png). ![closeup_edgecolor_none](https://user-images.githubusercontent.com/8238804/46847011-f7244680-ce2d-11e8-9647-e33714f816a4.png). But I'd think that shouldn't do anything if I haven't passed an `edges` argument, right? Reseting my `rcParams` doesn't seem to do anything. Using matplotlib v3.0.0, I get the outline behavior with Scanpy v1.3.2, but not for Scanpy at 1ec7af2.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/293
https://github.com/scverse/scanpy/issues/293:61,usability,user,user-images,61,"`edgecolor='none'` seems to work:. ![edgecolor_none](https://user-images.githubusercontent.com/8238804/46846796-fd65f300-ce2c-11e8-8273-f2cd53a41389.png). ![closeup_edgecolor_none](https://user-images.githubusercontent.com/8238804/46847011-f7244680-ce2d-11e8-9647-e33714f816a4.png). But I'd think that shouldn't do anything if I haven't passed an `edges` argument, right? Reseting my `rcParams` doesn't seem to do anything. Using matplotlib v3.0.0, I get the outline behavior with Scanpy v1.3.2, but not for Scanpy at 1ec7af2.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/293
https://github.com/scverse/scanpy/issues/293:189,usability,user,user-images,189,"`edgecolor='none'` seems to work:. ![edgecolor_none](https://user-images.githubusercontent.com/8238804/46846796-fd65f300-ce2c-11e8-8273-f2cd53a41389.png). ![closeup_edgecolor_none](https://user-images.githubusercontent.com/8238804/46847011-f7244680-ce2d-11e8-9647-e33714f816a4.png). But I'd think that shouldn't do anything if I haven't passed an `edges` argument, right? Reseting my `rcParams` doesn't seem to do anything. Using matplotlib v3.0.0, I get the outline behavior with Scanpy v1.3.2, but not for Scanpy at 1ec7af2.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/293
https://github.com/scverse/scanpy/issues/293:467,usability,behavi,behavior,467,"`edgecolor='none'` seems to work:. ![edgecolor_none](https://user-images.githubusercontent.com/8238804/46846796-fd65f300-ce2c-11e8-8273-f2cd53a41389.png). ![closeup_edgecolor_none](https://user-images.githubusercontent.com/8238804/46847011-f7244680-ce2d-11e8-9647-e33714f816a4.png). But I'd think that shouldn't do anything if I haven't passed an `edges` argument, right? Reseting my `rcParams` doesn't seem to do anything. Using matplotlib v3.0.0, I get the outline behavior with Scanpy v1.3.2, but not for Scanpy at 1ec7af2.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/293
https://github.com/scverse/scanpy/issues/293:78,availability,replic,replicate,78,"I don't need to manually pass size with `edgecolor='none'`. I'll see if I can replicate in a conda environment, and then try to cut down the example a bit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/293
https://github.com/scverse/scanpy/issues/293:132,availability,down,down,132,"I don't need to manually pass size with `edgecolor='none'`. I'll see if I can replicate in a conda environment, and then try to cut down the example a bit.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/293
https://github.com/scverse/scanpy/issues/293:365,deployability,api,api,365,"[Here are conda environment files](https://gist.github.com/ivirshup/505a63e54d0aad6ed6289cd1e080a8a2) for mac and linux which reproduce this behavior for me. I've also included a little script which should generate a plot demonstrating the issue [(here's an example output)](https://github.com/theislab/scanpy/files/2472189/pcatest.pdf). . ```python. import scanpy.api as sc. import numpy as np. import pandas as pd. sc.set_figure_params(dpi=300, dpi_save=300) # Makes it more visible. N = 1000. M = 2000. adata = sc.AnnData(. X=np.random.random_sample((N, M)),. obs=pd.DataFrame({""a"": np.random.randint(0, 2, N)}). ). adata.obs[""a""] = adata.obs[""a""].astype(str). sc.pp.pca(adata). sc.pl.pca(adata, color=""a"", size=0.1, save=""test.pdf""). ```. Could you try running this and let me know if you get a similar result?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/293
https://github.com/scverse/scanpy/issues/293:365,integrability,api,api,365,"[Here are conda environment files](https://gist.github.com/ivirshup/505a63e54d0aad6ed6289cd1e080a8a2) for mac and linux which reproduce this behavior for me. I've also included a little script which should generate a plot demonstrating the issue [(here's an example output)](https://github.com/theislab/scanpy/files/2472189/pcatest.pdf). . ```python. import scanpy.api as sc. import numpy as np. import pandas as pd. sc.set_figure_params(dpi=300, dpi_save=300) # Makes it more visible. N = 1000. M = 2000. adata = sc.AnnData(. X=np.random.random_sample((N, M)),. obs=pd.DataFrame({""a"": np.random.randint(0, 2, N)}). ). adata.obs[""a""] = adata.obs[""a""].astype(str). sc.pp.pca(adata). sc.pl.pca(adata, color=""a"", size=0.1, save=""test.pdf""). ```. Could you try running this and let me know if you get a similar result?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/293
https://github.com/scverse/scanpy/issues/293:365,interoperability,api,api,365,"[Here are conda environment files](https://gist.github.com/ivirshup/505a63e54d0aad6ed6289cd1e080a8a2) for mac and linux which reproduce this behavior for me. I've also included a little script which should generate a plot demonstrating the issue [(here's an example output)](https://github.com/theislab/scanpy/files/2472189/pcatest.pdf). . ```python. import scanpy.api as sc. import numpy as np. import pandas as pd. sc.set_figure_params(dpi=300, dpi_save=300) # Makes it more visible. N = 1000. M = 2000. adata = sc.AnnData(. X=np.random.random_sample((N, M)),. obs=pd.DataFrame({""a"": np.random.randint(0, 2, N)}). ). adata.obs[""a""] = adata.obs[""a""].astype(str). sc.pp.pca(adata). sc.pl.pca(adata, color=""a"", size=0.1, save=""test.pdf""). ```. Could you try running this and let me know if you get a similar result?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/293
https://github.com/scverse/scanpy/issues/293:726,safety,test,test,726,"[Here are conda environment files](https://gist.github.com/ivirshup/505a63e54d0aad6ed6289cd1e080a8a2) for mac and linux which reproduce this behavior for me. I've also included a little script which should generate a plot demonstrating the issue [(here's an example output)](https://github.com/theislab/scanpy/files/2472189/pcatest.pdf). . ```python. import scanpy.api as sc. import numpy as np. import pandas as pd. sc.set_figure_params(dpi=300, dpi_save=300) # Makes it more visible. N = 1000. M = 2000. adata = sc.AnnData(. X=np.random.random_sample((N, M)),. obs=pd.DataFrame({""a"": np.random.randint(0, 2, N)}). ). adata.obs[""a""] = adata.obs[""a""].astype(str). sc.pp.pca(adata). sc.pl.pca(adata, color=""a"", size=0.1, save=""test.pdf""). ```. Could you try running this and let me know if you get a similar result?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/293
https://github.com/scverse/scanpy/issues/293:726,testability,test,test,726,"[Here are conda environment files](https://gist.github.com/ivirshup/505a63e54d0aad6ed6289cd1e080a8a2) for mac and linux which reproduce this behavior for me. I've also included a little script which should generate a plot demonstrating the issue [(here's an example output)](https://github.com/theislab/scanpy/files/2472189/pcatest.pdf). . ```python. import scanpy.api as sc. import numpy as np. import pandas as pd. sc.set_figure_params(dpi=300, dpi_save=300) # Makes it more visible. N = 1000. M = 2000. adata = sc.AnnData(. X=np.random.random_sample((N, M)),. obs=pd.DataFrame({""a"": np.random.randint(0, 2, N)}). ). adata.obs[""a""] = adata.obs[""a""].astype(str). sc.pp.pca(adata). sc.pl.pca(adata, color=""a"", size=0.1, save=""test.pdf""). ```. Could you try running this and let me know if you get a similar result?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/293
https://github.com/scverse/scanpy/issues/293:141,usability,behavi,behavior,141,"[Here are conda environment files](https://gist.github.com/ivirshup/505a63e54d0aad6ed6289cd1e080a8a2) for mac and linux which reproduce this behavior for me. I've also included a little script which should generate a plot demonstrating the issue [(here's an example output)](https://github.com/theislab/scanpy/files/2472189/pcatest.pdf). . ```python. import scanpy.api as sc. import numpy as np. import pandas as pd. sc.set_figure_params(dpi=300, dpi_save=300) # Makes it more visible. N = 1000. M = 2000. adata = sc.AnnData(. X=np.random.random_sample((N, M)),. obs=pd.DataFrame({""a"": np.random.randint(0, 2, N)}). ). adata.obs[""a""] = adata.obs[""a""].astype(str). sc.pp.pca(adata). sc.pl.pca(adata, color=""a"", size=0.1, save=""test.pdf""). ```. Could you try running this and let me know if you get a similar result?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/293
https://github.com/scverse/scanpy/issues/293:33,deployability,manag,managed,33,"@ivirshup Thanks for the data. I managed to reproduce the problem. What I gather is that when the size of the dot is very small, the edge of the dot can be larger. This has probably something to do with the linewidth of the edge which is a fixed parameter and can not be smaller than certain value. E.g.:. ```PYTHON. ax= sc.pl.pca(adata, color=""a"", size=20, show=False, linewidths=10). ax.set_ylim(0,0.5). ax.set_xlim(0,0.5). ```. ![image](https://user-images.githubusercontent.com/4964309/46872585-d4496080-ce34-11e8-8902-4e6839d9feda.png). The solution is to set edgecolor='none' be default. I will make a PR to fix this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/293
https://github.com/scverse/scanpy/issues/293:33,energy efficiency,manag,managed,33,"@ivirshup Thanks for the data. I managed to reproduce the problem. What I gather is that when the size of the dot is very small, the edge of the dot can be larger. This has probably something to do with the linewidth of the edge which is a fixed parameter and can not be smaller than certain value. E.g.:. ```PYTHON. ax= sc.pl.pca(adata, color=""a"", size=20, show=False, linewidths=10). ax.set_ylim(0,0.5). ax.set_xlim(0,0.5). ```. ![image](https://user-images.githubusercontent.com/4964309/46872585-d4496080-ce34-11e8-8902-4e6839d9feda.png). The solution is to set edgecolor='none' be default. I will make a PR to fix this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/293
https://github.com/scverse/scanpy/issues/293:246,modifiability,paramet,parameter,246,"@ivirshup Thanks for the data. I managed to reproduce the problem. What I gather is that when the size of the dot is very small, the edge of the dot can be larger. This has probably something to do with the linewidth of the edge which is a fixed parameter and can not be smaller than certain value. E.g.:. ```PYTHON. ax= sc.pl.pca(adata, color=""a"", size=20, show=False, linewidths=10). ax.set_ylim(0,0.5). ax.set_xlim(0,0.5). ```. ![image](https://user-images.githubusercontent.com/4964309/46872585-d4496080-ce34-11e8-8902-4e6839d9feda.png). The solution is to set edgecolor='none' be default. I will make a PR to fix this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/293
https://github.com/scverse/scanpy/issues/293:33,safety,manag,managed,33,"@ivirshup Thanks for the data. I managed to reproduce the problem. What I gather is that when the size of the dot is very small, the edge of the dot can be larger. This has probably something to do with the linewidth of the edge which is a fixed parameter and can not be smaller than certain value. E.g.:. ```PYTHON. ax= sc.pl.pca(adata, color=""a"", size=20, show=False, linewidths=10). ax.set_ylim(0,0.5). ax.set_xlim(0,0.5). ```. ![image](https://user-images.githubusercontent.com/4964309/46872585-d4496080-ce34-11e8-8902-4e6839d9feda.png). The solution is to set edgecolor='none' be default. I will make a PR to fix this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/293
https://github.com/scverse/scanpy/issues/293:448,usability,user,user-images,448,"@ivirshup Thanks for the data. I managed to reproduce the problem. What I gather is that when the size of the dot is very small, the edge of the dot can be larger. This has probably something to do with the linewidth of the edge which is a fixed parameter and can not be smaller than certain value. E.g.:. ```PYTHON. ax= sc.pl.pca(adata, color=""a"", size=20, show=False, linewidths=10). ax.set_ylim(0,0.5). ax.set_xlim(0,0.5). ```. ![image](https://user-images.githubusercontent.com/4964309/46872585-d4496080-ce34-11e8-8902-4e6839d9feda.png). The solution is to set edgecolor='none' be default. I will make a PR to fix this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/293
https://github.com/scverse/scanpy/issues/298:347,deployability,api,api,347,"I didn't know this was possible before! I Will fix that. On Mon, Oct 15, 2018 at 7:32 AM Isaac Virshup <notifications@github.com>. wrote:. > As an example, the following code produces different outputs in scanpy. > v1.3.1 and 80e635d. > <https://github.com/theislab/scanpy/commit/80e635d2b7864623a3342c635d13b79ce5838b35>. > :. >. > import scanpy.api as scimport numpy as np. >. > N = 1000. > M = 2000. >. > adata = sc.AnnData(X=np.random.random_sample((N, M))). >. > sc.pp.pca(adata). > sc.pl.pca(adata, color=[""1"",""2"",""3""], title=[""a"", ""b"", ""c""]). >. > In v1.3.1 this shows:. >. > [image: pca_scatterprevious_titles]. > <https://user-images.githubusercontent.com/8238804/46931190-e0802880-d095-11e8-8f94-78e3cc71f2d4.png>. >. > In 80e635d. > <https://github.com/theislab/scanpy/commit/80e635d2b7864623a3342c635d13b79ce5838b35>. > this shows:. >. > [image: pcacurrent_titles]. > <https://user-images.githubusercontent.com/8238804/46931205-f261cb80-d095-11e8-8160-50997c16d35a.png>. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/298>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Z_tMZ_GO-hAbBe3t8Gjd5nEXhZ0ks5ulB3pgaJpZM4XbnS1>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/298
https://github.com/scverse/scanpy/issues/298:347,integrability,api,api,347,"I didn't know this was possible before! I Will fix that. On Mon, Oct 15, 2018 at 7:32 AM Isaac Virshup <notifications@github.com>. wrote:. > As an example, the following code produces different outputs in scanpy. > v1.3.1 and 80e635d. > <https://github.com/theislab/scanpy/commit/80e635d2b7864623a3342c635d13b79ce5838b35>. > :. >. > import scanpy.api as scimport numpy as np. >. > N = 1000. > M = 2000. >. > adata = sc.AnnData(X=np.random.random_sample((N, M))). >. > sc.pp.pca(adata). > sc.pl.pca(adata, color=[""1"",""2"",""3""], title=[""a"", ""b"", ""c""]). >. > In v1.3.1 this shows:. >. > [image: pca_scatterprevious_titles]. > <https://user-images.githubusercontent.com/8238804/46931190-e0802880-d095-11e8-8f94-78e3cc71f2d4.png>. >. > In 80e635d. > <https://github.com/theislab/scanpy/commit/80e635d2b7864623a3342c635d13b79ce5838b35>. > this shows:. >. > [image: pcacurrent_titles]. > <https://user-images.githubusercontent.com/8238804/46931205-f261cb80-d095-11e8-8160-50997c16d35a.png>. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/298>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Z_tMZ_GO-hAbBe3t8Gjd5nEXhZ0ks5ulB3pgaJpZM4XbnS1>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/298
https://github.com/scverse/scanpy/issues/298:1032,integrability,sub,subscribed,1032,"I didn't know this was possible before! I Will fix that. On Mon, Oct 15, 2018 at 7:32 AM Isaac Virshup <notifications@github.com>. wrote:. > As an example, the following code produces different outputs in scanpy. > v1.3.1 and 80e635d. > <https://github.com/theislab/scanpy/commit/80e635d2b7864623a3342c635d13b79ce5838b35>. > :. >. > import scanpy.api as scimport numpy as np. >. > N = 1000. > M = 2000. >. > adata = sc.AnnData(X=np.random.random_sample((N, M))). >. > sc.pp.pca(adata). > sc.pl.pca(adata, color=[""1"",""2"",""3""], title=[""a"", ""b"", ""c""]). >. > In v1.3.1 this shows:. >. > [image: pca_scatterprevious_titles]. > <https://user-images.githubusercontent.com/8238804/46931190-e0802880-d095-11e8-8f94-78e3cc71f2d4.png>. >. > In 80e635d. > <https://github.com/theislab/scanpy/commit/80e635d2b7864623a3342c635d13b79ce5838b35>. > this shows:. >. > [image: pcacurrent_titles]. > <https://user-images.githubusercontent.com/8238804/46931205-f261cb80-d095-11e8-8160-50997c16d35a.png>. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/298>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Z_tMZ_GO-hAbBe3t8Gjd5nEXhZ0ks5ulB3pgaJpZM4XbnS1>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/298
https://github.com/scverse/scanpy/issues/298:347,interoperability,api,api,347,"I didn't know this was possible before! I Will fix that. On Mon, Oct 15, 2018 at 7:32 AM Isaac Virshup <notifications@github.com>. wrote:. > As an example, the following code produces different outputs in scanpy. > v1.3.1 and 80e635d. > <https://github.com/theislab/scanpy/commit/80e635d2b7864623a3342c635d13b79ce5838b35>. > :. >. > import scanpy.api as scimport numpy as np. >. > N = 1000. > M = 2000. >. > adata = sc.AnnData(X=np.random.random_sample((N, M))). >. > sc.pp.pca(adata). > sc.pl.pca(adata, color=[""1"",""2"",""3""], title=[""a"", ""b"", ""c""]). >. > In v1.3.1 this shows:. >. > [image: pca_scatterprevious_titles]. > <https://user-images.githubusercontent.com/8238804/46931190-e0802880-d095-11e8-8f94-78e3cc71f2d4.png>. >. > In 80e635d. > <https://github.com/theislab/scanpy/commit/80e635d2b7864623a3342c635d13b79ce5838b35>. > this shows:. >. > [image: pcacurrent_titles]. > <https://user-images.githubusercontent.com/8238804/46931205-f261cb80-d095-11e8-8160-50997c16d35a.png>. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/298>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Z_tMZ_GO-hAbBe3t8Gjd5nEXhZ0ks5ulB3pgaJpZM4XbnS1>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/298
https://github.com/scverse/scanpy/issues/298:1229,security,auth,auth,1229,"I didn't know this was possible before! I Will fix that. On Mon, Oct 15, 2018 at 7:32 AM Isaac Virshup <notifications@github.com>. wrote:. > As an example, the following code produces different outputs in scanpy. > v1.3.1 and 80e635d. > <https://github.com/theislab/scanpy/commit/80e635d2b7864623a3342c635d13b79ce5838b35>. > :. >. > import scanpy.api as scimport numpy as np. >. > N = 1000. > M = 2000. >. > adata = sc.AnnData(X=np.random.random_sample((N, M))). >. > sc.pp.pca(adata). > sc.pl.pca(adata, color=[""1"",""2"",""3""], title=[""a"", ""b"", ""c""]). >. > In v1.3.1 this shows:. >. > [image: pca_scatterprevious_titles]. > <https://user-images.githubusercontent.com/8238804/46931190-e0802880-d095-11e8-8f94-78e3cc71f2d4.png>. >. > In 80e635d. > <https://github.com/theislab/scanpy/commit/80e635d2b7864623a3342c635d13b79ce5838b35>. > this shows:. >. > [image: pcacurrent_titles]. > <https://user-images.githubusercontent.com/8238804/46931205-f261cb80-d095-11e8-8160-50997c16d35a.png>. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/298>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Z_tMZ_GO-hAbBe3t8Gjd5nEXhZ0ks5ulB3pgaJpZM4XbnS1>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/298
https://github.com/scverse/scanpy/issues/298:631,usability,user,user-images,631,"I didn't know this was possible before! I Will fix that. On Mon, Oct 15, 2018 at 7:32 AM Isaac Virshup <notifications@github.com>. wrote:. > As an example, the following code produces different outputs in scanpy. > v1.3.1 and 80e635d. > <https://github.com/theislab/scanpy/commit/80e635d2b7864623a3342c635d13b79ce5838b35>. > :. >. > import scanpy.api as scimport numpy as np. >. > N = 1000. > M = 2000. >. > adata = sc.AnnData(X=np.random.random_sample((N, M))). >. > sc.pp.pca(adata). > sc.pl.pca(adata, color=[""1"",""2"",""3""], title=[""a"", ""b"", ""c""]). >. > In v1.3.1 this shows:. >. > [image: pca_scatterprevious_titles]. > <https://user-images.githubusercontent.com/8238804/46931190-e0802880-d095-11e8-8f94-78e3cc71f2d4.png>. >. > In 80e635d. > <https://github.com/theislab/scanpy/commit/80e635d2b7864623a3342c635d13b79ce5838b35>. > this shows:. >. > [image: pcacurrent_titles]. > <https://user-images.githubusercontent.com/8238804/46931205-f261cb80-d095-11e8-8160-50997c16d35a.png>. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/298>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Z_tMZ_GO-hAbBe3t8Gjd5nEXhZ0ks5ulB3pgaJpZM4XbnS1>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/298
https://github.com/scverse/scanpy/issues/298:889,usability,user,user-images,889,"I didn't know this was possible before! I Will fix that. On Mon, Oct 15, 2018 at 7:32 AM Isaac Virshup <notifications@github.com>. wrote:. > As an example, the following code produces different outputs in scanpy. > v1.3.1 and 80e635d. > <https://github.com/theislab/scanpy/commit/80e635d2b7864623a3342c635d13b79ce5838b35>. > :. >. > import scanpy.api as scimport numpy as np. >. > N = 1000. > M = 2000. >. > adata = sc.AnnData(X=np.random.random_sample((N, M))). >. > sc.pp.pca(adata). > sc.pl.pca(adata, color=[""1"",""2"",""3""], title=[""a"", ""b"", ""c""]). >. > In v1.3.1 this shows:. >. > [image: pca_scatterprevious_titles]. > <https://user-images.githubusercontent.com/8238804/46931190-e0802880-d095-11e8-8f94-78e3cc71f2d4.png>. >. > In 80e635d. > <https://github.com/theislab/scanpy/commit/80e635d2b7864623a3342c635d13b79ce5838b35>. > this shows:. >. > [image: pcacurrent_titles]. > <https://user-images.githubusercontent.com/8238804/46931205-f261cb80-d095-11e8-8160-50997c16d35a.png>. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/298>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1Z_tMZ_GO-hAbBe3t8Gjd5nEXhZ0ks5ulB3pgaJpZM4XbnS1>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/298
https://github.com/scverse/scanpy/issues/299:420,deployability,api,api,420,"I the argument has to be a list not a numpy array. I don't know why the. colorbars are repeated when a numpy array is passed. I will add a PR to fix. this. On Mon, Oct 15, 2018 at 7:48 AM Isaac Virshup <notifications@github.com>. wrote:. > Comparing v1.3.1 and 80e635d. > <https://github.com/theislab/scanpy/commit/80e635d2b7864623a3342c635d13b79ce5838b35>. > (current master) on the following code:. >. > import scanpy.api as scimport numpy as np. >. > N = 1000. > M = 2000. >. > adata = sc.AnnData(X=np.random.random_sample((N, M))). >. > sc.pp.pca(adata). > sc.pl.pca(adata, color=np.array([""1"",""2"",""3""])). >. > In v1.3.1 this shows:. >. > [image: pca_scatter_prev_colorarray]. > <https://user-images.githubusercontent.com/8238804/46932091-ce07ee00-d099-11e8-8002-f4921e30849c.png>. >. > In 80e635d. > <https://github.com/theislab/scanpy/commit/80e635d2b7864623a3342c635d13b79ce5838b35>. > this shows:. >. > [image: pca_cur_colorarray]. > <https://user-images.githubusercontent.com/8238804/46932109-e1b35480-d099-11e8-9baa-a8fcf67496f7.png>. >. > If the argument to color is a list, they show similar plots. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/299>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1UobyNTFpz9cVbjmCoc1A4cml3KBks5ulCGmgaJpZM4XboJG>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/299
https://github.com/scverse/scanpy/issues/299:361,energy efficiency,current,current,361,"I the argument has to be a list not a numpy array. I don't know why the. colorbars are repeated when a numpy array is passed. I will add a PR to fix. this. On Mon, Oct 15, 2018 at 7:48 AM Isaac Virshup <notifications@github.com>. wrote:. > Comparing v1.3.1 and 80e635d. > <https://github.com/theislab/scanpy/commit/80e635d2b7864623a3342c635d13b79ce5838b35>. > (current master) on the following code:. >. > import scanpy.api as scimport numpy as np. >. > N = 1000. > M = 2000. >. > adata = sc.AnnData(X=np.random.random_sample((N, M))). >. > sc.pp.pca(adata). > sc.pl.pca(adata, color=np.array([""1"",""2"",""3""])). >. > In v1.3.1 this shows:. >. > [image: pca_scatter_prev_colorarray]. > <https://user-images.githubusercontent.com/8238804/46932091-ce07ee00-d099-11e8-8002-f4921e30849c.png>. >. > In 80e635d. > <https://github.com/theislab/scanpy/commit/80e635d2b7864623a3342c635d13b79ce5838b35>. > this shows:. >. > [image: pca_cur_colorarray]. > <https://user-images.githubusercontent.com/8238804/46932109-e1b35480-d099-11e8-9baa-a8fcf67496f7.png>. >. > If the argument to color is a list, they show similar plots. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/299>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1UobyNTFpz9cVbjmCoc1A4cml3KBks5ulCGmgaJpZM4XboJG>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/299
https://github.com/scverse/scanpy/issues/299:420,integrability,api,api,420,"I the argument has to be a list not a numpy array. I don't know why the. colorbars are repeated when a numpy array is passed. I will add a PR to fix. this. On Mon, Oct 15, 2018 at 7:48 AM Isaac Virshup <notifications@github.com>. wrote:. > Comparing v1.3.1 and 80e635d. > <https://github.com/theislab/scanpy/commit/80e635d2b7864623a3342c635d13b79ce5838b35>. > (current master) on the following code:. >. > import scanpy.api as scimport numpy as np. >. > N = 1000. > M = 2000. >. > adata = sc.AnnData(X=np.random.random_sample((N, M))). >. > sc.pp.pca(adata). > sc.pl.pca(adata, color=np.array([""1"",""2"",""3""])). >. > In v1.3.1 this shows:. >. > [image: pca_scatter_prev_colorarray]. > <https://user-images.githubusercontent.com/8238804/46932091-ce07ee00-d099-11e8-8002-f4921e30849c.png>. >. > In 80e635d. > <https://github.com/theislab/scanpy/commit/80e635d2b7864623a3342c635d13b79ce5838b35>. > this shows:. >. > [image: pca_cur_colorarray]. > <https://user-images.githubusercontent.com/8238804/46932109-e1b35480-d099-11e8-9baa-a8fcf67496f7.png>. >. > If the argument to color is a list, they show similar plots. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/299>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1UobyNTFpz9cVbjmCoc1A4cml3KBks5ulCGmgaJpZM4XboJG>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/299
https://github.com/scverse/scanpy/issues/299:1160,integrability,sub,subscribed,1160,"I the argument has to be a list not a numpy array. I don't know why the. colorbars are repeated when a numpy array is passed. I will add a PR to fix. this. On Mon, Oct 15, 2018 at 7:48 AM Isaac Virshup <notifications@github.com>. wrote:. > Comparing v1.3.1 and 80e635d. > <https://github.com/theislab/scanpy/commit/80e635d2b7864623a3342c635d13b79ce5838b35>. > (current master) on the following code:. >. > import scanpy.api as scimport numpy as np. >. > N = 1000. > M = 2000. >. > adata = sc.AnnData(X=np.random.random_sample((N, M))). >. > sc.pp.pca(adata). > sc.pl.pca(adata, color=np.array([""1"",""2"",""3""])). >. > In v1.3.1 this shows:. >. > [image: pca_scatter_prev_colorarray]. > <https://user-images.githubusercontent.com/8238804/46932091-ce07ee00-d099-11e8-8002-f4921e30849c.png>. >. > In 80e635d. > <https://github.com/theislab/scanpy/commit/80e635d2b7864623a3342c635d13b79ce5838b35>. > this shows:. >. > [image: pca_cur_colorarray]. > <https://user-images.githubusercontent.com/8238804/46932109-e1b35480-d099-11e8-9baa-a8fcf67496f7.png>. >. > If the argument to color is a list, they show similar plots. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/299>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1UobyNTFpz9cVbjmCoc1A4cml3KBks5ulCGmgaJpZM4XboJG>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/299
https://github.com/scverse/scanpy/issues/299:420,interoperability,api,api,420,"I the argument has to be a list not a numpy array. I don't know why the. colorbars are repeated when a numpy array is passed. I will add a PR to fix. this. On Mon, Oct 15, 2018 at 7:48 AM Isaac Virshup <notifications@github.com>. wrote:. > Comparing v1.3.1 and 80e635d. > <https://github.com/theislab/scanpy/commit/80e635d2b7864623a3342c635d13b79ce5838b35>. > (current master) on the following code:. >. > import scanpy.api as scimport numpy as np. >. > N = 1000. > M = 2000. >. > adata = sc.AnnData(X=np.random.random_sample((N, M))). >. > sc.pp.pca(adata). > sc.pl.pca(adata, color=np.array([""1"",""2"",""3""])). >. > In v1.3.1 this shows:. >. > [image: pca_scatter_prev_colorarray]. > <https://user-images.githubusercontent.com/8238804/46932091-ce07ee00-d099-11e8-8002-f4921e30849c.png>. >. > In 80e635d. > <https://github.com/theislab/scanpy/commit/80e635d2b7864623a3342c635d13b79ce5838b35>. > this shows:. >. > [image: pca_cur_colorarray]. > <https://user-images.githubusercontent.com/8238804/46932109-e1b35480-d099-11e8-9baa-a8fcf67496f7.png>. >. > If the argument to color is a list, they show similar plots. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/299>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1UobyNTFpz9cVbjmCoc1A4cml3KBks5ulCGmgaJpZM4XboJG>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/299
https://github.com/scverse/scanpy/issues/299:1357,security,auth,auth,1357,"I the argument has to be a list not a numpy array. I don't know why the. colorbars are repeated when a numpy array is passed. I will add a PR to fix. this. On Mon, Oct 15, 2018 at 7:48 AM Isaac Virshup <notifications@github.com>. wrote:. > Comparing v1.3.1 and 80e635d. > <https://github.com/theislab/scanpy/commit/80e635d2b7864623a3342c635d13b79ce5838b35>. > (current master) on the following code:. >. > import scanpy.api as scimport numpy as np. >. > N = 1000. > M = 2000. >. > adata = sc.AnnData(X=np.random.random_sample((N, M))). >. > sc.pp.pca(adata). > sc.pl.pca(adata, color=np.array([""1"",""2"",""3""])). >. > In v1.3.1 this shows:. >. > [image: pca_scatter_prev_colorarray]. > <https://user-images.githubusercontent.com/8238804/46932091-ce07ee00-d099-11e8-8002-f4921e30849c.png>. >. > In 80e635d. > <https://github.com/theislab/scanpy/commit/80e635d2b7864623a3342c635d13b79ce5838b35>. > this shows:. >. > [image: pca_cur_colorarray]. > <https://user-images.githubusercontent.com/8238804/46932109-e1b35480-d099-11e8-9baa-a8fcf67496f7.png>. >. > If the argument to color is a list, they show similar plots. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/299>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1UobyNTFpz9cVbjmCoc1A4cml3KBks5ulCGmgaJpZM4XboJG>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/299
https://github.com/scverse/scanpy/issues/299:692,usability,user,user-images,692,"I the argument has to be a list not a numpy array. I don't know why the. colorbars are repeated when a numpy array is passed. I will add a PR to fix. this. On Mon, Oct 15, 2018 at 7:48 AM Isaac Virshup <notifications@github.com>. wrote:. > Comparing v1.3.1 and 80e635d. > <https://github.com/theislab/scanpy/commit/80e635d2b7864623a3342c635d13b79ce5838b35>. > (current master) on the following code:. >. > import scanpy.api as scimport numpy as np. >. > N = 1000. > M = 2000. >. > adata = sc.AnnData(X=np.random.random_sample((N, M))). >. > sc.pp.pca(adata). > sc.pl.pca(adata, color=np.array([""1"",""2"",""3""])). >. > In v1.3.1 this shows:. >. > [image: pca_scatter_prev_colorarray]. > <https://user-images.githubusercontent.com/8238804/46932091-ce07ee00-d099-11e8-8002-f4921e30849c.png>. >. > In 80e635d. > <https://github.com/theislab/scanpy/commit/80e635d2b7864623a3342c635d13b79ce5838b35>. > this shows:. >. > [image: pca_cur_colorarray]. > <https://user-images.githubusercontent.com/8238804/46932109-e1b35480-d099-11e8-9baa-a8fcf67496f7.png>. >. > If the argument to color is a list, they show similar plots. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/299>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1UobyNTFpz9cVbjmCoc1A4cml3KBks5ulCGmgaJpZM4XboJG>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/299
https://github.com/scverse/scanpy/issues/299:951,usability,user,user-images,951,"I the argument has to be a list not a numpy array. I don't know why the. colorbars are repeated when a numpy array is passed. I will add a PR to fix. this. On Mon, Oct 15, 2018 at 7:48 AM Isaac Virshup <notifications@github.com>. wrote:. > Comparing v1.3.1 and 80e635d. > <https://github.com/theislab/scanpy/commit/80e635d2b7864623a3342c635d13b79ce5838b35>. > (current master) on the following code:. >. > import scanpy.api as scimport numpy as np. >. > N = 1000. > M = 2000. >. > adata = sc.AnnData(X=np.random.random_sample((N, M))). >. > sc.pp.pca(adata). > sc.pl.pca(adata, color=np.array([""1"",""2"",""3""])). >. > In v1.3.1 this shows:. >. > [image: pca_scatter_prev_colorarray]. > <https://user-images.githubusercontent.com/8238804/46932091-ce07ee00-d099-11e8-8002-f4921e30849c.png>. >. > In 80e635d. > <https://github.com/theislab/scanpy/commit/80e635d2b7864623a3342c635d13b79ce5838b35>. > this shows:. >. > [image: pca_cur_colorarray]. > <https://user-images.githubusercontent.com/8238804/46932109-e1b35480-d099-11e8-9baa-a8fcf67496f7.png>. >. > If the argument to color is a list, they show similar plots. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/299>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1UobyNTFpz9cVbjmCoc1A4cml3KBks5ulCGmgaJpZM4XboJG>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/299
https://github.com/scverse/scanpy/issues/299:85,usability,behavi,behavior,85,Thanks! Looks like this and #298 are solved. But now I'm getting some strange layout behavior. I'll dig into that a bit and open a new issue.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/299
https://github.com/scverse/scanpy/issues/301:316,energy efficiency,estimat,estimate,316,"Responding to your email, here: yes, `max_fraction` should correspond to `quantile`. And yes, the code is correct, only the genes contributing less than the chosen quantile should be the basis for the normalization, so that very highly expressed genes (potential outliers) do not confound the normalization constant estimate.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/301
https://github.com/scverse/scanpy/issues/301:204,integrability,filter,filter,204,"@falexwolf . Another question - now [normalize_per_cell_weinreb16_deprecated](https://github.com/theislab/scanpy/blob/b4c2479eed302707a4d098f8f3c85037c82f07ca/scanpy/preprocessing/simple.py#L579) doesn't filter anything, just divides by sums of chosen genes, this normalization looks strange. ```. X = np.array([[1, 0, 1], [3, 0, 1], [5, 6, 1]]). normalize_per_cell_weinreb16_deprecated(X, max_fraction=0.7). array([[1. , 0. , 1. ],. [3. , 0. , 1. ],. [0.71428571, 0.85714286, 0.14285714]]). ```. Should it be this way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/301
https://github.com/scverse/scanpy/issues/301:196,reliability,doe,doesn,196,"@falexwolf . Another question - now [normalize_per_cell_weinreb16_deprecated](https://github.com/theislab/scanpy/blob/b4c2479eed302707a4d098f8f3c85037c82f07ca/scanpy/preprocessing/simple.py#L579) doesn't filter anything, just divides by sums of chosen genes, this normalization looks strange. ```. X = np.array([[1, 0, 1], [3, 0, 1], [5, 6, 1]]). normalize_per_cell_weinreb16_deprecated(X, max_fraction=0.7). array([[1. , 0. , 1. ],. [3. , 0. , 1. ],. [0.71428571, 0.85714286, 0.14285714]]). ```. Should it be this way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/301
https://github.com/scverse/scanpy/issues/301:180,testability,simpl,simple,180,"@falexwolf . Another question - now [normalize_per_cell_weinreb16_deprecated](https://github.com/theislab/scanpy/blob/b4c2479eed302707a4d098f8f3c85037c82f07ca/scanpy/preprocessing/simple.py#L579) doesn't filter anything, just divides by sums of chosen genes, this normalization looks strange. ```. X = np.array([[1, 0, 1], [3, 0, 1], [5, 6, 1]]). normalize_per_cell_weinreb16_deprecated(X, max_fraction=0.7). array([[1. , 0. , 1. ],. [3. , 0. , 1. ],. [0.71428571, 0.85714286, 0.14285714]]). ```. Should it be this way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/301
https://github.com/scverse/scanpy/issues/301:180,usability,simpl,simple,180,"@falexwolf . Another question - now [normalize_per_cell_weinreb16_deprecated](https://github.com/theislab/scanpy/blob/b4c2479eed302707a4d098f8f3c85037c82f07ca/scanpy/preprocessing/simple.py#L579) doesn't filter anything, just divides by sums of chosen genes, this normalization looks strange. ```. X = np.array([[1, 0, 1], [3, 0, 1], [5, 6, 1]]). normalize_per_cell_weinreb16_deprecated(X, max_fraction=0.7). array([[1. , 0. , 1. ],. [3. , 0. , 1. ],. [0.71428571, 0.85714286, 0.14285714]]). ```. Should it be this way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/301
https://github.com/scverse/scanpy/issues/301:103,integrability,filter,filtering,103,for now i have this [norml branch](https://github.com/theislab/scanpy/tree/norml). But i'm not sure if filtering of genes is needed and how to use `materialize_as_ndarray `properly here.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/301
https://github.com/scverse/scanpy/issues/301:102,integrability,filter,filtering,102,"Why does normalize_per_weinreb look strange? We can skype, I think it makes sense. There should be no filtering whatsoever in the new functions, I'd say.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/301
https://github.com/scverse/scanpy/issues/301:4,reliability,doe,does,4,"Why does normalize_per_weinreb look strange? We can skype, I think it makes sense. There should be no filtering whatsoever in the new functions, I'd say.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/301
https://github.com/scverse/scanpy/issues/301:94,energy efficiency,current,currently,94,Why don't you make a PR and then we can use the commenting and reviewing tools of GitHub? I'm currently going through your code and would have a few comments.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/301
https://github.com/scverse/scanpy/issues/301:63,safety,review,reviewing,63,Why don't you make a PR and then we can use the commenting and reviewing tools of GitHub? I'm currently going through your code and would have a few comments.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/301
https://github.com/scverse/scanpy/issues/301:63,testability,review,reviewing,63,Why don't you make a PR and then we can use the commenting and reviewing tools of GitHub? I'm currently going through your code and would have a few comments.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/301
https://github.com/scverse/scanpy/issues/301:73,usability,tool,tools,73,Why don't you make a PR and then we can use the commenting and reviewing tools of GitHub? I'm currently going through your code and would have a few comments.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/301
https://github.com/scverse/scanpy/issues/301:15,integrability,filter,filtering,15,"Ok, got it, no filtering. I'll remove filtering and make a PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/301
https://github.com/scverse/scanpy/issues/301:38,integrability,filter,filtering,38,"Ok, got it, no filtering. I'll remove filtering and make a PR.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/301
https://github.com/scverse/scanpy/issues/302:56,safety,compl,completely,56,"Why not using https://nbsphinx.readthedocs.io? It works completely fine for me. So, I would simply moved forward with it as soon as there is some bandwidth.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302
https://github.com/scverse/scanpy/issues/302:56,security,compl,completely,56,"Why not using https://nbsphinx.readthedocs.io? It works completely fine for me. So, I would simply moved forward with it as soon as there is some bandwidth.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302
https://github.com/scverse/scanpy/issues/302:92,testability,simpl,simply,92,"Why not using https://nbsphinx.readthedocs.io? It works completely fine for me. So, I would simply moved forward with it as soon as there is some bandwidth.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302
https://github.com/scverse/scanpy/issues/302:92,usability,simpl,simply,92,"Why not using https://nbsphinx.readthedocs.io? It works completely fine for me. So, I would simply moved forward with it as soon as there is some bandwidth.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302
https://github.com/scverse/scanpy/issues/302:169,deployability,version,version,169,"Sure, there's nothing wrong with nbsphinx! . Actually, the only advantage of jupytext is that it provides an elegant way to only keep the input cells of notebooks under version control.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302
https://github.com/scverse/scanpy/issues/302:169,integrability,version,version,169,"Sure, there's nothing wrong with nbsphinx! . Actually, the only advantage of jupytext is that it provides an elegant way to only keep the input cells of notebooks under version control.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302
https://github.com/scverse/scanpy/issues/302:169,modifiability,version,version,169,"Sure, there's nothing wrong with nbsphinx! . Actually, the only advantage of jupytext is that it provides an elegant way to only keep the input cells of notebooks under version control.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302
https://github.com/scverse/scanpy/issues/302:138,safety,input,input,138,"Sure, there's nothing wrong with nbsphinx! . Actually, the only advantage of jupytext is that it provides an elegant way to only keep the input cells of notebooks under version control.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302
https://github.com/scverse/scanpy/issues/302:177,security,control,control,177,"Sure, there's nothing wrong with nbsphinx! . Actually, the only advantage of jupytext is that it provides an elegant way to only keep the input cells of notebooks under version control.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302
https://github.com/scverse/scanpy/issues/302:177,testability,control,control,177,"Sure, there's nothing wrong with nbsphinx! . Actually, the only advantage of jupytext is that it provides an elegant way to only keep the input cells of notebooks under version control.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302
https://github.com/scverse/scanpy/issues/302:138,usability,input,input,138,"Sure, there's nothing wrong with nbsphinx! . Actually, the only advantage of jupytext is that it provides an elegant way to only keep the input cells of notebooks under version control.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302
https://github.com/scverse/scanpy/issues/302:178,deployability,build,build,178,You mean it uses/relies on [nbstripout](https://github.com/kynan/nbstripout)? In that case:. Pro | Con. --- | ---. Tests the Notebooks by running them | Docs need a long time to build. Only Text under version control |,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302
https://github.com/scverse/scanpy/issues/302:201,deployability,version,version,201,You mean it uses/relies on [nbstripout](https://github.com/kynan/nbstripout)? In that case:. Pro | Con. --- | ---. Tests the Notebooks by running them | Docs need a long time to build. Only Text under version control |,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302
https://github.com/scverse/scanpy/issues/302:201,integrability,version,version,201,You mean it uses/relies on [nbstripout](https://github.com/kynan/nbstripout)? In that case:. Pro | Con. --- | ---. Tests the Notebooks by running them | Docs need a long time to build. Only Text under version control |,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302
https://github.com/scverse/scanpy/issues/302:201,modifiability,version,version,201,You mean it uses/relies on [nbstripout](https://github.com/kynan/nbstripout)? In that case:. Pro | Con. --- | ---. Tests the Notebooks by running them | Docs need a long time to build. Only Text under version control |,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302
https://github.com/scverse/scanpy/issues/302:170,performance,time,time,170,You mean it uses/relies on [nbstripout](https://github.com/kynan/nbstripout)? In that case:. Pro | Con. --- | ---. Tests the Notebooks by running them | Docs need a long time to build. Only Text under version control |,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302
https://github.com/scverse/scanpy/issues/302:115,safety,Test,Tests,115,You mean it uses/relies on [nbstripout](https://github.com/kynan/nbstripout)? In that case:. Pro | Con. --- | ---. Tests the Notebooks by running them | Docs need a long time to build. Only Text under version control |,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302
https://github.com/scverse/scanpy/issues/302:209,security,control,control,209,You mean it uses/relies on [nbstripout](https://github.com/kynan/nbstripout)? In that case:. Pro | Con. --- | ---. Tests the Notebooks by running them | Docs need a long time to build. Only Text under version control |,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302
https://github.com/scverse/scanpy/issues/302:115,testability,Test,Tests,115,You mean it uses/relies on [nbstripout](https://github.com/kynan/nbstripout)? In that case:. Pro | Con. --- | ---. Tests the Notebooks by running them | Docs need a long time to build. Only Text under version control |,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302
https://github.com/scverse/scanpy/issues/302:209,testability,control,control,209,You mean it uses/relies on [nbstripout](https://github.com/kynan/nbstripout)? In that case:. Pro | Con. --- | ---. Tests the Notebooks by running them | Docs need a long time to build. Only Text under version control |,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302
https://github.com/scverse/scanpy/issues/302:351,deployability,version,version,351,"The idea is the same, but it works differently. . Jupytext allows to save jupyter notebooks as various text formats (e.g. `Rmd` or plain python with comments). One of the supported formats is `sphinx-gallery`, that can be directly rendered by readthedocs if I'm not mistaken. . Using a text-representation of notebooks is great when having them under version control. . But of course the pros/cons apply as you pointed out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302
https://github.com/scverse/scanpy/issues/302:351,integrability,version,version,351,"The idea is the same, but it works differently. . Jupytext allows to save jupyter notebooks as various text formats (e.g. `Rmd` or plain python with comments). One of the supported formats is `sphinx-gallery`, that can be directly rendered by readthedocs if I'm not mistaken. . Using a text-representation of notebooks is great when having them under version control. . But of course the pros/cons apply as you pointed out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302
https://github.com/scverse/scanpy/issues/302:108,interoperability,format,formats,108,"The idea is the same, but it works differently. . Jupytext allows to save jupyter notebooks as various text formats (e.g. `Rmd` or plain python with comments). One of the supported formats is `sphinx-gallery`, that can be directly rendered by readthedocs if I'm not mistaken. . Using a text-representation of notebooks is great when having them under version control. . But of course the pros/cons apply as you pointed out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302
https://github.com/scverse/scanpy/issues/302:181,interoperability,format,formats,181,"The idea is the same, but it works differently. . Jupytext allows to save jupyter notebooks as various text formats (e.g. `Rmd` or plain python with comments). One of the supported formats is `sphinx-gallery`, that can be directly rendered by readthedocs if I'm not mistaken. . Using a text-representation of notebooks is great when having them under version control. . But of course the pros/cons apply as you pointed out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302
https://github.com/scverse/scanpy/issues/302:351,modifiability,version,version,351,"The idea is the same, but it works differently. . Jupytext allows to save jupyter notebooks as various text formats (e.g. `Rmd` or plain python with comments). One of the supported formats is `sphinx-gallery`, that can be directly rendered by readthedocs if I'm not mistaken. . Using a text-representation of notebooks is great when having them under version control. . But of course the pros/cons apply as you pointed out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302
https://github.com/scverse/scanpy/issues/302:359,security,control,control,359,"The idea is the same, but it works differently. . Jupytext allows to save jupyter notebooks as various text formats (e.g. `Rmd` or plain python with comments). One of the supported formats is `sphinx-gallery`, that can be directly rendered by readthedocs if I'm not mistaken. . Using a text-representation of notebooks is great when having them under version control. . But of course the pros/cons apply as you pointed out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302
https://github.com/scverse/scanpy/issues/302:359,testability,control,control,359,"The idea is the same, but it works differently. . Jupytext allows to save jupyter notebooks as various text formats (e.g. `Rmd` or plain python with comments). One of the supported formats is `sphinx-gallery`, that can be directly rendered by readthedocs if I'm not mistaken. . Using a text-representation of notebooks is great when having them under version control. . But of course the pros/cons apply as you pointed out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302
https://github.com/scverse/scanpy/issues/302:171,usability,support,supported,171,"The idea is the same, but it works differently. . Jupytext allows to save jupyter notebooks as various text formats (e.g. `Rmd` or plain python with comments). One of the supported formats is `sphinx-gallery`, that can be directly rendered by readthedocs if I'm not mistaken. . Using a text-representation of notebooks is great when having them under version control. . But of course the pros/cons apply as you pointed out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302
https://github.com/scverse/scanpy/issues/302:601,deployability,build,build,601,"There are different ways of stripping output from jupyter notebooks, several of them can be combined with the git commit workflow. I've played around a bit with them. The only thing I need to make sure is if there is a way that people *cannot* commit notebooks with output in them to the scanpy repo. The latter is an absolute no go. @grst, committing things in sphinx-gallery format could be a new way. I'll check whether this offers some convenience. In general I'd rather prefer to have the stripped `.ipynb` files in the repo, as it's done in tensorflow. @flying-sheep: I'm not worrying about the build time of the docs, that's decently fast in all my tests so far. @flying-sheep: After a bit of research I've done some time ago, I also don't consider tests based on notebooks a possibility. I made [versions of two notebooks](https://github.com/theislab/scanpy/tree/master/scanpy/tests/notebooks) running on subsampled data producing ugly looking low-resolution figures, which ensures small test times and prevents the repo from blowing up. For the tutorials, I want extremely high resolution figures and no subsampled data. I think I should be almost there. Hopefully we'll have the first two notebooks in the Scanpy repo this or next week...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302
https://github.com/scverse/scanpy/issues/302:804,deployability,version,versions,804,"There are different ways of stripping output from jupyter notebooks, several of them can be combined with the git commit workflow. I've played around a bit with them. The only thing I need to make sure is if there is a way that people *cannot* commit notebooks with output in them to the scanpy repo. The latter is an absolute no go. @grst, committing things in sphinx-gallery format could be a new way. I'll check whether this offers some convenience. In general I'd rather prefer to have the stripped `.ipynb` files in the repo, as it's done in tensorflow. @flying-sheep: I'm not worrying about the build time of the docs, that's decently fast in all my tests so far. @flying-sheep: After a bit of research I've done some time ago, I also don't consider tests based on notebooks a possibility. I made [versions of two notebooks](https://github.com/theislab/scanpy/tree/master/scanpy/tests/notebooks) running on subsampled data producing ugly looking low-resolution figures, which ensures small test times and prevents the repo from blowing up. For the tutorials, I want extremely high resolution figures and no subsampled data. I think I should be almost there. Hopefully we'll have the first two notebooks in the Scanpy repo this or next week...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302
https://github.com/scverse/scanpy/issues/302:804,integrability,version,versions,804,"There are different ways of stripping output from jupyter notebooks, several of them can be combined with the git commit workflow. I've played around a bit with them. The only thing I need to make sure is if there is a way that people *cannot* commit notebooks with output in them to the scanpy repo. The latter is an absolute no go. @grst, committing things in sphinx-gallery format could be a new way. I'll check whether this offers some convenience. In general I'd rather prefer to have the stripped `.ipynb` files in the repo, as it's done in tensorflow. @flying-sheep: I'm not worrying about the build time of the docs, that's decently fast in all my tests so far. @flying-sheep: After a bit of research I've done some time ago, I also don't consider tests based on notebooks a possibility. I made [versions of two notebooks](https://github.com/theislab/scanpy/tree/master/scanpy/tests/notebooks) running on subsampled data producing ugly looking low-resolution figures, which ensures small test times and prevents the repo from blowing up. For the tutorials, I want extremely high resolution figures and no subsampled data. I think I should be almost there. Hopefully we'll have the first two notebooks in the Scanpy repo this or next week...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302
https://github.com/scverse/scanpy/issues/302:913,integrability,sub,subsampled,913,"There are different ways of stripping output from jupyter notebooks, several of them can be combined with the git commit workflow. I've played around a bit with them. The only thing I need to make sure is if there is a way that people *cannot* commit notebooks with output in them to the scanpy repo. The latter is an absolute no go. @grst, committing things in sphinx-gallery format could be a new way. I'll check whether this offers some convenience. In general I'd rather prefer to have the stripped `.ipynb` files in the repo, as it's done in tensorflow. @flying-sheep: I'm not worrying about the build time of the docs, that's decently fast in all my tests so far. @flying-sheep: After a bit of research I've done some time ago, I also don't consider tests based on notebooks a possibility. I made [versions of two notebooks](https://github.com/theislab/scanpy/tree/master/scanpy/tests/notebooks) running on subsampled data producing ugly looking low-resolution figures, which ensures small test times and prevents the repo from blowing up. For the tutorials, I want extremely high resolution figures and no subsampled data. I think I should be almost there. Hopefully we'll have the first two notebooks in the Scanpy repo this or next week...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302
https://github.com/scverse/scanpy/issues/302:1113,integrability,sub,subsampled,1113,"There are different ways of stripping output from jupyter notebooks, several of them can be combined with the git commit workflow. I've played around a bit with them. The only thing I need to make sure is if there is a way that people *cannot* commit notebooks with output in them to the scanpy repo. The latter is an absolute no go. @grst, committing things in sphinx-gallery format could be a new way. I'll check whether this offers some convenience. In general I'd rather prefer to have the stripped `.ipynb` files in the repo, as it's done in tensorflow. @flying-sheep: I'm not worrying about the build time of the docs, that's decently fast in all my tests so far. @flying-sheep: After a bit of research I've done some time ago, I also don't consider tests based on notebooks a possibility. I made [versions of two notebooks](https://github.com/theislab/scanpy/tree/master/scanpy/tests/notebooks) running on subsampled data producing ugly looking low-resolution figures, which ensures small test times and prevents the repo from blowing up. For the tutorials, I want extremely high resolution figures and no subsampled data. I think I should be almost there. Hopefully we'll have the first two notebooks in the Scanpy repo this or next week...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302
https://github.com/scverse/scanpy/issues/302:377,interoperability,format,format,377,"There are different ways of stripping output from jupyter notebooks, several of them can be combined with the git commit workflow. I've played around a bit with them. The only thing I need to make sure is if there is a way that people *cannot* commit notebooks with output in them to the scanpy repo. The latter is an absolute no go. @grst, committing things in sphinx-gallery format could be a new way. I'll check whether this offers some convenience. In general I'd rather prefer to have the stripped `.ipynb` files in the repo, as it's done in tensorflow. @flying-sheep: I'm not worrying about the build time of the docs, that's decently fast in all my tests so far. @flying-sheep: After a bit of research I've done some time ago, I also don't consider tests based on notebooks a possibility. I made [versions of two notebooks](https://github.com/theislab/scanpy/tree/master/scanpy/tests/notebooks) running on subsampled data producing ugly looking low-resolution figures, which ensures small test times and prevents the repo from blowing up. For the tutorials, I want extremely high resolution figures and no subsampled data. I think I should be almost there. Hopefully we'll have the first two notebooks in the Scanpy repo this or next week...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302
https://github.com/scverse/scanpy/issues/302:804,modifiability,version,versions,804,"There are different ways of stripping output from jupyter notebooks, several of them can be combined with the git commit workflow. I've played around a bit with them. The only thing I need to make sure is if there is a way that people *cannot* commit notebooks with output in them to the scanpy repo. The latter is an absolute no go. @grst, committing things in sphinx-gallery format could be a new way. I'll check whether this offers some convenience. In general I'd rather prefer to have the stripped `.ipynb` files in the repo, as it's done in tensorflow. @flying-sheep: I'm not worrying about the build time of the docs, that's decently fast in all my tests so far. @flying-sheep: After a bit of research I've done some time ago, I also don't consider tests based on notebooks a possibility. I made [versions of two notebooks](https://github.com/theislab/scanpy/tree/master/scanpy/tests/notebooks) running on subsampled data producing ugly looking low-resolution figures, which ensures small test times and prevents the repo from blowing up. For the tutorials, I want extremely high resolution figures and no subsampled data. I think I should be almost there. Hopefully we'll have the first two notebooks in the Scanpy repo this or next week...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302
https://github.com/scverse/scanpy/issues/302:607,performance,time,time,607,"There are different ways of stripping output from jupyter notebooks, several of them can be combined with the git commit workflow. I've played around a bit with them. The only thing I need to make sure is if there is a way that people *cannot* commit notebooks with output in them to the scanpy repo. The latter is an absolute no go. @grst, committing things in sphinx-gallery format could be a new way. I'll check whether this offers some convenience. In general I'd rather prefer to have the stripped `.ipynb` files in the repo, as it's done in tensorflow. @flying-sheep: I'm not worrying about the build time of the docs, that's decently fast in all my tests so far. @flying-sheep: After a bit of research I've done some time ago, I also don't consider tests based on notebooks a possibility. I made [versions of two notebooks](https://github.com/theislab/scanpy/tree/master/scanpy/tests/notebooks) running on subsampled data producing ugly looking low-resolution figures, which ensures small test times and prevents the repo from blowing up. For the tutorials, I want extremely high resolution figures and no subsampled data. I think I should be almost there. Hopefully we'll have the first two notebooks in the Scanpy repo this or next week...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302
https://github.com/scverse/scanpy/issues/302:724,performance,time,time,724,"There are different ways of stripping output from jupyter notebooks, several of them can be combined with the git commit workflow. I've played around a bit with them. The only thing I need to make sure is if there is a way that people *cannot* commit notebooks with output in them to the scanpy repo. The latter is an absolute no go. @grst, committing things in sphinx-gallery format could be a new way. I'll check whether this offers some convenience. In general I'd rather prefer to have the stripped `.ipynb` files in the repo, as it's done in tensorflow. @flying-sheep: I'm not worrying about the build time of the docs, that's decently fast in all my tests so far. @flying-sheep: After a bit of research I've done some time ago, I also don't consider tests based on notebooks a possibility. I made [versions of two notebooks](https://github.com/theislab/scanpy/tree/master/scanpy/tests/notebooks) running on subsampled data producing ugly looking low-resolution figures, which ensures small test times and prevents the repo from blowing up. For the tutorials, I want extremely high resolution figures and no subsampled data. I think I should be almost there. Hopefully we'll have the first two notebooks in the Scanpy repo this or next week...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302
https://github.com/scverse/scanpy/issues/302:1001,performance,time,times,1001,"There are different ways of stripping output from jupyter notebooks, several of them can be combined with the git commit workflow. I've played around a bit with them. The only thing I need to make sure is if there is a way that people *cannot* commit notebooks with output in them to the scanpy repo. The latter is an absolute no go. @grst, committing things in sphinx-gallery format could be a new way. I'll check whether this offers some convenience. In general I'd rather prefer to have the stripped `.ipynb` files in the repo, as it's done in tensorflow. @flying-sheep: I'm not worrying about the build time of the docs, that's decently fast in all my tests so far. @flying-sheep: After a bit of research I've done some time ago, I also don't consider tests based on notebooks a possibility. I made [versions of two notebooks](https://github.com/theislab/scanpy/tree/master/scanpy/tests/notebooks) running on subsampled data producing ugly looking low-resolution figures, which ensures small test times and prevents the repo from blowing up. For the tutorials, I want extremely high resolution figures and no subsampled data. I think I should be almost there. Hopefully we'll have the first two notebooks in the Scanpy repo this or next week...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302
https://github.com/scverse/scanpy/issues/302:656,safety,test,tests,656,"There are different ways of stripping output from jupyter notebooks, several of them can be combined with the git commit workflow. I've played around a bit with them. The only thing I need to make sure is if there is a way that people *cannot* commit notebooks with output in them to the scanpy repo. The latter is an absolute no go. @grst, committing things in sphinx-gallery format could be a new way. I'll check whether this offers some convenience. In general I'd rather prefer to have the stripped `.ipynb` files in the repo, as it's done in tensorflow. @flying-sheep: I'm not worrying about the build time of the docs, that's decently fast in all my tests so far. @flying-sheep: After a bit of research I've done some time ago, I also don't consider tests based on notebooks a possibility. I made [versions of two notebooks](https://github.com/theislab/scanpy/tree/master/scanpy/tests/notebooks) running on subsampled data producing ugly looking low-resolution figures, which ensures small test times and prevents the repo from blowing up. For the tutorials, I want extremely high resolution figures and no subsampled data. I think I should be almost there. Hopefully we'll have the first two notebooks in the Scanpy repo this or next week...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302
https://github.com/scverse/scanpy/issues/302:756,safety,test,tests,756,"There are different ways of stripping output from jupyter notebooks, several of them can be combined with the git commit workflow. I've played around a bit with them. The only thing I need to make sure is if there is a way that people *cannot* commit notebooks with output in them to the scanpy repo. The latter is an absolute no go. @grst, committing things in sphinx-gallery format could be a new way. I'll check whether this offers some convenience. In general I'd rather prefer to have the stripped `.ipynb` files in the repo, as it's done in tensorflow. @flying-sheep: I'm not worrying about the build time of the docs, that's decently fast in all my tests so far. @flying-sheep: After a bit of research I've done some time ago, I also don't consider tests based on notebooks a possibility. I made [versions of two notebooks](https://github.com/theislab/scanpy/tree/master/scanpy/tests/notebooks) running on subsampled data producing ugly looking low-resolution figures, which ensures small test times and prevents the repo from blowing up. For the tutorials, I want extremely high resolution figures and no subsampled data. I think I should be almost there. Hopefully we'll have the first two notebooks in the Scanpy repo this or next week...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302
https://github.com/scverse/scanpy/issues/302:885,safety,test,tests,885,"There are different ways of stripping output from jupyter notebooks, several of them can be combined with the git commit workflow. I've played around a bit with them. The only thing I need to make sure is if there is a way that people *cannot* commit notebooks with output in them to the scanpy repo. The latter is an absolute no go. @grst, committing things in sphinx-gallery format could be a new way. I'll check whether this offers some convenience. In general I'd rather prefer to have the stripped `.ipynb` files in the repo, as it's done in tensorflow. @flying-sheep: I'm not worrying about the build time of the docs, that's decently fast in all my tests so far. @flying-sheep: After a bit of research I've done some time ago, I also don't consider tests based on notebooks a possibility. I made [versions of two notebooks](https://github.com/theislab/scanpy/tree/master/scanpy/tests/notebooks) running on subsampled data producing ugly looking low-resolution figures, which ensures small test times and prevents the repo from blowing up. For the tutorials, I want extremely high resolution figures and no subsampled data. I think I should be almost there. Hopefully we'll have the first two notebooks in the Scanpy repo this or next week...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302
https://github.com/scverse/scanpy/issues/302:996,safety,test,test,996,"There are different ways of stripping output from jupyter notebooks, several of them can be combined with the git commit workflow. I've played around a bit with them. The only thing I need to make sure is if there is a way that people *cannot* commit notebooks with output in them to the scanpy repo. The latter is an absolute no go. @grst, committing things in sphinx-gallery format could be a new way. I'll check whether this offers some convenience. In general I'd rather prefer to have the stripped `.ipynb` files in the repo, as it's done in tensorflow. @flying-sheep: I'm not worrying about the build time of the docs, that's decently fast in all my tests so far. @flying-sheep: After a bit of research I've done some time ago, I also don't consider tests based on notebooks a possibility. I made [versions of two notebooks](https://github.com/theislab/scanpy/tree/master/scanpy/tests/notebooks) running on subsampled data producing ugly looking low-resolution figures, which ensures small test times and prevents the repo from blowing up. For the tutorials, I want extremely high resolution figures and no subsampled data. I think I should be almost there. Hopefully we'll have the first two notebooks in the Scanpy repo this or next week...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302
https://github.com/scverse/scanpy/issues/302:1011,safety,prevent,prevents,1011,"There are different ways of stripping output from jupyter notebooks, several of them can be combined with the git commit workflow. I've played around a bit with them. The only thing I need to make sure is if there is a way that people *cannot* commit notebooks with output in them to the scanpy repo. The latter is an absolute no go. @grst, committing things in sphinx-gallery format could be a new way. I'll check whether this offers some convenience. In general I'd rather prefer to have the stripped `.ipynb` files in the repo, as it's done in tensorflow. @flying-sheep: I'm not worrying about the build time of the docs, that's decently fast in all my tests so far. @flying-sheep: After a bit of research I've done some time ago, I also don't consider tests based on notebooks a possibility. I made [versions of two notebooks](https://github.com/theislab/scanpy/tree/master/scanpy/tests/notebooks) running on subsampled data producing ugly looking low-resolution figures, which ensures small test times and prevents the repo from blowing up. For the tutorials, I want extremely high resolution figures and no subsampled data. I think I should be almost there. Hopefully we'll have the first two notebooks in the Scanpy repo this or next week...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302
https://github.com/scverse/scanpy/issues/302:1011,security,preven,prevents,1011,"There are different ways of stripping output from jupyter notebooks, several of them can be combined with the git commit workflow. I've played around a bit with them. The only thing I need to make sure is if there is a way that people *cannot* commit notebooks with output in them to the scanpy repo. The latter is an absolute no go. @grst, committing things in sphinx-gallery format could be a new way. I'll check whether this offers some convenience. In general I'd rather prefer to have the stripped `.ipynb` files in the repo, as it's done in tensorflow. @flying-sheep: I'm not worrying about the build time of the docs, that's decently fast in all my tests so far. @flying-sheep: After a bit of research I've done some time ago, I also don't consider tests based on notebooks a possibility. I made [versions of two notebooks](https://github.com/theislab/scanpy/tree/master/scanpy/tests/notebooks) running on subsampled data producing ugly looking low-resolution figures, which ensures small test times and prevents the repo from blowing up. For the tutorials, I want extremely high resolution figures and no subsampled data. I think I should be almost there. Hopefully we'll have the first two notebooks in the Scanpy repo this or next week...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302
https://github.com/scverse/scanpy/issues/302:656,testability,test,tests,656,"There are different ways of stripping output from jupyter notebooks, several of them can be combined with the git commit workflow. I've played around a bit with them. The only thing I need to make sure is if there is a way that people *cannot* commit notebooks with output in them to the scanpy repo. The latter is an absolute no go. @grst, committing things in sphinx-gallery format could be a new way. I'll check whether this offers some convenience. In general I'd rather prefer to have the stripped `.ipynb` files in the repo, as it's done in tensorflow. @flying-sheep: I'm not worrying about the build time of the docs, that's decently fast in all my tests so far. @flying-sheep: After a bit of research I've done some time ago, I also don't consider tests based on notebooks a possibility. I made [versions of two notebooks](https://github.com/theislab/scanpy/tree/master/scanpy/tests/notebooks) running on subsampled data producing ugly looking low-resolution figures, which ensures small test times and prevents the repo from blowing up. For the tutorials, I want extremely high resolution figures and no subsampled data. I think I should be almost there. Hopefully we'll have the first two notebooks in the Scanpy repo this or next week...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302
https://github.com/scverse/scanpy/issues/302:756,testability,test,tests,756,"There are different ways of stripping output from jupyter notebooks, several of them can be combined with the git commit workflow. I've played around a bit with them. The only thing I need to make sure is if there is a way that people *cannot* commit notebooks with output in them to the scanpy repo. The latter is an absolute no go. @grst, committing things in sphinx-gallery format could be a new way. I'll check whether this offers some convenience. In general I'd rather prefer to have the stripped `.ipynb` files in the repo, as it's done in tensorflow. @flying-sheep: I'm not worrying about the build time of the docs, that's decently fast in all my tests so far. @flying-sheep: After a bit of research I've done some time ago, I also don't consider tests based on notebooks a possibility. I made [versions of two notebooks](https://github.com/theislab/scanpy/tree/master/scanpy/tests/notebooks) running on subsampled data producing ugly looking low-resolution figures, which ensures small test times and prevents the repo from blowing up. For the tutorials, I want extremely high resolution figures and no subsampled data. I think I should be almost there. Hopefully we'll have the first two notebooks in the Scanpy repo this or next week...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302
https://github.com/scverse/scanpy/issues/302:885,testability,test,tests,885,"There are different ways of stripping output from jupyter notebooks, several of them can be combined with the git commit workflow. I've played around a bit with them. The only thing I need to make sure is if there is a way that people *cannot* commit notebooks with output in them to the scanpy repo. The latter is an absolute no go. @grst, committing things in sphinx-gallery format could be a new way. I'll check whether this offers some convenience. In general I'd rather prefer to have the stripped `.ipynb` files in the repo, as it's done in tensorflow. @flying-sheep: I'm not worrying about the build time of the docs, that's decently fast in all my tests so far. @flying-sheep: After a bit of research I've done some time ago, I also don't consider tests based on notebooks a possibility. I made [versions of two notebooks](https://github.com/theislab/scanpy/tree/master/scanpy/tests/notebooks) running on subsampled data producing ugly looking low-resolution figures, which ensures small test times and prevents the repo from blowing up. For the tutorials, I want extremely high resolution figures and no subsampled data. I think I should be almost there. Hopefully we'll have the first two notebooks in the Scanpy repo this or next week...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302
https://github.com/scverse/scanpy/issues/302:996,testability,test,test,996,"There are different ways of stripping output from jupyter notebooks, several of them can be combined with the git commit workflow. I've played around a bit with them. The only thing I need to make sure is if there is a way that people *cannot* commit notebooks with output in them to the scanpy repo. The latter is an absolute no go. @grst, committing things in sphinx-gallery format could be a new way. I'll check whether this offers some convenience. In general I'd rather prefer to have the stripped `.ipynb` files in the repo, as it's done in tensorflow. @flying-sheep: I'm not worrying about the build time of the docs, that's decently fast in all my tests so far. @flying-sheep: After a bit of research I've done some time ago, I also don't consider tests based on notebooks a possibility. I made [versions of two notebooks](https://github.com/theislab/scanpy/tree/master/scanpy/tests/notebooks) running on subsampled data producing ugly looking low-resolution figures, which ensures small test times and prevents the repo from blowing up. For the tutorials, I want extremely high resolution figures and no subsampled data. I think I should be almost there. Hopefully we'll have the first two notebooks in the Scanpy repo this or next week...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302
https://github.com/scverse/scanpy/issues/302:121,usability,workflow,workflow,121,"There are different ways of stripping output from jupyter notebooks, several of them can be combined with the git commit workflow. I've played around a bit with them. The only thing I need to make sure is if there is a way that people *cannot* commit notebooks with output in them to the scanpy repo. The latter is an absolute no go. @grst, committing things in sphinx-gallery format could be a new way. I'll check whether this offers some convenience. In general I'd rather prefer to have the stripped `.ipynb` files in the repo, as it's done in tensorflow. @flying-sheep: I'm not worrying about the build time of the docs, that's decently fast in all my tests so far. @flying-sheep: After a bit of research I've done some time ago, I also don't consider tests based on notebooks a possibility. I made [versions of two notebooks](https://github.com/theislab/scanpy/tree/master/scanpy/tests/notebooks) running on subsampled data producing ugly looking low-resolution figures, which ensures small test times and prevents the repo from blowing up. For the tutorials, I want extremely high resolution figures and no subsampled data. I think I should be almost there. Hopefully we'll have the first two notebooks in the Scanpy repo this or next week...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302
https://github.com/scverse/scanpy/issues/302:475,usability,prefer,prefer,475,"There are different ways of stripping output from jupyter notebooks, several of them can be combined with the git commit workflow. I've played around a bit with them. The only thing I need to make sure is if there is a way that people *cannot* commit notebooks with output in them to the scanpy repo. The latter is an absolute no go. @grst, committing things in sphinx-gallery format could be a new way. I'll check whether this offers some convenience. In general I'd rather prefer to have the stripped `.ipynb` files in the repo, as it's done in tensorflow. @flying-sheep: I'm not worrying about the build time of the docs, that's decently fast in all my tests so far. @flying-sheep: After a bit of research I've done some time ago, I also don't consider tests based on notebooks a possibility. I made [versions of two notebooks](https://github.com/theislab/scanpy/tree/master/scanpy/tests/notebooks) running on subsampled data producing ugly looking low-resolution figures, which ensures small test times and prevents the repo from blowing up. For the tutorials, I want extremely high resolution figures and no subsampled data. I think I should be almost there. Hopefully we'll have the first two notebooks in the Scanpy repo this or next week...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302
https://github.com/scverse/scanpy/issues/302:252,interoperability,format,format,252,"> The only thing I need to make sure is if there is a way that people cannot commit notebooks with output in them to the scanpy repo. The latter is an absolute no go. Should be easy to have that checked by travis. > committing things in sphinx-gallery format could be a new way. I'll check whether this offers some convenience. If you have any questions/issues regarding this approach, don't hesitate to open an issue in the jupytext repo. The maintainer is extremely responsive and willing to help.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302
https://github.com/scverse/scanpy/issues/302:444,modifiability,maintain,maintainer,444,"> The only thing I need to make sure is if there is a way that people cannot commit notebooks with output in them to the scanpy repo. The latter is an absolute no go. Should be easy to have that checked by travis. > committing things in sphinx-gallery format could be a new way. I'll check whether this offers some convenience. If you have any questions/issues regarding this approach, don't hesitate to open an issue in the jupytext repo. The maintainer is extremely responsive and willing to help.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302
https://github.com/scverse/scanpy/issues/302:444,safety,maintain,maintainer,444,"> The only thing I need to make sure is if there is a way that people cannot commit notebooks with output in them to the scanpy repo. The latter is an absolute no go. Should be easy to have that checked by travis. > committing things in sphinx-gallery format could be a new way. I'll check whether this offers some convenience. If you have any questions/issues regarding this approach, don't hesitate to open an issue in the jupytext repo. The maintainer is extremely responsive and willing to help.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302
https://github.com/scverse/scanpy/issues/302:468,usability,responsiv,responsive,468,"> The only thing I need to make sure is if there is a way that people cannot commit notebooks with output in them to the scanpy repo. The latter is an absolute no go. Should be easy to have that checked by travis. > committing things in sphinx-gallery format could be a new way. I'll check whether this offers some convenience. If you have any questions/issues regarding this approach, don't hesitate to open an issue in the jupytext repo. The maintainer is extremely responsive and willing to help.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302
https://github.com/scverse/scanpy/issues/302:494,usability,help,help,494,"> The only thing I need to make sure is if there is a way that people cannot commit notebooks with output in them to the scanpy repo. The latter is an absolute no go. Should be easy to have that checked by travis. > committing things in sphinx-gallery format could be a new way. I'll check whether this offers some convenience. If you have any questions/issues regarding this approach, don't hesitate to open an issue in the jupytext repo. The maintainer is extremely responsive and willing to help.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302
https://github.com/scverse/scanpy/issues/302:413,deployability,build,building,413,"@flying-sheep and anyone interested: I started putting the notebooks on `scanpy-tutorials`: https://github.com/theislab/scanpy-tutorials and https://scanpy.readthedocs.io/en/latest/tutorials.html now links to the docs generated from `scanpy-tutorials`. I'm doing it like this for now as it's quite a bit less work than getting everything to run on readthedocs, there might indeed be problems with the runtime for building the docs and I think this updated solution isn't so bad after all... . Opinions are welcome!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302
https://github.com/scverse/scanpy/issues/302:448,deployability,updat,updated,448,"@flying-sheep and anyone interested: I started putting the notebooks on `scanpy-tutorials`: https://github.com/theislab/scanpy-tutorials and https://scanpy.readthedocs.io/en/latest/tutorials.html now links to the docs generated from `scanpy-tutorials`. I'm doing it like this for now as it's quite a bit less work than getting everything to run on readthedocs, there might indeed be problems with the runtime for building the docs and I think this updated solution isn't so bad after all... . Opinions are welcome!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302
https://github.com/scverse/scanpy/issues/302:448,safety,updat,updated,448,"@flying-sheep and anyone interested: I started putting the notebooks on `scanpy-tutorials`: https://github.com/theislab/scanpy-tutorials and https://scanpy.readthedocs.io/en/latest/tutorials.html now links to the docs generated from `scanpy-tutorials`. I'm doing it like this for now as it's quite a bit less work than getting everything to run on readthedocs, there might indeed be problems with the runtime for building the docs and I think this updated solution isn't so bad after all... . Opinions are welcome!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302
https://github.com/scverse/scanpy/issues/302:448,security,updat,updated,448,"@flying-sheep and anyone interested: I started putting the notebooks on `scanpy-tutorials`: https://github.com/theislab/scanpy-tutorials and https://scanpy.readthedocs.io/en/latest/tutorials.html now links to the docs generated from `scanpy-tutorials`. I'm doing it like this for now as it's quite a bit less work than getting everything to run on readthedocs, there might indeed be problems with the runtime for building the docs and I think this updated solution isn't so bad after all... . Opinions are welcome!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/302
https://github.com/scverse/scanpy/issues/303:731,interoperability,format,format,731,"OK, very interesting! Can we have a video call on this? I'd be very interested in seeing a few benchmarks. . At first sight, I'd say it shouldn't be that as the problem also appears when there are no ""deep"" recursions. I'd have thought that it could be this line that brings considerable performance gain (I sent you the reference in an email some time ago):. https://github.com/cmap/cmapPy/blob/7a2e18030f713865e8038bc7351e5ca44d061205/cmapPy/pandasGEXpress/parse_gctx.py#L332-L333. To get away from the recursions and to use `read_direct`, one needs to start exploiting the naming conventions in the `.h5ad` files. As these has have converged since about a year ago, it's save to do it, along with a table that explains the file format and provides an official reference. Right now, the only reference on the file format is [this](https://github.com/theislab/scanpy_usage/blob/master/170505_seurat/info_h5ad.md), which is ridiculous. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/303
https://github.com/scverse/scanpy/issues/303:816,interoperability,format,format,816,"OK, very interesting! Can we have a video call on this? I'd be very interested in seeing a few benchmarks. . At first sight, I'd say it shouldn't be that as the problem also appears when there are no ""deep"" recursions. I'd have thought that it could be this line that brings considerable performance gain (I sent you the reference in an email some time ago):. https://github.com/cmap/cmapPy/blob/7a2e18030f713865e8038bc7351e5ca44d061205/cmapPy/pandasGEXpress/parse_gctx.py#L332-L333. To get away from the recursions and to use `read_direct`, one needs to start exploiting the naming conventions in the `.h5ad` files. As these has have converged since about a year ago, it's save to do it, along with a table that explains the file format and provides an official reference. Right now, the only reference on the file format is [this](https://github.com/theislab/scanpy_usage/blob/master/170505_seurat/info_h5ad.md), which is ridiculous. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/303
https://github.com/scverse/scanpy/issues/303:288,performance,perform,performance,288,"OK, very interesting! Can we have a video call on this? I'd be very interested in seeing a few benchmarks. . At first sight, I'd say it shouldn't be that as the problem also appears when there are no ""deep"" recursions. I'd have thought that it could be this line that brings considerable performance gain (I sent you the reference in an email some time ago):. https://github.com/cmap/cmapPy/blob/7a2e18030f713865e8038bc7351e5ca44d061205/cmapPy/pandasGEXpress/parse_gctx.py#L332-L333. To get away from the recursions and to use `read_direct`, one needs to start exploiting the naming conventions in the `.h5ad` files. As these has have converged since about a year ago, it's save to do it, along with a table that explains the file format and provides an official reference. Right now, the only reference on the file format is [this](https://github.com/theislab/scanpy_usage/blob/master/170505_seurat/info_h5ad.md), which is ridiculous. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/303
https://github.com/scverse/scanpy/issues/303:348,performance,time,time,348,"OK, very interesting! Can we have a video call on this? I'd be very interested in seeing a few benchmarks. . At first sight, I'd say it shouldn't be that as the problem also appears when there are no ""deep"" recursions. I'd have thought that it could be this line that brings considerable performance gain (I sent you the reference in an email some time ago):. https://github.com/cmap/cmapPy/blob/7a2e18030f713865e8038bc7351e5ca44d061205/cmapPy/pandasGEXpress/parse_gctx.py#L332-L333. To get away from the recursions and to use `read_direct`, one needs to start exploiting the naming conventions in the `.h5ad` files. As these has have converged since about a year ago, it's save to do it, along with a table that explains the file format and provides an official reference. Right now, the only reference on the file format is [this](https://github.com/theislab/scanpy_usage/blob/master/170505_seurat/info_h5ad.md), which is ridiculous. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/303
https://github.com/scverse/scanpy/issues/303:288,usability,perform,performance,288,"OK, very interesting! Can we have a video call on this? I'd be very interested in seeing a few benchmarks. . At first sight, I'd say it shouldn't be that as the problem also appears when there are no ""deep"" recursions. I'd have thought that it could be this line that brings considerable performance gain (I sent you the reference in an email some time ago):. https://github.com/cmap/cmapPy/blob/7a2e18030f713865e8038bc7351e5ca44d061205/cmapPy/pandasGEXpress/parse_gctx.py#L332-L333. To get away from the recursions and to use `read_direct`, one needs to start exploiting the naming conventions in the `.h5ad` files. As these has have converged since about a year ago, it's save to do it, along with a table that explains the file format and provides an official reference. Right now, the only reference on the file format is [this](https://github.com/theislab/scanpy_usage/blob/master/170505_seurat/info_h5ad.md), which is ridiculous. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/303
https://github.com/scverse/scanpy/issues/303:374,integrability,transform,transforming,374,"I haven't tried `read_direct ` yet but, in my opinion, it is not that helpful when we are reading the full array in memory without any type conversions. But i will check it of course. Now it seems like the problem in the recursion as reading simple files with pre-specified paths is faster and takes less memory. Also, it can be that the problem is somewhere in the step of transforming dictionary to AnnData, but i don't see where for now. I'll check a few things, prepare readable benchmarks next week and we can have a call about it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/303
https://github.com/scverse/scanpy/issues/303:140,interoperability,convers,conversions,140,"I haven't tried `read_direct ` yet but, in my opinion, it is not that helpful when we are reading the full array in memory without any type conversions. But i will check it of course. Now it seems like the problem in the recursion as reading simple files with pre-specified paths is faster and takes less memory. Also, it can be that the problem is somewhere in the step of transforming dictionary to AnnData, but i don't see where for now. I'll check a few things, prepare readable benchmarks next week and we can have a call about it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/303
https://github.com/scverse/scanpy/issues/303:264,interoperability,specif,specified,264,"I haven't tried `read_direct ` yet but, in my opinion, it is not that helpful when we are reading the full array in memory without any type conversions. But i will check it of course. Now it seems like the problem in the recursion as reading simple files with pre-specified paths is faster and takes less memory. Also, it can be that the problem is somewhere in the step of transforming dictionary to AnnData, but i don't see where for now. I'll check a few things, prepare readable benchmarks next week and we can have a call about it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/303
https://github.com/scverse/scanpy/issues/303:374,interoperability,transform,transforming,374,"I haven't tried `read_direct ` yet but, in my opinion, it is not that helpful when we are reading the full array in memory without any type conversions. But i will check it of course. Now it seems like the problem in the recursion as reading simple files with pre-specified paths is faster and takes less memory. Also, it can be that the problem is somewhere in the step of transforming dictionary to AnnData, but i don't see where for now. I'll check a few things, prepare readable benchmarks next week and we can have a call about it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/303
https://github.com/scverse/scanpy/issues/303:116,performance,memor,memory,116,"I haven't tried `read_direct ` yet but, in my opinion, it is not that helpful when we are reading the full array in memory without any type conversions. But i will check it of course. Now it seems like the problem in the recursion as reading simple files with pre-specified paths is faster and takes less memory. Also, it can be that the problem is somewhere in the step of transforming dictionary to AnnData, but i don't see where for now. I'll check a few things, prepare readable benchmarks next week and we can have a call about it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/303
https://github.com/scverse/scanpy/issues/303:305,performance,memor,memory,305,"I haven't tried `read_direct ` yet but, in my opinion, it is not that helpful when we are reading the full array in memory without any type conversions. But i will check it of course. Now it seems like the problem in the recursion as reading simple files with pre-specified paths is faster and takes less memory. Also, it can be that the problem is somewhere in the step of transforming dictionary to AnnData, but i don't see where for now. I'll check a few things, prepare readable benchmarks next week and we can have a call about it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/303
https://github.com/scverse/scanpy/issues/303:242,testability,simpl,simple,242,"I haven't tried `read_direct ` yet but, in my opinion, it is not that helpful when we are reading the full array in memory without any type conversions. But i will check it of course. Now it seems like the problem in the recursion as reading simple files with pre-specified paths is faster and takes less memory. Also, it can be that the problem is somewhere in the step of transforming dictionary to AnnData, but i don't see where for now. I'll check a few things, prepare readable benchmarks next week and we can have a call about it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/303
https://github.com/scverse/scanpy/issues/303:70,usability,help,helpful,70,"I haven't tried `read_direct ` yet but, in my opinion, it is not that helpful when we are reading the full array in memory without any type conversions. But i will check it of course. Now it seems like the problem in the recursion as reading simple files with pre-specified paths is faster and takes less memory. Also, it can be that the problem is somewhere in the step of transforming dictionary to AnnData, but i don't see where for now. I'll check a few things, prepare readable benchmarks next week and we can have a call about it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/303
https://github.com/scverse/scanpy/issues/303:116,usability,memor,memory,116,"I haven't tried `read_direct ` yet but, in my opinion, it is not that helpful when we are reading the full array in memory without any type conversions. But i will check it of course. Now it seems like the problem in the recursion as reading simple files with pre-specified paths is faster and takes less memory. Also, it can be that the problem is somewhere in the step of transforming dictionary to AnnData, but i don't see where for now. I'll check a few things, prepare readable benchmarks next week and we can have a call about it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/303
https://github.com/scverse/scanpy/issues/303:242,usability,simpl,simple,242,"I haven't tried `read_direct ` yet but, in my opinion, it is not that helpful when we are reading the full array in memory without any type conversions. But i will check it of course. Now it seems like the problem in the recursion as reading simple files with pre-specified paths is faster and takes less memory. Also, it can be that the problem is somewhere in the step of transforming dictionary to AnnData, but i don't see where for now. I'll check a few things, prepare readable benchmarks next week and we can have a call about it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/303
https://github.com/scverse/scanpy/issues/303:305,usability,memor,memory,305,"I haven't tried `read_direct ` yet but, in my opinion, it is not that helpful when we are reading the full array in memory without any type conversions. But i will check it of course. Now it seems like the problem in the recursion as reading simple files with pre-specified paths is faster and takes less memory. Also, it can be that the problem is somewhere in the step of transforming dictionary to AnnData, but i don't see where for now. I'll check a few things, prepare readable benchmarks next week and we can have a call about it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/303
https://github.com/scverse/scanpy/issues/303:273,deployability,contain,container,273,"Awesome, thank you! Making use of the file conventions, we can move completely away from the dict. The way this was done is a pain and is really only there for historical reasons (I started working with dicts and then @flying-sheep said I shouldn't do that but make a data container...). So, I'm more than happy if the dict disappears completely and instead, one simply walks through the files and checks for the presence of certain predefined things. Of course, there will still be a lot of flexibility and a need to iterate through the `.uns` group, which can store dicts. But I hope that this won't be a performance bottleneck, as it's all small-scale.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/303
https://github.com/scverse/scanpy/issues/303:649,deployability,scale,scale,649,"Awesome, thank you! Making use of the file conventions, we can move completely away from the dict. The way this was done is a pain and is really only there for historical reasons (I started working with dicts and then @flying-sheep said I shouldn't do that but make a data container...). So, I'm more than happy if the dict disappears completely and instead, one simply walks through the files and checks for the presence of certain predefined things. Of course, there will still be a lot of flexibility and a need to iterate through the `.uns` group, which can store dicts. But I hope that this won't be a performance bottleneck, as it's all small-scale.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/303
https://github.com/scverse/scanpy/issues/303:649,energy efficiency,scale,scale,649,"Awesome, thank you! Making use of the file conventions, we can move completely away from the dict. The way this was done is a pain and is really only there for historical reasons (I started working with dicts and then @flying-sheep said I shouldn't do that but make a data container...). So, I'm more than happy if the dict disappears completely and instead, one simply walks through the files and checks for the presence of certain predefined things. Of course, there will still be a lot of flexibility and a need to iterate through the `.uns` group, which can store dicts. But I hope that this won't be a performance bottleneck, as it's all small-scale.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/303
https://github.com/scverse/scanpy/issues/303:649,modifiability,scal,scale,649,"Awesome, thank you! Making use of the file conventions, we can move completely away from the dict. The way this was done is a pain and is really only there for historical reasons (I started working with dicts and then @flying-sheep said I shouldn't do that but make a data container...). So, I'm more than happy if the dict disappears completely and instead, one simply walks through the files and checks for the presence of certain predefined things. Of course, there will still be a lot of flexibility and a need to iterate through the `.uns` group, which can store dicts. But I hope that this won't be a performance bottleneck, as it's all small-scale.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/303
https://github.com/scverse/scanpy/issues/303:607,performance,perform,performance,607,"Awesome, thank you! Making use of the file conventions, we can move completely away from the dict. The way this was done is a pain and is really only there for historical reasons (I started working with dicts and then @flying-sheep said I shouldn't do that but make a data container...). So, I'm more than happy if the dict disappears completely and instead, one simply walks through the files and checks for the presence of certain predefined things. Of course, there will still be a lot of flexibility and a need to iterate through the `.uns` group, which can store dicts. But I hope that this won't be a performance bottleneck, as it's all small-scale.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/303
https://github.com/scverse/scanpy/issues/303:619,performance,bottleneck,bottleneck,619,"Awesome, thank you! Making use of the file conventions, we can move completely away from the dict. The way this was done is a pain and is really only there for historical reasons (I started working with dicts and then @flying-sheep said I shouldn't do that but make a data container...). So, I'm more than happy if the dict disappears completely and instead, one simply walks through the files and checks for the presence of certain predefined things. Of course, there will still be a lot of flexibility and a need to iterate through the `.uns` group, which can store dicts. But I hope that this won't be a performance bottleneck, as it's all small-scale.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/303
https://github.com/scverse/scanpy/issues/303:649,performance,scale,scale,649,"Awesome, thank you! Making use of the file conventions, we can move completely away from the dict. The way this was done is a pain and is really only there for historical reasons (I started working with dicts and then @flying-sheep said I shouldn't do that but make a data container...). So, I'm more than happy if the dict disappears completely and instead, one simply walks through the files and checks for the presence of certain predefined things. Of course, there will still be a lot of flexibility and a need to iterate through the `.uns` group, which can store dicts. But I hope that this won't be a performance bottleneck, as it's all small-scale.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/303
https://github.com/scverse/scanpy/issues/303:68,safety,compl,completely,68,"Awesome, thank you! Making use of the file conventions, we can move completely away from the dict. The way this was done is a pain and is really only there for historical reasons (I started working with dicts and then @flying-sheep said I shouldn't do that but make a data container...). So, I'm more than happy if the dict disappears completely and instead, one simply walks through the files and checks for the presence of certain predefined things. Of course, there will still be a lot of flexibility and a need to iterate through the `.uns` group, which can store dicts. But I hope that this won't be a performance bottleneck, as it's all small-scale.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/303
https://github.com/scverse/scanpy/issues/303:335,safety,compl,completely,335,"Awesome, thank you! Making use of the file conventions, we can move completely away from the dict. The way this was done is a pain and is really only there for historical reasons (I started working with dicts and then @flying-sheep said I shouldn't do that but make a data container...). So, I'm more than happy if the dict disappears completely and instead, one simply walks through the files and checks for the presence of certain predefined things. Of course, there will still be a lot of flexibility and a need to iterate through the `.uns` group, which can store dicts. But I hope that this won't be a performance bottleneck, as it's all small-scale.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/303
https://github.com/scverse/scanpy/issues/303:68,security,compl,completely,68,"Awesome, thank you! Making use of the file conventions, we can move completely away from the dict. The way this was done is a pain and is really only there for historical reasons (I started working with dicts and then @flying-sheep said I shouldn't do that but make a data container...). So, I'm more than happy if the dict disappears completely and instead, one simply walks through the files and checks for the presence of certain predefined things. Of course, there will still be a lot of flexibility and a need to iterate through the `.uns` group, which can store dicts. But I hope that this won't be a performance bottleneck, as it's all small-scale.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/303
https://github.com/scverse/scanpy/issues/303:335,security,compl,completely,335,"Awesome, thank you! Making use of the file conventions, we can move completely away from the dict. The way this was done is a pain and is really only there for historical reasons (I started working with dicts and then @flying-sheep said I shouldn't do that but make a data container...). So, I'm more than happy if the dict disappears completely and instead, one simply walks through the files and checks for the presence of certain predefined things. Of course, there will still be a lot of flexibility and a need to iterate through the `.uns` group, which can store dicts. But I hope that this won't be a performance bottleneck, as it's all small-scale.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/303
https://github.com/scverse/scanpy/issues/303:363,testability,simpl,simply,363,"Awesome, thank you! Making use of the file conventions, we can move completely away from the dict. The way this was done is a pain and is really only there for historical reasons (I started working with dicts and then @flying-sheep said I shouldn't do that but make a data container...). So, I'm more than happy if the dict disappears completely and instead, one simply walks through the files and checks for the presence of certain predefined things. Of course, there will still be a lot of flexibility and a need to iterate through the `.uns` group, which can store dicts. But I hope that this won't be a performance bottleneck, as it's all small-scale.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/303
https://github.com/scverse/scanpy/issues/303:363,usability,simpl,simply,363,"Awesome, thank you! Making use of the file conventions, we can move completely away from the dict. The way this was done is a pain and is really only there for historical reasons (I started working with dicts and then @flying-sheep said I shouldn't do that but make a data container...). So, I'm more than happy if the dict disappears completely and instead, one simply walks through the files and checks for the presence of certain predefined things. Of course, there will still be a lot of flexibility and a need to iterate through the `.uns` group, which can store dicts. But I hope that this won't be a performance bottleneck, as it's all small-scale.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/303
https://github.com/scverse/scanpy/issues/303:607,usability,perform,performance,607,"Awesome, thank you! Making use of the file conventions, we can move completely away from the dict. The way this was done is a pain and is really only there for historical reasons (I started working with dicts and then @flying-sheep said I shouldn't do that but make a data container...). So, I'm more than happy if the dict disappears completely and instead, one simply walks through the files and checks for the presence of certain predefined things. Of course, there will still be a lot of flexibility and a need to iterate through the `.uns` group, which can store dicts. But I hope that this won't be a performance bottleneck, as it's all small-scale.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/303
https://github.com/scverse/scanpy/issues/303:18,testability,plan,plan,18,"Yeah, this was my plan also.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/303
https://github.com/scverse/scanpy/issues/303:202,usability,efficien,efficient,202,"https://github.com/theislab/anndata/pull/85. https://github.com/Koncopd/anndata-scanpy-benchmarks/blob/master/memory_issue_huge.ipynb. I wasn't right about recursion, almost no effect. `read_direct` is efficient.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/303
https://github.com/scverse/scanpy/pull/304:322,integrability,event,event-,322,"Thanks! On Mon, Oct 15, 2018 at 8:24 PM Alex Wolf <notifications@github.com> wrote:. > Merged #304 <https://github.com/theislab/scanpy/pull/304> into master. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/304#event-1904871033>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1X1kzdMFubtvfmXdm3bfv6i_PGZeks5ulNLfgaJpZM4XcU1H>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/304
https://github.com/scverse/scanpy/pull/304:203,security,auth,authored,203,"Thanks! On Mon, Oct 15, 2018 at 8:24 PM Alex Wolf <notifications@github.com> wrote:. > Merged #304 <https://github.com/theislab/scanpy/pull/304> into master. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/304#event-1904871033>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1X1kzdMFubtvfmXdm3bfv6i_PGZeks5ulNLfgaJpZM4XcU1H>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/304
https://github.com/scverse/scanpy/pull/304:412,security,auth,auth,412,"Thanks! On Mon, Oct 15, 2018 at 8:24 PM Alex Wolf <notifications@github.com> wrote:. > Merged #304 <https://github.com/theislab/scanpy/pull/304> into master. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/304#event-1904871033>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1X1kzdMFubtvfmXdm3bfv6i_PGZeks5ulNLfgaJpZM4XcU1H>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/304
https://github.com/scverse/scanpy/pull/305:133,deployability,instal,installed,133,"Sorry, all of these packages aren't necessary for Scanpy's core functionality, supposed to be treated as extensions and shouldn't be installed by default. Hopefully we'll have a way of handling this that makes it more clear in the future.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/305
https://github.com/scverse/scanpy/pull/305:59,energy efficiency,core,core,59,"Sorry, all of these packages aren't necessary for Scanpy's core functionality, supposed to be treated as extensions and shouldn't be installed by default. Hopefully we'll have a way of handling this that makes it more clear in the future.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/305
https://github.com/scverse/scanpy/pull/305:20,modifiability,pac,packages,20,"Sorry, all of these packages aren't necessary for Scanpy's core functionality, supposed to be treated as extensions and shouldn't be installed by default. Hopefully we'll have a way of handling this that makes it more clear in the future.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/305
https://github.com/scverse/scanpy/pull/305:105,modifiability,extens,extensions,105,"Sorry, all of these packages aren't necessary for Scanpy's core functionality, supposed to be treated as extensions and shouldn't be installed by default. Hopefully we'll have a way of handling this that makes it more clear in the future.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/305
https://github.com/scverse/scanpy/pull/305:218,usability,clear,clear,218,"Sorry, all of these packages aren't necessary for Scanpy's core functionality, supposed to be treated as extensions and shouldn't be installed by default. Hopefully we'll have a way of handling this that makes it more clear in the future.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/305
https://github.com/scverse/scanpy/pull/305:89,availability,down,down,89,"@falexwolf the problem is that there are functions that do now work without them and our down stream packages do not work ootb. Would it be possible to add a second file, e.g. `requirements-ect.txt` to keep track of this? This file could also be used to create testing environments easily.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/305
https://github.com/scverse/scanpy/pull/305:101,modifiability,pac,packages,101,"@falexwolf the problem is that there are functions that do now work without them and our down stream packages do not work ootb. Would it be possible to add a second file, e.g. `requirements-ect.txt` to keep track of this? This file could also be used to create testing environments easily.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/305
https://github.com/scverse/scanpy/pull/305:261,safety,test,testing,261,"@falexwolf the problem is that there are functions that do now work without them and our down stream packages do not work ootb. Would it be possible to add a second file, e.g. `requirements-ect.txt` to keep track of this? This file could also be used to create testing environments easily.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/305
https://github.com/scverse/scanpy/pull/305:261,testability,test,testing,261,"@falexwolf the problem is that there are functions that do now work without them and our down stream packages do not work ootb. Would it be possible to add a second file, e.g. `requirements-ect.txt` to keep track of this? This file could also be used to create testing environments easily.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/305
https://github.com/scverse/scanpy/pull/305:113,reliability,Doe,Doesn,113,here we are:. https://github.com/theislab/scanpy/blob/6c786e70cecc2581a8009f83e3c843c8e998946c/setup.py#L25-L29. Doesn’t look complete though. Would someone like to complete it?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/305
https://github.com/scverse/scanpy/pull/305:126,safety,compl,complete,126,here we are:. https://github.com/theislab/scanpy/blob/6c786e70cecc2581a8009f83e3c843c8e998946c/setup.py#L25-L29. Doesn’t look complete though. Would someone like to complete it?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/305
https://github.com/scverse/scanpy/pull/305:165,safety,compl,complete,165,here we are:. https://github.com/theislab/scanpy/blob/6c786e70cecc2581a8009f83e3c843c8e998946c/setup.py#L25-L29. Doesn’t look complete though. Would someone like to complete it?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/305
https://github.com/scverse/scanpy/pull/305:126,security,compl,complete,126,here we are:. https://github.com/theislab/scanpy/blob/6c786e70cecc2581a8009f83e3c843c8e998946c/setup.py#L25-L29. Doesn’t look complete though. Would someone like to complete it?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/305
https://github.com/scverse/scanpy/pull/305:165,security,compl,complete,165,here we are:. https://github.com/theislab/scanpy/blob/6c786e70cecc2581a8009f83e3c843c8e998946c/setup.py#L25-L29. Doesn’t look complete though. Would someone like to complete it?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/305
https://github.com/scverse/scanpy/pull/305:34,modifiability,pac,packages,34,"Yes, we can simply add @bebatut's packages as an `ext` flag in the setup.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/305
https://github.com/scverse/scanpy/pull/305:12,testability,simpl,simply,12,"Yes, we can simply add @bebatut's packages as an `ext` flag in the setup.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/305
https://github.com/scverse/scanpy/pull/305:12,usability,simpl,simply,12,"Yes, we can simply add @bebatut's packages as an `ext` flag in the setup.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/305
https://github.com/scverse/scanpy/issues/306:28,availability,error,error,28,"It's a bug that there is no error output. I haven't thought about drawing the PAGA graph in higher dimensions yet, hence no possibility to initialize from that. One could think about adding this functionality... but I don't know how meaningful that is at this stage.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/306
https://github.com/scverse/scanpy/issues/306:260,deployability,stage,stage,260,"It's a bug that there is no error output. I haven't thought about drawing the PAGA graph in higher dimensions yet, hence no possibility to initialize from that. One could think about adding this functionality... but I don't know how meaningful that is at this stage.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/306
https://github.com/scverse/scanpy/issues/306:66,energy efficiency,draw,drawing,66,"It's a bug that there is no error output. I haven't thought about drawing the PAGA graph in higher dimensions yet, hence no possibility to initialize from that. One could think about adding this functionality... but I don't know how meaningful that is at this stage.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/306
https://github.com/scverse/scanpy/issues/306:28,performance,error,error,28,"It's a bug that there is no error output. I haven't thought about drawing the PAGA graph in higher dimensions yet, hence no possibility to initialize from that. One could think about adding this functionality... but I don't know how meaningful that is at this stage.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/306
https://github.com/scverse/scanpy/issues/306:28,safety,error,error,28,"It's a bug that there is no error output. I haven't thought about drawing the PAGA graph in higher dimensions yet, hence no possibility to initialize from that. One could think about adding this functionality... but I don't know how meaningful that is at this stage.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/306
https://github.com/scverse/scanpy/issues/306:28,usability,error,error,28,"It's a bug that there is no error output. I haven't thought about drawing the PAGA graph in higher dimensions yet, hence no possibility to initialize from that. One could think about adding this functionality... but I don't know how meaningful that is at this stage.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/306
https://github.com/scverse/scanpy/issues/307:183,deployability,observ,observation,183,"use the `size` parameter. Eg. `size=10`. On Wed, Oct 17, 2018 at 10:38 AM justinesjw <notifications@github.com>. wrote:. > Hi,. >. > I am trying to separate my umap according to some observation. > I'm sure I've have missed it but is there a parameter in sc.pl.umap() to. > control the dot size? >. > Thank you,. > Justine :). >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/307>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1ftCpMP2-O0Wp9Smpawa0E6VxoLnks5uluyHgaJpZM4XjaFP>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/307
https://github.com/scverse/scanpy/issues/307:376,integrability,sub,subscribed,376,"use the `size` parameter. Eg. `size=10`. On Wed, Oct 17, 2018 at 10:38 AM justinesjw <notifications@github.com>. wrote:. > Hi,. >. > I am trying to separate my umap according to some observation. > I'm sure I've have missed it but is there a parameter in sc.pl.umap() to. > control the dot size? >. > Thank you,. > Justine :). >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/307>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1ftCpMP2-O0Wp9Smpawa0E6VxoLnks5uluyHgaJpZM4XjaFP>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/307
https://github.com/scverse/scanpy/issues/307:15,modifiability,paramet,parameter,15,"use the `size` parameter. Eg. `size=10`. On Wed, Oct 17, 2018 at 10:38 AM justinesjw <notifications@github.com>. wrote:. > Hi,. >. > I am trying to separate my umap according to some observation. > I'm sure I've have missed it but is there a parameter in sc.pl.umap() to. > control the dot size? >. > Thank you,. > Justine :). >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/307>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1ftCpMP2-O0Wp9Smpawa0E6VxoLnks5uluyHgaJpZM4XjaFP>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/307
https://github.com/scverse/scanpy/issues/307:242,modifiability,paramet,parameter,242,"use the `size` parameter. Eg. `size=10`. On Wed, Oct 17, 2018 at 10:38 AM justinesjw <notifications@github.com>. wrote:. > Hi,. >. > I am trying to separate my umap according to some observation. > I'm sure I've have missed it but is there a parameter in sc.pl.umap() to. > control the dot size? >. > Thank you,. > Justine :). >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/307>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1ftCpMP2-O0Wp9Smpawa0E6VxoLnks5uluyHgaJpZM4XjaFP>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/307
https://github.com/scverse/scanpy/issues/307:274,security,control,control,274,"use the `size` parameter. Eg. `size=10`. On Wed, Oct 17, 2018 at 10:38 AM justinesjw <notifications@github.com>. wrote:. > Hi,. >. > I am trying to separate my umap according to some observation. > I'm sure I've have missed it but is there a parameter in sc.pl.umap() to. > control the dot size? >. > Thank you,. > Justine :). >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/307>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1ftCpMP2-O0Wp9Smpawa0E6VxoLnks5uluyHgaJpZM4XjaFP>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/307
https://github.com/scverse/scanpy/issues/307:573,security,auth,auth,573,"use the `size` parameter. Eg. `size=10`. On Wed, Oct 17, 2018 at 10:38 AM justinesjw <notifications@github.com>. wrote:. > Hi,. >. > I am trying to separate my umap according to some observation. > I'm sure I've have missed it but is there a parameter in sc.pl.umap() to. > control the dot size? >. > Thank you,. > Justine :). >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/307>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1ftCpMP2-O0Wp9Smpawa0E6VxoLnks5uluyHgaJpZM4XjaFP>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/307
https://github.com/scverse/scanpy/issues/307:183,testability,observ,observation,183,"use the `size` parameter. Eg. `size=10`. On Wed, Oct 17, 2018 at 10:38 AM justinesjw <notifications@github.com>. wrote:. > Hi,. >. > I am trying to separate my umap according to some observation. > I'm sure I've have missed it but is there a parameter in sc.pl.umap() to. > control the dot size? >. > Thank you,. > Justine :). >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/307>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1ftCpMP2-O0Wp9Smpawa0E6VxoLnks5uluyHgaJpZM4XjaFP>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/307
https://github.com/scverse/scanpy/issues/307:274,testability,control,control,274,"use the `size` parameter. Eg. `size=10`. On Wed, Oct 17, 2018 at 10:38 AM justinesjw <notifications@github.com>. wrote:. > Hi,. >. > I am trying to separate my umap according to some observation. > I'm sure I've have missed it but is there a parameter in sc.pl.umap() to. > control the dot size? >. > Thank you,. > Justine :). >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/307>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1ftCpMP2-O0Wp9Smpawa0E6VxoLnks5uluyHgaJpZM4XjaFP>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/307
https://github.com/scverse/scanpy/pull/308:301,energy efficiency,current,currently,301,"So quick! thanks. I have been using the dendrograms for a while so. hopefully not so many bugs appear. Something that I wanted to have for. discussion is on some parameters relevant for the dendrogram, like the. genes used, the correlation method and the linkage method. All this can be. modified but currently is hard coded as I didn't want to add 3 more. parameters to the plotting functions. Maybe you have faced a similar problem and have an elegant solution. I. thought about setting some variables like the rcParams for matplotlib but I. think is not justified for just 3 parameters and can be very confusing. Or. we can have a function to compute a dendrogram with all parameters. required, and save this in .uns like rank_genes_groups. Then if other. functions find this information they add the dendrogram. On Wed, Oct 17, 2018 at 4:30 PM Alex Wolf <notifications@github.com> wrote:. > Merged #308 <https://github.com/theislab/scanpy/pull/308> into master. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/308#event-1909725548>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1RD6Qm1iNFaKaG6elUL189hS5yFcks5ulz8SgaJpZM4Xjwsu>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/308
https://github.com/scverse/scanpy/pull/308:1130,integrability,event,event-,1130,"So quick! thanks. I have been using the dendrograms for a while so. hopefully not so many bugs appear. Something that I wanted to have for. discussion is on some parameters relevant for the dendrogram, like the. genes used, the correlation method and the linkage method. All this can be. modified but currently is hard coded as I didn't want to add 3 more. parameters to the plotting functions. Maybe you have faced a similar problem and have an elegant solution. I. thought about setting some variables like the rcParams for matplotlib but I. think is not justified for just 3 parameters and can be very confusing. Or. we can have a function to compute a dendrogram with all parameters. required, and save this in .uns like rank_genes_groups. Then if other. functions find this information they add the dendrogram. On Wed, Oct 17, 2018 at 4:30 PM Alex Wolf <notifications@github.com> wrote:. > Merged #308 <https://github.com/theislab/scanpy/pull/308> into master. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/308#event-1909725548>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1RD6Qm1iNFaKaG6elUL189hS5yFcks5ulz8SgaJpZM4Xjwsu>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/308
https://github.com/scverse/scanpy/pull/308:162,modifiability,paramet,parameters,162,"So quick! thanks. I have been using the dendrograms for a while so. hopefully not so many bugs appear. Something that I wanted to have for. discussion is on some parameters relevant for the dendrogram, like the. genes used, the correlation method and the linkage method. All this can be. modified but currently is hard coded as I didn't want to add 3 more. parameters to the plotting functions. Maybe you have faced a similar problem and have an elegant solution. I. thought about setting some variables like the rcParams for matplotlib but I. think is not justified for just 3 parameters and can be very confusing. Or. we can have a function to compute a dendrogram with all parameters. required, and save this in .uns like rank_genes_groups. Then if other. functions find this information they add the dendrogram. On Wed, Oct 17, 2018 at 4:30 PM Alex Wolf <notifications@github.com> wrote:. > Merged #308 <https://github.com/theislab/scanpy/pull/308> into master. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/308#event-1909725548>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1RD6Qm1iNFaKaG6elUL189hS5yFcks5ulz8SgaJpZM4Xjwsu>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/308
https://github.com/scverse/scanpy/pull/308:357,modifiability,paramet,parameters,357,"So quick! thanks. I have been using the dendrograms for a while so. hopefully not so many bugs appear. Something that I wanted to have for. discussion is on some parameters relevant for the dendrogram, like the. genes used, the correlation method and the linkage method. All this can be. modified but currently is hard coded as I didn't want to add 3 more. parameters to the plotting functions. Maybe you have faced a similar problem and have an elegant solution. I. thought about setting some variables like the rcParams for matplotlib but I. think is not justified for just 3 parameters and can be very confusing. Or. we can have a function to compute a dendrogram with all parameters. required, and save this in .uns like rank_genes_groups. Then if other. functions find this information they add the dendrogram. On Wed, Oct 17, 2018 at 4:30 PM Alex Wolf <notifications@github.com> wrote:. > Merged #308 <https://github.com/theislab/scanpy/pull/308> into master. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/308#event-1909725548>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1RD6Qm1iNFaKaG6elUL189hS5yFcks5ulz8SgaJpZM4Xjwsu>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/308
https://github.com/scverse/scanpy/pull/308:494,modifiability,variab,variables,494,"So quick! thanks. I have been using the dendrograms for a while so. hopefully not so many bugs appear. Something that I wanted to have for. discussion is on some parameters relevant for the dendrogram, like the. genes used, the correlation method and the linkage method. All this can be. modified but currently is hard coded as I didn't want to add 3 more. parameters to the plotting functions. Maybe you have faced a similar problem and have an elegant solution. I. thought about setting some variables like the rcParams for matplotlib but I. think is not justified for just 3 parameters and can be very confusing. Or. we can have a function to compute a dendrogram with all parameters. required, and save this in .uns like rank_genes_groups. Then if other. functions find this information they add the dendrogram. On Wed, Oct 17, 2018 at 4:30 PM Alex Wolf <notifications@github.com> wrote:. > Merged #308 <https://github.com/theislab/scanpy/pull/308> into master. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/308#event-1909725548>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1RD6Qm1iNFaKaG6elUL189hS5yFcks5ulz8SgaJpZM4Xjwsu>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/308
https://github.com/scverse/scanpy/pull/308:578,modifiability,paramet,parameters,578,"So quick! thanks. I have been using the dendrograms for a while so. hopefully not so many bugs appear. Something that I wanted to have for. discussion is on some parameters relevant for the dendrogram, like the. genes used, the correlation method and the linkage method. All this can be. modified but currently is hard coded as I didn't want to add 3 more. parameters to the plotting functions. Maybe you have faced a similar problem and have an elegant solution. I. thought about setting some variables like the rcParams for matplotlib but I. think is not justified for just 3 parameters and can be very confusing. Or. we can have a function to compute a dendrogram with all parameters. required, and save this in .uns like rank_genes_groups. Then if other. functions find this information they add the dendrogram. On Wed, Oct 17, 2018 at 4:30 PM Alex Wolf <notifications@github.com> wrote:. > Merged #308 <https://github.com/theislab/scanpy/pull/308> into master. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/308#event-1909725548>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1RD6Qm1iNFaKaG6elUL189hS5yFcks5ulz8SgaJpZM4Xjwsu>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/308
https://github.com/scverse/scanpy/pull/308:676,modifiability,paramet,parameters,676,"So quick! thanks. I have been using the dendrograms for a while so. hopefully not so many bugs appear. Something that I wanted to have for. discussion is on some parameters relevant for the dendrogram, like the. genes used, the correlation method and the linkage method. All this can be. modified but currently is hard coded as I didn't want to add 3 more. parameters to the plotting functions. Maybe you have faced a similar problem and have an elegant solution. I. thought about setting some variables like the rcParams for matplotlib but I. think is not justified for just 3 parameters and can be very confusing. Or. we can have a function to compute a dendrogram with all parameters. required, and save this in .uns like rank_genes_groups. Then if other. functions find this information they add the dendrogram. On Wed, Oct 17, 2018 at 4:30 PM Alex Wolf <notifications@github.com> wrote:. > Merged #308 <https://github.com/theislab/scanpy/pull/308> into master. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/308#event-1909725548>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1RD6Qm1iNFaKaG6elUL189hS5yFcks5ulz8SgaJpZM4Xjwsu>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/308
https://github.com/scverse/scanpy/pull/308:288,security,modif,modified,288,"So quick! thanks. I have been using the dendrograms for a while so. hopefully not so many bugs appear. Something that I wanted to have for. discussion is on some parameters relevant for the dendrogram, like the. genes used, the correlation method and the linkage method. All this can be. modified but currently is hard coded as I didn't want to add 3 more. parameters to the plotting functions. Maybe you have faced a similar problem and have an elegant solution. I. thought about setting some variables like the rcParams for matplotlib but I. think is not justified for just 3 parameters and can be very confusing. Or. we can have a function to compute a dendrogram with all parameters. required, and save this in .uns like rank_genes_groups. Then if other. functions find this information they add the dendrogram. On Wed, Oct 17, 2018 at 4:30 PM Alex Wolf <notifications@github.com> wrote:. > Merged #308 <https://github.com/theislab/scanpy/pull/308> into master. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/308#event-1909725548>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1RD6Qm1iNFaKaG6elUL189hS5yFcks5ulz8SgaJpZM4Xjwsu>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/308
https://github.com/scverse/scanpy/pull/308:1011,security,auth,authored,1011,"So quick! thanks. I have been using the dendrograms for a while so. hopefully not so many bugs appear. Something that I wanted to have for. discussion is on some parameters relevant for the dendrogram, like the. genes used, the correlation method and the linkage method. All this can be. modified but currently is hard coded as I didn't want to add 3 more. parameters to the plotting functions. Maybe you have faced a similar problem and have an elegant solution. I. thought about setting some variables like the rcParams for matplotlib but I. think is not justified for just 3 parameters and can be very confusing. Or. we can have a function to compute a dendrogram with all parameters. required, and save this in .uns like rank_genes_groups. Then if other. functions find this information they add the dendrogram. On Wed, Oct 17, 2018 at 4:30 PM Alex Wolf <notifications@github.com> wrote:. > Merged #308 <https://github.com/theislab/scanpy/pull/308> into master. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/308#event-1909725548>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1RD6Qm1iNFaKaG6elUL189hS5yFcks5ulz8SgaJpZM4Xjwsu>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/308
https://github.com/scverse/scanpy/pull/308:1220,security,auth,auth,1220,"So quick! thanks. I have been using the dendrograms for a while so. hopefully not so many bugs appear. Something that I wanted to have for. discussion is on some parameters relevant for the dendrogram, like the. genes used, the correlation method and the linkage method. All this can be. modified but currently is hard coded as I didn't want to add 3 more. parameters to the plotting functions. Maybe you have faced a similar problem and have an elegant solution. I. thought about setting some variables like the rcParams for matplotlib but I. think is not justified for just 3 parameters and can be very confusing. Or. we can have a function to compute a dendrogram with all parameters. required, and save this in .uns like rank_genes_groups. Then if other. functions find this information they add the dendrogram. On Wed, Oct 17, 2018 at 4:30 PM Alex Wolf <notifications@github.com> wrote:. > Merged #308 <https://github.com/theislab/scanpy/pull/308> into master. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/308#event-1909725548>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1RD6Qm1iNFaKaG6elUL189hS5yFcks5ulz8SgaJpZM4Xjwsu>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/308
https://github.com/scverse/scanpy/pull/308:89,availability,cluster,clustering,89,"The last option is the way forward, I'd say. We should have a tool `tl.dendogram` in the clustering section that has a parameter to select the clustering for which one wants a dendogram, typically defaulting to `louvain` (unfortunately, we still don't have a good consistent naming convention across all tools; `groupby` predominates but is not ideal in this setting. `grouping` or `cluster_key` would also be possible). You can still have the plotting functions call that function with default parameters. But the user wants more control, he or she can run the dendogram tool. Of course, we also want dendograms for genes. I think the most elegant (but maybe confusing solution) is to do it as in `pl.scatter`, where annotation of observations is selected if the key is in `.obs` and variables annotations are selected if the key is in `.var`. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/308
https://github.com/scverse/scanpy/pull/308:143,availability,cluster,clustering,143,"The last option is the way forward, I'd say. We should have a tool `tl.dendogram` in the clustering section that has a parameter to select the clustering for which one wants a dendogram, typically defaulting to `louvain` (unfortunately, we still don't have a good consistent naming convention across all tools; `groupby` predominates but is not ideal in this setting. `grouping` or `cluster_key` would also be possible). You can still have the plotting functions call that function with default parameters. But the user wants more control, he or she can run the dendogram tool. Of course, we also want dendograms for genes. I think the most elegant (but maybe confusing solution) is to do it as in `pl.scatter`, where annotation of observations is selected if the key is in `.obs` and variables annotations are selected if the key is in `.var`. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/308
https://github.com/scverse/scanpy/pull/308:264,availability,consist,consistent,264,"The last option is the way forward, I'd say. We should have a tool `tl.dendogram` in the clustering section that has a parameter to select the clustering for which one wants a dendogram, typically defaulting to `louvain` (unfortunately, we still don't have a good consistent naming convention across all tools; `groupby` predominates but is not ideal in this setting. `grouping` or `cluster_key` would also be possible). You can still have the plotting functions call that function with default parameters. But the user wants more control, he or she can run the dendogram tool. Of course, we also want dendograms for genes. I think the most elegant (but maybe confusing solution) is to do it as in `pl.scatter`, where annotation of observations is selected if the key is in `.obs` and variables annotations are selected if the key is in `.var`. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/308
https://github.com/scverse/scanpy/pull/308:89,deployability,cluster,clustering,89,"The last option is the way forward, I'd say. We should have a tool `tl.dendogram` in the clustering section that has a parameter to select the clustering for which one wants a dendogram, typically defaulting to `louvain` (unfortunately, we still don't have a good consistent naming convention across all tools; `groupby` predominates but is not ideal in this setting. `grouping` or `cluster_key` would also be possible). You can still have the plotting functions call that function with default parameters. But the user wants more control, he or she can run the dendogram tool. Of course, we also want dendograms for genes. I think the most elegant (but maybe confusing solution) is to do it as in `pl.scatter`, where annotation of observations is selected if the key is in `.obs` and variables annotations are selected if the key is in `.var`. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/308
https://github.com/scverse/scanpy/pull/308:143,deployability,cluster,clustering,143,"The last option is the way forward, I'd say. We should have a tool `tl.dendogram` in the clustering section that has a parameter to select the clustering for which one wants a dendogram, typically defaulting to `louvain` (unfortunately, we still don't have a good consistent naming convention across all tools; `groupby` predominates but is not ideal in this setting. `grouping` or `cluster_key` would also be possible). You can still have the plotting functions call that function with default parameters. But the user wants more control, he or she can run the dendogram tool. Of course, we also want dendograms for genes. I think the most elegant (but maybe confusing solution) is to do it as in `pl.scatter`, where annotation of observations is selected if the key is in `.obs` and variables annotations are selected if the key is in `.var`. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/308
https://github.com/scverse/scanpy/pull/308:732,deployability,observ,observations,732,"The last option is the way forward, I'd say. We should have a tool `tl.dendogram` in the clustering section that has a parameter to select the clustering for which one wants a dendogram, typically defaulting to `louvain` (unfortunately, we still don't have a good consistent naming convention across all tools; `groupby` predominates but is not ideal in this setting. `grouping` or `cluster_key` would also be possible). You can still have the plotting functions call that function with default parameters. But the user wants more control, he or she can run the dendogram tool. Of course, we also want dendograms for genes. I think the most elegant (but maybe confusing solution) is to do it as in `pl.scatter`, where annotation of observations is selected if the key is in `.obs` and variables annotations are selected if the key is in `.var`. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/308
https://github.com/scverse/scanpy/pull/308:119,modifiability,paramet,parameter,119,"The last option is the way forward, I'd say. We should have a tool `tl.dendogram` in the clustering section that has a parameter to select the clustering for which one wants a dendogram, typically defaulting to `louvain` (unfortunately, we still don't have a good consistent naming convention across all tools; `groupby` predominates but is not ideal in this setting. `grouping` or `cluster_key` would also be possible). You can still have the plotting functions call that function with default parameters. But the user wants more control, he or she can run the dendogram tool. Of course, we also want dendograms for genes. I think the most elegant (but maybe confusing solution) is to do it as in `pl.scatter`, where annotation of observations is selected if the key is in `.obs` and variables annotations are selected if the key is in `.var`. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/308
https://github.com/scverse/scanpy/pull/308:495,modifiability,paramet,parameters,495,"The last option is the way forward, I'd say. We should have a tool `tl.dendogram` in the clustering section that has a parameter to select the clustering for which one wants a dendogram, typically defaulting to `louvain` (unfortunately, we still don't have a good consistent naming convention across all tools; `groupby` predominates but is not ideal in this setting. `grouping` or `cluster_key` would also be possible). You can still have the plotting functions call that function with default parameters. But the user wants more control, he or she can run the dendogram tool. Of course, we also want dendograms for genes. I think the most elegant (but maybe confusing solution) is to do it as in `pl.scatter`, where annotation of observations is selected if the key is in `.obs` and variables annotations are selected if the key is in `.var`. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/308
https://github.com/scverse/scanpy/pull/308:785,modifiability,variab,variables,785,"The last option is the way forward, I'd say. We should have a tool `tl.dendogram` in the clustering section that has a parameter to select the clustering for which one wants a dendogram, typically defaulting to `louvain` (unfortunately, we still don't have a good consistent naming convention across all tools; `groupby` predominates but is not ideal in this setting. `grouping` or `cluster_key` would also be possible). You can still have the plotting functions call that function with default parameters. But the user wants more control, he or she can run the dendogram tool. Of course, we also want dendograms for genes. I think the most elegant (but maybe confusing solution) is to do it as in `pl.scatter`, where annotation of observations is selected if the key is in `.obs` and variables annotations are selected if the key is in `.var`. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/308
https://github.com/scverse/scanpy/pull/308:531,security,control,control,531,"The last option is the way forward, I'd say. We should have a tool `tl.dendogram` in the clustering section that has a parameter to select the clustering for which one wants a dendogram, typically defaulting to `louvain` (unfortunately, we still don't have a good consistent naming convention across all tools; `groupby` predominates but is not ideal in this setting. `grouping` or `cluster_key` would also be possible). You can still have the plotting functions call that function with default parameters. But the user wants more control, he or she can run the dendogram tool. Of course, we also want dendograms for genes. I think the most elegant (but maybe confusing solution) is to do it as in `pl.scatter`, where annotation of observations is selected if the key is in `.obs` and variables annotations are selected if the key is in `.var`. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/308
https://github.com/scverse/scanpy/pull/308:531,testability,control,control,531,"The last option is the way forward, I'd say. We should have a tool `tl.dendogram` in the clustering section that has a parameter to select the clustering for which one wants a dendogram, typically defaulting to `louvain` (unfortunately, we still don't have a good consistent naming convention across all tools; `groupby` predominates but is not ideal in this setting. `grouping` or `cluster_key` would also be possible). You can still have the plotting functions call that function with default parameters. But the user wants more control, he or she can run the dendogram tool. Of course, we also want dendograms for genes. I think the most elegant (but maybe confusing solution) is to do it as in `pl.scatter`, where annotation of observations is selected if the key is in `.obs` and variables annotations are selected if the key is in `.var`. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/308
https://github.com/scverse/scanpy/pull/308:732,testability,observ,observations,732,"The last option is the way forward, I'd say. We should have a tool `tl.dendogram` in the clustering section that has a parameter to select the clustering for which one wants a dendogram, typically defaulting to `louvain` (unfortunately, we still don't have a good consistent naming convention across all tools; `groupby` predominates but is not ideal in this setting. `grouping` or `cluster_key` would also be possible). You can still have the plotting functions call that function with default parameters. But the user wants more control, he or she can run the dendogram tool. Of course, we also want dendograms for genes. I think the most elegant (but maybe confusing solution) is to do it as in `pl.scatter`, where annotation of observations is selected if the key is in `.obs` and variables annotations are selected if the key is in `.var`. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/308
https://github.com/scverse/scanpy/pull/308:62,usability,tool,tool,62,"The last option is the way forward, I'd say. We should have a tool `tl.dendogram` in the clustering section that has a parameter to select the clustering for which one wants a dendogram, typically defaulting to `louvain` (unfortunately, we still don't have a good consistent naming convention across all tools; `groupby` predominates but is not ideal in this setting. `grouping` or `cluster_key` would also be possible). You can still have the plotting functions call that function with default parameters. But the user wants more control, he or she can run the dendogram tool. Of course, we also want dendograms for genes. I think the most elegant (but maybe confusing solution) is to do it as in `pl.scatter`, where annotation of observations is selected if the key is in `.obs` and variables annotations are selected if the key is in `.var`. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/308
https://github.com/scverse/scanpy/pull/308:264,usability,consist,consistent,264,"The last option is the way forward, I'd say. We should have a tool `tl.dendogram` in the clustering section that has a parameter to select the clustering for which one wants a dendogram, typically defaulting to `louvain` (unfortunately, we still don't have a good consistent naming convention across all tools; `groupby` predominates but is not ideal in this setting. `grouping` or `cluster_key` would also be possible). You can still have the plotting functions call that function with default parameters. But the user wants more control, he or she can run the dendogram tool. Of course, we also want dendograms for genes. I think the most elegant (but maybe confusing solution) is to do it as in `pl.scatter`, where annotation of observations is selected if the key is in `.obs` and variables annotations are selected if the key is in `.var`. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/308
https://github.com/scverse/scanpy/pull/308:304,usability,tool,tools,304,"The last option is the way forward, I'd say. We should have a tool `tl.dendogram` in the clustering section that has a parameter to select the clustering for which one wants a dendogram, typically defaulting to `louvain` (unfortunately, we still don't have a good consistent naming convention across all tools; `groupby` predominates but is not ideal in this setting. `grouping` or `cluster_key` would also be possible). You can still have the plotting functions call that function with default parameters. But the user wants more control, he or she can run the dendogram tool. Of course, we also want dendograms for genes. I think the most elegant (but maybe confusing solution) is to do it as in `pl.scatter`, where annotation of observations is selected if the key is in `.obs` and variables annotations are selected if the key is in `.var`. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/308
https://github.com/scverse/scanpy/pull/308:515,usability,user,user,515,"The last option is the way forward, I'd say. We should have a tool `tl.dendogram` in the clustering section that has a parameter to select the clustering for which one wants a dendogram, typically defaulting to `louvain` (unfortunately, we still don't have a good consistent naming convention across all tools; `groupby` predominates but is not ideal in this setting. `grouping` or `cluster_key` would also be possible). You can still have the plotting functions call that function with default parameters. But the user wants more control, he or she can run the dendogram tool. Of course, we also want dendograms for genes. I think the most elegant (but maybe confusing solution) is to do it as in `pl.scatter`, where annotation of observations is selected if the key is in `.obs` and variables annotations are selected if the key is in `.var`. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/308
https://github.com/scverse/scanpy/pull/308:572,usability,tool,tool,572,"The last option is the way forward, I'd say. We should have a tool `tl.dendogram` in the clustering section that has a parameter to select the clustering for which one wants a dendogram, typically defaulting to `louvain` (unfortunately, we still don't have a good consistent naming convention across all tools; `groupby` predominates but is not ideal in this setting. `grouping` or `cluster_key` would also be possible). You can still have the plotting functions call that function with default parameters. But the user wants more control, he or she can run the dendogram tool. Of course, we also want dendograms for genes. I think the most elegant (but maybe confusing solution) is to do it as in `pl.scatter`, where annotation of observations is selected if the key is in `.obs` and variables annotations are selected if the key is in `.var`. What do you think?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/308
https://github.com/scverse/scanpy/pull/308:233,availability,cluster,clustering,233,"Sounds good. If I find some time soon I will try that approach. On Wed, Oct 17, 2018 at 5:25 PM Alex Wolf <notifications@github.com> wrote:. > The last option is the way forward, I'd say. We should have a tool. > tl.dendogram in the clustering section that has a parameter to select the. > clustering for which one wants a dendogram, typically defaulting to. > louvain (unfortunately, we still don't have a good consistent naming. > convention across all tools; groupby predominates but is not ideal in. > this setting. grouping or cluster_key would also be possible). >. > You can still have the plotting functions call that function with default. > parameters. But the user wants more control, he or she can run the. > dendogram tool. >. > Of course, we also want dendograms for genes. I think the most elegant. > (but maybe confusing solution) is to do it as in pl.scatter, where. > annotation of observations is selected if the key is in .obs and. > variables annotations are selected if the key is in .var. What do you. > think? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/308#issuecomment-430674069>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1TgBY1iDfvfhL2ravwfKRfL-A6wxks5ul0wAgaJpZM4Xjwsu>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/308
https://github.com/scverse/scanpy/pull/308:290,availability,cluster,clustering,290,"Sounds good. If I find some time soon I will try that approach. On Wed, Oct 17, 2018 at 5:25 PM Alex Wolf <notifications@github.com> wrote:. > The last option is the way forward, I'd say. We should have a tool. > tl.dendogram in the clustering section that has a parameter to select the. > clustering for which one wants a dendogram, typically defaulting to. > louvain (unfortunately, we still don't have a good consistent naming. > convention across all tools; groupby predominates but is not ideal in. > this setting. grouping or cluster_key would also be possible). >. > You can still have the plotting functions call that function with default. > parameters. But the user wants more control, he or she can run the. > dendogram tool. >. > Of course, we also want dendograms for genes. I think the most elegant. > (but maybe confusing solution) is to do it as in pl.scatter, where. > annotation of observations is selected if the key is in .obs and. > variables annotations are selected if the key is in .var. What do you. > think? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/308#issuecomment-430674069>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1TgBY1iDfvfhL2ravwfKRfL-A6wxks5ul0wAgaJpZM4Xjwsu>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/308
https://github.com/scverse/scanpy/pull/308:412,availability,consist,consistent,412,"Sounds good. If I find some time soon I will try that approach. On Wed, Oct 17, 2018 at 5:25 PM Alex Wolf <notifications@github.com> wrote:. > The last option is the way forward, I'd say. We should have a tool. > tl.dendogram in the clustering section that has a parameter to select the. > clustering for which one wants a dendogram, typically defaulting to. > louvain (unfortunately, we still don't have a good consistent naming. > convention across all tools; groupby predominates but is not ideal in. > this setting. grouping or cluster_key would also be possible). >. > You can still have the plotting functions call that function with default. > parameters. But the user wants more control, he or she can run the. > dendogram tool. >. > Of course, we also want dendograms for genes. I think the most elegant. > (but maybe confusing solution) is to do it as in pl.scatter, where. > annotation of observations is selected if the key is in .obs and. > variables annotations are selected if the key is in .var. What do you. > think? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/308#issuecomment-430674069>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1TgBY1iDfvfhL2ravwfKRfL-A6wxks5ul0wAgaJpZM4Xjwsu>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/308
https://github.com/scverse/scanpy/pull/308:233,deployability,cluster,clustering,233,"Sounds good. If I find some time soon I will try that approach. On Wed, Oct 17, 2018 at 5:25 PM Alex Wolf <notifications@github.com> wrote:. > The last option is the way forward, I'd say. We should have a tool. > tl.dendogram in the clustering section that has a parameter to select the. > clustering for which one wants a dendogram, typically defaulting to. > louvain (unfortunately, we still don't have a good consistent naming. > convention across all tools; groupby predominates but is not ideal in. > this setting. grouping or cluster_key would also be possible). >. > You can still have the plotting functions call that function with default. > parameters. But the user wants more control, he or she can run the. > dendogram tool. >. > Of course, we also want dendograms for genes. I think the most elegant. > (but maybe confusing solution) is to do it as in pl.scatter, where. > annotation of observations is selected if the key is in .obs and. > variables annotations are selected if the key is in .var. What do you. > think? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/308#issuecomment-430674069>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1TgBY1iDfvfhL2ravwfKRfL-A6wxks5ul0wAgaJpZM4Xjwsu>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/308
https://github.com/scverse/scanpy/pull/308:290,deployability,cluster,clustering,290,"Sounds good. If I find some time soon I will try that approach. On Wed, Oct 17, 2018 at 5:25 PM Alex Wolf <notifications@github.com> wrote:. > The last option is the way forward, I'd say. We should have a tool. > tl.dendogram in the clustering section that has a parameter to select the. > clustering for which one wants a dendogram, typically defaulting to. > louvain (unfortunately, we still don't have a good consistent naming. > convention across all tools; groupby predominates but is not ideal in. > this setting. grouping or cluster_key would also be possible). >. > You can still have the plotting functions call that function with default. > parameters. But the user wants more control, he or she can run the. > dendogram tool. >. > Of course, we also want dendograms for genes. I think the most elegant. > (but maybe confusing solution) is to do it as in pl.scatter, where. > annotation of observations is selected if the key is in .obs and. > variables annotations are selected if the key is in .var. What do you. > think? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/308#issuecomment-430674069>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1TgBY1iDfvfhL2ravwfKRfL-A6wxks5ul0wAgaJpZM4Xjwsu>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/308
https://github.com/scverse/scanpy/pull/308:900,deployability,observ,observations,900,"Sounds good. If I find some time soon I will try that approach. On Wed, Oct 17, 2018 at 5:25 PM Alex Wolf <notifications@github.com> wrote:. > The last option is the way forward, I'd say. We should have a tool. > tl.dendogram in the clustering section that has a parameter to select the. > clustering for which one wants a dendogram, typically defaulting to. > louvain (unfortunately, we still don't have a good consistent naming. > convention across all tools; groupby predominates but is not ideal in. > this setting. grouping or cluster_key would also be possible). >. > You can still have the plotting functions call that function with default. > parameters. But the user wants more control, he or she can run the. > dendogram tool. >. > Of course, we also want dendograms for genes. I think the most elegant. > (but maybe confusing solution) is to do it as in pl.scatter, where. > annotation of observations is selected if the key is in .obs and. > variables annotations are selected if the key is in .var. What do you. > think? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/308#issuecomment-430674069>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1TgBY1iDfvfhL2ravwfKRfL-A6wxks5ul0wAgaJpZM4Xjwsu>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/308
https://github.com/scverse/scanpy/pull/308:263,modifiability,paramet,parameter,263,"Sounds good. If I find some time soon I will try that approach. On Wed, Oct 17, 2018 at 5:25 PM Alex Wolf <notifications@github.com> wrote:. > The last option is the way forward, I'd say. We should have a tool. > tl.dendogram in the clustering section that has a parameter to select the. > clustering for which one wants a dendogram, typically defaulting to. > louvain (unfortunately, we still don't have a good consistent naming. > convention across all tools; groupby predominates but is not ideal in. > this setting. grouping or cluster_key would also be possible). >. > You can still have the plotting functions call that function with default. > parameters. But the user wants more control, he or she can run the. > dendogram tool. >. > Of course, we also want dendograms for genes. I think the most elegant. > (but maybe confusing solution) is to do it as in pl.scatter, where. > annotation of observations is selected if the key is in .obs and. > variables annotations are selected if the key is in .var. What do you. > think? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/308#issuecomment-430674069>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1TgBY1iDfvfhL2ravwfKRfL-A6wxks5ul0wAgaJpZM4Xjwsu>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/308
https://github.com/scverse/scanpy/pull/308:651,modifiability,paramet,parameters,651,"Sounds good. If I find some time soon I will try that approach. On Wed, Oct 17, 2018 at 5:25 PM Alex Wolf <notifications@github.com> wrote:. > The last option is the way forward, I'd say. We should have a tool. > tl.dendogram in the clustering section that has a parameter to select the. > clustering for which one wants a dendogram, typically defaulting to. > louvain (unfortunately, we still don't have a good consistent naming. > convention across all tools; groupby predominates but is not ideal in. > this setting. grouping or cluster_key would also be possible). >. > You can still have the plotting functions call that function with default. > parameters. But the user wants more control, he or she can run the. > dendogram tool. >. > Of course, we also want dendograms for genes. I think the most elegant. > (but maybe confusing solution) is to do it as in pl.scatter, where. > annotation of observations is selected if the key is in .obs and. > variables annotations are selected if the key is in .var. What do you. > think? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/308#issuecomment-430674069>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1TgBY1iDfvfhL2ravwfKRfL-A6wxks5ul0wAgaJpZM4Xjwsu>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/308
https://github.com/scverse/scanpy/pull/308:954,modifiability,variab,variables,954,"Sounds good. If I find some time soon I will try that approach. On Wed, Oct 17, 2018 at 5:25 PM Alex Wolf <notifications@github.com> wrote:. > The last option is the way forward, I'd say. We should have a tool. > tl.dendogram in the clustering section that has a parameter to select the. > clustering for which one wants a dendogram, typically defaulting to. > louvain (unfortunately, we still don't have a good consistent naming. > convention across all tools; groupby predominates but is not ideal in. > this setting. grouping or cluster_key would also be possible). >. > You can still have the plotting functions call that function with default. > parameters. But the user wants more control, he or she can run the. > dendogram tool. >. > Of course, we also want dendograms for genes. I think the most elegant. > (but maybe confusing solution) is to do it as in pl.scatter, where. > annotation of observations is selected if the key is in .obs and. > variables annotations are selected if the key is in .var. What do you. > think? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/308#issuecomment-430674069>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1TgBY1iDfvfhL2ravwfKRfL-A6wxks5ul0wAgaJpZM4Xjwsu>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/308
https://github.com/scverse/scanpy/pull/308:28,performance,time,time,28,"Sounds good. If I find some time soon I will try that approach. On Wed, Oct 17, 2018 at 5:25 PM Alex Wolf <notifications@github.com> wrote:. > The last option is the way forward, I'd say. We should have a tool. > tl.dendogram in the clustering section that has a parameter to select the. > clustering for which one wants a dendogram, typically defaulting to. > louvain (unfortunately, we still don't have a good consistent naming. > convention across all tools; groupby predominates but is not ideal in. > this setting. grouping or cluster_key would also be possible). >. > You can still have the plotting functions call that function with default. > parameters. But the user wants more control, he or she can run the. > dendogram tool. >. > Of course, we also want dendograms for genes. I think the most elegant. > (but maybe confusing solution) is to do it as in pl.scatter, where. > annotation of observations is selected if the key is in .obs and. > variables annotations are selected if the key is in .var. What do you. > think? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/308#issuecomment-430674069>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1TgBY1iDfvfhL2ravwfKRfL-A6wxks5ul0wAgaJpZM4Xjwsu>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/308
https://github.com/scverse/scanpy/pull/308:687,security,control,control,687,"Sounds good. If I find some time soon I will try that approach. On Wed, Oct 17, 2018 at 5:25 PM Alex Wolf <notifications@github.com> wrote:. > The last option is the way forward, I'd say. We should have a tool. > tl.dendogram in the clustering section that has a parameter to select the. > clustering for which one wants a dendogram, typically defaulting to. > louvain (unfortunately, we still don't have a good consistent naming. > convention across all tools; groupby predominates but is not ideal in. > this setting. grouping or cluster_key would also be possible). >. > You can still have the plotting functions call that function with default. > parameters. But the user wants more control, he or she can run the. > dendogram tool. >. > Of course, we also want dendograms for genes. I think the most elegant. > (but maybe confusing solution) is to do it as in pl.scatter, where. > annotation of observations is selected if the key is in .obs and. > variables annotations are selected if the key is in .var. What do you. > think? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/308#issuecomment-430674069>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1TgBY1iDfvfhL2ravwfKRfL-A6wxks5ul0wAgaJpZM4Xjwsu>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/308
https://github.com/scverse/scanpy/pull/308:1079,security,auth,authored,1079,"Sounds good. If I find some time soon I will try that approach. On Wed, Oct 17, 2018 at 5:25 PM Alex Wolf <notifications@github.com> wrote:. > The last option is the way forward, I'd say. We should have a tool. > tl.dendogram in the clustering section that has a parameter to select the. > clustering for which one wants a dendogram, typically defaulting to. > louvain (unfortunately, we still don't have a good consistent naming. > convention across all tools; groupby predominates but is not ideal in. > this setting. grouping or cluster_key would also be possible). >. > You can still have the plotting functions call that function with default. > parameters. But the user wants more control, he or she can run the. > dendogram tool. >. > Of course, we also want dendograms for genes. I think the most elegant. > (but maybe confusing solution) is to do it as in pl.scatter, where. > annotation of observations is selected if the key is in .obs and. > variables annotations are selected if the key is in .var. What do you. > think? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/308#issuecomment-430674069>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1TgBY1iDfvfhL2ravwfKRfL-A6wxks5ul0wAgaJpZM4Xjwsu>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/308
https://github.com/scverse/scanpy/pull/308:1294,security,auth,auth,1294,"Sounds good. If I find some time soon I will try that approach. On Wed, Oct 17, 2018 at 5:25 PM Alex Wolf <notifications@github.com> wrote:. > The last option is the way forward, I'd say. We should have a tool. > tl.dendogram in the clustering section that has a parameter to select the. > clustering for which one wants a dendogram, typically defaulting to. > louvain (unfortunately, we still don't have a good consistent naming. > convention across all tools; groupby predominates but is not ideal in. > this setting. grouping or cluster_key would also be possible). >. > You can still have the plotting functions call that function with default. > parameters. But the user wants more control, he or she can run the. > dendogram tool. >. > Of course, we also want dendograms for genes. I think the most elegant. > (but maybe confusing solution) is to do it as in pl.scatter, where. > annotation of observations is selected if the key is in .obs and. > variables annotations are selected if the key is in .var. What do you. > think? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/308#issuecomment-430674069>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1TgBY1iDfvfhL2ravwfKRfL-A6wxks5ul0wAgaJpZM4Xjwsu>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/308
https://github.com/scverse/scanpy/pull/308:687,testability,control,control,687,"Sounds good. If I find some time soon I will try that approach. On Wed, Oct 17, 2018 at 5:25 PM Alex Wolf <notifications@github.com> wrote:. > The last option is the way forward, I'd say. We should have a tool. > tl.dendogram in the clustering section that has a parameter to select the. > clustering for which one wants a dendogram, typically defaulting to. > louvain (unfortunately, we still don't have a good consistent naming. > convention across all tools; groupby predominates but is not ideal in. > this setting. grouping or cluster_key would also be possible). >. > You can still have the plotting functions call that function with default. > parameters. But the user wants more control, he or she can run the. > dendogram tool. >. > Of course, we also want dendograms for genes. I think the most elegant. > (but maybe confusing solution) is to do it as in pl.scatter, where. > annotation of observations is selected if the key is in .obs and. > variables annotations are selected if the key is in .var. What do you. > think? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/308#issuecomment-430674069>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1TgBY1iDfvfhL2ravwfKRfL-A6wxks5ul0wAgaJpZM4Xjwsu>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/308
https://github.com/scverse/scanpy/pull/308:900,testability,observ,observations,900,"Sounds good. If I find some time soon I will try that approach. On Wed, Oct 17, 2018 at 5:25 PM Alex Wolf <notifications@github.com> wrote:. > The last option is the way forward, I'd say. We should have a tool. > tl.dendogram in the clustering section that has a parameter to select the. > clustering for which one wants a dendogram, typically defaulting to. > louvain (unfortunately, we still don't have a good consistent naming. > convention across all tools; groupby predominates but is not ideal in. > this setting. grouping or cluster_key would also be possible). >. > You can still have the plotting functions call that function with default. > parameters. But the user wants more control, he or she can run the. > dendogram tool. >. > Of course, we also want dendograms for genes. I think the most elegant. > (but maybe confusing solution) is to do it as in pl.scatter, where. > annotation of observations is selected if the key is in .obs and. > variables annotations are selected if the key is in .var. What do you. > think? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/308#issuecomment-430674069>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1TgBY1iDfvfhL2ravwfKRfL-A6wxks5ul0wAgaJpZM4Xjwsu>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/308
https://github.com/scverse/scanpy/pull/308:205,usability,tool,tool,205,"Sounds good. If I find some time soon I will try that approach. On Wed, Oct 17, 2018 at 5:25 PM Alex Wolf <notifications@github.com> wrote:. > The last option is the way forward, I'd say. We should have a tool. > tl.dendogram in the clustering section that has a parameter to select the. > clustering for which one wants a dendogram, typically defaulting to. > louvain (unfortunately, we still don't have a good consistent naming. > convention across all tools; groupby predominates but is not ideal in. > this setting. grouping or cluster_key would also be possible). >. > You can still have the plotting functions call that function with default. > parameters. But the user wants more control, he or she can run the. > dendogram tool. >. > Of course, we also want dendograms for genes. I think the most elegant. > (but maybe confusing solution) is to do it as in pl.scatter, where. > annotation of observations is selected if the key is in .obs and. > variables annotations are selected if the key is in .var. What do you. > think? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/308#issuecomment-430674069>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1TgBY1iDfvfhL2ravwfKRfL-A6wxks5ul0wAgaJpZM4Xjwsu>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/308
https://github.com/scverse/scanpy/pull/308:412,usability,consist,consistent,412,"Sounds good. If I find some time soon I will try that approach. On Wed, Oct 17, 2018 at 5:25 PM Alex Wolf <notifications@github.com> wrote:. > The last option is the way forward, I'd say. We should have a tool. > tl.dendogram in the clustering section that has a parameter to select the. > clustering for which one wants a dendogram, typically defaulting to. > louvain (unfortunately, we still don't have a good consistent naming. > convention across all tools; groupby predominates but is not ideal in. > this setting. grouping or cluster_key would also be possible). >. > You can still have the plotting functions call that function with default. > parameters. But the user wants more control, he or she can run the. > dendogram tool. >. > Of course, we also want dendograms for genes. I think the most elegant. > (but maybe confusing solution) is to do it as in pl.scatter, where. > annotation of observations is selected if the key is in .obs and. > variables annotations are selected if the key is in .var. What do you. > think? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/308#issuecomment-430674069>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1TgBY1iDfvfhL2ravwfKRfL-A6wxks5ul0wAgaJpZM4Xjwsu>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/308
https://github.com/scverse/scanpy/pull/308:455,usability,tool,tools,455,"Sounds good. If I find some time soon I will try that approach. On Wed, Oct 17, 2018 at 5:25 PM Alex Wolf <notifications@github.com> wrote:. > The last option is the way forward, I'd say. We should have a tool. > tl.dendogram in the clustering section that has a parameter to select the. > clustering for which one wants a dendogram, typically defaulting to. > louvain (unfortunately, we still don't have a good consistent naming. > convention across all tools; groupby predominates but is not ideal in. > this setting. grouping or cluster_key would also be possible). >. > You can still have the plotting functions call that function with default. > parameters. But the user wants more control, he or she can run the. > dendogram tool. >. > Of course, we also want dendograms for genes. I think the most elegant. > (but maybe confusing solution) is to do it as in pl.scatter, where. > annotation of observations is selected if the key is in .obs and. > variables annotations are selected if the key is in .var. What do you. > think? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/308#issuecomment-430674069>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1TgBY1iDfvfhL2ravwfKRfL-A6wxks5ul0wAgaJpZM4Xjwsu>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/308
https://github.com/scverse/scanpy/pull/308:671,usability,user,user,671,"Sounds good. If I find some time soon I will try that approach. On Wed, Oct 17, 2018 at 5:25 PM Alex Wolf <notifications@github.com> wrote:. > The last option is the way forward, I'd say. We should have a tool. > tl.dendogram in the clustering section that has a parameter to select the. > clustering for which one wants a dendogram, typically defaulting to. > louvain (unfortunately, we still don't have a good consistent naming. > convention across all tools; groupby predominates but is not ideal in. > this setting. grouping or cluster_key would also be possible). >. > You can still have the plotting functions call that function with default. > parameters. But the user wants more control, he or she can run the. > dendogram tool. >. > Of course, we also want dendograms for genes. I think the most elegant. > (but maybe confusing solution) is to do it as in pl.scatter, where. > annotation of observations is selected if the key is in .obs and. > variables annotations are selected if the key is in .var. What do you. > think? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/308#issuecomment-430674069>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1TgBY1iDfvfhL2ravwfKRfL-A6wxks5ul0wAgaJpZM4Xjwsu>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/308
https://github.com/scverse/scanpy/pull/308:731,usability,tool,tool,731,"Sounds good. If I find some time soon I will try that approach. On Wed, Oct 17, 2018 at 5:25 PM Alex Wolf <notifications@github.com> wrote:. > The last option is the way forward, I'd say. We should have a tool. > tl.dendogram in the clustering section that has a parameter to select the. > clustering for which one wants a dendogram, typically defaulting to. > louvain (unfortunately, we still don't have a good consistent naming. > convention across all tools; groupby predominates but is not ideal in. > this setting. grouping or cluster_key would also be possible). >. > You can still have the plotting functions call that function with default. > parameters. But the user wants more control, he or she can run the. > dendogram tool. >. > Of course, we also want dendograms for genes. I think the most elegant. > (but maybe confusing solution) is to do it as in pl.scatter, where. > annotation of observations is selected if the key is in .obs and. > variables annotations are selected if the key is in .var. What do you. > think? >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/pull/308#issuecomment-430674069>, or mute. > the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1TgBY1iDfvfhL2ravwfKRfL-A6wxks5ul0wAgaJpZM4Xjwsu>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/308
https://github.com/scverse/scanpy/issues/309:348,deployability,api,api,348,"thanks for the report! if you type plt.tight_layout() do you get better. images? On Thu, Oct 18, 2018 at 8:50 AM Isaac Virshup <notifications@github.com>. wrote:. > As mentioned in #299 <https://github.com/theislab/scanpy/issues/299>, I'm. > getting some strange layout behavior when plotting at the repl. >. > Using the setup:. >. > import scanpy.api as scimport numpy as np. >. > N = 1000. > M = 2000. >. > adata = sc.AnnData(X=np.random.random_sample((N, M))). >. > sc.pp.pca(adata). >. > I get outputs that look like the following. >. > sc.pl.pca(adata, color=list(""123"")). >. > [image: screen shot 2018-10-18 at 4 46 46 pm]. > <https://user-images.githubusercontent.com/8238804/47133668-72d23780-d2f5-11e8-9630-f6e676a146ca.png>. >. > sc.pl.pca(adata, color=list(""1234"")). >. > [image: screen shot 2018-10-18 at 4 47 37 pm]. > <https://user-images.githubusercontent.com/8238804/47133696-909f9c80-d2f5-11e8-8482-eab30e84fe46.png>. >. > sc.pl.pca(adata, color=list(""12""), ncols=1). >. > [image: screen shot 2018-10-18 at 4 50 11 pm]. > <https://user-images.githubusercontent.com/8238804/47133806-17547980-d2f6-11e8-97ad-6d9528ea280b.png>. >. > I get similar results with the MacOSX and GR backends (haven't checked. > others). >. > I think the issue is due to the implicit bbox_inches=""tight"" occurring. > with the matplotlib inline backend, and its explicit usage for. > scanpy.plotting.utils.savefig. In the cases here, the bounding box is not. > being trimmed. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/309>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1RC33MewvC1wM45k3iek6QjaeHcyks5umCSYgaJpZM4Xsxiv>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/309
https://github.com/scverse/scanpy/issues/309:348,integrability,api,api,348,"thanks for the report! if you type plt.tight_layout() do you get better. images? On Thu, Oct 18, 2018 at 8:50 AM Isaac Virshup <notifications@github.com>. wrote:. > As mentioned in #299 <https://github.com/theislab/scanpy/issues/299>, I'm. > getting some strange layout behavior when plotting at the repl. >. > Using the setup:. >. > import scanpy.api as scimport numpy as np. >. > N = 1000. > M = 2000. >. > adata = sc.AnnData(X=np.random.random_sample((N, M))). >. > sc.pp.pca(adata). >. > I get outputs that look like the following. >. > sc.pl.pca(adata, color=list(""123"")). >. > [image: screen shot 2018-10-18 at 4 46 46 pm]. > <https://user-images.githubusercontent.com/8238804/47133668-72d23780-d2f5-11e8-9630-f6e676a146ca.png>. >. > sc.pl.pca(adata, color=list(""1234"")). >. > [image: screen shot 2018-10-18 at 4 47 37 pm]. > <https://user-images.githubusercontent.com/8238804/47133696-909f9c80-d2f5-11e8-8482-eab30e84fe46.png>. >. > sc.pl.pca(adata, color=list(""12""), ncols=1). >. > [image: screen shot 2018-10-18 at 4 50 11 pm]. > <https://user-images.githubusercontent.com/8238804/47133806-17547980-d2f6-11e8-97ad-6d9528ea280b.png>. >. > I get similar results with the MacOSX and GR backends (haven't checked. > others). >. > I think the issue is due to the implicit bbox_inches=""tight"" occurring. > with the matplotlib inline backend, and its explicit usage for. > scanpy.plotting.utils.savefig. In the cases here, the bounding box is not. > being trimmed. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/309>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1RC33MewvC1wM45k3iek6QjaeHcyks5umCSYgaJpZM4Xsxiv>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/309
https://github.com/scverse/scanpy/issues/309:1516,integrability,sub,subscribed,1516,"thanks for the report! if you type plt.tight_layout() do you get better. images? On Thu, Oct 18, 2018 at 8:50 AM Isaac Virshup <notifications@github.com>. wrote:. > As mentioned in #299 <https://github.com/theislab/scanpy/issues/299>, I'm. > getting some strange layout behavior when plotting at the repl. >. > Using the setup:. >. > import scanpy.api as scimport numpy as np. >. > N = 1000. > M = 2000. >. > adata = sc.AnnData(X=np.random.random_sample((N, M))). >. > sc.pp.pca(adata). >. > I get outputs that look like the following. >. > sc.pl.pca(adata, color=list(""123"")). >. > [image: screen shot 2018-10-18 at 4 46 46 pm]. > <https://user-images.githubusercontent.com/8238804/47133668-72d23780-d2f5-11e8-9630-f6e676a146ca.png>. >. > sc.pl.pca(adata, color=list(""1234"")). >. > [image: screen shot 2018-10-18 at 4 47 37 pm]. > <https://user-images.githubusercontent.com/8238804/47133696-909f9c80-d2f5-11e8-8482-eab30e84fe46.png>. >. > sc.pl.pca(adata, color=list(""12""), ncols=1). >. > [image: screen shot 2018-10-18 at 4 50 11 pm]. > <https://user-images.githubusercontent.com/8238804/47133806-17547980-d2f6-11e8-97ad-6d9528ea280b.png>. >. > I get similar results with the MacOSX and GR backends (haven't checked. > others). >. > I think the issue is due to the implicit bbox_inches=""tight"" occurring. > with the matplotlib inline backend, and its explicit usage for. > scanpy.plotting.utils.savefig. In the cases here, the bounding box is not. > being trimmed. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/309>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1RC33MewvC1wM45k3iek6QjaeHcyks5umCSYgaJpZM4Xsxiv>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/309
https://github.com/scverse/scanpy/issues/309:348,interoperability,api,api,348,"thanks for the report! if you type plt.tight_layout() do you get better. images? On Thu, Oct 18, 2018 at 8:50 AM Isaac Virshup <notifications@github.com>. wrote:. > As mentioned in #299 <https://github.com/theislab/scanpy/issues/299>, I'm. > getting some strange layout behavior when plotting at the repl. >. > Using the setup:. >. > import scanpy.api as scimport numpy as np. >. > N = 1000. > M = 2000. >. > adata = sc.AnnData(X=np.random.random_sample((N, M))). >. > sc.pp.pca(adata). >. > I get outputs that look like the following. >. > sc.pl.pca(adata, color=list(""123"")). >. > [image: screen shot 2018-10-18 at 4 46 46 pm]. > <https://user-images.githubusercontent.com/8238804/47133668-72d23780-d2f5-11e8-9630-f6e676a146ca.png>. >. > sc.pl.pca(adata, color=list(""1234"")). >. > [image: screen shot 2018-10-18 at 4 47 37 pm]. > <https://user-images.githubusercontent.com/8238804/47133696-909f9c80-d2f5-11e8-8482-eab30e84fe46.png>. >. > sc.pl.pca(adata, color=list(""12""), ncols=1). >. > [image: screen shot 2018-10-18 at 4 50 11 pm]. > <https://user-images.githubusercontent.com/8238804/47133806-17547980-d2f6-11e8-97ad-6d9528ea280b.png>. >. > I get similar results with the MacOSX and GR backends (haven't checked. > others). >. > I think the issue is due to the implicit bbox_inches=""tight"" occurring. > with the matplotlib inline backend, and its explicit usage for. > scanpy.plotting.utils.savefig. In the cases here, the bounding box is not. > being trimmed. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/309>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1RC33MewvC1wM45k3iek6QjaeHcyks5umCSYgaJpZM4Xsxiv>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/309
https://github.com/scverse/scanpy/issues/309:1713,security,auth,auth,1713,"thanks for the report! if you type plt.tight_layout() do you get better. images? On Thu, Oct 18, 2018 at 8:50 AM Isaac Virshup <notifications@github.com>. wrote:. > As mentioned in #299 <https://github.com/theislab/scanpy/issues/299>, I'm. > getting some strange layout behavior when plotting at the repl. >. > Using the setup:. >. > import scanpy.api as scimport numpy as np. >. > N = 1000. > M = 2000. >. > adata = sc.AnnData(X=np.random.random_sample((N, M))). >. > sc.pp.pca(adata). >. > I get outputs that look like the following. >. > sc.pl.pca(adata, color=list(""123"")). >. > [image: screen shot 2018-10-18 at 4 46 46 pm]. > <https://user-images.githubusercontent.com/8238804/47133668-72d23780-d2f5-11e8-9630-f6e676a146ca.png>. >. > sc.pl.pca(adata, color=list(""1234"")). >. > [image: screen shot 2018-10-18 at 4 47 37 pm]. > <https://user-images.githubusercontent.com/8238804/47133696-909f9c80-d2f5-11e8-8482-eab30e84fe46.png>. >. > sc.pl.pca(adata, color=list(""12""), ncols=1). >. > [image: screen shot 2018-10-18 at 4 50 11 pm]. > <https://user-images.githubusercontent.com/8238804/47133806-17547980-d2f6-11e8-97ad-6d9528ea280b.png>. >. > I get similar results with the MacOSX and GR backends (haven't checked. > others). >. > I think the issue is due to the implicit bbox_inches=""tight"" occurring. > with the matplotlib inline backend, and its explicit usage for. > scanpy.plotting.utils.savefig. In the cases here, the bounding box is not. > being trimmed. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/309>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1RC33MewvC1wM45k3iek6QjaeHcyks5umCSYgaJpZM4Xsxiv>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/309
https://github.com/scverse/scanpy/issues/309:270,usability,behavi,behavior,270,"thanks for the report! if you type plt.tight_layout() do you get better. images? On Thu, Oct 18, 2018 at 8:50 AM Isaac Virshup <notifications@github.com>. wrote:. > As mentioned in #299 <https://github.com/theislab/scanpy/issues/299>, I'm. > getting some strange layout behavior when plotting at the repl. >. > Using the setup:. >. > import scanpy.api as scimport numpy as np. >. > N = 1000. > M = 2000. >. > adata = sc.AnnData(X=np.random.random_sample((N, M))). >. > sc.pp.pca(adata). >. > I get outputs that look like the following. >. > sc.pl.pca(adata, color=list(""123"")). >. > [image: screen shot 2018-10-18 at 4 46 46 pm]. > <https://user-images.githubusercontent.com/8238804/47133668-72d23780-d2f5-11e8-9630-f6e676a146ca.png>. >. > sc.pl.pca(adata, color=list(""1234"")). >. > [image: screen shot 2018-10-18 at 4 47 37 pm]. > <https://user-images.githubusercontent.com/8238804/47133696-909f9c80-d2f5-11e8-8482-eab30e84fe46.png>. >. > sc.pl.pca(adata, color=list(""12""), ncols=1). >. > [image: screen shot 2018-10-18 at 4 50 11 pm]. > <https://user-images.githubusercontent.com/8238804/47133806-17547980-d2f6-11e8-97ad-6d9528ea280b.png>. >. > I get similar results with the MacOSX and GR backends (haven't checked. > others). >. > I think the issue is due to the implicit bbox_inches=""tight"" occurring. > with the matplotlib inline backend, and its explicit usage for. > scanpy.plotting.utils.savefig. In the cases here, the bounding box is not. > being trimmed. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/309>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1RC33MewvC1wM45k3iek6QjaeHcyks5umCSYgaJpZM4Xsxiv>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/309
https://github.com/scverse/scanpy/issues/309:641,usability,user,user-images,641,"thanks for the report! if you type plt.tight_layout() do you get better. images? On Thu, Oct 18, 2018 at 8:50 AM Isaac Virshup <notifications@github.com>. wrote:. > As mentioned in #299 <https://github.com/theislab/scanpy/issues/299>, I'm. > getting some strange layout behavior when plotting at the repl. >. > Using the setup:. >. > import scanpy.api as scimport numpy as np. >. > N = 1000. > M = 2000. >. > adata = sc.AnnData(X=np.random.random_sample((N, M))). >. > sc.pp.pca(adata). >. > I get outputs that look like the following. >. > sc.pl.pca(adata, color=list(""123"")). >. > [image: screen shot 2018-10-18 at 4 46 46 pm]. > <https://user-images.githubusercontent.com/8238804/47133668-72d23780-d2f5-11e8-9630-f6e676a146ca.png>. >. > sc.pl.pca(adata, color=list(""1234"")). >. > [image: screen shot 2018-10-18 at 4 47 37 pm]. > <https://user-images.githubusercontent.com/8238804/47133696-909f9c80-d2f5-11e8-8482-eab30e84fe46.png>. >. > sc.pl.pca(adata, color=list(""12""), ncols=1). >. > [image: screen shot 2018-10-18 at 4 50 11 pm]. > <https://user-images.githubusercontent.com/8238804/47133806-17547980-d2f6-11e8-97ad-6d9528ea280b.png>. >. > I get similar results with the MacOSX and GR backends (haven't checked. > others). >. > I think the issue is due to the implicit bbox_inches=""tight"" occurring. > with the matplotlib inline backend, and its explicit usage for. > scanpy.plotting.utils.savefig. In the cases here, the bounding box is not. > being trimmed. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/309>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1RC33MewvC1wM45k3iek6QjaeHcyks5umCSYgaJpZM4Xsxiv>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/309
https://github.com/scverse/scanpy/issues/309:841,usability,user,user-images,841,"thanks for the report! if you type plt.tight_layout() do you get better. images? On Thu, Oct 18, 2018 at 8:50 AM Isaac Virshup <notifications@github.com>. wrote:. > As mentioned in #299 <https://github.com/theislab/scanpy/issues/299>, I'm. > getting some strange layout behavior when plotting at the repl. >. > Using the setup:. >. > import scanpy.api as scimport numpy as np. >. > N = 1000. > M = 2000. >. > adata = sc.AnnData(X=np.random.random_sample((N, M))). >. > sc.pp.pca(adata). >. > I get outputs that look like the following. >. > sc.pl.pca(adata, color=list(""123"")). >. > [image: screen shot 2018-10-18 at 4 46 46 pm]. > <https://user-images.githubusercontent.com/8238804/47133668-72d23780-d2f5-11e8-9630-f6e676a146ca.png>. >. > sc.pl.pca(adata, color=list(""1234"")). >. > [image: screen shot 2018-10-18 at 4 47 37 pm]. > <https://user-images.githubusercontent.com/8238804/47133696-909f9c80-d2f5-11e8-8482-eab30e84fe46.png>. >. > sc.pl.pca(adata, color=list(""12""), ncols=1). >. > [image: screen shot 2018-10-18 at 4 50 11 pm]. > <https://user-images.githubusercontent.com/8238804/47133806-17547980-d2f6-11e8-97ad-6d9528ea280b.png>. >. > I get similar results with the MacOSX and GR backends (haven't checked. > others). >. > I think the issue is due to the implicit bbox_inches=""tight"" occurring. > with the matplotlib inline backend, and its explicit usage for. > scanpy.plotting.utils.savefig. In the cases here, the bounding box is not. > being trimmed. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/309>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1RC33MewvC1wM45k3iek6QjaeHcyks5umCSYgaJpZM4Xsxiv>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/309
https://github.com/scverse/scanpy/issues/309:1048,usability,user,user-images,1048,"thanks for the report! if you type plt.tight_layout() do you get better. images? On Thu, Oct 18, 2018 at 8:50 AM Isaac Virshup <notifications@github.com>. wrote:. > As mentioned in #299 <https://github.com/theislab/scanpy/issues/299>, I'm. > getting some strange layout behavior when plotting at the repl. >. > Using the setup:. >. > import scanpy.api as scimport numpy as np. >. > N = 1000. > M = 2000. >. > adata = sc.AnnData(X=np.random.random_sample((N, M))). >. > sc.pp.pca(adata). >. > I get outputs that look like the following. >. > sc.pl.pca(adata, color=list(""123"")). >. > [image: screen shot 2018-10-18 at 4 46 46 pm]. > <https://user-images.githubusercontent.com/8238804/47133668-72d23780-d2f5-11e8-9630-f6e676a146ca.png>. >. > sc.pl.pca(adata, color=list(""1234"")). >. > [image: screen shot 2018-10-18 at 4 47 37 pm]. > <https://user-images.githubusercontent.com/8238804/47133696-909f9c80-d2f5-11e8-8482-eab30e84fe46.png>. >. > sc.pl.pca(adata, color=list(""12""), ncols=1). >. > [image: screen shot 2018-10-18 at 4 50 11 pm]. > <https://user-images.githubusercontent.com/8238804/47133806-17547980-d2f6-11e8-97ad-6d9528ea280b.png>. >. > I get similar results with the MacOSX and GR backends (haven't checked. > others). >. > I think the issue is due to the implicit bbox_inches=""tight"" occurring. > with the matplotlib inline backend, and its explicit usage for. > scanpy.plotting.utils.savefig. In the cases here, the bounding box is not. > being trimmed. >. > —. > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/309>, or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1RC33MewvC1wM45k3iek6QjaeHcyks5umCSYgaJpZM4Xsxiv>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/309
https://github.com/scverse/scanpy/issues/309:150,interoperability,compatib,compatible,150,"I get this warning:. ```python. /usr/local/lib/python3.6/site-packages/matplotlib/figure.py:2362: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect. warnings.warn(""This figure includes Axes that are not compatible "". ```. and the same layout.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/309
https://github.com/scverse/scanpy/issues/309:265,interoperability,compatib,compatible,265,"I get this warning:. ```python. /usr/local/lib/python3.6/site-packages/matplotlib/figure.py:2362: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect. warnings.warn(""This figure includes Axes that are not compatible "". ```. and the same layout.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/309
https://github.com/scverse/scanpy/issues/309:62,modifiability,pac,packages,62,"I get this warning:. ```python. /usr/local/lib/python3.6/site-packages/matplotlib/figure.py:2362: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect. warnings.warn(""This figure includes Axes that are not compatible "". ```. and the same layout.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/309
https://github.com/scverse/scanpy/issues/309:98,usability,User,UserWarning,98,"I get this warning:. ```python. /usr/local/lib/python3.6/site-packages/matplotlib/figure.py:2362: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect. warnings.warn(""This figure includes Axes that are not compatible "". ```. and the same layout.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/309
https://github.com/scverse/scanpy/issues/309:0,reliability,Doe,Does,0,Does this problem also happens with the previous code? this is just to guide me on a solution.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/309
https://github.com/scverse/scanpy/issues/309:71,usability,guid,guide,71,Does this problem also happens with the previous code? this is just to guide me on a solution.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/309
https://github.com/scverse/scanpy/issues/309:102,usability,user,user-images,102,"On a6165678, there are still larger margins on the right, but much less extreme:. ![figure_1](https://user-images.githubusercontent.com/8238804/47275093-1d996d00-d5f8-11e8-9373-fd8e3a16ea18.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/309
https://github.com/scverse/scanpy/issues/309:92,integrability,compon,components,92,"Great, thanks! Kind of unrelated, but I just noticed I can do:. ```python. sc.pl.pca(adata, components=[""1,2"", ""2,3""], color=[""sample_type"", ""total_features_by_counts""], ncols=2). ```. Which is awesome, thanks for that!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/309
https://github.com/scverse/scanpy/issues/309:92,interoperability,compon,components,92,"Great, thanks! Kind of unrelated, but I just noticed I can do:. ```python. sc.pl.pca(adata, components=[""1,2"", ""2,3""], color=[""sample_type"", ""total_features_by_counts""], ncols=2). ```. Which is awesome, thanks for that!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/309
https://github.com/scverse/scanpy/issues/309:92,modifiability,compon,components,92,"Great, thanks! Kind of unrelated, but I just noticed I can do:. ```python. sc.pl.pca(adata, components=[""1,2"", ""2,3""], color=[""sample_type"", ""total_features_by_counts""], ncols=2). ```. Which is awesome, thanks for that!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/309
https://github.com/scverse/scanpy/issues/309:255,integrability,compon,components,255,"Yes, that is possible. I think I added this probably after one of your. issues. On Wed, Oct 24, 2018 at 3:34 AM Isaac Virshup <notifications@github.com>. wrote:. > Great, thanks! >. > Kind of unrelated, but I just noticed I can do:. >. > sc.pl.pca(adata, components=[""1,2"", ""2,3""], color=[""sample_type"", ""total_features_by_counts""], ncols=2). >. > Which is awesome, thanks for that! >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/309#issuecomment-432480196>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1bf9IHD4vQkchUZv29zkr9Bvn71yks5un8OfgaJpZM4Xsxiv>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/309
https://github.com/scverse/scanpy/issues/309:255,interoperability,compon,components,255,"Yes, that is possible. I think I added this probably after one of your. issues. On Wed, Oct 24, 2018 at 3:34 AM Isaac Virshup <notifications@github.com>. wrote:. > Great, thanks! >. > Kind of unrelated, but I just noticed I can do:. >. > sc.pl.pca(adata, components=[""1,2"", ""2,3""], color=[""sample_type"", ""total_features_by_counts""], ncols=2). >. > Which is awesome, thanks for that! >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/309#issuecomment-432480196>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1bf9IHD4vQkchUZv29zkr9Bvn71yks5un8OfgaJpZM4Xsxiv>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/309
https://github.com/scverse/scanpy/issues/309:255,modifiability,compon,components,255,"Yes, that is possible. I think I added this probably after one of your. issues. On Wed, Oct 24, 2018 at 3:34 AM Isaac Virshup <notifications@github.com>. wrote:. > Great, thanks! >. > Kind of unrelated, but I just noticed I can do:. >. > sc.pl.pca(adata, components=[""1,2"", ""2,3""], color=[""sample_type"", ""total_features_by_counts""], ncols=2). >. > Which is awesome, thanks for that! >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/309#issuecomment-432480196>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1bf9IHD4vQkchUZv29zkr9Bvn71yks5un8OfgaJpZM4Xsxiv>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/309
https://github.com/scverse/scanpy/issues/309:635,security,auth,auth,635,"Yes, that is possible. I think I added this probably after one of your. issues. On Wed, Oct 24, 2018 at 3:34 AM Isaac Virshup <notifications@github.com>. wrote:. > Great, thanks! >. > Kind of unrelated, but I just noticed I can do:. >. > sc.pl.pca(adata, components=[""1,2"", ""2,3""], color=[""sample_type"", ""total_features_by_counts""], ncols=2). >. > Which is awesome, thanks for that! >. > —. > You are receiving this because you commented. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/309#issuecomment-432480196>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1bf9IHD4vQkchUZv29zkr9Bvn71yks5un8OfgaJpZM4Xsxiv>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/309
https://github.com/scverse/scanpy/issues/310:92,integrability,interfac,interface,92,"Yes, thank you. That would be very welcome! . @rfechtner: could it be that compat with your interface got messed up? It would be nice if you'd maintained the interface when you do so drastic changes to the underlying package. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/310
https://github.com/scverse/scanpy/issues/310:158,integrability,interfac,interface,158,"Yes, thank you. That would be very welcome! . @rfechtner: could it be that compat with your interface got messed up? It would be nice if you'd maintained the interface when you do so drastic changes to the underlying package. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/310
https://github.com/scverse/scanpy/issues/310:92,interoperability,interfac,interface,92,"Yes, thank you. That would be very welcome! . @rfechtner: could it be that compat with your interface got messed up? It would be nice if you'd maintained the interface when you do so drastic changes to the underlying package. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/310
https://github.com/scverse/scanpy/issues/310:158,interoperability,interfac,interface,158,"Yes, thank you. That would be very welcome! . @rfechtner: could it be that compat with your interface got messed up? It would be nice if you'd maintained the interface when you do so drastic changes to the underlying package. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/310
https://github.com/scverse/scanpy/issues/310:92,modifiability,interfac,interface,92,"Yes, thank you. That would be very welcome! . @rfechtner: could it be that compat with your interface got messed up? It would be nice if you'd maintained the interface when you do so drastic changes to the underlying package. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/310
https://github.com/scverse/scanpy/issues/310:143,modifiability,maintain,maintained,143,"Yes, thank you. That would be very welcome! . @rfechtner: could it be that compat with your interface got messed up? It would be nice if you'd maintained the interface when you do so drastic changes to the underlying package. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/310
https://github.com/scverse/scanpy/issues/310:158,modifiability,interfac,interface,158,"Yes, thank you. That would be very welcome! . @rfechtner: could it be that compat with your interface got messed up? It would be nice if you'd maintained the interface when you do so drastic changes to the underlying package. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/310
https://github.com/scverse/scanpy/issues/310:217,modifiability,pac,package,217,"Yes, thank you. That would be very welcome! . @rfechtner: could it be that compat with your interface got messed up? It would be nice if you'd maintained the interface when you do so drastic changes to the underlying package. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/310
https://github.com/scverse/scanpy/issues/310:143,safety,maintain,maintained,143,"Yes, thank you. That would be very welcome! . @rfechtner: could it be that compat with your interface got messed up? It would be nice if you'd maintained the interface when you do so drastic changes to the underlying package. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/310
https://github.com/scverse/scanpy/issues/310:137,deployability,updat,updates,137,"@bebatut Thank you for pointing that out. I will as well take a look at it. . @falexwolf I will make sure to keep it in mind for further updates. The restructuring of the package structure came with the update from 1.x to 2.x and was necessary for some major improvements. I'm sorry for caused inconveniences. @bebatut if you have further questions or issues, please let me know, I'd be happy to help you out. Ron",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/310
https://github.com/scverse/scanpy/issues/310:203,deployability,updat,update,203,"@bebatut Thank you for pointing that out. I will as well take a look at it. . @falexwolf I will make sure to keep it in mind for further updates. The restructuring of the package structure came with the update from 1.x to 2.x and was necessary for some major improvements. I'm sorry for caused inconveniences. @bebatut if you have further questions or issues, please let me know, I'd be happy to help you out. Ron",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/310
https://github.com/scverse/scanpy/issues/310:171,modifiability,pac,package,171,"@bebatut Thank you for pointing that out. I will as well take a look at it. . @falexwolf I will make sure to keep it in mind for further updates. The restructuring of the package structure came with the update from 1.x to 2.x and was necessary for some major improvements. I'm sorry for caused inconveniences. @bebatut if you have further questions or issues, please let me know, I'd be happy to help you out. Ron",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/310
https://github.com/scverse/scanpy/issues/310:137,safety,updat,updates,137,"@bebatut Thank you for pointing that out. I will as well take a look at it. . @falexwolf I will make sure to keep it in mind for further updates. The restructuring of the package structure came with the update from 1.x to 2.x and was necessary for some major improvements. I'm sorry for caused inconveniences. @bebatut if you have further questions or issues, please let me know, I'd be happy to help you out. Ron",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/310
https://github.com/scverse/scanpy/issues/310:203,safety,updat,update,203,"@bebatut Thank you for pointing that out. I will as well take a look at it. . @falexwolf I will make sure to keep it in mind for further updates. The restructuring of the package structure came with the update from 1.x to 2.x and was necessary for some major improvements. I'm sorry for caused inconveniences. @bebatut if you have further questions or issues, please let me know, I'd be happy to help you out. Ron",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/310
https://github.com/scverse/scanpy/issues/310:137,security,updat,updates,137,"@bebatut Thank you for pointing that out. I will as well take a look at it. . @falexwolf I will make sure to keep it in mind for further updates. The restructuring of the package structure came with the update from 1.x to 2.x and was necessary for some major improvements. I'm sorry for caused inconveniences. @bebatut if you have further questions or issues, please let me know, I'd be happy to help you out. Ron",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/310
https://github.com/scverse/scanpy/issues/310:203,security,updat,update,203,"@bebatut Thank you for pointing that out. I will as well take a look at it. . @falexwolf I will make sure to keep it in mind for further updates. The restructuring of the package structure came with the update from 1.x to 2.x and was necessary for some major improvements. I'm sorry for caused inconveniences. @bebatut if you have further questions or issues, please let me know, I'd be happy to help you out. Ron",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/310
https://github.com/scverse/scanpy/issues/310:396,usability,help,help,396,"@bebatut Thank you for pointing that out. I will as well take a look at it. . @falexwolf I will make sure to keep it in mind for further updates. The restructuring of the package structure came with the update from 1.x to 2.x and was necessary for some major improvements. I'm sorry for caused inconveniences. @bebatut if you have further questions or issues, please let me know, I'd be happy to help you out. Ron",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/310
https://github.com/scverse/scanpy/issues/311:196,usability,command,command,196,"Dear Bérénice, thanks for reporting these issues. Recently, I replaced most of the functions related to scatter so I think that what you report refers to the previous code. Can you maybe post the command that you used.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:287,availability,error,error,287,"Hi @fidelram, good to see you :smile:. I was working on the Galaxy integration. I tested that with the `1.3.2` version from Bioconda. I tested with adata from krumsiek11. - For colors, I tried with `sc.pl.scatter(adata=adata, x='EKLF', y='Cebpa', color=['EgrNab', 'cJun']) and I got the error:. ```. ... and (color is None or color in adata.obs.keys() or color in adata.var.index)):. File ""path/to/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 2035, in __contains__. hash(key). TypeError: unhashable type: 'list'. ```. - For components: the command was . ```. sc.pl.scatter(. adata=adata,. x='EKLF',. y='Cebpa',. color='EgrNab',. layers=('X', 'X', 'X'),. use_raw=False,. sort_order=True,. components='all',. projection='2d',. legend_loc='right margin',. legend_fontsize=1,. legend_fontweight='normal',. palette='viridis',. frameon=True,. right_margin=1.0,. size=1.0,. show=False,. save='.png'). ```. and the error:. ```. components = np.array(components).astype(int) - 1. ValueError: invalid literal for int() with base 10: 'all'. ```. Did I put the parameters in a wrong way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:925,availability,error,error,925,"Hi @fidelram, good to see you :smile:. I was working on the Galaxy integration. I tested that with the `1.3.2` version from Bioconda. I tested with adata from krumsiek11. - For colors, I tried with `sc.pl.scatter(adata=adata, x='EKLF', y='Cebpa', color=['EgrNab', 'cJun']) and I got the error:. ```. ... and (color is None or color in adata.obs.keys() or color in adata.var.index)):. File ""path/to/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 2035, in __contains__. hash(key). TypeError: unhashable type: 'list'. ```. - For components: the command was . ```. sc.pl.scatter(. adata=adata,. x='EKLF',. y='Cebpa',. color='EgrNab',. layers=('X', 'X', 'X'),. use_raw=False,. sort_order=True,. components='all',. projection='2d',. legend_loc='right margin',. legend_fontsize=1,. legend_fontweight='normal',. palette='viridis',. frameon=True,. right_margin=1.0,. size=1.0,. show=False,. save='.png'). ```. and the error:. ```. components = np.array(components).astype(int) - 1. ValueError: invalid literal for int() with base 10: 'all'. ```. Did I put the parameters in a wrong way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:67,deployability,integr,integration,67,"Hi @fidelram, good to see you :smile:. I was working on the Galaxy integration. I tested that with the `1.3.2` version from Bioconda. I tested with adata from krumsiek11. - For colors, I tried with `sc.pl.scatter(adata=adata, x='EKLF', y='Cebpa', color=['EgrNab', 'cJun']) and I got the error:. ```. ... and (color is None or color in adata.obs.keys() or color in adata.var.index)):. File ""path/to/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 2035, in __contains__. hash(key). TypeError: unhashable type: 'list'. ```. - For components: the command was . ```. sc.pl.scatter(. adata=adata,. x='EKLF',. y='Cebpa',. color='EgrNab',. layers=('X', 'X', 'X'),. use_raw=False,. sort_order=True,. components='all',. projection='2d',. legend_loc='right margin',. legend_fontsize=1,. legend_fontweight='normal',. palette='viridis',. frameon=True,. right_margin=1.0,. size=1.0,. show=False,. save='.png'). ```. and the error:. ```. components = np.array(components).astype(int) - 1. ValueError: invalid literal for int() with base 10: 'all'. ```. Did I put the parameters in a wrong way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:111,deployability,version,version,111,"Hi @fidelram, good to see you :smile:. I was working on the Galaxy integration. I tested that with the `1.3.2` version from Bioconda. I tested with adata from krumsiek11. - For colors, I tried with `sc.pl.scatter(adata=adata, x='EKLF', y='Cebpa', color=['EgrNab', 'cJun']) and I got the error:. ```. ... and (color is None or color in adata.obs.keys() or color in adata.var.index)):. File ""path/to/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 2035, in __contains__. hash(key). TypeError: unhashable type: 'list'. ```. - For components: the command was . ```. sc.pl.scatter(. adata=adata,. x='EKLF',. y='Cebpa',. color='EgrNab',. layers=('X', 'X', 'X'),. use_raw=False,. sort_order=True,. components='all',. projection='2d',. legend_loc='right margin',. legend_fontsize=1,. legend_fontweight='normal',. palette='viridis',. frameon=True,. right_margin=1.0,. size=1.0,. show=False,. save='.png'). ```. and the error:. ```. components = np.array(components).astype(int) - 1. ValueError: invalid literal for int() with base 10: 'all'. ```. Did I put the parameters in a wrong way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:433,energy efficiency,core,core,433,"Hi @fidelram, good to see you :smile:. I was working on the Galaxy integration. I tested that with the `1.3.2` version from Bioconda. I tested with adata from krumsiek11. - For colors, I tried with `sc.pl.scatter(adata=adata, x='EKLF', y='Cebpa', color=['EgrNab', 'cJun']) and I got the error:. ```. ... and (color is None or color in adata.obs.keys() or color in adata.var.index)):. File ""path/to/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 2035, in __contains__. hash(key). TypeError: unhashable type: 'list'. ```. - For components: the command was . ```. sc.pl.scatter(. adata=adata,. x='EKLF',. y='Cebpa',. color='EgrNab',. layers=('X', 'X', 'X'),. use_raw=False,. sort_order=True,. components='all',. projection='2d',. legend_loc='right margin',. legend_fontsize=1,. legend_fontweight='normal',. palette='viridis',. frameon=True,. right_margin=1.0,. size=1.0,. show=False,. save='.png'). ```. and the error:. ```. components = np.array(components).astype(int) - 1. ValueError: invalid literal for int() with base 10: 'all'. ```. Did I put the parameters in a wrong way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:67,integrability,integr,integration,67,"Hi @fidelram, good to see you :smile:. I was working on the Galaxy integration. I tested that with the `1.3.2` version from Bioconda. I tested with adata from krumsiek11. - For colors, I tried with `sc.pl.scatter(adata=adata, x='EKLF', y='Cebpa', color=['EgrNab', 'cJun']) and I got the error:. ```. ... and (color is None or color in adata.obs.keys() or color in adata.var.index)):. File ""path/to/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 2035, in __contains__. hash(key). TypeError: unhashable type: 'list'. ```. - For components: the command was . ```. sc.pl.scatter(. adata=adata,. x='EKLF',. y='Cebpa',. color='EgrNab',. layers=('X', 'X', 'X'),. use_raw=False,. sort_order=True,. components='all',. projection='2d',. legend_loc='right margin',. legend_fontsize=1,. legend_fontweight='normal',. palette='viridis',. frameon=True,. right_margin=1.0,. size=1.0,. show=False,. save='.png'). ```. and the error:. ```. components = np.array(components).astype(int) - 1. ValueError: invalid literal for int() with base 10: 'all'. ```. Did I put the parameters in a wrong way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:111,integrability,version,version,111,"Hi @fidelram, good to see you :smile:. I was working on the Galaxy integration. I tested that with the `1.3.2` version from Bioconda. I tested with adata from krumsiek11. - For colors, I tried with `sc.pl.scatter(adata=adata, x='EKLF', y='Cebpa', color=['EgrNab', 'cJun']) and I got the error:. ```. ... and (color is None or color in adata.obs.keys() or color in adata.var.index)):. File ""path/to/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 2035, in __contains__. hash(key). TypeError: unhashable type: 'list'. ```. - For components: the command was . ```. sc.pl.scatter(. adata=adata,. x='EKLF',. y='Cebpa',. color='EgrNab',. layers=('X', 'X', 'X'),. use_raw=False,. sort_order=True,. components='all',. projection='2d',. legend_loc='right margin',. legend_fontsize=1,. legend_fontweight='normal',. palette='viridis',. frameon=True,. right_margin=1.0,. size=1.0,. show=False,. save='.png'). ```. and the error:. ```. components = np.array(components).astype(int) - 1. ValueError: invalid literal for int() with base 10: 'all'. ```. Did I put the parameters in a wrong way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:542,integrability,compon,components,542,"Hi @fidelram, good to see you :smile:. I was working on the Galaxy integration. I tested that with the `1.3.2` version from Bioconda. I tested with adata from krumsiek11. - For colors, I tried with `sc.pl.scatter(adata=adata, x='EKLF', y='Cebpa', color=['EgrNab', 'cJun']) and I got the error:. ```. ... and (color is None or color in adata.obs.keys() or color in adata.var.index)):. File ""path/to/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 2035, in __contains__. hash(key). TypeError: unhashable type: 'list'. ```. - For components: the command was . ```. sc.pl.scatter(. adata=adata,. x='EKLF',. y='Cebpa',. color='EgrNab',. layers=('X', 'X', 'X'),. use_raw=False,. sort_order=True,. components='all',. projection='2d',. legend_loc='right margin',. legend_fontsize=1,. legend_fontweight='normal',. palette='viridis',. frameon=True,. right_margin=1.0,. size=1.0,. show=False,. save='.png'). ```. and the error:. ```. components = np.array(components).astype(int) - 1. ValueError: invalid literal for int() with base 10: 'all'. ```. Did I put the parameters in a wrong way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:706,integrability,compon,components,706,"Hi @fidelram, good to see you :smile:. I was working on the Galaxy integration. I tested that with the `1.3.2` version from Bioconda. I tested with adata from krumsiek11. - For colors, I tried with `sc.pl.scatter(adata=adata, x='EKLF', y='Cebpa', color=['EgrNab', 'cJun']) and I got the error:. ```. ... and (color is None or color in adata.obs.keys() or color in adata.var.index)):. File ""path/to/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 2035, in __contains__. hash(key). TypeError: unhashable type: 'list'. ```. - For components: the command was . ```. sc.pl.scatter(. adata=adata,. x='EKLF',. y='Cebpa',. color='EgrNab',. layers=('X', 'X', 'X'),. use_raw=False,. sort_order=True,. components='all',. projection='2d',. legend_loc='right margin',. legend_fontsize=1,. legend_fontweight='normal',. palette='viridis',. frameon=True,. right_margin=1.0,. size=1.0,. show=False,. save='.png'). ```. and the error:. ```. components = np.array(components).astype(int) - 1. ValueError: invalid literal for int() with base 10: 'all'. ```. Did I put the parameters in a wrong way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:938,integrability,compon,components,938,"Hi @fidelram, good to see you :smile:. I was working on the Galaxy integration. I tested that with the `1.3.2` version from Bioconda. I tested with adata from krumsiek11. - For colors, I tried with `sc.pl.scatter(adata=adata, x='EKLF', y='Cebpa', color=['EgrNab', 'cJun']) and I got the error:. ```. ... and (color is None or color in adata.obs.keys() or color in adata.var.index)):. File ""path/to/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 2035, in __contains__. hash(key). TypeError: unhashable type: 'list'. ```. - For components: the command was . ```. sc.pl.scatter(. adata=adata,. x='EKLF',. y='Cebpa',. color='EgrNab',. layers=('X', 'X', 'X'),. use_raw=False,. sort_order=True,. components='all',. projection='2d',. legend_loc='right margin',. legend_fontsize=1,. legend_fontweight='normal',. palette='viridis',. frameon=True,. right_margin=1.0,. size=1.0,. show=False,. save='.png'). ```. and the error:. ```. components = np.array(components).astype(int) - 1. ValueError: invalid literal for int() with base 10: 'all'. ```. Did I put the parameters in a wrong way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:960,integrability,compon,components,960,"Hi @fidelram, good to see you :smile:. I was working on the Galaxy integration. I tested that with the `1.3.2` version from Bioconda. I tested with adata from krumsiek11. - For colors, I tried with `sc.pl.scatter(adata=adata, x='EKLF', y='Cebpa', color=['EgrNab', 'cJun']) and I got the error:. ```. ... and (color is None or color in adata.obs.keys() or color in adata.var.index)):. File ""path/to/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 2035, in __contains__. hash(key). TypeError: unhashable type: 'list'. ```. - For components: the command was . ```. sc.pl.scatter(. adata=adata,. x='EKLF',. y='Cebpa',. color='EgrNab',. layers=('X', 'X', 'X'),. use_raw=False,. sort_order=True,. components='all',. projection='2d',. legend_loc='right margin',. legend_fontsize=1,. legend_fontweight='normal',. palette='viridis',. frameon=True,. right_margin=1.0,. size=1.0,. show=False,. save='.png'). ```. and the error:. ```. components = np.array(components).astype(int) - 1. ValueError: invalid literal for int() with base 10: 'all'. ```. Did I put the parameters in a wrong way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:67,interoperability,integr,integration,67,"Hi @fidelram, good to see you :smile:. I was working on the Galaxy integration. I tested that with the `1.3.2` version from Bioconda. I tested with adata from krumsiek11. - For colors, I tried with `sc.pl.scatter(adata=adata, x='EKLF', y='Cebpa', color=['EgrNab', 'cJun']) and I got the error:. ```. ... and (color is None or color in adata.obs.keys() or color in adata.var.index)):. File ""path/to/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 2035, in __contains__. hash(key). TypeError: unhashable type: 'list'. ```. - For components: the command was . ```. sc.pl.scatter(. adata=adata,. x='EKLF',. y='Cebpa',. color='EgrNab',. layers=('X', 'X', 'X'),. use_raw=False,. sort_order=True,. components='all',. projection='2d',. legend_loc='right margin',. legend_fontsize=1,. legend_fontweight='normal',. palette='viridis',. frameon=True,. right_margin=1.0,. size=1.0,. show=False,. save='.png'). ```. and the error:. ```. components = np.array(components).astype(int) - 1. ValueError: invalid literal for int() with base 10: 'all'. ```. Did I put the parameters in a wrong way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:542,interoperability,compon,components,542,"Hi @fidelram, good to see you :smile:. I was working on the Galaxy integration. I tested that with the `1.3.2` version from Bioconda. I tested with adata from krumsiek11. - For colors, I tried with `sc.pl.scatter(adata=adata, x='EKLF', y='Cebpa', color=['EgrNab', 'cJun']) and I got the error:. ```. ... and (color is None or color in adata.obs.keys() or color in adata.var.index)):. File ""path/to/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 2035, in __contains__. hash(key). TypeError: unhashable type: 'list'. ```. - For components: the command was . ```. sc.pl.scatter(. adata=adata,. x='EKLF',. y='Cebpa',. color='EgrNab',. layers=('X', 'X', 'X'),. use_raw=False,. sort_order=True,. components='all',. projection='2d',. legend_loc='right margin',. legend_fontsize=1,. legend_fontweight='normal',. palette='viridis',. frameon=True,. right_margin=1.0,. size=1.0,. show=False,. save='.png'). ```. and the error:. ```. components = np.array(components).astype(int) - 1. ValueError: invalid literal for int() with base 10: 'all'. ```. Did I put the parameters in a wrong way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:706,interoperability,compon,components,706,"Hi @fidelram, good to see you :smile:. I was working on the Galaxy integration. I tested that with the `1.3.2` version from Bioconda. I tested with adata from krumsiek11. - For colors, I tried with `sc.pl.scatter(adata=adata, x='EKLF', y='Cebpa', color=['EgrNab', 'cJun']) and I got the error:. ```. ... and (color is None or color in adata.obs.keys() or color in adata.var.index)):. File ""path/to/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 2035, in __contains__. hash(key). TypeError: unhashable type: 'list'. ```. - For components: the command was . ```. sc.pl.scatter(. adata=adata,. x='EKLF',. y='Cebpa',. color='EgrNab',. layers=('X', 'X', 'X'),. use_raw=False,. sort_order=True,. components='all',. projection='2d',. legend_loc='right margin',. legend_fontsize=1,. legend_fontweight='normal',. palette='viridis',. frameon=True,. right_margin=1.0,. size=1.0,. show=False,. save='.png'). ```. and the error:. ```. components = np.array(components).astype(int) - 1. ValueError: invalid literal for int() with base 10: 'all'. ```. Did I put the parameters in a wrong way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:938,interoperability,compon,components,938,"Hi @fidelram, good to see you :smile:. I was working on the Galaxy integration. I tested that with the `1.3.2` version from Bioconda. I tested with adata from krumsiek11. - For colors, I tried with `sc.pl.scatter(adata=adata, x='EKLF', y='Cebpa', color=['EgrNab', 'cJun']) and I got the error:. ```. ... and (color is None or color in adata.obs.keys() or color in adata.var.index)):. File ""path/to/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 2035, in __contains__. hash(key). TypeError: unhashable type: 'list'. ```. - For components: the command was . ```. sc.pl.scatter(. adata=adata,. x='EKLF',. y='Cebpa',. color='EgrNab',. layers=('X', 'X', 'X'),. use_raw=False,. sort_order=True,. components='all',. projection='2d',. legend_loc='right margin',. legend_fontsize=1,. legend_fontweight='normal',. palette='viridis',. frameon=True,. right_margin=1.0,. size=1.0,. show=False,. save='.png'). ```. and the error:. ```. components = np.array(components).astype(int) - 1. ValueError: invalid literal for int() with base 10: 'all'. ```. Did I put the parameters in a wrong way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:960,interoperability,compon,components,960,"Hi @fidelram, good to see you :smile:. I was working on the Galaxy integration. I tested that with the `1.3.2` version from Bioconda. I tested with adata from krumsiek11. - For colors, I tried with `sc.pl.scatter(adata=adata, x='EKLF', y='Cebpa', color=['EgrNab', 'cJun']) and I got the error:. ```. ... and (color is None or color in adata.obs.keys() or color in adata.var.index)):. File ""path/to/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 2035, in __contains__. hash(key). TypeError: unhashable type: 'list'. ```. - For components: the command was . ```. sc.pl.scatter(. adata=adata,. x='EKLF',. y='Cebpa',. color='EgrNab',. layers=('X', 'X', 'X'),. use_raw=False,. sort_order=True,. components='all',. projection='2d',. legend_loc='right margin',. legend_fontsize=1,. legend_fontweight='normal',. palette='viridis',. frameon=True,. right_margin=1.0,. size=1.0,. show=False,. save='.png'). ```. and the error:. ```. components = np.array(components).astype(int) - 1. ValueError: invalid literal for int() with base 10: 'all'. ```. Did I put the parameters in a wrong way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:67,modifiability,integr,integration,67,"Hi @fidelram, good to see you :smile:. I was working on the Galaxy integration. I tested that with the `1.3.2` version from Bioconda. I tested with adata from krumsiek11. - For colors, I tried with `sc.pl.scatter(adata=adata, x='EKLF', y='Cebpa', color=['EgrNab', 'cJun']) and I got the error:. ```. ... and (color is None or color in adata.obs.keys() or color in adata.var.index)):. File ""path/to/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 2035, in __contains__. hash(key). TypeError: unhashable type: 'list'. ```. - For components: the command was . ```. sc.pl.scatter(. adata=adata,. x='EKLF',. y='Cebpa',. color='EgrNab',. layers=('X', 'X', 'X'),. use_raw=False,. sort_order=True,. components='all',. projection='2d',. legend_loc='right margin',. legend_fontsize=1,. legend_fontweight='normal',. palette='viridis',. frameon=True,. right_margin=1.0,. size=1.0,. show=False,. save='.png'). ```. and the error:. ```. components = np.array(components).astype(int) - 1. ValueError: invalid literal for int() with base 10: 'all'. ```. Did I put the parameters in a wrong way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:111,modifiability,version,version,111,"Hi @fidelram, good to see you :smile:. I was working on the Galaxy integration. I tested that with the `1.3.2` version from Bioconda. I tested with adata from krumsiek11. - For colors, I tried with `sc.pl.scatter(adata=adata, x='EKLF', y='Cebpa', color=['EgrNab', 'cJun']) and I got the error:. ```. ... and (color is None or color in adata.obs.keys() or color in adata.var.index)):. File ""path/to/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 2035, in __contains__. hash(key). TypeError: unhashable type: 'list'. ```. - For components: the command was . ```. sc.pl.scatter(. adata=adata,. x='EKLF',. y='Cebpa',. color='EgrNab',. layers=('X', 'X', 'X'),. use_raw=False,. sort_order=True,. components='all',. projection='2d',. legend_loc='right margin',. legend_fontsize=1,. legend_fontweight='normal',. palette='viridis',. frameon=True,. right_margin=1.0,. size=1.0,. show=False,. save='.png'). ```. and the error:. ```. components = np.array(components).astype(int) - 1. ValueError: invalid literal for int() with base 10: 'all'. ```. Did I put the parameters in a wrong way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:417,modifiability,pac,packages,417,"Hi @fidelram, good to see you :smile:. I was working on the Galaxy integration. I tested that with the `1.3.2` version from Bioconda. I tested with adata from krumsiek11. - For colors, I tried with `sc.pl.scatter(adata=adata, x='EKLF', y='Cebpa', color=['EgrNab', 'cJun']) and I got the error:. ```. ... and (color is None or color in adata.obs.keys() or color in adata.var.index)):. File ""path/to/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 2035, in __contains__. hash(key). TypeError: unhashable type: 'list'. ```. - For components: the command was . ```. sc.pl.scatter(. adata=adata,. x='EKLF',. y='Cebpa',. color='EgrNab',. layers=('X', 'X', 'X'),. use_raw=False,. sort_order=True,. components='all',. projection='2d',. legend_loc='right margin',. legend_fontsize=1,. legend_fontweight='normal',. palette='viridis',. frameon=True,. right_margin=1.0,. size=1.0,. show=False,. save='.png'). ```. and the error:. ```. components = np.array(components).astype(int) - 1. ValueError: invalid literal for int() with base 10: 'all'. ```. Did I put the parameters in a wrong way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:542,modifiability,compon,components,542,"Hi @fidelram, good to see you :smile:. I was working on the Galaxy integration. I tested that with the `1.3.2` version from Bioconda. I tested with adata from krumsiek11. - For colors, I tried with `sc.pl.scatter(adata=adata, x='EKLF', y='Cebpa', color=['EgrNab', 'cJun']) and I got the error:. ```. ... and (color is None or color in adata.obs.keys() or color in adata.var.index)):. File ""path/to/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 2035, in __contains__. hash(key). TypeError: unhashable type: 'list'. ```. - For components: the command was . ```. sc.pl.scatter(. adata=adata,. x='EKLF',. y='Cebpa',. color='EgrNab',. layers=('X', 'X', 'X'),. use_raw=False,. sort_order=True,. components='all',. projection='2d',. legend_loc='right margin',. legend_fontsize=1,. legend_fontweight='normal',. palette='viridis',. frameon=True,. right_margin=1.0,. size=1.0,. show=False,. save='.png'). ```. and the error:. ```. components = np.array(components).astype(int) - 1. ValueError: invalid literal for int() with base 10: 'all'. ```. Did I put the parameters in a wrong way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:647,modifiability,layer,layers,647,"Hi @fidelram, good to see you :smile:. I was working on the Galaxy integration. I tested that with the `1.3.2` version from Bioconda. I tested with adata from krumsiek11. - For colors, I tried with `sc.pl.scatter(adata=adata, x='EKLF', y='Cebpa', color=['EgrNab', 'cJun']) and I got the error:. ```. ... and (color is None or color in adata.obs.keys() or color in adata.var.index)):. File ""path/to/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 2035, in __contains__. hash(key). TypeError: unhashable type: 'list'. ```. - For components: the command was . ```. sc.pl.scatter(. adata=adata,. x='EKLF',. y='Cebpa',. color='EgrNab',. layers=('X', 'X', 'X'),. use_raw=False,. sort_order=True,. components='all',. projection='2d',. legend_loc='right margin',. legend_fontsize=1,. legend_fontweight='normal',. palette='viridis',. frameon=True,. right_margin=1.0,. size=1.0,. show=False,. save='.png'). ```. and the error:. ```. components = np.array(components).astype(int) - 1. ValueError: invalid literal for int() with base 10: 'all'. ```. Did I put the parameters in a wrong way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:706,modifiability,compon,components,706,"Hi @fidelram, good to see you :smile:. I was working on the Galaxy integration. I tested that with the `1.3.2` version from Bioconda. I tested with adata from krumsiek11. - For colors, I tried with `sc.pl.scatter(adata=adata, x='EKLF', y='Cebpa', color=['EgrNab', 'cJun']) and I got the error:. ```. ... and (color is None or color in adata.obs.keys() or color in adata.var.index)):. File ""path/to/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 2035, in __contains__. hash(key). TypeError: unhashable type: 'list'. ```. - For components: the command was . ```. sc.pl.scatter(. adata=adata,. x='EKLF',. y='Cebpa',. color='EgrNab',. layers=('X', 'X', 'X'),. use_raw=False,. sort_order=True,. components='all',. projection='2d',. legend_loc='right margin',. legend_fontsize=1,. legend_fontweight='normal',. palette='viridis',. frameon=True,. right_margin=1.0,. size=1.0,. show=False,. save='.png'). ```. and the error:. ```. components = np.array(components).astype(int) - 1. ValueError: invalid literal for int() with base 10: 'all'. ```. Did I put the parameters in a wrong way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:938,modifiability,compon,components,938,"Hi @fidelram, good to see you :smile:. I was working on the Galaxy integration. I tested that with the `1.3.2` version from Bioconda. I tested with adata from krumsiek11. - For colors, I tried with `sc.pl.scatter(adata=adata, x='EKLF', y='Cebpa', color=['EgrNab', 'cJun']) and I got the error:. ```. ... and (color is None or color in adata.obs.keys() or color in adata.var.index)):. File ""path/to/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 2035, in __contains__. hash(key). TypeError: unhashable type: 'list'. ```. - For components: the command was . ```. sc.pl.scatter(. adata=adata,. x='EKLF',. y='Cebpa',. color='EgrNab',. layers=('X', 'X', 'X'),. use_raw=False,. sort_order=True,. components='all',. projection='2d',. legend_loc='right margin',. legend_fontsize=1,. legend_fontweight='normal',. palette='viridis',. frameon=True,. right_margin=1.0,. size=1.0,. show=False,. save='.png'). ```. and the error:. ```. components = np.array(components).astype(int) - 1. ValueError: invalid literal for int() with base 10: 'all'. ```. Did I put the parameters in a wrong way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:960,modifiability,compon,components,960,"Hi @fidelram, good to see you :smile:. I was working on the Galaxy integration. I tested that with the `1.3.2` version from Bioconda. I tested with adata from krumsiek11. - For colors, I tried with `sc.pl.scatter(adata=adata, x='EKLF', y='Cebpa', color=['EgrNab', 'cJun']) and I got the error:. ```. ... and (color is None or color in adata.obs.keys() or color in adata.var.index)):. File ""path/to/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 2035, in __contains__. hash(key). TypeError: unhashable type: 'list'. ```. - For components: the command was . ```. sc.pl.scatter(. adata=adata,. x='EKLF',. y='Cebpa',. color='EgrNab',. layers=('X', 'X', 'X'),. use_raw=False,. sort_order=True,. components='all',. projection='2d',. legend_loc='right margin',. legend_fontsize=1,. legend_fontweight='normal',. palette='viridis',. frameon=True,. right_margin=1.0,. size=1.0,. show=False,. save='.png'). ```. and the error:. ```. components = np.array(components).astype(int) - 1. ValueError: invalid literal for int() with base 10: 'all'. ```. Did I put the parameters in a wrong way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:1067,modifiability,paramet,parameters,1067,"Hi @fidelram, good to see you :smile:. I was working on the Galaxy integration. I tested that with the `1.3.2` version from Bioconda. I tested with adata from krumsiek11. - For colors, I tried with `sc.pl.scatter(adata=adata, x='EKLF', y='Cebpa', color=['EgrNab', 'cJun']) and I got the error:. ```. ... and (color is None or color in adata.obs.keys() or color in adata.var.index)):. File ""path/to/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 2035, in __contains__. hash(key). TypeError: unhashable type: 'list'. ```. - For components: the command was . ```. sc.pl.scatter(. adata=adata,. x='EKLF',. y='Cebpa',. color='EgrNab',. layers=('X', 'X', 'X'),. use_raw=False,. sort_order=True,. components='all',. projection='2d',. legend_loc='right margin',. legend_fontsize=1,. legend_fontweight='normal',. palette='viridis',. frameon=True,. right_margin=1.0,. size=1.0,. show=False,. save='.png'). ```. and the error:. ```. components = np.array(components).astype(int) - 1. ValueError: invalid literal for int() with base 10: 'all'. ```. Did I put the parameters in a wrong way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:287,performance,error,error,287,"Hi @fidelram, good to see you :smile:. I was working on the Galaxy integration. I tested that with the `1.3.2` version from Bioconda. I tested with adata from krumsiek11. - For colors, I tried with `sc.pl.scatter(adata=adata, x='EKLF', y='Cebpa', color=['EgrNab', 'cJun']) and I got the error:. ```. ... and (color is None or color in adata.obs.keys() or color in adata.var.index)):. File ""path/to/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 2035, in __contains__. hash(key). TypeError: unhashable type: 'list'. ```. - For components: the command was . ```. sc.pl.scatter(. adata=adata,. x='EKLF',. y='Cebpa',. color='EgrNab',. layers=('X', 'X', 'X'),. use_raw=False,. sort_order=True,. components='all',. projection='2d',. legend_loc='right margin',. legend_fontsize=1,. legend_fontweight='normal',. palette='viridis',. frameon=True,. right_margin=1.0,. size=1.0,. show=False,. save='.png'). ```. and the error:. ```. components = np.array(components).astype(int) - 1. ValueError: invalid literal for int() with base 10: 'all'. ```. Did I put the parameters in a wrong way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:925,performance,error,error,925,"Hi @fidelram, good to see you :smile:. I was working on the Galaxy integration. I tested that with the `1.3.2` version from Bioconda. I tested with adata from krumsiek11. - For colors, I tried with `sc.pl.scatter(adata=adata, x='EKLF', y='Cebpa', color=['EgrNab', 'cJun']) and I got the error:. ```. ... and (color is None or color in adata.obs.keys() or color in adata.var.index)):. File ""path/to/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 2035, in __contains__. hash(key). TypeError: unhashable type: 'list'. ```. - For components: the command was . ```. sc.pl.scatter(. adata=adata,. x='EKLF',. y='Cebpa',. color='EgrNab',. layers=('X', 'X', 'X'),. use_raw=False,. sort_order=True,. components='all',. projection='2d',. legend_loc='right margin',. legend_fontsize=1,. legend_fontweight='normal',. palette='viridis',. frameon=True,. right_margin=1.0,. size=1.0,. show=False,. save='.png'). ```. and the error:. ```. components = np.array(components).astype(int) - 1. ValueError: invalid literal for int() with base 10: 'all'. ```. Did I put the parameters in a wrong way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:67,reliability,integr,integration,67,"Hi @fidelram, good to see you :smile:. I was working on the Galaxy integration. I tested that with the `1.3.2` version from Bioconda. I tested with adata from krumsiek11. - For colors, I tried with `sc.pl.scatter(adata=adata, x='EKLF', y='Cebpa', color=['EgrNab', 'cJun']) and I got the error:. ```. ... and (color is None or color in adata.obs.keys() or color in adata.var.index)):. File ""path/to/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 2035, in __contains__. hash(key). TypeError: unhashable type: 'list'. ```. - For components: the command was . ```. sc.pl.scatter(. adata=adata,. x='EKLF',. y='Cebpa',. color='EgrNab',. layers=('X', 'X', 'X'),. use_raw=False,. sort_order=True,. components='all',. projection='2d',. legend_loc='right margin',. legend_fontsize=1,. legend_fontweight='normal',. palette='viridis',. frameon=True,. right_margin=1.0,. size=1.0,. show=False,. save='.png'). ```. and the error:. ```. components = np.array(components).astype(int) - 1. ValueError: invalid literal for int() with base 10: 'all'. ```. Did I put the parameters in a wrong way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:82,safety,test,tested,82,"Hi @fidelram, good to see you :smile:. I was working on the Galaxy integration. I tested that with the `1.3.2` version from Bioconda. I tested with adata from krumsiek11. - For colors, I tried with `sc.pl.scatter(adata=adata, x='EKLF', y='Cebpa', color=['EgrNab', 'cJun']) and I got the error:. ```. ... and (color is None or color in adata.obs.keys() or color in adata.var.index)):. File ""path/to/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 2035, in __contains__. hash(key). TypeError: unhashable type: 'list'. ```. - For components: the command was . ```. sc.pl.scatter(. adata=adata,. x='EKLF',. y='Cebpa',. color='EgrNab',. layers=('X', 'X', 'X'),. use_raw=False,. sort_order=True,. components='all',. projection='2d',. legend_loc='right margin',. legend_fontsize=1,. legend_fontweight='normal',. palette='viridis',. frameon=True,. right_margin=1.0,. size=1.0,. show=False,. save='.png'). ```. and the error:. ```. components = np.array(components).astype(int) - 1. ValueError: invalid literal for int() with base 10: 'all'. ```. Did I put the parameters in a wrong way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:136,safety,test,tested,136,"Hi @fidelram, good to see you :smile:. I was working on the Galaxy integration. I tested that with the `1.3.2` version from Bioconda. I tested with adata from krumsiek11. - For colors, I tried with `sc.pl.scatter(adata=adata, x='EKLF', y='Cebpa', color=['EgrNab', 'cJun']) and I got the error:. ```. ... and (color is None or color in adata.obs.keys() or color in adata.var.index)):. File ""path/to/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 2035, in __contains__. hash(key). TypeError: unhashable type: 'list'. ```. - For components: the command was . ```. sc.pl.scatter(. adata=adata,. x='EKLF',. y='Cebpa',. color='EgrNab',. layers=('X', 'X', 'X'),. use_raw=False,. sort_order=True,. components='all',. projection='2d',. legend_loc='right margin',. legend_fontsize=1,. legend_fontweight='normal',. palette='viridis',. frameon=True,. right_margin=1.0,. size=1.0,. show=False,. save='.png'). ```. and the error:. ```. components = np.array(components).astype(int) - 1. ValueError: invalid literal for int() with base 10: 'all'. ```. Did I put the parameters in a wrong way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:287,safety,error,error,287,"Hi @fidelram, good to see you :smile:. I was working on the Galaxy integration. I tested that with the `1.3.2` version from Bioconda. I tested with adata from krumsiek11. - For colors, I tried with `sc.pl.scatter(adata=adata, x='EKLF', y='Cebpa', color=['EgrNab', 'cJun']) and I got the error:. ```. ... and (color is None or color in adata.obs.keys() or color in adata.var.index)):. File ""path/to/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 2035, in __contains__. hash(key). TypeError: unhashable type: 'list'. ```. - For components: the command was . ```. sc.pl.scatter(. adata=adata,. x='EKLF',. y='Cebpa',. color='EgrNab',. layers=('X', 'X', 'X'),. use_raw=False,. sort_order=True,. components='all',. projection='2d',. legend_loc='right margin',. legend_fontsize=1,. legend_fontweight='normal',. palette='viridis',. frameon=True,. right_margin=1.0,. size=1.0,. show=False,. save='.png'). ```. and the error:. ```. components = np.array(components).astype(int) - 1. ValueError: invalid literal for int() with base 10: 'all'. ```. Did I put the parameters in a wrong way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:925,safety,error,error,925,"Hi @fidelram, good to see you :smile:. I was working on the Galaxy integration. I tested that with the `1.3.2` version from Bioconda. I tested with adata from krumsiek11. - For colors, I tried with `sc.pl.scatter(adata=adata, x='EKLF', y='Cebpa', color=['EgrNab', 'cJun']) and I got the error:. ```. ... and (color is None or color in adata.obs.keys() or color in adata.var.index)):. File ""path/to/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 2035, in __contains__. hash(key). TypeError: unhashable type: 'list'. ```. - For components: the command was . ```. sc.pl.scatter(. adata=adata,. x='EKLF',. y='Cebpa',. color='EgrNab',. layers=('X', 'X', 'X'),. use_raw=False,. sort_order=True,. components='all',. projection='2d',. legend_loc='right margin',. legend_fontsize=1,. legend_fontweight='normal',. palette='viridis',. frameon=True,. right_margin=1.0,. size=1.0,. show=False,. save='.png'). ```. and the error:. ```. components = np.array(components).astype(int) - 1. ValueError: invalid literal for int() with base 10: 'all'. ```. Did I put the parameters in a wrong way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:67,security,integr,integration,67,"Hi @fidelram, good to see you :smile:. I was working on the Galaxy integration. I tested that with the `1.3.2` version from Bioconda. I tested with adata from krumsiek11. - For colors, I tried with `sc.pl.scatter(adata=adata, x='EKLF', y='Cebpa', color=['EgrNab', 'cJun']) and I got the error:. ```. ... and (color is None or color in adata.obs.keys() or color in adata.var.index)):. File ""path/to/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 2035, in __contains__. hash(key). TypeError: unhashable type: 'list'. ```. - For components: the command was . ```. sc.pl.scatter(. adata=adata,. x='EKLF',. y='Cebpa',. color='EgrNab',. layers=('X', 'X', 'X'),. use_raw=False,. sort_order=True,. components='all',. projection='2d',. legend_loc='right margin',. legend_fontsize=1,. legend_fontweight='normal',. palette='viridis',. frameon=True,. right_margin=1.0,. size=1.0,. show=False,. save='.png'). ```. and the error:. ```. components = np.array(components).astype(int) - 1. ValueError: invalid literal for int() with base 10: 'all'. ```. Did I put the parameters in a wrong way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:484,security,hash,hash,484,"Hi @fidelram, good to see you :smile:. I was working on the Galaxy integration. I tested that with the `1.3.2` version from Bioconda. I tested with adata from krumsiek11. - For colors, I tried with `sc.pl.scatter(adata=adata, x='EKLF', y='Cebpa', color=['EgrNab', 'cJun']) and I got the error:. ```. ... and (color is None or color in adata.obs.keys() or color in adata.var.index)):. File ""path/to/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 2035, in __contains__. hash(key). TypeError: unhashable type: 'list'. ```. - For components: the command was . ```. sc.pl.scatter(. adata=adata,. x='EKLF',. y='Cebpa',. color='EgrNab',. layers=('X', 'X', 'X'),. use_raw=False,. sort_order=True,. components='all',. projection='2d',. legend_loc='right margin',. legend_fontsize=1,. legend_fontweight='normal',. palette='viridis',. frameon=True,. right_margin=1.0,. size=1.0,. show=False,. save='.png'). ```. and the error:. ```. components = np.array(components).astype(int) - 1. ValueError: invalid literal for int() with base 10: 'all'. ```. Did I put the parameters in a wrong way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:67,testability,integr,integration,67,"Hi @fidelram, good to see you :smile:. I was working on the Galaxy integration. I tested that with the `1.3.2` version from Bioconda. I tested with adata from krumsiek11. - For colors, I tried with `sc.pl.scatter(adata=adata, x='EKLF', y='Cebpa', color=['EgrNab', 'cJun']) and I got the error:. ```. ... and (color is None or color in adata.obs.keys() or color in adata.var.index)):. File ""path/to/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 2035, in __contains__. hash(key). TypeError: unhashable type: 'list'. ```. - For components: the command was . ```. sc.pl.scatter(. adata=adata,. x='EKLF',. y='Cebpa',. color='EgrNab',. layers=('X', 'X', 'X'),. use_raw=False,. sort_order=True,. components='all',. projection='2d',. legend_loc='right margin',. legend_fontsize=1,. legend_fontweight='normal',. palette='viridis',. frameon=True,. right_margin=1.0,. size=1.0,. show=False,. save='.png'). ```. and the error:. ```. components = np.array(components).astype(int) - 1. ValueError: invalid literal for int() with base 10: 'all'. ```. Did I put the parameters in a wrong way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:82,testability,test,tested,82,"Hi @fidelram, good to see you :smile:. I was working on the Galaxy integration. I tested that with the `1.3.2` version from Bioconda. I tested with adata from krumsiek11. - For colors, I tried with `sc.pl.scatter(adata=adata, x='EKLF', y='Cebpa', color=['EgrNab', 'cJun']) and I got the error:. ```. ... and (color is None or color in adata.obs.keys() or color in adata.var.index)):. File ""path/to/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 2035, in __contains__. hash(key). TypeError: unhashable type: 'list'. ```. - For components: the command was . ```. sc.pl.scatter(. adata=adata,. x='EKLF',. y='Cebpa',. color='EgrNab',. layers=('X', 'X', 'X'),. use_raw=False,. sort_order=True,. components='all',. projection='2d',. legend_loc='right margin',. legend_fontsize=1,. legend_fontweight='normal',. palette='viridis',. frameon=True,. right_margin=1.0,. size=1.0,. show=False,. save='.png'). ```. and the error:. ```. components = np.array(components).astype(int) - 1. ValueError: invalid literal for int() with base 10: 'all'. ```. Did I put the parameters in a wrong way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:136,testability,test,tested,136,"Hi @fidelram, good to see you :smile:. I was working on the Galaxy integration. I tested that with the `1.3.2` version from Bioconda. I tested with adata from krumsiek11. - For colors, I tried with `sc.pl.scatter(adata=adata, x='EKLF', y='Cebpa', color=['EgrNab', 'cJun']) and I got the error:. ```. ... and (color is None or color in adata.obs.keys() or color in adata.var.index)):. File ""path/to/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 2035, in __contains__. hash(key). TypeError: unhashable type: 'list'. ```. - For components: the command was . ```. sc.pl.scatter(. adata=adata,. x='EKLF',. y='Cebpa',. color='EgrNab',. layers=('X', 'X', 'X'),. use_raw=False,. sort_order=True,. components='all',. projection='2d',. legend_loc='right margin',. legend_fontsize=1,. legend_fontweight='normal',. palette='viridis',. frameon=True,. right_margin=1.0,. size=1.0,. show=False,. save='.png'). ```. and the error:. ```. components = np.array(components).astype(int) - 1. ValueError: invalid literal for int() with base 10: 'all'. ```. Did I put the parameters in a wrong way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:287,usability,error,error,287,"Hi @fidelram, good to see you :smile:. I was working on the Galaxy integration. I tested that with the `1.3.2` version from Bioconda. I tested with adata from krumsiek11. - For colors, I tried with `sc.pl.scatter(adata=adata, x='EKLF', y='Cebpa', color=['EgrNab', 'cJun']) and I got the error:. ```. ... and (color is None or color in adata.obs.keys() or color in adata.var.index)):. File ""path/to/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 2035, in __contains__. hash(key). TypeError: unhashable type: 'list'. ```. - For components: the command was . ```. sc.pl.scatter(. adata=adata,. x='EKLF',. y='Cebpa',. color='EgrNab',. layers=('X', 'X', 'X'),. use_raw=False,. sort_order=True,. components='all',. projection='2d',. legend_loc='right margin',. legend_fontsize=1,. legend_fontweight='normal',. palette='viridis',. frameon=True,. right_margin=1.0,. size=1.0,. show=False,. save='.png'). ```. and the error:. ```. components = np.array(components).astype(int) - 1. ValueError: invalid literal for int() with base 10: 'all'. ```. Did I put the parameters in a wrong way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:558,usability,command,command,558,"Hi @fidelram, good to see you :smile:. I was working on the Galaxy integration. I tested that with the `1.3.2` version from Bioconda. I tested with adata from krumsiek11. - For colors, I tried with `sc.pl.scatter(adata=adata, x='EKLF', y='Cebpa', color=['EgrNab', 'cJun']) and I got the error:. ```. ... and (color is None or color in adata.obs.keys() or color in adata.var.index)):. File ""path/to/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 2035, in __contains__. hash(key). TypeError: unhashable type: 'list'. ```. - For components: the command was . ```. sc.pl.scatter(. adata=adata,. x='EKLF',. y='Cebpa',. color='EgrNab',. layers=('X', 'X', 'X'),. use_raw=False,. sort_order=True,. components='all',. projection='2d',. legend_loc='right margin',. legend_fontsize=1,. legend_fontweight='normal',. palette='viridis',. frameon=True,. right_margin=1.0,. size=1.0,. show=False,. save='.png'). ```. and the error:. ```. components = np.array(components).astype(int) - 1. ValueError: invalid literal for int() with base 10: 'all'. ```. Did I put the parameters in a wrong way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:925,usability,error,error,925,"Hi @fidelram, good to see you :smile:. I was working on the Galaxy integration. I tested that with the `1.3.2` version from Bioconda. I tested with adata from krumsiek11. - For colors, I tried with `sc.pl.scatter(adata=adata, x='EKLF', y='Cebpa', color=['EgrNab', 'cJun']) and I got the error:. ```. ... and (color is None or color in adata.obs.keys() or color in adata.var.index)):. File ""path/to/lib/python3.6/site-packages/pandas/core/indexes/base.py"", line 2035, in __contains__. hash(key). TypeError: unhashable type: 'list'. ```. - For components: the command was . ```. sc.pl.scatter(. adata=adata,. x='EKLF',. y='Cebpa',. color='EgrNab',. layers=('X', 'X', 'X'),. use_raw=False,. sort_order=True,. components='all',. projection='2d',. legend_loc='right margin',. legend_fontsize=1,. legend_fontweight='normal',. palette='viridis',. frameon=True,. right_margin=1.0,. size=1.0,. show=False,. save='.png'). ```. and the error:. ```. components = np.array(components).astype(int) - 1. ValueError: invalid literal for int() with base 10: 'all'. ```. Did I put the parameters in a wrong way?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:19,deployability,contain,contain,19,"Yes, `1.3.2` might contain still a few bugs on the scatter side (I should have made this a prerelease); I wanted to release `1.3.3` quickly so that the fixes are there but I think there still remain small issues. There will be more bug-free release soon. You can just go back to `1.3.1`, which is working fine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:116,deployability,releas,release,116,"Yes, `1.3.2` might contain still a few bugs on the scatter side (I should have made this a prerelease); I wanted to release `1.3.3` quickly so that the fixes are there but I think there still remain small issues. There will be more bug-free release soon. You can just go back to `1.3.1`, which is working fine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:241,deployability,releas,release,241,"Yes, `1.3.2` might contain still a few bugs on the scatter side (I should have made this a prerelease); I wanted to release `1.3.3` quickly so that the fixes are there but I think there still remain small issues. There will be more bug-free release soon. You can just go back to `1.3.1`, which is working fine.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:35,availability,error,errors,35,@falexwolf Do you think that these errors come from the code that I modified? I think I didn't touch the function that @bebatut is using.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:35,performance,error,errors,35,@falexwolf Do you think that these errors come from the code that I modified? I think I didn't touch the function that @bebatut is using.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:35,safety,error,errors,35,@falexwolf Do you think that these errors come from the code that I modified? I think I didn't touch the function that @bebatut is using.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:68,security,modif,modified,68,@falexwolf Do you think that these errors come from the code that I modified? I think I didn't touch the function that @bebatut is using.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:35,usability,error,errors,35,@falexwolf Do you think that these errors come from the code that I modified? I think I didn't touch the function that @bebatut is using.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:385,deployability,continu,continue,385,"Ah right, we now have to separate scatter functions, which isn't a good situation. @bebatut `components` does only make sense if you provide `basis` as an argument. a list-like `color` was also only meant for that case. both is now deprecated as @fidelram wrote a whole new scatter backend; however, which for now, misses the `x` and `y` parameters... In any case, `pl.scatter` should continue to work with the canonical calls and also with non-list-like color.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:93,integrability,compon,components,93,"Ah right, we now have to separate scatter functions, which isn't a good situation. @bebatut `components` does only make sense if you provide `basis` as an argument. a list-like `color` was also only meant for that case. both is now deprecated as @fidelram wrote a whole new scatter backend; however, which for now, misses the `x` and `y` parameters... In any case, `pl.scatter` should continue to work with the canonical calls and also with non-list-like color.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:93,interoperability,compon,components,93,"Ah right, we now have to separate scatter functions, which isn't a good situation. @bebatut `components` does only make sense if you provide `basis` as an argument. a list-like `color` was also only meant for that case. both is now deprecated as @fidelram wrote a whole new scatter backend; however, which for now, misses the `x` and `y` parameters... In any case, `pl.scatter` should continue to work with the canonical calls and also with non-list-like color.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:93,modifiability,compon,components,93,"Ah right, we now have to separate scatter functions, which isn't a good situation. @bebatut `components` does only make sense if you provide `basis` as an argument. a list-like `color` was also only meant for that case. both is now deprecated as @fidelram wrote a whole new scatter backend; however, which for now, misses the `x` and `y` parameters... In any case, `pl.scatter` should continue to work with the canonical calls and also with non-list-like color.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:338,modifiability,paramet,parameters,338,"Ah right, we now have to separate scatter functions, which isn't a good situation. @bebatut `components` does only make sense if you provide `basis` as an argument. a list-like `color` was also only meant for that case. both is now deprecated as @fidelram wrote a whole new scatter backend; however, which for now, misses the `x` and `y` parameters... In any case, `pl.scatter` should continue to work with the canonical calls and also with non-list-like color.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:105,reliability,doe,does,105,"Ah right, we now have to separate scatter functions, which isn't a good situation. @bebatut `components` does only make sense if you provide `basis` as an argument. a list-like `color` was also only meant for that case. both is now deprecated as @fidelram wrote a whole new scatter backend; however, which for now, misses the `x` and `y` parameters... In any case, `pl.scatter` should continue to work with the canonical calls and also with non-list-like color.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:116,deployability,version,version,116,"I think that the best is to have separate scatter functions. For the. remaining use cases of scatter a much simpler version can be devised. But. meanwhile, @bebatut <https://github.com/bebatut> why did you try that. combination of parameters? Is this in some tutorial? On Tue, Oct 23, 2018 at 7:15 PM Alex Wolf <notifications@github.com> wrote:. > Ah right, we now have to separate scatter functions, which isn't a good. > situation. @bebatut <https://github.com/bebatut> components does only. > make sense if you provide basis as an argument. a list-like color was. > also only meant for that case. both is now deprecated as @fidelram. > <https://github.com/fidelram> wrote a whole new scatter backend; however,. > which for now, misses the x and y parameters... >. > In any case, pl.scatter should continue to work with the canonical calls. > and also with non-list-like color. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/311#issuecomment-432336990>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1TxK09r7RbbmfArW1Pt-UBFWhFQzks5un07HgaJpZM4XtV0c>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:800,deployability,continu,continue,800,"I think that the best is to have separate scatter functions. For the. remaining use cases of scatter a much simpler version can be devised. But. meanwhile, @bebatut <https://github.com/bebatut> why did you try that. combination of parameters? Is this in some tutorial? On Tue, Oct 23, 2018 at 7:15 PM Alex Wolf <notifications@github.com> wrote:. > Ah right, we now have to separate scatter functions, which isn't a good. > situation. @bebatut <https://github.com/bebatut> components does only. > make sense if you provide basis as an argument. a list-like color was. > also only meant for that case. both is now deprecated as @fidelram. > <https://github.com/fidelram> wrote a whole new scatter backend; however,. > which for now, misses the x and y parameters... >. > In any case, pl.scatter should continue to work with the canonical calls. > and also with non-list-like color. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/311#issuecomment-432336990>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1TxK09r7RbbmfArW1Pt-UBFWhFQzks5un07HgaJpZM4XtV0c>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:116,integrability,version,version,116,"I think that the best is to have separate scatter functions. For the. remaining use cases of scatter a much simpler version can be devised. But. meanwhile, @bebatut <https://github.com/bebatut> why did you try that. combination of parameters? Is this in some tutorial? On Tue, Oct 23, 2018 at 7:15 PM Alex Wolf <notifications@github.com> wrote:. > Ah right, we now have to separate scatter functions, which isn't a good. > situation. @bebatut <https://github.com/bebatut> components does only. > make sense if you provide basis as an argument. a list-like color was. > also only meant for that case. both is now deprecated as @fidelram. > <https://github.com/fidelram> wrote a whole new scatter backend; however,. > which for now, misses the x and y parameters... >. > In any case, pl.scatter should continue to work with the canonical calls. > and also with non-list-like color. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/311#issuecomment-432336990>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1TxK09r7RbbmfArW1Pt-UBFWhFQzks5un07HgaJpZM4XtV0c>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:472,integrability,compon,components,472,"I think that the best is to have separate scatter functions. For the. remaining use cases of scatter a much simpler version can be devised. But. meanwhile, @bebatut <https://github.com/bebatut> why did you try that. combination of parameters? Is this in some tutorial? On Tue, Oct 23, 2018 at 7:15 PM Alex Wolf <notifications@github.com> wrote:. > Ah right, we now have to separate scatter functions, which isn't a good. > situation. @bebatut <https://github.com/bebatut> components does only. > make sense if you provide basis as an argument. a list-like color was. > also only meant for that case. both is now deprecated as @fidelram. > <https://github.com/fidelram> wrote a whole new scatter backend; however,. > which for now, misses the x and y parameters... >. > In any case, pl.scatter should continue to work with the canonical calls. > and also with non-list-like color. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/311#issuecomment-432336990>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1TxK09r7RbbmfArW1Pt-UBFWhFQzks5un07HgaJpZM4XtV0c>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:472,interoperability,compon,components,472,"I think that the best is to have separate scatter functions. For the. remaining use cases of scatter a much simpler version can be devised. But. meanwhile, @bebatut <https://github.com/bebatut> why did you try that. combination of parameters? Is this in some tutorial? On Tue, Oct 23, 2018 at 7:15 PM Alex Wolf <notifications@github.com> wrote:. > Ah right, we now have to separate scatter functions, which isn't a good. > situation. @bebatut <https://github.com/bebatut> components does only. > make sense if you provide basis as an argument. a list-like color was. > also only meant for that case. both is now deprecated as @fidelram. > <https://github.com/fidelram> wrote a whole new scatter backend; however,. > which for now, misses the x and y parameters... >. > In any case, pl.scatter should continue to work with the canonical calls. > and also with non-list-like color. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/311#issuecomment-432336990>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1TxK09r7RbbmfArW1Pt-UBFWhFQzks5un07HgaJpZM4XtV0c>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:116,modifiability,version,version,116,"I think that the best is to have separate scatter functions. For the. remaining use cases of scatter a much simpler version can be devised. But. meanwhile, @bebatut <https://github.com/bebatut> why did you try that. combination of parameters? Is this in some tutorial? On Tue, Oct 23, 2018 at 7:15 PM Alex Wolf <notifications@github.com> wrote:. > Ah right, we now have to separate scatter functions, which isn't a good. > situation. @bebatut <https://github.com/bebatut> components does only. > make sense if you provide basis as an argument. a list-like color was. > also only meant for that case. both is now deprecated as @fidelram. > <https://github.com/fidelram> wrote a whole new scatter backend; however,. > which for now, misses the x and y parameters... >. > In any case, pl.scatter should continue to work with the canonical calls. > and also with non-list-like color. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/311#issuecomment-432336990>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1TxK09r7RbbmfArW1Pt-UBFWhFQzks5un07HgaJpZM4XtV0c>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:231,modifiability,paramet,parameters,231,"I think that the best is to have separate scatter functions. For the. remaining use cases of scatter a much simpler version can be devised. But. meanwhile, @bebatut <https://github.com/bebatut> why did you try that. combination of parameters? Is this in some tutorial? On Tue, Oct 23, 2018 at 7:15 PM Alex Wolf <notifications@github.com> wrote:. > Ah right, we now have to separate scatter functions, which isn't a good. > situation. @bebatut <https://github.com/bebatut> components does only. > make sense if you provide basis as an argument. a list-like color was. > also only meant for that case. both is now deprecated as @fidelram. > <https://github.com/fidelram> wrote a whole new scatter backend; however,. > which for now, misses the x and y parameters... >. > In any case, pl.scatter should continue to work with the canonical calls. > and also with non-list-like color. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/311#issuecomment-432336990>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1TxK09r7RbbmfArW1Pt-UBFWhFQzks5un07HgaJpZM4XtV0c>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:472,modifiability,compon,components,472,"I think that the best is to have separate scatter functions. For the. remaining use cases of scatter a much simpler version can be devised. But. meanwhile, @bebatut <https://github.com/bebatut> why did you try that. combination of parameters? Is this in some tutorial? On Tue, Oct 23, 2018 at 7:15 PM Alex Wolf <notifications@github.com> wrote:. > Ah right, we now have to separate scatter functions, which isn't a good. > situation. @bebatut <https://github.com/bebatut> components does only. > make sense if you provide basis as an argument. a list-like color was. > also only meant for that case. both is now deprecated as @fidelram. > <https://github.com/fidelram> wrote a whole new scatter backend; however,. > which for now, misses the x and y parameters... >. > In any case, pl.scatter should continue to work with the canonical calls. > and also with non-list-like color. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/311#issuecomment-432336990>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1TxK09r7RbbmfArW1Pt-UBFWhFQzks5un07HgaJpZM4XtV0c>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:750,modifiability,paramet,parameters,750,"I think that the best is to have separate scatter functions. For the. remaining use cases of scatter a much simpler version can be devised. But. meanwhile, @bebatut <https://github.com/bebatut> why did you try that. combination of parameters? Is this in some tutorial? On Tue, Oct 23, 2018 at 7:15 PM Alex Wolf <notifications@github.com> wrote:. > Ah right, we now have to separate scatter functions, which isn't a good. > situation. @bebatut <https://github.com/bebatut> components does only. > make sense if you provide basis as an argument. a list-like color was. > also only meant for that case. both is now deprecated as @fidelram. > <https://github.com/fidelram> wrote a whole new scatter backend; however,. > which for now, misses the x and y parameters... >. > In any case, pl.scatter should continue to work with the canonical calls. > and also with non-list-like color. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/311#issuecomment-432336990>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1TxK09r7RbbmfArW1Pt-UBFWhFQzks5un07HgaJpZM4XtV0c>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:483,reliability,doe,does,483,"I think that the best is to have separate scatter functions. For the. remaining use cases of scatter a much simpler version can be devised. But. meanwhile, @bebatut <https://github.com/bebatut> why did you try that. combination of parameters? Is this in some tutorial? On Tue, Oct 23, 2018 at 7:15 PM Alex Wolf <notifications@github.com> wrote:. > Ah right, we now have to separate scatter functions, which isn't a good. > situation. @bebatut <https://github.com/bebatut> components does only. > make sense if you provide basis as an argument. a list-like color was. > also only meant for that case. both is now deprecated as @fidelram. > <https://github.com/fidelram> wrote a whole new scatter backend; however,. > which for now, misses the x and y parameters... >. > In any case, pl.scatter should continue to work with the canonical calls. > and also with non-list-like color. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/311#issuecomment-432336990>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1TxK09r7RbbmfArW1Pt-UBFWhFQzks5un07HgaJpZM4XtV0c>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:1137,security,auth,auth,1137,"I think that the best is to have separate scatter functions. For the. remaining use cases of scatter a much simpler version can be devised. But. meanwhile, @bebatut <https://github.com/bebatut> why did you try that. combination of parameters? Is this in some tutorial? On Tue, Oct 23, 2018 at 7:15 PM Alex Wolf <notifications@github.com> wrote:. > Ah right, we now have to separate scatter functions, which isn't a good. > situation. @bebatut <https://github.com/bebatut> components does only. > make sense if you provide basis as an argument. a list-like color was. > also only meant for that case. both is now deprecated as @fidelram. > <https://github.com/fidelram> wrote a whole new scatter backend; however,. > which for now, misses the x and y parameters... >. > In any case, pl.scatter should continue to work with the canonical calls. > and also with non-list-like color. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/311#issuecomment-432336990>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1TxK09r7RbbmfArW1Pt-UBFWhFQzks5un07HgaJpZM4XtV0c>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:108,testability,simpl,simpler,108,"I think that the best is to have separate scatter functions. For the. remaining use cases of scatter a much simpler version can be devised. But. meanwhile, @bebatut <https://github.com/bebatut> why did you try that. combination of parameters? Is this in some tutorial? On Tue, Oct 23, 2018 at 7:15 PM Alex Wolf <notifications@github.com> wrote:. > Ah right, we now have to separate scatter functions, which isn't a good. > situation. @bebatut <https://github.com/bebatut> components does only. > make sense if you provide basis as an argument. a list-like color was. > also only meant for that case. both is now deprecated as @fidelram. > <https://github.com/fidelram> wrote a whole new scatter backend; however,. > which for now, misses the x and y parameters... >. > In any case, pl.scatter should continue to work with the canonical calls. > and also with non-list-like color. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/311#issuecomment-432336990>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1TxK09r7RbbmfArW1Pt-UBFWhFQzks5un07HgaJpZM4XtV0c>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:108,usability,simpl,simpler,108,"I think that the best is to have separate scatter functions. For the. remaining use cases of scatter a much simpler version can be devised. But. meanwhile, @bebatut <https://github.com/bebatut> why did you try that. combination of parameters? Is this in some tutorial? On Tue, Oct 23, 2018 at 7:15 PM Alex Wolf <notifications@github.com> wrote:. > Ah right, we now have to separate scatter functions, which isn't a good. > situation. @bebatut <https://github.com/bebatut> components does only. > make sense if you provide basis as an argument. a list-like color was. > also only meant for that case. both is now deprecated as @fidelram. > <https://github.com/fidelram> wrote a whole new scatter backend; however,. > which for now, misses the x and y parameters... >. > In any case, pl.scatter should continue to work with the canonical calls. > and also with non-list-like color. >. > —. > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub. > <https://github.com/theislab/scanpy/issues/311#issuecomment-432336990>,. > or mute the thread. > <https://github.com/notifications/unsubscribe-auth/AEu_1TxK09r7RbbmfArW1Pt-UBFWhFQzks5un07HgaJpZM4XtV0c>. > . >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:30,energy efficiency,reduc,reduce,30,"Yes, you are right! We should reduce the complexity of the current `sc.pl.scatter` now that it's no longer used for the embeddings...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:59,energy efficiency,current,current,59,"Yes, you are right! We should reduce the complexity of the current `sc.pl.scatter` now that it's no longer used for the embeddings...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:41,safety,compl,complexity,41,"Yes, you are right! We should reduce the complexity of the current `sc.pl.scatter` now that it's no longer used for the embeddings...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:41,security,compl,complexity,41,"Yes, you are right! We should reduce the complexity of the current `sc.pl.scatter` now that it's no longer used for the embeddings...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:13,deployability,updat,update,13,"Hi all,. any update on this? I'm on version 1.4 and even if in the documentation the color parameter is defined as string or list of strings, I'm still unable to pass to the scatter method a list of strings as value. Thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:36,deployability,version,version,36,"Hi all,. any update on this? I'm on version 1.4 and even if in the documentation the color parameter is defined as string or list of strings, I'm still unable to pass to the scatter method a list of strings as value. Thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:36,integrability,version,version,36,"Hi all,. any update on this? I'm on version 1.4 and even if in the documentation the color parameter is defined as string or list of strings, I'm still unable to pass to the scatter method a list of strings as value. Thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:36,modifiability,version,version,36,"Hi all,. any update on this? I'm on version 1.4 and even if in the documentation the color parameter is defined as string or list of strings, I'm still unable to pass to the scatter method a list of strings as value. Thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:91,modifiability,paramet,parameter,91,"Hi all,. any update on this? I'm on version 1.4 and even if in the documentation the color parameter is defined as string or list of strings, I'm still unable to pass to the scatter method a list of strings as value. Thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:13,safety,updat,update,13,"Hi all,. any update on this? I'm on version 1.4 and even if in the documentation the color parameter is defined as string or list of strings, I'm still unable to pass to the scatter method a list of strings as value. Thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:13,security,updat,update,13,"Hi all,. any update on this? I'm on version 1.4 and even if in the documentation the color parameter is defined as string or list of strings, I'm still unable to pass to the scatter method a list of strings as value. Thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:67,usability,document,documentation,67,"Hi all,. any update on this? I'm on version 1.4 and even if in the documentation the color parameter is defined as string or list of strings, I'm still unable to pass to the scatter method a list of strings as value. Thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:17,usability,help,help,17,@fbrundu can you help me by posting an example hopefully using the datasets included in scanpy?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:305,integrability,sub,subplot,305,"Hi @fidelram,. One way in which I'd like to do it is like the following:. ```python. sc.pl.scatter(adata, x='<gene1>', y='<gene2>', color=['Mki67', 'Pclaf'],. save=False, use_raw=False). ```. to show the relationship between two genes (i.e. gene1 and gene2), and one third gene (in this case Mki67 in one subplot, Pclaf in the second). One of the subplots could be like the following:. ![test](https://user-images.githubusercontent.com/697622/52814026-1e538480-3069-11e9-9af5-ef7a4761ff25.png). Hope this is clear. Thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:347,integrability,sub,subplots,347,"Hi @fidelram,. One way in which I'd like to do it is like the following:. ```python. sc.pl.scatter(adata, x='<gene1>', y='<gene2>', color=['Mki67', 'Pclaf'],. save=False, use_raw=False). ```. to show the relationship between two genes (i.e. gene1 and gene2), and one third gene (in this case Mki67 in one subplot, Pclaf in the second). One of the subplots could be like the following:. ![test](https://user-images.githubusercontent.com/697622/52814026-1e538480-3069-11e9-9af5-ef7a4761ff25.png). Hope this is clear. Thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:388,safety,test,test,388,"Hi @fidelram,. One way in which I'd like to do it is like the following:. ```python. sc.pl.scatter(adata, x='<gene1>', y='<gene2>', color=['Mki67', 'Pclaf'],. save=False, use_raw=False). ```. to show the relationship between two genes (i.e. gene1 and gene2), and one third gene (in this case Mki67 in one subplot, Pclaf in the second). One of the subplots could be like the following:. ![test](https://user-images.githubusercontent.com/697622/52814026-1e538480-3069-11e9-9af5-ef7a4761ff25.png). Hope this is clear. Thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:388,testability,test,test,388,"Hi @fidelram,. One way in which I'd like to do it is like the following:. ```python. sc.pl.scatter(adata, x='<gene1>', y='<gene2>', color=['Mki67', 'Pclaf'],. save=False, use_raw=False). ```. to show the relationship between two genes (i.e. gene1 and gene2), and one third gene (in this case Mki67 in one subplot, Pclaf in the second). One of the subplots could be like the following:. ![test](https://user-images.githubusercontent.com/697622/52814026-1e538480-3069-11e9-9af5-ef7a4761ff25.png). Hope this is clear. Thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:402,usability,user,user-images,402,"Hi @fidelram,. One way in which I'd like to do it is like the following:. ```python. sc.pl.scatter(adata, x='<gene1>', y='<gene2>', color=['Mki67', 'Pclaf'],. save=False, use_raw=False). ```. to show the relationship between two genes (i.e. gene1 and gene2), and one third gene (in this case Mki67 in one subplot, Pclaf in the second). One of the subplots could be like the following:. ![test](https://user-images.githubusercontent.com/697622/52814026-1e538480-3069-11e9-9af5-ef7a4761ff25.png). Hope this is clear. Thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/311:508,usability,clear,clear,508,"Hi @fidelram,. One way in which I'd like to do it is like the following:. ```python. sc.pl.scatter(adata, x='<gene1>', y='<gene2>', color=['Mki67', 'Pclaf'],. save=False, use_raw=False). ```. to show the relationship between two genes (i.e. gene1 and gene2), and one third gene (in this case Mki67 in one subplot, Pclaf in the second). One of the subplots could be like the following:. ![test](https://user-images.githubusercontent.com/697622/52814026-1e538480-3069-11e9-9af5-ef7a4761ff25.png). Hope this is clear. Thanks",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/311
https://github.com/scverse/scanpy/issues/313:349,availability,down,down,349,"Hm, but it looks like https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.score_genes.html should be reproducible with the default settings. In what you describe, only pca, louvain and umap have stochastic elements all of which set a default seed in the function call. I have never observed issues with reproducibility there. Can we narrow it down to score_genes? Or do you think it's somewhere else?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313
https://github.com/scverse/scanpy/issues/313:62,deployability,api,api,62,"Hm, but it looks like https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.score_genes.html should be reproducible with the default settings. In what you describe, only pca, louvain and umap have stochastic elements all of which set a default seed in the function call. I have never observed issues with reproducibility there. Can we narrow it down to score_genes? Or do you think it's somewhere else?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313
https://github.com/scverse/scanpy/issues/313:73,deployability,api,api,73,"Hm, but it looks like https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.score_genes.html should be reproducible with the default settings. In what you describe, only pca, louvain and umap have stochastic elements all of which set a default seed in the function call. I have never observed issues with reproducibility there. Can we narrow it down to score_genes? Or do you think it's somewhere else?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313
https://github.com/scverse/scanpy/issues/313:288,deployability,observ,observed,288,"Hm, but it looks like https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.score_genes.html should be reproducible with the default settings. In what you describe, only pca, louvain and umap have stochastic elements all of which set a default seed in the function call. I have never observed issues with reproducibility there. Can we narrow it down to score_genes? Or do you think it's somewhere else?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313
https://github.com/scverse/scanpy/issues/313:62,integrability,api,api,62,"Hm, but it looks like https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.score_genes.html should be reproducible with the default settings. In what you describe, only pca, louvain and umap have stochastic elements all of which set a default seed in the function call. I have never observed issues with reproducibility there. Can we narrow it down to score_genes? Or do you think it's somewhere else?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313
https://github.com/scverse/scanpy/issues/313:73,integrability,api,api,73,"Hm, but it looks like https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.score_genes.html should be reproducible with the default settings. In what you describe, only pca, louvain and umap have stochastic elements all of which set a default seed in the function call. I have never observed issues with reproducibility there. Can we narrow it down to score_genes? Or do you think it's somewhere else?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313
https://github.com/scverse/scanpy/issues/313:62,interoperability,api,api,62,"Hm, but it looks like https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.score_genes.html should be reproducible with the default settings. In what you describe, only pca, louvain and umap have stochastic elements all of which set a default seed in the function call. I have never observed issues with reproducibility there. Can we narrow it down to score_genes? Or do you think it's somewhere else?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313
https://github.com/scverse/scanpy/issues/313:73,interoperability,api,api,73,"Hm, but it looks like https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.score_genes.html should be reproducible with the default settings. In what you describe, only pca, louvain and umap have stochastic elements all of which set a default seed in the function call. I have never observed issues with reproducibility there. Can we narrow it down to score_genes? Or do you think it's somewhere else?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313
https://github.com/scverse/scanpy/issues/313:288,testability,observ,observed,288,"Hm, but it looks like https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.score_genes.html should be reproducible with the default settings. In what you describe, only pca, louvain and umap have stochastic elements all of which set a default seed in the function call. I have never observed issues with reproducibility there. Can we narrow it down to score_genes? Or do you think it's somewhere else?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313
https://github.com/scverse/scanpy/issues/313:20,usability,close,closed,20,I think this can be closed as @coh-racng found a solution.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313
https://github.com/scverse/scanpy/issues/313:27,usability,tool,tool,27,"Yes, but I think that each tool should have the possibility of setting a seed without needing to set it externally. So, I'd like to know why @coh-racng had to do this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313
https://github.com/scverse/scanpy/issues/313:104,availability,state,state,104,"Thanks for the help. I have traced the randomness to sc.tl.score_genes_cell_cycle. Even if I explicitly state the random state to be 0, two repeats of score_genes_cell_cycle from the same data would give different downstream clustering. On the other hand, if I use one score_genes_cell_cycle output and repeat the downstream clustering methods, the clusters are the same. . I am currently using scanpy==1.3.1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313
https://github.com/scverse/scanpy/issues/313:121,availability,state,state,121,"Thanks for the help. I have traced the randomness to sc.tl.score_genes_cell_cycle. Even if I explicitly state the random state to be 0, two repeats of score_genes_cell_cycle from the same data would give different downstream clustering. On the other hand, if I use one score_genes_cell_cycle output and repeat the downstream clustering methods, the clusters are the same. . I am currently using scanpy==1.3.1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313
https://github.com/scverse/scanpy/issues/313:214,availability,down,downstream,214,"Thanks for the help. I have traced the randomness to sc.tl.score_genes_cell_cycle. Even if I explicitly state the random state to be 0, two repeats of score_genes_cell_cycle from the same data would give different downstream clustering. On the other hand, if I use one score_genes_cell_cycle output and repeat the downstream clustering methods, the clusters are the same. . I am currently using scanpy==1.3.1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313
https://github.com/scverse/scanpy/issues/313:225,availability,cluster,clustering,225,"Thanks for the help. I have traced the randomness to sc.tl.score_genes_cell_cycle. Even if I explicitly state the random state to be 0, two repeats of score_genes_cell_cycle from the same data would give different downstream clustering. On the other hand, if I use one score_genes_cell_cycle output and repeat the downstream clustering methods, the clusters are the same. . I am currently using scanpy==1.3.1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313
https://github.com/scverse/scanpy/issues/313:314,availability,down,downstream,314,"Thanks for the help. I have traced the randomness to sc.tl.score_genes_cell_cycle. Even if I explicitly state the random state to be 0, two repeats of score_genes_cell_cycle from the same data would give different downstream clustering. On the other hand, if I use one score_genes_cell_cycle output and repeat the downstream clustering methods, the clusters are the same. . I am currently using scanpy==1.3.1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313
https://github.com/scverse/scanpy/issues/313:325,availability,cluster,clustering,325,"Thanks for the help. I have traced the randomness to sc.tl.score_genes_cell_cycle. Even if I explicitly state the random state to be 0, two repeats of score_genes_cell_cycle from the same data would give different downstream clustering. On the other hand, if I use one score_genes_cell_cycle output and repeat the downstream clustering methods, the clusters are the same. . I am currently using scanpy==1.3.1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313
https://github.com/scverse/scanpy/issues/313:349,availability,cluster,clusters,349,"Thanks for the help. I have traced the randomness to sc.tl.score_genes_cell_cycle. Even if I explicitly state the random state to be 0, two repeats of score_genes_cell_cycle from the same data would give different downstream clustering. On the other hand, if I use one score_genes_cell_cycle output and repeat the downstream clustering methods, the clusters are the same. . I am currently using scanpy==1.3.1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313
https://github.com/scverse/scanpy/issues/313:225,deployability,cluster,clustering,225,"Thanks for the help. I have traced the randomness to sc.tl.score_genes_cell_cycle. Even if I explicitly state the random state to be 0, two repeats of score_genes_cell_cycle from the same data would give different downstream clustering. On the other hand, if I use one score_genes_cell_cycle output and repeat the downstream clustering methods, the clusters are the same. . I am currently using scanpy==1.3.1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313
https://github.com/scverse/scanpy/issues/313:325,deployability,cluster,clustering,325,"Thanks for the help. I have traced the randomness to sc.tl.score_genes_cell_cycle. Even if I explicitly state the random state to be 0, two repeats of score_genes_cell_cycle from the same data would give different downstream clustering. On the other hand, if I use one score_genes_cell_cycle output and repeat the downstream clustering methods, the clusters are the same. . I am currently using scanpy==1.3.1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313
https://github.com/scverse/scanpy/issues/313:349,deployability,cluster,clusters,349,"Thanks for the help. I have traced the randomness to sc.tl.score_genes_cell_cycle. Even if I explicitly state the random state to be 0, two repeats of score_genes_cell_cycle from the same data would give different downstream clustering. On the other hand, if I use one score_genes_cell_cycle output and repeat the downstream clustering methods, the clusters are the same. . I am currently using scanpy==1.3.1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313
https://github.com/scverse/scanpy/issues/313:379,energy efficiency,current,currently,379,"Thanks for the help. I have traced the randomness to sc.tl.score_genes_cell_cycle. Even if I explicitly state the random state to be 0, two repeats of score_genes_cell_cycle from the same data would give different downstream clustering. On the other hand, if I use one score_genes_cell_cycle output and repeat the downstream clustering methods, the clusters are the same. . I am currently using scanpy==1.3.1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313
https://github.com/scverse/scanpy/issues/313:104,integrability,state,state,104,"Thanks for the help. I have traced the randomness to sc.tl.score_genes_cell_cycle. Even if I explicitly state the random state to be 0, two repeats of score_genes_cell_cycle from the same data would give different downstream clustering. On the other hand, if I use one score_genes_cell_cycle output and repeat the downstream clustering methods, the clusters are the same. . I am currently using scanpy==1.3.1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313
https://github.com/scverse/scanpy/issues/313:121,integrability,state,state,121,"Thanks for the help. I have traced the randomness to sc.tl.score_genes_cell_cycle. Even if I explicitly state the random state to be 0, two repeats of score_genes_cell_cycle from the same data would give different downstream clustering. On the other hand, if I use one score_genes_cell_cycle output and repeat the downstream clustering methods, the clusters are the same. . I am currently using scanpy==1.3.1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313
https://github.com/scverse/scanpy/issues/313:28,testability,trace,traced,28,"Thanks for the help. I have traced the randomness to sc.tl.score_genes_cell_cycle. Even if I explicitly state the random state to be 0, two repeats of score_genes_cell_cycle from the same data would give different downstream clustering. On the other hand, if I use one score_genes_cell_cycle output and repeat the downstream clustering methods, the clusters are the same. . I am currently using scanpy==1.3.1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313
https://github.com/scverse/scanpy/issues/313:15,usability,help,help,15,"Thanks for the help. I have traced the randomness to sc.tl.score_genes_cell_cycle. Even if I explicitly state the random state to be 0, two repeats of score_genes_cell_cycle from the same data would give different downstream clustering. On the other hand, if I use one score_genes_cell_cycle output and repeat the downstream clustering methods, the clusters are the same. . I am currently using scanpy==1.3.1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313
https://github.com/scverse/scanpy/issues/313:360,security,hash,hash,360,"Was there any progress on this? I also have an issue of non-reproducible UMAPs when re-starting Jupyter notebook. I have not been able to trace the exact cause of the issue, but it appears unrelated to the PCA step (I am using `svd_solver='arpack'`). In my case setting `random.seed` did not fix the issue. However, surprisingly, combining this with disabling hash randomization by setting `PYTHONHASHSEED` to `0` did generate reproducible results",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313
https://github.com/scverse/scanpy/issues/313:138,testability,trace,trace,138,"Was there any progress on this? I also have an issue of non-reproducible UMAPs when re-starting Jupyter notebook. I have not been able to trace the exact cause of the issue, but it appears unrelated to the PCA step (I am using `svd_solver='arpack'`). In my case setting `random.seed` did not fix the issue. However, surprisingly, combining this with disabling hash randomization by setting `PYTHONHASHSEED` to `0` did generate reproducible results",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313
https://github.com/scverse/scanpy/issues/313:14,usability,progress,progress,14,"Was there any progress on this? I also have an issue of non-reproducible UMAPs when re-starting Jupyter notebook. I have not been able to trace the exact cause of the issue, but it appears unrelated to the PCA step (I am using `svd_solver='arpack'`). In my case setting `random.seed` did not fix the issue. However, surprisingly, combining this with disabling hash randomization by setting `PYTHONHASHSEED` to `0` did generate reproducible results",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313
https://github.com/scverse/scanpy/issues/313:825,availability,state,statement,825,"I think I encountered this or a very similar problem and I have a fix. In my analysis I used the `scanpy.tl.score_genes_cell_cycle` to assign cell cycle scores, followed by `scanpy.tl.regress_out` and `scanpy.tl.pca`. On different machines I observed differences in the PCA values. What was surprising, I could see these differences even when using the exact **same singularity container**. I analysed the functions step by step and found that small differences were coming from the `scanpy.tl.score_genes function`, specifically computation of the means here:. `X_list = np.nanmean(X_list, axis=1)`. and. `X_control = np.nanmean(X_control, axis=1)`. I suspect this had something to do with machine precision and maybe choosing between float32 and float64 types. I managed to fix this by setting the dtype to float64 in each statement. This produced identical gene/cell cycle scores on each machine. . The fix (in the `scanpy.tl.score_genes` function):. `X_list = np.nanmean(X_list, axis=1, dtype = 'float64')`. and. `X_control = np.nanmean(X_control, axis=1, dtype = 'float64')`. And to keep the data types consistent convert back to float32 at the end:. `adata.obs[score_name] = pd.Series(np.array(score).ravel(), index=adata.obs_names).astype('float32')`. I hope this will help someone. I am happy to open a pull request if you think this is worthwhile.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313
https://github.com/scverse/scanpy/issues/313:1108,availability,consist,consistent,1108,"I think I encountered this or a very similar problem and I have a fix. In my analysis I used the `scanpy.tl.score_genes_cell_cycle` to assign cell cycle scores, followed by `scanpy.tl.regress_out` and `scanpy.tl.pca`. On different machines I observed differences in the PCA values. What was surprising, I could see these differences even when using the exact **same singularity container**. I analysed the functions step by step and found that small differences were coming from the `scanpy.tl.score_genes function`, specifically computation of the means here:. `X_list = np.nanmean(X_list, axis=1)`. and. `X_control = np.nanmean(X_control, axis=1)`. I suspect this had something to do with machine precision and maybe choosing between float32 and float64 types. I managed to fix this by setting the dtype to float64 in each statement. This produced identical gene/cell cycle scores on each machine. . The fix (in the `scanpy.tl.score_genes` function):. `X_list = np.nanmean(X_list, axis=1, dtype = 'float64')`. and. `X_control = np.nanmean(X_control, axis=1, dtype = 'float64')`. And to keep the data types consistent convert back to float32 at the end:. `adata.obs[score_name] = pd.Series(np.array(score).ravel(), index=adata.obs_names).astype('float32')`. I hope this will help someone. I am happy to open a pull request if you think this is worthwhile.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313
https://github.com/scverse/scanpy/issues/313:242,deployability,observ,observed,242,"I think I encountered this or a very similar problem and I have a fix. In my analysis I used the `scanpy.tl.score_genes_cell_cycle` to assign cell cycle scores, followed by `scanpy.tl.regress_out` and `scanpy.tl.pca`. On different machines I observed differences in the PCA values. What was surprising, I could see these differences even when using the exact **same singularity container**. I analysed the functions step by step and found that small differences were coming from the `scanpy.tl.score_genes function`, specifically computation of the means here:. `X_list = np.nanmean(X_list, axis=1)`. and. `X_control = np.nanmean(X_control, axis=1)`. I suspect this had something to do with machine precision and maybe choosing between float32 and float64 types. I managed to fix this by setting the dtype to float64 in each statement. This produced identical gene/cell cycle scores on each machine. . The fix (in the `scanpy.tl.score_genes` function):. `X_list = np.nanmean(X_list, axis=1, dtype = 'float64')`. and. `X_control = np.nanmean(X_control, axis=1, dtype = 'float64')`. And to keep the data types consistent convert back to float32 at the end:. `adata.obs[score_name] = pd.Series(np.array(score).ravel(), index=adata.obs_names).astype('float32')`. I hope this will help someone. I am happy to open a pull request if you think this is worthwhile.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313
https://github.com/scverse/scanpy/issues/313:378,deployability,contain,container,378,"I think I encountered this or a very similar problem and I have a fix. In my analysis I used the `scanpy.tl.score_genes_cell_cycle` to assign cell cycle scores, followed by `scanpy.tl.regress_out` and `scanpy.tl.pca`. On different machines I observed differences in the PCA values. What was surprising, I could see these differences even when using the exact **same singularity container**. I analysed the functions step by step and found that small differences were coming from the `scanpy.tl.score_genes function`, specifically computation of the means here:. `X_list = np.nanmean(X_list, axis=1)`. and. `X_control = np.nanmean(X_control, axis=1)`. I suspect this had something to do with machine precision and maybe choosing between float32 and float64 types. I managed to fix this by setting the dtype to float64 in each statement. This produced identical gene/cell cycle scores on each machine. . The fix (in the `scanpy.tl.score_genes` function):. `X_list = np.nanmean(X_list, axis=1, dtype = 'float64')`. and. `X_control = np.nanmean(X_control, axis=1, dtype = 'float64')`. And to keep the data types consistent convert back to float32 at the end:. `adata.obs[score_name] = pd.Series(np.array(score).ravel(), index=adata.obs_names).astype('float32')`. I hope this will help someone. I am happy to open a pull request if you think this is worthwhile.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313
https://github.com/scverse/scanpy/issues/313:765,deployability,manag,managed,765,"I think I encountered this or a very similar problem and I have a fix. In my analysis I used the `scanpy.tl.score_genes_cell_cycle` to assign cell cycle scores, followed by `scanpy.tl.regress_out` and `scanpy.tl.pca`. On different machines I observed differences in the PCA values. What was surprising, I could see these differences even when using the exact **same singularity container**. I analysed the functions step by step and found that small differences were coming from the `scanpy.tl.score_genes function`, specifically computation of the means here:. `X_list = np.nanmean(X_list, axis=1)`. and. `X_control = np.nanmean(X_control, axis=1)`. I suspect this had something to do with machine precision and maybe choosing between float32 and float64 types. I managed to fix this by setting the dtype to float64 in each statement. This produced identical gene/cell cycle scores on each machine. . The fix (in the `scanpy.tl.score_genes` function):. `X_list = np.nanmean(X_list, axis=1, dtype = 'float64')`. and. `X_control = np.nanmean(X_control, axis=1, dtype = 'float64')`. And to keep the data types consistent convert back to float32 at the end:. `adata.obs[score_name] = pd.Series(np.array(score).ravel(), index=adata.obs_names).astype('float32')`. I hope this will help someone. I am happy to open a pull request if you think this is worthwhile.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313
https://github.com/scverse/scanpy/issues/313:765,energy efficiency,manag,managed,765,"I think I encountered this or a very similar problem and I have a fix. In my analysis I used the `scanpy.tl.score_genes_cell_cycle` to assign cell cycle scores, followed by `scanpy.tl.regress_out` and `scanpy.tl.pca`. On different machines I observed differences in the PCA values. What was surprising, I could see these differences even when using the exact **same singularity container**. I analysed the functions step by step and found that small differences were coming from the `scanpy.tl.score_genes function`, specifically computation of the means here:. `X_list = np.nanmean(X_list, axis=1)`. and. `X_control = np.nanmean(X_control, axis=1)`. I suspect this had something to do with machine precision and maybe choosing between float32 and float64 types. I managed to fix this by setting the dtype to float64 in each statement. This produced identical gene/cell cycle scores on each machine. . The fix (in the `scanpy.tl.score_genes` function):. `X_list = np.nanmean(X_list, axis=1, dtype = 'float64')`. and. `X_control = np.nanmean(X_control, axis=1, dtype = 'float64')`. And to keep the data types consistent convert back to float32 at the end:. `adata.obs[score_name] = pd.Series(np.array(score).ravel(), index=adata.obs_names).astype('float32')`. I hope this will help someone. I am happy to open a pull request if you think this is worthwhile.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313
https://github.com/scverse/scanpy/issues/313:825,integrability,state,statement,825,"I think I encountered this or a very similar problem and I have a fix. In my analysis I used the `scanpy.tl.score_genes_cell_cycle` to assign cell cycle scores, followed by `scanpy.tl.regress_out` and `scanpy.tl.pca`. On different machines I observed differences in the PCA values. What was surprising, I could see these differences even when using the exact **same singularity container**. I analysed the functions step by step and found that small differences were coming from the `scanpy.tl.score_genes function`, specifically computation of the means here:. `X_list = np.nanmean(X_list, axis=1)`. and. `X_control = np.nanmean(X_control, axis=1)`. I suspect this had something to do with machine precision and maybe choosing between float32 and float64 types. I managed to fix this by setting the dtype to float64 in each statement. This produced identical gene/cell cycle scores on each machine. . The fix (in the `scanpy.tl.score_genes` function):. `X_list = np.nanmean(X_list, axis=1, dtype = 'float64')`. and. `X_control = np.nanmean(X_control, axis=1, dtype = 'float64')`. And to keep the data types consistent convert back to float32 at the end:. `adata.obs[score_name] = pd.Series(np.array(score).ravel(), index=adata.obs_names).astype('float32')`. I hope this will help someone. I am happy to open a pull request if you think this is worthwhile.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313
https://github.com/scverse/scanpy/issues/313:517,interoperability,specif,specifically,517,"I think I encountered this or a very similar problem and I have a fix. In my analysis I used the `scanpy.tl.score_genes_cell_cycle` to assign cell cycle scores, followed by `scanpy.tl.regress_out` and `scanpy.tl.pca`. On different machines I observed differences in the PCA values. What was surprising, I could see these differences even when using the exact **same singularity container**. I analysed the functions step by step and found that small differences were coming from the `scanpy.tl.score_genes function`, specifically computation of the means here:. `X_list = np.nanmean(X_list, axis=1)`. and. `X_control = np.nanmean(X_control, axis=1)`. I suspect this had something to do with machine precision and maybe choosing between float32 and float64 types. I managed to fix this by setting the dtype to float64 in each statement. This produced identical gene/cell cycle scores on each machine. . The fix (in the `scanpy.tl.score_genes` function):. `X_list = np.nanmean(X_list, axis=1, dtype = 'float64')`. and. `X_control = np.nanmean(X_control, axis=1, dtype = 'float64')`. And to keep the data types consistent convert back to float32 at the end:. `adata.obs[score_name] = pd.Series(np.array(score).ravel(), index=adata.obs_names).astype('float32')`. I hope this will help someone. I am happy to open a pull request if you think this is worthwhile.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313
https://github.com/scverse/scanpy/issues/313:765,safety,manag,managed,765,"I think I encountered this or a very similar problem and I have a fix. In my analysis I used the `scanpy.tl.score_genes_cell_cycle` to assign cell cycle scores, followed by `scanpy.tl.regress_out` and `scanpy.tl.pca`. On different machines I observed differences in the PCA values. What was surprising, I could see these differences even when using the exact **same singularity container**. I analysed the functions step by step and found that small differences were coming from the `scanpy.tl.score_genes function`, specifically computation of the means here:. `X_list = np.nanmean(X_list, axis=1)`. and. `X_control = np.nanmean(X_control, axis=1)`. I suspect this had something to do with machine precision and maybe choosing between float32 and float64 types. I managed to fix this by setting the dtype to float64 in each statement. This produced identical gene/cell cycle scores on each machine. . The fix (in the `scanpy.tl.score_genes` function):. `X_list = np.nanmean(X_list, axis=1, dtype = 'float64')`. and. `X_control = np.nanmean(X_control, axis=1, dtype = 'float64')`. And to keep the data types consistent convert back to float32 at the end:. `adata.obs[score_name] = pd.Series(np.array(score).ravel(), index=adata.obs_names).astype('float32')`. I hope this will help someone. I am happy to open a pull request if you think this is worthwhile.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313
https://github.com/scverse/scanpy/issues/313:850,security,ident,identical,850,"I think I encountered this or a very similar problem and I have a fix. In my analysis I used the `scanpy.tl.score_genes_cell_cycle` to assign cell cycle scores, followed by `scanpy.tl.regress_out` and `scanpy.tl.pca`. On different machines I observed differences in the PCA values. What was surprising, I could see these differences even when using the exact **same singularity container**. I analysed the functions step by step and found that small differences were coming from the `scanpy.tl.score_genes function`, specifically computation of the means here:. `X_list = np.nanmean(X_list, axis=1)`. and. `X_control = np.nanmean(X_control, axis=1)`. I suspect this had something to do with machine precision and maybe choosing between float32 and float64 types. I managed to fix this by setting the dtype to float64 in each statement. This produced identical gene/cell cycle scores on each machine. . The fix (in the `scanpy.tl.score_genes` function):. `X_list = np.nanmean(X_list, axis=1, dtype = 'float64')`. and. `X_control = np.nanmean(X_control, axis=1, dtype = 'float64')`. And to keep the data types consistent convert back to float32 at the end:. `adata.obs[score_name] = pd.Series(np.array(score).ravel(), index=adata.obs_names).astype('float32')`. I hope this will help someone. I am happy to open a pull request if you think this is worthwhile.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313
https://github.com/scverse/scanpy/issues/313:242,testability,observ,observed,242,"I think I encountered this or a very similar problem and I have a fix. In my analysis I used the `scanpy.tl.score_genes_cell_cycle` to assign cell cycle scores, followed by `scanpy.tl.regress_out` and `scanpy.tl.pca`. On different machines I observed differences in the PCA values. What was surprising, I could see these differences even when using the exact **same singularity container**. I analysed the functions step by step and found that small differences were coming from the `scanpy.tl.score_genes function`, specifically computation of the means here:. `X_list = np.nanmean(X_list, axis=1)`. and. `X_control = np.nanmean(X_control, axis=1)`. I suspect this had something to do with machine precision and maybe choosing between float32 and float64 types. I managed to fix this by setting the dtype to float64 in each statement. This produced identical gene/cell cycle scores on each machine. . The fix (in the `scanpy.tl.score_genes` function):. `X_list = np.nanmean(X_list, axis=1, dtype = 'float64')`. and. `X_control = np.nanmean(X_control, axis=1, dtype = 'float64')`. And to keep the data types consistent convert back to float32 at the end:. `adata.obs[score_name] = pd.Series(np.array(score).ravel(), index=adata.obs_names).astype('float32')`. I hope this will help someone. I am happy to open a pull request if you think this is worthwhile.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313
https://github.com/scverse/scanpy/issues/313:1108,usability,consist,consistent,1108,"I think I encountered this or a very similar problem and I have a fix. In my analysis I used the `scanpy.tl.score_genes_cell_cycle` to assign cell cycle scores, followed by `scanpy.tl.regress_out` and `scanpy.tl.pca`. On different machines I observed differences in the PCA values. What was surprising, I could see these differences even when using the exact **same singularity container**. I analysed the functions step by step and found that small differences were coming from the `scanpy.tl.score_genes function`, specifically computation of the means here:. `X_list = np.nanmean(X_list, axis=1)`. and. `X_control = np.nanmean(X_control, axis=1)`. I suspect this had something to do with machine precision and maybe choosing between float32 and float64 types. I managed to fix this by setting the dtype to float64 in each statement. This produced identical gene/cell cycle scores on each machine. . The fix (in the `scanpy.tl.score_genes` function):. `X_list = np.nanmean(X_list, axis=1, dtype = 'float64')`. and. `X_control = np.nanmean(X_control, axis=1, dtype = 'float64')`. And to keep the data types consistent convert back to float32 at the end:. `adata.obs[score_name] = pd.Series(np.array(score).ravel(), index=adata.obs_names).astype('float32')`. I hope this will help someone. I am happy to open a pull request if you think this is worthwhile.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313
https://github.com/scverse/scanpy/issues/313:1276,usability,help,help,1276,"I think I encountered this or a very similar problem and I have a fix. In my analysis I used the `scanpy.tl.score_genes_cell_cycle` to assign cell cycle scores, followed by `scanpy.tl.regress_out` and `scanpy.tl.pca`. On different machines I observed differences in the PCA values. What was surprising, I could see these differences even when using the exact **same singularity container**. I analysed the functions step by step and found that small differences were coming from the `scanpy.tl.score_genes function`, specifically computation of the means here:. `X_list = np.nanmean(X_list, axis=1)`. and. `X_control = np.nanmean(X_control, axis=1)`. I suspect this had something to do with machine precision and maybe choosing between float32 and float64 types. I managed to fix this by setting the dtype to float64 in each statement. This produced identical gene/cell cycle scores on each machine. . The fix (in the `scanpy.tl.score_genes` function):. `X_list = np.nanmean(X_list, axis=1, dtype = 'float64')`. and. `X_control = np.nanmean(X_control, axis=1, dtype = 'float64')`. And to keep the data types consistent convert back to float32 at the end:. `adata.obs[score_name] = pd.Series(np.array(score).ravel(), index=adata.obs_names).astype('float32')`. I hope this will help someone. I am happy to open a pull request if you think this is worthwhile.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313
https://github.com/scverse/scanpy/issues/313:0,availability,Ping,Pinging,0,"Pinging this, as I've encountered it as well. I ran into non-reproducible UMAPs when rerunning code/notebooks and systematically went through my pipeline to find the source(s) of error, one of which was `sc.tl.score_genes_cell_cycle`. Setting the random seed externally did not help, but @Iwo-K's comment got me on the right track. I am now using the following simple hack, which fixes the issue for me:. ```python. adata.X = adata.X.astype('<f8') # Make float64 to ensure stability. sc.tl.score_genes_cell_cycle(adata, use_raw=False,. s_genes=cc_s_genes, g2m_genes=cc_g2m_genes,. random_state=0). adata.X = adata.X.astype('<f4') # Return to float32 for consistency. ```. Would be great if this would be fixed internally, perhaps using @Iwo-K's solution?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313
https://github.com/scverse/scanpy/issues/313:179,availability,error,error,179,"Pinging this, as I've encountered it as well. I ran into non-reproducible UMAPs when rerunning code/notebooks and systematically went through my pipeline to find the source(s) of error, one of which was `sc.tl.score_genes_cell_cycle`. Setting the random seed externally did not help, but @Iwo-K's comment got me on the right track. I am now using the following simple hack, which fixes the issue for me:. ```python. adata.X = adata.X.astype('<f8') # Make float64 to ensure stability. sc.tl.score_genes_cell_cycle(adata, use_raw=False,. s_genes=cc_s_genes, g2m_genes=cc_g2m_genes,. random_state=0). adata.X = adata.X.astype('<f4') # Return to float32 for consistency. ```. Would be great if this would be fixed internally, perhaps using @Iwo-K's solution?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313
https://github.com/scverse/scanpy/issues/313:654,availability,consist,consistency,654,"Pinging this, as I've encountered it as well. I ran into non-reproducible UMAPs when rerunning code/notebooks and systematically went through my pipeline to find the source(s) of error, one of which was `sc.tl.score_genes_cell_cycle`. Setting the random seed externally did not help, but @Iwo-K's comment got me on the right track. I am now using the following simple hack, which fixes the issue for me:. ```python. adata.X = adata.X.astype('<f8') # Make float64 to ensure stability. sc.tl.score_genes_cell_cycle(adata, use_raw=False,. s_genes=cc_s_genes, g2m_genes=cc_g2m_genes,. random_state=0). adata.X = adata.X.astype('<f4') # Return to float32 for consistency. ```. Would be great if this would be fixed internally, perhaps using @Iwo-K's solution?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313
https://github.com/scverse/scanpy/issues/313:145,deployability,pipelin,pipeline,145,"Pinging this, as I've encountered it as well. I ran into non-reproducible UMAPs when rerunning code/notebooks and systematically went through my pipeline to find the source(s) of error, one of which was `sc.tl.score_genes_cell_cycle`. Setting the random seed externally did not help, but @Iwo-K's comment got me on the right track. I am now using the following simple hack, which fixes the issue for me:. ```python. adata.X = adata.X.astype('<f8') # Make float64 to ensure stability. sc.tl.score_genes_cell_cycle(adata, use_raw=False,. s_genes=cc_s_genes, g2m_genes=cc_g2m_genes,. random_state=0). adata.X = adata.X.astype('<f4') # Return to float32 for consistency. ```. Would be great if this would be fixed internally, perhaps using @Iwo-K's solution?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313
https://github.com/scverse/scanpy/issues/313:145,integrability,pipelin,pipeline,145,"Pinging this, as I've encountered it as well. I ran into non-reproducible UMAPs when rerunning code/notebooks and systematically went through my pipeline to find the source(s) of error, one of which was `sc.tl.score_genes_cell_cycle`. Setting the random seed externally did not help, but @Iwo-K's comment got me on the right track. I am now using the following simple hack, which fixes the issue for me:. ```python. adata.X = adata.X.astype('<f8') # Make float64 to ensure stability. sc.tl.score_genes_cell_cycle(adata, use_raw=False,. s_genes=cc_s_genes, g2m_genes=cc_g2m_genes,. random_state=0). adata.X = adata.X.astype('<f4') # Return to float32 for consistency. ```. Would be great if this would be fixed internally, perhaps using @Iwo-K's solution?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313
https://github.com/scverse/scanpy/issues/313:179,performance,error,error,179,"Pinging this, as I've encountered it as well. I ran into non-reproducible UMAPs when rerunning code/notebooks and systematically went through my pipeline to find the source(s) of error, one of which was `sc.tl.score_genes_cell_cycle`. Setting the random seed externally did not help, but @Iwo-K's comment got me on the right track. I am now using the following simple hack, which fixes the issue for me:. ```python. adata.X = adata.X.astype('<f8') # Make float64 to ensure stability. sc.tl.score_genes_cell_cycle(adata, use_raw=False,. s_genes=cc_s_genes, g2m_genes=cc_g2m_genes,. random_state=0). adata.X = adata.X.astype('<f4') # Return to float32 for consistency. ```. Would be great if this would be fixed internally, perhaps using @Iwo-K's solution?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313
https://github.com/scverse/scanpy/issues/313:473,reliability,stabil,stability,473,"Pinging this, as I've encountered it as well. I ran into non-reproducible UMAPs when rerunning code/notebooks and systematically went through my pipeline to find the source(s) of error, one of which was `sc.tl.score_genes_cell_cycle`. Setting the random seed externally did not help, but @Iwo-K's comment got me on the right track. I am now using the following simple hack, which fixes the issue for me:. ```python. adata.X = adata.X.astype('<f8') # Make float64 to ensure stability. sc.tl.score_genes_cell_cycle(adata, use_raw=False,. s_genes=cc_s_genes, g2m_genes=cc_g2m_genes,. random_state=0). adata.X = adata.X.astype('<f4') # Return to float32 for consistency. ```. Would be great if this would be fixed internally, perhaps using @Iwo-K's solution?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313
https://github.com/scverse/scanpy/issues/313:179,safety,error,error,179,"Pinging this, as I've encountered it as well. I ran into non-reproducible UMAPs when rerunning code/notebooks and systematically went through my pipeline to find the source(s) of error, one of which was `sc.tl.score_genes_cell_cycle`. Setting the random seed externally did not help, but @Iwo-K's comment got me on the right track. I am now using the following simple hack, which fixes the issue for me:. ```python. adata.X = adata.X.astype('<f8') # Make float64 to ensure stability. sc.tl.score_genes_cell_cycle(adata, use_raw=False,. s_genes=cc_s_genes, g2m_genes=cc_g2m_genes,. random_state=0). adata.X = adata.X.astype('<f4') # Return to float32 for consistency. ```. Would be great if this would be fixed internally, perhaps using @Iwo-K's solution?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313
https://github.com/scverse/scanpy/issues/313:368,security,hack,hack,368,"Pinging this, as I've encountered it as well. I ran into non-reproducible UMAPs when rerunning code/notebooks and systematically went through my pipeline to find the source(s) of error, one of which was `sc.tl.score_genes_cell_cycle`. Setting the random seed externally did not help, but @Iwo-K's comment got me on the right track. I am now using the following simple hack, which fixes the issue for me:. ```python. adata.X = adata.X.astype('<f8') # Make float64 to ensure stability. sc.tl.score_genes_cell_cycle(adata, use_raw=False,. s_genes=cc_s_genes, g2m_genes=cc_g2m_genes,. random_state=0). adata.X = adata.X.astype('<f4') # Return to float32 for consistency. ```. Would be great if this would be fixed internally, perhaps using @Iwo-K's solution?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313
https://github.com/scverse/scanpy/issues/313:361,testability,simpl,simple,361,"Pinging this, as I've encountered it as well. I ran into non-reproducible UMAPs when rerunning code/notebooks and systematically went through my pipeline to find the source(s) of error, one of which was `sc.tl.score_genes_cell_cycle`. Setting the random seed externally did not help, but @Iwo-K's comment got me on the right track. I am now using the following simple hack, which fixes the issue for me:. ```python. adata.X = adata.X.astype('<f8') # Make float64 to ensure stability. sc.tl.score_genes_cell_cycle(adata, use_raw=False,. s_genes=cc_s_genes, g2m_genes=cc_g2m_genes,. random_state=0). adata.X = adata.X.astype('<f4') # Return to float32 for consistency. ```. Would be great if this would be fixed internally, perhaps using @Iwo-K's solution?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313
https://github.com/scverse/scanpy/issues/313:179,usability,error,error,179,"Pinging this, as I've encountered it as well. I ran into non-reproducible UMAPs when rerunning code/notebooks and systematically went through my pipeline to find the source(s) of error, one of which was `sc.tl.score_genes_cell_cycle`. Setting the random seed externally did not help, but @Iwo-K's comment got me on the right track. I am now using the following simple hack, which fixes the issue for me:. ```python. adata.X = adata.X.astype('<f8') # Make float64 to ensure stability. sc.tl.score_genes_cell_cycle(adata, use_raw=False,. s_genes=cc_s_genes, g2m_genes=cc_g2m_genes,. random_state=0). adata.X = adata.X.astype('<f4') # Return to float32 for consistency. ```. Would be great if this would be fixed internally, perhaps using @Iwo-K's solution?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313
https://github.com/scverse/scanpy/issues/313:278,usability,help,help,278,"Pinging this, as I've encountered it as well. I ran into non-reproducible UMAPs when rerunning code/notebooks and systematically went through my pipeline to find the source(s) of error, one of which was `sc.tl.score_genes_cell_cycle`. Setting the random seed externally did not help, but @Iwo-K's comment got me on the right track. I am now using the following simple hack, which fixes the issue for me:. ```python. adata.X = adata.X.astype('<f8') # Make float64 to ensure stability. sc.tl.score_genes_cell_cycle(adata, use_raw=False,. s_genes=cc_s_genes, g2m_genes=cc_g2m_genes,. random_state=0). adata.X = adata.X.astype('<f4') # Return to float32 for consistency. ```. Would be great if this would be fixed internally, perhaps using @Iwo-K's solution?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313
https://github.com/scverse/scanpy/issues/313:361,usability,simpl,simple,361,"Pinging this, as I've encountered it as well. I ran into non-reproducible UMAPs when rerunning code/notebooks and systematically went through my pipeline to find the source(s) of error, one of which was `sc.tl.score_genes_cell_cycle`. Setting the random seed externally did not help, but @Iwo-K's comment got me on the right track. I am now using the following simple hack, which fixes the issue for me:. ```python. adata.X = adata.X.astype('<f8') # Make float64 to ensure stability. sc.tl.score_genes_cell_cycle(adata, use_raw=False,. s_genes=cc_s_genes, g2m_genes=cc_g2m_genes,. random_state=0). adata.X = adata.X.astype('<f4') # Return to float32 for consistency. ```. Would be great if this would be fixed internally, perhaps using @Iwo-K's solution?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313
https://github.com/scverse/scanpy/issues/313:654,usability,consist,consistency,654,"Pinging this, as I've encountered it as well. I ran into non-reproducible UMAPs when rerunning code/notebooks and systematically went through my pipeline to find the source(s) of error, one of which was `sc.tl.score_genes_cell_cycle`. Setting the random seed externally did not help, but @Iwo-K's comment got me on the right track. I am now using the following simple hack, which fixes the issue for me:. ```python. adata.X = adata.X.astype('<f8') # Make float64 to ensure stability. sc.tl.score_genes_cell_cycle(adata, use_raw=False,. s_genes=cc_s_genes, g2m_genes=cc_g2m_genes,. random_state=0). adata.X = adata.X.astype('<f4') # Return to float32 for consistency. ```. Would be great if this would be fixed internally, perhaps using @Iwo-K's solution?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/313
https://github.com/scverse/scanpy/issues/314:176,availability,avail,available,176,"Using `adata.T.write_csvs(skip_data=False)` gives you this. If you only want the data matrix, you can also do `adata.to_df().to_csv()` using pandas. The last call will soon be available in a release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/314
https://github.com/scverse/scanpy/issues/314:191,deployability,releas,release,191,"Using `adata.T.write_csvs(skip_data=False)` gives you this. If you only want the data matrix, you can also do `adata.to_df().to_csv()` using pandas. The last call will soon be available in a release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/314
https://github.com/scverse/scanpy/issues/314:176,reliability,availab,available,176,"Using `adata.T.write_csvs(skip_data=False)` gives you this. If you only want the data matrix, you can also do `adata.to_df().to_csv()` using pandas. The last call will soon be available in a release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/314
https://github.com/scverse/scanpy/issues/314:176,safety,avail,available,176,"Using `adata.T.write_csvs(skip_data=False)` gives you this. If you only want the data matrix, you can also do `adata.to_df().to_csv()` using pandas. The last call will soon be available in a release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/314
https://github.com/scverse/scanpy/issues/314:176,security,availab,available,176,"Using `adata.T.write_csvs(skip_data=False)` gives you this. If you only want the data matrix, you can also do `adata.to_df().to_csv()` using pandas. The last call will soon be available in a release.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/314
https://github.com/scverse/scanpy/issues/314:209,availability,slo,slow,209,Hi @terooatt how are you writing the matrix in Seurat? I can't find a function... https://www.rdocumentation.org/packages/Seurat/versions/2.3.4. @falexwolf thanks! had no idea. adata.to_df() sounds relatively slow and an unnecessary conversion to pandas.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/314
https://github.com/scverse/scanpy/issues/314:129,deployability,version,versions,129,Hi @terooatt how are you writing the matrix in Seurat? I can't find a function... https://www.rdocumentation.org/packages/Seurat/versions/2.3.4. @falexwolf thanks! had no idea. adata.to_df() sounds relatively slow and an unnecessary conversion to pandas.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/314
https://github.com/scverse/scanpy/issues/314:129,integrability,version,versions,129,Hi @terooatt how are you writing the matrix in Seurat? I can't find a function... https://www.rdocumentation.org/packages/Seurat/versions/2.3.4. @falexwolf thanks! had no idea. adata.to_df() sounds relatively slow and an unnecessary conversion to pandas.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/314
https://github.com/scverse/scanpy/issues/314:233,interoperability,convers,conversion,233,Hi @terooatt how are you writing the matrix in Seurat? I can't find a function... https://www.rdocumentation.org/packages/Seurat/versions/2.3.4. @falexwolf thanks! had no idea. adata.to_df() sounds relatively slow and an unnecessary conversion to pandas.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/314
https://github.com/scverse/scanpy/issues/314:113,modifiability,pac,packages,113,Hi @terooatt how are you writing the matrix in Seurat? I can't find a function... https://www.rdocumentation.org/packages/Seurat/versions/2.3.4. @falexwolf thanks! had no idea. adata.to_df() sounds relatively slow and an unnecessary conversion to pandas.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/314
https://github.com/scverse/scanpy/issues/314:129,modifiability,version,versions,129,Hi @terooatt how are you writing the matrix in Seurat? I can't find a function... https://www.rdocumentation.org/packages/Seurat/versions/2.3.4. @falexwolf thanks! had no idea. adata.to_df() sounds relatively slow and an unnecessary conversion to pandas.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/314
https://github.com/scverse/scanpy/issues/314:209,reliability,slo,slow,209,Hi @terooatt how are you writing the matrix in Seurat? I can't find a function... https://www.rdocumentation.org/packages/Seurat/versions/2.3.4. @falexwolf thanks! had no idea. adata.to_df() sounds relatively slow and an unnecessary conversion to pandas.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/314
https://github.com/scverse/scanpy/pull/316:0,energy efficiency,Cool,Cool,0,"Cool! Just the first question for now: any sparse matrix is allowed for `adata.X`, but typically this is csr or csc.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:160,availability,cluster,clustering,160,All of this looks really good! Awesome! :smile:. Most important thing that misses: making use of it already in a tutorial? Do you think it fits in the standard clustering tutorial? I just think that you should probably also add the top-level function to the `qc.py` file in preprocessing.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:160,deployability,cluster,clustering,160,All of this looks really good! Awesome! :smile:. Most important thing that misses: making use of it already in a tutorial? Do you think it fits in the standard clustering tutorial? I just think that you should probably also add the top-level function to the `qc.py` file in preprocessing.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:151,interoperability,standard,standard,151,All of this looks really good! Awesome! :smile:. Most important thing that misses: making use of it already in a tutorial? Do you think it fits in the standard clustering tutorial? I just think that you should probably also add the top-level function to the `qc.py` file in preprocessing.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:836,deployability,releas,release,836,"* I think it fits the standard tutorial, I calculate these things all the time. All the tutorials are in the [`scanpy_usage`](https://github.com/theislab/scanpy_usage) repo, right? * Do you mean the `top_segment_proportions` and/ or `top_proportions` functions? ## Sparse matrix support. I took the easy way out for calculations on other sparse matrices types – just converted them to a `CSR` – so there's room for improvement. I'm considering writing a more involved implementation, but I'd have to benchmark it against conversion. . I'd probably try an online sort (insertion?) for each cell, keeping only the top `max(ns)` expression values, while iterating through the `COO` or `CSC` matrix. ## Numba. This currently throws a lot of warnings about `np.partition` not being implemented in `numba`. This should change with their next release, and give some speedup here. ## `cell_controls`. I'm trying to decide on including this. I haven't used data with control wells, so I don't know how common it is. It could be nice to implement it a bit differently, and have able to get something like`.var[""mean_counts_in_sampletype-CD8""]`, but I'm already returning a lot of values. Any thoughts? ## f-strings. Just noticed this is why my builds are failing. This might get ugly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:1234,deployability,build,builds,1234,"* I think it fits the standard tutorial, I calculate these things all the time. All the tutorials are in the [`scanpy_usage`](https://github.com/theislab/scanpy_usage) repo, right? * Do you mean the `top_segment_proportions` and/ or `top_proportions` functions? ## Sparse matrix support. I took the easy way out for calculations on other sparse matrices types – just converted them to a `CSR` – so there's room for improvement. I'm considering writing a more involved implementation, but I'd have to benchmark it against conversion. . I'd probably try an online sort (insertion?) for each cell, keeping only the top `max(ns)` expression values, while iterating through the `COO` or `CSC` matrix. ## Numba. This currently throws a lot of warnings about `np.partition` not being implemented in `numba`. This should change with their next release, and give some speedup here. ## `cell_controls`. I'm trying to decide on including this. I haven't used data with control wells, so I don't know how common it is. It could be nice to implement it a bit differently, and have able to get something like`.var[""mean_counts_in_sampletype-CD8""]`, but I'm already returning a lot of values. Any thoughts? ## f-strings. Just noticed this is why my builds are failing. This might get ugly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:1245,deployability,fail,failing,1245,"* I think it fits the standard tutorial, I calculate these things all the time. All the tutorials are in the [`scanpy_usage`](https://github.com/theislab/scanpy_usage) repo, right? * Do you mean the `top_segment_proportions` and/ or `top_proportions` functions? ## Sparse matrix support. I took the easy way out for calculations on other sparse matrices types – just converted them to a `CSR` – so there's room for improvement. I'm considering writing a more involved implementation, but I'd have to benchmark it against conversion. . I'd probably try an online sort (insertion?) for each cell, keeping only the top `max(ns)` expression values, while iterating through the `COO` or `CSC` matrix. ## Numba. This currently throws a lot of warnings about `np.partition` not being implemented in `numba`. This should change with their next release, and give some speedup here. ## `cell_controls`. I'm trying to decide on including this. I haven't used data with control wells, so I don't know how common it is. It could be nice to implement it a bit differently, and have able to get something like`.var[""mean_counts_in_sampletype-CD8""]`, but I'm already returning a lot of values. Any thoughts? ## f-strings. Just noticed this is why my builds are failing. This might get ugly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:711,energy efficiency,current,currently,711,"* I think it fits the standard tutorial, I calculate these things all the time. All the tutorials are in the [`scanpy_usage`](https://github.com/theislab/scanpy_usage) repo, right? * Do you mean the `top_segment_proportions` and/ or `top_proportions` functions? ## Sparse matrix support. I took the easy way out for calculations on other sparse matrices types – just converted them to a `CSR` – so there's room for improvement. I'm considering writing a more involved implementation, but I'd have to benchmark it against conversion. . I'd probably try an online sort (insertion?) for each cell, keeping only the top `max(ns)` expression values, while iterating through the `COO` or `CSC` matrix. ## Numba. This currently throws a lot of warnings about `np.partition` not being implemented in `numba`. This should change with their next release, and give some speedup here. ## `cell_controls`. I'm trying to decide on including this. I haven't used data with control wells, so I don't know how common it is. It could be nice to implement it a bit differently, and have able to get something like`.var[""mean_counts_in_sampletype-CD8""]`, but I'm already returning a lot of values. Any thoughts? ## f-strings. Just noticed this is why my builds are failing. This might get ugly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:22,interoperability,standard,standard,22,"* I think it fits the standard tutorial, I calculate these things all the time. All the tutorials are in the [`scanpy_usage`](https://github.com/theislab/scanpy_usage) repo, right? * Do you mean the `top_segment_proportions` and/ or `top_proportions` functions? ## Sparse matrix support. I took the easy way out for calculations on other sparse matrices types – just converted them to a `CSR` – so there's room for improvement. I'm considering writing a more involved implementation, but I'd have to benchmark it against conversion. . I'd probably try an online sort (insertion?) for each cell, keeping only the top `max(ns)` expression values, while iterating through the `COO` or `CSC` matrix. ## Numba. This currently throws a lot of warnings about `np.partition` not being implemented in `numba`. This should change with their next release, and give some speedup here. ## `cell_controls`. I'm trying to decide on including this. I haven't used data with control wells, so I don't know how common it is. It could be nice to implement it a bit differently, and have able to get something like`.var[""mean_counts_in_sampletype-CD8""]`, but I'm already returning a lot of values. Any thoughts? ## f-strings. Just noticed this is why my builds are failing. This might get ugly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:521,interoperability,convers,conversion,521,"* I think it fits the standard tutorial, I calculate these things all the time. All the tutorials are in the [`scanpy_usage`](https://github.com/theislab/scanpy_usage) repo, right? * Do you mean the `top_segment_proportions` and/ or `top_proportions` functions? ## Sparse matrix support. I took the easy way out for calculations on other sparse matrices types – just converted them to a `CSR` – so there's room for improvement. I'm considering writing a more involved implementation, but I'd have to benchmark it against conversion. . I'd probably try an online sort (insertion?) for each cell, keeping only the top `max(ns)` expression values, while iterating through the `COO` or `CSC` matrix. ## Numba. This currently throws a lot of warnings about `np.partition` not being implemented in `numba`. This should change with their next release, and give some speedup here. ## `cell_controls`. I'm trying to decide on including this. I haven't used data with control wells, so I don't know how common it is. It could be nice to implement it a bit differently, and have able to get something like`.var[""mean_counts_in_sampletype-CD8""]`, but I'm already returning a lot of values. Any thoughts? ## f-strings. Just noticed this is why my builds are failing. This might get ugly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:74,performance,time,time,74,"* I think it fits the standard tutorial, I calculate these things all the time. All the tutorials are in the [`scanpy_usage`](https://github.com/theislab/scanpy_usage) repo, right? * Do you mean the `top_segment_proportions` and/ or `top_proportions` functions? ## Sparse matrix support. I took the easy way out for calculations on other sparse matrices types – just converted them to a `CSR` – so there's room for improvement. I'm considering writing a more involved implementation, but I'd have to benchmark it against conversion. . I'd probably try an online sort (insertion?) for each cell, keeping only the top `max(ns)` expression values, while iterating through the `COO` or `CSC` matrix. ## Numba. This currently throws a lot of warnings about `np.partition` not being implemented in `numba`. This should change with their next release, and give some speedup here. ## `cell_controls`. I'm trying to decide on including this. I haven't used data with control wells, so I don't know how common it is. It could be nice to implement it a bit differently, and have able to get something like`.var[""mean_counts_in_sampletype-CD8""]`, but I'm already returning a lot of values. Any thoughts? ## f-strings. Just noticed this is why my builds are failing. This might get ugly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:1245,reliability,fail,failing,1245,"* I think it fits the standard tutorial, I calculate these things all the time. All the tutorials are in the [`scanpy_usage`](https://github.com/theislab/scanpy_usage) repo, right? * Do you mean the `top_segment_proportions` and/ or `top_proportions` functions? ## Sparse matrix support. I took the easy way out for calculations on other sparse matrices types – just converted them to a `CSR` – so there's room for improvement. I'm considering writing a more involved implementation, but I'd have to benchmark it against conversion. . I'd probably try an online sort (insertion?) for each cell, keeping only the top `max(ns)` expression values, while iterating through the `COO` or `CSC` matrix. ## Numba. This currently throws a lot of warnings about `np.partition` not being implemented in `numba`. This should change with their next release, and give some speedup here. ## `cell_controls`. I'm trying to decide on including this. I haven't used data with control wells, so I don't know how common it is. It could be nice to implement it a bit differently, and have able to get something like`.var[""mean_counts_in_sampletype-CD8""]`, but I'm already returning a lot of values. Any thoughts? ## f-strings. Just noticed this is why my builds are failing. This might get ugly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:958,security,control,control,958,"* I think it fits the standard tutorial, I calculate these things all the time. All the tutorials are in the [`scanpy_usage`](https://github.com/theislab/scanpy_usage) repo, right? * Do you mean the `top_segment_proportions` and/ or `top_proportions` functions? ## Sparse matrix support. I took the easy way out for calculations on other sparse matrices types – just converted them to a `CSR` – so there's room for improvement. I'm considering writing a more involved implementation, but I'd have to benchmark it against conversion. . I'd probably try an online sort (insertion?) for each cell, keeping only the top `max(ns)` expression values, while iterating through the `COO` or `CSC` matrix. ## Numba. This currently throws a lot of warnings about `np.partition` not being implemented in `numba`. This should change with their next release, and give some speedup here. ## `cell_controls`. I'm trying to decide on including this. I haven't used data with control wells, so I don't know how common it is. It could be nice to implement it a bit differently, and have able to get something like`.var[""mean_counts_in_sampletype-CD8""]`, but I'm already returning a lot of values. Any thoughts? ## f-strings. Just noticed this is why my builds are failing. This might get ugly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:958,testability,control,control,958,"* I think it fits the standard tutorial, I calculate these things all the time. All the tutorials are in the [`scanpy_usage`](https://github.com/theislab/scanpy_usage) repo, right? * Do you mean the `top_segment_proportions` and/ or `top_proportions` functions? ## Sparse matrix support. I took the easy way out for calculations on other sparse matrices types – just converted them to a `CSR` – so there's room for improvement. I'm considering writing a more involved implementation, but I'd have to benchmark it against conversion. . I'd probably try an online sort (insertion?) for each cell, keeping only the top `max(ns)` expression values, while iterating through the `COO` or `CSC` matrix. ## Numba. This currently throws a lot of warnings about `np.partition` not being implemented in `numba`. This should change with their next release, and give some speedup here. ## `cell_controls`. I'm trying to decide on including this. I haven't used data with control wells, so I don't know how common it is. It could be nice to implement it a bit differently, and have able to get something like`.var[""mean_counts_in_sampletype-CD8""]`, but I'm already returning a lot of values. Any thoughts? ## f-strings. Just noticed this is why my builds are failing. This might get ugly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:279,usability,support,support,279,"* I think it fits the standard tutorial, I calculate these things all the time. All the tutorials are in the [`scanpy_usage`](https://github.com/theislab/scanpy_usage) repo, right? * Do you mean the `top_segment_proportions` and/ or `top_proportions` functions? ## Sparse matrix support. I took the easy way out for calculations on other sparse matrices types – just converted them to a `CSR` – so there's room for improvement. I'm considering writing a more involved implementation, but I'd have to benchmark it against conversion. . I'd probably try an online sort (insertion?) for each cell, keeping only the top `max(ns)` expression values, while iterating through the `COO` or `CSC` matrix. ## Numba. This currently throws a lot of warnings about `np.partition` not being implemented in `numba`. This should change with their next release, and give some speedup here. ## `cell_controls`. I'm trying to decide on including this. I haven't used data with control wells, so I don't know how common it is. It could be nice to implement it a bit differently, and have able to get something like`.var[""mean_counts_in_sampletype-CD8""]`, but I'm already returning a lot of values. Any thoughts? ## f-strings. Just noticed this is why my builds are failing. This might get ugly.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:24,interoperability,standard,standard,24,"> - I think it fits the standard tutorial, I calculate these things all the time. All the tutorials are in the scanpy_usage repo, right? > - Do you mean the top_segment_proportions and/ or top_proportions functions? Yes to both! :). Let me know when I should merge this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:76,performance,time,time,76,"> - I think it fits the standard tutorial, I calculate these things all the time. All the tutorials are in the scanpy_usage repo, right? > - Do you mean the top_segment_proportions and/ or top_proportions functions? Yes to both! :). Let me know when I should merge this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:196,integrability,wrap,wrapper,196,"I've pretty much just gotta write the docs and rebase on my other PR, and this should be good to go. For adding `top_segment_proportions` and `top_proportions` to preprocessing, should they get a wrapper to work on AnnData objects? Also, I'm not super happy with the name `top_segment_proportions`, and am open to suggestions for a better name.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:196,interoperability,wrapper,wrapper,196,"I've pretty much just gotta write the docs and rebase on my other PR, and this should be good to go. For adding `top_segment_proportions` and `top_proportions` to preprocessing, should they get a wrapper to work on AnnData objects? Also, I'm not super happy with the name `top_segment_proportions`, and am open to suggestions for a better name.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:194,deployability,modul,module,194,"I'd say that if the user is supposed to work with `top_segment_proportions` and `top_proportions` on a regular base, it should also accept `AnnData`s. Many Scanpy functions in the preprocessing module do both. In the beginning, I did this via recursive call, these days, I'd wouldn't recommend it but simply do:. ```. X = data. if isinstance(data, AnnData):. X = data.X. ```. or if you don't like `X` then `passed_data` or something... If you provide examples for your stuff here on this PR (simply paste a few pictures with a few lines of code), then we can discuss on a better name. Maybe @fidelram also has some opinion on this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:194,modifiability,modul,module,194,"I'd say that if the user is supposed to work with `top_segment_proportions` and `top_proportions` on a regular base, it should also accept `AnnData`s. Many Scanpy functions in the preprocessing module do both. In the beginning, I did this via recursive call, these days, I'd wouldn't recommend it but simply do:. ```. X = data. if isinstance(data, AnnData):. X = data.X. ```. or if you don't like `X` then `passed_data` or something... If you provide examples for your stuff here on this PR (simply paste a few pictures with a few lines of code), then we can discuss on a better name. Maybe @fidelram also has some opinion on this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:194,safety,modul,module,194,"I'd say that if the user is supposed to work with `top_segment_proportions` and `top_proportions` on a regular base, it should also accept `AnnData`s. Many Scanpy functions in the preprocessing module do both. In the beginning, I did this via recursive call, these days, I'd wouldn't recommend it but simply do:. ```. X = data. if isinstance(data, AnnData):. X = data.X. ```. or if you don't like `X` then `passed_data` or something... If you provide examples for your stuff here on this PR (simply paste a few pictures with a few lines of code), then we can discuss on a better name. Maybe @fidelram also has some opinion on this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:301,testability,simpl,simply,301,"I'd say that if the user is supposed to work with `top_segment_proportions` and `top_proportions` on a regular base, it should also accept `AnnData`s. Many Scanpy functions in the preprocessing module do both. In the beginning, I did this via recursive call, these days, I'd wouldn't recommend it but simply do:. ```. X = data. if isinstance(data, AnnData):. X = data.X. ```. or if you don't like `X` then `passed_data` or something... If you provide examples for your stuff here on this PR (simply paste a few pictures with a few lines of code), then we can discuss on a better name. Maybe @fidelram also has some opinion on this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:492,testability,simpl,simply,492,"I'd say that if the user is supposed to work with `top_segment_proportions` and `top_proportions` on a regular base, it should also accept `AnnData`s. Many Scanpy functions in the preprocessing module do both. In the beginning, I did this via recursive call, these days, I'd wouldn't recommend it but simply do:. ```. X = data. if isinstance(data, AnnData):. X = data.X. ```. or if you don't like `X` then `passed_data` or something... If you provide examples for your stuff here on this PR (simply paste a few pictures with a few lines of code), then we can discuss on a better name. Maybe @fidelram also has some opinion on this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:20,usability,user,user,20,"I'd say that if the user is supposed to work with `top_segment_proportions` and `top_proportions` on a regular base, it should also accept `AnnData`s. Many Scanpy functions in the preprocessing module do both. In the beginning, I did this via recursive call, these days, I'd wouldn't recommend it but simply do:. ```. X = data. if isinstance(data, AnnData):. X = data.X. ```. or if you don't like `X` then `passed_data` or something... If you provide examples for your stuff here on this PR (simply paste a few pictures with a few lines of code), then we can discuss on a better name. Maybe @fidelram also has some opinion on this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:301,usability,simpl,simply,301,"I'd say that if the user is supposed to work with `top_segment_proportions` and `top_proportions` on a regular base, it should also accept `AnnData`s. Many Scanpy functions in the preprocessing module do both. In the beginning, I did this via recursive call, these days, I'd wouldn't recommend it but simply do:. ```. X = data. if isinstance(data, AnnData):. X = data.X. ```. or if you don't like `X` then `passed_data` or something... If you provide examples for your stuff here on this PR (simply paste a few pictures with a few lines of code), then we can discuss on a better name. Maybe @fidelram also has some opinion on this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:492,usability,simpl,simply,492,"I'd say that if the user is supposed to work with `top_segment_proportions` and `top_proportions` on a regular base, it should also accept `AnnData`s. Many Scanpy functions in the preprocessing module do both. In the beginning, I did this via recursive call, these days, I'd wouldn't recommend it but simply do:. ```. X = data. if isinstance(data, AnnData):. X = data.X. ```. or if you don't like `X` then `passed_data` or something... If you provide examples for your stuff here on this PR (simply paste a few pictures with a few lines of code), then we can discuss on a better name. Maybe @fidelram also has some opinion on this.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:49,energy efficiency,current,currently,49,"As mentioned before, can you move everything you currently have in `/preprocessing/simple.py` to `qc.py`? We shouldn't grow that file even larger...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:83,testability,simpl,simple,83,"As mentioned before, can you move everything you currently have in `/preprocessing/simple.py` to `qc.py`? We shouldn't grow that file even larger...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:83,usability,simpl,simple,83,"As mentioned before, can you move everything you currently have in `/preprocessing/simple.py` to `qc.py`? We shouldn't grow that file even larger...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:304,deployability,modul,module,304,"Oh, I think I misunderstood earlier when you said:. > I just think that you should probably also add the top-level function to the qc.py file in preprocessing. . I wasn't sure if you meant move `calculate_qc_metrics` to `qc.py` or add `top_proportions` and `top_segment_proportions` to the preprocessing module. If you're not asking for that, I'm not sure if they're important enough to go there. I use `top_proportions` to make a `plotScater` kind of plot, but that's about it. Otherwise, I think this might be good for now. I was thinking I'd update the tutorial to use this function after the PR is merged. Once that's done, is there a script to update the tests under `notebooks` or is that done manually?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
https://github.com/scverse/scanpy/pull/316:545,deployability,updat,update,545,"Oh, I think I misunderstood earlier when you said:. > I just think that you should probably also add the top-level function to the qc.py file in preprocessing. . I wasn't sure if you meant move `calculate_qc_metrics` to `qc.py` or add `top_proportions` and `top_segment_proportions` to the preprocessing module. If you're not asking for that, I'm not sure if they're important enough to go there. I use `top_proportions` to make a `plotScater` kind of plot, but that's about it. Otherwise, I think this might be good for now. I was thinking I'd update the tutorial to use this function after the PR is merged. Once that's done, is there a script to update the tests under `notebooks` or is that done manually?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/316
