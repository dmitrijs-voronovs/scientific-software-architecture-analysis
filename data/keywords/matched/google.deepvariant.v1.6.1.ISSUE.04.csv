id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/google/deepvariant/issues/136:228,interoperability,standard,standard-,228,"Cannot create VM with more than 4 P100 GPUs; When I tried to create a VM with 8 GPUs using this command line:. export IMAGE_FAMILY=""tf-latest-gpu"". export ZONE=""us-west1-a"". export INSTANCE_NAME=""deep"". export INSTANCE_TYPE=""n1-standard-8"". gcloud compute instances create $INSTANCE_NAME \. --zone=$ZONE \. --image-family=$IMAGE_FAMILY \. --image-project=deeplearning-platform-release \. --maintenance-policy=TERMINATE \. --accelerator=""type=nvidia-tesla-p100,count=8"" \. --machine-type=$INSTANCE_TYPE \. --boot-disk-size=200GB \. --metadata=""install-nvidia-driver=True"". I got this error:. ERROR: (gcloud.compute.instances.create) Could not fetch resource:. - Invalid value for field 'resource.guestAccelerators[0].acceleratorCount': '8'. Number of accelerator cards attached to an instance must be one of [1, 2, 4]. My Quota shows that I have access to 8 P100 GPUs:. ![screenshot from 2019-01-04 12-53-33](https://user-images.githubusercontent.com/19914123/50707923-cd8b4d80-101f-11e9-976d-b04df93f0f26.png).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/136
https://github.com/google/deepvariant/issues/136:368,interoperability,platform,platform-release,368,"Cannot create VM with more than 4 P100 GPUs; When I tried to create a VM with 8 GPUs using this command line:. export IMAGE_FAMILY=""tf-latest-gpu"". export ZONE=""us-west1-a"". export INSTANCE_NAME=""deep"". export INSTANCE_TYPE=""n1-standard-8"". gcloud compute instances create $INSTANCE_NAME \. --zone=$ZONE \. --image-family=$IMAGE_FAMILY \. --image-project=deeplearning-platform-release \. --maintenance-policy=TERMINATE \. --accelerator=""type=nvidia-tesla-p100,count=8"" \. --machine-type=$INSTANCE_TYPE \. --boot-disk-size=200GB \. --metadata=""install-nvidia-driver=True"". I got this error:. ERROR: (gcloud.compute.instances.create) Could not fetch resource:. - Invalid value for field 'resource.guestAccelerators[0].acceleratorCount': '8'. Number of accelerator cards attached to an instance must be one of [1, 2, 4]. My Quota shows that I have access to 8 P100 GPUs:. ![screenshot from 2019-01-04 12-53-33](https://user-images.githubusercontent.com/19914123/50707923-cd8b4d80-101f-11e9-976d-b04df93f0f26.png).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/136
https://github.com/google/deepvariant/issues/136:39,performance,GPU,GPUs,39,"Cannot create VM with more than 4 P100 GPUs; When I tried to create a VM with 8 GPUs using this command line:. export IMAGE_FAMILY=""tf-latest-gpu"". export ZONE=""us-west1-a"". export INSTANCE_NAME=""deep"". export INSTANCE_TYPE=""n1-standard-8"". gcloud compute instances create $INSTANCE_NAME \. --zone=$ZONE \. --image-family=$IMAGE_FAMILY \. --image-project=deeplearning-platform-release \. --maintenance-policy=TERMINATE \. --accelerator=""type=nvidia-tesla-p100,count=8"" \. --machine-type=$INSTANCE_TYPE \. --boot-disk-size=200GB \. --metadata=""install-nvidia-driver=True"". I got this error:. ERROR: (gcloud.compute.instances.create) Could not fetch resource:. - Invalid value for field 'resource.guestAccelerators[0].acceleratorCount': '8'. Number of accelerator cards attached to an instance must be one of [1, 2, 4]. My Quota shows that I have access to 8 P100 GPUs:. ![screenshot from 2019-01-04 12-53-33](https://user-images.githubusercontent.com/19914123/50707923-cd8b4d80-101f-11e9-976d-b04df93f0f26.png).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/136
https://github.com/google/deepvariant/issues/136:80,performance,GPU,GPUs,80,"Cannot create VM with more than 4 P100 GPUs; When I tried to create a VM with 8 GPUs using this command line:. export IMAGE_FAMILY=""tf-latest-gpu"". export ZONE=""us-west1-a"". export INSTANCE_NAME=""deep"". export INSTANCE_TYPE=""n1-standard-8"". gcloud compute instances create $INSTANCE_NAME \. --zone=$ZONE \. --image-family=$IMAGE_FAMILY \. --image-project=deeplearning-platform-release \. --maintenance-policy=TERMINATE \. --accelerator=""type=nvidia-tesla-p100,count=8"" \. --machine-type=$INSTANCE_TYPE \. --boot-disk-size=200GB \. --metadata=""install-nvidia-driver=True"". I got this error:. ERROR: (gcloud.compute.instances.create) Could not fetch resource:. - Invalid value for field 'resource.guestAccelerators[0].acceleratorCount': '8'. Number of accelerator cards attached to an instance must be one of [1, 2, 4]. My Quota shows that I have access to 8 P100 GPUs:. ![screenshot from 2019-01-04 12-53-33](https://user-images.githubusercontent.com/19914123/50707923-cd8b4d80-101f-11e9-976d-b04df93f0f26.png).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/136
https://github.com/google/deepvariant/issues/136:142,performance,gpu,gpu,142,"Cannot create VM with more than 4 P100 GPUs; When I tried to create a VM with 8 GPUs using this command line:. export IMAGE_FAMILY=""tf-latest-gpu"". export ZONE=""us-west1-a"". export INSTANCE_NAME=""deep"". export INSTANCE_TYPE=""n1-standard-8"". gcloud compute instances create $INSTANCE_NAME \. --zone=$ZONE \. --image-family=$IMAGE_FAMILY \. --image-project=deeplearning-platform-release \. --maintenance-policy=TERMINATE \. --accelerator=""type=nvidia-tesla-p100,count=8"" \. --machine-type=$INSTANCE_TYPE \. --boot-disk-size=200GB \. --metadata=""install-nvidia-driver=True"". I got this error:. ERROR: (gcloud.compute.instances.create) Could not fetch resource:. - Invalid value for field 'resource.guestAccelerators[0].acceleratorCount': '8'. Number of accelerator cards attached to an instance must be one of [1, 2, 4]. My Quota shows that I have access to 8 P100 GPUs:. ![screenshot from 2019-01-04 12-53-33](https://user-images.githubusercontent.com/19914123/50707923-cd8b4d80-101f-11e9-976d-b04df93f0f26.png).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/136
https://github.com/google/deepvariant/issues/136:512,performance,disk,disk-size,512,"Cannot create VM with more than 4 P100 GPUs; When I tried to create a VM with 8 GPUs using this command line:. export IMAGE_FAMILY=""tf-latest-gpu"". export ZONE=""us-west1-a"". export INSTANCE_NAME=""deep"". export INSTANCE_TYPE=""n1-standard-8"". gcloud compute instances create $INSTANCE_NAME \. --zone=$ZONE \. --image-family=$IMAGE_FAMILY \. --image-project=deeplearning-platform-release \. --maintenance-policy=TERMINATE \. --accelerator=""type=nvidia-tesla-p100,count=8"" \. --machine-type=$INSTANCE_TYPE \. --boot-disk-size=200GB \. --metadata=""install-nvidia-driver=True"". I got this error:. ERROR: (gcloud.compute.instances.create) Could not fetch resource:. - Invalid value for field 'resource.guestAccelerators[0].acceleratorCount': '8'. Number of accelerator cards attached to an instance must be one of [1, 2, 4]. My Quota shows that I have access to 8 P100 GPUs:. ![screenshot from 2019-01-04 12-53-33](https://user-images.githubusercontent.com/19914123/50707923-cd8b4d80-101f-11e9-976d-b04df93f0f26.png).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/136
https://github.com/google/deepvariant/issues/136:583,performance,error,error,583,"Cannot create VM with more than 4 P100 GPUs; When I tried to create a VM with 8 GPUs using this command line:. export IMAGE_FAMILY=""tf-latest-gpu"". export ZONE=""us-west1-a"". export INSTANCE_NAME=""deep"". export INSTANCE_TYPE=""n1-standard-8"". gcloud compute instances create $INSTANCE_NAME \. --zone=$ZONE \. --image-family=$IMAGE_FAMILY \. --image-project=deeplearning-platform-release \. --maintenance-policy=TERMINATE \. --accelerator=""type=nvidia-tesla-p100,count=8"" \. --machine-type=$INSTANCE_TYPE \. --boot-disk-size=200GB \. --metadata=""install-nvidia-driver=True"". I got this error:. ERROR: (gcloud.compute.instances.create) Could not fetch resource:. - Invalid value for field 'resource.guestAccelerators[0].acceleratorCount': '8'. Number of accelerator cards attached to an instance must be one of [1, 2, 4]. My Quota shows that I have access to 8 P100 GPUs:. ![screenshot from 2019-01-04 12-53-33](https://user-images.githubusercontent.com/19914123/50707923-cd8b4d80-101f-11e9-976d-b04df93f0f26.png).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/136
https://github.com/google/deepvariant/issues/136:591,performance,ERROR,ERROR,591,"Cannot create VM with more than 4 P100 GPUs; When I tried to create a VM with 8 GPUs using this command line:. export IMAGE_FAMILY=""tf-latest-gpu"". export ZONE=""us-west1-a"". export INSTANCE_NAME=""deep"". export INSTANCE_TYPE=""n1-standard-8"". gcloud compute instances create $INSTANCE_NAME \. --zone=$ZONE \. --image-family=$IMAGE_FAMILY \. --image-project=deeplearning-platform-release \. --maintenance-policy=TERMINATE \. --accelerator=""type=nvidia-tesla-p100,count=8"" \. --machine-type=$INSTANCE_TYPE \. --boot-disk-size=200GB \. --metadata=""install-nvidia-driver=True"". I got this error:. ERROR: (gcloud.compute.instances.create) Could not fetch resource:. - Invalid value for field 'resource.guestAccelerators[0].acceleratorCount': '8'. Number of accelerator cards attached to an instance must be one of [1, 2, 4]. My Quota shows that I have access to 8 P100 GPUs:. ![screenshot from 2019-01-04 12-53-33](https://user-images.githubusercontent.com/19914123/50707923-cd8b4d80-101f-11e9-976d-b04df93f0f26.png).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/136
https://github.com/google/deepvariant/issues/136:648,performance,resourc,resource,648,"Cannot create VM with more than 4 P100 GPUs; When I tried to create a VM with 8 GPUs using this command line:. export IMAGE_FAMILY=""tf-latest-gpu"". export ZONE=""us-west1-a"". export INSTANCE_NAME=""deep"". export INSTANCE_TYPE=""n1-standard-8"". gcloud compute instances create $INSTANCE_NAME \. --zone=$ZONE \. --image-family=$IMAGE_FAMILY \. --image-project=deeplearning-platform-release \. --maintenance-policy=TERMINATE \. --accelerator=""type=nvidia-tesla-p100,count=8"" \. --machine-type=$INSTANCE_TYPE \. --boot-disk-size=200GB \. --metadata=""install-nvidia-driver=True"". I got this error:. ERROR: (gcloud.compute.instances.create) Could not fetch resource:. - Invalid value for field 'resource.guestAccelerators[0].acceleratorCount': '8'. Number of accelerator cards attached to an instance must be one of [1, 2, 4]. My Quota shows that I have access to 8 P100 GPUs:. ![screenshot from 2019-01-04 12-53-33](https://user-images.githubusercontent.com/19914123/50707923-cd8b4d80-101f-11e9-976d-b04df93f0f26.png).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/136
https://github.com/google/deepvariant/issues/136:686,performance,resourc,resource,686,"Cannot create VM with more than 4 P100 GPUs; When I tried to create a VM with 8 GPUs using this command line:. export IMAGE_FAMILY=""tf-latest-gpu"". export ZONE=""us-west1-a"". export INSTANCE_NAME=""deep"". export INSTANCE_TYPE=""n1-standard-8"". gcloud compute instances create $INSTANCE_NAME \. --zone=$ZONE \. --image-family=$IMAGE_FAMILY \. --image-project=deeplearning-platform-release \. --maintenance-policy=TERMINATE \. --accelerator=""type=nvidia-tesla-p100,count=8"" \. --machine-type=$INSTANCE_TYPE \. --boot-disk-size=200GB \. --metadata=""install-nvidia-driver=True"". I got this error:. ERROR: (gcloud.compute.instances.create) Could not fetch resource:. - Invalid value for field 'resource.guestAccelerators[0].acceleratorCount': '8'. Number of accelerator cards attached to an instance must be one of [1, 2, 4]. My Quota shows that I have access to 8 P100 GPUs:. ![screenshot from 2019-01-04 12-53-33](https://user-images.githubusercontent.com/19914123/50707923-cd8b4d80-101f-11e9-976d-b04df93f0f26.png).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/136
https://github.com/google/deepvariant/issues/136:862,performance,GPU,GPUs,862,"Cannot create VM with more than 4 P100 GPUs; When I tried to create a VM with 8 GPUs using this command line:. export IMAGE_FAMILY=""tf-latest-gpu"". export ZONE=""us-west1-a"". export INSTANCE_NAME=""deep"". export INSTANCE_TYPE=""n1-standard-8"". gcloud compute instances create $INSTANCE_NAME \. --zone=$ZONE \. --image-family=$IMAGE_FAMILY \. --image-project=deeplearning-platform-release \. --maintenance-policy=TERMINATE \. --accelerator=""type=nvidia-tesla-p100,count=8"" \. --machine-type=$INSTANCE_TYPE \. --boot-disk-size=200GB \. --metadata=""install-nvidia-driver=True"". I got this error:. ERROR: (gcloud.compute.instances.create) Could not fetch resource:. - Invalid value for field 'resource.guestAccelerators[0].acceleratorCount': '8'. Number of accelerator cards attached to an instance must be one of [1, 2, 4]. My Quota shows that I have access to 8 P100 GPUs:. ![screenshot from 2019-01-04 12-53-33](https://user-images.githubusercontent.com/19914123/50707923-cd8b4d80-101f-11e9-976d-b04df93f0f26.png).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/136
https://github.com/google/deepvariant/issues/136:390,reliability,mainten,maintenance-policy,390,"Cannot create VM with more than 4 P100 GPUs; When I tried to create a VM with 8 GPUs using this command line:. export IMAGE_FAMILY=""tf-latest-gpu"". export ZONE=""us-west1-a"". export INSTANCE_NAME=""deep"". export INSTANCE_TYPE=""n1-standard-8"". gcloud compute instances create $INSTANCE_NAME \. --zone=$ZONE \. --image-family=$IMAGE_FAMILY \. --image-project=deeplearning-platform-release \. --maintenance-policy=TERMINATE \. --accelerator=""type=nvidia-tesla-p100,count=8"" \. --machine-type=$INSTANCE_TYPE \. --boot-disk-size=200GB \. --metadata=""install-nvidia-driver=True"". I got this error:. ERROR: (gcloud.compute.instances.create) Could not fetch resource:. - Invalid value for field 'resource.guestAccelerators[0].acceleratorCount': '8'. Number of accelerator cards attached to an instance must be one of [1, 2, 4]. My Quota shows that I have access to 8 P100 GPUs:. ![screenshot from 2019-01-04 12-53-33](https://user-images.githubusercontent.com/19914123/50707923-cd8b4d80-101f-11e9-976d-b04df93f0f26.png).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/136
https://github.com/google/deepvariant/issues/136:583,safety,error,error,583,"Cannot create VM with more than 4 P100 GPUs; When I tried to create a VM with 8 GPUs using this command line:. export IMAGE_FAMILY=""tf-latest-gpu"". export ZONE=""us-west1-a"". export INSTANCE_NAME=""deep"". export INSTANCE_TYPE=""n1-standard-8"". gcloud compute instances create $INSTANCE_NAME \. --zone=$ZONE \. --image-family=$IMAGE_FAMILY \. --image-project=deeplearning-platform-release \. --maintenance-policy=TERMINATE \. --accelerator=""type=nvidia-tesla-p100,count=8"" \. --machine-type=$INSTANCE_TYPE \. --boot-disk-size=200GB \. --metadata=""install-nvidia-driver=True"". I got this error:. ERROR: (gcloud.compute.instances.create) Could not fetch resource:. - Invalid value for field 'resource.guestAccelerators[0].acceleratorCount': '8'. Number of accelerator cards attached to an instance must be one of [1, 2, 4]. My Quota shows that I have access to 8 P100 GPUs:. ![screenshot from 2019-01-04 12-53-33](https://user-images.githubusercontent.com/19914123/50707923-cd8b4d80-101f-11e9-976d-b04df93f0f26.png).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/136
https://github.com/google/deepvariant/issues/136:591,safety,ERROR,ERROR,591,"Cannot create VM with more than 4 P100 GPUs; When I tried to create a VM with 8 GPUs using this command line:. export IMAGE_FAMILY=""tf-latest-gpu"". export ZONE=""us-west1-a"". export INSTANCE_NAME=""deep"". export INSTANCE_TYPE=""n1-standard-8"". gcloud compute instances create $INSTANCE_NAME \. --zone=$ZONE \. --image-family=$IMAGE_FAMILY \. --image-project=deeplearning-platform-release \. --maintenance-policy=TERMINATE \. --accelerator=""type=nvidia-tesla-p100,count=8"" \. --machine-type=$INSTANCE_TYPE \. --boot-disk-size=200GB \. --metadata=""install-nvidia-driver=True"". I got this error:. ERROR: (gcloud.compute.instances.create) Could not fetch resource:. - Invalid value for field 'resource.guestAccelerators[0].acceleratorCount': '8'. Number of accelerator cards attached to an instance must be one of [1, 2, 4]. My Quota shows that I have access to 8 P100 GPUs:. ![screenshot from 2019-01-04 12-53-33](https://user-images.githubusercontent.com/19914123/50707923-cd8b4d80-101f-11e9-976d-b04df93f0f26.png).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/136
https://github.com/google/deepvariant/issues/136:648,safety,resourc,resource,648,"Cannot create VM with more than 4 P100 GPUs; When I tried to create a VM with 8 GPUs using this command line:. export IMAGE_FAMILY=""tf-latest-gpu"". export ZONE=""us-west1-a"". export INSTANCE_NAME=""deep"". export INSTANCE_TYPE=""n1-standard-8"". gcloud compute instances create $INSTANCE_NAME \. --zone=$ZONE \. --image-family=$IMAGE_FAMILY \. --image-project=deeplearning-platform-release \. --maintenance-policy=TERMINATE \. --accelerator=""type=nvidia-tesla-p100,count=8"" \. --machine-type=$INSTANCE_TYPE \. --boot-disk-size=200GB \. --metadata=""install-nvidia-driver=True"". I got this error:. ERROR: (gcloud.compute.instances.create) Could not fetch resource:. - Invalid value for field 'resource.guestAccelerators[0].acceleratorCount': '8'. Number of accelerator cards attached to an instance must be one of [1, 2, 4]. My Quota shows that I have access to 8 P100 GPUs:. ![screenshot from 2019-01-04 12-53-33](https://user-images.githubusercontent.com/19914123/50707923-cd8b4d80-101f-11e9-976d-b04df93f0f26.png).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/136
https://github.com/google/deepvariant/issues/136:686,safety,resourc,resource,686,"Cannot create VM with more than 4 P100 GPUs; When I tried to create a VM with 8 GPUs using this command line:. export IMAGE_FAMILY=""tf-latest-gpu"". export ZONE=""us-west1-a"". export INSTANCE_NAME=""deep"". export INSTANCE_TYPE=""n1-standard-8"". gcloud compute instances create $INSTANCE_NAME \. --zone=$ZONE \. --image-family=$IMAGE_FAMILY \. --image-project=deeplearning-platform-release \. --maintenance-policy=TERMINATE \. --accelerator=""type=nvidia-tesla-p100,count=8"" \. --machine-type=$INSTANCE_TYPE \. --boot-disk-size=200GB \. --metadata=""install-nvidia-driver=True"". I got this error:. ERROR: (gcloud.compute.instances.create) Could not fetch resource:. - Invalid value for field 'resource.guestAccelerators[0].acceleratorCount': '8'. Number of accelerator cards attached to an instance must be one of [1, 2, 4]. My Quota shows that I have access to 8 P100 GPUs:. ![screenshot from 2019-01-04 12-53-33](https://user-images.githubusercontent.com/19914123/50707923-cd8b4d80-101f-11e9-976d-b04df93f0f26.png).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/136
https://github.com/google/deepvariant/issues/136:402,security,polic,policy,402,"Cannot create VM with more than 4 P100 GPUs; When I tried to create a VM with 8 GPUs using this command line:. export IMAGE_FAMILY=""tf-latest-gpu"". export ZONE=""us-west1-a"". export INSTANCE_NAME=""deep"". export INSTANCE_TYPE=""n1-standard-8"". gcloud compute instances create $INSTANCE_NAME \. --zone=$ZONE \. --image-family=$IMAGE_FAMILY \. --image-project=deeplearning-platform-release \. --maintenance-policy=TERMINATE \. --accelerator=""type=nvidia-tesla-p100,count=8"" \. --machine-type=$INSTANCE_TYPE \. --boot-disk-size=200GB \. --metadata=""install-nvidia-driver=True"". I got this error:. ERROR: (gcloud.compute.instances.create) Could not fetch resource:. - Invalid value for field 'resource.guestAccelerators[0].acceleratorCount': '8'. Number of accelerator cards attached to an instance must be one of [1, 2, 4]. My Quota shows that I have access to 8 P100 GPUs:. ![screenshot from 2019-01-04 12-53-33](https://user-images.githubusercontent.com/19914123/50707923-cd8b4d80-101f-11e9-976d-b04df93f0f26.png).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/136
https://github.com/google/deepvariant/issues/136:845,security,access,access,845,"Cannot create VM with more than 4 P100 GPUs; When I tried to create a VM with 8 GPUs using this command line:. export IMAGE_FAMILY=""tf-latest-gpu"". export ZONE=""us-west1-a"". export INSTANCE_NAME=""deep"". export INSTANCE_TYPE=""n1-standard-8"". gcloud compute instances create $INSTANCE_NAME \. --zone=$ZONE \. --image-family=$IMAGE_FAMILY \. --image-project=deeplearning-platform-release \. --maintenance-policy=TERMINATE \. --accelerator=""type=nvidia-tesla-p100,count=8"" \. --machine-type=$INSTANCE_TYPE \. --boot-disk-size=200GB \. --metadata=""install-nvidia-driver=True"". I got this error:. ERROR: (gcloud.compute.instances.create) Could not fetch resource:. - Invalid value for field 'resource.guestAccelerators[0].acceleratorCount': '8'. Number of accelerator cards attached to an instance must be one of [1, 2, 4]. My Quota shows that I have access to 8 P100 GPUs:. ![screenshot from 2019-01-04 12-53-33](https://user-images.githubusercontent.com/19914123/50707923-cd8b4d80-101f-11e9-976d-b04df93f0f26.png).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/136
https://github.com/google/deepvariant/issues/136:648,testability,resourc,resource,648,"Cannot create VM with more than 4 P100 GPUs; When I tried to create a VM with 8 GPUs using this command line:. export IMAGE_FAMILY=""tf-latest-gpu"". export ZONE=""us-west1-a"". export INSTANCE_NAME=""deep"". export INSTANCE_TYPE=""n1-standard-8"". gcloud compute instances create $INSTANCE_NAME \. --zone=$ZONE \. --image-family=$IMAGE_FAMILY \. --image-project=deeplearning-platform-release \. --maintenance-policy=TERMINATE \. --accelerator=""type=nvidia-tesla-p100,count=8"" \. --machine-type=$INSTANCE_TYPE \. --boot-disk-size=200GB \. --metadata=""install-nvidia-driver=True"". I got this error:. ERROR: (gcloud.compute.instances.create) Could not fetch resource:. - Invalid value for field 'resource.guestAccelerators[0].acceleratorCount': '8'. Number of accelerator cards attached to an instance must be one of [1, 2, 4]. My Quota shows that I have access to 8 P100 GPUs:. ![screenshot from 2019-01-04 12-53-33](https://user-images.githubusercontent.com/19914123/50707923-cd8b4d80-101f-11e9-976d-b04df93f0f26.png).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/136
https://github.com/google/deepvariant/issues/136:686,testability,resourc,resource,686,"Cannot create VM with more than 4 P100 GPUs; When I tried to create a VM with 8 GPUs using this command line:. export IMAGE_FAMILY=""tf-latest-gpu"". export ZONE=""us-west1-a"". export INSTANCE_NAME=""deep"". export INSTANCE_TYPE=""n1-standard-8"". gcloud compute instances create $INSTANCE_NAME \. --zone=$ZONE \. --image-family=$IMAGE_FAMILY \. --image-project=deeplearning-platform-release \. --maintenance-policy=TERMINATE \. --accelerator=""type=nvidia-tesla-p100,count=8"" \. --machine-type=$INSTANCE_TYPE \. --boot-disk-size=200GB \. --metadata=""install-nvidia-driver=True"". I got this error:. ERROR: (gcloud.compute.instances.create) Could not fetch resource:. - Invalid value for field 'resource.guestAccelerators[0].acceleratorCount': '8'. Number of accelerator cards attached to an instance must be one of [1, 2, 4]. My Quota shows that I have access to 8 P100 GPUs:. ![screenshot from 2019-01-04 12-53-33](https://user-images.githubusercontent.com/19914123/50707923-cd8b4d80-101f-11e9-976d-b04df93f0f26.png).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/136
https://github.com/google/deepvariant/issues/136:96,usability,command,command,96,"Cannot create VM with more than 4 P100 GPUs; When I tried to create a VM with 8 GPUs using this command line:. export IMAGE_FAMILY=""tf-latest-gpu"". export ZONE=""us-west1-a"". export INSTANCE_NAME=""deep"". export INSTANCE_TYPE=""n1-standard-8"". gcloud compute instances create $INSTANCE_NAME \. --zone=$ZONE \. --image-family=$IMAGE_FAMILY \. --image-project=deeplearning-platform-release \. --maintenance-policy=TERMINATE \. --accelerator=""type=nvidia-tesla-p100,count=8"" \. --machine-type=$INSTANCE_TYPE \. --boot-disk-size=200GB \. --metadata=""install-nvidia-driver=True"". I got this error:. ERROR: (gcloud.compute.instances.create) Could not fetch resource:. - Invalid value for field 'resource.guestAccelerators[0].acceleratorCount': '8'. Number of accelerator cards attached to an instance must be one of [1, 2, 4]. My Quota shows that I have access to 8 P100 GPUs:. ![screenshot from 2019-01-04 12-53-33](https://user-images.githubusercontent.com/19914123/50707923-cd8b4d80-101f-11e9-976d-b04df93f0f26.png).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/136
https://github.com/google/deepvariant/issues/136:583,usability,error,error,583,"Cannot create VM with more than 4 P100 GPUs; When I tried to create a VM with 8 GPUs using this command line:. export IMAGE_FAMILY=""tf-latest-gpu"". export ZONE=""us-west1-a"". export INSTANCE_NAME=""deep"". export INSTANCE_TYPE=""n1-standard-8"". gcloud compute instances create $INSTANCE_NAME \. --zone=$ZONE \. --image-family=$IMAGE_FAMILY \. --image-project=deeplearning-platform-release \. --maintenance-policy=TERMINATE \. --accelerator=""type=nvidia-tesla-p100,count=8"" \. --machine-type=$INSTANCE_TYPE \. --boot-disk-size=200GB \. --metadata=""install-nvidia-driver=True"". I got this error:. ERROR: (gcloud.compute.instances.create) Could not fetch resource:. - Invalid value for field 'resource.guestAccelerators[0].acceleratorCount': '8'. Number of accelerator cards attached to an instance must be one of [1, 2, 4]. My Quota shows that I have access to 8 P100 GPUs:. ![screenshot from 2019-01-04 12-53-33](https://user-images.githubusercontent.com/19914123/50707923-cd8b4d80-101f-11e9-976d-b04df93f0f26.png).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/136
https://github.com/google/deepvariant/issues/136:591,usability,ERROR,ERROR,591,"Cannot create VM with more than 4 P100 GPUs; When I tried to create a VM with 8 GPUs using this command line:. export IMAGE_FAMILY=""tf-latest-gpu"". export ZONE=""us-west1-a"". export INSTANCE_NAME=""deep"". export INSTANCE_TYPE=""n1-standard-8"". gcloud compute instances create $INSTANCE_NAME \. --zone=$ZONE \. --image-family=$IMAGE_FAMILY \. --image-project=deeplearning-platform-release \. --maintenance-policy=TERMINATE \. --accelerator=""type=nvidia-tesla-p100,count=8"" \. --machine-type=$INSTANCE_TYPE \. --boot-disk-size=200GB \. --metadata=""install-nvidia-driver=True"". I got this error:. ERROR: (gcloud.compute.instances.create) Could not fetch resource:. - Invalid value for field 'resource.guestAccelerators[0].acceleratorCount': '8'. Number of accelerator cards attached to an instance must be one of [1, 2, 4]. My Quota shows that I have access to 8 P100 GPUs:. ![screenshot from 2019-01-04 12-53-33](https://user-images.githubusercontent.com/19914123/50707923-cd8b4d80-101f-11e9-976d-b04df93f0f26.png).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/136
https://github.com/google/deepvariant/issues/136:916,usability,user,user-images,916,"Cannot create VM with more than 4 P100 GPUs; When I tried to create a VM with 8 GPUs using this command line:. export IMAGE_FAMILY=""tf-latest-gpu"". export ZONE=""us-west1-a"". export INSTANCE_NAME=""deep"". export INSTANCE_TYPE=""n1-standard-8"". gcloud compute instances create $INSTANCE_NAME \. --zone=$ZONE \. --image-family=$IMAGE_FAMILY \. --image-project=deeplearning-platform-release \. --maintenance-policy=TERMINATE \. --accelerator=""type=nvidia-tesla-p100,count=8"" \. --machine-type=$INSTANCE_TYPE \. --boot-disk-size=200GB \. --metadata=""install-nvidia-driver=True"". I got this error:. ERROR: (gcloud.compute.instances.create) Could not fetch resource:. - Invalid value for field 'resource.guestAccelerators[0].acceleratorCount': '8'. Number of accelerator cards attached to an instance must be one of [1, 2, 4]. My Quota shows that I have access to 8 P100 GPUs:. ![screenshot from 2019-01-04 12-53-33](https://user-images.githubusercontent.com/19914123/50707923-cd8b4d80-101f-11e9-976d-b04df93f0f26.png).",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/136
https://github.com/google/deepvariant/issues/137:120,availability,cluster,cluster,120,bioconda installation v0.7.2; I am trying to use bioconda Deepvariant (https://anaconda.org/bioconda/deepvariant ) on a cluster with CentOS 7. I am getting this error. ERROR conda.core.link:_execute(507): An error occurred while installing package 'bioconda::deepvariant-0.7.2-py27h5d9141f_1'. LinkError: post-link script failed for package bioconda::deepvariant-0.7.2-py27h5d9141f_1. running your command again with `-v` will provide additional information. location of failed script: /mnt/home/mansourt/miniconda3/envs/deepVar/bin/.deepvariant-post-link.sh. ==> script messages <==. <None>.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/137
https://github.com/google/deepvariant/issues/137:161,availability,error,error,161,bioconda installation v0.7.2; I am trying to use bioconda Deepvariant (https://anaconda.org/bioconda/deepvariant ) on a cluster with CentOS 7. I am getting this error. ERROR conda.core.link:_execute(507): An error occurred while installing package 'bioconda::deepvariant-0.7.2-py27h5d9141f_1'. LinkError: post-link script failed for package bioconda::deepvariant-0.7.2-py27h5d9141f_1. running your command again with `-v` will provide additional information. location of failed script: /mnt/home/mansourt/miniconda3/envs/deepVar/bin/.deepvariant-post-link.sh. ==> script messages <==. <None>.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/137
https://github.com/google/deepvariant/issues/137:168,availability,ERROR,ERROR,168,bioconda installation v0.7.2; I am trying to use bioconda Deepvariant (https://anaconda.org/bioconda/deepvariant ) on a cluster with CentOS 7. I am getting this error. ERROR conda.core.link:_execute(507): An error occurred while installing package 'bioconda::deepvariant-0.7.2-py27h5d9141f_1'. LinkError: post-link script failed for package bioconda::deepvariant-0.7.2-py27h5d9141f_1. running your command again with `-v` will provide additional information. location of failed script: /mnt/home/mansourt/miniconda3/envs/deepVar/bin/.deepvariant-post-link.sh. ==> script messages <==. <None>.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/137
https://github.com/google/deepvariant/issues/137:208,availability,error,error,208,bioconda installation v0.7.2; I am trying to use bioconda Deepvariant (https://anaconda.org/bioconda/deepvariant ) on a cluster with CentOS 7. I am getting this error. ERROR conda.core.link:_execute(507): An error occurred while installing package 'bioconda::deepvariant-0.7.2-py27h5d9141f_1'. LinkError: post-link script failed for package bioconda::deepvariant-0.7.2-py27h5d9141f_1. running your command again with `-v` will provide additional information. location of failed script: /mnt/home/mansourt/miniconda3/envs/deepVar/bin/.deepvariant-post-link.sh. ==> script messages <==. <None>.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/137
https://github.com/google/deepvariant/issues/137:9,deployability,instal,installation,9,bioconda installation v0.7.2; I am trying to use bioconda Deepvariant (https://anaconda.org/bioconda/deepvariant ) on a cluster with CentOS 7. I am getting this error. ERROR conda.core.link:_execute(507): An error occurred while installing package 'bioconda::deepvariant-0.7.2-py27h5d9141f_1'. LinkError: post-link script failed for package bioconda::deepvariant-0.7.2-py27h5d9141f_1. running your command again with `-v` will provide additional information. location of failed script: /mnt/home/mansourt/miniconda3/envs/deepVar/bin/.deepvariant-post-link.sh. ==> script messages <==. <None>.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/137
https://github.com/google/deepvariant/issues/137:120,deployability,cluster,cluster,120,bioconda installation v0.7.2; I am trying to use bioconda Deepvariant (https://anaconda.org/bioconda/deepvariant ) on a cluster with CentOS 7. I am getting this error. ERROR conda.core.link:_execute(507): An error occurred while installing package 'bioconda::deepvariant-0.7.2-py27h5d9141f_1'. LinkError: post-link script failed for package bioconda::deepvariant-0.7.2-py27h5d9141f_1. running your command again with `-v` will provide additional information. location of failed script: /mnt/home/mansourt/miniconda3/envs/deepVar/bin/.deepvariant-post-link.sh. ==> script messages <==. <None>.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/137
https://github.com/google/deepvariant/issues/137:229,deployability,instal,installing,229,bioconda installation v0.7.2; I am trying to use bioconda Deepvariant (https://anaconda.org/bioconda/deepvariant ) on a cluster with CentOS 7. I am getting this error. ERROR conda.core.link:_execute(507): An error occurred while installing package 'bioconda::deepvariant-0.7.2-py27h5d9141f_1'. LinkError: post-link script failed for package bioconda::deepvariant-0.7.2-py27h5d9141f_1. running your command again with `-v` will provide additional information. location of failed script: /mnt/home/mansourt/miniconda3/envs/deepVar/bin/.deepvariant-post-link.sh. ==> script messages <==. <None>.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/137
https://github.com/google/deepvariant/issues/137:322,deployability,fail,failed,322,bioconda installation v0.7.2; I am trying to use bioconda Deepvariant (https://anaconda.org/bioconda/deepvariant ) on a cluster with CentOS 7. I am getting this error. ERROR conda.core.link:_execute(507): An error occurred while installing package 'bioconda::deepvariant-0.7.2-py27h5d9141f_1'. LinkError: post-link script failed for package bioconda::deepvariant-0.7.2-py27h5d9141f_1. running your command again with `-v` will provide additional information. location of failed script: /mnt/home/mansourt/miniconda3/envs/deepVar/bin/.deepvariant-post-link.sh. ==> script messages <==. <None>.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/137
https://github.com/google/deepvariant/issues/137:471,deployability,fail,failed,471,bioconda installation v0.7.2; I am trying to use bioconda Deepvariant (https://anaconda.org/bioconda/deepvariant ) on a cluster with CentOS 7. I am getting this error. ERROR conda.core.link:_execute(507): An error occurred while installing package 'bioconda::deepvariant-0.7.2-py27h5d9141f_1'. LinkError: post-link script failed for package bioconda::deepvariant-0.7.2-py27h5d9141f_1. running your command again with `-v` will provide additional information. location of failed script: /mnt/home/mansourt/miniconda3/envs/deepVar/bin/.deepvariant-post-link.sh. ==> script messages <==. <None>.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/137
https://github.com/google/deepvariant/issues/137:180,energy efficiency,core,core,180,bioconda installation v0.7.2; I am trying to use bioconda Deepvariant (https://anaconda.org/bioconda/deepvariant ) on a cluster with CentOS 7. I am getting this error. ERROR conda.core.link:_execute(507): An error occurred while installing package 'bioconda::deepvariant-0.7.2-py27h5d9141f_1'. LinkError: post-link script failed for package bioconda::deepvariant-0.7.2-py27h5d9141f_1. running your command again with `-v` will provide additional information. location of failed script: /mnt/home/mansourt/miniconda3/envs/deepVar/bin/.deepvariant-post-link.sh. ==> script messages <==. <None>.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/137
https://github.com/google/deepvariant/issues/137:571,integrability,messag,messages,571,bioconda installation v0.7.2; I am trying to use bioconda Deepvariant (https://anaconda.org/bioconda/deepvariant ) on a cluster with CentOS 7. I am getting this error. ERROR conda.core.link:_execute(507): An error occurred while installing package 'bioconda::deepvariant-0.7.2-py27h5d9141f_1'. LinkError: post-link script failed for package bioconda::deepvariant-0.7.2-py27h5d9141f_1. running your command again with `-v` will provide additional information. location of failed script: /mnt/home/mansourt/miniconda3/envs/deepVar/bin/.deepvariant-post-link.sh. ==> script messages <==. <None>.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/137
https://github.com/google/deepvariant/issues/137:571,interoperability,messag,messages,571,bioconda installation v0.7.2; I am trying to use bioconda Deepvariant (https://anaconda.org/bioconda/deepvariant ) on a cluster with CentOS 7. I am getting this error. ERROR conda.core.link:_execute(507): An error occurred while installing package 'bioconda::deepvariant-0.7.2-py27h5d9141f_1'. LinkError: post-link script failed for package bioconda::deepvariant-0.7.2-py27h5d9141f_1. running your command again with `-v` will provide additional information. location of failed script: /mnt/home/mansourt/miniconda3/envs/deepVar/bin/.deepvariant-post-link.sh. ==> script messages <==. <None>.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/137
https://github.com/google/deepvariant/issues/137:240,modifiability,pac,package,240,bioconda installation v0.7.2; I am trying to use bioconda Deepvariant (https://anaconda.org/bioconda/deepvariant ) on a cluster with CentOS 7. I am getting this error. ERROR conda.core.link:_execute(507): An error occurred while installing package 'bioconda::deepvariant-0.7.2-py27h5d9141f_1'. LinkError: post-link script failed for package bioconda::deepvariant-0.7.2-py27h5d9141f_1. running your command again with `-v` will provide additional information. location of failed script: /mnt/home/mansourt/miniconda3/envs/deepVar/bin/.deepvariant-post-link.sh. ==> script messages <==. <None>.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/137
https://github.com/google/deepvariant/issues/137:333,modifiability,pac,package,333,bioconda installation v0.7.2; I am trying to use bioconda Deepvariant (https://anaconda.org/bioconda/deepvariant ) on a cluster with CentOS 7. I am getting this error. ERROR conda.core.link:_execute(507): An error occurred while installing package 'bioconda::deepvariant-0.7.2-py27h5d9141f_1'. LinkError: post-link script failed for package bioconda::deepvariant-0.7.2-py27h5d9141f_1. running your command again with `-v` will provide additional information. location of failed script: /mnt/home/mansourt/miniconda3/envs/deepVar/bin/.deepvariant-post-link.sh. ==> script messages <==. <None>.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/137
https://github.com/google/deepvariant/issues/137:161,performance,error,error,161,bioconda installation v0.7.2; I am trying to use bioconda Deepvariant (https://anaconda.org/bioconda/deepvariant ) on a cluster with CentOS 7. I am getting this error. ERROR conda.core.link:_execute(507): An error occurred while installing package 'bioconda::deepvariant-0.7.2-py27h5d9141f_1'. LinkError: post-link script failed for package bioconda::deepvariant-0.7.2-py27h5d9141f_1. running your command again with `-v` will provide additional information. location of failed script: /mnt/home/mansourt/miniconda3/envs/deepVar/bin/.deepvariant-post-link.sh. ==> script messages <==. <None>.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/137
https://github.com/google/deepvariant/issues/137:168,performance,ERROR,ERROR,168,bioconda installation v0.7.2; I am trying to use bioconda Deepvariant (https://anaconda.org/bioconda/deepvariant ) on a cluster with CentOS 7. I am getting this error. ERROR conda.core.link:_execute(507): An error occurred while installing package 'bioconda::deepvariant-0.7.2-py27h5d9141f_1'. LinkError: post-link script failed for package bioconda::deepvariant-0.7.2-py27h5d9141f_1. running your command again with `-v` will provide additional information. location of failed script: /mnt/home/mansourt/miniconda3/envs/deepVar/bin/.deepvariant-post-link.sh. ==> script messages <==. <None>.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/137
https://github.com/google/deepvariant/issues/137:208,performance,error,error,208,bioconda installation v0.7.2; I am trying to use bioconda Deepvariant (https://anaconda.org/bioconda/deepvariant ) on a cluster with CentOS 7. I am getting this error. ERROR conda.core.link:_execute(507): An error occurred while installing package 'bioconda::deepvariant-0.7.2-py27h5d9141f_1'. LinkError: post-link script failed for package bioconda::deepvariant-0.7.2-py27h5d9141f_1. running your command again with `-v` will provide additional information. location of failed script: /mnt/home/mansourt/miniconda3/envs/deepVar/bin/.deepvariant-post-link.sh. ==> script messages <==. <None>.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/137
https://github.com/google/deepvariant/issues/137:322,reliability,fail,failed,322,bioconda installation v0.7.2; I am trying to use bioconda Deepvariant (https://anaconda.org/bioconda/deepvariant ) on a cluster with CentOS 7. I am getting this error. ERROR conda.core.link:_execute(507): An error occurred while installing package 'bioconda::deepvariant-0.7.2-py27h5d9141f_1'. LinkError: post-link script failed for package bioconda::deepvariant-0.7.2-py27h5d9141f_1. running your command again with `-v` will provide additional information. location of failed script: /mnt/home/mansourt/miniconda3/envs/deepVar/bin/.deepvariant-post-link.sh. ==> script messages <==. <None>.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/137
https://github.com/google/deepvariant/issues/137:471,reliability,fail,failed,471,bioconda installation v0.7.2; I am trying to use bioconda Deepvariant (https://anaconda.org/bioconda/deepvariant ) on a cluster with CentOS 7. I am getting this error. ERROR conda.core.link:_execute(507): An error occurred while installing package 'bioconda::deepvariant-0.7.2-py27h5d9141f_1'. LinkError: post-link script failed for package bioconda::deepvariant-0.7.2-py27h5d9141f_1. running your command again with `-v` will provide additional information. location of failed script: /mnt/home/mansourt/miniconda3/envs/deepVar/bin/.deepvariant-post-link.sh. ==> script messages <==. <None>.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/137
https://github.com/google/deepvariant/issues/137:161,safety,error,error,161,bioconda installation v0.7.2; I am trying to use bioconda Deepvariant (https://anaconda.org/bioconda/deepvariant ) on a cluster with CentOS 7. I am getting this error. ERROR conda.core.link:_execute(507): An error occurred while installing package 'bioconda::deepvariant-0.7.2-py27h5d9141f_1'. LinkError: post-link script failed for package bioconda::deepvariant-0.7.2-py27h5d9141f_1. running your command again with `-v` will provide additional information. location of failed script: /mnt/home/mansourt/miniconda3/envs/deepVar/bin/.deepvariant-post-link.sh. ==> script messages <==. <None>.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/137
https://github.com/google/deepvariant/issues/137:168,safety,ERROR,ERROR,168,bioconda installation v0.7.2; I am trying to use bioconda Deepvariant (https://anaconda.org/bioconda/deepvariant ) on a cluster with CentOS 7. I am getting this error. ERROR conda.core.link:_execute(507): An error occurred while installing package 'bioconda::deepvariant-0.7.2-py27h5d9141f_1'. LinkError: post-link script failed for package bioconda::deepvariant-0.7.2-py27h5d9141f_1. running your command again with `-v` will provide additional information. location of failed script: /mnt/home/mansourt/miniconda3/envs/deepVar/bin/.deepvariant-post-link.sh. ==> script messages <==. <None>.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/137
https://github.com/google/deepvariant/issues/137:208,safety,error,error,208,bioconda installation v0.7.2; I am trying to use bioconda Deepvariant (https://anaconda.org/bioconda/deepvariant ) on a cluster with CentOS 7. I am getting this error. ERROR conda.core.link:_execute(507): An error occurred while installing package 'bioconda::deepvariant-0.7.2-py27h5d9141f_1'. LinkError: post-link script failed for package bioconda::deepvariant-0.7.2-py27h5d9141f_1. running your command again with `-v` will provide additional information. location of failed script: /mnt/home/mansourt/miniconda3/envs/deepVar/bin/.deepvariant-post-link.sh. ==> script messages <==. <None>.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/137
https://github.com/google/deepvariant/issues/137:161,usability,error,error,161,bioconda installation v0.7.2; I am trying to use bioconda Deepvariant (https://anaconda.org/bioconda/deepvariant ) on a cluster with CentOS 7. I am getting this error. ERROR conda.core.link:_execute(507): An error occurred while installing package 'bioconda::deepvariant-0.7.2-py27h5d9141f_1'. LinkError: post-link script failed for package bioconda::deepvariant-0.7.2-py27h5d9141f_1. running your command again with `-v` will provide additional information. location of failed script: /mnt/home/mansourt/miniconda3/envs/deepVar/bin/.deepvariant-post-link.sh. ==> script messages <==. <None>.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/137
https://github.com/google/deepvariant/issues/137:168,usability,ERROR,ERROR,168,bioconda installation v0.7.2; I am trying to use bioconda Deepvariant (https://anaconda.org/bioconda/deepvariant ) on a cluster with CentOS 7. I am getting this error. ERROR conda.core.link:_execute(507): An error occurred while installing package 'bioconda::deepvariant-0.7.2-py27h5d9141f_1'. LinkError: post-link script failed for package bioconda::deepvariant-0.7.2-py27h5d9141f_1. running your command again with `-v` will provide additional information. location of failed script: /mnt/home/mansourt/miniconda3/envs/deepVar/bin/.deepvariant-post-link.sh. ==> script messages <==. <None>.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/137
https://github.com/google/deepvariant/issues/137:208,usability,error,error,208,bioconda installation v0.7.2; I am trying to use bioconda Deepvariant (https://anaconda.org/bioconda/deepvariant ) on a cluster with CentOS 7. I am getting this error. ERROR conda.core.link:_execute(507): An error occurred while installing package 'bioconda::deepvariant-0.7.2-py27h5d9141f_1'. LinkError: post-link script failed for package bioconda::deepvariant-0.7.2-py27h5d9141f_1. running your command again with `-v` will provide additional information. location of failed script: /mnt/home/mansourt/miniconda3/envs/deepVar/bin/.deepvariant-post-link.sh. ==> script messages <==. <None>.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/137
https://github.com/google/deepvariant/issues/137:398,usability,command,command,398,bioconda installation v0.7.2; I am trying to use bioconda Deepvariant (https://anaconda.org/bioconda/deepvariant ) on a cluster with CentOS 7. I am getting this error. ERROR conda.core.link:_execute(507): An error occurred while installing package 'bioconda::deepvariant-0.7.2-py27h5d9141f_1'. LinkError: post-link script failed for package bioconda::deepvariant-0.7.2-py27h5d9141f_1. running your command again with `-v` will provide additional information. location of failed script: /mnt/home/mansourt/miniconda3/envs/deepVar/bin/.deepvariant-post-link.sh. ==> script messages <==. <None>.,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/137
https://github.com/google/deepvariant/issues/138:135,testability,trace,trace,135,"make_examples: 0 candidates; hello,. I am running the make_examples script on a _NA12878 chr20_ data. I got this data-set:. _ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/NA12878/NA12878_PacBio_MtSinai_. and this reference that is mentioned in the `NA12878.sorted.vcf` file from that data-set:. http://hgdownload.cse.ucsc.edu/goldenPath/hg19/bigZips/chromFa.tar.gz. I ran this command:. ```. python bin/make_examples.zip \. --mode training \. --ref ""data/chr20.fa"" \. --reads ""data/sorted_final_merged.bam"" \. --examples ""training-examples/training_set.with_label.tfrecord.gz"" \. --confident_regions ""data/NA12878.sorted.bed"" \. --regions ""chr20"" \. --truth_variants ""data/NA12878.sorted.vcf.gz"". ```. The command succeeded but no candidates were found at all, therefore it didn't produce no examples. Can someone explain why this is happening? Thank you",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/138
https://github.com/google/deepvariant/issues/138:378,usability,command,command,378,"make_examples: 0 candidates; hello,. I am running the make_examples script on a _NA12878 chr20_ data. I got this data-set:. _ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/NA12878/NA12878_PacBio_MtSinai_. and this reference that is mentioned in the `NA12878.sorted.vcf` file from that data-set:. http://hgdownload.cse.ucsc.edu/goldenPath/hg19/bigZips/chromFa.tar.gz. I ran this command:. ```. python bin/make_examples.zip \. --mode training \. --ref ""data/chr20.fa"" \. --reads ""data/sorted_final_merged.bam"" \. --examples ""training-examples/training_set.with_label.tfrecord.gz"" \. --confident_regions ""data/NA12878.sorted.bed"" \. --regions ""chr20"" \. --truth_variants ""data/NA12878.sorted.vcf.gz"". ```. The command succeeded but no candidates were found at all, therefore it didn't produce no examples. Can someone explain why this is happening? Thank you",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/138
https://github.com/google/deepvariant/issues/138:707,usability,command,command,707,"make_examples: 0 candidates; hello,. I am running the make_examples script on a _NA12878 chr20_ data. I got this data-set:. _ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/NA12878/NA12878_PacBio_MtSinai_. and this reference that is mentioned in the `NA12878.sorted.vcf` file from that data-set:. http://hgdownload.cse.ucsc.edu/goldenPath/hg19/bigZips/chromFa.tar.gz. I ran this command:. ```. python bin/make_examples.zip \. --mode training \. --ref ""data/chr20.fa"" \. --reads ""data/sorted_final_merged.bam"" \. --examples ""training-examples/training_set.with_label.tfrecord.gz"" \. --confident_regions ""data/NA12878.sorted.bed"" \. --regions ""chr20"" \. --truth_variants ""data/NA12878.sorted.vcf.gz"". ```. The command succeeded but no candidates were found at all, therefore it didn't produce no examples. Can someone explain why this is happening? Thank you",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/138
https://github.com/google/deepvariant/issues/139:248,availability,cluster,cluster,248,"training using bioconda build ; Hello team,. First, i really want to thank you for your gigantic effort in building and documenting deepvariant. I personally learned (and still learning) a lot from you. I am interested in training deepvariant on a cluster with no root privileges, so the docker image is not an option for me. Conda is my most efficient way to go, however, I am having the same I am having the exact same error described in issue #137 Is there is any update in regards of this error? My other question is the training scripts available on the conda build or not? If not, what do you think is the best way to go with training if I have no root privileges? . thank you again!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/139
https://github.com/google/deepvariant/issues/139:421,availability,error,error,421,"training using bioconda build ; Hello team,. First, i really want to thank you for your gigantic effort in building and documenting deepvariant. I personally learned (and still learning) a lot from you. I am interested in training deepvariant on a cluster with no root privileges, so the docker image is not an option for me. Conda is my most efficient way to go, however, I am having the same I am having the exact same error described in issue #137 Is there is any update in regards of this error? My other question is the training scripts available on the conda build or not? If not, what do you think is the best way to go with training if I have no root privileges? . thank you again!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/139
https://github.com/google/deepvariant/issues/139:493,availability,error,error,493,"training using bioconda build ; Hello team,. First, i really want to thank you for your gigantic effort in building and documenting deepvariant. I personally learned (and still learning) a lot from you. I am interested in training deepvariant on a cluster with no root privileges, so the docker image is not an option for me. Conda is my most efficient way to go, however, I am having the same I am having the exact same error described in issue #137 Is there is any update in regards of this error? My other question is the training scripts available on the conda build or not? If not, what do you think is the best way to go with training if I have no root privileges? . thank you again!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/139
https://github.com/google/deepvariant/issues/139:542,availability,avail,available,542,"training using bioconda build ; Hello team,. First, i really want to thank you for your gigantic effort in building and documenting deepvariant. I personally learned (and still learning) a lot from you. I am interested in training deepvariant on a cluster with no root privileges, so the docker image is not an option for me. Conda is my most efficient way to go, however, I am having the same I am having the exact same error described in issue #137 Is there is any update in regards of this error? My other question is the training scripts available on the conda build or not? If not, what do you think is the best way to go with training if I have no root privileges? . thank you again!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/139
https://github.com/google/deepvariant/issues/139:24,deployability,build,build,24,"training using bioconda build ; Hello team,. First, i really want to thank you for your gigantic effort in building and documenting deepvariant. I personally learned (and still learning) a lot from you. I am interested in training deepvariant on a cluster with no root privileges, so the docker image is not an option for me. Conda is my most efficient way to go, however, I am having the same I am having the exact same error described in issue #137 Is there is any update in regards of this error? My other question is the training scripts available on the conda build or not? If not, what do you think is the best way to go with training if I have no root privileges? . thank you again!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/139
https://github.com/google/deepvariant/issues/139:107,deployability,build,building,107,"training using bioconda build ; Hello team,. First, i really want to thank you for your gigantic effort in building and documenting deepvariant. I personally learned (and still learning) a lot from you. I am interested in training deepvariant on a cluster with no root privileges, so the docker image is not an option for me. Conda is my most efficient way to go, however, I am having the same I am having the exact same error described in issue #137 Is there is any update in regards of this error? My other question is the training scripts available on the conda build or not? If not, what do you think is the best way to go with training if I have no root privileges? . thank you again!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/139
https://github.com/google/deepvariant/issues/139:248,deployability,cluster,cluster,248,"training using bioconda build ; Hello team,. First, i really want to thank you for your gigantic effort in building and documenting deepvariant. I personally learned (and still learning) a lot from you. I am interested in training deepvariant on a cluster with no root privileges, so the docker image is not an option for me. Conda is my most efficient way to go, however, I am having the same I am having the exact same error described in issue #137 Is there is any update in regards of this error? My other question is the training scripts available on the conda build or not? If not, what do you think is the best way to go with training if I have no root privileges? . thank you again!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/139
https://github.com/google/deepvariant/issues/139:467,deployability,updat,update,467,"training using bioconda build ; Hello team,. First, i really want to thank you for your gigantic effort in building and documenting deepvariant. I personally learned (and still learning) a lot from you. I am interested in training deepvariant on a cluster with no root privileges, so the docker image is not an option for me. Conda is my most efficient way to go, however, I am having the same I am having the exact same error described in issue #137 Is there is any update in regards of this error? My other question is the training scripts available on the conda build or not? If not, what do you think is the best way to go with training if I have no root privileges? . thank you again!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/139
https://github.com/google/deepvariant/issues/139:565,deployability,build,build,565,"training using bioconda build ; Hello team,. First, i really want to thank you for your gigantic effort in building and documenting deepvariant. I personally learned (and still learning) a lot from you. I am interested in training deepvariant on a cluster with no root privileges, so the docker image is not an option for me. Conda is my most efficient way to go, however, I am having the same I am having the exact same error described in issue #137 Is there is any update in regards of this error? My other question is the training scripts available on the conda build or not? If not, what do you think is the best way to go with training if I have no root privileges? . thank you again!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/139
https://github.com/google/deepvariant/issues/139:421,performance,error,error,421,"training using bioconda build ; Hello team,. First, i really want to thank you for your gigantic effort in building and documenting deepvariant. I personally learned (and still learning) a lot from you. I am interested in training deepvariant on a cluster with no root privileges, so the docker image is not an option for me. Conda is my most efficient way to go, however, I am having the same I am having the exact same error described in issue #137 Is there is any update in regards of this error? My other question is the training scripts available on the conda build or not? If not, what do you think is the best way to go with training if I have no root privileges? . thank you again!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/139
https://github.com/google/deepvariant/issues/139:493,performance,error,error,493,"training using bioconda build ; Hello team,. First, i really want to thank you for your gigantic effort in building and documenting deepvariant. I personally learned (and still learning) a lot from you. I am interested in training deepvariant on a cluster with no root privileges, so the docker image is not an option for me. Conda is my most efficient way to go, however, I am having the same I am having the exact same error described in issue #137 Is there is any update in regards of this error? My other question is the training scripts available on the conda build or not? If not, what do you think is the best way to go with training if I have no root privileges? . thank you again!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/139
https://github.com/google/deepvariant/issues/139:542,reliability,availab,available,542,"training using bioconda build ; Hello team,. First, i really want to thank you for your gigantic effort in building and documenting deepvariant. I personally learned (and still learning) a lot from you. I am interested in training deepvariant on a cluster with no root privileges, so the docker image is not an option for me. Conda is my most efficient way to go, however, I am having the same I am having the exact same error described in issue #137 Is there is any update in regards of this error? My other question is the training scripts available on the conda build or not? If not, what do you think is the best way to go with training if I have no root privileges? . thank you again!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/139
https://github.com/google/deepvariant/issues/139:421,safety,error,error,421,"training using bioconda build ; Hello team,. First, i really want to thank you for your gigantic effort in building and documenting deepvariant. I personally learned (and still learning) a lot from you. I am interested in training deepvariant on a cluster with no root privileges, so the docker image is not an option for me. Conda is my most efficient way to go, however, I am having the same I am having the exact same error described in issue #137 Is there is any update in regards of this error? My other question is the training scripts available on the conda build or not? If not, what do you think is the best way to go with training if I have no root privileges? . thank you again!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/139
https://github.com/google/deepvariant/issues/139:467,safety,updat,update,467,"training using bioconda build ; Hello team,. First, i really want to thank you for your gigantic effort in building and documenting deepvariant. I personally learned (and still learning) a lot from you. I am interested in training deepvariant on a cluster with no root privileges, so the docker image is not an option for me. Conda is my most efficient way to go, however, I am having the same I am having the exact same error described in issue #137 Is there is any update in regards of this error? My other question is the training scripts available on the conda build or not? If not, what do you think is the best way to go with training if I have no root privileges? . thank you again!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/139
https://github.com/google/deepvariant/issues/139:493,safety,error,error,493,"training using bioconda build ; Hello team,. First, i really want to thank you for your gigantic effort in building and documenting deepvariant. I personally learned (and still learning) a lot from you. I am interested in training deepvariant on a cluster with no root privileges, so the docker image is not an option for me. Conda is my most efficient way to go, however, I am having the same I am having the exact same error described in issue #137 Is there is any update in regards of this error? My other question is the training scripts available on the conda build or not? If not, what do you think is the best way to go with training if I have no root privileges? . thank you again!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/139
https://github.com/google/deepvariant/issues/139:542,safety,avail,available,542,"training using bioconda build ; Hello team,. First, i really want to thank you for your gigantic effort in building and documenting deepvariant. I personally learned (and still learning) a lot from you. I am interested in training deepvariant on a cluster with no root privileges, so the docker image is not an option for me. Conda is my most efficient way to go, however, I am having the same I am having the exact same error described in issue #137 Is there is any update in regards of this error? My other question is the training scripts available on the conda build or not? If not, what do you think is the best way to go with training if I have no root privileges? . thank you again!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/139
https://github.com/google/deepvariant/issues/139:38,security,team,team,38,"training using bioconda build ; Hello team,. First, i really want to thank you for your gigantic effort in building and documenting deepvariant. I personally learned (and still learning) a lot from you. I am interested in training deepvariant on a cluster with no root privileges, so the docker image is not an option for me. Conda is my most efficient way to go, however, I am having the same I am having the exact same error described in issue #137 Is there is any update in regards of this error? My other question is the training scripts available on the conda build or not? If not, what do you think is the best way to go with training if I have no root privileges? . thank you again!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/139
https://github.com/google/deepvariant/issues/139:269,security,privil,privileges,269,"training using bioconda build ; Hello team,. First, i really want to thank you for your gigantic effort in building and documenting deepvariant. I personally learned (and still learning) a lot from you. I am interested in training deepvariant on a cluster with no root privileges, so the docker image is not an option for me. Conda is my most efficient way to go, however, I am having the same I am having the exact same error described in issue #137 Is there is any update in regards of this error? My other question is the training scripts available on the conda build or not? If not, what do you think is the best way to go with training if I have no root privileges? . thank you again!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/139
https://github.com/google/deepvariant/issues/139:467,security,updat,update,467,"training using bioconda build ; Hello team,. First, i really want to thank you for your gigantic effort in building and documenting deepvariant. I personally learned (and still learning) a lot from you. I am interested in training deepvariant on a cluster with no root privileges, so the docker image is not an option for me. Conda is my most efficient way to go, however, I am having the same I am having the exact same error described in issue #137 Is there is any update in regards of this error? My other question is the training scripts available on the conda build or not? If not, what do you think is the best way to go with training if I have no root privileges? . thank you again!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/139
https://github.com/google/deepvariant/issues/139:542,security,availab,available,542,"training using bioconda build ; Hello team,. First, i really want to thank you for your gigantic effort in building and documenting deepvariant. I personally learned (and still learning) a lot from you. I am interested in training deepvariant on a cluster with no root privileges, so the docker image is not an option for me. Conda is my most efficient way to go, however, I am having the same I am having the exact same error described in issue #137 Is there is any update in regards of this error? My other question is the training scripts available on the conda build or not? If not, what do you think is the best way to go with training if I have no root privileges? . thank you again!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/139
https://github.com/google/deepvariant/issues/139:659,security,privil,privileges,659,"training using bioconda build ; Hello team,. First, i really want to thank you for your gigantic effort in building and documenting deepvariant. I personally learned (and still learning) a lot from you. I am interested in training deepvariant on a cluster with no root privileges, so the docker image is not an option for me. Conda is my most efficient way to go, however, I am having the same I am having the exact same error described in issue #137 Is there is any update in regards of this error? My other question is the training scripts available on the conda build or not? If not, what do you think is the best way to go with training if I have no root privileges? . thank you again!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/139
https://github.com/google/deepvariant/issues/139:120,usability,document,documenting,120,"training using bioconda build ; Hello team,. First, i really want to thank you for your gigantic effort in building and documenting deepvariant. I personally learned (and still learning) a lot from you. I am interested in training deepvariant on a cluster with no root privileges, so the docker image is not an option for me. Conda is my most efficient way to go, however, I am having the same I am having the exact same error described in issue #137 Is there is any update in regards of this error? My other question is the training scripts available on the conda build or not? If not, what do you think is the best way to go with training if I have no root privileges? . thank you again!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/139
https://github.com/google/deepvariant/issues/139:147,usability,person,personally,147,"training using bioconda build ; Hello team,. First, i really want to thank you for your gigantic effort in building and documenting deepvariant. I personally learned (and still learning) a lot from you. I am interested in training deepvariant on a cluster with no root privileges, so the docker image is not an option for me. Conda is my most efficient way to go, however, I am having the same I am having the exact same error described in issue #137 Is there is any update in regards of this error? My other question is the training scripts available on the conda build or not? If not, what do you think is the best way to go with training if I have no root privileges? . thank you again!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/139
https://github.com/google/deepvariant/issues/139:158,usability,learn,learned,158,"training using bioconda build ; Hello team,. First, i really want to thank you for your gigantic effort in building and documenting deepvariant. I personally learned (and still learning) a lot from you. I am interested in training deepvariant on a cluster with no root privileges, so the docker image is not an option for me. Conda is my most efficient way to go, however, I am having the same I am having the exact same error described in issue #137 Is there is any update in regards of this error? My other question is the training scripts available on the conda build or not? If not, what do you think is the best way to go with training if I have no root privileges? . thank you again!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/139
https://github.com/google/deepvariant/issues/139:177,usability,learn,learning,177,"training using bioconda build ; Hello team,. First, i really want to thank you for your gigantic effort in building and documenting deepvariant. I personally learned (and still learning) a lot from you. I am interested in training deepvariant on a cluster with no root privileges, so the docker image is not an option for me. Conda is my most efficient way to go, however, I am having the same I am having the exact same error described in issue #137 Is there is any update in regards of this error? My other question is the training scripts available on the conda build or not? If not, what do you think is the best way to go with training if I have no root privileges? . thank you again!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/139
https://github.com/google/deepvariant/issues/139:343,usability,efficien,efficient,343,"training using bioconda build ; Hello team,. First, i really want to thank you for your gigantic effort in building and documenting deepvariant. I personally learned (and still learning) a lot from you. I am interested in training deepvariant on a cluster with no root privileges, so the docker image is not an option for me. Conda is my most efficient way to go, however, I am having the same I am having the exact same error described in issue #137 Is there is any update in regards of this error? My other question is the training scripts available on the conda build or not? If not, what do you think is the best way to go with training if I have no root privileges? . thank you again!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/139
https://github.com/google/deepvariant/issues/139:421,usability,error,error,421,"training using bioconda build ; Hello team,. First, i really want to thank you for your gigantic effort in building and documenting deepvariant. I personally learned (and still learning) a lot from you. I am interested in training deepvariant on a cluster with no root privileges, so the docker image is not an option for me. Conda is my most efficient way to go, however, I am having the same I am having the exact same error described in issue #137 Is there is any update in regards of this error? My other question is the training scripts available on the conda build or not? If not, what do you think is the best way to go with training if I have no root privileges? . thank you again!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/139
https://github.com/google/deepvariant/issues/139:493,usability,error,error,493,"training using bioconda build ; Hello team,. First, i really want to thank you for your gigantic effort in building and documenting deepvariant. I personally learned (and still learning) a lot from you. I am interested in training deepvariant on a cluster with no root privileges, so the docker image is not an option for me. Conda is my most efficient way to go, however, I am having the same I am having the exact same error described in issue #137 Is there is any update in regards of this error? My other question is the training scripts available on the conda build or not? If not, what do you think is the best way to go with training if I have no root privileges? . thank you again!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/139
https://github.com/google/deepvariant/pull/140:7,modifiability,Pac,PacBio,7,Adding PacBio blog post and minor fixes to MalariaGen post.;,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/140
https://github.com/google/deepvariant/issues/141:62,availability,error,error,62,"ValueError: Not found: Could not open BAM file; Hi, I got the error in below. ```. # docker run \. -v ${HOME}:${HOME} \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --logging_level DEBUG \. --mode calling \. --ref ""${REF}"" \. --reads /data/home/dhflanrnr/Deep_data/Bam_data/sample.printrecal.bam \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". ```. > [E::hts_open_format] Failed to open file /data/home/dhflanrnr/Deep_data/Bam_data/sample.printrecal.bam. > Traceback (most recent call last):. > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1205, in <module>. > tf.app.run(). > File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. > _sys.exit(main(argv)). > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1156, in main. > options = default_options(add_flags=True, flags_obj=FLAGS). > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 287, in default_options. > with sam.SamReader(flags_obj.reads) as sam_reader:. > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 211, in __init__. > self._reader = self._native_reader(input_path, **kwargs). > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 249, in _native_reader. > return NativeSamReader(input_path, **kwargs). > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. > random_seed=random_seed)). > ValueError: Not found: Could not open /data/home/dhflanrnr/Deep_data/Bam_data/sample.printrecal.bam. Example BAM header. ![2019-01-15 15 24 32](https://user-images.githubusercontent.com/4966343/51162512-cf83c500-18d9-11e9-900a-24418b2a82cd.png). My BAM header. ![2019-01-15 15 24 38](https://user-ima",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/141
https://github.com/google/deepvariant/issues/141:428,deployability,Fail,Failed,428,"ValueError: Not found: Could not open BAM file; Hi, I got the error in below. ```. # docker run \. -v ${HOME}:${HOME} \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --logging_level DEBUG \. --mode calling \. --ref ""${REF}"" \. --reads /data/home/dhflanrnr/Deep_data/Bam_data/sample.printrecal.bam \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". ```. > [E::hts_open_format] Failed to open file /data/home/dhflanrnr/Deep_data/Bam_data/sample.printrecal.bam. > Traceback (most recent call last):. > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1205, in <module>. > tf.app.run(). > File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. > _sys.exit(main(argv)). > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1156, in main. > options = default_options(add_flags=True, flags_obj=FLAGS). > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 287, in default_options. > with sam.SamReader(flags_obj.reads) as sam_reader:. > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 211, in __init__. > self._reader = self._native_reader(input_path, **kwargs). > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 249, in _native_reader. > return NativeSamReader(input_path, **kwargs). > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. > random_seed=random_seed)). > ValueError: Not found: Could not open /data/home/dhflanrnr/Deep_data/Bam_data/sample.printrecal.bam. Example BAM header. ![2019-01-15 15 24 32](https://user-images.githubusercontent.com/4966343/51162512-cf83c500-18d9-11e9-900a-24418b2a82cd.png). My BAM header. ![2019-01-15 15 24 38](https://user-ima",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/141
https://github.com/google/deepvariant/issues/141:662,deployability,modul,module,662,"ValueError: Not found: Could not open BAM file; Hi, I got the error in below. ```. # docker run \. -v ${HOME}:${HOME} \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --logging_level DEBUG \. --mode calling \. --ref ""${REF}"" \. --reads /data/home/dhflanrnr/Deep_data/Bam_data/sample.printrecal.bam \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". ```. > [E::hts_open_format] Failed to open file /data/home/dhflanrnr/Deep_data/Bam_data/sample.printrecal.bam. > Traceback (most recent call last):. > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1205, in <module>. > tf.app.run(). > File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. > _sys.exit(main(argv)). > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1156, in main. > options = default_options(add_flags=True, flags_obj=FLAGS). > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 287, in default_options. > with sam.SamReader(flags_obj.reads) as sam_reader:. > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 211, in __init__. > self._reader = self._native_reader(input_path, **kwargs). > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 249, in _native_reader. > return NativeSamReader(input_path, **kwargs). > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. > random_seed=random_seed)). > ValueError: Not found: Could not open /data/home/dhflanrnr/Deep_data/Bam_data/sample.printrecal.bam. Example BAM header. ![2019-01-15 15 24 32](https://user-images.githubusercontent.com/4966343/51162512-cf83c500-18d9-11e9-900a-24418b2a82cd.png). My BAM header. ![2019-01-15 15 24 38](https://user-ima",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/141
https://github.com/google/deepvariant/issues/141:754,interoperability,platform,platform,754,"ValueError: Not found: Could not open BAM file; Hi, I got the error in below. ```. # docker run \. -v ${HOME}:${HOME} \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --logging_level DEBUG \. --mode calling \. --ref ""${REF}"" \. --reads /data/home/dhflanrnr/Deep_data/Bam_data/sample.printrecal.bam \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". ```. > [E::hts_open_format] Failed to open file /data/home/dhflanrnr/Deep_data/Bam_data/sample.printrecal.bam. > Traceback (most recent call last):. > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1205, in <module>. > tf.app.run(). > File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. > _sys.exit(main(argv)). > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1156, in main. > options = default_options(add_flags=True, flags_obj=FLAGS). > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 287, in default_options. > with sam.SamReader(flags_obj.reads) as sam_reader:. > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 211, in __init__. > self._reader = self._native_reader(input_path, **kwargs). > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 249, in _native_reader. > return NativeSamReader(input_path, **kwargs). > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. > random_seed=random_seed)). > ValueError: Not found: Could not open /data/home/dhflanrnr/Deep_data/Bam_data/sample.printrecal.bam. Example BAM header. ![2019-01-15 15 24 32](https://user-images.githubusercontent.com/4966343/51162512-cf83c500-18d9-11e9-900a-24418b2a82cd.png). My BAM header. ![2019-01-15 15 24 38](https://user-ima",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/141
https://github.com/google/deepvariant/issues/141:662,modifiability,modul,module,662,"ValueError: Not found: Could not open BAM file; Hi, I got the error in below. ```. # docker run \. -v ${HOME}:${HOME} \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --logging_level DEBUG \. --mode calling \. --ref ""${REF}"" \. --reads /data/home/dhflanrnr/Deep_data/Bam_data/sample.printrecal.bam \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". ```. > [E::hts_open_format] Failed to open file /data/home/dhflanrnr/Deep_data/Bam_data/sample.printrecal.bam. > Traceback (most recent call last):. > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1205, in <module>. > tf.app.run(). > File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. > _sys.exit(main(argv)). > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1156, in main. > options = default_options(add_flags=True, flags_obj=FLAGS). > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 287, in default_options. > with sam.SamReader(flags_obj.reads) as sam_reader:. > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 211, in __init__. > self._reader = self._native_reader(input_path, **kwargs). > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 249, in _native_reader. > return NativeSamReader(input_path, **kwargs). > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. > random_seed=random_seed)). > ValueError: Not found: Could not open /data/home/dhflanrnr/Deep_data/Bam_data/sample.printrecal.bam. Example BAM header. ![2019-01-15 15 24 32](https://user-images.githubusercontent.com/4966343/51162512-cf83c500-18d9-11e9-900a-24418b2a82cd.png). My BAM header. ![2019-01-15 15 24 38](https://user-ima",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/141
https://github.com/google/deepvariant/issues/141:727,modifiability,pac,packages,727,"ValueError: Not found: Could not open BAM file; Hi, I got the error in below. ```. # docker run \. -v ${HOME}:${HOME} \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --logging_level DEBUG \. --mode calling \. --ref ""${REF}"" \. --reads /data/home/dhflanrnr/Deep_data/Bam_data/sample.printrecal.bam \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". ```. > [E::hts_open_format] Failed to open file /data/home/dhflanrnr/Deep_data/Bam_data/sample.printrecal.bam. > Traceback (most recent call last):. > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1205, in <module>. > tf.app.run(). > File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. > _sys.exit(main(argv)). > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1156, in main. > options = default_options(add_flags=True, flags_obj=FLAGS). > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 287, in default_options. > with sam.SamReader(flags_obj.reads) as sam_reader:. > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 211, in __init__. > self._reader = self._native_reader(input_path, **kwargs). > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 249, in _native_reader. > return NativeSamReader(input_path, **kwargs). > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. > random_seed=random_seed)). > ValueError: Not found: Could not open /data/home/dhflanrnr/Deep_data/Bam_data/sample.printrecal.bam. Example BAM header. ![2019-01-15 15 24 32](https://user-images.githubusercontent.com/4966343/51162512-cf83c500-18d9-11e9-900a-24418b2a82cd.png). My BAM header. ![2019-01-15 15 24 38](https://user-ima",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/141
https://github.com/google/deepvariant/issues/141:62,performance,error,error,62,"ValueError: Not found: Could not open BAM file; Hi, I got the error in below. ```. # docker run \. -v ${HOME}:${HOME} \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --logging_level DEBUG \. --mode calling \. --ref ""${REF}"" \. --reads /data/home/dhflanrnr/Deep_data/Bam_data/sample.printrecal.bam \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". ```. > [E::hts_open_format] Failed to open file /data/home/dhflanrnr/Deep_data/Bam_data/sample.printrecal.bam. > Traceback (most recent call last):. > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1205, in <module>. > tf.app.run(). > File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. > _sys.exit(main(argv)). > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1156, in main. > options = default_options(add_flags=True, flags_obj=FLAGS). > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 287, in default_options. > with sam.SamReader(flags_obj.reads) as sam_reader:. > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 211, in __init__. > self._reader = self._native_reader(input_path, **kwargs). > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 249, in _native_reader. > return NativeSamReader(input_path, **kwargs). > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. > random_seed=random_seed)). > ValueError: Not found: Could not open /data/home/dhflanrnr/Deep_data/Bam_data/sample.printrecal.bam. Example BAM header. ![2019-01-15 15 24 32](https://user-images.githubusercontent.com/4966343/51162512-cf83c500-18d9-11e9-900a-24418b2a82cd.png). My BAM header. ![2019-01-15 15 24 38](https://user-ima",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/141
https://github.com/google/deepvariant/issues/141:428,reliability,Fail,Failed,428,"ValueError: Not found: Could not open BAM file; Hi, I got the error in below. ```. # docker run \. -v ${HOME}:${HOME} \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --logging_level DEBUG \. --mode calling \. --ref ""${REF}"" \. --reads /data/home/dhflanrnr/Deep_data/Bam_data/sample.printrecal.bam \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". ```. > [E::hts_open_format] Failed to open file /data/home/dhflanrnr/Deep_data/Bam_data/sample.printrecal.bam. > Traceback (most recent call last):. > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1205, in <module>. > tf.app.run(). > File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. > _sys.exit(main(argv)). > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1156, in main. > options = default_options(add_flags=True, flags_obj=FLAGS). > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 287, in default_options. > with sam.SamReader(flags_obj.reads) as sam_reader:. > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 211, in __init__. > self._reader = self._native_reader(input_path, **kwargs). > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 249, in _native_reader. > return NativeSamReader(input_path, **kwargs). > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. > random_seed=random_seed)). > ValueError: Not found: Could not open /data/home/dhflanrnr/Deep_data/Bam_data/sample.printrecal.bam. Example BAM header. ![2019-01-15 15 24 32](https://user-images.githubusercontent.com/4966343/51162512-cf83c500-18d9-11e9-900a-24418b2a82cd.png). My BAM header. ![2019-01-15 15 24 38](https://user-ima",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/141
https://github.com/google/deepvariant/issues/141:62,safety,error,error,62,"ValueError: Not found: Could not open BAM file; Hi, I got the error in below. ```. # docker run \. -v ${HOME}:${HOME} \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --logging_level DEBUG \. --mode calling \. --ref ""${REF}"" \. --reads /data/home/dhflanrnr/Deep_data/Bam_data/sample.printrecal.bam \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". ```. > [E::hts_open_format] Failed to open file /data/home/dhflanrnr/Deep_data/Bam_data/sample.printrecal.bam. > Traceback (most recent call last):. > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1205, in <module>. > tf.app.run(). > File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. > _sys.exit(main(argv)). > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1156, in main. > options = default_options(add_flags=True, flags_obj=FLAGS). > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 287, in default_options. > with sam.SamReader(flags_obj.reads) as sam_reader:. > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 211, in __init__. > self._reader = self._native_reader(input_path, **kwargs). > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 249, in _native_reader. > return NativeSamReader(input_path, **kwargs). > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. > random_seed=random_seed)). > ValueError: Not found: Could not open /data/home/dhflanrnr/Deep_data/Bam_data/sample.printrecal.bam. Example BAM header. ![2019-01-15 15 24 32](https://user-images.githubusercontent.com/4966343/51162512-cf83c500-18d9-11e9-900a-24418b2a82cd.png). My BAM header. ![2019-01-15 15 24 38](https://user-ima",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/141
https://github.com/google/deepvariant/issues/141:662,safety,modul,module,662,"ValueError: Not found: Could not open BAM file; Hi, I got the error in below. ```. # docker run \. -v ${HOME}:${HOME} \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --logging_level DEBUG \. --mode calling \. --ref ""${REF}"" \. --reads /data/home/dhflanrnr/Deep_data/Bam_data/sample.printrecal.bam \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". ```. > [E::hts_open_format] Failed to open file /data/home/dhflanrnr/Deep_data/Bam_data/sample.printrecal.bam. > Traceback (most recent call last):. > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1205, in <module>. > tf.app.run(). > File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. > _sys.exit(main(argv)). > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1156, in main. > options = default_options(add_flags=True, flags_obj=FLAGS). > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 287, in default_options. > with sam.SamReader(flags_obj.reads) as sam_reader:. > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 211, in __init__. > self._reader = self._native_reader(input_path, **kwargs). > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 249, in _native_reader. > return NativeSamReader(input_path, **kwargs). > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. > random_seed=random_seed)). > ValueError: Not found: Could not open /data/home/dhflanrnr/Deep_data/Bam_data/sample.printrecal.bam. Example BAM header. ![2019-01-15 15 24 32](https://user-images.githubusercontent.com/4966343/51162512-cf83c500-18d9-11e9-900a-24418b2a82cd.png). My BAM header. ![2019-01-15 15 24 38](https://user-ima",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/141
https://github.com/google/deepvariant/issues/141:513,testability,Trace,Traceback,513,"ValueError: Not found: Could not open BAM file; Hi, I got the error in below. ```. # docker run \. -v ${HOME}:${HOME} \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --logging_level DEBUG \. --mode calling \. --ref ""${REF}"" \. --reads /data/home/dhflanrnr/Deep_data/Bam_data/sample.printrecal.bam \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". ```. > [E::hts_open_format] Failed to open file /data/home/dhflanrnr/Deep_data/Bam_data/sample.printrecal.bam. > Traceback (most recent call last):. > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1205, in <module>. > tf.app.run(). > File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. > _sys.exit(main(argv)). > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1156, in main. > options = default_options(add_flags=True, flags_obj=FLAGS). > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 287, in default_options. > with sam.SamReader(flags_obj.reads) as sam_reader:. > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 211, in __init__. > self._reader = self._native_reader(input_path, **kwargs). > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 249, in _native_reader. > return NativeSamReader(input_path, **kwargs). > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. > random_seed=random_seed)). > ValueError: Not found: Could not open /data/home/dhflanrnr/Deep_data/Bam_data/sample.printrecal.bam. Example BAM header. ![2019-01-15 15 24 32](https://user-images.githubusercontent.com/4966343/51162512-cf83c500-18d9-11e9-900a-24418b2a82cd.png). My BAM header. ![2019-01-15 15 24 38](https://user-ima",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/141
https://github.com/google/deepvariant/issues/141:62,usability,error,error,62,"ValueError: Not found: Could not open BAM file; Hi, I got the error in below. ```. # docker run \. -v ${HOME}:${HOME} \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --logging_level DEBUG \. --mode calling \. --ref ""${REF}"" \. --reads /data/home/dhflanrnr/Deep_data/Bam_data/sample.printrecal.bam \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". ```. > [E::hts_open_format] Failed to open file /data/home/dhflanrnr/Deep_data/Bam_data/sample.printrecal.bam. > Traceback (most recent call last):. > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1205, in <module>. > tf.app.run(). > File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. > _sys.exit(main(argv)). > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1156, in main. > options = default_options(add_flags=True, flags_obj=FLAGS). > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 287, in default_options. > with sam.SamReader(flags_obj.reads) as sam_reader:. > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 211, in __init__. > self._reader = self._native_reader(input_path, **kwargs). > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 249, in _native_reader. > return NativeSamReader(input_path, **kwargs). > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. > random_seed=random_seed)). > ValueError: Not found: Could not open /data/home/dhflanrnr/Deep_data/Bam_data/sample.printrecal.bam. Example BAM header. ![2019-01-15 15 24 32](https://user-images.githubusercontent.com/4966343/51162512-cf83c500-18d9-11e9-900a-24418b2a82cd.png). My BAM header. ![2019-01-15 15 24 38](https://user-ima",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/141
https://github.com/google/deepvariant/issues/141:1852,usability,user,user-images,1852,"OME} \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --logging_level DEBUG \. --mode calling \. --ref ""${REF}"" \. --reads /data/home/dhflanrnr/Deep_data/Bam_data/sample.printrecal.bam \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". ```. > [E::hts_open_format] Failed to open file /data/home/dhflanrnr/Deep_data/Bam_data/sample.printrecal.bam. > Traceback (most recent call last):. > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1205, in <module>. > tf.app.run(). > File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. > _sys.exit(main(argv)). > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1156, in main. > options = default_options(add_flags=True, flags_obj=FLAGS). > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 287, in default_options. > with sam.SamReader(flags_obj.reads) as sam_reader:. > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 211, in __init__. > self._reader = self._native_reader(input_path, **kwargs). > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 249, in _native_reader. > return NativeSamReader(input_path, **kwargs). > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. > random_seed=random_seed)). > ValueError: Not found: Could not open /data/home/dhflanrnr/Deep_data/Bam_data/sample.printrecal.bam. Example BAM header. ![2019-01-15 15 24 32](https://user-images.githubusercontent.com/4966343/51162512-cf83c500-18d9-11e9-900a-24418b2a82cd.png). My BAM header. ![2019-01-15 15 24 38](https://user-images.githubusercontent.com/4966343/51162515-d27eb580-18d9-11e9-9076-b635d17ea87b.png). How can I solve this issue?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/141
https://github.com/google/deepvariant/issues/141:1992,usability,user,user-images,1992,"OME} \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/make_examples \. --logging_level DEBUG \. --mode calling \. --ref ""${REF}"" \. --reads /data/home/dhflanrnr/Deep_data/Bam_data/sample.printrecal.bam \. --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". ```. > [E::hts_open_format] Failed to open file /data/home/dhflanrnr/Deep_data/Bam_data/sample.printrecal.bam. > Traceback (most recent call last):. > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1205, in <module>. > tf.app.run(). > File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. > _sys.exit(main(argv)). > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1156, in main. > options = default_options(add_flags=True, flags_obj=FLAGS). > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 287, in default_options. > with sam.SamReader(flags_obj.reads) as sam_reader:. > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 211, in __init__. > self._reader = self._native_reader(input_path, **kwargs). > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 249, in _native_reader. > return NativeSamReader(input_path, **kwargs). > File ""/tmp/Bazel.runfiles_VAS8eS/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__. > random_seed=random_seed)). > ValueError: Not found: Could not open /data/home/dhflanrnr/Deep_data/Bam_data/sample.printrecal.bam. Example BAM header. ![2019-01-15 15 24 32](https://user-images.githubusercontent.com/4966343/51162512-cf83c500-18d9-11e9-900a-24418b2a82cd.png). My BAM header. ![2019-01-15 15 24 38](https://user-images.githubusercontent.com/4966343/51162515-d27eb580-18d9-11e9-9076-b635d17ea87b.png). How can I solve this issue?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/141
https://github.com/google/deepvariant/issues/142:106,energy efficiency,optim,optimize,106,"Using DeepVariant on Large Sample Size (WES or WGS); - Can you suggest what would be the best strategy to optimize deepvariant variant calling on large sample sizes (>500 WES/ WGS samples)? - Is Deepvariant optimized for N+1 Strategy or joint calling (I read there is an option for gVCF output) and if so, what would be the best way or recommended tool to convert multiple deepvariant's gVCFs (500 gVCFs) to one VCF??",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/142
https://github.com/google/deepvariant/issues/142:207,energy efficiency,optim,optimized,207,"Using DeepVariant on Large Sample Size (WES or WGS); - Can you suggest what would be the best strategy to optimize deepvariant variant calling on large sample sizes (>500 WES/ WGS samples)? - Is Deepvariant optimized for N+1 Strategy or joint calling (I read there is an option for gVCF output) and if so, what would be the best way or recommended tool to convert multiple deepvariant's gVCFs (500 gVCFs) to one VCF??",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/142
https://github.com/google/deepvariant/issues/142:106,performance,optimiz,optimize,106,"Using DeepVariant on Large Sample Size (WES or WGS); - Can you suggest what would be the best strategy to optimize deepvariant variant calling on large sample sizes (>500 WES/ WGS samples)? - Is Deepvariant optimized for N+1 Strategy or joint calling (I read there is an option for gVCF output) and if so, what would be the best way or recommended tool to convert multiple deepvariant's gVCFs (500 gVCFs) to one VCF??",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/142
https://github.com/google/deepvariant/issues/142:207,performance,optimiz,optimized,207,"Using DeepVariant on Large Sample Size (WES or WGS); - Can you suggest what would be the best strategy to optimize deepvariant variant calling on large sample sizes (>500 WES/ WGS samples)? - Is Deepvariant optimized for N+1 Strategy or joint calling (I read there is an option for gVCF output) and if so, what would be the best way or recommended tool to convert multiple deepvariant's gVCFs (500 gVCFs) to one VCF??",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/142
https://github.com/google/deepvariant/issues/142:348,usability,tool,tool,348,"Using DeepVariant on Large Sample Size (WES or WGS); - Can you suggest what would be the best strategy to optimize deepvariant variant calling on large sample sizes (>500 WES/ WGS samples)? - Is Deepvariant optimized for N+1 Strategy or joint calling (I read there is an option for gVCF output) and if so, what would be the best way or recommended tool to convert multiple deepvariant's gVCFs (500 gVCFs) to one VCF??",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/142
https://github.com/google/deepvariant/issues/143:364,availability,echo,echo,364,"Sharding issue for make_examples; Hi,. I'm trying to run make_examples with 64 shards using 64 cores (1 worker per shard default I assume) and the following settings:. docker: ""gcr.io/deepvariant-docker/deepvariant_gpu:0.6.0"". zones: [""us-central1-c""] . I am trying to use essentially the same code as is listed in the documentation:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads /reads.bam \. --examples ""${sample_id}.examples.tfrecord${numShards}"" \. --gvcf ""${sample_id}.gvcf.tfrecord${numShards}"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. However, I am getting the following error for each of the shards: . `ValueError: ('Output is not sharded but shard > 0', 4)`. What does this mean? I assume it means I am not actually sharding though it seems I am. Any help appreciated, thanks. Another example with full stack trace:. ```. Traceback (most recent call last):. File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1120, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1071, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 335, in default_options. flags_obj.gvcf or ''). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py"", line 212, in resolve_filespecs. raise ValueError('Output is not ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/143
https://github.com/google/deepvariant/issues/143:781,availability,echo,echo,781,"Sharding issue for make_examples; Hi,. I'm trying to run make_examples with 64 shards using 64 cores (1 worker per shard default I assume) and the following settings:. docker: ""gcr.io/deepvariant-docker/deepvariant_gpu:0.6.0"". zones: [""us-central1-c""] . I am trying to use essentially the same code as is listed in the documentation:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads /reads.bam \. --examples ""${sample_id}.examples.tfrecord${numShards}"" \. --gvcf ""${sample_id}.gvcf.tfrecord${numShards}"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. However, I am getting the following error for each of the shards: . `ValueError: ('Output is not sharded but shard > 0', 4)`. What does this mean? I assume it means I am not actually sharding though it seems I am. Any help appreciated, thanks. Another example with full stack trace:. ```. Traceback (most recent call last):. File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1120, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1071, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 335, in default_options. flags_obj.gvcf or ''). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py"", line 212, in resolve_filespecs. raise ValueError('Output is not ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/143
https://github.com/google/deepvariant/issues/143:795,availability,echo,echo,795,"Sharding issue for make_examples; Hi,. I'm trying to run make_examples with 64 shards using 64 cores (1 worker per shard default I assume) and the following settings:. docker: ""gcr.io/deepvariant-docker/deepvariant_gpu:0.6.0"". zones: [""us-central1-c""] . I am trying to use essentially the same code as is listed in the documentation:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads /reads.bam \. --examples ""${sample_id}.examples.tfrecord${numShards}"" \. --gvcf ""${sample_id}.gvcf.tfrecord${numShards}"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. However, I am getting the following error for each of the shards: . `ValueError: ('Output is not sharded but shard > 0', 4)`. What does this mean? I assume it means I am not actually sharding though it seems I am. Any help appreciated, thanks. Another example with full stack trace:. ```. Traceback (most recent call last):. File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1120, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1071, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 335, in default_options. flags_obj.gvcf or ''). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py"", line 212, in resolve_filespecs. raise ValueError('Output is not ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/143
https://github.com/google/deepvariant/issues/143:842,availability,error,error,842,"Sharding issue for make_examples; Hi,. I'm trying to run make_examples with 64 shards using 64 cores (1 worker per shard default I assume) and the following settings:. docker: ""gcr.io/deepvariant-docker/deepvariant_gpu:0.6.0"". zones: [""us-central1-c""] . I am trying to use essentially the same code as is listed in the documentation:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads /reads.bam \. --examples ""${sample_id}.examples.tfrecord${numShards}"" \. --gvcf ""${sample_id}.gvcf.tfrecord${numShards}"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. However, I am getting the following error for each of the shards: . `ValueError: ('Output is not sharded but shard > 0', 4)`. What does this mean? I assume it means I am not actually sharding though it seems I am. Any help appreciated, thanks. Another example with full stack trace:. ```. Traceback (most recent call last):. File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1120, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1071, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 335, in default_options. flags_obj.gvcf or ''). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py"", line 212, in resolve_filespecs. raise ValueError('Output is not ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/143
https://github.com/google/deepvariant/issues/143:400,deployability,Log,Log,400,"Sharding issue for make_examples; Hi,. I'm trying to run make_examples with 64 shards using 64 cores (1 worker per shard default I assume) and the following settings:. docker: ""gcr.io/deepvariant-docker/deepvariant_gpu:0.6.0"". zones: [""us-central1-c""] . I am trying to use essentially the same code as is listed in the documentation:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads /reads.bam \. --examples ""${sample_id}.examples.tfrecord${numShards}"" \. --gvcf ""${sample_id}.gvcf.tfrecord${numShards}"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. However, I am getting the following error for each of the shards: . `ValueError: ('Output is not sharded but shard > 0', 4)`. What does this mean? I assume it means I am not actually sharding though it seems I am. Any help appreciated, thanks. Another example with full stack trace:. ```. Traceback (most recent call last):. File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1120, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1071, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 335, in default_options. flags_obj.gvcf or ''). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py"", line 212, in resolve_filespecs. raise ValueError('Output is not ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/143
https://github.com/google/deepvariant/issues/143:454,deployability,log,log,454,"Sharding issue for make_examples; Hi,. I'm trying to run make_examples with 64 shards using 64 cores (1 worker per shard default I assume) and the following settings:. docker: ""gcr.io/deepvariant-docker/deepvariant_gpu:0.6.0"". zones: [""us-central1-c""] . I am trying to use essentially the same code as is listed in the documentation:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads /reads.bam \. --examples ""${sample_id}.examples.tfrecord${numShards}"" \. --gvcf ""${sample_id}.gvcf.tfrecord${numShards}"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. However, I am getting the following error for each of the shards: . `ValueError: ('Output is not sharded but shard > 0', 4)`. What does this mean? I assume it means I am not actually sharding though it seems I am. Any help appreciated, thanks. Another example with full stack trace:. ```. Traceback (most recent call last):. File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1120, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1071, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 335, in default_options. flags_obj.gvcf or ''). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py"", line 212, in resolve_filespecs. raise ValueError('Output is not ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/143
https://github.com/google/deepvariant/issues/143:775,deployability,log,log,775,"Sharding issue for make_examples; Hi,. I'm trying to run make_examples with 64 shards using 64 cores (1 worker per shard default I assume) and the following settings:. docker: ""gcr.io/deepvariant-docker/deepvariant_gpu:0.6.0"". zones: [""us-central1-c""] . I am trying to use essentially the same code as is listed in the documentation:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads /reads.bam \. --examples ""${sample_id}.examples.tfrecord${numShards}"" \. --gvcf ""${sample_id}.gvcf.tfrecord${numShards}"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. However, I am getting the following error for each of the shards: . `ValueError: ('Output is not sharded but shard > 0', 4)`. What does this mean? I assume it means I am not actually sharding though it seems I am. Any help appreciated, thanks. Another example with full stack trace:. ```. Traceback (most recent call last):. File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1120, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1071, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 335, in default_options. flags_obj.gvcf or ''). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py"", line 212, in resolve_filespecs. raise ValueError('Output is not ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/143
https://github.com/google/deepvariant/issues/143:1076,deployability,stack,stack,1076,"shards using 64 cores (1 worker per shard default I assume) and the following settings:. docker: ""gcr.io/deepvariant-docker/deepvariant_gpu:0.6.0"". zones: [""us-central1-c""] . I am trying to use essentially the same code as is listed in the documentation:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads /reads.bam \. --examples ""${sample_id}.examples.tfrecord${numShards}"" \. --gvcf ""${sample_id}.gvcf.tfrecord${numShards}"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. However, I am getting the following error for each of the shards: . `ValueError: ('Output is not sharded but shard > 0', 4)`. What does this mean? I assume it means I am not actually sharding though it seems I am. Any help appreciated, thanks. Another example with full stack trace:. ```. Traceback (most recent call last):. File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1120, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1071, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 335, in default_options. flags_obj.gvcf or ''). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py"", line 212, in resolve_filespecs. raise ValueError('Output is not sharded but shard > 0', shard). ValueError: ('Output is not sharded but shard >",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/143
https://github.com/google/deepvariant/issues/143:1265,deployability,modul,module,1265,"64 cores (1 worker per shard default I assume) and the following settings:. docker: ""gcr.io/deepvariant-docker/deepvariant_gpu:0.6.0"". zones: [""us-central1-c""] . I am trying to use essentially the same code as is listed in the documentation:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads /reads.bam \. --examples ""${sample_id}.examples.tfrecord${numShards}"" \. --gvcf ""${sample_id}.gvcf.tfrecord${numShards}"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. However, I am getting the following error for each of the shards: . `ValueError: ('Output is not sharded but shard > 0', 4)`. What does this mean? I assume it means I am not actually sharding though it seems I am. Any help appreciated, thanks. Another example with full stack trace:. ```. Traceback (most recent call last):. File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1120, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1071, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 335, in default_options. flags_obj.gvcf or ''). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py"", line 212, in resolve_filespecs. raise ValueError('Output is not sharded but shard > 0', shard). ValueError: ('Output is not sharded but shard > 0', 44). ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/143
https://github.com/google/deepvariant/issues/143:95,energy efficiency,core,cores,95,"Sharding issue for make_examples; Hi,. I'm trying to run make_examples with 64 shards using 64 cores (1 worker per shard default I assume) and the following settings:. docker: ""gcr.io/deepvariant-docker/deepvariant_gpu:0.6.0"". zones: [""us-central1-c""] . I am trying to use essentially the same code as is listed in the documentation:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads /reads.bam \. --examples ""${sample_id}.examples.tfrecord${numShards}"" \. --gvcf ""${sample_id}.gvcf.tfrecord${numShards}"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. However, I am getting the following error for each of the shards: . `ValueError: ('Output is not sharded but shard > 0', 4)`. What does this mean? I assume it means I am not actually sharding though it seems I am. Any help appreciated, thanks. Another example with full stack trace:. ```. Traceback (most recent call last):. File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1120, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1071, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 335, in default_options. flags_obj.gvcf or ''). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py"", line 212, in resolve_filespecs. raise ValueError('Output is not ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/143
https://github.com/google/deepvariant/issues/143:518,integrability,buffer,buffer,518,"Sharding issue for make_examples; Hi,. I'm trying to run make_examples with 64 shards using 64 cores (1 worker per shard default I assume) and the following settings:. docker: ""gcr.io/deepvariant-docker/deepvariant_gpu:0.6.0"". zones: [""us-central1-c""] . I am trying to use essentially the same code as is listed in the documentation:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads /reads.bam \. --examples ""${sample_id}.examples.tfrecord${numShards}"" \. --gvcf ""${sample_id}.gvcf.tfrecord${numShards}"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. However, I am getting the following error for each of the shards: . `ValueError: ('Output is not sharded but shard > 0', 4)`. What does this mean? I assume it means I am not actually sharding though it seems I am. Any help appreciated, thanks. Another example with full stack trace:. ```. Traceback (most recent call last):. File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1120, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1071, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 335, in default_options. flags_obj.gvcf or ''). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py"", line 212, in resolve_filespecs. raise ValueError('Output is not ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/143
https://github.com/google/deepvariant/issues/143:1351,interoperability,platform,platform,1351,"64 cores (1 worker per shard default I assume) and the following settings:. docker: ""gcr.io/deepvariant-docker/deepvariant_gpu:0.6.0"". zones: [""us-central1-c""] . I am trying to use essentially the same code as is listed in the documentation:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads /reads.bam \. --examples ""${sample_id}.examples.tfrecord${numShards}"" \. --gvcf ""${sample_id}.gvcf.tfrecord${numShards}"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. However, I am getting the following error for each of the shards: . `ValueError: ('Output is not sharded but shard > 0', 4)`. What does this mean? I assume it means I am not actually sharding though it seems I am. Any help appreciated, thanks. Another example with full stack trace:. ```. Traceback (most recent call last):. File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1120, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1071, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 335, in default_options. flags_obj.gvcf or ''). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py"", line 212, in resolve_filespecs. raise ValueError('Output is not sharded but shard > 0', shard). ValueError: ('Output is not sharded but shard > 0', 44). ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/143
https://github.com/google/deepvariant/issues/143:1265,modifiability,modul,module,1265,"64 cores (1 worker per shard default I assume) and the following settings:. docker: ""gcr.io/deepvariant-docker/deepvariant_gpu:0.6.0"". zones: [""us-central1-c""] . I am trying to use essentially the same code as is listed in the documentation:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads /reads.bam \. --examples ""${sample_id}.examples.tfrecord${numShards}"" \. --gvcf ""${sample_id}.gvcf.tfrecord${numShards}"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. However, I am getting the following error for each of the shards: . `ValueError: ('Output is not sharded but shard > 0', 4)`. What does this mean? I assume it means I am not actually sharding though it seems I am. Any help appreciated, thanks. Another example with full stack trace:. ```. Traceback (most recent call last):. File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1120, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1071, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 335, in default_options. flags_obj.gvcf or ''). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py"", line 212, in resolve_filespecs. raise ValueError('Output is not sharded but shard > 0', shard). ValueError: ('Output is not sharded but shard > 0', 44). ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/143
https://github.com/google/deepvariant/issues/143:1324,modifiability,pac,packages,1324,"64 cores (1 worker per shard default I assume) and the following settings:. docker: ""gcr.io/deepvariant-docker/deepvariant_gpu:0.6.0"". zones: [""us-central1-c""] . I am trying to use essentially the same code as is listed in the documentation:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads /reads.bam \. --examples ""${sample_id}.examples.tfrecord${numShards}"" \. --gvcf ""${sample_id}.gvcf.tfrecord${numShards}"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. However, I am getting the following error for each of the shards: . `ValueError: ('Output is not sharded but shard > 0', 4)`. What does this mean? I assume it means I am not actually sharding though it seems I am. Any help appreciated, thanks. Another example with full stack trace:. ```. Traceback (most recent call last):. File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1120, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1071, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 335, in default_options. flags_obj.gvcf or ''). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py"", line 212, in resolve_filespecs. raise ValueError('Output is not sharded but shard > 0', shard). ValueError: ('Output is not sharded but shard > 0', 44). ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/143
https://github.com/google/deepvariant/issues/143:463,performance,time,time,463,"Sharding issue for make_examples; Hi,. I'm trying to run make_examples with 64 shards using 64 cores (1 worker per shard default I assume) and the following settings:. docker: ""gcr.io/deepvariant-docker/deepvariant_gpu:0.6.0"". zones: [""us-central1-c""] . I am trying to use essentially the same code as is listed in the documentation:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads /reads.bam \. --examples ""${sample_id}.examples.tfrecord${numShards}"" \. --gvcf ""${sample_id}.gvcf.tfrecord${numShards}"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. However, I am getting the following error for each of the shards: . `ValueError: ('Output is not sharded but shard > 0', 4)`. What does this mean? I assume it means I am not actually sharding though it seems I am. Any help appreciated, thanks. Another example with full stack trace:. ```. Traceback (most recent call last):. File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1120, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1071, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 335, in default_options. flags_obj.gvcf or ''). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py"", line 212, in resolve_filespecs. raise ValueError('Output is not ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/143
https://github.com/google/deepvariant/issues/143:499,performance,parallel,parallel,499,"Sharding issue for make_examples; Hi,. I'm trying to run make_examples with 64 shards using 64 cores (1 worker per shard default I assume) and the following settings:. docker: ""gcr.io/deepvariant-docker/deepvariant_gpu:0.6.0"". zones: [""us-central1-c""] . I am trying to use essentially the same code as is listed in the documentation:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads /reads.bam \. --examples ""${sample_id}.examples.tfrecord${numShards}"" \. --gvcf ""${sample_id}.gvcf.tfrecord${numShards}"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. However, I am getting the following error for each of the shards: . `ValueError: ('Output is not sharded but shard > 0', 4)`. What does this mean? I assume it means I am not actually sharding though it seems I am. Any help appreciated, thanks. Another example with full stack trace:. ```. Traceback (most recent call last):. File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1120, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1071, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 335, in default_options. flags_obj.gvcf or ''). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py"", line 212, in resolve_filespecs. raise ValueError('Output is not ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/143
https://github.com/google/deepvariant/issues/143:842,performance,error,error,842,"Sharding issue for make_examples; Hi,. I'm trying to run make_examples with 64 shards using 64 cores (1 worker per shard default I assume) and the following settings:. docker: ""gcr.io/deepvariant-docker/deepvariant_gpu:0.6.0"". zones: [""us-central1-c""] . I am trying to use essentially the same code as is listed in the documentation:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads /reads.bam \. --examples ""${sample_id}.examples.tfrecord${numShards}"" \. --gvcf ""${sample_id}.gvcf.tfrecord${numShards}"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. However, I am getting the following error for each of the shards: . `ValueError: ('Output is not sharded but shard > 0', 4)`. What does this mean? I assume it means I am not actually sharding though it seems I am. Any help appreciated, thanks. Another example with full stack trace:. ```. Traceback (most recent call last):. File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1120, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1071, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 335, in default_options. flags_obj.gvcf or ''). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py"", line 212, in resolve_filespecs. raise ValueError('Output is not ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/143
https://github.com/google/deepvariant/issues/143:937,reliability,doe,does,937,"Sharding issue for make_examples; Hi,. I'm trying to run make_examples with 64 shards using 64 cores (1 worker per shard default I assume) and the following settings:. docker: ""gcr.io/deepvariant-docker/deepvariant_gpu:0.6.0"". zones: [""us-central1-c""] . I am trying to use essentially the same code as is listed in the documentation:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads /reads.bam \. --examples ""${sample_id}.examples.tfrecord${numShards}"" \. --gvcf ""${sample_id}.gvcf.tfrecord${numShards}"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. However, I am getting the following error for each of the shards: . `ValueError: ('Output is not sharded but shard > 0', 4)`. What does this mean? I assume it means I am not actually sharding though it seems I am. Any help appreciated, thanks. Another example with full stack trace:. ```. Traceback (most recent call last):. File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1120, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1071, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 335, in default_options. flags_obj.gvcf or ''). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py"", line 212, in resolve_filespecs. raise ValueError('Output is not ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/143
https://github.com/google/deepvariant/issues/143:400,safety,Log,Log,400,"Sharding issue for make_examples; Hi,. I'm trying to run make_examples with 64 shards using 64 cores (1 worker per shard default I assume) and the following settings:. docker: ""gcr.io/deepvariant-docker/deepvariant_gpu:0.6.0"". zones: [""us-central1-c""] . I am trying to use essentially the same code as is listed in the documentation:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads /reads.bam \. --examples ""${sample_id}.examples.tfrecord${numShards}"" \. --gvcf ""${sample_id}.gvcf.tfrecord${numShards}"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. However, I am getting the following error for each of the shards: . `ValueError: ('Output is not sharded but shard > 0', 4)`. What does this mean? I assume it means I am not actually sharding though it seems I am. Any help appreciated, thanks. Another example with full stack trace:. ```. Traceback (most recent call last):. File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1120, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1071, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 335, in default_options. flags_obj.gvcf or ''). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py"", line 212, in resolve_filespecs. raise ValueError('Output is not ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/143
https://github.com/google/deepvariant/issues/143:454,safety,log,log,454,"Sharding issue for make_examples; Hi,. I'm trying to run make_examples with 64 shards using 64 cores (1 worker per shard default I assume) and the following settings:. docker: ""gcr.io/deepvariant-docker/deepvariant_gpu:0.6.0"". zones: [""us-central1-c""] . I am trying to use essentially the same code as is listed in the documentation:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads /reads.bam \. --examples ""${sample_id}.examples.tfrecord${numShards}"" \. --gvcf ""${sample_id}.gvcf.tfrecord${numShards}"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. However, I am getting the following error for each of the shards: . `ValueError: ('Output is not sharded but shard > 0', 4)`. What does this mean? I assume it means I am not actually sharding though it seems I am. Any help appreciated, thanks. Another example with full stack trace:. ```. Traceback (most recent call last):. File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1120, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1071, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 335, in default_options. flags_obj.gvcf or ''). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py"", line 212, in resolve_filespecs. raise ValueError('Output is not ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/143
https://github.com/google/deepvariant/issues/143:775,safety,log,log,775,"Sharding issue for make_examples; Hi,. I'm trying to run make_examples with 64 shards using 64 cores (1 worker per shard default I assume) and the following settings:. docker: ""gcr.io/deepvariant-docker/deepvariant_gpu:0.6.0"". zones: [""us-central1-c""] . I am trying to use essentially the same code as is listed in the documentation:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads /reads.bam \. --examples ""${sample_id}.examples.tfrecord${numShards}"" \. --gvcf ""${sample_id}.gvcf.tfrecord${numShards}"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. However, I am getting the following error for each of the shards: . `ValueError: ('Output is not sharded but shard > 0', 4)`. What does this mean? I assume it means I am not actually sharding though it seems I am. Any help appreciated, thanks. Another example with full stack trace:. ```. Traceback (most recent call last):. File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1120, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1071, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 335, in default_options. flags_obj.gvcf or ''). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py"", line 212, in resolve_filespecs. raise ValueError('Output is not ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/143
https://github.com/google/deepvariant/issues/143:842,safety,error,error,842,"Sharding issue for make_examples; Hi,. I'm trying to run make_examples with 64 shards using 64 cores (1 worker per shard default I assume) and the following settings:. docker: ""gcr.io/deepvariant-docker/deepvariant_gpu:0.6.0"". zones: [""us-central1-c""] . I am trying to use essentially the same code as is listed in the documentation:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads /reads.bam \. --examples ""${sample_id}.examples.tfrecord${numShards}"" \. --gvcf ""${sample_id}.gvcf.tfrecord${numShards}"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. However, I am getting the following error for each of the shards: . `ValueError: ('Output is not sharded but shard > 0', 4)`. What does this mean? I assume it means I am not actually sharding though it seems I am. Any help appreciated, thanks. Another example with full stack trace:. ```. Traceback (most recent call last):. File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1120, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1071, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 335, in default_options. flags_obj.gvcf or ''). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py"", line 212, in resolve_filespecs. raise ValueError('Output is not ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/143
https://github.com/google/deepvariant/issues/143:1265,safety,modul,module,1265,"64 cores (1 worker per shard default I assume) and the following settings:. docker: ""gcr.io/deepvariant-docker/deepvariant_gpu:0.6.0"". zones: [""us-central1-c""] . I am trying to use essentially the same code as is listed in the documentation:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads /reads.bam \. --examples ""${sample_id}.examples.tfrecord${numShards}"" \. --gvcf ""${sample_id}.gvcf.tfrecord${numShards}"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. However, I am getting the following error for each of the shards: . `ValueError: ('Output is not sharded but shard > 0', 4)`. What does this mean? I assume it means I am not actually sharding though it seems I am. Any help appreciated, thanks. Another example with full stack trace:. ```. Traceback (most recent call last):. File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1120, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1071, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 335, in default_options. flags_obj.gvcf or ''). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py"", line 212, in resolve_filespecs. raise ValueError('Output is not sharded but shard > 0', shard). ValueError: ('Output is not sharded but shard > 0', 44). ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/143
https://github.com/google/deepvariant/issues/143:400,security,Log,Log,400,"Sharding issue for make_examples; Hi,. I'm trying to run make_examples with 64 shards using 64 cores (1 worker per shard default I assume) and the following settings:. docker: ""gcr.io/deepvariant-docker/deepvariant_gpu:0.6.0"". zones: [""us-central1-c""] . I am trying to use essentially the same code as is listed in the documentation:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads /reads.bam \. --examples ""${sample_id}.examples.tfrecord${numShards}"" \. --gvcf ""${sample_id}.gvcf.tfrecord${numShards}"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. However, I am getting the following error for each of the shards: . `ValueError: ('Output is not sharded but shard > 0', 4)`. What does this mean? I assume it means I am not actually sharding though it seems I am. Any help appreciated, thanks. Another example with full stack trace:. ```. Traceback (most recent call last):. File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1120, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1071, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 335, in default_options. flags_obj.gvcf or ''). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py"", line 212, in resolve_filespecs. raise ValueError('Output is not ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/143
https://github.com/google/deepvariant/issues/143:454,security,log,log,454,"Sharding issue for make_examples; Hi,. I'm trying to run make_examples with 64 shards using 64 cores (1 worker per shard default I assume) and the following settings:. docker: ""gcr.io/deepvariant-docker/deepvariant_gpu:0.6.0"". zones: [""us-central1-c""] . I am trying to use essentially the same code as is listed in the documentation:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads /reads.bam \. --examples ""${sample_id}.examples.tfrecord${numShards}"" \. --gvcf ""${sample_id}.gvcf.tfrecord${numShards}"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. However, I am getting the following error for each of the shards: . `ValueError: ('Output is not sharded but shard > 0', 4)`. What does this mean? I assume it means I am not actually sharding though it seems I am. Any help appreciated, thanks. Another example with full stack trace:. ```. Traceback (most recent call last):. File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1120, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1071, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 335, in default_options. flags_obj.gvcf or ''). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py"", line 212, in resolve_filespecs. raise ValueError('Output is not ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/143
https://github.com/google/deepvariant/issues/143:775,security,log,log,775,"Sharding issue for make_examples; Hi,. I'm trying to run make_examples with 64 shards using 64 cores (1 worker per shard default I assume) and the following settings:. docker: ""gcr.io/deepvariant-docker/deepvariant_gpu:0.6.0"". zones: [""us-central1-c""] . I am trying to use essentially the same code as is listed in the documentation:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads /reads.bam \. --examples ""${sample_id}.examples.tfrecord${numShards}"" \. --gvcf ""${sample_id}.gvcf.tfrecord${numShards}"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. However, I am getting the following error for each of the shards: . `ValueError: ('Output is not sharded but shard > 0', 4)`. What does this mean? I assume it means I am not actually sharding though it seems I am. Any help appreciated, thanks. Another example with full stack trace:. ```. Traceback (most recent call last):. File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1120, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1071, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 335, in default_options. flags_obj.gvcf or ''). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py"", line 212, in resolve_filespecs. raise ValueError('Output is not ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/143
https://github.com/google/deepvariant/issues/143:400,testability,Log,Log,400,"Sharding issue for make_examples; Hi,. I'm trying to run make_examples with 64 shards using 64 cores (1 worker per shard default I assume) and the following settings:. docker: ""gcr.io/deepvariant-docker/deepvariant_gpu:0.6.0"". zones: [""us-central1-c""] . I am trying to use essentially the same code as is listed in the documentation:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads /reads.bam \. --examples ""${sample_id}.examples.tfrecord${numShards}"" \. --gvcf ""${sample_id}.gvcf.tfrecord${numShards}"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. However, I am getting the following error for each of the shards: . `ValueError: ('Output is not sharded but shard > 0', 4)`. What does this mean? I assume it means I am not actually sharding though it seems I am. Any help appreciated, thanks. Another example with full stack trace:. ```. Traceback (most recent call last):. File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1120, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1071, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 335, in default_options. flags_obj.gvcf or ''). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py"", line 212, in resolve_filespecs. raise ValueError('Output is not ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/143
https://github.com/google/deepvariant/issues/143:454,testability,log,log,454,"Sharding issue for make_examples; Hi,. I'm trying to run make_examples with 64 shards using 64 cores (1 worker per shard default I assume) and the following settings:. docker: ""gcr.io/deepvariant-docker/deepvariant_gpu:0.6.0"". zones: [""us-central1-c""] . I am trying to use essentially the same code as is listed in the documentation:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads /reads.bam \. --examples ""${sample_id}.examples.tfrecord${numShards}"" \. --gvcf ""${sample_id}.gvcf.tfrecord${numShards}"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. However, I am getting the following error for each of the shards: . `ValueError: ('Output is not sharded but shard > 0', 4)`. What does this mean? I assume it means I am not actually sharding though it seems I am. Any help appreciated, thanks. Another example with full stack trace:. ```. Traceback (most recent call last):. File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1120, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1071, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 335, in default_options. flags_obj.gvcf or ''). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py"", line 212, in resolve_filespecs. raise ValueError('Output is not ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/143
https://github.com/google/deepvariant/issues/143:775,testability,log,log,775,"Sharding issue for make_examples; Hi,. I'm trying to run make_examples with 64 shards using 64 cores (1 worker per shard default I assume) and the following settings:. docker: ""gcr.io/deepvariant-docker/deepvariant_gpu:0.6.0"". zones: [""us-central1-c""] . I am trying to use essentially the same code as is listed in the documentation:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads /reads.bam \. --examples ""${sample_id}.examples.tfrecord${numShards}"" \. --gvcf ""${sample_id}.gvcf.tfrecord${numShards}"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. However, I am getting the following error for each of the shards: . `ValueError: ('Output is not sharded but shard > 0', 4)`. What does this mean? I assume it means I am not actually sharding though it seems I am. Any help appreciated, thanks. Another example with full stack trace:. ```. Traceback (most recent call last):. File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1120, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1071, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 335, in default_options. flags_obj.gvcf or ''). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py"", line 212, in resolve_filespecs. raise ValueError('Output is not ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/143
https://github.com/google/deepvariant/issues/143:1082,testability,trace,trace,1082," using 64 cores (1 worker per shard default I assume) and the following settings:. docker: ""gcr.io/deepvariant-docker/deepvariant_gpu:0.6.0"". zones: [""us-central1-c""] . I am trying to use essentially the same code as is listed in the documentation:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads /reads.bam \. --examples ""${sample_id}.examples.tfrecord${numShards}"" \. --gvcf ""${sample_id}.gvcf.tfrecord${numShards}"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. However, I am getting the following error for each of the shards: . `ValueError: ('Output is not sharded but shard > 0', 4)`. What does this mean? I assume it means I am not actually sharding though it seems I am. Any help appreciated, thanks. Another example with full stack trace:. ```. Traceback (most recent call last):. File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1120, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1071, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 335, in default_options. flags_obj.gvcf or ''). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py"", line 212, in resolve_filespecs. raise ValueError('Output is not sharded but shard > 0', shard). ValueError: ('Output is not sharded but shard > 0', 4",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/143
https://github.com/google/deepvariant/issues/143:1095,testability,Trace,Traceback,1095,"64 cores (1 worker per shard default I assume) and the following settings:. docker: ""gcr.io/deepvariant-docker/deepvariant_gpu:0.6.0"". zones: [""us-central1-c""] . I am trying to use essentially the same code as is listed in the documentation:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads /reads.bam \. --examples ""${sample_id}.examples.tfrecord${numShards}"" \. --gvcf ""${sample_id}.gvcf.tfrecord${numShards}"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. However, I am getting the following error for each of the shards: . `ValueError: ('Output is not sharded but shard > 0', 4)`. What does this mean? I assume it means I am not actually sharding though it seems I am. Any help appreciated, thanks. Another example with full stack trace:. ```. Traceback (most recent call last):. File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1120, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1071, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 335, in default_options. flags_obj.gvcf or ''). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py"", line 212, in resolve_filespecs. raise ValueError('Output is not sharded but shard > 0', shard). ValueError: ('Output is not sharded but shard > 0', 44). ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/143
https://github.com/google/deepvariant/issues/143:319,usability,document,documentation,319,"Sharding issue for make_examples; Hi,. I'm trying to run make_examples with 64 shards using 64 cores (1 worker per shard default I assume) and the following settings:. docker: ""gcr.io/deepvariant-docker/deepvariant_gpu:0.6.0"". zones: [""us-central1-c""] . I am trying to use essentially the same code as is listed in the documentation:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads /reads.bam \. --examples ""${sample_id}.examples.tfrecord${numShards}"" \. --gvcf ""${sample_id}.gvcf.tfrecord${numShards}"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. However, I am getting the following error for each of the shards: . `ValueError: ('Output is not sharded but shard > 0', 4)`. What does this mean? I assume it means I am not actually sharding though it seems I am. Any help appreciated, thanks. Another example with full stack trace:. ```. Traceback (most recent call last):. File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1120, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1071, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 335, in default_options. flags_obj.gvcf or ''). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py"", line 212, in resolve_filespecs. raise ValueError('Output is not ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/143
https://github.com/google/deepvariant/issues/143:842,usability,error,error,842,"Sharding issue for make_examples; Hi,. I'm trying to run make_examples with 64 shards using 64 cores (1 worker per shard default I assume) and the following settings:. docker: ""gcr.io/deepvariant-docker/deepvariant_gpu:0.6.0"". zones: [""us-central1-c""] . I am trying to use essentially the same code as is listed in the documentation:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads /reads.bam \. --examples ""${sample_id}.examples.tfrecord${numShards}"" \. --gvcf ""${sample_id}.gvcf.tfrecord${numShards}"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. However, I am getting the following error for each of the shards: . `ValueError: ('Output is not sharded but shard > 0', 4)`. What does this mean? I assume it means I am not actually sharding though it seems I am. Any help appreciated, thanks. Another example with full stack trace:. ```. Traceback (most recent call last):. File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1120, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1071, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 335, in default_options. flags_obj.gvcf or ''). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py"", line 212, in resolve_filespecs. raise ValueError('Output is not ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/143
https://github.com/google/deepvariant/issues/143:1024,usability,help,help,1024,"amples; Hi,. I'm trying to run make_examples with 64 shards using 64 cores (1 worker per shard default I assume) and the following settings:. docker: ""gcr.io/deepvariant-docker/deepvariant_gpu:0.6.0"". zones: [""us-central1-c""] . I am trying to use essentially the same code as is listed in the documentation:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads /reads.bam \. --examples ""${sample_id}.examples.tfrecord${numShards}"" \. --gvcf ""${sample_id}.gvcf.tfrecord${numShards}"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. However, I am getting the following error for each of the shards: . `ValueError: ('Output is not sharded but shard > 0', 4)`. What does this mean? I assume it means I am not actually sharding though it seems I am. Any help appreciated, thanks. Another example with full stack trace:. ```. Traceback (most recent call last):. File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1120, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1071, in main. options = default_options(add_flags=True, flags_obj=FLAGS). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 335, in default_options. flags_obj.gvcf or ''). File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py"", line 212, in resolve_filespecs. raise ValueError('Output is not sharded but shard > 0', sh",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/143
https://github.com/google/deepvariant/pull/144:22,availability,Error,Error,22,Adding DNA Sequencing Error Correction tutorial blog post.; Note: This pull request is from the DeepVariant team,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/144
https://github.com/google/deepvariant/pull/144:22,performance,Error,Error,22,Adding DNA Sequencing Error Correction tutorial blog post.; Note: This pull request is from the DeepVariant team,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/144
https://github.com/google/deepvariant/pull/144:22,safety,Error,Error,22,Adding DNA Sequencing Error Correction tutorial blog post.; Note: This pull request is from the DeepVariant team,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/144
https://github.com/google/deepvariant/pull/144:108,security,team,team,108,Adding DNA Sequencing Error Correction tutorial blog post.; Note: This pull request is from the DeepVariant team,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/144
https://github.com/google/deepvariant/pull/144:22,usability,Error,Error,22,Adding DNA Sequencing Error Correction tutorial blog post.; Note: This pull request is from the DeepVariant team,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/144
https://github.com/google/deepvariant/issues/145:840,availability,error,error,840,"Cannot build deepvariant with tensorflow master due to bazel version requirements; With `DV_CPP_TENSORFLOW_TAG=master`, I believe deepvariant will not build from source. TF master does not build with Bazel=0.15.0:. ```. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/... [bazel INFO src/main/cpp/option_processor.cc:235] Looking for master bazelrcs in the following three paths: /root/deepvariant/tools/bazel.rc, , /etc/bazel.bazelrc. [bazel INFO src/main/cpp/option_processor.cc:165] User provided no rc file. [bazel INFO src/main/cpp/rc_file.cc:53] Parsing the RcFile /root/deepvariant/tools/bazel.rc. [bazel INFO src/main/cpp/rc_file.cc:53] Parsing the RcFile /root/deepvariant/../tensorflow/tools/bazel.rc. [bazel FATAL src/main/cpp/blaze.cc:1263] Unexpected error reading .blazerc file '/root/deepvariant/../tensorflow/tools/bazel.rc'. ```. And deepvariant does not build with bazel=0.19.0, see #134. ___. Linux xxx 4.15.0-1031-aws #33-Ubuntu SMP Fri Dec 7 09:32:27 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux. ```. # Copyright 2017 Google LLC. #. # Redistribution and use in source and binary forms, with or without. # modification, are permitted provided that the following conditions. # are met:. #. # 1. Redistributions of source code must retain the above copyright notice,. # this list of conditions and the following disclaimer. #. # 2. Redistributions in binary form must reproduce the above copyright. # notice, this list of conditions and the following disclaimer in the. # documentation and/or other materials provided with the distribution. #. # 3. Neither the name of the copyright holder nor the names of its. # contributors may be used to endorse or promote products derived from this. # software without specific prior written permission. #. # THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"". # AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE. # IMPLIE",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:2182,availability,INCID,INCIDENTAL,2182,"without. # modification, are permitted provided that the following conditions. # are met:. #. # 1. Redistributions of source code must retain the above copyright notice,. # this list of conditions and the following disclaimer. #. # 2. Redistributions in binary form must reproduce the above copyright. # notice, this list of conditions and the following disclaimer in the. # documentation and/or other materials provided with the distribution. #. # 3. Neither the name of the copyright holder nor the names of its. # contributors may be used to endorse or promote products derived from this. # software without specific prior written permission. #. # THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"". # AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE. # IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. # ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE. # LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR. # CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF. # SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS. # INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN. # CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE). # ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE. # POSSIBILITY OF SUCH DAMAGE. # Source this file---these options are needed for TF config and for. # successive bazel runs. # Set this to 1 if the system image already has TensorFlow preinstalled. This. # will skip the installation of TensorFlow. export DV_USE_PREINSTALLED_TF=""${DV_USE_PREINSTALLED_TF:-0}"". export TF_CUDA_CLANG=0. export TF_ENABLE_XLA=1. export TF_NEED_CUDA=1. export TF_NEED_GCP=1. export TF_NEED_GDR=0. export TF_NEED_HDFS=0. export TF_NEED_JEMALLOC=0. export TF_NEED_MKL=1. export TF_NEED_MPI=0. export TF_NEED_OPENCL=0. export TF_NEED_OPENCL_SYCL=0. export TF_NEED_S3=1.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:2312,availability,SERVIC,SERVICES,2312," must retain the above copyright notice,. # this list of conditions and the following disclaimer. #. # 2. Redistributions in binary form must reproduce the above copyright. # notice, this list of conditions and the following disclaimer in the. # documentation and/or other materials provided with the distribution. #. # 3. Neither the name of the copyright holder nor the names of its. # contributors may be used to endorse or promote products derived from this. # software without specific prior written permission. #. # THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"". # AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE. # IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. # ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE. # LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR. # CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF. # SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS. # INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN. # CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE). # ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE. # POSSIBILITY OF SUCH DAMAGE. # Source this file---these options are needed for TF config and for. # successive bazel runs. # Set this to 1 if the system image already has TensorFlow preinstalled. This. # will skip the installation of TensorFlow. export DV_USE_PREINSTALLED_TF=""${DV_USE_PREINSTALLED_TF:-0}"". export TF_CUDA_CLANG=0. export TF_ENABLE_XLA=1. export TF_NEED_CUDA=1. export TF_NEED_GCP=1. export TF_NEED_GDR=0. export TF_NEED_HDFS=0. export TF_NEED_JEMALLOC=0. export TF_NEED_MKL=1. export TF_NEED_MPI=0. export TF_NEED_OPENCL=0. export TF_NEED_OPENCL_SYCL=0. export TF_NEED_S3=1. export TF_NEED_VERBS=0. # Used if TF_NEED_CUDA=1. export TF_CUDA_VERSION=""10.0"". export CUDA_TOOLKIT_PATH=""/usr/local/cuda"". exp",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:6814,availability,echo,echo,6814,"t runtime. if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then. export DV_CPP_TENSORFLOW_TAG=""master"". else. export DV_CPP_TENSORFLOW_TAG=""r1.12"". fi. export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing. # value in the environment (allowing command line control of the build),. # defaulting to 0 (CPU only build). export DV_GPU_BUILD=""${DV_GPU_BUILD:-1}"". # If this variable is set to 1, DeepVariant will use a TensorFlow wheel file. # compiled with MKL support for corei7 or better chipsets, which. # significantly speeds up execution when running on modern CPUs. The default. # TensorFlow wheel files don't contain these instructions (and thereby run on a. # broader set of CPUs). Using this optimized wheel reduces the runtime of. # DeepVariant's call_variants step by >3x. This is called the GCP (Google Cloud. # Platform) optimized wheel because all GCP instances have at least Sandy Bridge. # or better chipsets, so this wheel should run anywhere on GCP. export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}"". export GCP_OPTIMIZED_TF_WHL_FILENAME=""tensorflow-${DV_GCP_OPTIMIZED_TF_WHL_VERSION}.deepvariant_gcp-cp27-none-linux_x86_64.whl"". export GCP_OPTIMIZED_TF_WHL_PATH=""${DV_PACKAGE_BUCKET_PATH}/tensorflow"". export GCP_OPTIMIZED_TF_WHL_CURL_PATH=""${DV_PACKAGE_CURL_PATH}/tensorflow"". # Set this to 1 to make our prereq scripts install the CUDA libraries. # If you already have CUDA installed, such as on a properly provisioned. # Docker image, it shouldn't be necessary. export DV_INSTALL_GPU_DRIVERS=""${DV_INSTALL_GPU_DRIVERS:-0}"". export PYTHON_BIN_PATH=$(which python). export USE_DEFAULT_PYTHON_LIB_PATH=1. export DV_COPT_FLAGS=""--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings"". function note_build_stage {. echo ""========== [$(date)] Stage '${1}' starting"". }. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:7,deployability,build,build,7,"Cannot build deepvariant with tensorflow master due to bazel version requirements; With `DV_CPP_TENSORFLOW_TAG=master`, I believe deepvariant will not build from source. TF master does not build with Bazel=0.15.0:. ```. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/... [bazel INFO src/main/cpp/option_processor.cc:235] Looking for master bazelrcs in the following three paths: /root/deepvariant/tools/bazel.rc, , /etc/bazel.bazelrc. [bazel INFO src/main/cpp/option_processor.cc:165] User provided no rc file. [bazel INFO src/main/cpp/rc_file.cc:53] Parsing the RcFile /root/deepvariant/tools/bazel.rc. [bazel INFO src/main/cpp/rc_file.cc:53] Parsing the RcFile /root/deepvariant/../tensorflow/tools/bazel.rc. [bazel FATAL src/main/cpp/blaze.cc:1263] Unexpected error reading .blazerc file '/root/deepvariant/../tensorflow/tools/bazel.rc'. ```. And deepvariant does not build with bazel=0.19.0, see #134. ___. Linux xxx 4.15.0-1031-aws #33-Ubuntu SMP Fri Dec 7 09:32:27 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux. ```. # Copyright 2017 Google LLC. #. # Redistribution and use in source and binary forms, with or without. # modification, are permitted provided that the following conditions. # are met:. #. # 1. Redistributions of source code must retain the above copyright notice,. # this list of conditions and the following disclaimer. #. # 2. Redistributions in binary form must reproduce the above copyright. # notice, this list of conditions and the following disclaimer in the. # documentation and/or other materials provided with the distribution. #. # 3. Neither the name of the copyright holder nor the names of its. # contributors may be used to endorse or promote products derived from this. # software without specific prior written permission. #. # THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"". # AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE. # IMPLIE",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:61,deployability,version,version,61,"Cannot build deepvariant with tensorflow master due to bazel version requirements; With `DV_CPP_TENSORFLOW_TAG=master`, I believe deepvariant will not build from source. TF master does not build with Bazel=0.15.0:. ```. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/... [bazel INFO src/main/cpp/option_processor.cc:235] Looking for master bazelrcs in the following three paths: /root/deepvariant/tools/bazel.rc, , /etc/bazel.bazelrc. [bazel INFO src/main/cpp/option_processor.cc:165] User provided no rc file. [bazel INFO src/main/cpp/rc_file.cc:53] Parsing the RcFile /root/deepvariant/tools/bazel.rc. [bazel INFO src/main/cpp/rc_file.cc:53] Parsing the RcFile /root/deepvariant/../tensorflow/tools/bazel.rc. [bazel FATAL src/main/cpp/blaze.cc:1263] Unexpected error reading .blazerc file '/root/deepvariant/../tensorflow/tools/bazel.rc'. ```. And deepvariant does not build with bazel=0.19.0, see #134. ___. Linux xxx 4.15.0-1031-aws #33-Ubuntu SMP Fri Dec 7 09:32:27 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux. ```. # Copyright 2017 Google LLC. #. # Redistribution and use in source and binary forms, with or without. # modification, are permitted provided that the following conditions. # are met:. #. # 1. Redistributions of source code must retain the above copyright notice,. # this list of conditions and the following disclaimer. #. # 2. Redistributions in binary form must reproduce the above copyright. # notice, this list of conditions and the following disclaimer in the. # documentation and/or other materials provided with the distribution. #. # 3. Neither the name of the copyright holder nor the names of its. # contributors may be used to endorse or promote products derived from this. # software without specific prior written permission. #. # THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"". # AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE. # IMPLIE",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:151,deployability,build,build,151,"Cannot build deepvariant with tensorflow master due to bazel version requirements; With `DV_CPP_TENSORFLOW_TAG=master`, I believe deepvariant will not build from source. TF master does not build with Bazel=0.15.0:. ```. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/... [bazel INFO src/main/cpp/option_processor.cc:235] Looking for master bazelrcs in the following three paths: /root/deepvariant/tools/bazel.rc, , /etc/bazel.bazelrc. [bazel INFO src/main/cpp/option_processor.cc:165] User provided no rc file. [bazel INFO src/main/cpp/rc_file.cc:53] Parsing the RcFile /root/deepvariant/tools/bazel.rc. [bazel INFO src/main/cpp/rc_file.cc:53] Parsing the RcFile /root/deepvariant/../tensorflow/tools/bazel.rc. [bazel FATAL src/main/cpp/blaze.cc:1263] Unexpected error reading .blazerc file '/root/deepvariant/../tensorflow/tools/bazel.rc'. ```. And deepvariant does not build with bazel=0.19.0, see #134. ___. Linux xxx 4.15.0-1031-aws #33-Ubuntu SMP Fri Dec 7 09:32:27 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux. ```. # Copyright 2017 Google LLC. #. # Redistribution and use in source and binary forms, with or without. # modification, are permitted provided that the following conditions. # are met:. #. # 1. Redistributions of source code must retain the above copyright notice,. # this list of conditions and the following disclaimer. #. # 2. Redistributions in binary form must reproduce the above copyright. # notice, this list of conditions and the following disclaimer in the. # documentation and/or other materials provided with the distribution. #. # 3. Neither the name of the copyright holder nor the names of its. # contributors may be used to endorse or promote products derived from this. # software without specific prior written permission. #. # THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"". # AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE. # IMPLIE",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:189,deployability,build,build,189,"Cannot build deepvariant with tensorflow master due to bazel version requirements; With `DV_CPP_TENSORFLOW_TAG=master`, I believe deepvariant will not build from source. TF master does not build with Bazel=0.15.0:. ```. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/... [bazel INFO src/main/cpp/option_processor.cc:235] Looking for master bazelrcs in the following three paths: /root/deepvariant/tools/bazel.rc, , /etc/bazel.bazelrc. [bazel INFO src/main/cpp/option_processor.cc:165] User provided no rc file. [bazel INFO src/main/cpp/rc_file.cc:53] Parsing the RcFile /root/deepvariant/tools/bazel.rc. [bazel INFO src/main/cpp/rc_file.cc:53] Parsing the RcFile /root/deepvariant/../tensorflow/tools/bazel.rc. [bazel FATAL src/main/cpp/blaze.cc:1263] Unexpected error reading .blazerc file '/root/deepvariant/../tensorflow/tools/bazel.rc'. ```. And deepvariant does not build with bazel=0.19.0, see #134. ___. Linux xxx 4.15.0-1031-aws #33-Ubuntu SMP Fri Dec 7 09:32:27 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux. ```. # Copyright 2017 Google LLC. #. # Redistribution and use in source and binary forms, with or without. # modification, are permitted provided that the following conditions. # are met:. #. # 1. Redistributions of source code must retain the above copyright notice,. # this list of conditions and the following disclaimer. #. # 2. Redistributions in binary form must reproduce the above copyright. # notice, this list of conditions and the following disclaimer in the. # documentation and/or other materials provided with the distribution. #. # 3. Neither the name of the copyright holder nor the names of its. # contributors may be used to endorse or promote products derived from this. # software without specific prior written permission. #. # THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"". # AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE. # IMPLIE",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:948,deployability,build,build,948,"Cannot build deepvariant with tensorflow master due to bazel version requirements; With `DV_CPP_TENSORFLOW_TAG=master`, I believe deepvariant will not build from source. TF master does not build with Bazel=0.15.0:. ```. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/... [bazel INFO src/main/cpp/option_processor.cc:235] Looking for master bazelrcs in the following three paths: /root/deepvariant/tools/bazel.rc, , /etc/bazel.bazelrc. [bazel INFO src/main/cpp/option_processor.cc:165] User provided no rc file. [bazel INFO src/main/cpp/rc_file.cc:53] Parsing the RcFile /root/deepvariant/tools/bazel.rc. [bazel INFO src/main/cpp/rc_file.cc:53] Parsing the RcFile /root/deepvariant/../tensorflow/tools/bazel.rc. [bazel FATAL src/main/cpp/blaze.cc:1263] Unexpected error reading .blazerc file '/root/deepvariant/../tensorflow/tools/bazel.rc'. ```. And deepvariant does not build with bazel=0.19.0, see #134. ___. Linux xxx 4.15.0-1031-aws #33-Ubuntu SMP Fri Dec 7 09:32:27 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux. ```. # Copyright 2017 Google LLC. #. # Redistribution and use in source and binary forms, with or without. # modification, are permitted provided that the following conditions. # are met:. #. # 1. Redistributions of source code must retain the above copyright notice,. # this list of conditions and the following disclaimer. #. # 2. Redistributions in binary form must reproduce the above copyright. # notice, this list of conditions and the following disclaimer in the. # documentation and/or other materials provided with the distribution. #. # 3. Neither the name of the copyright holder nor the names of its. # contributors may be used to endorse or promote products derived from this. # software without specific prior written permission. #. # THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"". # AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE. # IMPLIE",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:2312,deployability,SERVIC,SERVICES,2312," must retain the above copyright notice,. # this list of conditions and the following disclaimer. #. # 2. Redistributions in binary form must reproduce the above copyright. # notice, this list of conditions and the following disclaimer in the. # documentation and/or other materials provided with the distribution. #. # 3. Neither the name of the copyright holder nor the names of its. # contributors may be used to endorse or promote products derived from this. # software without specific prior written permission. #. # THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"". # AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE. # IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. # ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE. # LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR. # CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF. # SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS. # INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN. # CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE). # ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE. # POSSIBILITY OF SUCH DAMAGE. # Source this file---these options are needed for TF config and for. # successive bazel runs. # Set this to 1 if the system image already has TensorFlow preinstalled. This. # will skip the installation of TensorFlow. export DV_USE_PREINSTALLED_TF=""${DV_USE_PREINSTALLED_TF:-0}"". export TF_CUDA_CLANG=0. export TF_ENABLE_XLA=1. export TF_NEED_CUDA=1. export TF_NEED_GCP=1. export TF_NEED_GDR=0. export TF_NEED_HDFS=0. export TF_NEED_JEMALLOC=0. export TF_NEED_MKL=1. export TF_NEED_MPI=0. export TF_NEED_OPENCL=0. export TF_NEED_OPENCL_SYCL=0. export TF_NEED_S3=1. export TF_NEED_VERBS=0. # Used if TF_NEED_CUDA=1. export TF_CUDA_VERSION=""10.0"". export CUDA_TOOLKIT_PATH=""/usr/local/cuda"". exp",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:2813,deployability,instal,installation,2813,"n permission. #. # THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"". # AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE. # IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. # ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE. # LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR. # CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF. # SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS. # INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN. # CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE). # ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE. # POSSIBILITY OF SUCH DAMAGE. # Source this file---these options are needed for TF config and for. # successive bazel runs. # Set this to 1 if the system image already has TensorFlow preinstalled. This. # will skip the installation of TensorFlow. export DV_USE_PREINSTALLED_TF=""${DV_USE_PREINSTALLED_TF:-0}"". export TF_CUDA_CLANG=0. export TF_ENABLE_XLA=1. export TF_NEED_CUDA=1. export TF_NEED_GCP=1. export TF_NEED_GDR=0. export TF_NEED_HDFS=0. export TF_NEED_JEMALLOC=0. export TF_NEED_MKL=1. export TF_NEED_MPI=0. export TF_NEED_OPENCL=0. export TF_NEED_OPENCL_SYCL=0. export TF_NEED_S3=1. export TF_NEED_VERBS=0. # Used if TF_NEED_CUDA=1. export TF_CUDA_VERSION=""10.0"". export CUDA_TOOLKIT_PATH=""/usr/local/cuda"". export TF_CUDNN_VERSION=""7"". export CUDNN_INSTALL_PATH=""/usr/lib/x86_64-linux-gnu"". # The version of bazel we want to build DeepVariant. DV_BAZEL_VERSION=""0.15.0"". # We need to make sure that $HOME/bin is first in the binary search path so that. # `bazel` will find the latest version of bazel installed in the user's home. # directory. This is set in setting.sh as all DeepVariant scripts source. # settings.sh and assume that `bazel` will find the right version. export PATH=""$HOME/bin:$PATH"". # Path to ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:3403,deployability,version,version,3403," THEORY OF LIABILITY, WHETHER IN. # CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE). # ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE. # POSSIBILITY OF SUCH DAMAGE. # Source this file---these options are needed for TF config and for. # successive bazel runs. # Set this to 1 if the system image already has TensorFlow preinstalled. This. # will skip the installation of TensorFlow. export DV_USE_PREINSTALLED_TF=""${DV_USE_PREINSTALLED_TF:-0}"". export TF_CUDA_CLANG=0. export TF_ENABLE_XLA=1. export TF_NEED_CUDA=1. export TF_NEED_GCP=1. export TF_NEED_GDR=0. export TF_NEED_HDFS=0. export TF_NEED_JEMALLOC=0. export TF_NEED_MKL=1. export TF_NEED_MPI=0. export TF_NEED_OPENCL=0. export TF_NEED_OPENCL_SYCL=0. export TF_NEED_S3=1. export TF_NEED_VERBS=0. # Used if TF_NEED_CUDA=1. export TF_CUDA_VERSION=""10.0"". export CUDA_TOOLKIT_PATH=""/usr/local/cuda"". export TF_CUDNN_VERSION=""7"". export CUDNN_INSTALL_PATH=""/usr/lib/x86_64-linux-gnu"". # The version of bazel we want to build DeepVariant. DV_BAZEL_VERSION=""0.15.0"". # We need to make sure that $HOME/bin is first in the binary search path so that. # `bazel` will find the latest version of bazel installed in the user's home. # directory. This is set in setting.sh as all DeepVariant scripts source. # settings.sh and assume that `bazel` will find the right version. export PATH=""$HOME/bin:$PATH"". # Path to the public bucket containing DeepVariant-related artifacts. export DEEPVARIANT_BUCKET=""gs://deepvariant"". export DV_PACKAGE_BUCKET_PATH=""${DEEPVARIANT_BUCKET}/packages"". export DV_PACKAGE_CURL_PATH=""https://storage.googleapis.com/deepvariant/packages"". # Set this to 1 to use the nightly (latest) build of TensorFlow instead of a. # named release version. Set it to an already existing value in the environment. # (allowing command line control of the build), defaulting to 0 (release build). # Note that setting this to 1 implies that the C++ code in DeepVariant will be. # build using t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:3431,deployability,build,build,3431,"ER IN. # CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE). # ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE. # POSSIBILITY OF SUCH DAMAGE. # Source this file---these options are needed for TF config and for. # successive bazel runs. # Set this to 1 if the system image already has TensorFlow preinstalled. This. # will skip the installation of TensorFlow. export DV_USE_PREINSTALLED_TF=""${DV_USE_PREINSTALLED_TF:-0}"". export TF_CUDA_CLANG=0. export TF_ENABLE_XLA=1. export TF_NEED_CUDA=1. export TF_NEED_GCP=1. export TF_NEED_GDR=0. export TF_NEED_HDFS=0. export TF_NEED_JEMALLOC=0. export TF_NEED_MKL=1. export TF_NEED_MPI=0. export TF_NEED_OPENCL=0. export TF_NEED_OPENCL_SYCL=0. export TF_NEED_S3=1. export TF_NEED_VERBS=0. # Used if TF_NEED_CUDA=1. export TF_CUDA_VERSION=""10.0"". export CUDA_TOOLKIT_PATH=""/usr/local/cuda"". export TF_CUDNN_VERSION=""7"". export CUDNN_INSTALL_PATH=""/usr/lib/x86_64-linux-gnu"". # The version of bazel we want to build DeepVariant. DV_BAZEL_VERSION=""0.15.0"". # We need to make sure that $HOME/bin is first in the binary search path so that. # `bazel` will find the latest version of bazel installed in the user's home. # directory. This is set in setting.sh as all DeepVariant scripts source. # settings.sh and assume that `bazel` will find the right version. export PATH=""$HOME/bin:$PATH"". # Path to the public bucket containing DeepVariant-related artifacts. export DEEPVARIANT_BUCKET=""gs://deepvariant"". export DV_PACKAGE_BUCKET_PATH=""${DEEPVARIANT_BUCKET}/packages"". export DV_PACKAGE_CURL_PATH=""https://storage.googleapis.com/deepvariant/packages"". # Set this to 1 to use the nightly (latest) build of TensorFlow instead of a. # named release version. Set it to an already existing value in the environment. # (allowing command line control of the build), defaulting to 0 (release build). # Note that setting this to 1 implies that the C++ code in DeepVariant will be. # build using the master branch and not th",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:3590,deployability,version,version,3590,"# POSSIBILITY OF SUCH DAMAGE. # Source this file---these options are needed for TF config and for. # successive bazel runs. # Set this to 1 if the system image already has TensorFlow preinstalled. This. # will skip the installation of TensorFlow. export DV_USE_PREINSTALLED_TF=""${DV_USE_PREINSTALLED_TF:-0}"". export TF_CUDA_CLANG=0. export TF_ENABLE_XLA=1. export TF_NEED_CUDA=1. export TF_NEED_GCP=1. export TF_NEED_GDR=0. export TF_NEED_HDFS=0. export TF_NEED_JEMALLOC=0. export TF_NEED_MKL=1. export TF_NEED_MPI=0. export TF_NEED_OPENCL=0. export TF_NEED_OPENCL_SYCL=0. export TF_NEED_S3=1. export TF_NEED_VERBS=0. # Used if TF_NEED_CUDA=1. export TF_CUDA_VERSION=""10.0"". export CUDA_TOOLKIT_PATH=""/usr/local/cuda"". export TF_CUDNN_VERSION=""7"". export CUDNN_INSTALL_PATH=""/usr/lib/x86_64-linux-gnu"". # The version of bazel we want to build DeepVariant. DV_BAZEL_VERSION=""0.15.0"". # We need to make sure that $HOME/bin is first in the binary search path so that. # `bazel` will find the latest version of bazel installed in the user's home. # directory. This is set in setting.sh as all DeepVariant scripts source. # settings.sh and assume that `bazel` will find the right version. export PATH=""$HOME/bin:$PATH"". # Path to the public bucket containing DeepVariant-related artifacts. export DEEPVARIANT_BUCKET=""gs://deepvariant"". export DV_PACKAGE_BUCKET_PATH=""${DEEPVARIANT_BUCKET}/packages"". export DV_PACKAGE_CURL_PATH=""https://storage.googleapis.com/deepvariant/packages"". # Set this to 1 to use the nightly (latest) build of TensorFlow instead of a. # named release version. Set it to an already existing value in the environment. # (allowing command line control of the build), defaulting to 0 (release build). # Note that setting this to 1 implies that the C++ code in DeepVariant will be. # build using the master branch and not the pinned version to avoid. # incompatibilities between TensorFlow C++ used to build DeepVariant and the. # tf-nightly wheel. export DV_TF_NIGHTLY_BUILD=""${DV_TF",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:3607,deployability,instal,installed,3607,"UCH DAMAGE. # Source this file---these options are needed for TF config and for. # successive bazel runs. # Set this to 1 if the system image already has TensorFlow preinstalled. This. # will skip the installation of TensorFlow. export DV_USE_PREINSTALLED_TF=""${DV_USE_PREINSTALLED_TF:-0}"". export TF_CUDA_CLANG=0. export TF_ENABLE_XLA=1. export TF_NEED_CUDA=1. export TF_NEED_GCP=1. export TF_NEED_GDR=0. export TF_NEED_HDFS=0. export TF_NEED_JEMALLOC=0. export TF_NEED_MKL=1. export TF_NEED_MPI=0. export TF_NEED_OPENCL=0. export TF_NEED_OPENCL_SYCL=0. export TF_NEED_S3=1. export TF_NEED_VERBS=0. # Used if TF_NEED_CUDA=1. export TF_CUDA_VERSION=""10.0"". export CUDA_TOOLKIT_PATH=""/usr/local/cuda"". export TF_CUDNN_VERSION=""7"". export CUDNN_INSTALL_PATH=""/usr/lib/x86_64-linux-gnu"". # The version of bazel we want to build DeepVariant. DV_BAZEL_VERSION=""0.15.0"". # We need to make sure that $HOME/bin is first in the binary search path so that. # `bazel` will find the latest version of bazel installed in the user's home. # directory. This is set in setting.sh as all DeepVariant scripts source. # settings.sh and assume that `bazel` will find the right version. export PATH=""$HOME/bin:$PATH"". # Path to the public bucket containing DeepVariant-related artifacts. export DEEPVARIANT_BUCKET=""gs://deepvariant"". export DV_PACKAGE_BUCKET_PATH=""${DEEPVARIANT_BUCKET}/packages"". export DV_PACKAGE_CURL_PATH=""https://storage.googleapis.com/deepvariant/packages"". # Set this to 1 to use the nightly (latest) build of TensorFlow instead of a. # named release version. Set it to an already existing value in the environment. # (allowing command line control of the build), defaulting to 0 (release build). # Note that setting this to 1 implies that the C++ code in DeepVariant will be. # build using the master branch and not the pinned version to avoid. # incompatibilities between TensorFlow C++ used to build DeepVariant and the. # tf-nightly wheel. export DV_TF_NIGHTLY_BUILD=""${DV_TF_NIGHTLY_BUILD:-1}",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:3769,deployability,version,version,3769,"low preinstalled. This. # will skip the installation of TensorFlow. export DV_USE_PREINSTALLED_TF=""${DV_USE_PREINSTALLED_TF:-0}"". export TF_CUDA_CLANG=0. export TF_ENABLE_XLA=1. export TF_NEED_CUDA=1. export TF_NEED_GCP=1. export TF_NEED_GDR=0. export TF_NEED_HDFS=0. export TF_NEED_JEMALLOC=0. export TF_NEED_MKL=1. export TF_NEED_MPI=0. export TF_NEED_OPENCL=0. export TF_NEED_OPENCL_SYCL=0. export TF_NEED_S3=1. export TF_NEED_VERBS=0. # Used if TF_NEED_CUDA=1. export TF_CUDA_VERSION=""10.0"". export CUDA_TOOLKIT_PATH=""/usr/local/cuda"". export TF_CUDNN_VERSION=""7"". export CUDNN_INSTALL_PATH=""/usr/lib/x86_64-linux-gnu"". # The version of bazel we want to build DeepVariant. DV_BAZEL_VERSION=""0.15.0"". # We need to make sure that $HOME/bin is first in the binary search path so that. # `bazel` will find the latest version of bazel installed in the user's home. # directory. This is set in setting.sh as all DeepVariant scripts source. # settings.sh and assume that `bazel` will find the right version. export PATH=""$HOME/bin:$PATH"". # Path to the public bucket containing DeepVariant-related artifacts. export DEEPVARIANT_BUCKET=""gs://deepvariant"". export DV_PACKAGE_BUCKET_PATH=""${DEEPVARIANT_BUCKET}/packages"". export DV_PACKAGE_CURL_PATH=""https://storage.googleapis.com/deepvariant/packages"". # Set this to 1 to use the nightly (latest) build of TensorFlow instead of a. # named release version. Set it to an already existing value in the environment. # (allowing command line control of the build), defaulting to 0 (release build). # Note that setting this to 1 implies that the C++ code in DeepVariant will be. # build using the master branch and not the pinned version to avoid. # incompatibilities between TensorFlow C++ used to build DeepVariant and the. # tf-nightly wheel. export DV_TF_NIGHTLY_BUILD=""${DV_TF_NIGHTLY_BUILD:-1}"". # The branch/tag we checkout to build our C++ dependencies against. This is not. # the same as the python version of TensorFlow we use, but should be similar ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:3837,deployability,contain,containing,3837,"xport DV_USE_PREINSTALLED_TF=""${DV_USE_PREINSTALLED_TF:-0}"". export TF_CUDA_CLANG=0. export TF_ENABLE_XLA=1. export TF_NEED_CUDA=1. export TF_NEED_GCP=1. export TF_NEED_GDR=0. export TF_NEED_HDFS=0. export TF_NEED_JEMALLOC=0. export TF_NEED_MKL=1. export TF_NEED_MPI=0. export TF_NEED_OPENCL=0. export TF_NEED_OPENCL_SYCL=0. export TF_NEED_S3=1. export TF_NEED_VERBS=0. # Used if TF_NEED_CUDA=1. export TF_CUDA_VERSION=""10.0"". export CUDA_TOOLKIT_PATH=""/usr/local/cuda"". export TF_CUDNN_VERSION=""7"". export CUDNN_INSTALL_PATH=""/usr/lib/x86_64-linux-gnu"". # The version of bazel we want to build DeepVariant. DV_BAZEL_VERSION=""0.15.0"". # We need to make sure that $HOME/bin is first in the binary search path so that. # `bazel` will find the latest version of bazel installed in the user's home. # directory. This is set in setting.sh as all DeepVariant scripts source. # settings.sh and assume that `bazel` will find the right version. export PATH=""$HOME/bin:$PATH"". # Path to the public bucket containing DeepVariant-related artifacts. export DEEPVARIANT_BUCKET=""gs://deepvariant"". export DV_PACKAGE_BUCKET_PATH=""${DEEPVARIANT_BUCKET}/packages"". export DV_PACKAGE_CURL_PATH=""https://storage.googleapis.com/deepvariant/packages"". # Set this to 1 to use the nightly (latest) build of TensorFlow instead of a. # named release version. Set it to an already existing value in the environment. # (allowing command line control of the build), defaulting to 0 (release build). # Note that setting this to 1 implies that the C++ code in DeepVariant will be. # build using the master branch and not the pinned version to avoid. # incompatibilities between TensorFlow C++ used to build DeepVariant and the. # tf-nightly wheel. export DV_TF_NIGHTLY_BUILD=""${DV_TF_NIGHTLY_BUILD:-1}"". # The branch/tag we checkout to build our C++ dependencies against. This is not. # the same as the python version of TensorFlow we use, but should be similar or. # we risk having version incompatibilities between our C++ code a",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:3868,deployability,artifact,artifacts,3868,"{DV_USE_PREINSTALLED_TF:-0}"". export TF_CUDA_CLANG=0. export TF_ENABLE_XLA=1. export TF_NEED_CUDA=1. export TF_NEED_GCP=1. export TF_NEED_GDR=0. export TF_NEED_HDFS=0. export TF_NEED_JEMALLOC=0. export TF_NEED_MKL=1. export TF_NEED_MPI=0. export TF_NEED_OPENCL=0. export TF_NEED_OPENCL_SYCL=0. export TF_NEED_S3=1. export TF_NEED_VERBS=0. # Used if TF_NEED_CUDA=1. export TF_CUDA_VERSION=""10.0"". export CUDA_TOOLKIT_PATH=""/usr/local/cuda"". export TF_CUDNN_VERSION=""7"". export CUDNN_INSTALL_PATH=""/usr/lib/x86_64-linux-gnu"". # The version of bazel we want to build DeepVariant. DV_BAZEL_VERSION=""0.15.0"". # We need to make sure that $HOME/bin is first in the binary search path so that. # `bazel` will find the latest version of bazel installed in the user's home. # directory. This is set in setting.sh as all DeepVariant scripts source. # settings.sh and assume that `bazel` will find the right version. export PATH=""$HOME/bin:$PATH"". # Path to the public bucket containing DeepVariant-related artifacts. export DEEPVARIANT_BUCKET=""gs://deepvariant"". export DV_PACKAGE_BUCKET_PATH=""${DEEPVARIANT_BUCKET}/packages"". export DV_PACKAGE_CURL_PATH=""https://storage.googleapis.com/deepvariant/packages"". # Set this to 1 to use the nightly (latest) build of TensorFlow instead of a. # named release version. Set it to an already existing value in the environment. # (allowing command line control of the build), defaulting to 0 (release build). # Note that setting this to 1 implies that the C++ code in DeepVariant will be. # build using the master branch and not the pinned version to avoid. # incompatibilities between TensorFlow C++ used to build DeepVariant and the. # tf-nightly wheel. export DV_TF_NIGHTLY_BUILD=""${DV_TF_NIGHTLY_BUILD:-1}"". # The branch/tag we checkout to build our C++ dependencies against. This is not. # the same as the python version of TensorFlow we use, but should be similar or. # we risk having version incompatibilities between our C++ code and the Python. # code we use at",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:4116,deployability,build,build,4116,"TF_NEED_OPENCL=0. export TF_NEED_OPENCL_SYCL=0. export TF_NEED_S3=1. export TF_NEED_VERBS=0. # Used if TF_NEED_CUDA=1. export TF_CUDA_VERSION=""10.0"". export CUDA_TOOLKIT_PATH=""/usr/local/cuda"". export TF_CUDNN_VERSION=""7"". export CUDNN_INSTALL_PATH=""/usr/lib/x86_64-linux-gnu"". # The version of bazel we want to build DeepVariant. DV_BAZEL_VERSION=""0.15.0"". # We need to make sure that $HOME/bin is first in the binary search path so that. # `bazel` will find the latest version of bazel installed in the user's home. # directory. This is set in setting.sh as all DeepVariant scripts source. # settings.sh and assume that `bazel` will find the right version. export PATH=""$HOME/bin:$PATH"". # Path to the public bucket containing DeepVariant-related artifacts. export DEEPVARIANT_BUCKET=""gs://deepvariant"". export DV_PACKAGE_BUCKET_PATH=""${DEEPVARIANT_BUCKET}/packages"". export DV_PACKAGE_CURL_PATH=""https://storage.googleapis.com/deepvariant/packages"". # Set this to 1 to use the nightly (latest) build of TensorFlow instead of a. # named release version. Set it to an already existing value in the environment. # (allowing command line control of the build), defaulting to 0 (release build). # Note that setting this to 1 implies that the C++ code in DeepVariant will be. # build using the master branch and not the pinned version to avoid. # incompatibilities between TensorFlow C++ used to build DeepVariant and the. # tf-nightly wheel. export DV_TF_NIGHTLY_BUILD=""${DV_TF_NIGHTLY_BUILD:-1}"". # The branch/tag we checkout to build our C++ dependencies against. This is not. # the same as the python version of TensorFlow we use, but should be similar or. # we risk having version incompatibilities between our C++ code and the Python. # code we use at runtime. if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then. export DV_CPP_TENSORFLOW_TAG=""master"". else. export DV_CPP_TENSORFLOW_TAG=""r1.12"". fi. export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:4158,deployability,releas,release,4158,"L=0. export TF_NEED_S3=1. export TF_NEED_VERBS=0. # Used if TF_NEED_CUDA=1. export TF_CUDA_VERSION=""10.0"". export CUDA_TOOLKIT_PATH=""/usr/local/cuda"". export TF_CUDNN_VERSION=""7"". export CUDNN_INSTALL_PATH=""/usr/lib/x86_64-linux-gnu"". # The version of bazel we want to build DeepVariant. DV_BAZEL_VERSION=""0.15.0"". # We need to make sure that $HOME/bin is first in the binary search path so that. # `bazel` will find the latest version of bazel installed in the user's home. # directory. This is set in setting.sh as all DeepVariant scripts source. # settings.sh and assume that `bazel` will find the right version. export PATH=""$HOME/bin:$PATH"". # Path to the public bucket containing DeepVariant-related artifacts. export DEEPVARIANT_BUCKET=""gs://deepvariant"". export DV_PACKAGE_BUCKET_PATH=""${DEEPVARIANT_BUCKET}/packages"". export DV_PACKAGE_CURL_PATH=""https://storage.googleapis.com/deepvariant/packages"". # Set this to 1 to use the nightly (latest) build of TensorFlow instead of a. # named release version. Set it to an already existing value in the environment. # (allowing command line control of the build), defaulting to 0 (release build). # Note that setting this to 1 implies that the C++ code in DeepVariant will be. # build using the master branch and not the pinned version to avoid. # incompatibilities between TensorFlow C++ used to build DeepVariant and the. # tf-nightly wheel. export DV_TF_NIGHTLY_BUILD=""${DV_TF_NIGHTLY_BUILD:-1}"". # The branch/tag we checkout to build our C++ dependencies against. This is not. # the same as the python version of TensorFlow we use, but should be similar or. # we risk having version incompatibilities between our C++ code and the Python. # code we use at runtime. if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then. export DV_CPP_TENSORFLOW_TAG=""master"". else. export DV_CPP_TENSORFLOW_TAG=""r1.12"". fi. export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_CPU_WHL_VE",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:4166,deployability,version,version,4166,"ort TF_NEED_S3=1. export TF_NEED_VERBS=0. # Used if TF_NEED_CUDA=1. export TF_CUDA_VERSION=""10.0"". export CUDA_TOOLKIT_PATH=""/usr/local/cuda"". export TF_CUDNN_VERSION=""7"". export CUDNN_INSTALL_PATH=""/usr/lib/x86_64-linux-gnu"". # The version of bazel we want to build DeepVariant. DV_BAZEL_VERSION=""0.15.0"". # We need to make sure that $HOME/bin is first in the binary search path so that. # `bazel` will find the latest version of bazel installed in the user's home. # directory. This is set in setting.sh as all DeepVariant scripts source. # settings.sh and assume that `bazel` will find the right version. export PATH=""$HOME/bin:$PATH"". # Path to the public bucket containing DeepVariant-related artifacts. export DEEPVARIANT_BUCKET=""gs://deepvariant"". export DV_PACKAGE_BUCKET_PATH=""${DEEPVARIANT_BUCKET}/packages"". export DV_PACKAGE_CURL_PATH=""https://storage.googleapis.com/deepvariant/packages"". # Set this to 1 to use the nightly (latest) build of TensorFlow instead of a. # named release version. Set it to an already existing value in the environment. # (allowing command line control of the build), defaulting to 0 (release build). # Note that setting this to 1 implies that the C++ code in DeepVariant will be. # build using the master branch and not the pinned version to avoid. # incompatibilities between TensorFlow C++ used to build DeepVariant and the. # tf-nightly wheel. export DV_TF_NIGHTLY_BUILD=""${DV_TF_NIGHTLY_BUILD:-1}"". # The branch/tag we checkout to build our C++ dependencies against. This is not. # the same as the python version of TensorFlow we use, but should be similar or. # we risk having version incompatibilities between our C++ code and the Python. # code we use at runtime. if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then. export DV_CPP_TENSORFLOW_TAG=""master"". else. export DV_CPP_TENSORFLOW_TAG=""r1.12"". fi. export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:4271,deployability,build,build,4271,"t CUDA_TOOLKIT_PATH=""/usr/local/cuda"". export TF_CUDNN_VERSION=""7"". export CUDNN_INSTALL_PATH=""/usr/lib/x86_64-linux-gnu"". # The version of bazel we want to build DeepVariant. DV_BAZEL_VERSION=""0.15.0"". # We need to make sure that $HOME/bin is first in the binary search path so that. # `bazel` will find the latest version of bazel installed in the user's home. # directory. This is set in setting.sh as all DeepVariant scripts source. # settings.sh and assume that `bazel` will find the right version. export PATH=""$HOME/bin:$PATH"". # Path to the public bucket containing DeepVariant-related artifacts. export DEEPVARIANT_BUCKET=""gs://deepvariant"". export DV_PACKAGE_BUCKET_PATH=""${DEEPVARIANT_BUCKET}/packages"". export DV_PACKAGE_CURL_PATH=""https://storage.googleapis.com/deepvariant/packages"". # Set this to 1 to use the nightly (latest) build of TensorFlow instead of a. # named release version. Set it to an already existing value in the environment. # (allowing command line control of the build), defaulting to 0 (release build). # Note that setting this to 1 implies that the C++ code in DeepVariant will be. # build using the master branch and not the pinned version to avoid. # incompatibilities between TensorFlow C++ used to build DeepVariant and the. # tf-nightly wheel. export DV_TF_NIGHTLY_BUILD=""${DV_TF_NIGHTLY_BUILD:-1}"". # The branch/tag we checkout to build our C++ dependencies against. This is not. # the same as the python version of TensorFlow we use, but should be similar or. # we risk having version incompatibilities between our C++ code and the Python. # code we use at runtime. if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then. export DV_CPP_TENSORFLOW_TAG=""master"". else. export DV_CPP_TENSORFLOW_TAG=""r1.12"". fi. export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing. # value in the envi",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:4296,deployability,releas,release,4296,"local/cuda"". export TF_CUDNN_VERSION=""7"". export CUDNN_INSTALL_PATH=""/usr/lib/x86_64-linux-gnu"". # The version of bazel we want to build DeepVariant. DV_BAZEL_VERSION=""0.15.0"". # We need to make sure that $HOME/bin is first in the binary search path so that. # `bazel` will find the latest version of bazel installed in the user's home. # directory. This is set in setting.sh as all DeepVariant scripts source. # settings.sh and assume that `bazel` will find the right version. export PATH=""$HOME/bin:$PATH"". # Path to the public bucket containing DeepVariant-related artifacts. export DEEPVARIANT_BUCKET=""gs://deepvariant"". export DV_PACKAGE_BUCKET_PATH=""${DEEPVARIANT_BUCKET}/packages"". export DV_PACKAGE_CURL_PATH=""https://storage.googleapis.com/deepvariant/packages"". # Set this to 1 to use the nightly (latest) build of TensorFlow instead of a. # named release version. Set it to an already existing value in the environment. # (allowing command line control of the build), defaulting to 0 (release build). # Note that setting this to 1 implies that the C++ code in DeepVariant will be. # build using the master branch and not the pinned version to avoid. # incompatibilities between TensorFlow C++ used to build DeepVariant and the. # tf-nightly wheel. export DV_TF_NIGHTLY_BUILD=""${DV_TF_NIGHTLY_BUILD:-1}"". # The branch/tag we checkout to build our C++ dependencies against. This is not. # the same as the python version of TensorFlow we use, but should be similar or. # we risk having version incompatibilities between our C++ code and the Python. # code we use at runtime. if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then. export DV_CPP_TENSORFLOW_TAG=""master"". else. export DV_CPP_TENSORFLOW_TAG=""r1.12"". fi. export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing. # value in the environment (allowing command ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:4304,deployability,build,build,4304,"uda"". export TF_CUDNN_VERSION=""7"". export CUDNN_INSTALL_PATH=""/usr/lib/x86_64-linux-gnu"". # The version of bazel we want to build DeepVariant. DV_BAZEL_VERSION=""0.15.0"". # We need to make sure that $HOME/bin is first in the binary search path so that. # `bazel` will find the latest version of bazel installed in the user's home. # directory. This is set in setting.sh as all DeepVariant scripts source. # settings.sh and assume that `bazel` will find the right version. export PATH=""$HOME/bin:$PATH"". # Path to the public bucket containing DeepVariant-related artifacts. export DEEPVARIANT_BUCKET=""gs://deepvariant"". export DV_PACKAGE_BUCKET_PATH=""${DEEPVARIANT_BUCKET}/packages"". export DV_PACKAGE_CURL_PATH=""https://storage.googleapis.com/deepvariant/packages"". # Set this to 1 to use the nightly (latest) build of TensorFlow instead of a. # named release version. Set it to an already existing value in the environment. # (allowing command line control of the build), defaulting to 0 (release build). # Note that setting this to 1 implies that the C++ code in DeepVariant will be. # build using the master branch and not the pinned version to avoid. # incompatibilities between TensorFlow C++ used to build DeepVariant and the. # tf-nightly wheel. export DV_TF_NIGHTLY_BUILD=""${DV_TF_NIGHTLY_BUILD:-1}"". # The branch/tag we checkout to build our C++ dependencies against. This is not. # the same as the python version of TensorFlow we use, but should be similar or. # we risk having version incompatibilities between our C++ code and the Python. # code we use at runtime. if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then. export DV_CPP_TENSORFLOW_TAG=""master"". else. export DV_CPP_TENSORFLOW_TAG=""r1.12"". fi. export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing. # value in the environment (allowing command line co",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:4394,deployability,build,build,4394,"# The version of bazel we want to build DeepVariant. DV_BAZEL_VERSION=""0.15.0"". # We need to make sure that $HOME/bin is first in the binary search path so that. # `bazel` will find the latest version of bazel installed in the user's home. # directory. This is set in setting.sh as all DeepVariant scripts source. # settings.sh and assume that `bazel` will find the right version. export PATH=""$HOME/bin:$PATH"". # Path to the public bucket containing DeepVariant-related artifacts. export DEEPVARIANT_BUCKET=""gs://deepvariant"". export DV_PACKAGE_BUCKET_PATH=""${DEEPVARIANT_BUCKET}/packages"". export DV_PACKAGE_CURL_PATH=""https://storage.googleapis.com/deepvariant/packages"". # Set this to 1 to use the nightly (latest) build of TensorFlow instead of a. # named release version. Set it to an already existing value in the environment. # (allowing command line control of the build), defaulting to 0 (release build). # Note that setting this to 1 implies that the C++ code in DeepVariant will be. # build using the master branch and not the pinned version to avoid. # incompatibilities between TensorFlow C++ used to build DeepVariant and the. # tf-nightly wheel. export DV_TF_NIGHTLY_BUILD=""${DV_TF_NIGHTLY_BUILD:-1}"". # The branch/tag we checkout to build our C++ dependencies against. This is not. # the same as the python version of TensorFlow we use, but should be similar or. # we risk having version incompatibilities between our C++ code and the Python. # code we use at runtime. if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then. export DV_CPP_TENSORFLOW_TAG=""master"". else. export DV_CPP_TENSORFLOW_TAG=""r1.12"". fi. export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing. # value in the environment (allowing command line control of the build),. # defaulting to 0 (CPU only build). export DV_GPU_BUILD=""${DV_GPU_BU",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:4443,deployability,version,version,4443,"t. DV_BAZEL_VERSION=""0.15.0"". # We need to make sure that $HOME/bin is first in the binary search path so that. # `bazel` will find the latest version of bazel installed in the user's home. # directory. This is set in setting.sh as all DeepVariant scripts source. # settings.sh and assume that `bazel` will find the right version. export PATH=""$HOME/bin:$PATH"". # Path to the public bucket containing DeepVariant-related artifacts. export DEEPVARIANT_BUCKET=""gs://deepvariant"". export DV_PACKAGE_BUCKET_PATH=""${DEEPVARIANT_BUCKET}/packages"". export DV_PACKAGE_CURL_PATH=""https://storage.googleapis.com/deepvariant/packages"". # Set this to 1 to use the nightly (latest) build of TensorFlow instead of a. # named release version. Set it to an already existing value in the environment. # (allowing command line control of the build), defaulting to 0 (release build). # Note that setting this to 1 implies that the C++ code in DeepVariant will be. # build using the master branch and not the pinned version to avoid. # incompatibilities between TensorFlow C++ used to build DeepVariant and the. # tf-nightly wheel. export DV_TF_NIGHTLY_BUILD=""${DV_TF_NIGHTLY_BUILD:-1}"". # The branch/tag we checkout to build our C++ dependencies against. This is not. # the same as the python version of TensorFlow we use, but should be similar or. # we risk having version incompatibilities between our C++ code and the Python. # code we use at runtime. if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then. export DV_CPP_TENSORFLOW_TAG=""master"". else. export DV_CPP_TENSORFLOW_TAG=""r1.12"". fi. export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing. # value in the environment (allowing command line control of the build),. # defaulting to 0 (CPU only build). export DV_GPU_BUILD=""${DV_GPU_BUILD:-1}"". # If this variable is set to 1, DeepVari",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:4512,deployability,build,build,4512,"is first in the binary search path so that. # `bazel` will find the latest version of bazel installed in the user's home. # directory. This is set in setting.sh as all DeepVariant scripts source. # settings.sh and assume that `bazel` will find the right version. export PATH=""$HOME/bin:$PATH"". # Path to the public bucket containing DeepVariant-related artifacts. export DEEPVARIANT_BUCKET=""gs://deepvariant"". export DV_PACKAGE_BUCKET_PATH=""${DEEPVARIANT_BUCKET}/packages"". export DV_PACKAGE_CURL_PATH=""https://storage.googleapis.com/deepvariant/packages"". # Set this to 1 to use the nightly (latest) build of TensorFlow instead of a. # named release version. Set it to an already existing value in the environment. # (allowing command line control of the build), defaulting to 0 (release build). # Note that setting this to 1 implies that the C++ code in DeepVariant will be. # build using the master branch and not the pinned version to avoid. # incompatibilities between TensorFlow C++ used to build DeepVariant and the. # tf-nightly wheel. export DV_TF_NIGHTLY_BUILD=""${DV_TF_NIGHTLY_BUILD:-1}"". # The branch/tag we checkout to build our C++ dependencies against. This is not. # the same as the python version of TensorFlow we use, but should be similar or. # we risk having version incompatibilities between our C++ code and the Python. # code we use at runtime. if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then. export DV_CPP_TENSORFLOW_TAG=""master"". else. export DV_CPP_TENSORFLOW_TAG=""r1.12"". fi. export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing. # value in the environment (allowing command line control of the build),. # defaulting to 0 (CPU only build). export DV_GPU_BUILD=""${DV_GPU_BUILD:-1}"". # If this variable is set to 1, DeepVariant will use a TensorFlow wheel file. # compiled with MKL support fo",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:4647,deployability,build,build,4647,"This is set in setting.sh as all DeepVariant scripts source. # settings.sh and assume that `bazel` will find the right version. export PATH=""$HOME/bin:$PATH"". # Path to the public bucket containing DeepVariant-related artifacts. export DEEPVARIANT_BUCKET=""gs://deepvariant"". export DV_PACKAGE_BUCKET_PATH=""${DEEPVARIANT_BUCKET}/packages"". export DV_PACKAGE_CURL_PATH=""https://storage.googleapis.com/deepvariant/packages"". # Set this to 1 to use the nightly (latest) build of TensorFlow instead of a. # named release version. Set it to an already existing value in the environment. # (allowing command line control of the build), defaulting to 0 (release build). # Note that setting this to 1 implies that the C++ code in DeepVariant will be. # build using the master branch and not the pinned version to avoid. # incompatibilities between TensorFlow C++ used to build DeepVariant and the. # tf-nightly wheel. export DV_TF_NIGHTLY_BUILD=""${DV_TF_NIGHTLY_BUILD:-1}"". # The branch/tag we checkout to build our C++ dependencies against. This is not. # the same as the python version of TensorFlow we use, but should be similar or. # we risk having version incompatibilities between our C++ code and the Python. # code we use at runtime. if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then. export DV_CPP_TENSORFLOW_TAG=""master"". else. export DV_CPP_TENSORFLOW_TAG=""r1.12"". fi. export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing. # value in the environment (allowing command line control of the build),. # defaulting to 0 (CPU only build). export DV_GPU_BUILD=""${DV_GPU_BUILD:-1}"". # If this variable is set to 1, DeepVariant will use a TensorFlow wheel file. # compiled with MKL support for corei7 or better chipsets, which. # significantly speeds up execution when running on modern CPUs. The default. # TensorFlow wheel fi",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:4661,deployability,depend,dependencies,4661,"tting.sh as all DeepVariant scripts source. # settings.sh and assume that `bazel` will find the right version. export PATH=""$HOME/bin:$PATH"". # Path to the public bucket containing DeepVariant-related artifacts. export DEEPVARIANT_BUCKET=""gs://deepvariant"". export DV_PACKAGE_BUCKET_PATH=""${DEEPVARIANT_BUCKET}/packages"". export DV_PACKAGE_CURL_PATH=""https://storage.googleapis.com/deepvariant/packages"". # Set this to 1 to use the nightly (latest) build of TensorFlow instead of a. # named release version. Set it to an already existing value in the environment. # (allowing command line control of the build), defaulting to 0 (release build). # Note that setting this to 1 implies that the C++ code in DeepVariant will be. # build using the master branch and not the pinned version to avoid. # incompatibilities between TensorFlow C++ used to build DeepVariant and the. # tf-nightly wheel. export DV_TF_NIGHTLY_BUILD=""${DV_TF_NIGHTLY_BUILD:-1}"". # The branch/tag we checkout to build our C++ dependencies against. This is not. # the same as the python version of TensorFlow we use, but should be similar or. # we risk having version incompatibilities between our C++ code and the Python. # code we use at runtime. if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then. export DV_CPP_TENSORFLOW_TAG=""master"". else. export DV_CPP_TENSORFLOW_TAG=""r1.12"". fi. export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing. # value in the environment (allowing command line control of the build),. # defaulting to 0 (CPU only build). export DV_GPU_BUILD=""${DV_GPU_BUILD:-1}"". # If this variable is set to 1, DeepVariant will use a TensorFlow wheel file. # compiled with MKL support for corei7 or better chipsets, which. # significantly speeds up execution when running on modern CPUs. The default. # TensorFlow wheel files don't contain",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:4721,deployability,version,version,4721,"and assume that `bazel` will find the right version. export PATH=""$HOME/bin:$PATH"". # Path to the public bucket containing DeepVariant-related artifacts. export DEEPVARIANT_BUCKET=""gs://deepvariant"". export DV_PACKAGE_BUCKET_PATH=""${DEEPVARIANT_BUCKET}/packages"". export DV_PACKAGE_CURL_PATH=""https://storage.googleapis.com/deepvariant/packages"". # Set this to 1 to use the nightly (latest) build of TensorFlow instead of a. # named release version. Set it to an already existing value in the environment. # (allowing command line control of the build), defaulting to 0 (release build). # Note that setting this to 1 implies that the C++ code in DeepVariant will be. # build using the master branch and not the pinned version to avoid. # incompatibilities between TensorFlow C++ used to build DeepVariant and the. # tf-nightly wheel. export DV_TF_NIGHTLY_BUILD=""${DV_TF_NIGHTLY_BUILD:-1}"". # The branch/tag we checkout to build our C++ dependencies against. This is not. # the same as the python version of TensorFlow we use, but should be similar or. # we risk having version incompatibilities between our C++ code and the Python. # code we use at runtime. if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then. export DV_CPP_TENSORFLOW_TAG=""master"". else. export DV_CPP_TENSORFLOW_TAG=""r1.12"". fi. export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing. # value in the environment (allowing command line control of the build),. # defaulting to 0 (CPU only build). export DV_GPU_BUILD=""${DV_GPU_BUILD:-1}"". # If this variable is set to 1, DeepVariant will use a TensorFlow wheel file. # compiled with MKL support for corei7 or better chipsets, which. # significantly speeds up execution when running on modern CPUs. The default. # TensorFlow wheel files don't contain these instructions (and thereby run on a. # broader set o",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:4794,deployability,version,version,4794,"in:$PATH"". # Path to the public bucket containing DeepVariant-related artifacts. export DEEPVARIANT_BUCKET=""gs://deepvariant"". export DV_PACKAGE_BUCKET_PATH=""${DEEPVARIANT_BUCKET}/packages"". export DV_PACKAGE_CURL_PATH=""https://storage.googleapis.com/deepvariant/packages"". # Set this to 1 to use the nightly (latest) build of TensorFlow instead of a. # named release version. Set it to an already existing value in the environment. # (allowing command line control of the build), defaulting to 0 (release build). # Note that setting this to 1 implies that the C++ code in DeepVariant will be. # build using the master branch and not the pinned version to avoid. # incompatibilities between TensorFlow C++ used to build DeepVariant and the. # tf-nightly wheel. export DV_TF_NIGHTLY_BUILD=""${DV_TF_NIGHTLY_BUILD:-1}"". # The branch/tag we checkout to build our C++ dependencies against. This is not. # the same as the python version of TensorFlow we use, but should be similar or. # we risk having version incompatibilities between our C++ code and the Python. # code we use at runtime. if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then. export DV_CPP_TENSORFLOW_TAG=""master"". else. export DV_CPP_TENSORFLOW_TAG=""r1.12"". fi. export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing. # value in the environment (allowing command line control of the build),. # defaulting to 0 (CPU only build). export DV_GPU_BUILD=""${DV_GPU_BUILD:-1}"". # If this variable is set to 1, DeepVariant will use a TensorFlow wheel file. # compiled with MKL support for corei7 or better chipsets, which. # significantly speeds up execution when running on modern CPUs. The default. # TensorFlow wheel files don't contain these instructions (and thereby run on a. # broader set of CPUs). Using this optimized wheel reduces the runtime of. # DeepVariant",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:5320,deployability,build,build,5320," setting this to 1 implies that the C++ code in DeepVariant will be. # build using the master branch and not the pinned version to avoid. # incompatibilities between TensorFlow C++ used to build DeepVariant and the. # tf-nightly wheel. export DV_TF_NIGHTLY_BUILD=""${DV_TF_NIGHTLY_BUILD:-1}"". # The branch/tag we checkout to build our C++ dependencies against. This is not. # the same as the python version of TensorFlow we use, but should be similar or. # we risk having version incompatibilities between our C++ code and the Python. # code we use at runtime. if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then. export DV_CPP_TENSORFLOW_TAG=""master"". else. export DV_CPP_TENSORFLOW_TAG=""r1.12"". fi. export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing. # value in the environment (allowing command line control of the build),. # defaulting to 0 (CPU only build). export DV_GPU_BUILD=""${DV_GPU_BUILD:-1}"". # If this variable is set to 1, DeepVariant will use a TensorFlow wheel file. # compiled with MKL support for corei7 or better chipsets, which. # significantly speeds up execution when running on modern CPUs. The default. # TensorFlow wheel files don't contain these instructions (and thereby run on a. # broader set of CPUs). Using this optimized wheel reduces the runtime of. # DeepVariant's call_variants step by >3x. This is called the GCP (Google Cloud. # Platform) optimized wheel because all GCP instances have at least Sandy Bridge. # or better chipsets, so this wheel should run anywhere on GCP. export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}"". export GCP_OPTIMIZED_TF_WHL_FILENAME=""tensorflow-${DV_GCP_OPTIMIZED_TF_WHL_VERSION}.deepvariant_gcp-cp27-none-linux_x86_64.whl"". export GCP_OPTIMIZED_TF_WHL_PATH=""${DV_PACKAGE_BUCKET_PATH}/tensorflow"". export GCP_OPTIMIZED_TF_WHL_CURL_PATH",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:5357,deployability,build,build,5357,"++ code in DeepVariant will be. # build using the master branch and not the pinned version to avoid. # incompatibilities between TensorFlow C++ used to build DeepVariant and the. # tf-nightly wheel. export DV_TF_NIGHTLY_BUILD=""${DV_TF_NIGHTLY_BUILD:-1}"". # The branch/tag we checkout to build our C++ dependencies against. This is not. # the same as the python version of TensorFlow we use, but should be similar or. # we risk having version incompatibilities between our C++ code and the Python. # code we use at runtime. if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then. export DV_CPP_TENSORFLOW_TAG=""master"". else. export DV_CPP_TENSORFLOW_TAG=""r1.12"". fi. export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing. # value in the environment (allowing command line control of the build),. # defaulting to 0 (CPU only build). export DV_GPU_BUILD=""${DV_GPU_BUILD:-1}"". # If this variable is set to 1, DeepVariant will use a TensorFlow wheel file. # compiled with MKL support for corei7 or better chipsets, which. # significantly speeds up execution when running on modern CPUs. The default. # TensorFlow wheel files don't contain these instructions (and thereby run on a. # broader set of CPUs). Using this optimized wheel reduces the runtime of. # DeepVariant's call_variants step by >3x. This is called the GCP (Google Cloud. # Platform) optimized wheel because all GCP instances have at least Sandy Bridge. # or better chipsets, so this wheel should run anywhere on GCP. export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}"". export GCP_OPTIMIZED_TF_WHL_FILENAME=""tensorflow-${DV_GCP_OPTIMIZED_TF_WHL_VERSION}.deepvariant_gcp-cp27-none-linux_x86_64.whl"". export GCP_OPTIMIZED_TF_WHL_PATH=""${DV_PACKAGE_BUCKET_PATH}/tensorflow"". export GCP_OPTIMIZED_TF_WHL_CURL_PATH=""${DV_PACKAGE_CURL_PATH}/tensorflow""",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:5660,deployability,contain,contain,5660,"endencies against. This is not. # the same as the python version of TensorFlow we use, but should be similar or. # we risk having version incompatibilities between our C++ code and the Python. # code we use at runtime. if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then. export DV_CPP_TENSORFLOW_TAG=""master"". else. export DV_CPP_TENSORFLOW_TAG=""r1.12"". fi. export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing. # value in the environment (allowing command line control of the build),. # defaulting to 0 (CPU only build). export DV_GPU_BUILD=""${DV_GPU_BUILD:-1}"". # If this variable is set to 1, DeepVariant will use a TensorFlow wheel file. # compiled with MKL support for corei7 or better chipsets, which. # significantly speeds up execution when running on modern CPUs. The default. # TensorFlow wheel files don't contain these instructions (and thereby run on a. # broader set of CPUs). Using this optimized wheel reduces the runtime of. # DeepVariant's call_variants step by >3x. This is called the GCP (Google Cloud. # Platform) optimized wheel because all GCP instances have at least Sandy Bridge. # or better chipsets, so this wheel should run anywhere on GCP. export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}"". export GCP_OPTIMIZED_TF_WHL_FILENAME=""tensorflow-${DV_GCP_OPTIMIZED_TF_WHL_VERSION}.deepvariant_gcp-cp27-none-linux_x86_64.whl"". export GCP_OPTIMIZED_TF_WHL_PATH=""${DV_PACKAGE_BUCKET_PATH}/tensorflow"". export GCP_OPTIMIZED_TF_WHL_CURL_PATH=""${DV_PACKAGE_CURL_PATH}/tensorflow"". # Set this to 1 to make our prereq scripts install the CUDA libraries. # If you already have CUDA installed, such as on a properly provisioned. # Docker image, it shouldn't be necessary. export DV_INSTALL_GPU_DRIVERS=""${DV_INSTALL_GPU_DRIVERS:-0}"". export PYTHON_BIN_PATH=$(which python). export USE_DE",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:6405,deployability,instal,install,6405,"t runtime. if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then. export DV_CPP_TENSORFLOW_TAG=""master"". else. export DV_CPP_TENSORFLOW_TAG=""r1.12"". fi. export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing. # value in the environment (allowing command line control of the build),. # defaulting to 0 (CPU only build). export DV_GPU_BUILD=""${DV_GPU_BUILD:-1}"". # If this variable is set to 1, DeepVariant will use a TensorFlow wheel file. # compiled with MKL support for corei7 or better chipsets, which. # significantly speeds up execution when running on modern CPUs. The default. # TensorFlow wheel files don't contain these instructions (and thereby run on a. # broader set of CPUs). Using this optimized wheel reduces the runtime of. # DeepVariant's call_variants step by >3x. This is called the GCP (Google Cloud. # Platform) optimized wheel because all GCP instances have at least Sandy Bridge. # or better chipsets, so this wheel should run anywhere on GCP. export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}"". export GCP_OPTIMIZED_TF_WHL_FILENAME=""tensorflow-${DV_GCP_OPTIMIZED_TF_WHL_VERSION}.deepvariant_gcp-cp27-none-linux_x86_64.whl"". export GCP_OPTIMIZED_TF_WHL_PATH=""${DV_PACKAGE_BUCKET_PATH}/tensorflow"". export GCP_OPTIMIZED_TF_WHL_CURL_PATH=""${DV_PACKAGE_CURL_PATH}/tensorflow"". # Set this to 1 to make our prereq scripts install the CUDA libraries. # If you already have CUDA installed, such as on a properly provisioned. # Docker image, it shouldn't be necessary. export DV_INSTALL_GPU_DRIVERS=""${DV_INSTALL_GPU_DRIVERS:-0}"". export PYTHON_BIN_PATH=$(which python). export USE_DEFAULT_PYTHON_LIB_PATH=1. export DV_COPT_FLAGS=""--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings"". function note_build_stage {. echo ""========== [$(date)] Stage '${1}' starting"". }. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:6460,deployability,instal,installed,6460,"t runtime. if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then. export DV_CPP_TENSORFLOW_TAG=""master"". else. export DV_CPP_TENSORFLOW_TAG=""r1.12"". fi. export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing. # value in the environment (allowing command line control of the build),. # defaulting to 0 (CPU only build). export DV_GPU_BUILD=""${DV_GPU_BUILD:-1}"". # If this variable is set to 1, DeepVariant will use a TensorFlow wheel file. # compiled with MKL support for corei7 or better chipsets, which. # significantly speeds up execution when running on modern CPUs. The default. # TensorFlow wheel files don't contain these instructions (and thereby run on a. # broader set of CPUs). Using this optimized wheel reduces the runtime of. # DeepVariant's call_variants step by >3x. This is called the GCP (Google Cloud. # Platform) optimized wheel because all GCP instances have at least Sandy Bridge. # or better chipsets, so this wheel should run anywhere on GCP. export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}"". export GCP_OPTIMIZED_TF_WHL_FILENAME=""tensorflow-${DV_GCP_OPTIMIZED_TF_WHL_VERSION}.deepvariant_gcp-cp27-none-linux_x86_64.whl"". export GCP_OPTIMIZED_TF_WHL_PATH=""${DV_PACKAGE_BUCKET_PATH}/tensorflow"". export GCP_OPTIMIZED_TF_WHL_CURL_PATH=""${DV_PACKAGE_CURL_PATH}/tensorflow"". # Set this to 1 to make our prereq scripts install the CUDA libraries. # If you already have CUDA installed, such as on a properly provisioned. # Docker image, it shouldn't be necessary. export DV_INSTALL_GPU_DRIVERS=""${DV_INSTALL_GPU_DRIVERS:-0}"". export PYTHON_BIN_PATH=$(which python). export USE_DEFAULT_PYTHON_LIB_PATH=1. export DV_COPT_FLAGS=""--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings"". function note_build_stage {. echo ""========== [$(date)] Stage '${1}' starting"". }. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:6493,deployability,provis,provisioned,6493,"t runtime. if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then. export DV_CPP_TENSORFLOW_TAG=""master"". else. export DV_CPP_TENSORFLOW_TAG=""r1.12"". fi. export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing. # value in the environment (allowing command line control of the build),. # defaulting to 0 (CPU only build). export DV_GPU_BUILD=""${DV_GPU_BUILD:-1}"". # If this variable is set to 1, DeepVariant will use a TensorFlow wheel file. # compiled with MKL support for corei7 or better chipsets, which. # significantly speeds up execution when running on modern CPUs. The default. # TensorFlow wheel files don't contain these instructions (and thereby run on a. # broader set of CPUs). Using this optimized wheel reduces the runtime of. # DeepVariant's call_variants step by >3x. This is called the GCP (Google Cloud. # Platform) optimized wheel because all GCP instances have at least Sandy Bridge. # or better chipsets, so this wheel should run anywhere on GCP. export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}"". export GCP_OPTIMIZED_TF_WHL_FILENAME=""tensorflow-${DV_GCP_OPTIMIZED_TF_WHL_VERSION}.deepvariant_gcp-cp27-none-linux_x86_64.whl"". export GCP_OPTIMIZED_TF_WHL_PATH=""${DV_PACKAGE_BUCKET_PATH}/tensorflow"". export GCP_OPTIMIZED_TF_WHL_CURL_PATH=""${DV_PACKAGE_CURL_PATH}/tensorflow"". # Set this to 1 to make our prereq scripts install the CUDA libraries. # If you already have CUDA installed, such as on a properly provisioned. # Docker image, it shouldn't be necessary. export DV_INSTALL_GPU_DRIVERS=""${DV_INSTALL_GPU_DRIVERS:-0}"". export PYTHON_BIN_PATH=$(which python). export USE_DEFAULT_PYTHON_LIB_PATH=1. export DV_COPT_FLAGS=""--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings"". function note_build_stage {. echo ""========== [$(date)] Stage '${1}' starting"". }. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:6841,deployability,Stage,Stage,6841,"t runtime. if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then. export DV_CPP_TENSORFLOW_TAG=""master"". else. export DV_CPP_TENSORFLOW_TAG=""r1.12"". fi. export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing. # value in the environment (allowing command line control of the build),. # defaulting to 0 (CPU only build). export DV_GPU_BUILD=""${DV_GPU_BUILD:-1}"". # If this variable is set to 1, DeepVariant will use a TensorFlow wheel file. # compiled with MKL support for corei7 or better chipsets, which. # significantly speeds up execution when running on modern CPUs. The default. # TensorFlow wheel files don't contain these instructions (and thereby run on a. # broader set of CPUs). Using this optimized wheel reduces the runtime of. # DeepVariant's call_variants step by >3x. This is called the GCP (Google Cloud. # Platform) optimized wheel because all GCP instances have at least Sandy Bridge. # or better chipsets, so this wheel should run anywhere on GCP. export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}"". export GCP_OPTIMIZED_TF_WHL_FILENAME=""tensorflow-${DV_GCP_OPTIMIZED_TF_WHL_VERSION}.deepvariant_gcp-cp27-none-linux_x86_64.whl"". export GCP_OPTIMIZED_TF_WHL_PATH=""${DV_PACKAGE_BUCKET_PATH}/tensorflow"". export GCP_OPTIMIZED_TF_WHL_CURL_PATH=""${DV_PACKAGE_CURL_PATH}/tensorflow"". # Set this to 1 to make our prereq scripts install the CUDA libraries. # If you already have CUDA installed, such as on a properly provisioned. # Docker image, it shouldn't be necessary. export DV_INSTALL_GPU_DRIVERS=""${DV_INSTALL_GPU_DRIVERS:-0}"". export PYTHON_BIN_PATH=$(which python). export USE_DEFAULT_PYTHON_LIB_PATH=1. export DV_COPT_FLAGS=""--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings"". function note_build_stage {. echo ""========== [$(date)] Stage '${1}' starting"". }. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:5218,energy efficiency,GPU,GPUs,5218,"vironment. # (allowing command line control of the build), defaulting to 0 (release build). # Note that setting this to 1 implies that the C++ code in DeepVariant will be. # build using the master branch and not the pinned version to avoid. # incompatibilities between TensorFlow C++ used to build DeepVariant and the. # tf-nightly wheel. export DV_TF_NIGHTLY_BUILD=""${DV_TF_NIGHTLY_BUILD:-1}"". # The branch/tag we checkout to build our C++ dependencies against. This is not. # the same as the python version of TensorFlow we use, but should be similar or. # we risk having version incompatibilities between our C++ code and the Python. # code we use at runtime. if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then. export DV_CPP_TENSORFLOW_TAG=""master"". else. export DV_CPP_TENSORFLOW_TAG=""r1.12"". fi. export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing. # value in the environment (allowing command line control of the build),. # defaulting to 0 (CPU only build). export DV_GPU_BUILD=""${DV_GPU_BUILD:-1}"". # If this variable is set to 1, DeepVariant will use a TensorFlow wheel file. # compiled with MKL support for corei7 or better chipsets, which. # significantly speeds up execution when running on modern CPUs. The default. # TensorFlow wheel files don't contain these instructions (and thereby run on a. # broader set of CPUs). Using this optimized wheel reduces the runtime of. # DeepVariant's call_variants step by >3x. This is called the GCP (Google Cloud. # Platform) optimized wheel because all GCP instances have at least Sandy Bridge. # or better chipsets, so this wheel should run anywhere on GCP. export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}"". export GCP_OPTIMIZED_TF_WHL_FILENAME=""tensorflow-${DV_GCP_OPTIMIZED_TF_WHL_VERSION}.deepvariant_gcp-cp27-none-linux_x86_64.whl"". export ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:5348,energy efficiency,CPU,CPU,5348,"that the C++ code in DeepVariant will be. # build using the master branch and not the pinned version to avoid. # incompatibilities between TensorFlow C++ used to build DeepVariant and the. # tf-nightly wheel. export DV_TF_NIGHTLY_BUILD=""${DV_TF_NIGHTLY_BUILD:-1}"". # The branch/tag we checkout to build our C++ dependencies against. This is not. # the same as the python version of TensorFlow we use, but should be similar or. # we risk having version incompatibilities between our C++ code and the Python. # code we use at runtime. if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then. export DV_CPP_TENSORFLOW_TAG=""master"". else. export DV_CPP_TENSORFLOW_TAG=""r1.12"". fi. export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing. # value in the environment (allowing command line control of the build),. # defaulting to 0 (CPU only build). export DV_GPU_BUILD=""${DV_GPU_BUILD:-1}"". # If this variable is set to 1, DeepVariant will use a TensorFlow wheel file. # compiled with MKL support for corei7 or better chipsets, which. # significantly speeds up execution when running on modern CPUs. The default. # TensorFlow wheel files don't contain these instructions (and thereby run on a. # broader set of CPUs). Using this optimized wheel reduces the runtime of. # DeepVariant's call_variants step by >3x. This is called the GCP (Google Cloud. # Platform) optimized wheel because all GCP instances have at least Sandy Bridge. # or better chipsets, so this wheel should run anywhere on GCP. export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}"". export GCP_OPTIMIZED_TF_WHL_FILENAME=""tensorflow-${DV_GCP_OPTIMIZED_TF_WHL_VERSION}.deepvariant_gcp-cp27-none-linux_x86_64.whl"". export GCP_OPTIMIZED_TF_WHL_PATH=""${DV_PACKAGE_BUCKET_PATH}/tensorflow"". export GCP_OPTIMIZED_TF_WHL_CURL_PATH=""${DV_PACKAGE_CURL_PATH}/t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:5610,energy efficiency,CPU,CPUs,5610,""". # The branch/tag we checkout to build our C++ dependencies against. This is not. # the same as the python version of TensorFlow we use, but should be similar or. # we risk having version incompatibilities between our C++ code and the Python. # code we use at runtime. if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then. export DV_CPP_TENSORFLOW_TAG=""master"". else. export DV_CPP_TENSORFLOW_TAG=""r1.12"". fi. export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing. # value in the environment (allowing command line control of the build),. # defaulting to 0 (CPU only build). export DV_GPU_BUILD=""${DV_GPU_BUILD:-1}"". # If this variable is set to 1, DeepVariant will use a TensorFlow wheel file. # compiled with MKL support for corei7 or better chipsets, which. # significantly speeds up execution when running on modern CPUs. The default. # TensorFlow wheel files don't contain these instructions (and thereby run on a. # broader set of CPUs). Using this optimized wheel reduces the runtime of. # DeepVariant's call_variants step by >3x. This is called the GCP (Google Cloud. # Platform) optimized wheel because all GCP instances have at least Sandy Bridge. # or better chipsets, so this wheel should run anywhere on GCP. export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}"". export GCP_OPTIMIZED_TF_WHL_FILENAME=""tensorflow-${DV_GCP_OPTIMIZED_TF_WHL_VERSION}.deepvariant_gcp-cp27-none-linux_x86_64.whl"". export GCP_OPTIMIZED_TF_WHL_PATH=""${DV_PACKAGE_BUCKET_PATH}/tensorflow"". export GCP_OPTIMIZED_TF_WHL_CURL_PATH=""${DV_PACKAGE_CURL_PATH}/tensorflow"". # Set this to 1 to make our prereq scripts install the CUDA libraries. # If you already have CUDA installed, such as on a properly provisioned. # Docker image, it shouldn't be necessary. export DV_INSTALL_GPU_DRIVERS=""${DV_INSTALL_GPU_DRIVERS:-0}"". e",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:5727,energy efficiency,CPU,CPUs,5727,"of TensorFlow we use, but should be similar or. # we risk having version incompatibilities between our C++ code and the Python. # code we use at runtime. if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then. export DV_CPP_TENSORFLOW_TAG=""master"". else. export DV_CPP_TENSORFLOW_TAG=""r1.12"". fi. export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing. # value in the environment (allowing command line control of the build),. # defaulting to 0 (CPU only build). export DV_GPU_BUILD=""${DV_GPU_BUILD:-1}"". # If this variable is set to 1, DeepVariant will use a TensorFlow wheel file. # compiled with MKL support for corei7 or better chipsets, which. # significantly speeds up execution when running on modern CPUs. The default. # TensorFlow wheel files don't contain these instructions (and thereby run on a. # broader set of CPUs). Using this optimized wheel reduces the runtime of. # DeepVariant's call_variants step by >3x. This is called the GCP (Google Cloud. # Platform) optimized wheel because all GCP instances have at least Sandy Bridge. # or better chipsets, so this wheel should run anywhere on GCP. export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}"". export GCP_OPTIMIZED_TF_WHL_FILENAME=""tensorflow-${DV_GCP_OPTIMIZED_TF_WHL_VERSION}.deepvariant_gcp-cp27-none-linux_x86_64.whl"". export GCP_OPTIMIZED_TF_WHL_PATH=""${DV_PACKAGE_BUCKET_PATH}/tensorflow"". export GCP_OPTIMIZED_TF_WHL_CURL_PATH=""${DV_PACKAGE_CURL_PATH}/tensorflow"". # Set this to 1 to make our prereq scripts install the CUDA libraries. # If you already have CUDA installed, such as on a properly provisioned. # Docker image, it shouldn't be necessary. export DV_INSTALL_GPU_DRIVERS=""${DV_INSTALL_GPU_DRIVERS:-0}"". export PYTHON_BIN_PATH=$(which python). export USE_DEFAULT_PYTHON_LIB_PATH=1. export DV_COPT_FLAGS=""--copt=-march=core",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:5745,energy efficiency,optim,optimized,5745," but should be similar or. # we risk having version incompatibilities between our C++ code and the Python. # code we use at runtime. if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then. export DV_CPP_TENSORFLOW_TAG=""master"". else. export DV_CPP_TENSORFLOW_TAG=""r1.12"". fi. export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing. # value in the environment (allowing command line control of the build),. # defaulting to 0 (CPU only build). export DV_GPU_BUILD=""${DV_GPU_BUILD:-1}"". # If this variable is set to 1, DeepVariant will use a TensorFlow wheel file. # compiled with MKL support for corei7 or better chipsets, which. # significantly speeds up execution when running on modern CPUs. The default. # TensorFlow wheel files don't contain these instructions (and thereby run on a. # broader set of CPUs). Using this optimized wheel reduces the runtime of. # DeepVariant's call_variants step by >3x. This is called the GCP (Google Cloud. # Platform) optimized wheel because all GCP instances have at least Sandy Bridge. # or better chipsets, so this wheel should run anywhere on GCP. export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}"". export GCP_OPTIMIZED_TF_WHL_FILENAME=""tensorflow-${DV_GCP_OPTIMIZED_TF_WHL_VERSION}.deepvariant_gcp-cp27-none-linux_x86_64.whl"". export GCP_OPTIMIZED_TF_WHL_PATH=""${DV_PACKAGE_BUCKET_PATH}/tensorflow"". export GCP_OPTIMIZED_TF_WHL_CURL_PATH=""${DV_PACKAGE_CURL_PATH}/tensorflow"". # Set this to 1 to make our prereq scripts install the CUDA libraries. # If you already have CUDA installed, such as on a properly provisioned. # Docker image, it shouldn't be necessary. export DV_INSTALL_GPU_DRIVERS=""${DV_INSTALL_GPU_DRIVERS:-0}"". export PYTHON_BIN_PATH=$(which python). export USE_DEFAULT_PYTHON_LIB_PATH=1. export DV_COPT_FLAGS=""--copt=-march=corei7 --copt=-Wno-sign-c",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:5761,energy efficiency,reduc,reduces,5761,"similar or. # we risk having version incompatibilities between our C++ code and the Python. # code we use at runtime. if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then. export DV_CPP_TENSORFLOW_TAG=""master"". else. export DV_CPP_TENSORFLOW_TAG=""r1.12"". fi. export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing. # value in the environment (allowing command line control of the build),. # defaulting to 0 (CPU only build). export DV_GPU_BUILD=""${DV_GPU_BUILD:-1}"". # If this variable is set to 1, DeepVariant will use a TensorFlow wheel file. # compiled with MKL support for corei7 or better chipsets, which. # significantly speeds up execution when running on modern CPUs. The default. # TensorFlow wheel files don't contain these instructions (and thereby run on a. # broader set of CPUs). Using this optimized wheel reduces the runtime of. # DeepVariant's call_variants step by >3x. This is called the GCP (Google Cloud. # Platform) optimized wheel because all GCP instances have at least Sandy Bridge. # or better chipsets, so this wheel should run anywhere on GCP. export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}"". export GCP_OPTIMIZED_TF_WHL_FILENAME=""tensorflow-${DV_GCP_OPTIMIZED_TF_WHL_VERSION}.deepvariant_gcp-cp27-none-linux_x86_64.whl"". export GCP_OPTIMIZED_TF_WHL_PATH=""${DV_PACKAGE_BUCKET_PATH}/tensorflow"". export GCP_OPTIMIZED_TF_WHL_CURL_PATH=""${DV_PACKAGE_CURL_PATH}/tensorflow"". # Set this to 1 to make our prereq scripts install the CUDA libraries. # If you already have CUDA installed, such as on a properly provisioned. # Docker image, it shouldn't be necessary. export DV_INSTALL_GPU_DRIVERS=""${DV_INSTALL_GPU_DRIVERS:-0}"". export PYTHON_BIN_PATH=$(which python). export USE_DEFAULT_PYTHON_LIB_PATH=1. export DV_COPT_FLAGS=""--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:5859,energy efficiency,Cloud,Cloud,5859,"e we use at runtime. if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then. export DV_CPP_TENSORFLOW_TAG=""master"". else. export DV_CPP_TENSORFLOW_TAG=""r1.12"". fi. export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing. # value in the environment (allowing command line control of the build),. # defaulting to 0 (CPU only build). export DV_GPU_BUILD=""${DV_GPU_BUILD:-1}"". # If this variable is set to 1, DeepVariant will use a TensorFlow wheel file. # compiled with MKL support for corei7 or better chipsets, which. # significantly speeds up execution when running on modern CPUs. The default. # TensorFlow wheel files don't contain these instructions (and thereby run on a. # broader set of CPUs). Using this optimized wheel reduces the runtime of. # DeepVariant's call_variants step by >3x. This is called the GCP (Google Cloud. # Platform) optimized wheel because all GCP instances have at least Sandy Bridge. # or better chipsets, so this wheel should run anywhere on GCP. export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}"". export GCP_OPTIMIZED_TF_WHL_FILENAME=""tensorflow-${DV_GCP_OPTIMIZED_TF_WHL_VERSION}.deepvariant_gcp-cp27-none-linux_x86_64.whl"". export GCP_OPTIMIZED_TF_WHL_PATH=""${DV_PACKAGE_BUCKET_PATH}/tensorflow"". export GCP_OPTIMIZED_TF_WHL_CURL_PATH=""${DV_PACKAGE_CURL_PATH}/tensorflow"". # Set this to 1 to make our prereq scripts install the CUDA libraries. # If you already have CUDA installed, such as on a properly provisioned. # Docker image, it shouldn't be necessary. export DV_INSTALL_GPU_DRIVERS=""${DV_INSTALL_GPU_DRIVERS:-0}"". export PYTHON_BIN_PATH=$(which python). export USE_DEFAULT_PYTHON_LIB_PATH=1. export DV_COPT_FLAGS=""--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings"". function note_build_stage {. echo ""========== [$(date)] Stage '${1}' starting",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:5878,energy efficiency,optim,optimized,5878,"t runtime. if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then. export DV_CPP_TENSORFLOW_TAG=""master"". else. export DV_CPP_TENSORFLOW_TAG=""r1.12"". fi. export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing. # value in the environment (allowing command line control of the build),. # defaulting to 0 (CPU only build). export DV_GPU_BUILD=""${DV_GPU_BUILD:-1}"". # If this variable is set to 1, DeepVariant will use a TensorFlow wheel file. # compiled with MKL support for corei7 or better chipsets, which. # significantly speeds up execution when running on modern CPUs. The default. # TensorFlow wheel files don't contain these instructions (and thereby run on a. # broader set of CPUs). Using this optimized wheel reduces the runtime of. # DeepVariant's call_variants step by >3x. This is called the GCP (Google Cloud. # Platform) optimized wheel because all GCP instances have at least Sandy Bridge. # or better chipsets, so this wheel should run anywhere on GCP. export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}"". export GCP_OPTIMIZED_TF_WHL_FILENAME=""tensorflow-${DV_GCP_OPTIMIZED_TF_WHL_VERSION}.deepvariant_gcp-cp27-none-linux_x86_64.whl"". export GCP_OPTIMIZED_TF_WHL_PATH=""${DV_PACKAGE_BUCKET_PATH}/tensorflow"". export GCP_OPTIMIZED_TF_WHL_CURL_PATH=""${DV_PACKAGE_CURL_PATH}/tensorflow"". # Set this to 1 to make our prereq scripts install the CUDA libraries. # If you already have CUDA installed, such as on a properly provisioned. # Docker image, it shouldn't be necessary. export DV_INSTALL_GPU_DRIVERS=""${DV_INSTALL_GPU_DRIVERS:-0}"". export PYTHON_BIN_PATH=$(which python). export USE_DEFAULT_PYTHON_LIB_PATH=1. export DV_COPT_FLAGS=""--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings"". function note_build_stage {. echo ""========== [$(date)] Stage '${1}' starting"". }. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:61,integrability,version,version,61,"Cannot build deepvariant with tensorflow master due to bazel version requirements; With `DV_CPP_TENSORFLOW_TAG=master`, I believe deepvariant will not build from source. TF master does not build with Bazel=0.15.0:. ```. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/... [bazel INFO src/main/cpp/option_processor.cc:235] Looking for master bazelrcs in the following three paths: /root/deepvariant/tools/bazel.rc, , /etc/bazel.bazelrc. [bazel INFO src/main/cpp/option_processor.cc:165] User provided no rc file. [bazel INFO src/main/cpp/rc_file.cc:53] Parsing the RcFile /root/deepvariant/tools/bazel.rc. [bazel INFO src/main/cpp/rc_file.cc:53] Parsing the RcFile /root/deepvariant/../tensorflow/tools/bazel.rc. [bazel FATAL src/main/cpp/blaze.cc:1263] Unexpected error reading .blazerc file '/root/deepvariant/../tensorflow/tools/bazel.rc'. ```. And deepvariant does not build with bazel=0.19.0, see #134. ___. Linux xxx 4.15.0-1031-aws #33-Ubuntu SMP Fri Dec 7 09:32:27 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux. ```. # Copyright 2017 Google LLC. #. # Redistribution and use in source and binary forms, with or without. # modification, are permitted provided that the following conditions. # are met:. #. # 1. Redistributions of source code must retain the above copyright notice,. # this list of conditions and the following disclaimer. #. # 2. Redistributions in binary form must reproduce the above copyright. # notice, this list of conditions and the following disclaimer in the. # documentation and/or other materials provided with the distribution. #. # 3. Neither the name of the copyright holder nor the names of its. # contributors may be used to endorse or promote products derived from this. # software without specific prior written permission. #. # THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"". # AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE. # IMPLIE",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:2094,integrability,EVENT,EVENT,2094,"opyright 2017 Google LLC. #. # Redistribution and use in source and binary forms, with or without. # modification, are permitted provided that the following conditions. # are met:. #. # 1. Redistributions of source code must retain the above copyright notice,. # this list of conditions and the following disclaimer. #. # 2. Redistributions in binary form must reproduce the above copyright. # notice, this list of conditions and the following disclaimer in the. # documentation and/or other materials provided with the distribution. #. # 3. Neither the name of the copyright holder nor the names of its. # contributors may be used to endorse or promote products derived from this. # software without specific prior written permission. #. # THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"". # AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE. # IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. # ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE. # LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR. # CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF. # SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS. # INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN. # CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE). # ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE. # POSSIBILITY OF SUCH DAMAGE. # Source this file---these options are needed for TF config and for. # successive bazel runs. # Set this to 1 if the system image already has TensorFlow preinstalled. This. # will skip the installation of TensorFlow. export DV_USE_PREINSTALLED_TF=""${DV_USE_PREINSTALLED_TF:-0}"". export TF_CUDA_CLANG=0. export TF_ENABLE_XLA=1. export TF_NEED_CUDA=1. export TF_NEED_GCP=1. export TF_NEED_GDR=0. export TF_NEED_HDFS=0. export TF_NEED_JEMALLOC=0. export TF_NEED_MKL=1. export ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:2292,integrability,SUB,SUBSTITUTE,2292,"ions of source code must retain the above copyright notice,. # this list of conditions and the following disclaimer. #. # 2. Redistributions in binary form must reproduce the above copyright. # notice, this list of conditions and the following disclaimer in the. # documentation and/or other materials provided with the distribution. #. # 3. Neither the name of the copyright holder nor the names of its. # contributors may be used to endorse or promote products derived from this. # software without specific prior written permission. #. # THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"". # AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE. # IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. # ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE. # LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR. # CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF. # SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS. # INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN. # CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE). # ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE. # POSSIBILITY OF SUCH DAMAGE. # Source this file---these options are needed for TF config and for. # successive bazel runs. # Set this to 1 if the system image already has TensorFlow preinstalled. This. # will skip the installation of TensorFlow. export DV_USE_PREINSTALLED_TF=""${DV_USE_PREINSTALLED_TF:-0}"". export TF_CUDA_CLANG=0. export TF_ENABLE_XLA=1. export TF_NEED_CUDA=1. export TF_NEED_GCP=1. export TF_NEED_GDR=0. export TF_NEED_HDFS=0. export TF_NEED_JEMALLOC=0. export TF_NEED_MKL=1. export TF_NEED_MPI=0. export TF_NEED_OPENCL=0. export TF_NEED_OPENCL_SYCL=0. export TF_NEED_S3=1. export TF_NEED_VERBS=0. # Used if TF_NEED_CUDA=1. export TF_CUDA_VERSION=""10.0"". export CUDA_TOOLKIT_PATH=""/u",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:2312,integrability,SERVIC,SERVICES,2312," must retain the above copyright notice,. # this list of conditions and the following disclaimer. #. # 2. Redistributions in binary form must reproduce the above copyright. # notice, this list of conditions and the following disclaimer in the. # documentation and/or other materials provided with the distribution. #. # 3. Neither the name of the copyright holder nor the names of its. # contributors may be used to endorse or promote products derived from this. # software without specific prior written permission. #. # THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"". # AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE. # IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. # ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE. # LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR. # CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF. # SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS. # INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN. # CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE). # ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE. # POSSIBILITY OF SUCH DAMAGE. # Source this file---these options are needed for TF config and for. # successive bazel runs. # Set this to 1 if the system image already has TensorFlow preinstalled. This. # will skip the installation of TensorFlow. export DV_USE_PREINSTALLED_TF=""${DV_USE_PREINSTALLED_TF:-0}"". export TF_CUDA_CLANG=0. export TF_ENABLE_XLA=1. export TF_NEED_CUDA=1. export TF_NEED_GCP=1. export TF_NEED_GDR=0. export TF_NEED_HDFS=0. export TF_NEED_JEMALLOC=0. export TF_NEED_MKL=1. export TF_NEED_MPI=0. export TF_NEED_OPENCL=0. export TF_NEED_OPENCL_SYCL=0. export TF_NEED_S3=1. export TF_NEED_VERBS=0. # Used if TF_NEED_CUDA=1. export TF_CUDA_VERSION=""10.0"". export CUDA_TOOLKIT_PATH=""/usr/local/cuda"". exp",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:2443,integrability,CONTRACT,CONTRACT,2443," form must reproduce the above copyright. # notice, this list of conditions and the following disclaimer in the. # documentation and/or other materials provided with the distribution. #. # 3. Neither the name of the copyright holder nor the names of its. # contributors may be used to endorse or promote products derived from this. # software without specific prior written permission. #. # THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"". # AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE. # IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. # ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE. # LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR. # CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF. # SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS. # INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN. # CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE). # ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE. # POSSIBILITY OF SUCH DAMAGE. # Source this file---these options are needed for TF config and for. # successive bazel runs. # Set this to 1 if the system image already has TensorFlow preinstalled. This. # will skip the installation of TensorFlow. export DV_USE_PREINSTALLED_TF=""${DV_USE_PREINSTALLED_TF:-0}"". export TF_CUDA_CLANG=0. export TF_ENABLE_XLA=1. export TF_NEED_CUDA=1. export TF_NEED_GCP=1. export TF_NEED_GDR=0. export TF_NEED_HDFS=0. export TF_NEED_JEMALLOC=0. export TF_NEED_MKL=1. export TF_NEED_MPI=0. export TF_NEED_OPENCL=0. export TF_NEED_OPENCL_SYCL=0. export TF_NEED_S3=1. export TF_NEED_VERBS=0. # Used if TF_NEED_CUDA=1. export TF_CUDA_VERSION=""10.0"". export CUDA_TOOLKIT_PATH=""/usr/local/cuda"". export TF_CUDNN_VERSION=""7"". export CUDNN_INSTALL_PATH=""/usr/lib/x86_64-linux-gnu"". # The version of bazel we want to build DeepVarian",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:3403,integrability,version,version,3403," THEORY OF LIABILITY, WHETHER IN. # CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE). # ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE. # POSSIBILITY OF SUCH DAMAGE. # Source this file---these options are needed for TF config and for. # successive bazel runs. # Set this to 1 if the system image already has TensorFlow preinstalled. This. # will skip the installation of TensorFlow. export DV_USE_PREINSTALLED_TF=""${DV_USE_PREINSTALLED_TF:-0}"". export TF_CUDA_CLANG=0. export TF_ENABLE_XLA=1. export TF_NEED_CUDA=1. export TF_NEED_GCP=1. export TF_NEED_GDR=0. export TF_NEED_HDFS=0. export TF_NEED_JEMALLOC=0. export TF_NEED_MKL=1. export TF_NEED_MPI=0. export TF_NEED_OPENCL=0. export TF_NEED_OPENCL_SYCL=0. export TF_NEED_S3=1. export TF_NEED_VERBS=0. # Used if TF_NEED_CUDA=1. export TF_CUDA_VERSION=""10.0"". export CUDA_TOOLKIT_PATH=""/usr/local/cuda"". export TF_CUDNN_VERSION=""7"". export CUDNN_INSTALL_PATH=""/usr/lib/x86_64-linux-gnu"". # The version of bazel we want to build DeepVariant. DV_BAZEL_VERSION=""0.15.0"". # We need to make sure that $HOME/bin is first in the binary search path so that. # `bazel` will find the latest version of bazel installed in the user's home. # directory. This is set in setting.sh as all DeepVariant scripts source. # settings.sh and assume that `bazel` will find the right version. export PATH=""$HOME/bin:$PATH"". # Path to the public bucket containing DeepVariant-related artifacts. export DEEPVARIANT_BUCKET=""gs://deepvariant"". export DV_PACKAGE_BUCKET_PATH=""${DEEPVARIANT_BUCKET}/packages"". export DV_PACKAGE_CURL_PATH=""https://storage.googleapis.com/deepvariant/packages"". # Set this to 1 to use the nightly (latest) build of TensorFlow instead of a. # named release version. Set it to an already existing value in the environment. # (allowing command line control of the build), defaulting to 0 (release build). # Note that setting this to 1 implies that the C++ code in DeepVariant will be. # build using t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:3590,integrability,version,version,3590,"# POSSIBILITY OF SUCH DAMAGE. # Source this file---these options are needed for TF config and for. # successive bazel runs. # Set this to 1 if the system image already has TensorFlow preinstalled. This. # will skip the installation of TensorFlow. export DV_USE_PREINSTALLED_TF=""${DV_USE_PREINSTALLED_TF:-0}"". export TF_CUDA_CLANG=0. export TF_ENABLE_XLA=1. export TF_NEED_CUDA=1. export TF_NEED_GCP=1. export TF_NEED_GDR=0. export TF_NEED_HDFS=0. export TF_NEED_JEMALLOC=0. export TF_NEED_MKL=1. export TF_NEED_MPI=0. export TF_NEED_OPENCL=0. export TF_NEED_OPENCL_SYCL=0. export TF_NEED_S3=1. export TF_NEED_VERBS=0. # Used if TF_NEED_CUDA=1. export TF_CUDA_VERSION=""10.0"". export CUDA_TOOLKIT_PATH=""/usr/local/cuda"". export TF_CUDNN_VERSION=""7"". export CUDNN_INSTALL_PATH=""/usr/lib/x86_64-linux-gnu"". # The version of bazel we want to build DeepVariant. DV_BAZEL_VERSION=""0.15.0"". # We need to make sure that $HOME/bin is first in the binary search path so that. # `bazel` will find the latest version of bazel installed in the user's home. # directory. This is set in setting.sh as all DeepVariant scripts source. # settings.sh and assume that `bazel` will find the right version. export PATH=""$HOME/bin:$PATH"". # Path to the public bucket containing DeepVariant-related artifacts. export DEEPVARIANT_BUCKET=""gs://deepvariant"". export DV_PACKAGE_BUCKET_PATH=""${DEEPVARIANT_BUCKET}/packages"". export DV_PACKAGE_CURL_PATH=""https://storage.googleapis.com/deepvariant/packages"". # Set this to 1 to use the nightly (latest) build of TensorFlow instead of a. # named release version. Set it to an already existing value in the environment. # (allowing command line control of the build), defaulting to 0 (release build). # Note that setting this to 1 implies that the C++ code in DeepVariant will be. # build using the master branch and not the pinned version to avoid. # incompatibilities between TensorFlow C++ used to build DeepVariant and the. # tf-nightly wheel. export DV_TF_NIGHTLY_BUILD=""${DV_TF",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:3769,integrability,version,version,3769,"low preinstalled. This. # will skip the installation of TensorFlow. export DV_USE_PREINSTALLED_TF=""${DV_USE_PREINSTALLED_TF:-0}"". export TF_CUDA_CLANG=0. export TF_ENABLE_XLA=1. export TF_NEED_CUDA=1. export TF_NEED_GCP=1. export TF_NEED_GDR=0. export TF_NEED_HDFS=0. export TF_NEED_JEMALLOC=0. export TF_NEED_MKL=1. export TF_NEED_MPI=0. export TF_NEED_OPENCL=0. export TF_NEED_OPENCL_SYCL=0. export TF_NEED_S3=1. export TF_NEED_VERBS=0. # Used if TF_NEED_CUDA=1. export TF_CUDA_VERSION=""10.0"". export CUDA_TOOLKIT_PATH=""/usr/local/cuda"". export TF_CUDNN_VERSION=""7"". export CUDNN_INSTALL_PATH=""/usr/lib/x86_64-linux-gnu"". # The version of bazel we want to build DeepVariant. DV_BAZEL_VERSION=""0.15.0"". # We need to make sure that $HOME/bin is first in the binary search path so that. # `bazel` will find the latest version of bazel installed in the user's home. # directory. This is set in setting.sh as all DeepVariant scripts source. # settings.sh and assume that `bazel` will find the right version. export PATH=""$HOME/bin:$PATH"". # Path to the public bucket containing DeepVariant-related artifacts. export DEEPVARIANT_BUCKET=""gs://deepvariant"". export DV_PACKAGE_BUCKET_PATH=""${DEEPVARIANT_BUCKET}/packages"". export DV_PACKAGE_CURL_PATH=""https://storage.googleapis.com/deepvariant/packages"". # Set this to 1 to use the nightly (latest) build of TensorFlow instead of a. # named release version. Set it to an already existing value in the environment. # (allowing command line control of the build), defaulting to 0 (release build). # Note that setting this to 1 implies that the C++ code in DeepVariant will be. # build using the master branch and not the pinned version to avoid. # incompatibilities between TensorFlow C++ used to build DeepVariant and the. # tf-nightly wheel. export DV_TF_NIGHTLY_BUILD=""${DV_TF_NIGHTLY_BUILD:-1}"". # The branch/tag we checkout to build our C++ dependencies against. This is not. # the same as the python version of TensorFlow we use, but should be similar ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:3823,integrability,pub,public,3823,"of TensorFlow. export DV_USE_PREINSTALLED_TF=""${DV_USE_PREINSTALLED_TF:-0}"". export TF_CUDA_CLANG=0. export TF_ENABLE_XLA=1. export TF_NEED_CUDA=1. export TF_NEED_GCP=1. export TF_NEED_GDR=0. export TF_NEED_HDFS=0. export TF_NEED_JEMALLOC=0. export TF_NEED_MKL=1. export TF_NEED_MPI=0. export TF_NEED_OPENCL=0. export TF_NEED_OPENCL_SYCL=0. export TF_NEED_S3=1. export TF_NEED_VERBS=0. # Used if TF_NEED_CUDA=1. export TF_CUDA_VERSION=""10.0"". export CUDA_TOOLKIT_PATH=""/usr/local/cuda"". export TF_CUDNN_VERSION=""7"". export CUDNN_INSTALL_PATH=""/usr/lib/x86_64-linux-gnu"". # The version of bazel we want to build DeepVariant. DV_BAZEL_VERSION=""0.15.0"". # We need to make sure that $HOME/bin is first in the binary search path so that. # `bazel` will find the latest version of bazel installed in the user's home. # directory. This is set in setting.sh as all DeepVariant scripts source. # settings.sh and assume that `bazel` will find the right version. export PATH=""$HOME/bin:$PATH"". # Path to the public bucket containing DeepVariant-related artifacts. export DEEPVARIANT_BUCKET=""gs://deepvariant"". export DV_PACKAGE_BUCKET_PATH=""${DEEPVARIANT_BUCKET}/packages"". export DV_PACKAGE_CURL_PATH=""https://storage.googleapis.com/deepvariant/packages"". # Set this to 1 to use the nightly (latest) build of TensorFlow instead of a. # named release version. Set it to an already existing value in the environment. # (allowing command line control of the build), defaulting to 0 (release build). # Note that setting this to 1 implies that the C++ code in DeepVariant will be. # build using the master branch and not the pinned version to avoid. # incompatibilities between TensorFlow C++ used to build DeepVariant and the. # tf-nightly wheel. export DV_TF_NIGHTLY_BUILD=""${DV_TF_NIGHTLY_BUILD:-1}"". # The branch/tag we checkout to build our C++ dependencies against. This is not. # the same as the python version of TensorFlow we use, but should be similar or. # we risk having version incompatibilities betwee",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:4166,integrability,version,version,4166,"ort TF_NEED_S3=1. export TF_NEED_VERBS=0. # Used if TF_NEED_CUDA=1. export TF_CUDA_VERSION=""10.0"". export CUDA_TOOLKIT_PATH=""/usr/local/cuda"". export TF_CUDNN_VERSION=""7"". export CUDNN_INSTALL_PATH=""/usr/lib/x86_64-linux-gnu"". # The version of bazel we want to build DeepVariant. DV_BAZEL_VERSION=""0.15.0"". # We need to make sure that $HOME/bin is first in the binary search path so that. # `bazel` will find the latest version of bazel installed in the user's home. # directory. This is set in setting.sh as all DeepVariant scripts source. # settings.sh and assume that `bazel` will find the right version. export PATH=""$HOME/bin:$PATH"". # Path to the public bucket containing DeepVariant-related artifacts. export DEEPVARIANT_BUCKET=""gs://deepvariant"". export DV_PACKAGE_BUCKET_PATH=""${DEEPVARIANT_BUCKET}/packages"". export DV_PACKAGE_CURL_PATH=""https://storage.googleapis.com/deepvariant/packages"". # Set this to 1 to use the nightly (latest) build of TensorFlow instead of a. # named release version. Set it to an already existing value in the environment. # (allowing command line control of the build), defaulting to 0 (release build). # Note that setting this to 1 implies that the C++ code in DeepVariant will be. # build using the master branch and not the pinned version to avoid. # incompatibilities between TensorFlow C++ used to build DeepVariant and the. # tf-nightly wheel. export DV_TF_NIGHTLY_BUILD=""${DV_TF_NIGHTLY_BUILD:-1}"". # The branch/tag we checkout to build our C++ dependencies against. This is not. # the same as the python version of TensorFlow we use, but should be similar or. # we risk having version incompatibilities between our C++ code and the Python. # code we use at runtime. if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then. export DV_CPP_TENSORFLOW_TAG=""master"". else. export DV_CPP_TENSORFLOW_TAG=""r1.12"". fi. export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:4443,integrability,version,version,4443,"t. DV_BAZEL_VERSION=""0.15.0"". # We need to make sure that $HOME/bin is first in the binary search path so that. # `bazel` will find the latest version of bazel installed in the user's home. # directory. This is set in setting.sh as all DeepVariant scripts source. # settings.sh and assume that `bazel` will find the right version. export PATH=""$HOME/bin:$PATH"". # Path to the public bucket containing DeepVariant-related artifacts. export DEEPVARIANT_BUCKET=""gs://deepvariant"". export DV_PACKAGE_BUCKET_PATH=""${DEEPVARIANT_BUCKET}/packages"". export DV_PACKAGE_CURL_PATH=""https://storage.googleapis.com/deepvariant/packages"". # Set this to 1 to use the nightly (latest) build of TensorFlow instead of a. # named release version. Set it to an already existing value in the environment. # (allowing command line control of the build), defaulting to 0 (release build). # Note that setting this to 1 implies that the C++ code in DeepVariant will be. # build using the master branch and not the pinned version to avoid. # incompatibilities between TensorFlow C++ used to build DeepVariant and the. # tf-nightly wheel. export DV_TF_NIGHTLY_BUILD=""${DV_TF_NIGHTLY_BUILD:-1}"". # The branch/tag we checkout to build our C++ dependencies against. This is not. # the same as the python version of TensorFlow we use, but should be similar or. # we risk having version incompatibilities between our C++ code and the Python. # code we use at runtime. if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then. export DV_CPP_TENSORFLOW_TAG=""master"". else. export DV_CPP_TENSORFLOW_TAG=""r1.12"". fi. export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing. # value in the environment (allowing command line control of the build),. # defaulting to 0 (CPU only build). export DV_GPU_BUILD=""${DV_GPU_BUILD:-1}"". # If this variable is set to 1, DeepVari",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:4661,integrability,depend,dependencies,4661,"tting.sh as all DeepVariant scripts source. # settings.sh and assume that `bazel` will find the right version. export PATH=""$HOME/bin:$PATH"". # Path to the public bucket containing DeepVariant-related artifacts. export DEEPVARIANT_BUCKET=""gs://deepvariant"". export DV_PACKAGE_BUCKET_PATH=""${DEEPVARIANT_BUCKET}/packages"". export DV_PACKAGE_CURL_PATH=""https://storage.googleapis.com/deepvariant/packages"". # Set this to 1 to use the nightly (latest) build of TensorFlow instead of a. # named release version. Set it to an already existing value in the environment. # (allowing command line control of the build), defaulting to 0 (release build). # Note that setting this to 1 implies that the C++ code in DeepVariant will be. # build using the master branch and not the pinned version to avoid. # incompatibilities between TensorFlow C++ used to build DeepVariant and the. # tf-nightly wheel. export DV_TF_NIGHTLY_BUILD=""${DV_TF_NIGHTLY_BUILD:-1}"". # The branch/tag we checkout to build our C++ dependencies against. This is not. # the same as the python version of TensorFlow we use, but should be similar or. # we risk having version incompatibilities between our C++ code and the Python. # code we use at runtime. if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then. export DV_CPP_TENSORFLOW_TAG=""master"". else. export DV_CPP_TENSORFLOW_TAG=""r1.12"". fi. export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing. # value in the environment (allowing command line control of the build),. # defaulting to 0 (CPU only build). export DV_GPU_BUILD=""${DV_GPU_BUILD:-1}"". # If this variable is set to 1, DeepVariant will use a TensorFlow wheel file. # compiled with MKL support for corei7 or better chipsets, which. # significantly speeds up execution when running on modern CPUs. The default. # TensorFlow wheel files don't contain",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:4721,integrability,version,version,4721,"and assume that `bazel` will find the right version. export PATH=""$HOME/bin:$PATH"". # Path to the public bucket containing DeepVariant-related artifacts. export DEEPVARIANT_BUCKET=""gs://deepvariant"". export DV_PACKAGE_BUCKET_PATH=""${DEEPVARIANT_BUCKET}/packages"". export DV_PACKAGE_CURL_PATH=""https://storage.googleapis.com/deepvariant/packages"". # Set this to 1 to use the nightly (latest) build of TensorFlow instead of a. # named release version. Set it to an already existing value in the environment. # (allowing command line control of the build), defaulting to 0 (release build). # Note that setting this to 1 implies that the C++ code in DeepVariant will be. # build using the master branch and not the pinned version to avoid. # incompatibilities between TensorFlow C++ used to build DeepVariant and the. # tf-nightly wheel. export DV_TF_NIGHTLY_BUILD=""${DV_TF_NIGHTLY_BUILD:-1}"". # The branch/tag we checkout to build our C++ dependencies against. This is not. # the same as the python version of TensorFlow we use, but should be similar or. # we risk having version incompatibilities between our C++ code and the Python. # code we use at runtime. if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then. export DV_CPP_TENSORFLOW_TAG=""master"". else. export DV_CPP_TENSORFLOW_TAG=""r1.12"". fi. export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing. # value in the environment (allowing command line control of the build),. # defaulting to 0 (CPU only build). export DV_GPU_BUILD=""${DV_GPU_BUILD:-1}"". # If this variable is set to 1, DeepVariant will use a TensorFlow wheel file. # compiled with MKL support for corei7 or better chipsets, which. # significantly speeds up execution when running on modern CPUs. The default. # TensorFlow wheel files don't contain these instructions (and thereby run on a. # broader set o",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:4794,integrability,version,version,4794,"in:$PATH"". # Path to the public bucket containing DeepVariant-related artifacts. export DEEPVARIANT_BUCKET=""gs://deepvariant"". export DV_PACKAGE_BUCKET_PATH=""${DEEPVARIANT_BUCKET}/packages"". export DV_PACKAGE_CURL_PATH=""https://storage.googleapis.com/deepvariant/packages"". # Set this to 1 to use the nightly (latest) build of TensorFlow instead of a. # named release version. Set it to an already existing value in the environment. # (allowing command line control of the build), defaulting to 0 (release build). # Note that setting this to 1 implies that the C++ code in DeepVariant will be. # build using the master branch and not the pinned version to avoid. # incompatibilities between TensorFlow C++ used to build DeepVariant and the. # tf-nightly wheel. export DV_TF_NIGHTLY_BUILD=""${DV_TF_NIGHTLY_BUILD:-1}"". # The branch/tag we checkout to build our C++ dependencies against. This is not. # the same as the python version of TensorFlow we use, but should be similar or. # we risk having version incompatibilities between our C++ code and the Python. # code we use at runtime. if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then. export DV_CPP_TENSORFLOW_TAG=""master"". else. export DV_CPP_TENSORFLOW_TAG=""r1.12"". fi. export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing. # value in the environment (allowing command line control of the build),. # defaulting to 0 (CPU only build). export DV_GPU_BUILD=""${DV_GPU_BUILD:-1}"". # If this variable is set to 1, DeepVariant will use a TensorFlow wheel file. # compiled with MKL support for corei7 or better chipsets, which. # significantly speeds up execution when running on modern CPUs. The default. # TensorFlow wheel files don't contain these instructions (and thereby run on a. # broader set of CPUs). Using this optimized wheel reduces the runtime of. # DeepVariant",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:5940,integrability,Bridg,Bridge,5940,"t runtime. if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then. export DV_CPP_TENSORFLOW_TAG=""master"". else. export DV_CPP_TENSORFLOW_TAG=""r1.12"". fi. export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing. # value in the environment (allowing command line control of the build),. # defaulting to 0 (CPU only build). export DV_GPU_BUILD=""${DV_GPU_BUILD:-1}"". # If this variable is set to 1, DeepVariant will use a TensorFlow wheel file. # compiled with MKL support for corei7 or better chipsets, which. # significantly speeds up execution when running on modern CPUs. The default. # TensorFlow wheel files don't contain these instructions (and thereby run on a. # broader set of CPUs). Using this optimized wheel reduces the runtime of. # DeepVariant's call_variants step by >3x. This is called the GCP (Google Cloud. # Platform) optimized wheel because all GCP instances have at least Sandy Bridge. # or better chipsets, so this wheel should run anywhere on GCP. export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}"". export GCP_OPTIMIZED_TF_WHL_FILENAME=""tensorflow-${DV_GCP_OPTIMIZED_TF_WHL_VERSION}.deepvariant_gcp-cp27-none-linux_x86_64.whl"". export GCP_OPTIMIZED_TF_WHL_PATH=""${DV_PACKAGE_BUCKET_PATH}/tensorflow"". export GCP_OPTIMIZED_TF_WHL_CURL_PATH=""${DV_PACKAGE_CURL_PATH}/tensorflow"". # Set this to 1 to make our prereq scripts install the CUDA libraries. # If you already have CUDA installed, such as on a properly provisioned. # Docker image, it shouldn't be necessary. export DV_INSTALL_GPU_DRIVERS=""${DV_INSTALL_GPU_DRIVERS:-0}"". export PYTHON_BIN_PATH=$(which python). export USE_DEFAULT_PYTHON_LIB_PATH=1. export DV_COPT_FLAGS=""--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings"". function note_build_stage {. echo ""========== [$(date)] Stage '${1}' starting"". }. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:1617,interoperability,distribut,distribution,1617,":53] Parsing the RcFile /root/deepvariant/tools/bazel.rc. [bazel INFO src/main/cpp/rc_file.cc:53] Parsing the RcFile /root/deepvariant/../tensorflow/tools/bazel.rc. [bazel FATAL src/main/cpp/blaze.cc:1263] Unexpected error reading .blazerc file '/root/deepvariant/../tensorflow/tools/bazel.rc'. ```. And deepvariant does not build with bazel=0.19.0, see #134. ___. Linux xxx 4.15.0-1031-aws #33-Ubuntu SMP Fri Dec 7 09:32:27 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux. ```. # Copyright 2017 Google LLC. #. # Redistribution and use in source and binary forms, with or without. # modification, are permitted provided that the following conditions. # are met:. #. # 1. Redistributions of source code must retain the above copyright notice,. # this list of conditions and the following disclaimer. #. # 2. Redistributions in binary form must reproduce the above copyright. # notice, this list of conditions and the following disclaimer in the. # documentation and/or other materials provided with the distribution. #. # 3. Neither the name of the copyright holder nor the names of its. # contributors may be used to endorse or promote products derived from this. # software without specific prior written permission. #. # THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"". # AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE. # IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. # ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE. # LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR. # CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF. # SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS. # INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN. # CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE). # ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE. # POSSIBILITY OF SUCH DAMAGE.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:1798,interoperability,specif,specific,1798,"rc/main/cpp/blaze.cc:1263] Unexpected error reading .blazerc file '/root/deepvariant/../tensorflow/tools/bazel.rc'. ```. And deepvariant does not build with bazel=0.19.0, see #134. ___. Linux xxx 4.15.0-1031-aws #33-Ubuntu SMP Fri Dec 7 09:32:27 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux. ```. # Copyright 2017 Google LLC. #. # Redistribution and use in source and binary forms, with or without. # modification, are permitted provided that the following conditions. # are met:. #. # 1. Redistributions of source code must retain the above copyright notice,. # this list of conditions and the following disclaimer. #. # 2. Redistributions in binary form must reproduce the above copyright. # notice, this list of conditions and the following disclaimer in the. # documentation and/or other materials provided with the distribution. #. # 3. Neither the name of the copyright holder nor the names of its. # contributors may be used to endorse or promote products derived from this. # software without specific prior written permission. #. # THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"". # AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE. # IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. # ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE. # LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR. # CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF. # SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS. # INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN. # CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE). # ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE. # POSSIBILITY OF SUCH DAMAGE. # Source this file---these options are needed for TF config and for. # successive bazel runs. # Set this to 1 if the system image already has TensorFlow preinstalled. This. # wil",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:2443,interoperability,CONTRACT,CONTRACT,2443," form must reproduce the above copyright. # notice, this list of conditions and the following disclaimer in the. # documentation and/or other materials provided with the distribution. #. # 3. Neither the name of the copyright holder nor the names of its. # contributors may be used to endorse or promote products derived from this. # software without specific prior written permission. #. # THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"". # AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE. # IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. # ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE. # LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR. # CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF. # SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS. # INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN. # CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE). # ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE. # POSSIBILITY OF SUCH DAMAGE. # Source this file---these options are needed for TF config and for. # successive bazel runs. # Set this to 1 if the system image already has TensorFlow preinstalled. This. # will skip the installation of TensorFlow. export DV_USE_PREINSTALLED_TF=""${DV_USE_PREINSTALLED_TF:-0}"". export TF_CUDA_CLANG=0. export TF_ENABLE_XLA=1. export TF_NEED_CUDA=1. export TF_NEED_GCP=1. export TF_NEED_GDR=0. export TF_NEED_HDFS=0. export TF_NEED_JEMALLOC=0. export TF_NEED_MKL=1. export TF_NEED_MPI=0. export TF_NEED_OPENCL=0. export TF_NEED_OPENCL_SYCL=0. export TF_NEED_S3=1. export TF_NEED_VERBS=0. # Used if TF_NEED_CUDA=1. export TF_CUDA_VERSION=""10.0"". export CUDA_TOOLKIT_PATH=""/usr/local/cuda"". export TF_CUDNN_VERSION=""7"". export CUDNN_INSTALL_PATH=""/usr/lib/x86_64-linux-gnu"". # The version of bazel we want to build DeepVarian",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:4463,interoperability,incompatib,incompatibilities,4463,".0"". # We need to make sure that $HOME/bin is first in the binary search path so that. # `bazel` will find the latest version of bazel installed in the user's home. # directory. This is set in setting.sh as all DeepVariant scripts source. # settings.sh and assume that `bazel` will find the right version. export PATH=""$HOME/bin:$PATH"". # Path to the public bucket containing DeepVariant-related artifacts. export DEEPVARIANT_BUCKET=""gs://deepvariant"". export DV_PACKAGE_BUCKET_PATH=""${DEEPVARIANT_BUCKET}/packages"". export DV_PACKAGE_CURL_PATH=""https://storage.googleapis.com/deepvariant/packages"". # Set this to 1 to use the nightly (latest) build of TensorFlow instead of a. # named release version. Set it to an already existing value in the environment. # (allowing command line control of the build), defaulting to 0 (release build). # Note that setting this to 1 implies that the C++ code in DeepVariant will be. # build using the master branch and not the pinned version to avoid. # incompatibilities between TensorFlow C++ used to build DeepVariant and the. # tf-nightly wheel. export DV_TF_NIGHTLY_BUILD=""${DV_TF_NIGHTLY_BUILD:-1}"". # The branch/tag we checkout to build our C++ dependencies against. This is not. # the same as the python version of TensorFlow we use, but should be similar or. # we risk having version incompatibilities between our C++ code and the Python. # code we use at runtime. if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then. export DV_CPP_TENSORFLOW_TAG=""master"". else. export DV_CPP_TENSORFLOW_TAG=""r1.12"". fi. export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing. # value in the environment (allowing command line control of the build),. # defaulting to 0 (CPU only build). export DV_GPU_BUILD=""${DV_GPU_BUILD:-1}"". # If this variable is set to 1, DeepVariant will use a TensorFlow",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:4802,interoperability,incompatib,incompatibilities,4802,"Path to the public bucket containing DeepVariant-related artifacts. export DEEPVARIANT_BUCKET=""gs://deepvariant"". export DV_PACKAGE_BUCKET_PATH=""${DEEPVARIANT_BUCKET}/packages"". export DV_PACKAGE_CURL_PATH=""https://storage.googleapis.com/deepvariant/packages"". # Set this to 1 to use the nightly (latest) build of TensorFlow instead of a. # named release version. Set it to an already existing value in the environment. # (allowing command line control of the build), defaulting to 0 (release build). # Note that setting this to 1 implies that the C++ code in DeepVariant will be. # build using the master branch and not the pinned version to avoid. # incompatibilities between TensorFlow C++ used to build DeepVariant and the. # tf-nightly wheel. export DV_TF_NIGHTLY_BUILD=""${DV_TF_NIGHTLY_BUILD:-1}"". # The branch/tag we checkout to build our C++ dependencies against. This is not. # the same as the python version of TensorFlow we use, but should be similar or. # we risk having version incompatibilities between our C++ code and the Python. # code we use at runtime. if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then. export DV_CPP_TENSORFLOW_TAG=""master"". else. export DV_CPP_TENSORFLOW_TAG=""r1.12"". fi. export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing. # value in the environment (allowing command line control of the build),. # defaulting to 0 (CPU only build). export DV_GPU_BUILD=""${DV_GPU_BUILD:-1}"". # If this variable is set to 1, DeepVariant will use a TensorFlow wheel file. # compiled with MKL support for corei7 or better chipsets, which. # significantly speeds up execution when running on modern CPUs. The default. # TensorFlow wheel files don't contain these instructions (and thereby run on a. # broader set of CPUs). Using this optimized wheel reduces the runtime of. # DeepVariant's call_varia",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:5868,interoperability,Platform,Platform,5868,"t runtime. if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then. export DV_CPP_TENSORFLOW_TAG=""master"". else. export DV_CPP_TENSORFLOW_TAG=""r1.12"". fi. export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing. # value in the environment (allowing command line control of the build),. # defaulting to 0 (CPU only build). export DV_GPU_BUILD=""${DV_GPU_BUILD:-1}"". # If this variable is set to 1, DeepVariant will use a TensorFlow wheel file. # compiled with MKL support for corei7 or better chipsets, which. # significantly speeds up execution when running on modern CPUs. The default. # TensorFlow wheel files don't contain these instructions (and thereby run on a. # broader set of CPUs). Using this optimized wheel reduces the runtime of. # DeepVariant's call_variants step by >3x. This is called the GCP (Google Cloud. # Platform) optimized wheel because all GCP instances have at least Sandy Bridge. # or better chipsets, so this wheel should run anywhere on GCP. export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}"". export GCP_OPTIMIZED_TF_WHL_FILENAME=""tensorflow-${DV_GCP_OPTIMIZED_TF_WHL_VERSION}.deepvariant_gcp-cp27-none-linux_x86_64.whl"". export GCP_OPTIMIZED_TF_WHL_PATH=""${DV_PACKAGE_BUCKET_PATH}/tensorflow"". export GCP_OPTIMIZED_TF_WHL_CURL_PATH=""${DV_PACKAGE_CURL_PATH}/tensorflow"". # Set this to 1 to make our prereq scripts install the CUDA libraries. # If you already have CUDA installed, such as on a properly provisioned. # Docker image, it shouldn't be necessary. export DV_INSTALL_GPU_DRIVERS=""${DV_INSTALL_GPU_DRIVERS:-0}"". export PYTHON_BIN_PATH=$(which python). export USE_DEFAULT_PYTHON_LIB_PATH=1. export DV_COPT_FLAGS=""--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings"". function note_build_stage {. echo ""========== [$(date)] Stage '${1}' starting"". }. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:5940,interoperability,Bridg,Bridge,5940,"t runtime. if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then. export DV_CPP_TENSORFLOW_TAG=""master"". else. export DV_CPP_TENSORFLOW_TAG=""r1.12"". fi. export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing. # value in the environment (allowing command line control of the build),. # defaulting to 0 (CPU only build). export DV_GPU_BUILD=""${DV_GPU_BUILD:-1}"". # If this variable is set to 1, DeepVariant will use a TensorFlow wheel file. # compiled with MKL support for corei7 or better chipsets, which. # significantly speeds up execution when running on modern CPUs. The default. # TensorFlow wheel files don't contain these instructions (and thereby run on a. # broader set of CPUs). Using this optimized wheel reduces the runtime of. # DeepVariant's call_variants step by >3x. This is called the GCP (Google Cloud. # Platform) optimized wheel because all GCP instances have at least Sandy Bridge. # or better chipsets, so this wheel should run anywhere on GCP. export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}"". export GCP_OPTIMIZED_TF_WHL_FILENAME=""tensorflow-${DV_GCP_OPTIMIZED_TF_WHL_VERSION}.deepvariant_gcp-cp27-none-linux_x86_64.whl"". export GCP_OPTIMIZED_TF_WHL_PATH=""${DV_PACKAGE_BUCKET_PATH}/tensorflow"". export GCP_OPTIMIZED_TF_WHL_CURL_PATH=""${DV_PACKAGE_CURL_PATH}/tensorflow"". # Set this to 1 to make our prereq scripts install the CUDA libraries. # If you already have CUDA installed, such as on a properly provisioned. # Docker image, it shouldn't be necessary. export DV_INSTALL_GPU_DRIVERS=""${DV_INSTALL_GPU_DRIVERS:-0}"". export PYTHON_BIN_PATH=$(which python). export USE_DEFAULT_PYTHON_LIB_PATH=1. export DV_COPT_FLAGS=""--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings"". function note_build_stage {. echo ""========== [$(date)] Stage '${1}' starting"". }. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:61,modifiability,version,version,61,"Cannot build deepvariant with tensorflow master due to bazel version requirements; With `DV_CPP_TENSORFLOW_TAG=master`, I believe deepvariant will not build from source. TF master does not build with Bazel=0.15.0:. ```. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/... [bazel INFO src/main/cpp/option_processor.cc:235] Looking for master bazelrcs in the following three paths: /root/deepvariant/tools/bazel.rc, , /etc/bazel.bazelrc. [bazel INFO src/main/cpp/option_processor.cc:165] User provided no rc file. [bazel INFO src/main/cpp/rc_file.cc:53] Parsing the RcFile /root/deepvariant/tools/bazel.rc. [bazel INFO src/main/cpp/rc_file.cc:53] Parsing the RcFile /root/deepvariant/../tensorflow/tools/bazel.rc. [bazel FATAL src/main/cpp/blaze.cc:1263] Unexpected error reading .blazerc file '/root/deepvariant/../tensorflow/tools/bazel.rc'. ```. And deepvariant does not build with bazel=0.19.0, see #134. ___. Linux xxx 4.15.0-1031-aws #33-Ubuntu SMP Fri Dec 7 09:32:27 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux. ```. # Copyright 2017 Google LLC. #. # Redistribution and use in source and binary forms, with or without. # modification, are permitted provided that the following conditions. # are met:. #. # 1. Redistributions of source code must retain the above copyright notice,. # this list of conditions and the following disclaimer. #. # 2. Redistributions in binary form must reproduce the above copyright. # notice, this list of conditions and the following disclaimer in the. # documentation and/or other materials provided with the distribution. #. # 3. Neither the name of the copyright holder nor the names of its. # contributors may be used to endorse or promote products derived from this. # software without specific prior written permission. #. # THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"". # AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE. # IMPLIE",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:2312,modifiability,SERVIC,SERVICES,2312," must retain the above copyright notice,. # this list of conditions and the following disclaimer. #. # 2. Redistributions in binary form must reproduce the above copyright. # notice, this list of conditions and the following disclaimer in the. # documentation and/or other materials provided with the distribution. #. # 3. Neither the name of the copyright holder nor the names of its. # contributors may be used to endorse or promote products derived from this. # software without specific prior written permission. #. # THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"". # AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE. # IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. # ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE. # LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR. # CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF. # SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS. # INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN. # CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE). # ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE. # POSSIBILITY OF SUCH DAMAGE. # Source this file---these options are needed for TF config and for. # successive bazel runs. # Set this to 1 if the system image already has TensorFlow preinstalled. This. # will skip the installation of TensorFlow. export DV_USE_PREINSTALLED_TF=""${DV_USE_PREINSTALLED_TF:-0}"". export TF_CUDA_CLANG=0. export TF_ENABLE_XLA=1. export TF_NEED_CUDA=1. export TF_NEED_GCP=1. export TF_NEED_GDR=0. export TF_NEED_HDFS=0. export TF_NEED_JEMALLOC=0. export TF_NEED_MKL=1. export TF_NEED_MPI=0. export TF_NEED_OPENCL=0. export TF_NEED_OPENCL_SYCL=0. export TF_NEED_S3=1. export TF_NEED_VERBS=0. # Used if TF_NEED_CUDA=1. export TF_CUDA_VERSION=""10.0"". export CUDA_TOOLKIT_PATH=""/usr/local/cuda"". exp",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:3403,modifiability,version,version,3403," THEORY OF LIABILITY, WHETHER IN. # CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE). # ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE. # POSSIBILITY OF SUCH DAMAGE. # Source this file---these options are needed for TF config and for. # successive bazel runs. # Set this to 1 if the system image already has TensorFlow preinstalled. This. # will skip the installation of TensorFlow. export DV_USE_PREINSTALLED_TF=""${DV_USE_PREINSTALLED_TF:-0}"". export TF_CUDA_CLANG=0. export TF_ENABLE_XLA=1. export TF_NEED_CUDA=1. export TF_NEED_GCP=1. export TF_NEED_GDR=0. export TF_NEED_HDFS=0. export TF_NEED_JEMALLOC=0. export TF_NEED_MKL=1. export TF_NEED_MPI=0. export TF_NEED_OPENCL=0. export TF_NEED_OPENCL_SYCL=0. export TF_NEED_S3=1. export TF_NEED_VERBS=0. # Used if TF_NEED_CUDA=1. export TF_CUDA_VERSION=""10.0"". export CUDA_TOOLKIT_PATH=""/usr/local/cuda"". export TF_CUDNN_VERSION=""7"". export CUDNN_INSTALL_PATH=""/usr/lib/x86_64-linux-gnu"". # The version of bazel we want to build DeepVariant. DV_BAZEL_VERSION=""0.15.0"". # We need to make sure that $HOME/bin is first in the binary search path so that. # `bazel` will find the latest version of bazel installed in the user's home. # directory. This is set in setting.sh as all DeepVariant scripts source. # settings.sh and assume that `bazel` will find the right version. export PATH=""$HOME/bin:$PATH"". # Path to the public bucket containing DeepVariant-related artifacts. export DEEPVARIANT_BUCKET=""gs://deepvariant"". export DV_PACKAGE_BUCKET_PATH=""${DEEPVARIANT_BUCKET}/packages"". export DV_PACKAGE_CURL_PATH=""https://storage.googleapis.com/deepvariant/packages"". # Set this to 1 to use the nightly (latest) build of TensorFlow instead of a. # named release version. Set it to an already existing value in the environment. # (allowing command line control of the build), defaulting to 0 (release build). # Note that setting this to 1 implies that the C++ code in DeepVariant will be. # build using t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:3590,modifiability,version,version,3590,"# POSSIBILITY OF SUCH DAMAGE. # Source this file---these options are needed for TF config and for. # successive bazel runs. # Set this to 1 if the system image already has TensorFlow preinstalled. This. # will skip the installation of TensorFlow. export DV_USE_PREINSTALLED_TF=""${DV_USE_PREINSTALLED_TF:-0}"". export TF_CUDA_CLANG=0. export TF_ENABLE_XLA=1. export TF_NEED_CUDA=1. export TF_NEED_GCP=1. export TF_NEED_GDR=0. export TF_NEED_HDFS=0. export TF_NEED_JEMALLOC=0. export TF_NEED_MKL=1. export TF_NEED_MPI=0. export TF_NEED_OPENCL=0. export TF_NEED_OPENCL_SYCL=0. export TF_NEED_S3=1. export TF_NEED_VERBS=0. # Used if TF_NEED_CUDA=1. export TF_CUDA_VERSION=""10.0"". export CUDA_TOOLKIT_PATH=""/usr/local/cuda"". export TF_CUDNN_VERSION=""7"". export CUDNN_INSTALL_PATH=""/usr/lib/x86_64-linux-gnu"". # The version of bazel we want to build DeepVariant. DV_BAZEL_VERSION=""0.15.0"". # We need to make sure that $HOME/bin is first in the binary search path so that. # `bazel` will find the latest version of bazel installed in the user's home. # directory. This is set in setting.sh as all DeepVariant scripts source. # settings.sh and assume that `bazel` will find the right version. export PATH=""$HOME/bin:$PATH"". # Path to the public bucket containing DeepVariant-related artifacts. export DEEPVARIANT_BUCKET=""gs://deepvariant"". export DV_PACKAGE_BUCKET_PATH=""${DEEPVARIANT_BUCKET}/packages"". export DV_PACKAGE_CURL_PATH=""https://storage.googleapis.com/deepvariant/packages"". # Set this to 1 to use the nightly (latest) build of TensorFlow instead of a. # named release version. Set it to an already existing value in the environment. # (allowing command line control of the build), defaulting to 0 (release build). # Note that setting this to 1 implies that the C++ code in DeepVariant will be. # build using the master branch and not the pinned version to avoid. # incompatibilities between TensorFlow C++ used to build DeepVariant and the. # tf-nightly wheel. export DV_TF_NIGHTLY_BUILD=""${DV_TF",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:3769,modifiability,version,version,3769,"low preinstalled. This. # will skip the installation of TensorFlow. export DV_USE_PREINSTALLED_TF=""${DV_USE_PREINSTALLED_TF:-0}"". export TF_CUDA_CLANG=0. export TF_ENABLE_XLA=1. export TF_NEED_CUDA=1. export TF_NEED_GCP=1. export TF_NEED_GDR=0. export TF_NEED_HDFS=0. export TF_NEED_JEMALLOC=0. export TF_NEED_MKL=1. export TF_NEED_MPI=0. export TF_NEED_OPENCL=0. export TF_NEED_OPENCL_SYCL=0. export TF_NEED_S3=1. export TF_NEED_VERBS=0. # Used if TF_NEED_CUDA=1. export TF_CUDA_VERSION=""10.0"". export CUDA_TOOLKIT_PATH=""/usr/local/cuda"". export TF_CUDNN_VERSION=""7"". export CUDNN_INSTALL_PATH=""/usr/lib/x86_64-linux-gnu"". # The version of bazel we want to build DeepVariant. DV_BAZEL_VERSION=""0.15.0"". # We need to make sure that $HOME/bin is first in the binary search path so that. # `bazel` will find the latest version of bazel installed in the user's home. # directory. This is set in setting.sh as all DeepVariant scripts source. # settings.sh and assume that `bazel` will find the right version. export PATH=""$HOME/bin:$PATH"". # Path to the public bucket containing DeepVariant-related artifacts. export DEEPVARIANT_BUCKET=""gs://deepvariant"". export DV_PACKAGE_BUCKET_PATH=""${DEEPVARIANT_BUCKET}/packages"". export DV_PACKAGE_CURL_PATH=""https://storage.googleapis.com/deepvariant/packages"". # Set this to 1 to use the nightly (latest) build of TensorFlow instead of a. # named release version. Set it to an already existing value in the environment. # (allowing command line control of the build), defaulting to 0 (release build). # Note that setting this to 1 implies that the C++ code in DeepVariant will be. # build using the master branch and not the pinned version to avoid. # incompatibilities between TensorFlow C++ used to build DeepVariant and the. # tf-nightly wheel. export DV_TF_NIGHTLY_BUILD=""${DV_TF_NIGHTLY_BUILD:-1}"". # The branch/tag we checkout to build our C++ dependencies against. This is not. # the same as the python version of TensorFlow we use, but should be similar ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:3978,modifiability,pac,packages,3978,"F_NEED_GCP=1. export TF_NEED_GDR=0. export TF_NEED_HDFS=0. export TF_NEED_JEMALLOC=0. export TF_NEED_MKL=1. export TF_NEED_MPI=0. export TF_NEED_OPENCL=0. export TF_NEED_OPENCL_SYCL=0. export TF_NEED_S3=1. export TF_NEED_VERBS=0. # Used if TF_NEED_CUDA=1. export TF_CUDA_VERSION=""10.0"". export CUDA_TOOLKIT_PATH=""/usr/local/cuda"". export TF_CUDNN_VERSION=""7"". export CUDNN_INSTALL_PATH=""/usr/lib/x86_64-linux-gnu"". # The version of bazel we want to build DeepVariant. DV_BAZEL_VERSION=""0.15.0"". # We need to make sure that $HOME/bin is first in the binary search path so that. # `bazel` will find the latest version of bazel installed in the user's home. # directory. This is set in setting.sh as all DeepVariant scripts source. # settings.sh and assume that `bazel` will find the right version. export PATH=""$HOME/bin:$PATH"". # Path to the public bucket containing DeepVariant-related artifacts. export DEEPVARIANT_BUCKET=""gs://deepvariant"". export DV_PACKAGE_BUCKET_PATH=""${DEEPVARIANT_BUCKET}/packages"". export DV_PACKAGE_CURL_PATH=""https://storage.googleapis.com/deepvariant/packages"". # Set this to 1 to use the nightly (latest) build of TensorFlow instead of a. # named release version. Set it to an already existing value in the environment. # (allowing command line control of the build), defaulting to 0 (release build). # Note that setting this to 1 implies that the C++ code in DeepVariant will be. # build using the master branch and not the pinned version to avoid. # incompatibilities between TensorFlow C++ used to build DeepVariant and the. # tf-nightly wheel. export DV_TF_NIGHTLY_BUILD=""${DV_TF_NIGHTLY_BUILD:-1}"". # The branch/tag we checkout to build our C++ dependencies against. This is not. # the same as the python version of TensorFlow we use, but should be similar or. # we risk having version incompatibilities between our C++ code and the Python. # code we use at runtime. if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then. export DV_CPP_TENSORFLOW_TAG=""master"". else. export ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:4061,modifiability,pac,packages,4061,"0. export TF_NEED_MKL=1. export TF_NEED_MPI=0. export TF_NEED_OPENCL=0. export TF_NEED_OPENCL_SYCL=0. export TF_NEED_S3=1. export TF_NEED_VERBS=0. # Used if TF_NEED_CUDA=1. export TF_CUDA_VERSION=""10.0"". export CUDA_TOOLKIT_PATH=""/usr/local/cuda"". export TF_CUDNN_VERSION=""7"". export CUDNN_INSTALL_PATH=""/usr/lib/x86_64-linux-gnu"". # The version of bazel we want to build DeepVariant. DV_BAZEL_VERSION=""0.15.0"". # We need to make sure that $HOME/bin is first in the binary search path so that. # `bazel` will find the latest version of bazel installed in the user's home. # directory. This is set in setting.sh as all DeepVariant scripts source. # settings.sh and assume that `bazel` will find the right version. export PATH=""$HOME/bin:$PATH"". # Path to the public bucket containing DeepVariant-related artifacts. export DEEPVARIANT_BUCKET=""gs://deepvariant"". export DV_PACKAGE_BUCKET_PATH=""${DEEPVARIANT_BUCKET}/packages"". export DV_PACKAGE_CURL_PATH=""https://storage.googleapis.com/deepvariant/packages"". # Set this to 1 to use the nightly (latest) build of TensorFlow instead of a. # named release version. Set it to an already existing value in the environment. # (allowing command line control of the build), defaulting to 0 (release build). # Note that setting this to 1 implies that the C++ code in DeepVariant will be. # build using the master branch and not the pinned version to avoid. # incompatibilities between TensorFlow C++ used to build DeepVariant and the. # tf-nightly wheel. export DV_TF_NIGHTLY_BUILD=""${DV_TF_NIGHTLY_BUILD:-1}"". # The branch/tag we checkout to build our C++ dependencies against. This is not. # the same as the python version of TensorFlow we use, but should be similar or. # we risk having version incompatibilities between our C++ code and the Python. # code we use at runtime. if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then. export DV_CPP_TENSORFLOW_TAG=""master"". else. export DV_CPP_TENSORFLOW_TAG=""r1.12"". fi. export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0"".",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:4166,modifiability,version,version,4166,"ort TF_NEED_S3=1. export TF_NEED_VERBS=0. # Used if TF_NEED_CUDA=1. export TF_CUDA_VERSION=""10.0"". export CUDA_TOOLKIT_PATH=""/usr/local/cuda"". export TF_CUDNN_VERSION=""7"". export CUDNN_INSTALL_PATH=""/usr/lib/x86_64-linux-gnu"". # The version of bazel we want to build DeepVariant. DV_BAZEL_VERSION=""0.15.0"". # We need to make sure that $HOME/bin is first in the binary search path so that. # `bazel` will find the latest version of bazel installed in the user's home. # directory. This is set in setting.sh as all DeepVariant scripts source. # settings.sh and assume that `bazel` will find the right version. export PATH=""$HOME/bin:$PATH"". # Path to the public bucket containing DeepVariant-related artifacts. export DEEPVARIANT_BUCKET=""gs://deepvariant"". export DV_PACKAGE_BUCKET_PATH=""${DEEPVARIANT_BUCKET}/packages"". export DV_PACKAGE_CURL_PATH=""https://storage.googleapis.com/deepvariant/packages"". # Set this to 1 to use the nightly (latest) build of TensorFlow instead of a. # named release version. Set it to an already existing value in the environment. # (allowing command line control of the build), defaulting to 0 (release build). # Note that setting this to 1 implies that the C++ code in DeepVariant will be. # build using the master branch and not the pinned version to avoid. # incompatibilities between TensorFlow C++ used to build DeepVariant and the. # tf-nightly wheel. export DV_TF_NIGHTLY_BUILD=""${DV_TF_NIGHTLY_BUILD:-1}"". # The branch/tag we checkout to build our C++ dependencies against. This is not. # the same as the python version of TensorFlow we use, but should be similar or. # we risk having version incompatibilities between our C++ code and the Python. # code we use at runtime. if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then. export DV_CPP_TENSORFLOW_TAG=""master"". else. export DV_CPP_TENSORFLOW_TAG=""r1.12"". fi. export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:4443,modifiability,version,version,4443,"t. DV_BAZEL_VERSION=""0.15.0"". # We need to make sure that $HOME/bin is first in the binary search path so that. # `bazel` will find the latest version of bazel installed in the user's home. # directory. This is set in setting.sh as all DeepVariant scripts source. # settings.sh and assume that `bazel` will find the right version. export PATH=""$HOME/bin:$PATH"". # Path to the public bucket containing DeepVariant-related artifacts. export DEEPVARIANT_BUCKET=""gs://deepvariant"". export DV_PACKAGE_BUCKET_PATH=""${DEEPVARIANT_BUCKET}/packages"". export DV_PACKAGE_CURL_PATH=""https://storage.googleapis.com/deepvariant/packages"". # Set this to 1 to use the nightly (latest) build of TensorFlow instead of a. # named release version. Set it to an already existing value in the environment. # (allowing command line control of the build), defaulting to 0 (release build). # Note that setting this to 1 implies that the C++ code in DeepVariant will be. # build using the master branch and not the pinned version to avoid. # incompatibilities between TensorFlow C++ used to build DeepVariant and the. # tf-nightly wheel. export DV_TF_NIGHTLY_BUILD=""${DV_TF_NIGHTLY_BUILD:-1}"". # The branch/tag we checkout to build our C++ dependencies against. This is not. # the same as the python version of TensorFlow we use, but should be similar or. # we risk having version incompatibilities between our C++ code and the Python. # code we use at runtime. if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then. export DV_CPP_TENSORFLOW_TAG=""master"". else. export DV_CPP_TENSORFLOW_TAG=""r1.12"". fi. export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing. # value in the environment (allowing command line control of the build),. # defaulting to 0 (CPU only build). export DV_GPU_BUILD=""${DV_GPU_BUILD:-1}"". # If this variable is set to 1, DeepVari",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:4661,modifiability,depend,dependencies,4661,"tting.sh as all DeepVariant scripts source. # settings.sh and assume that `bazel` will find the right version. export PATH=""$HOME/bin:$PATH"". # Path to the public bucket containing DeepVariant-related artifacts. export DEEPVARIANT_BUCKET=""gs://deepvariant"". export DV_PACKAGE_BUCKET_PATH=""${DEEPVARIANT_BUCKET}/packages"". export DV_PACKAGE_CURL_PATH=""https://storage.googleapis.com/deepvariant/packages"". # Set this to 1 to use the nightly (latest) build of TensorFlow instead of a. # named release version. Set it to an already existing value in the environment. # (allowing command line control of the build), defaulting to 0 (release build). # Note that setting this to 1 implies that the C++ code in DeepVariant will be. # build using the master branch and not the pinned version to avoid. # incompatibilities between TensorFlow C++ used to build DeepVariant and the. # tf-nightly wheel. export DV_TF_NIGHTLY_BUILD=""${DV_TF_NIGHTLY_BUILD:-1}"". # The branch/tag we checkout to build our C++ dependencies against. This is not. # the same as the python version of TensorFlow we use, but should be similar or. # we risk having version incompatibilities between our C++ code and the Python. # code we use at runtime. if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then. export DV_CPP_TENSORFLOW_TAG=""master"". else. export DV_CPP_TENSORFLOW_TAG=""r1.12"". fi. export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing. # value in the environment (allowing command line control of the build),. # defaulting to 0 (CPU only build). export DV_GPU_BUILD=""${DV_GPU_BUILD:-1}"". # If this variable is set to 1, DeepVariant will use a TensorFlow wheel file. # compiled with MKL support for corei7 or better chipsets, which. # significantly speeds up execution when running on modern CPUs. The default. # TensorFlow wheel files don't contain",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:4721,modifiability,version,version,4721,"and assume that `bazel` will find the right version. export PATH=""$HOME/bin:$PATH"". # Path to the public bucket containing DeepVariant-related artifacts. export DEEPVARIANT_BUCKET=""gs://deepvariant"". export DV_PACKAGE_BUCKET_PATH=""${DEEPVARIANT_BUCKET}/packages"". export DV_PACKAGE_CURL_PATH=""https://storage.googleapis.com/deepvariant/packages"". # Set this to 1 to use the nightly (latest) build of TensorFlow instead of a. # named release version. Set it to an already existing value in the environment. # (allowing command line control of the build), defaulting to 0 (release build). # Note that setting this to 1 implies that the C++ code in DeepVariant will be. # build using the master branch and not the pinned version to avoid. # incompatibilities between TensorFlow C++ used to build DeepVariant and the. # tf-nightly wheel. export DV_TF_NIGHTLY_BUILD=""${DV_TF_NIGHTLY_BUILD:-1}"". # The branch/tag we checkout to build our C++ dependencies against. This is not. # the same as the python version of TensorFlow we use, but should be similar or. # we risk having version incompatibilities between our C++ code and the Python. # code we use at runtime. if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then. export DV_CPP_TENSORFLOW_TAG=""master"". else. export DV_CPP_TENSORFLOW_TAG=""r1.12"". fi. export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing. # value in the environment (allowing command line control of the build),. # defaulting to 0 (CPU only build). export DV_GPU_BUILD=""${DV_GPU_BUILD:-1}"". # If this variable is set to 1, DeepVariant will use a TensorFlow wheel file. # compiled with MKL support for corei7 or better chipsets, which. # significantly speeds up execution when running on modern CPUs. The default. # TensorFlow wheel files don't contain these instructions (and thereby run on a. # broader set o",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:4794,modifiability,version,version,4794,"in:$PATH"". # Path to the public bucket containing DeepVariant-related artifacts. export DEEPVARIANT_BUCKET=""gs://deepvariant"". export DV_PACKAGE_BUCKET_PATH=""${DEEPVARIANT_BUCKET}/packages"". export DV_PACKAGE_CURL_PATH=""https://storage.googleapis.com/deepvariant/packages"". # Set this to 1 to use the nightly (latest) build of TensorFlow instead of a. # named release version. Set it to an already existing value in the environment. # (allowing command line control of the build), defaulting to 0 (release build). # Note that setting this to 1 implies that the C++ code in DeepVariant will be. # build using the master branch and not the pinned version to avoid. # incompatibilities between TensorFlow C++ used to build DeepVariant and the. # tf-nightly wheel. export DV_TF_NIGHTLY_BUILD=""${DV_TF_NIGHTLY_BUILD:-1}"". # The branch/tag we checkout to build our C++ dependencies against. This is not. # the same as the python version of TensorFlow we use, but should be similar or. # we risk having version incompatibilities between our C++ code and the Python. # code we use at runtime. if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then. export DV_CPP_TENSORFLOW_TAG=""master"". else. export DV_CPP_TENSORFLOW_TAG=""r1.12"". fi. export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing. # value in the environment (allowing command line control of the build),. # defaulting to 0 (CPU only build). export DV_GPU_BUILD=""${DV_GPU_BUILD:-1}"". # If this variable is set to 1, DeepVariant will use a TensorFlow wheel file. # compiled with MKL support for corei7 or better chipsets, which. # significantly speeds up execution when running on modern CPUs. The default. # TensorFlow wheel files don't contain these instructions (and thereby run on a. # broader set of CPUs). Using this optimized wheel reduces the runtime of. # DeepVariant",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:5417,modifiability,variab,variable,5417,"ch and not the pinned version to avoid. # incompatibilities between TensorFlow C++ used to build DeepVariant and the. # tf-nightly wheel. export DV_TF_NIGHTLY_BUILD=""${DV_TF_NIGHTLY_BUILD:-1}"". # The branch/tag we checkout to build our C++ dependencies against. This is not. # the same as the python version of TensorFlow we use, but should be similar or. # we risk having version incompatibilities between our C++ code and the Python. # code we use at runtime. if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then. export DV_CPP_TENSORFLOW_TAG=""master"". else. export DV_CPP_TENSORFLOW_TAG=""r1.12"". fi. export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing. # value in the environment (allowing command line control of the build),. # defaulting to 0 (CPU only build). export DV_GPU_BUILD=""${DV_GPU_BUILD:-1}"". # If this variable is set to 1, DeepVariant will use a TensorFlow wheel file. # compiled with MKL support for corei7 or better chipsets, which. # significantly speeds up execution when running on modern CPUs. The default. # TensorFlow wheel files don't contain these instructions (and thereby run on a. # broader set of CPUs). Using this optimized wheel reduces the runtime of. # DeepVariant's call_variants step by >3x. This is called the GCP (Google Cloud. # Platform) optimized wheel because all GCP instances have at least Sandy Bridge. # or better chipsets, so this wheel should run anywhere on GCP. export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}"". export GCP_OPTIMIZED_TF_WHL_FILENAME=""tensorflow-${DV_GCP_OPTIMIZED_TF_WHL_VERSION}.deepvariant_gcp-cp27-none-linux_x86_64.whl"". export GCP_OPTIMIZED_TF_WHL_PATH=""${DV_PACKAGE_BUCKET_PATH}/tensorflow"". export GCP_OPTIMIZED_TF_WHL_CURL_PATH=""${DV_PACKAGE_CURL_PATH}/tensorflow"". # Set this to 1 to make our prereq scripts install the CUDA",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:840,performance,error,error,840,"Cannot build deepvariant with tensorflow master due to bazel version requirements; With `DV_CPP_TENSORFLOW_TAG=master`, I believe deepvariant will not build from source. TF master does not build with Bazel=0.15.0:. ```. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/... [bazel INFO src/main/cpp/option_processor.cc:235] Looking for master bazelrcs in the following three paths: /root/deepvariant/tools/bazel.rc, , /etc/bazel.bazelrc. [bazel INFO src/main/cpp/option_processor.cc:165] User provided no rc file. [bazel INFO src/main/cpp/rc_file.cc:53] Parsing the RcFile /root/deepvariant/tools/bazel.rc. [bazel INFO src/main/cpp/rc_file.cc:53] Parsing the RcFile /root/deepvariant/../tensorflow/tools/bazel.rc. [bazel FATAL src/main/cpp/blaze.cc:1263] Unexpected error reading .blazerc file '/root/deepvariant/../tensorflow/tools/bazel.rc'. ```. And deepvariant does not build with bazel=0.19.0, see #134. ___. Linux xxx 4.15.0-1031-aws #33-Ubuntu SMP Fri Dec 7 09:32:27 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux. ```. # Copyright 2017 Google LLC. #. # Redistribution and use in source and binary forms, with or without. # modification, are permitted provided that the following conditions. # are met:. #. # 1. Redistributions of source code must retain the above copyright notice,. # this list of conditions and the following disclaimer. #. # 2. Redistributions in binary form must reproduce the above copyright. # notice, this list of conditions and the following disclaimer in the. # documentation and/or other materials provided with the distribution. #. # 3. Neither the name of the copyright holder nor the names of its. # contributors may be used to endorse or promote products derived from this. # software without specific prior written permission. #. # THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"". # AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE. # IMPLIE",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:5218,performance,GPU,GPUs,5218,"vironment. # (allowing command line control of the build), defaulting to 0 (release build). # Note that setting this to 1 implies that the C++ code in DeepVariant will be. # build using the master branch and not the pinned version to avoid. # incompatibilities between TensorFlow C++ used to build DeepVariant and the. # tf-nightly wheel. export DV_TF_NIGHTLY_BUILD=""${DV_TF_NIGHTLY_BUILD:-1}"". # The branch/tag we checkout to build our C++ dependencies against. This is not. # the same as the python version of TensorFlow we use, but should be similar or. # we risk having version incompatibilities between our C++ code and the Python. # code we use at runtime. if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then. export DV_CPP_TENSORFLOW_TAG=""master"". else. export DV_CPP_TENSORFLOW_TAG=""r1.12"". fi. export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing. # value in the environment (allowing command line control of the build),. # defaulting to 0 (CPU only build). export DV_GPU_BUILD=""${DV_GPU_BUILD:-1}"". # If this variable is set to 1, DeepVariant will use a TensorFlow wheel file. # compiled with MKL support for corei7 or better chipsets, which. # significantly speeds up execution when running on modern CPUs. The default. # TensorFlow wheel files don't contain these instructions (and thereby run on a. # broader set of CPUs). Using this optimized wheel reduces the runtime of. # DeepVariant's call_variants step by >3x. This is called the GCP (Google Cloud. # Platform) optimized wheel because all GCP instances have at least Sandy Bridge. # or better chipsets, so this wheel should run anywhere on GCP. export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}"". export GCP_OPTIMIZED_TF_WHL_FILENAME=""tensorflow-${DV_GCP_OPTIMIZED_TF_WHL_VERSION}.deepvariant_gcp-cp27-none-linux_x86_64.whl"". export ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:5348,performance,CPU,CPU,5348,"that the C++ code in DeepVariant will be. # build using the master branch and not the pinned version to avoid. # incompatibilities between TensorFlow C++ used to build DeepVariant and the. # tf-nightly wheel. export DV_TF_NIGHTLY_BUILD=""${DV_TF_NIGHTLY_BUILD:-1}"". # The branch/tag we checkout to build our C++ dependencies against. This is not. # the same as the python version of TensorFlow we use, but should be similar or. # we risk having version incompatibilities between our C++ code and the Python. # code we use at runtime. if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then. export DV_CPP_TENSORFLOW_TAG=""master"". else. export DV_CPP_TENSORFLOW_TAG=""r1.12"". fi. export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing. # value in the environment (allowing command line control of the build),. # defaulting to 0 (CPU only build). export DV_GPU_BUILD=""${DV_GPU_BUILD:-1}"". # If this variable is set to 1, DeepVariant will use a TensorFlow wheel file. # compiled with MKL support for corei7 or better chipsets, which. # significantly speeds up execution when running on modern CPUs. The default. # TensorFlow wheel files don't contain these instructions (and thereby run on a. # broader set of CPUs). Using this optimized wheel reduces the runtime of. # DeepVariant's call_variants step by >3x. This is called the GCP (Google Cloud. # Platform) optimized wheel because all GCP instances have at least Sandy Bridge. # or better chipsets, so this wheel should run anywhere on GCP. export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}"". export GCP_OPTIMIZED_TF_WHL_FILENAME=""tensorflow-${DV_GCP_OPTIMIZED_TF_WHL_VERSION}.deepvariant_gcp-cp27-none-linux_x86_64.whl"". export GCP_OPTIMIZED_TF_WHL_PATH=""${DV_PACKAGE_BUCKET_PATH}/tensorflow"". export GCP_OPTIMIZED_TF_WHL_CURL_PATH=""${DV_PACKAGE_CURL_PATH}/t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:5610,performance,CPU,CPUs,5610,""". # The branch/tag we checkout to build our C++ dependencies against. This is not. # the same as the python version of TensorFlow we use, but should be similar or. # we risk having version incompatibilities between our C++ code and the Python. # code we use at runtime. if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then. export DV_CPP_TENSORFLOW_TAG=""master"". else. export DV_CPP_TENSORFLOW_TAG=""r1.12"". fi. export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing. # value in the environment (allowing command line control of the build),. # defaulting to 0 (CPU only build). export DV_GPU_BUILD=""${DV_GPU_BUILD:-1}"". # If this variable is set to 1, DeepVariant will use a TensorFlow wheel file. # compiled with MKL support for corei7 or better chipsets, which. # significantly speeds up execution when running on modern CPUs. The default. # TensorFlow wheel files don't contain these instructions (and thereby run on a. # broader set of CPUs). Using this optimized wheel reduces the runtime of. # DeepVariant's call_variants step by >3x. This is called the GCP (Google Cloud. # Platform) optimized wheel because all GCP instances have at least Sandy Bridge. # or better chipsets, so this wheel should run anywhere on GCP. export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}"". export GCP_OPTIMIZED_TF_WHL_FILENAME=""tensorflow-${DV_GCP_OPTIMIZED_TF_WHL_VERSION}.deepvariant_gcp-cp27-none-linux_x86_64.whl"". export GCP_OPTIMIZED_TF_WHL_PATH=""${DV_PACKAGE_BUCKET_PATH}/tensorflow"". export GCP_OPTIMIZED_TF_WHL_CURL_PATH=""${DV_PACKAGE_CURL_PATH}/tensorflow"". # Set this to 1 to make our prereq scripts install the CUDA libraries. # If you already have CUDA installed, such as on a properly provisioned. # Docker image, it shouldn't be necessary. export DV_INSTALL_GPU_DRIVERS=""${DV_INSTALL_GPU_DRIVERS:-0}"". e",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:5727,performance,CPU,CPUs,5727,"of TensorFlow we use, but should be similar or. # we risk having version incompatibilities between our C++ code and the Python. # code we use at runtime. if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then. export DV_CPP_TENSORFLOW_TAG=""master"". else. export DV_CPP_TENSORFLOW_TAG=""r1.12"". fi. export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing. # value in the environment (allowing command line control of the build),. # defaulting to 0 (CPU only build). export DV_GPU_BUILD=""${DV_GPU_BUILD:-1}"". # If this variable is set to 1, DeepVariant will use a TensorFlow wheel file. # compiled with MKL support for corei7 or better chipsets, which. # significantly speeds up execution when running on modern CPUs. The default. # TensorFlow wheel files don't contain these instructions (and thereby run on a. # broader set of CPUs). Using this optimized wheel reduces the runtime of. # DeepVariant's call_variants step by >3x. This is called the GCP (Google Cloud. # Platform) optimized wheel because all GCP instances have at least Sandy Bridge. # or better chipsets, so this wheel should run anywhere on GCP. export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}"". export GCP_OPTIMIZED_TF_WHL_FILENAME=""tensorflow-${DV_GCP_OPTIMIZED_TF_WHL_VERSION}.deepvariant_gcp-cp27-none-linux_x86_64.whl"". export GCP_OPTIMIZED_TF_WHL_PATH=""${DV_PACKAGE_BUCKET_PATH}/tensorflow"". export GCP_OPTIMIZED_TF_WHL_CURL_PATH=""${DV_PACKAGE_CURL_PATH}/tensorflow"". # Set this to 1 to make our prereq scripts install the CUDA libraries. # If you already have CUDA installed, such as on a properly provisioned. # Docker image, it shouldn't be necessary. export DV_INSTALL_GPU_DRIVERS=""${DV_INSTALL_GPU_DRIVERS:-0}"". export PYTHON_BIN_PATH=$(which python). export USE_DEFAULT_PYTHON_LIB_PATH=1. export DV_COPT_FLAGS=""--copt=-march=core",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:5745,performance,optimiz,optimized,5745," but should be similar or. # we risk having version incompatibilities between our C++ code and the Python. # code we use at runtime. if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then. export DV_CPP_TENSORFLOW_TAG=""master"". else. export DV_CPP_TENSORFLOW_TAG=""r1.12"". fi. export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing. # value in the environment (allowing command line control of the build),. # defaulting to 0 (CPU only build). export DV_GPU_BUILD=""${DV_GPU_BUILD:-1}"". # If this variable is set to 1, DeepVariant will use a TensorFlow wheel file. # compiled with MKL support for corei7 or better chipsets, which. # significantly speeds up execution when running on modern CPUs. The default. # TensorFlow wheel files don't contain these instructions (and thereby run on a. # broader set of CPUs). Using this optimized wheel reduces the runtime of. # DeepVariant's call_variants step by >3x. This is called the GCP (Google Cloud. # Platform) optimized wheel because all GCP instances have at least Sandy Bridge. # or better chipsets, so this wheel should run anywhere on GCP. export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}"". export GCP_OPTIMIZED_TF_WHL_FILENAME=""tensorflow-${DV_GCP_OPTIMIZED_TF_WHL_VERSION}.deepvariant_gcp-cp27-none-linux_x86_64.whl"". export GCP_OPTIMIZED_TF_WHL_PATH=""${DV_PACKAGE_BUCKET_PATH}/tensorflow"". export GCP_OPTIMIZED_TF_WHL_CURL_PATH=""${DV_PACKAGE_CURL_PATH}/tensorflow"". # Set this to 1 to make our prereq scripts install the CUDA libraries. # If you already have CUDA installed, such as on a properly provisioned. # Docker image, it shouldn't be necessary. export DV_INSTALL_GPU_DRIVERS=""${DV_INSTALL_GPU_DRIVERS:-0}"". export PYTHON_BIN_PATH=$(which python). export USE_DEFAULT_PYTHON_LIB_PATH=1. export DV_COPT_FLAGS=""--copt=-march=corei7 --copt=-Wno-sign-c",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:5878,performance,optimiz,optimized,5878,"t runtime. if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then. export DV_CPP_TENSORFLOW_TAG=""master"". else. export DV_CPP_TENSORFLOW_TAG=""r1.12"". fi. export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing. # value in the environment (allowing command line control of the build),. # defaulting to 0 (CPU only build). export DV_GPU_BUILD=""${DV_GPU_BUILD:-1}"". # If this variable is set to 1, DeepVariant will use a TensorFlow wheel file. # compiled with MKL support for corei7 or better chipsets, which. # significantly speeds up execution when running on modern CPUs. The default. # TensorFlow wheel files don't contain these instructions (and thereby run on a. # broader set of CPUs). Using this optimized wheel reduces the runtime of. # DeepVariant's call_variants step by >3x. This is called the GCP (Google Cloud. # Platform) optimized wheel because all GCP instances have at least Sandy Bridge. # or better chipsets, so this wheel should run anywhere on GCP. export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}"". export GCP_OPTIMIZED_TF_WHL_FILENAME=""tensorflow-${DV_GCP_OPTIMIZED_TF_WHL_VERSION}.deepvariant_gcp-cp27-none-linux_x86_64.whl"". export GCP_OPTIMIZED_TF_WHL_PATH=""${DV_PACKAGE_BUCKET_PATH}/tensorflow"". export GCP_OPTIMIZED_TF_WHL_CURL_PATH=""${DV_PACKAGE_CURL_PATH}/tensorflow"". # Set this to 1 to make our prereq scripts install the CUDA libraries. # If you already have CUDA installed, such as on a properly provisioned. # Docker image, it shouldn't be necessary. export DV_INSTALL_GPU_DRIVERS=""${DV_INSTALL_GPU_DRIVERS:-0}"". export PYTHON_BIN_PATH=$(which python). export USE_DEFAULT_PYTHON_LIB_PATH=1. export DV_COPT_FLAGS=""--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings"". function note_build_stage {. echo ""========== [$(date)] Stage '${1}' starting"". }. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:180,reliability,doe,does,180,"Cannot build deepvariant with tensorflow master due to bazel version requirements; With `DV_CPP_TENSORFLOW_TAG=master`, I believe deepvariant will not build from source. TF master does not build with Bazel=0.15.0:. ```. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/... [bazel INFO src/main/cpp/option_processor.cc:235] Looking for master bazelrcs in the following three paths: /root/deepvariant/tools/bazel.rc, , /etc/bazel.bazelrc. [bazel INFO src/main/cpp/option_processor.cc:165] User provided no rc file. [bazel INFO src/main/cpp/rc_file.cc:53] Parsing the RcFile /root/deepvariant/tools/bazel.rc. [bazel INFO src/main/cpp/rc_file.cc:53] Parsing the RcFile /root/deepvariant/../tensorflow/tools/bazel.rc. [bazel FATAL src/main/cpp/blaze.cc:1263] Unexpected error reading .blazerc file '/root/deepvariant/../tensorflow/tools/bazel.rc'. ```. And deepvariant does not build with bazel=0.19.0, see #134. ___. Linux xxx 4.15.0-1031-aws #33-Ubuntu SMP Fri Dec 7 09:32:27 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux. ```. # Copyright 2017 Google LLC. #. # Redistribution and use in source and binary forms, with or without. # modification, are permitted provided that the following conditions. # are met:. #. # 1. Redistributions of source code must retain the above copyright notice,. # this list of conditions and the following disclaimer. #. # 2. Redistributions in binary form must reproduce the above copyright. # notice, this list of conditions and the following disclaimer in the. # documentation and/or other materials provided with the distribution. #. # 3. Neither the name of the copyright holder nor the names of its. # contributors may be used to endorse or promote products derived from this. # software without specific prior written permission. #. # THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"". # AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE. # IMPLIE",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:939,reliability,doe,does,939,"Cannot build deepvariant with tensorflow master due to bazel version requirements; With `DV_CPP_TENSORFLOW_TAG=master`, I believe deepvariant will not build from source. TF master does not build with Bazel=0.15.0:. ```. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/... [bazel INFO src/main/cpp/option_processor.cc:235] Looking for master bazelrcs in the following three paths: /root/deepvariant/tools/bazel.rc, , /etc/bazel.bazelrc. [bazel INFO src/main/cpp/option_processor.cc:165] User provided no rc file. [bazel INFO src/main/cpp/rc_file.cc:53] Parsing the RcFile /root/deepvariant/tools/bazel.rc. [bazel INFO src/main/cpp/rc_file.cc:53] Parsing the RcFile /root/deepvariant/../tensorflow/tools/bazel.rc. [bazel FATAL src/main/cpp/blaze.cc:1263] Unexpected error reading .blazerc file '/root/deepvariant/../tensorflow/tools/bazel.rc'. ```. And deepvariant does not build with bazel=0.19.0, see #134. ___. Linux xxx 4.15.0-1031-aws #33-Ubuntu SMP Fri Dec 7 09:32:27 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux. ```. # Copyright 2017 Google LLC. #. # Redistribution and use in source and binary forms, with or without. # modification, are permitted provided that the following conditions. # are met:. #. # 1. Redistributions of source code must retain the above copyright notice,. # this list of conditions and the following disclaimer. #. # 2. Redistributions in binary form must reproduce the above copyright. # notice, this list of conditions and the following disclaimer in the. # documentation and/or other materials provided with the distribution. #. # 3. Neither the name of the copyright holder nor the names of its. # contributors may be used to endorse or promote products derived from this. # software without specific prior written permission. #. # THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"". # AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE. # IMPLIE",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:2182,reliability,INCID,INCIDENTAL,2182,"without. # modification, are permitted provided that the following conditions. # are met:. #. # 1. Redistributions of source code must retain the above copyright notice,. # this list of conditions and the following disclaimer. #. # 2. Redistributions in binary form must reproduce the above copyright. # notice, this list of conditions and the following disclaimer in the. # documentation and/or other materials provided with the distribution. #. # 3. Neither the name of the copyright holder nor the names of its. # contributors may be used to endorse or promote products derived from this. # software without specific prior written permission. #. # THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"". # AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE. # IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. # ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE. # LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR. # CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF. # SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS. # INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN. # CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE). # ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE. # POSSIBILITY OF SUCH DAMAGE. # Source this file---these options are needed for TF config and for. # successive bazel runs. # Set this to 1 if the system image already has TensorFlow preinstalled. This. # will skip the installation of TensorFlow. export DV_USE_PREINSTALLED_TF=""${DV_USE_PREINSTALLED_TF:-0}"". export TF_CUDA_CLANG=0. export TF_ENABLE_XLA=1. export TF_NEED_CUDA=1. export TF_NEED_GCP=1. export TF_NEED_GDR=0. export TF_NEED_HDFS=0. export TF_NEED_JEMALLOC=0. export TF_NEED_MKL=1. export TF_NEED_MPI=0. export TF_NEED_OPENCL=0. export TF_NEED_OPENCL_SYCL=0. export TF_NEED_S3=1.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:228,safety,test,test,228,"Cannot build deepvariant with tensorflow master due to bazel version requirements; With `DV_CPP_TENSORFLOW_TAG=master`, I believe deepvariant will not build from source. TF master does not build with Bazel=0.15.0:. ```. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/... [bazel INFO src/main/cpp/option_processor.cc:235] Looking for master bazelrcs in the following three paths: /root/deepvariant/tools/bazel.rc, , /etc/bazel.bazelrc. [bazel INFO src/main/cpp/option_processor.cc:165] User provided no rc file. [bazel INFO src/main/cpp/rc_file.cc:53] Parsing the RcFile /root/deepvariant/tools/bazel.rc. [bazel INFO src/main/cpp/rc_file.cc:53] Parsing the RcFile /root/deepvariant/../tensorflow/tools/bazel.rc. [bazel FATAL src/main/cpp/blaze.cc:1263] Unexpected error reading .blazerc file '/root/deepvariant/../tensorflow/tools/bazel.rc'. ```. And deepvariant does not build with bazel=0.19.0, see #134. ___. Linux xxx 4.15.0-1031-aws #33-Ubuntu SMP Fri Dec 7 09:32:27 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux. ```. # Copyright 2017 Google LLC. #. # Redistribution and use in source and binary forms, with or without. # modification, are permitted provided that the following conditions. # are met:. #. # 1. Redistributions of source code must retain the above copyright notice,. # this list of conditions and the following disclaimer. #. # 2. Redistributions in binary form must reproduce the above copyright. # notice, this list of conditions and the following disclaimer in the. # documentation and/or other materials provided with the distribution. #. # 3. Neither the name of the copyright holder nor the names of its. # contributors may be used to endorse or promote products derived from this. # software without specific prior written permission. #. # THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"". # AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE. # IMPLIE",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:840,safety,error,error,840,"Cannot build deepvariant with tensorflow master due to bazel version requirements; With `DV_CPP_TENSORFLOW_TAG=master`, I believe deepvariant will not build from source. TF master does not build with Bazel=0.15.0:. ```. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/... [bazel INFO src/main/cpp/option_processor.cc:235] Looking for master bazelrcs in the following three paths: /root/deepvariant/tools/bazel.rc, , /etc/bazel.bazelrc. [bazel INFO src/main/cpp/option_processor.cc:165] User provided no rc file. [bazel INFO src/main/cpp/rc_file.cc:53] Parsing the RcFile /root/deepvariant/tools/bazel.rc. [bazel INFO src/main/cpp/rc_file.cc:53] Parsing the RcFile /root/deepvariant/../tensorflow/tools/bazel.rc. [bazel FATAL src/main/cpp/blaze.cc:1263] Unexpected error reading .blazerc file '/root/deepvariant/../tensorflow/tools/bazel.rc'. ```. And deepvariant does not build with bazel=0.19.0, see #134. ___. Linux xxx 4.15.0-1031-aws #33-Ubuntu SMP Fri Dec 7 09:32:27 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux. ```. # Copyright 2017 Google LLC. #. # Redistribution and use in source and binary forms, with or without. # modification, are permitted provided that the following conditions. # are met:. #. # 1. Redistributions of source code must retain the above copyright notice,. # this list of conditions and the following disclaimer. #. # 2. Redistributions in binary form must reproduce the above copyright. # notice, this list of conditions and the following disclaimer in the. # documentation and/or other materials provided with the distribution. #. # 3. Neither the name of the copyright holder nor the names of its. # contributors may be used to endorse or promote products derived from this. # software without specific prior written permission. #. # THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"". # AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE. # IMPLIE",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:1821,safety,permiss,permission,1821,"3] Unexpected error reading .blazerc file '/root/deepvariant/../tensorflow/tools/bazel.rc'. ```. And deepvariant does not build with bazel=0.19.0, see #134. ___. Linux xxx 4.15.0-1031-aws #33-Ubuntu SMP Fri Dec 7 09:32:27 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux. ```. # Copyright 2017 Google LLC. #. # Redistribution and use in source and binary forms, with or without. # modification, are permitted provided that the following conditions. # are met:. #. # 1. Redistributions of source code must retain the above copyright notice,. # this list of conditions and the following disclaimer. #. # 2. Redistributions in binary form must reproduce the above copyright. # notice, this list of conditions and the following disclaimer in the. # documentation and/or other materials provided with the distribution. #. # 3. Neither the name of the copyright holder nor the names of its. # contributors may be used to endorse or promote products derived from this. # software without specific prior written permission. #. # THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"". # AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE. # IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. # ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE. # LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR. # CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF. # SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS. # INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN. # CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE). # ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE. # POSSIBILITY OF SUCH DAMAGE. # Source this file---these options are needed for TF config and for. # successive bazel runs. # Set this to 1 if the system image already has TensorFlow preinstalled. This. # will skip the installation ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:4454,safety,avoid,avoid,4454,"L_VERSION=""0.15.0"". # We need to make sure that $HOME/bin is first in the binary search path so that. # `bazel` will find the latest version of bazel installed in the user's home. # directory. This is set in setting.sh as all DeepVariant scripts source. # settings.sh and assume that `bazel` will find the right version. export PATH=""$HOME/bin:$PATH"". # Path to the public bucket containing DeepVariant-related artifacts. export DEEPVARIANT_BUCKET=""gs://deepvariant"". export DV_PACKAGE_BUCKET_PATH=""${DEEPVARIANT_BUCKET}/packages"". export DV_PACKAGE_CURL_PATH=""https://storage.googleapis.com/deepvariant/packages"". # Set this to 1 to use the nightly (latest) build of TensorFlow instead of a. # named release version. Set it to an already existing value in the environment. # (allowing command line control of the build), defaulting to 0 (release build). # Note that setting this to 1 implies that the C++ code in DeepVariant will be. # build using the master branch and not the pinned version to avoid. # incompatibilities between TensorFlow C++ used to build DeepVariant and the. # tf-nightly wheel. export DV_TF_NIGHTLY_BUILD=""${DV_TF_NIGHTLY_BUILD:-1}"". # The branch/tag we checkout to build our C++ dependencies against. This is not. # the same as the python version of TensorFlow we use, but should be similar or. # we risk having version incompatibilities between our C++ code and the Python. # code we use at runtime. if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then. export DV_CPP_TENSORFLOW_TAG=""master"". else. export DV_CPP_TENSORFLOW_TAG=""r1.12"". fi. export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing. # value in the environment (allowing command line control of the build),. # defaulting to 0 (CPU only build). export DV_GPU_BUILD=""${DV_GPU_BUILD:-1}"". # If this variable is set to 1, DeepVariant will u",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:4661,safety,depend,dependencies,4661,"tting.sh as all DeepVariant scripts source. # settings.sh and assume that `bazel` will find the right version. export PATH=""$HOME/bin:$PATH"". # Path to the public bucket containing DeepVariant-related artifacts. export DEEPVARIANT_BUCKET=""gs://deepvariant"". export DV_PACKAGE_BUCKET_PATH=""${DEEPVARIANT_BUCKET}/packages"". export DV_PACKAGE_CURL_PATH=""https://storage.googleapis.com/deepvariant/packages"". # Set this to 1 to use the nightly (latest) build of TensorFlow instead of a. # named release version. Set it to an already existing value in the environment. # (allowing command line control of the build), defaulting to 0 (release build). # Note that setting this to 1 implies that the C++ code in DeepVariant will be. # build using the master branch and not the pinned version to avoid. # incompatibilities between TensorFlow C++ used to build DeepVariant and the. # tf-nightly wheel. export DV_TF_NIGHTLY_BUILD=""${DV_TF_NIGHTLY_BUILD:-1}"". # The branch/tag we checkout to build our C++ dependencies against. This is not. # the same as the python version of TensorFlow we use, but should be similar or. # we risk having version incompatibilities between our C++ code and the Python. # code we use at runtime. if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then. export DV_CPP_TENSORFLOW_TAG=""master"". else. export DV_CPP_TENSORFLOW_TAG=""r1.12"". fi. export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing. # value in the environment (allowing command line control of the build),. # defaulting to 0 (CPU only build). export DV_GPU_BUILD=""${DV_GPU_BUILD:-1}"". # If this variable is set to 1, DeepVariant will use a TensorFlow wheel file. # compiled with MKL support for corei7 or better chipsets, which. # significantly speeds up execution when running on modern CPUs. The default. # TensorFlow wheel files don't contain",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:4782,safety,risk,risk,4782," PATH=""$HOME/bin:$PATH"". # Path to the public bucket containing DeepVariant-related artifacts. export DEEPVARIANT_BUCKET=""gs://deepvariant"". export DV_PACKAGE_BUCKET_PATH=""${DEEPVARIANT_BUCKET}/packages"". export DV_PACKAGE_CURL_PATH=""https://storage.googleapis.com/deepvariant/packages"". # Set this to 1 to use the nightly (latest) build of TensorFlow instead of a. # named release version. Set it to an already existing value in the environment. # (allowing command line control of the build), defaulting to 0 (release build). # Note that setting this to 1 implies that the C++ code in DeepVariant will be. # build using the master branch and not the pinned version to avoid. # incompatibilities between TensorFlow C++ used to build DeepVariant and the. # tf-nightly wheel. export DV_TF_NIGHTLY_BUILD=""${DV_TF_NIGHTLY_BUILD:-1}"". # The branch/tag we checkout to build our C++ dependencies against. This is not. # the same as the python version of TensorFlow we use, but should be similar or. # we risk having version incompatibilities between our C++ code and the Python. # code we use at runtime. if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then. export DV_CPP_TENSORFLOW_TAG=""master"". else. export DV_CPP_TENSORFLOW_TAG=""r1.12"". fi. export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing. # value in the environment (allowing command line control of the build),. # defaulting to 0 (CPU only build). export DV_GPU_BUILD=""${DV_GPU_BUILD:-1}"". # If this variable is set to 1, DeepVariant will use a TensorFlow wheel file. # compiled with MKL support for corei7 or better chipsets, which. # significantly speeds up execution when running on modern CPUs. The default. # TensorFlow wheel files don't contain these instructions (and thereby run on a. # broader set of CPUs). Using this optimized wheel reduces the runtime of.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:293,security,sign,sign-compare,293,"Cannot build deepvariant with tensorflow master due to bazel version requirements; With `DV_CPP_TENSORFLOW_TAG=master`, I believe deepvariant will not build from source. TF master does not build with Bazel=0.15.0:. ```. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/... [bazel INFO src/main/cpp/option_processor.cc:235] Looking for master bazelrcs in the following three paths: /root/deepvariant/tools/bazel.rc, , /etc/bazel.bazelrc. [bazel INFO src/main/cpp/option_processor.cc:165] User provided no rc file. [bazel INFO src/main/cpp/rc_file.cc:53] Parsing the RcFile /root/deepvariant/tools/bazel.rc. [bazel INFO src/main/cpp/rc_file.cc:53] Parsing the RcFile /root/deepvariant/../tensorflow/tools/bazel.rc. [bazel FATAL src/main/cpp/blaze.cc:1263] Unexpected error reading .blazerc file '/root/deepvariant/../tensorflow/tools/bazel.rc'. ```. And deepvariant does not build with bazel=0.19.0, see #134. ___. Linux xxx 4.15.0-1031-aws #33-Ubuntu SMP Fri Dec 7 09:32:27 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux. ```. # Copyright 2017 Google LLC. #. # Redistribution and use in source and binary forms, with or without. # modification, are permitted provided that the following conditions. # are met:. #. # 1. Redistributions of source code must retain the above copyright notice,. # this list of conditions and the following disclaimer. #. # 2. Redistributions in binary form must reproduce the above copyright. # notice, this list of conditions and the following disclaimer in the. # documentation and/or other materials provided with the distribution. #. # 3. Neither the name of the copyright holder nor the names of its. # contributors may be used to endorse or promote products derived from this. # software without specific prior written permission. #. # THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"". # AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE. # IMPLIE",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:1198,security,modif,modification,1198,"l=0.15.0:. ```. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/... [bazel INFO src/main/cpp/option_processor.cc:235] Looking for master bazelrcs in the following three paths: /root/deepvariant/tools/bazel.rc, , /etc/bazel.bazelrc. [bazel INFO src/main/cpp/option_processor.cc:165] User provided no rc file. [bazel INFO src/main/cpp/rc_file.cc:53] Parsing the RcFile /root/deepvariant/tools/bazel.rc. [bazel INFO src/main/cpp/rc_file.cc:53] Parsing the RcFile /root/deepvariant/../tensorflow/tools/bazel.rc. [bazel FATAL src/main/cpp/blaze.cc:1263] Unexpected error reading .blazerc file '/root/deepvariant/../tensorflow/tools/bazel.rc'. ```. And deepvariant does not build with bazel=0.19.0, see #134. ___. Linux xxx 4.15.0-1031-aws #33-Ubuntu SMP Fri Dec 7 09:32:27 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux. ```. # Copyright 2017 Google LLC. #. # Redistribution and use in source and binary forms, with or without. # modification, are permitted provided that the following conditions. # are met:. #. # 1. Redistributions of source code must retain the above copyright notice,. # this list of conditions and the following disclaimer. #. # 2. Redistributions in binary form must reproduce the above copyright. # notice, this list of conditions and the following disclaimer in the. # documentation and/or other materials provided with the distribution. #. # 3. Neither the name of the copyright holder nor the names of its. # contributors may be used to endorse or promote products derived from this. # software without specific prior written permission. #. # THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"". # AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE. # IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. # ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE. # LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, E",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:2322,security,LOSS,LOSS,2322,"tain the above copyright notice,. # this list of conditions and the following disclaimer. #. # 2. Redistributions in binary form must reproduce the above copyright. # notice, this list of conditions and the following disclaimer in the. # documentation and/or other materials provided with the distribution. #. # 3. Neither the name of the copyright holder nor the names of its. # contributors may be used to endorse or promote products derived from this. # software without specific prior written permission. #. # THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"". # AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE. # IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. # ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE. # LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR. # CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF. # SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS. # INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN. # CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE). # ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE. # POSSIBILITY OF SUCH DAMAGE. # Source this file---these options are needed for TF config and for. # successive bazel runs. # Set this to 1 if the system image already has TensorFlow preinstalled. This. # will skip the installation of TensorFlow. export DV_USE_PREINSTALLED_TF=""${DV_USE_PREINSTALLED_TF:-0}"". export TF_CUDA_CLANG=0. export TF_ENABLE_XLA=1. export TF_NEED_CUDA=1. export TF_NEED_GCP=1. export TF_NEED_GDR=0. export TF_NEED_HDFS=0. export TF_NEED_JEMALLOC=0. export TF_NEED_MKL=1. export TF_NEED_MPI=0. export TF_NEED_OPENCL=0. export TF_NEED_OPENCL_SYCL=0. export TF_NEED_S3=1. export TF_NEED_VERBS=0. # Used if TF_NEED_CUDA=1. export TF_CUDA_VERSION=""10.0"". export CUDA_TOOLKIT_PATH=""/usr/local/cuda"". export TF_C",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:4256,security,control,control,4256,"=""10.0"". export CUDA_TOOLKIT_PATH=""/usr/local/cuda"". export TF_CUDNN_VERSION=""7"". export CUDNN_INSTALL_PATH=""/usr/lib/x86_64-linux-gnu"". # The version of bazel we want to build DeepVariant. DV_BAZEL_VERSION=""0.15.0"". # We need to make sure that $HOME/bin is first in the binary search path so that. # `bazel` will find the latest version of bazel installed in the user's home. # directory. This is set in setting.sh as all DeepVariant scripts source. # settings.sh and assume that `bazel` will find the right version. export PATH=""$HOME/bin:$PATH"". # Path to the public bucket containing DeepVariant-related artifacts. export DEEPVARIANT_BUCKET=""gs://deepvariant"". export DV_PACKAGE_BUCKET_PATH=""${DEEPVARIANT_BUCKET}/packages"". export DV_PACKAGE_CURL_PATH=""https://storage.googleapis.com/deepvariant/packages"". # Set this to 1 to use the nightly (latest) build of TensorFlow instead of a. # named release version. Set it to an already existing value in the environment. # (allowing command line control of the build), defaulting to 0 (release build). # Note that setting this to 1 implies that the C++ code in DeepVariant will be. # build using the master branch and not the pinned version to avoid. # incompatibilities between TensorFlow C++ used to build DeepVariant and the. # tf-nightly wheel. export DV_TF_NIGHTLY_BUILD=""${DV_TF_NIGHTLY_BUILD:-1}"". # The branch/tag we checkout to build our C++ dependencies against. This is not. # the same as the python version of TensorFlow we use, but should be similar or. # we risk having version incompatibilities between our C++ code and the Python. # code we use at runtime. if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then. export DV_CPP_TENSORFLOW_TAG=""master"". else. export DV_CPP_TENSORFLOW_TAG=""r1.12"". fi. export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing. # val",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:4782,security,risk,risk,4782," PATH=""$HOME/bin:$PATH"". # Path to the public bucket containing DeepVariant-related artifacts. export DEEPVARIANT_BUCKET=""gs://deepvariant"". export DV_PACKAGE_BUCKET_PATH=""${DEEPVARIANT_BUCKET}/packages"". export DV_PACKAGE_CURL_PATH=""https://storage.googleapis.com/deepvariant/packages"". # Set this to 1 to use the nightly (latest) build of TensorFlow instead of a. # named release version. Set it to an already existing value in the environment. # (allowing command line control of the build), defaulting to 0 (release build). # Note that setting this to 1 implies that the C++ code in DeepVariant will be. # build using the master branch and not the pinned version to avoid. # incompatibilities between TensorFlow C++ used to build DeepVariant and the. # tf-nightly wheel. export DV_TF_NIGHTLY_BUILD=""${DV_TF_NIGHTLY_BUILD:-1}"". # The branch/tag we checkout to build our C++ dependencies against. This is not. # the same as the python version of TensorFlow we use, but should be similar or. # we risk having version incompatibilities between our C++ code and the Python. # code we use at runtime. if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then. export DV_CPP_TENSORFLOW_TAG=""master"". else. export DV_CPP_TENSORFLOW_TAG=""r1.12"". fi. export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing. # value in the environment (allowing command line control of the build),. # defaulting to 0 (CPU only build). export DV_GPU_BUILD=""${DV_GPU_BUILD:-1}"". # If this variable is set to 1, DeepVariant will use a TensorFlow wheel file. # compiled with MKL support for corei7 or better chipsets, which. # significantly speeds up execution when running on modern CPUs. The default. # TensorFlow wheel files don't contain these instructions (and thereby run on a. # broader set of CPUs). Using this optimized wheel reduces the runtime of.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:5305,security,control,control,5305,"). # Note that setting this to 1 implies that the C++ code in DeepVariant will be. # build using the master branch and not the pinned version to avoid. # incompatibilities between TensorFlow C++ used to build DeepVariant and the. # tf-nightly wheel. export DV_TF_NIGHTLY_BUILD=""${DV_TF_NIGHTLY_BUILD:-1}"". # The branch/tag we checkout to build our C++ dependencies against. This is not. # the same as the python version of TensorFlow we use, but should be similar or. # we risk having version incompatibilities between our C++ code and the Python. # code we use at runtime. if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then. export DV_CPP_TENSORFLOW_TAG=""master"". else. export DV_CPP_TENSORFLOW_TAG=""r1.12"". fi. export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing. # value in the environment (allowing command line control of the build),. # defaulting to 0 (CPU only build). export DV_GPU_BUILD=""${DV_GPU_BUILD:-1}"". # If this variable is set to 1, DeepVariant will use a TensorFlow wheel file. # compiled with MKL support for corei7 or better chipsets, which. # significantly speeds up execution when running on modern CPUs. The default. # TensorFlow wheel files don't contain these instructions (and thereby run on a. # broader set of CPUs). Using this optimized wheel reduces the runtime of. # DeepVariant's call_variants step by >3x. This is called the GCP (Google Cloud. # Platform) optimized wheel because all GCP instances have at least Sandy Bridge. # or better chipsets, so this wheel should run anywhere on GCP. export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}"". export GCP_OPTIMIZED_TF_WHL_FILENAME=""tensorflow-${DV_GCP_OPTIMIZED_TF_WHL_VERSION}.deepvariant_gcp-cp27-none-linux_x86_64.whl"". export GCP_OPTIMIZED_TF_WHL_PATH=""${DV_PACKAGE_BUCKET_PATH}/tensorflow"". export GCP_OPTIMIZED_TF",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:5553,security,sign,significantly,5553,"xport DV_TF_NIGHTLY_BUILD=""${DV_TF_NIGHTLY_BUILD:-1}"". # The branch/tag we checkout to build our C++ dependencies against. This is not. # the same as the python version of TensorFlow we use, but should be similar or. # we risk having version incompatibilities between our C++ code and the Python. # code we use at runtime. if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then. export DV_CPP_TENSORFLOW_TAG=""master"". else. export DV_CPP_TENSORFLOW_TAG=""r1.12"". fi. export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing. # value in the environment (allowing command line control of the build),. # defaulting to 0 (CPU only build). export DV_GPU_BUILD=""${DV_GPU_BUILD:-1}"". # If this variable is set to 1, DeepVariant will use a TensorFlow wheel file. # compiled with MKL support for corei7 or better chipsets, which. # significantly speeds up execution when running on modern CPUs. The default. # TensorFlow wheel files don't contain these instructions (and thereby run on a. # broader set of CPUs). Using this optimized wheel reduces the runtime of. # DeepVariant's call_variants step by >3x. This is called the GCP (Google Cloud. # Platform) optimized wheel because all GCP instances have at least Sandy Bridge. # or better chipsets, so this wheel should run anywhere on GCP. export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}"". export GCP_OPTIMIZED_TF_WHL_FILENAME=""tensorflow-${DV_GCP_OPTIMIZED_TF_WHL_VERSION}.deepvariant_gcp-cp27-none-linux_x86_64.whl"". export GCP_OPTIMIZED_TF_WHL_PATH=""${DV_PACKAGE_BUCKET_PATH}/tensorflow"". export GCP_OPTIMIZED_TF_WHL_CURL_PATH=""${DV_PACKAGE_CURL_PATH}/tensorflow"". # Set this to 1 to make our prereq scripts install the CUDA libraries. # If you already have CUDA installed, such as on a properly provisioned. # Docker image, it shouldn't be necessary. export DV_I",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:6744,security,sign,sign-compare,6744,"t runtime. if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then. export DV_CPP_TENSORFLOW_TAG=""master"". else. export DV_CPP_TENSORFLOW_TAG=""r1.12"". fi. export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing. # value in the environment (allowing command line control of the build),. # defaulting to 0 (CPU only build). export DV_GPU_BUILD=""${DV_GPU_BUILD:-1}"". # If this variable is set to 1, DeepVariant will use a TensorFlow wheel file. # compiled with MKL support for corei7 or better chipsets, which. # significantly speeds up execution when running on modern CPUs. The default. # TensorFlow wheel files don't contain these instructions (and thereby run on a. # broader set of CPUs). Using this optimized wheel reduces the runtime of. # DeepVariant's call_variants step by >3x. This is called the GCP (Google Cloud. # Platform) optimized wheel because all GCP instances have at least Sandy Bridge. # or better chipsets, so this wheel should run anywhere on GCP. export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}"". export GCP_OPTIMIZED_TF_WHL_FILENAME=""tensorflow-${DV_GCP_OPTIMIZED_TF_WHL_VERSION}.deepvariant_gcp-cp27-none-linux_x86_64.whl"". export GCP_OPTIMIZED_TF_WHL_PATH=""${DV_PACKAGE_BUCKET_PATH}/tensorflow"". export GCP_OPTIMIZED_TF_WHL_CURL_PATH=""${DV_PACKAGE_CURL_PATH}/tensorflow"". # Set this to 1 to make our prereq scripts install the CUDA libraries. # If you already have CUDA installed, such as on a properly provisioned. # Docker image, it shouldn't be necessary. export DV_INSTALL_GPU_DRIVERS=""${DV_INSTALL_GPU_DRIVERS:-0}"". export PYTHON_BIN_PATH=$(which python). export USE_DEFAULT_PYTHON_LIB_PATH=1. export DV_COPT_FLAGS=""--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings"". function note_build_stage {. echo ""========== [$(date)] Stage '${1}' starting"". }. ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:228,testability,test,test,228,"Cannot build deepvariant with tensorflow master due to bazel version requirements; With `DV_CPP_TENSORFLOW_TAG=master`, I believe deepvariant will not build from source. TF master does not build with Bazel=0.15.0:. ```. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/... [bazel INFO src/main/cpp/option_processor.cc:235] Looking for master bazelrcs in the following three paths: /root/deepvariant/tools/bazel.rc, , /etc/bazel.bazelrc. [bazel INFO src/main/cpp/option_processor.cc:165] User provided no rc file. [bazel INFO src/main/cpp/rc_file.cc:53] Parsing the RcFile /root/deepvariant/tools/bazel.rc. [bazel INFO src/main/cpp/rc_file.cc:53] Parsing the RcFile /root/deepvariant/../tensorflow/tools/bazel.rc. [bazel FATAL src/main/cpp/blaze.cc:1263] Unexpected error reading .blazerc file '/root/deepvariant/../tensorflow/tools/bazel.rc'. ```. And deepvariant does not build with bazel=0.19.0, see #134. ___. Linux xxx 4.15.0-1031-aws #33-Ubuntu SMP Fri Dec 7 09:32:27 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux. ```. # Copyright 2017 Google LLC. #. # Redistribution and use in source and binary forms, with or without. # modification, are permitted provided that the following conditions. # are met:. #. # 1. Redistributions of source code must retain the above copyright notice,. # this list of conditions and the following disclaimer. #. # 2. Redistributions in binary form must reproduce the above copyright. # notice, this list of conditions and the following disclaimer in the. # documentation and/or other materials provided with the distribution. #. # 3. Neither the name of the copyright holder nor the names of its. # contributors may be used to endorse or promote products derived from this. # software without specific prior written permission. #. # THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"". # AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE. # IMPLIE",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:4256,testability,control,control,4256,"=""10.0"". export CUDA_TOOLKIT_PATH=""/usr/local/cuda"". export TF_CUDNN_VERSION=""7"". export CUDNN_INSTALL_PATH=""/usr/lib/x86_64-linux-gnu"". # The version of bazel we want to build DeepVariant. DV_BAZEL_VERSION=""0.15.0"". # We need to make sure that $HOME/bin is first in the binary search path so that. # `bazel` will find the latest version of bazel installed in the user's home. # directory. This is set in setting.sh as all DeepVariant scripts source. # settings.sh and assume that `bazel` will find the right version. export PATH=""$HOME/bin:$PATH"". # Path to the public bucket containing DeepVariant-related artifacts. export DEEPVARIANT_BUCKET=""gs://deepvariant"". export DV_PACKAGE_BUCKET_PATH=""${DEEPVARIANT_BUCKET}/packages"". export DV_PACKAGE_CURL_PATH=""https://storage.googleapis.com/deepvariant/packages"". # Set this to 1 to use the nightly (latest) build of TensorFlow instead of a. # named release version. Set it to an already existing value in the environment. # (allowing command line control of the build), defaulting to 0 (release build). # Note that setting this to 1 implies that the C++ code in DeepVariant will be. # build using the master branch and not the pinned version to avoid. # incompatibilities between TensorFlow C++ used to build DeepVariant and the. # tf-nightly wheel. export DV_TF_NIGHTLY_BUILD=""${DV_TF_NIGHTLY_BUILD:-1}"". # The branch/tag we checkout to build our C++ dependencies against. This is not. # the same as the python version of TensorFlow we use, but should be similar or. # we risk having version incompatibilities between our C++ code and the Python. # code we use at runtime. if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then. export DV_CPP_TENSORFLOW_TAG=""master"". else. export DV_CPP_TENSORFLOW_TAG=""r1.12"". fi. export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing. # val",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:4661,testability,depend,dependencies,4661,"tting.sh as all DeepVariant scripts source. # settings.sh and assume that `bazel` will find the right version. export PATH=""$HOME/bin:$PATH"". # Path to the public bucket containing DeepVariant-related artifacts. export DEEPVARIANT_BUCKET=""gs://deepvariant"". export DV_PACKAGE_BUCKET_PATH=""${DEEPVARIANT_BUCKET}/packages"". export DV_PACKAGE_CURL_PATH=""https://storage.googleapis.com/deepvariant/packages"". # Set this to 1 to use the nightly (latest) build of TensorFlow instead of a. # named release version. Set it to an already existing value in the environment. # (allowing command line control of the build), defaulting to 0 (release build). # Note that setting this to 1 implies that the C++ code in DeepVariant will be. # build using the master branch and not the pinned version to avoid. # incompatibilities between TensorFlow C++ used to build DeepVariant and the. # tf-nightly wheel. export DV_TF_NIGHTLY_BUILD=""${DV_TF_NIGHTLY_BUILD:-1}"". # The branch/tag we checkout to build our C++ dependencies against. This is not. # the same as the python version of TensorFlow we use, but should be similar or. # we risk having version incompatibilities between our C++ code and the Python. # code we use at runtime. if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then. export DV_CPP_TENSORFLOW_TAG=""master"". else. export DV_CPP_TENSORFLOW_TAG=""r1.12"". fi. export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing. # value in the environment (allowing command line control of the build),. # defaulting to 0 (CPU only build). export DV_GPU_BUILD=""${DV_GPU_BUILD:-1}"". # If this variable is set to 1, DeepVariant will use a TensorFlow wheel file. # compiled with MKL support for corei7 or better chipsets, which. # significantly speeds up execution when running on modern CPUs. The default. # TensorFlow wheel files don't contain",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:5305,testability,control,control,5305,"). # Note that setting this to 1 implies that the C++ code in DeepVariant will be. # build using the master branch and not the pinned version to avoid. # incompatibilities between TensorFlow C++ used to build DeepVariant and the. # tf-nightly wheel. export DV_TF_NIGHTLY_BUILD=""${DV_TF_NIGHTLY_BUILD:-1}"". # The branch/tag we checkout to build our C++ dependencies against. This is not. # the same as the python version of TensorFlow we use, but should be similar or. # we risk having version incompatibilities between our C++ code and the Python. # code we use at runtime. if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then. export DV_CPP_TENSORFLOW_TAG=""master"". else. export DV_CPP_TENSORFLOW_TAG=""r1.12"". fi. export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing. # value in the environment (allowing command line control of the build),. # defaulting to 0 (CPU only build). export DV_GPU_BUILD=""${DV_GPU_BUILD:-1}"". # If this variable is set to 1, DeepVariant will use a TensorFlow wheel file. # compiled with MKL support for corei7 or better chipsets, which. # significantly speeds up execution when running on modern CPUs. The default. # TensorFlow wheel files don't contain these instructions (and thereby run on a. # broader set of CPUs). Using this optimized wheel reduces the runtime of. # DeepVariant's call_variants step by >3x. This is called the GCP (Google Cloud. # Platform) optimized wheel because all GCP instances have at least Sandy Bridge. # or better chipsets, so this wheel should run anywhere on GCP. export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}"". export GCP_OPTIMIZED_TF_WHL_FILENAME=""tensorflow-${DV_GCP_OPTIMIZED_TF_WHL_VERSION}.deepvariant_gcp-cp27-none-linux_x86_64.whl"". export GCP_OPTIMIZED_TF_WHL_PATH=""${DV_PACKAGE_BUCKET_PATH}/tensorflow"". export GCP_OPTIMIZED_TF",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:474,usability,tool,tools,474,"Cannot build deepvariant with tensorflow master due to bazel version requirements; With `DV_CPP_TENSORFLOW_TAG=master`, I believe deepvariant will not build from source. TF master does not build with Bazel=0.15.0:. ```. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/... [bazel INFO src/main/cpp/option_processor.cc:235] Looking for master bazelrcs in the following three paths: /root/deepvariant/tools/bazel.rc, , /etc/bazel.bazelrc. [bazel INFO src/main/cpp/option_processor.cc:165] User provided no rc file. [bazel INFO src/main/cpp/rc_file.cc:53] Parsing the RcFile /root/deepvariant/tools/bazel.rc. [bazel INFO src/main/cpp/rc_file.cc:53] Parsing the RcFile /root/deepvariant/../tensorflow/tools/bazel.rc. [bazel FATAL src/main/cpp/blaze.cc:1263] Unexpected error reading .blazerc file '/root/deepvariant/../tensorflow/tools/bazel.rc'. ```. And deepvariant does not build with bazel=0.19.0, see #134. ___. Linux xxx 4.15.0-1031-aws #33-Ubuntu SMP Fri Dec 7 09:32:27 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux. ```. # Copyright 2017 Google LLC. #. # Redistribution and use in source and binary forms, with or without. # modification, are permitted provided that the following conditions. # are met:. #. # 1. Redistributions of source code must retain the above copyright notice,. # this list of conditions and the following disclaimer. #. # 2. Redistributions in binary form must reproduce the above copyright. # notice, this list of conditions and the following disclaimer in the. # documentation and/or other materials provided with the distribution. #. # 3. Neither the name of the copyright holder nor the names of its. # contributors may be used to endorse or promote products derived from this. # software without specific prior written permission. #. # THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"". # AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE. # IMPLIE",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:562,usability,User,User,562,"Cannot build deepvariant with tensorflow master due to bazel version requirements; With `DV_CPP_TENSORFLOW_TAG=master`, I believe deepvariant will not build from source. TF master does not build with Bazel=0.15.0:. ```. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/... [bazel INFO src/main/cpp/option_processor.cc:235] Looking for master bazelrcs in the following three paths: /root/deepvariant/tools/bazel.rc, , /etc/bazel.bazelrc. [bazel INFO src/main/cpp/option_processor.cc:165] User provided no rc file. [bazel INFO src/main/cpp/rc_file.cc:53] Parsing the RcFile /root/deepvariant/tools/bazel.rc. [bazel INFO src/main/cpp/rc_file.cc:53] Parsing the RcFile /root/deepvariant/../tensorflow/tools/bazel.rc. [bazel FATAL src/main/cpp/blaze.cc:1263] Unexpected error reading .blazerc file '/root/deepvariant/../tensorflow/tools/bazel.rc'. ```. And deepvariant does not build with bazel=0.19.0, see #134. ___. Linux xxx 4.15.0-1031-aws #33-Ubuntu SMP Fri Dec 7 09:32:27 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux. ```. # Copyright 2017 Google LLC. #. # Redistribution and use in source and binary forms, with or without. # modification, are permitted provided that the following conditions. # are met:. #. # 1. Redistributions of source code must retain the above copyright notice,. # this list of conditions and the following disclaimer. #. # 2. Redistributions in binary form must reproduce the above copyright. # notice, this list of conditions and the following disclaimer in the. # documentation and/or other materials provided with the distribution. #. # 3. Neither the name of the copyright holder nor the names of its. # contributors may be used to endorse or promote products derived from this. # software without specific prior written permission. #. # THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"". # AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE. # IMPLIE",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:665,usability,tool,tools,665,"Cannot build deepvariant with tensorflow master due to bazel version requirements; With `DV_CPP_TENSORFLOW_TAG=master`, I believe deepvariant will not build from source. TF master does not build with Bazel=0.15.0:. ```. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/... [bazel INFO src/main/cpp/option_processor.cc:235] Looking for master bazelrcs in the following three paths: /root/deepvariant/tools/bazel.rc, , /etc/bazel.bazelrc. [bazel INFO src/main/cpp/option_processor.cc:165] User provided no rc file. [bazel INFO src/main/cpp/rc_file.cc:53] Parsing the RcFile /root/deepvariant/tools/bazel.rc. [bazel INFO src/main/cpp/rc_file.cc:53] Parsing the RcFile /root/deepvariant/../tensorflow/tools/bazel.rc. [bazel FATAL src/main/cpp/blaze.cc:1263] Unexpected error reading .blazerc file '/root/deepvariant/../tensorflow/tools/bazel.rc'. ```. And deepvariant does not build with bazel=0.19.0, see #134. ___. Linux xxx 4.15.0-1031-aws #33-Ubuntu SMP Fri Dec 7 09:32:27 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux. ```. # Copyright 2017 Google LLC. #. # Redistribution and use in source and binary forms, with or without. # modification, are permitted provided that the following conditions. # are met:. #. # 1. Redistributions of source code must retain the above copyright notice,. # this list of conditions and the following disclaimer. #. # 2. Redistributions in binary form must reproduce the above copyright. # notice, this list of conditions and the following disclaimer in the. # documentation and/or other materials provided with the distribution. #. # 3. Neither the name of the copyright holder nor the names of its. # contributors may be used to endorse or promote products derived from this. # software without specific prior written permission. #. # THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"". # AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE. # IMPLIE",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:772,usability,tool,tools,772,"Cannot build deepvariant with tensorflow master due to bazel version requirements; With `DV_CPP_TENSORFLOW_TAG=master`, I believe deepvariant will not build from source. TF master does not build with Bazel=0.15.0:. ```. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/... [bazel INFO src/main/cpp/option_processor.cc:235] Looking for master bazelrcs in the following three paths: /root/deepvariant/tools/bazel.rc, , /etc/bazel.bazelrc. [bazel INFO src/main/cpp/option_processor.cc:165] User provided no rc file. [bazel INFO src/main/cpp/rc_file.cc:53] Parsing the RcFile /root/deepvariant/tools/bazel.rc. [bazel INFO src/main/cpp/rc_file.cc:53] Parsing the RcFile /root/deepvariant/../tensorflow/tools/bazel.rc. [bazel FATAL src/main/cpp/blaze.cc:1263] Unexpected error reading .blazerc file '/root/deepvariant/../tensorflow/tools/bazel.rc'. ```. And deepvariant does not build with bazel=0.19.0, see #134. ___. Linux xxx 4.15.0-1031-aws #33-Ubuntu SMP Fri Dec 7 09:32:27 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux. ```. # Copyright 2017 Google LLC. #. # Redistribution and use in source and binary forms, with or without. # modification, are permitted provided that the following conditions. # are met:. #. # 1. Redistributions of source code must retain the above copyright notice,. # this list of conditions and the following disclaimer. #. # 2. Redistributions in binary form must reproduce the above copyright. # notice, this list of conditions and the following disclaimer in the. # documentation and/or other materials provided with the distribution. #. # 3. Neither the name of the copyright holder nor the names of its. # contributors may be used to endorse or promote products derived from this. # software without specific prior written permission. #. # THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"". # AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE. # IMPLIE",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:840,usability,error,error,840,"Cannot build deepvariant with tensorflow master due to bazel version requirements; With `DV_CPP_TENSORFLOW_TAG=master`, I believe deepvariant will not build from source. TF master does not build with Bazel=0.15.0:. ```. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/... [bazel INFO src/main/cpp/option_processor.cc:235] Looking for master bazelrcs in the following three paths: /root/deepvariant/tools/bazel.rc, , /etc/bazel.bazelrc. [bazel INFO src/main/cpp/option_processor.cc:165] User provided no rc file. [bazel INFO src/main/cpp/rc_file.cc:53] Parsing the RcFile /root/deepvariant/tools/bazel.rc. [bazel INFO src/main/cpp/rc_file.cc:53] Parsing the RcFile /root/deepvariant/../tensorflow/tools/bazel.rc. [bazel FATAL src/main/cpp/blaze.cc:1263] Unexpected error reading .blazerc file '/root/deepvariant/../tensorflow/tools/bazel.rc'. ```. And deepvariant does not build with bazel=0.19.0, see #134. ___. Linux xxx 4.15.0-1031-aws #33-Ubuntu SMP Fri Dec 7 09:32:27 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux. ```. # Copyright 2017 Google LLC. #. # Redistribution and use in source and binary forms, with or without. # modification, are permitted provided that the following conditions. # are met:. #. # 1. Redistributions of source code must retain the above copyright notice,. # this list of conditions and the following disclaimer. #. # 2. Redistributions in binary form must reproduce the above copyright. # notice, this list of conditions and the following disclaimer in the. # documentation and/or other materials provided with the distribution. #. # 3. Neither the name of the copyright holder nor the names of its. # contributors may be used to endorse or promote products derived from this. # software without specific prior written permission. #. # THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"". # AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE. # IMPLIE",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:901,usability,tool,tools,901,"Cannot build deepvariant with tensorflow master due to bazel version requirements; With `DV_CPP_TENSORFLOW_TAG=master`, I believe deepvariant will not build from source. TF master does not build with Bazel=0.15.0:. ```. + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/... [bazel INFO src/main/cpp/option_processor.cc:235] Looking for master bazelrcs in the following three paths: /root/deepvariant/tools/bazel.rc, , /etc/bazel.bazelrc. [bazel INFO src/main/cpp/option_processor.cc:165] User provided no rc file. [bazel INFO src/main/cpp/rc_file.cc:53] Parsing the RcFile /root/deepvariant/tools/bazel.rc. [bazel INFO src/main/cpp/rc_file.cc:53] Parsing the RcFile /root/deepvariant/../tensorflow/tools/bazel.rc. [bazel FATAL src/main/cpp/blaze.cc:1263] Unexpected error reading .blazerc file '/root/deepvariant/../tensorflow/tools/bazel.rc'. ```. And deepvariant does not build with bazel=0.19.0, see #134. ___. Linux xxx 4.15.0-1031-aws #33-Ubuntu SMP Fri Dec 7 09:32:27 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux. ```. # Copyright 2017 Google LLC. #. # Redistribution and use in source and binary forms, with or without. # modification, are permitted provided that the following conditions. # are met:. #. # 1. Redistributions of source code must retain the above copyright notice,. # this list of conditions and the following disclaimer. #. # 2. Redistributions in binary form must reproduce the above copyright. # notice, this list of conditions and the following disclaimer in the. # documentation and/or other materials provided with the distribution. #. # 3. Neither the name of the copyright holder nor the names of its. # contributors may be used to endorse or promote products derived from this. # software without specific prior written permission. #. # THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"". # AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE. # IMPLIE",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:1562,usability,document,documentation,1562,"ovided no rc file. [bazel INFO src/main/cpp/rc_file.cc:53] Parsing the RcFile /root/deepvariant/tools/bazel.rc. [bazel INFO src/main/cpp/rc_file.cc:53] Parsing the RcFile /root/deepvariant/../tensorflow/tools/bazel.rc. [bazel FATAL src/main/cpp/blaze.cc:1263] Unexpected error reading .blazerc file '/root/deepvariant/../tensorflow/tools/bazel.rc'. ```. And deepvariant does not build with bazel=0.19.0, see #134. ___. Linux xxx 4.15.0-1031-aws #33-Ubuntu SMP Fri Dec 7 09:32:27 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux. ```. # Copyright 2017 Google LLC. #. # Redistribution and use in source and binary forms, with or without. # modification, are permitted provided that the following conditions. # are met:. #. # 1. Redistributions of source code must retain the above copyright notice,. # this list of conditions and the following disclaimer. #. # 2. Redistributions in binary form must reproduce the above copyright. # notice, this list of conditions and the following disclaimer in the. # documentation and/or other materials provided with the distribution. #. # 3. Neither the name of the copyright holder nor the names of its. # contributors may be used to endorse or promote products derived from this. # software without specific prior written permission. #. # THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"". # AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE. # IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. # ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE. # LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR. # CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF. # SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS. # INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN. # CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE). # ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:3624,usability,user,user,3624,"Source this file---these options are needed for TF config and for. # successive bazel runs. # Set this to 1 if the system image already has TensorFlow preinstalled. This. # will skip the installation of TensorFlow. export DV_USE_PREINSTALLED_TF=""${DV_USE_PREINSTALLED_TF:-0}"". export TF_CUDA_CLANG=0. export TF_ENABLE_XLA=1. export TF_NEED_CUDA=1. export TF_NEED_GCP=1. export TF_NEED_GDR=0. export TF_NEED_HDFS=0. export TF_NEED_JEMALLOC=0. export TF_NEED_MKL=1. export TF_NEED_MPI=0. export TF_NEED_OPENCL=0. export TF_NEED_OPENCL_SYCL=0. export TF_NEED_S3=1. export TF_NEED_VERBS=0. # Used if TF_NEED_CUDA=1. export TF_CUDA_VERSION=""10.0"". export CUDA_TOOLKIT_PATH=""/usr/local/cuda"". export TF_CUDNN_VERSION=""7"". export CUDNN_INSTALL_PATH=""/usr/lib/x86_64-linux-gnu"". # The version of bazel we want to build DeepVariant. DV_BAZEL_VERSION=""0.15.0"". # We need to make sure that $HOME/bin is first in the binary search path so that. # `bazel` will find the latest version of bazel installed in the user's home. # directory. This is set in setting.sh as all DeepVariant scripts source. # settings.sh and assume that `bazel` will find the right version. export PATH=""$HOME/bin:$PATH"". # Path to the public bucket containing DeepVariant-related artifacts. export DEEPVARIANT_BUCKET=""gs://deepvariant"". export DV_PACKAGE_BUCKET_PATH=""${DEEPVARIANT_BUCKET}/packages"". export DV_PACKAGE_CURL_PATH=""https://storage.googleapis.com/deepvariant/packages"". # Set this to 1 to use the nightly (latest) build of TensorFlow instead of a. # named release version. Set it to an already existing value in the environment. # (allowing command line control of the build), defaulting to 0 (release build). # Note that setting this to 1 implies that the C++ code in DeepVariant will be. # build using the master branch and not the pinned version to avoid. # incompatibilities between TensorFlow C++ used to build DeepVariant and the. # tf-nightly wheel. export DV_TF_NIGHTLY_BUILD=""${DV_TF_NIGHTLY_BUILD:-1}"". # The branc",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:4243,usability,command,command,4243,"_CUDA_VERSION=""10.0"". export CUDA_TOOLKIT_PATH=""/usr/local/cuda"". export TF_CUDNN_VERSION=""7"". export CUDNN_INSTALL_PATH=""/usr/lib/x86_64-linux-gnu"". # The version of bazel we want to build DeepVariant. DV_BAZEL_VERSION=""0.15.0"". # We need to make sure that $HOME/bin is first in the binary search path so that. # `bazel` will find the latest version of bazel installed in the user's home. # directory. This is set in setting.sh as all DeepVariant scripts source. # settings.sh and assume that `bazel` will find the right version. export PATH=""$HOME/bin:$PATH"". # Path to the public bucket containing DeepVariant-related artifacts. export DEEPVARIANT_BUCKET=""gs://deepvariant"". export DV_PACKAGE_BUCKET_PATH=""${DEEPVARIANT_BUCKET}/packages"". export DV_PACKAGE_CURL_PATH=""https://storage.googleapis.com/deepvariant/packages"". # Set this to 1 to use the nightly (latest) build of TensorFlow instead of a. # named release version. Set it to an already existing value in the environment. # (allowing command line control of the build), defaulting to 0 (release build). # Note that setting this to 1 implies that the C++ code in DeepVariant will be. # build using the master branch and not the pinned version to avoid. # incompatibilities between TensorFlow C++ used to build DeepVariant and the. # tf-nightly wheel. export DV_TF_NIGHTLY_BUILD=""${DV_TF_NIGHTLY_BUILD:-1}"". # The branch/tag we checkout to build our C++ dependencies against. This is not. # the same as the python version of TensorFlow we use, but should be similar or. # we risk having version incompatibilities between our C++ code and the Python. # code we use at runtime. if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then. export DV_CPP_TENSORFLOW_TAG=""master"". else. export DV_CPP_TENSORFLOW_TAG=""r1.12"". fi. export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already ex",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:5292,usability,command,command,5292,"release build). # Note that setting this to 1 implies that the C++ code in DeepVariant will be. # build using the master branch and not the pinned version to avoid. # incompatibilities between TensorFlow C++ used to build DeepVariant and the. # tf-nightly wheel. export DV_TF_NIGHTLY_BUILD=""${DV_TF_NIGHTLY_BUILD:-1}"". # The branch/tag we checkout to build our C++ dependencies against. This is not. # the same as the python version of TensorFlow we use, but should be similar or. # we risk having version incompatibilities between our C++ code and the Python. # code we use at runtime. if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then. export DV_CPP_TENSORFLOW_TAG=""master"". else. export DV_CPP_TENSORFLOW_TAG=""r1.12"". fi. export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing. # value in the environment (allowing command line control of the build),. # defaulting to 0 (CPU only build). export DV_GPU_BUILD=""${DV_GPU_BUILD:-1}"". # If this variable is set to 1, DeepVariant will use a TensorFlow wheel file. # compiled with MKL support for corei7 or better chipsets, which. # significantly speeds up execution when running on modern CPUs. The default. # TensorFlow wheel files don't contain these instructions (and thereby run on a. # broader set of CPUs). Using this optimized wheel reduces the runtime of. # DeepVariant's call_variants step by >3x. This is called the GCP (Google Cloud. # Platform) optimized wheel because all GCP instances have at least Sandy Bridge. # or better chipsets, so this wheel should run anywhere on GCP. export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}"". export GCP_OPTIMIZED_TF_WHL_FILENAME=""tensorflow-${DV_GCP_OPTIMIZED_TF_WHL_VERSION}.deepvariant_gcp-cp27-none-linux_x86_64.whl"". export GCP_OPTIMIZED_TF_WHL_PATH=""${DV_PACKAGE_BUCKET_PATH}/tensorflow"". export GCP",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/145:5505,usability,support,support,5505,"to build DeepVariant and the. # tf-nightly wheel. export DV_TF_NIGHTLY_BUILD=""${DV_TF_NIGHTLY_BUILD:-1}"". # The branch/tag we checkout to build our C++ dependencies against. This is not. # the same as the python version of TensorFlow we use, but should be similar or. # we risk having version incompatibilities between our C++ code and the Python. # code we use at runtime. if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then. export DV_CPP_TENSORFLOW_TAG=""master"". else. export DV_CPP_TENSORFLOW_TAG=""r1.12"". fi. export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0"". export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing. # value in the environment (allowing command line control of the build),. # defaulting to 0 (CPU only build). export DV_GPU_BUILD=""${DV_GPU_BUILD:-1}"". # If this variable is set to 1, DeepVariant will use a TensorFlow wheel file. # compiled with MKL support for corei7 or better chipsets, which. # significantly speeds up execution when running on modern CPUs. The default. # TensorFlow wheel files don't contain these instructions (and thereby run on a. # broader set of CPUs). Using this optimized wheel reduces the runtime of. # DeepVariant's call_variants step by >3x. This is called the GCP (Google Cloud. # Platform) optimized wheel because all GCP instances have at least Sandy Bridge. # or better chipsets, so this wheel should run anywhere on GCP. export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}"". export GCP_OPTIMIZED_TF_WHL_FILENAME=""tensorflow-${DV_GCP_OPTIMIZED_TF_WHL_VERSION}.deepvariant_gcp-cp27-none-linux_x86_64.whl"". export GCP_OPTIMIZED_TF_WHL_PATH=""${DV_PACKAGE_BUCKET_PATH}/tensorflow"". export GCP_OPTIMIZED_TF_WHL_CURL_PATH=""${DV_PACKAGE_CURL_PATH}/tensorflow"". # Set this to 1 to make our prereq scripts install the CUDA libraries. # If you already have CUDA installed, such as on a properly provisioned. # D",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/145
https://github.com/google/deepvariant/issues/146:97,modifiability,pac,packages,97,CLIF binary for Ubuntu 18 does not exist; Should be: `https://storage.googleapis.com/deepvariant/packages/oss_clif.ubuntu-18.latest.tgz`. Not: `https://storage.googleapis.com/deepvariant/packages/oss_clif/oss_clif.ubuntu-18.latest.tgz`,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/146
https://github.com/google/deepvariant/issues/146:187,modifiability,pac,packages,187,CLIF binary for Ubuntu 18 does not exist; Should be: `https://storage.googleapis.com/deepvariant/packages/oss_clif.ubuntu-18.latest.tgz`. Not: `https://storage.googleapis.com/deepvariant/packages/oss_clif/oss_clif.ubuntu-18.latest.tgz`,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/146
https://github.com/google/deepvariant/issues/146:26,reliability,doe,does,26,CLIF binary for Ubuntu 18 does not exist; Should be: `https://storage.googleapis.com/deepvariant/packages/oss_clif.ubuntu-18.latest.tgz`. Not: `https://storage.googleapis.com/deepvariant/packages/oss_clif/oss_clif.ubuntu-18.latest.tgz`,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/146
https://github.com/google/deepvariant/issues/147:55,deployability,observ,observed,55,"Swelling memory usage in training DeepVariant?; Hi,. I observed that CPU memory usage in training DeepVariant is very high and increased by proceeding training. I used DeepVariant v0.7.1 via docker. The tested server (standalone, in house server) has 512GB memory with 56CPU, 8GB swap region without GPU/TPU. I followed your tutorials and made tfrecords including 241,896 pileup images from our data. I tested the following 3 patterns of training and observed increasing memory usage in all cases:. - When batch size=32,. - Training was successfully finished (total 50,000 steps). - Memory usage at the end of this training was 268GB. ![batch_size32 mem](https://user-images.githubusercontent.com/16730135/52196627-ddf14b00-289f-11e9-90dc-05eff144191d.jpg). - When batch size=64 (source code default),. - At ~16,000 steps, all memory (512 GB) was completely used. - At ~19,000 steps, all swap region was used (and killed). ![batch_size64 mem](https://user-images.githubusercontent.com/16730135/52196624-ddf14b00-289f-11e9-983a-5dcc3de48227.jpg). - When batch size=512 (tutorial default),. - At ~3,900 steps, all memory (512 GB) was completely used. - At ~4,200 steps, all swap region was used (and killed). ![batch_size512 mem](https://user-images.githubusercontent.com/16730135/52196626-ddf14b00-289f-11e9-8910-67a7c64b324d.jpg). Are these swelling memory usage due to tensorflow itself or some memory leak? I want to know how to prevent these. Best,. Masaru.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:451,deployability,observ,observed,451,"Swelling memory usage in training DeepVariant?; Hi,. I observed that CPU memory usage in training DeepVariant is very high and increased by proceeding training. I used DeepVariant v0.7.1 via docker. The tested server (standalone, in house server) has 512GB memory with 56CPU, 8GB swap region without GPU/TPU. I followed your tutorials and made tfrecords including 241,896 pileup images from our data. I tested the following 3 patterns of training and observed increasing memory usage in all cases:. - When batch size=32,. - Training was successfully finished (total 50,000 steps). - Memory usage at the end of this training was 268GB. ![batch_size32 mem](https://user-images.githubusercontent.com/16730135/52196627-ddf14b00-289f-11e9-90dc-05eff144191d.jpg). - When batch size=64 (source code default),. - At ~16,000 steps, all memory (512 GB) was completely used. - At ~19,000 steps, all swap region was used (and killed). ![batch_size64 mem](https://user-images.githubusercontent.com/16730135/52196624-ddf14b00-289f-11e9-983a-5dcc3de48227.jpg). - When batch size=512 (tutorial default),. - At ~3,900 steps, all memory (512 GB) was completely used. - At ~4,200 steps, all swap region was used (and killed). ![batch_size512 mem](https://user-images.githubusercontent.com/16730135/52196626-ddf14b00-289f-11e9-8910-67a7c64b324d.jpg). Are these swelling memory usage due to tensorflow itself or some memory leak? I want to know how to prevent these. Best,. Masaru.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:69,energy efficiency,CPU,CPU,69,"Swelling memory usage in training DeepVariant?; Hi,. I observed that CPU memory usage in training DeepVariant is very high and increased by proceeding training. I used DeepVariant v0.7.1 via docker. The tested server (standalone, in house server) has 512GB memory with 56CPU, 8GB swap region without GPU/TPU. I followed your tutorials and made tfrecords including 241,896 pileup images from our data. I tested the following 3 patterns of training and observed increasing memory usage in all cases:. - When batch size=32,. - Training was successfully finished (total 50,000 steps). - Memory usage at the end of this training was 268GB. ![batch_size32 mem](https://user-images.githubusercontent.com/16730135/52196627-ddf14b00-289f-11e9-90dc-05eff144191d.jpg). - When batch size=64 (source code default),. - At ~16,000 steps, all memory (512 GB) was completely used. - At ~19,000 steps, all swap region was used (and killed). ![batch_size64 mem](https://user-images.githubusercontent.com/16730135/52196624-ddf14b00-289f-11e9-983a-5dcc3de48227.jpg). - When batch size=512 (tutorial default),. - At ~3,900 steps, all memory (512 GB) was completely used. - At ~4,200 steps, all swap region was used (and killed). ![batch_size512 mem](https://user-images.githubusercontent.com/16730135/52196626-ddf14b00-289f-11e9-8910-67a7c64b324d.jpg). Are these swelling memory usage due to tensorflow itself or some memory leak? I want to know how to prevent these. Best,. Masaru.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:300,energy efficiency,GPU,GPU,300,"Swelling memory usage in training DeepVariant?; Hi,. I observed that CPU memory usage in training DeepVariant is very high and increased by proceeding training. I used DeepVariant v0.7.1 via docker. The tested server (standalone, in house server) has 512GB memory with 56CPU, 8GB swap region without GPU/TPU. I followed your tutorials and made tfrecords including 241,896 pileup images from our data. I tested the following 3 patterns of training and observed increasing memory usage in all cases:. - When batch size=32,. - Training was successfully finished (total 50,000 steps). - Memory usage at the end of this training was 268GB. ![batch_size32 mem](https://user-images.githubusercontent.com/16730135/52196627-ddf14b00-289f-11e9-90dc-05eff144191d.jpg). - When batch size=64 (source code default),. - At ~16,000 steps, all memory (512 GB) was completely used. - At ~19,000 steps, all swap region was used (and killed). ![batch_size64 mem](https://user-images.githubusercontent.com/16730135/52196624-ddf14b00-289f-11e9-983a-5dcc3de48227.jpg). - When batch size=512 (tutorial default),. - At ~3,900 steps, all memory (512 GB) was completely used. - At ~4,200 steps, all swap region was used (and killed). ![batch_size512 mem](https://user-images.githubusercontent.com/16730135/52196626-ddf14b00-289f-11e9-8910-67a7c64b324d.jpg). Are these swelling memory usage due to tensorflow itself or some memory leak? I want to know how to prevent these. Best,. Masaru.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:506,integrability,batch,batch,506,"Swelling memory usage in training DeepVariant?; Hi,. I observed that CPU memory usage in training DeepVariant is very high and increased by proceeding training. I used DeepVariant v0.7.1 via docker. The tested server (standalone, in house server) has 512GB memory with 56CPU, 8GB swap region without GPU/TPU. I followed your tutorials and made tfrecords including 241,896 pileup images from our data. I tested the following 3 patterns of training and observed increasing memory usage in all cases:. - When batch size=32,. - Training was successfully finished (total 50,000 steps). - Memory usage at the end of this training was 268GB. ![batch_size32 mem](https://user-images.githubusercontent.com/16730135/52196627-ddf14b00-289f-11e9-90dc-05eff144191d.jpg). - When batch size=64 (source code default),. - At ~16,000 steps, all memory (512 GB) was completely used. - At ~19,000 steps, all swap region was used (and killed). ![batch_size64 mem](https://user-images.githubusercontent.com/16730135/52196624-ddf14b00-289f-11e9-983a-5dcc3de48227.jpg). - When batch size=512 (tutorial default),. - At ~3,900 steps, all memory (512 GB) was completely used. - At ~4,200 steps, all swap region was used (and killed). ![batch_size512 mem](https://user-images.githubusercontent.com/16730135/52196626-ddf14b00-289f-11e9-8910-67a7c64b324d.jpg). Are these swelling memory usage due to tensorflow itself or some memory leak? I want to know how to prevent these. Best,. Masaru.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:767,integrability,batch,batch,767,"Swelling memory usage in training DeepVariant?; Hi,. I observed that CPU memory usage in training DeepVariant is very high and increased by proceeding training. I used DeepVariant v0.7.1 via docker. The tested server (standalone, in house server) has 512GB memory with 56CPU, 8GB swap region without GPU/TPU. I followed your tutorials and made tfrecords including 241,896 pileup images from our data. I tested the following 3 patterns of training and observed increasing memory usage in all cases:. - When batch size=32,. - Training was successfully finished (total 50,000 steps). - Memory usage at the end of this training was 268GB. ![batch_size32 mem](https://user-images.githubusercontent.com/16730135/52196627-ddf14b00-289f-11e9-90dc-05eff144191d.jpg). - When batch size=64 (source code default),. - At ~16,000 steps, all memory (512 GB) was completely used. - At ~19,000 steps, all swap region was used (and killed). ![batch_size64 mem](https://user-images.githubusercontent.com/16730135/52196624-ddf14b00-289f-11e9-983a-5dcc3de48227.jpg). - When batch size=512 (tutorial default),. - At ~3,900 steps, all memory (512 GB) was completely used. - At ~4,200 steps, all swap region was used (and killed). ![batch_size512 mem](https://user-images.githubusercontent.com/16730135/52196626-ddf14b00-289f-11e9-8910-67a7c64b324d.jpg). Are these swelling memory usage due to tensorflow itself or some memory leak? I want to know how to prevent these. Best,. Masaru.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:1057,integrability,batch,batch,1057,"Swelling memory usage in training DeepVariant?; Hi,. I observed that CPU memory usage in training DeepVariant is very high and increased by proceeding training. I used DeepVariant v0.7.1 via docker. The tested server (standalone, in house server) has 512GB memory with 56CPU, 8GB swap region without GPU/TPU. I followed your tutorials and made tfrecords including 241,896 pileup images from our data. I tested the following 3 patterns of training and observed increasing memory usage in all cases:. - When batch size=32,. - Training was successfully finished (total 50,000 steps). - Memory usage at the end of this training was 268GB. ![batch_size32 mem](https://user-images.githubusercontent.com/16730135/52196627-ddf14b00-289f-11e9-90dc-05eff144191d.jpg). - When batch size=64 (source code default),. - At ~16,000 steps, all memory (512 GB) was completely used. - At ~19,000 steps, all swap region was used (and killed). ![batch_size64 mem](https://user-images.githubusercontent.com/16730135/52196624-ddf14b00-289f-11e9-983a-5dcc3de48227.jpg). - When batch size=512 (tutorial default),. - At ~3,900 steps, all memory (512 GB) was completely used. - At ~4,200 steps, all swap region was used (and killed). ![batch_size512 mem](https://user-images.githubusercontent.com/16730135/52196626-ddf14b00-289f-11e9-8910-67a7c64b324d.jpg). Are these swelling memory usage due to tensorflow itself or some memory leak? I want to know how to prevent these. Best,. Masaru.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:9,performance,memor,memory,9,"Swelling memory usage in training DeepVariant?; Hi,. I observed that CPU memory usage in training DeepVariant is very high and increased by proceeding training. I used DeepVariant v0.7.1 via docker. The tested server (standalone, in house server) has 512GB memory with 56CPU, 8GB swap region without GPU/TPU. I followed your tutorials and made tfrecords including 241,896 pileup images from our data. I tested the following 3 patterns of training and observed increasing memory usage in all cases:. - When batch size=32,. - Training was successfully finished (total 50,000 steps). - Memory usage at the end of this training was 268GB. ![batch_size32 mem](https://user-images.githubusercontent.com/16730135/52196627-ddf14b00-289f-11e9-90dc-05eff144191d.jpg). - When batch size=64 (source code default),. - At ~16,000 steps, all memory (512 GB) was completely used. - At ~19,000 steps, all swap region was used (and killed). ![batch_size64 mem](https://user-images.githubusercontent.com/16730135/52196624-ddf14b00-289f-11e9-983a-5dcc3de48227.jpg). - When batch size=512 (tutorial default),. - At ~3,900 steps, all memory (512 GB) was completely used. - At ~4,200 steps, all swap region was used (and killed). ![batch_size512 mem](https://user-images.githubusercontent.com/16730135/52196626-ddf14b00-289f-11e9-8910-67a7c64b324d.jpg). Are these swelling memory usage due to tensorflow itself or some memory leak? I want to know how to prevent these. Best,. Masaru.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:69,performance,CPU,CPU,69,"Swelling memory usage in training DeepVariant?; Hi,. I observed that CPU memory usage in training DeepVariant is very high and increased by proceeding training. I used DeepVariant v0.7.1 via docker. The tested server (standalone, in house server) has 512GB memory with 56CPU, 8GB swap region without GPU/TPU. I followed your tutorials and made tfrecords including 241,896 pileup images from our data. I tested the following 3 patterns of training and observed increasing memory usage in all cases:. - When batch size=32,. - Training was successfully finished (total 50,000 steps). - Memory usage at the end of this training was 268GB. ![batch_size32 mem](https://user-images.githubusercontent.com/16730135/52196627-ddf14b00-289f-11e9-90dc-05eff144191d.jpg). - When batch size=64 (source code default),. - At ~16,000 steps, all memory (512 GB) was completely used. - At ~19,000 steps, all swap region was used (and killed). ![batch_size64 mem](https://user-images.githubusercontent.com/16730135/52196624-ddf14b00-289f-11e9-983a-5dcc3de48227.jpg). - When batch size=512 (tutorial default),. - At ~3,900 steps, all memory (512 GB) was completely used. - At ~4,200 steps, all swap region was used (and killed). ![batch_size512 mem](https://user-images.githubusercontent.com/16730135/52196626-ddf14b00-289f-11e9-8910-67a7c64b324d.jpg). Are these swelling memory usage due to tensorflow itself or some memory leak? I want to know how to prevent these. Best,. Masaru.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:73,performance,memor,memory,73,"Swelling memory usage in training DeepVariant?; Hi,. I observed that CPU memory usage in training DeepVariant is very high and increased by proceeding training. I used DeepVariant v0.7.1 via docker. The tested server (standalone, in house server) has 512GB memory with 56CPU, 8GB swap region without GPU/TPU. I followed your tutorials and made tfrecords including 241,896 pileup images from our data. I tested the following 3 patterns of training and observed increasing memory usage in all cases:. - When batch size=32,. - Training was successfully finished (total 50,000 steps). - Memory usage at the end of this training was 268GB. ![batch_size32 mem](https://user-images.githubusercontent.com/16730135/52196627-ddf14b00-289f-11e9-90dc-05eff144191d.jpg). - When batch size=64 (source code default),. - At ~16,000 steps, all memory (512 GB) was completely used. - At ~19,000 steps, all swap region was used (and killed). ![batch_size64 mem](https://user-images.githubusercontent.com/16730135/52196624-ddf14b00-289f-11e9-983a-5dcc3de48227.jpg). - When batch size=512 (tutorial default),. - At ~3,900 steps, all memory (512 GB) was completely used. - At ~4,200 steps, all swap region was used (and killed). ![batch_size512 mem](https://user-images.githubusercontent.com/16730135/52196626-ddf14b00-289f-11e9-8910-67a7c64b324d.jpg). Are these swelling memory usage due to tensorflow itself or some memory leak? I want to know how to prevent these. Best,. Masaru.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:257,performance,memor,memory,257,"Swelling memory usage in training DeepVariant?; Hi,. I observed that CPU memory usage in training DeepVariant is very high and increased by proceeding training. I used DeepVariant v0.7.1 via docker. The tested server (standalone, in house server) has 512GB memory with 56CPU, 8GB swap region without GPU/TPU. I followed your tutorials and made tfrecords including 241,896 pileup images from our data. I tested the following 3 patterns of training and observed increasing memory usage in all cases:. - When batch size=32,. - Training was successfully finished (total 50,000 steps). - Memory usage at the end of this training was 268GB. ![batch_size32 mem](https://user-images.githubusercontent.com/16730135/52196627-ddf14b00-289f-11e9-90dc-05eff144191d.jpg). - When batch size=64 (source code default),. - At ~16,000 steps, all memory (512 GB) was completely used. - At ~19,000 steps, all swap region was used (and killed). ![batch_size64 mem](https://user-images.githubusercontent.com/16730135/52196624-ddf14b00-289f-11e9-983a-5dcc3de48227.jpg). - When batch size=512 (tutorial default),. - At ~3,900 steps, all memory (512 GB) was completely used. - At ~4,200 steps, all swap region was used (and killed). ![batch_size512 mem](https://user-images.githubusercontent.com/16730135/52196626-ddf14b00-289f-11e9-8910-67a7c64b324d.jpg). Are these swelling memory usage due to tensorflow itself or some memory leak? I want to know how to prevent these. Best,. Masaru.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:300,performance,GPU,GPU,300,"Swelling memory usage in training DeepVariant?; Hi,. I observed that CPU memory usage in training DeepVariant is very high and increased by proceeding training. I used DeepVariant v0.7.1 via docker. The tested server (standalone, in house server) has 512GB memory with 56CPU, 8GB swap region without GPU/TPU. I followed your tutorials and made tfrecords including 241,896 pileup images from our data. I tested the following 3 patterns of training and observed increasing memory usage in all cases:. - When batch size=32,. - Training was successfully finished (total 50,000 steps). - Memory usage at the end of this training was 268GB. ![batch_size32 mem](https://user-images.githubusercontent.com/16730135/52196627-ddf14b00-289f-11e9-90dc-05eff144191d.jpg). - When batch size=64 (source code default),. - At ~16,000 steps, all memory (512 GB) was completely used. - At ~19,000 steps, all swap region was used (and killed). ![batch_size64 mem](https://user-images.githubusercontent.com/16730135/52196624-ddf14b00-289f-11e9-983a-5dcc3de48227.jpg). - When batch size=512 (tutorial default),. - At ~3,900 steps, all memory (512 GB) was completely used. - At ~4,200 steps, all swap region was used (and killed). ![batch_size512 mem](https://user-images.githubusercontent.com/16730135/52196626-ddf14b00-289f-11e9-8910-67a7c64b324d.jpg). Are these swelling memory usage due to tensorflow itself or some memory leak? I want to know how to prevent these. Best,. Masaru.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:471,performance,memor,memory,471,"Swelling memory usage in training DeepVariant?; Hi,. I observed that CPU memory usage in training DeepVariant is very high and increased by proceeding training. I used DeepVariant v0.7.1 via docker. The tested server (standalone, in house server) has 512GB memory with 56CPU, 8GB swap region without GPU/TPU. I followed your tutorials and made tfrecords including 241,896 pileup images from our data. I tested the following 3 patterns of training and observed increasing memory usage in all cases:. - When batch size=32,. - Training was successfully finished (total 50,000 steps). - Memory usage at the end of this training was 268GB. ![batch_size32 mem](https://user-images.githubusercontent.com/16730135/52196627-ddf14b00-289f-11e9-90dc-05eff144191d.jpg). - When batch size=64 (source code default),. - At ~16,000 steps, all memory (512 GB) was completely used. - At ~19,000 steps, all swap region was used (and killed). ![batch_size64 mem](https://user-images.githubusercontent.com/16730135/52196624-ddf14b00-289f-11e9-983a-5dcc3de48227.jpg). - When batch size=512 (tutorial default),. - At ~3,900 steps, all memory (512 GB) was completely used. - At ~4,200 steps, all swap region was used (and killed). ![batch_size512 mem](https://user-images.githubusercontent.com/16730135/52196626-ddf14b00-289f-11e9-8910-67a7c64b324d.jpg). Are these swelling memory usage due to tensorflow itself or some memory leak? I want to know how to prevent these. Best,. Masaru.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:506,performance,batch,batch,506,"Swelling memory usage in training DeepVariant?; Hi,. I observed that CPU memory usage in training DeepVariant is very high and increased by proceeding training. I used DeepVariant v0.7.1 via docker. The tested server (standalone, in house server) has 512GB memory with 56CPU, 8GB swap region without GPU/TPU. I followed your tutorials and made tfrecords including 241,896 pileup images from our data. I tested the following 3 patterns of training and observed increasing memory usage in all cases:. - When batch size=32,. - Training was successfully finished (total 50,000 steps). - Memory usage at the end of this training was 268GB. ![batch_size32 mem](https://user-images.githubusercontent.com/16730135/52196627-ddf14b00-289f-11e9-90dc-05eff144191d.jpg). - When batch size=64 (source code default),. - At ~16,000 steps, all memory (512 GB) was completely used. - At ~19,000 steps, all swap region was used (and killed). ![batch_size64 mem](https://user-images.githubusercontent.com/16730135/52196624-ddf14b00-289f-11e9-983a-5dcc3de48227.jpg). - When batch size=512 (tutorial default),. - At ~3,900 steps, all memory (512 GB) was completely used. - At ~4,200 steps, all swap region was used (and killed). ![batch_size512 mem](https://user-images.githubusercontent.com/16730135/52196626-ddf14b00-289f-11e9-8910-67a7c64b324d.jpg). Are these swelling memory usage due to tensorflow itself or some memory leak? I want to know how to prevent these. Best,. Masaru.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:585,performance,Memor,Memory,585,"Swelling memory usage in training DeepVariant?; Hi,. I observed that CPU memory usage in training DeepVariant is very high and increased by proceeding training. I used DeepVariant v0.7.1 via docker. The tested server (standalone, in house server) has 512GB memory with 56CPU, 8GB swap region without GPU/TPU. I followed your tutorials and made tfrecords including 241,896 pileup images from our data. I tested the following 3 patterns of training and observed increasing memory usage in all cases:. - When batch size=32,. - Training was successfully finished (total 50,000 steps). - Memory usage at the end of this training was 268GB. ![batch_size32 mem](https://user-images.githubusercontent.com/16730135/52196627-ddf14b00-289f-11e9-90dc-05eff144191d.jpg). - When batch size=64 (source code default),. - At ~16,000 steps, all memory (512 GB) was completely used. - At ~19,000 steps, all swap region was used (and killed). ![batch_size64 mem](https://user-images.githubusercontent.com/16730135/52196624-ddf14b00-289f-11e9-983a-5dcc3de48227.jpg). - When batch size=512 (tutorial default),. - At ~3,900 steps, all memory (512 GB) was completely used. - At ~4,200 steps, all swap region was used (and killed). ![batch_size512 mem](https://user-images.githubusercontent.com/16730135/52196626-ddf14b00-289f-11e9-8910-67a7c64b324d.jpg). Are these swelling memory usage due to tensorflow itself or some memory leak? I want to know how to prevent these. Best,. Masaru.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:767,performance,batch,batch,767,"Swelling memory usage in training DeepVariant?; Hi,. I observed that CPU memory usage in training DeepVariant is very high and increased by proceeding training. I used DeepVariant v0.7.1 via docker. The tested server (standalone, in house server) has 512GB memory with 56CPU, 8GB swap region without GPU/TPU. I followed your tutorials and made tfrecords including 241,896 pileup images from our data. I tested the following 3 patterns of training and observed increasing memory usage in all cases:. - When batch size=32,. - Training was successfully finished (total 50,000 steps). - Memory usage at the end of this training was 268GB. ![batch_size32 mem](https://user-images.githubusercontent.com/16730135/52196627-ddf14b00-289f-11e9-90dc-05eff144191d.jpg). - When batch size=64 (source code default),. - At ~16,000 steps, all memory (512 GB) was completely used. - At ~19,000 steps, all swap region was used (and killed). ![batch_size64 mem](https://user-images.githubusercontent.com/16730135/52196624-ddf14b00-289f-11e9-983a-5dcc3de48227.jpg). - When batch size=512 (tutorial default),. - At ~3,900 steps, all memory (512 GB) was completely used. - At ~4,200 steps, all swap region was used (and killed). ![batch_size512 mem](https://user-images.githubusercontent.com/16730135/52196626-ddf14b00-289f-11e9-8910-67a7c64b324d.jpg). Are these swelling memory usage due to tensorflow itself or some memory leak? I want to know how to prevent these. Best,. Masaru.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:830,performance,memor,memory,830,"Swelling memory usage in training DeepVariant?; Hi,. I observed that CPU memory usage in training DeepVariant is very high and increased by proceeding training. I used DeepVariant v0.7.1 via docker. The tested server (standalone, in house server) has 512GB memory with 56CPU, 8GB swap region without GPU/TPU. I followed your tutorials and made tfrecords including 241,896 pileup images from our data. I tested the following 3 patterns of training and observed increasing memory usage in all cases:. - When batch size=32,. - Training was successfully finished (total 50,000 steps). - Memory usage at the end of this training was 268GB. ![batch_size32 mem](https://user-images.githubusercontent.com/16730135/52196627-ddf14b00-289f-11e9-90dc-05eff144191d.jpg). - When batch size=64 (source code default),. - At ~16,000 steps, all memory (512 GB) was completely used. - At ~19,000 steps, all swap region was used (and killed). ![batch_size64 mem](https://user-images.githubusercontent.com/16730135/52196624-ddf14b00-289f-11e9-983a-5dcc3de48227.jpg). - When batch size=512 (tutorial default),. - At ~3,900 steps, all memory (512 GB) was completely used. - At ~4,200 steps, all swap region was used (and killed). ![batch_size512 mem](https://user-images.githubusercontent.com/16730135/52196626-ddf14b00-289f-11e9-8910-67a7c64b324d.jpg). Are these swelling memory usage due to tensorflow itself or some memory leak? I want to know how to prevent these. Best,. Masaru.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:1057,performance,batch,batch,1057,"Swelling memory usage in training DeepVariant?; Hi,. I observed that CPU memory usage in training DeepVariant is very high and increased by proceeding training. I used DeepVariant v0.7.1 via docker. The tested server (standalone, in house server) has 512GB memory with 56CPU, 8GB swap region without GPU/TPU. I followed your tutorials and made tfrecords including 241,896 pileup images from our data. I tested the following 3 patterns of training and observed increasing memory usage in all cases:. - When batch size=32,. - Training was successfully finished (total 50,000 steps). - Memory usage at the end of this training was 268GB. ![batch_size32 mem](https://user-images.githubusercontent.com/16730135/52196627-ddf14b00-289f-11e9-90dc-05eff144191d.jpg). - When batch size=64 (source code default),. - At ~16,000 steps, all memory (512 GB) was completely used. - At ~19,000 steps, all swap region was used (and killed). ![batch_size64 mem](https://user-images.githubusercontent.com/16730135/52196624-ddf14b00-289f-11e9-983a-5dcc3de48227.jpg). - When batch size=512 (tutorial default),. - At ~3,900 steps, all memory (512 GB) was completely used. - At ~4,200 steps, all swap region was used (and killed). ![batch_size512 mem](https://user-images.githubusercontent.com/16730135/52196626-ddf14b00-289f-11e9-8910-67a7c64b324d.jpg). Are these swelling memory usage due to tensorflow itself or some memory leak? I want to know how to prevent these. Best,. Masaru.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:1117,performance,memor,memory,1117,"Swelling memory usage in training DeepVariant?; Hi,. I observed that CPU memory usage in training DeepVariant is very high and increased by proceeding training. I used DeepVariant v0.7.1 via docker. The tested server (standalone, in house server) has 512GB memory with 56CPU, 8GB swap region without GPU/TPU. I followed your tutorials and made tfrecords including 241,896 pileup images from our data. I tested the following 3 patterns of training and observed increasing memory usage in all cases:. - When batch size=32,. - Training was successfully finished (total 50,000 steps). - Memory usage at the end of this training was 268GB. ![batch_size32 mem](https://user-images.githubusercontent.com/16730135/52196627-ddf14b00-289f-11e9-90dc-05eff144191d.jpg). - When batch size=64 (source code default),. - At ~16,000 steps, all memory (512 GB) was completely used. - At ~19,000 steps, all swap region was used (and killed). ![batch_size64 mem](https://user-images.githubusercontent.com/16730135/52196624-ddf14b00-289f-11e9-983a-5dcc3de48227.jpg). - When batch size=512 (tutorial default),. - At ~3,900 steps, all memory (512 GB) was completely used. - At ~4,200 steps, all swap region was used (and killed). ![batch_size512 mem](https://user-images.githubusercontent.com/16730135/52196626-ddf14b00-289f-11e9-8910-67a7c64b324d.jpg). Are these swelling memory usage due to tensorflow itself or some memory leak? I want to know how to prevent these. Best,. Masaru.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:1356,performance,memor,memory,1356,"Swelling memory usage in training DeepVariant?; Hi,. I observed that CPU memory usage in training DeepVariant is very high and increased by proceeding training. I used DeepVariant v0.7.1 via docker. The tested server (standalone, in house server) has 512GB memory with 56CPU, 8GB swap region without GPU/TPU. I followed your tutorials and made tfrecords including 241,896 pileup images from our data. I tested the following 3 patterns of training and observed increasing memory usage in all cases:. - When batch size=32,. - Training was successfully finished (total 50,000 steps). - Memory usage at the end of this training was 268GB. ![batch_size32 mem](https://user-images.githubusercontent.com/16730135/52196627-ddf14b00-289f-11e9-90dc-05eff144191d.jpg). - When batch size=64 (source code default),. - At ~16,000 steps, all memory (512 GB) was completely used. - At ~19,000 steps, all swap region was used (and killed). ![batch_size64 mem](https://user-images.githubusercontent.com/16730135/52196624-ddf14b00-289f-11e9-983a-5dcc3de48227.jpg). - When batch size=512 (tutorial default),. - At ~3,900 steps, all memory (512 GB) was completely used. - At ~4,200 steps, all swap region was used (and killed). ![batch_size512 mem](https://user-images.githubusercontent.com/16730135/52196626-ddf14b00-289f-11e9-8910-67a7c64b324d.jpg). Are these swelling memory usage due to tensorflow itself or some memory leak? I want to know how to prevent these. Best,. Masaru.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:1402,performance,memor,memory,1402,"Swelling memory usage in training DeepVariant?; Hi,. I observed that CPU memory usage in training DeepVariant is very high and increased by proceeding training. I used DeepVariant v0.7.1 via docker. The tested server (standalone, in house server) has 512GB memory with 56CPU, 8GB swap region without GPU/TPU. I followed your tutorials and made tfrecords including 241,896 pileup images from our data. I tested the following 3 patterns of training and observed increasing memory usage in all cases:. - When batch size=32,. - Training was successfully finished (total 50,000 steps). - Memory usage at the end of this training was 268GB. ![batch_size32 mem](https://user-images.githubusercontent.com/16730135/52196627-ddf14b00-289f-11e9-90dc-05eff144191d.jpg). - When batch size=64 (source code default),. - At ~16,000 steps, all memory (512 GB) was completely used. - At ~19,000 steps, all swap region was used (and killed). ![batch_size64 mem](https://user-images.githubusercontent.com/16730135/52196624-ddf14b00-289f-11e9-983a-5dcc3de48227.jpg). - When batch size=512 (tutorial default),. - At ~3,900 steps, all memory (512 GB) was completely used. - At ~4,200 steps, all swap region was used (and killed). ![batch_size512 mem](https://user-images.githubusercontent.com/16730135/52196626-ddf14b00-289f-11e9-8910-67a7c64b324d.jpg). Are these swelling memory usage due to tensorflow itself or some memory leak? I want to know how to prevent these. Best,. Masaru.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:203,safety,test,tested,203,"Swelling memory usage in training DeepVariant?; Hi,. I observed that CPU memory usage in training DeepVariant is very high and increased by proceeding training. I used DeepVariant v0.7.1 via docker. The tested server (standalone, in house server) has 512GB memory with 56CPU, 8GB swap region without GPU/TPU. I followed your tutorials and made tfrecords including 241,896 pileup images from our data. I tested the following 3 patterns of training and observed increasing memory usage in all cases:. - When batch size=32,. - Training was successfully finished (total 50,000 steps). - Memory usage at the end of this training was 268GB. ![batch_size32 mem](https://user-images.githubusercontent.com/16730135/52196627-ddf14b00-289f-11e9-90dc-05eff144191d.jpg). - When batch size=64 (source code default),. - At ~16,000 steps, all memory (512 GB) was completely used. - At ~19,000 steps, all swap region was used (and killed). ![batch_size64 mem](https://user-images.githubusercontent.com/16730135/52196624-ddf14b00-289f-11e9-983a-5dcc3de48227.jpg). - When batch size=512 (tutorial default),. - At ~3,900 steps, all memory (512 GB) was completely used. - At ~4,200 steps, all swap region was used (and killed). ![batch_size512 mem](https://user-images.githubusercontent.com/16730135/52196626-ddf14b00-289f-11e9-8910-67a7c64b324d.jpg). Are these swelling memory usage due to tensorflow itself or some memory leak? I want to know how to prevent these. Best,. Masaru.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:403,safety,test,tested,403,"Swelling memory usage in training DeepVariant?; Hi,. I observed that CPU memory usage in training DeepVariant is very high and increased by proceeding training. I used DeepVariant v0.7.1 via docker. The tested server (standalone, in house server) has 512GB memory with 56CPU, 8GB swap region without GPU/TPU. I followed your tutorials and made tfrecords including 241,896 pileup images from our data. I tested the following 3 patterns of training and observed increasing memory usage in all cases:. - When batch size=32,. - Training was successfully finished (total 50,000 steps). - Memory usage at the end of this training was 268GB. ![batch_size32 mem](https://user-images.githubusercontent.com/16730135/52196627-ddf14b00-289f-11e9-90dc-05eff144191d.jpg). - When batch size=64 (source code default),. - At ~16,000 steps, all memory (512 GB) was completely used. - At ~19,000 steps, all swap region was used (and killed). ![batch_size64 mem](https://user-images.githubusercontent.com/16730135/52196624-ddf14b00-289f-11e9-983a-5dcc3de48227.jpg). - When batch size=512 (tutorial default),. - At ~3,900 steps, all memory (512 GB) was completely used. - At ~4,200 steps, all swap region was used (and killed). ![batch_size512 mem](https://user-images.githubusercontent.com/16730135/52196626-ddf14b00-289f-11e9-8910-67a7c64b324d.jpg). Are these swelling memory usage due to tensorflow itself or some memory leak? I want to know how to prevent these. Best,. Masaru.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:850,safety,compl,completely,850,"Swelling memory usage in training DeepVariant?; Hi,. I observed that CPU memory usage in training DeepVariant is very high and increased by proceeding training. I used DeepVariant v0.7.1 via docker. The tested server (standalone, in house server) has 512GB memory with 56CPU, 8GB swap region without GPU/TPU. I followed your tutorials and made tfrecords including 241,896 pileup images from our data. I tested the following 3 patterns of training and observed increasing memory usage in all cases:. - When batch size=32,. - Training was successfully finished (total 50,000 steps). - Memory usage at the end of this training was 268GB. ![batch_size32 mem](https://user-images.githubusercontent.com/16730135/52196627-ddf14b00-289f-11e9-90dc-05eff144191d.jpg). - When batch size=64 (source code default),. - At ~16,000 steps, all memory (512 GB) was completely used. - At ~19,000 steps, all swap region was used (and killed). ![batch_size64 mem](https://user-images.githubusercontent.com/16730135/52196624-ddf14b00-289f-11e9-983a-5dcc3de48227.jpg). - When batch size=512 (tutorial default),. - At ~3,900 steps, all memory (512 GB) was completely used. - At ~4,200 steps, all swap region was used (and killed). ![batch_size512 mem](https://user-images.githubusercontent.com/16730135/52196626-ddf14b00-289f-11e9-8910-67a7c64b324d.jpg). Are these swelling memory usage due to tensorflow itself or some memory leak? I want to know how to prevent these. Best,. Masaru.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:1137,safety,compl,completely,1137,"Swelling memory usage in training DeepVariant?; Hi,. I observed that CPU memory usage in training DeepVariant is very high and increased by proceeding training. I used DeepVariant v0.7.1 via docker. The tested server (standalone, in house server) has 512GB memory with 56CPU, 8GB swap region without GPU/TPU. I followed your tutorials and made tfrecords including 241,896 pileup images from our data. I tested the following 3 patterns of training and observed increasing memory usage in all cases:. - When batch size=32,. - Training was successfully finished (total 50,000 steps). - Memory usage at the end of this training was 268GB. ![batch_size32 mem](https://user-images.githubusercontent.com/16730135/52196627-ddf14b00-289f-11e9-90dc-05eff144191d.jpg). - When batch size=64 (source code default),. - At ~16,000 steps, all memory (512 GB) was completely used. - At ~19,000 steps, all swap region was used (and killed). ![batch_size64 mem](https://user-images.githubusercontent.com/16730135/52196624-ddf14b00-289f-11e9-983a-5dcc3de48227.jpg). - When batch size=512 (tutorial default),. - At ~3,900 steps, all memory (512 GB) was completely used. - At ~4,200 steps, all swap region was used (and killed). ![batch_size512 mem](https://user-images.githubusercontent.com/16730135/52196626-ddf14b00-289f-11e9-8910-67a7c64b324d.jpg). Are these swelling memory usage due to tensorflow itself or some memory leak? I want to know how to prevent these. Best,. Masaru.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:1437,safety,prevent,prevent,1437,"Swelling memory usage in training DeepVariant?; Hi,. I observed that CPU memory usage in training DeepVariant is very high and increased by proceeding training. I used DeepVariant v0.7.1 via docker. The tested server (standalone, in house server) has 512GB memory with 56CPU, 8GB swap region without GPU/TPU. I followed your tutorials and made tfrecords including 241,896 pileup images from our data. I tested the following 3 patterns of training and observed increasing memory usage in all cases:. - When batch size=32,. - Training was successfully finished (total 50,000 steps). - Memory usage at the end of this training was 268GB. ![batch_size32 mem](https://user-images.githubusercontent.com/16730135/52196627-ddf14b00-289f-11e9-90dc-05eff144191d.jpg). - When batch size=64 (source code default),. - At ~16,000 steps, all memory (512 GB) was completely used. - At ~19,000 steps, all swap region was used (and killed). ![batch_size64 mem](https://user-images.githubusercontent.com/16730135/52196624-ddf14b00-289f-11e9-983a-5dcc3de48227.jpg). - When batch size=512 (tutorial default),. - At ~3,900 steps, all memory (512 GB) was completely used. - At ~4,200 steps, all swap region was used (and killed). ![batch_size512 mem](https://user-images.githubusercontent.com/16730135/52196626-ddf14b00-289f-11e9-8910-67a7c64b324d.jpg). Are these swelling memory usage due to tensorflow itself or some memory leak? I want to know how to prevent these. Best,. Masaru.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:850,security,compl,completely,850,"Swelling memory usage in training DeepVariant?; Hi,. I observed that CPU memory usage in training DeepVariant is very high and increased by proceeding training. I used DeepVariant v0.7.1 via docker. The tested server (standalone, in house server) has 512GB memory with 56CPU, 8GB swap region without GPU/TPU. I followed your tutorials and made tfrecords including 241,896 pileup images from our data. I tested the following 3 patterns of training and observed increasing memory usage in all cases:. - When batch size=32,. - Training was successfully finished (total 50,000 steps). - Memory usage at the end of this training was 268GB. ![batch_size32 mem](https://user-images.githubusercontent.com/16730135/52196627-ddf14b00-289f-11e9-90dc-05eff144191d.jpg). - When batch size=64 (source code default),. - At ~16,000 steps, all memory (512 GB) was completely used. - At ~19,000 steps, all swap region was used (and killed). ![batch_size64 mem](https://user-images.githubusercontent.com/16730135/52196624-ddf14b00-289f-11e9-983a-5dcc3de48227.jpg). - When batch size=512 (tutorial default),. - At ~3,900 steps, all memory (512 GB) was completely used. - At ~4,200 steps, all swap region was used (and killed). ![batch_size512 mem](https://user-images.githubusercontent.com/16730135/52196626-ddf14b00-289f-11e9-8910-67a7c64b324d.jpg). Are these swelling memory usage due to tensorflow itself or some memory leak? I want to know how to prevent these. Best,. Masaru.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:1137,security,compl,completely,1137,"Swelling memory usage in training DeepVariant?; Hi,. I observed that CPU memory usage in training DeepVariant is very high and increased by proceeding training. I used DeepVariant v0.7.1 via docker. The tested server (standalone, in house server) has 512GB memory with 56CPU, 8GB swap region without GPU/TPU. I followed your tutorials and made tfrecords including 241,896 pileup images from our data. I tested the following 3 patterns of training and observed increasing memory usage in all cases:. - When batch size=32,. - Training was successfully finished (total 50,000 steps). - Memory usage at the end of this training was 268GB. ![batch_size32 mem](https://user-images.githubusercontent.com/16730135/52196627-ddf14b00-289f-11e9-90dc-05eff144191d.jpg). - When batch size=64 (source code default),. - At ~16,000 steps, all memory (512 GB) was completely used. - At ~19,000 steps, all swap region was used (and killed). ![batch_size64 mem](https://user-images.githubusercontent.com/16730135/52196624-ddf14b00-289f-11e9-983a-5dcc3de48227.jpg). - When batch size=512 (tutorial default),. - At ~3,900 steps, all memory (512 GB) was completely used. - At ~4,200 steps, all swap region was used (and killed). ![batch_size512 mem](https://user-images.githubusercontent.com/16730135/52196626-ddf14b00-289f-11e9-8910-67a7c64b324d.jpg). Are these swelling memory usage due to tensorflow itself or some memory leak? I want to know how to prevent these. Best,. Masaru.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:1437,security,preven,prevent,1437,"Swelling memory usage in training DeepVariant?; Hi,. I observed that CPU memory usage in training DeepVariant is very high and increased by proceeding training. I used DeepVariant v0.7.1 via docker. The tested server (standalone, in house server) has 512GB memory with 56CPU, 8GB swap region without GPU/TPU. I followed your tutorials and made tfrecords including 241,896 pileup images from our data. I tested the following 3 patterns of training and observed increasing memory usage in all cases:. - When batch size=32,. - Training was successfully finished (total 50,000 steps). - Memory usage at the end of this training was 268GB. ![batch_size32 mem](https://user-images.githubusercontent.com/16730135/52196627-ddf14b00-289f-11e9-90dc-05eff144191d.jpg). - When batch size=64 (source code default),. - At ~16,000 steps, all memory (512 GB) was completely used. - At ~19,000 steps, all swap region was used (and killed). ![batch_size64 mem](https://user-images.githubusercontent.com/16730135/52196624-ddf14b00-289f-11e9-983a-5dcc3de48227.jpg). - When batch size=512 (tutorial default),. - At ~3,900 steps, all memory (512 GB) was completely used. - At ~4,200 steps, all swap region was used (and killed). ![batch_size512 mem](https://user-images.githubusercontent.com/16730135/52196626-ddf14b00-289f-11e9-8910-67a7c64b324d.jpg). Are these swelling memory usage due to tensorflow itself or some memory leak? I want to know how to prevent these. Best,. Masaru.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:55,testability,observ,observed,55,"Swelling memory usage in training DeepVariant?; Hi,. I observed that CPU memory usage in training DeepVariant is very high and increased by proceeding training. I used DeepVariant v0.7.1 via docker. The tested server (standalone, in house server) has 512GB memory with 56CPU, 8GB swap region without GPU/TPU. I followed your tutorials and made tfrecords including 241,896 pileup images from our data. I tested the following 3 patterns of training and observed increasing memory usage in all cases:. - When batch size=32,. - Training was successfully finished (total 50,000 steps). - Memory usage at the end of this training was 268GB. ![batch_size32 mem](https://user-images.githubusercontent.com/16730135/52196627-ddf14b00-289f-11e9-90dc-05eff144191d.jpg). - When batch size=64 (source code default),. - At ~16,000 steps, all memory (512 GB) was completely used. - At ~19,000 steps, all swap region was used (and killed). ![batch_size64 mem](https://user-images.githubusercontent.com/16730135/52196624-ddf14b00-289f-11e9-983a-5dcc3de48227.jpg). - When batch size=512 (tutorial default),. - At ~3,900 steps, all memory (512 GB) was completely used. - At ~4,200 steps, all swap region was used (and killed). ![batch_size512 mem](https://user-images.githubusercontent.com/16730135/52196626-ddf14b00-289f-11e9-8910-67a7c64b324d.jpg). Are these swelling memory usage due to tensorflow itself or some memory leak? I want to know how to prevent these. Best,. Masaru.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:203,testability,test,tested,203,"Swelling memory usage in training DeepVariant?; Hi,. I observed that CPU memory usage in training DeepVariant is very high and increased by proceeding training. I used DeepVariant v0.7.1 via docker. The tested server (standalone, in house server) has 512GB memory with 56CPU, 8GB swap region without GPU/TPU. I followed your tutorials and made tfrecords including 241,896 pileup images from our data. I tested the following 3 patterns of training and observed increasing memory usage in all cases:. - When batch size=32,. - Training was successfully finished (total 50,000 steps). - Memory usage at the end of this training was 268GB. ![batch_size32 mem](https://user-images.githubusercontent.com/16730135/52196627-ddf14b00-289f-11e9-90dc-05eff144191d.jpg). - When batch size=64 (source code default),. - At ~16,000 steps, all memory (512 GB) was completely used. - At ~19,000 steps, all swap region was used (and killed). ![batch_size64 mem](https://user-images.githubusercontent.com/16730135/52196624-ddf14b00-289f-11e9-983a-5dcc3de48227.jpg). - When batch size=512 (tutorial default),. - At ~3,900 steps, all memory (512 GB) was completely used. - At ~4,200 steps, all swap region was used (and killed). ![batch_size512 mem](https://user-images.githubusercontent.com/16730135/52196626-ddf14b00-289f-11e9-8910-67a7c64b324d.jpg). Are these swelling memory usage due to tensorflow itself or some memory leak? I want to know how to prevent these. Best,. Masaru.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:403,testability,test,tested,403,"Swelling memory usage in training DeepVariant?; Hi,. I observed that CPU memory usage in training DeepVariant is very high and increased by proceeding training. I used DeepVariant v0.7.1 via docker. The tested server (standalone, in house server) has 512GB memory with 56CPU, 8GB swap region without GPU/TPU. I followed your tutorials and made tfrecords including 241,896 pileup images from our data. I tested the following 3 patterns of training and observed increasing memory usage in all cases:. - When batch size=32,. - Training was successfully finished (total 50,000 steps). - Memory usage at the end of this training was 268GB. ![batch_size32 mem](https://user-images.githubusercontent.com/16730135/52196627-ddf14b00-289f-11e9-90dc-05eff144191d.jpg). - When batch size=64 (source code default),. - At ~16,000 steps, all memory (512 GB) was completely used. - At ~19,000 steps, all swap region was used (and killed). ![batch_size64 mem](https://user-images.githubusercontent.com/16730135/52196624-ddf14b00-289f-11e9-983a-5dcc3de48227.jpg). - When batch size=512 (tutorial default),. - At ~3,900 steps, all memory (512 GB) was completely used. - At ~4,200 steps, all swap region was used (and killed). ![batch_size512 mem](https://user-images.githubusercontent.com/16730135/52196626-ddf14b00-289f-11e9-8910-67a7c64b324d.jpg). Are these swelling memory usage due to tensorflow itself or some memory leak? I want to know how to prevent these. Best,. Masaru.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:451,testability,observ,observed,451,"Swelling memory usage in training DeepVariant?; Hi,. I observed that CPU memory usage in training DeepVariant is very high and increased by proceeding training. I used DeepVariant v0.7.1 via docker. The tested server (standalone, in house server) has 512GB memory with 56CPU, 8GB swap region without GPU/TPU. I followed your tutorials and made tfrecords including 241,896 pileup images from our data. I tested the following 3 patterns of training and observed increasing memory usage in all cases:. - When batch size=32,. - Training was successfully finished (total 50,000 steps). - Memory usage at the end of this training was 268GB. ![batch_size32 mem](https://user-images.githubusercontent.com/16730135/52196627-ddf14b00-289f-11e9-90dc-05eff144191d.jpg). - When batch size=64 (source code default),. - At ~16,000 steps, all memory (512 GB) was completely used. - At ~19,000 steps, all swap region was used (and killed). ![batch_size64 mem](https://user-images.githubusercontent.com/16730135/52196624-ddf14b00-289f-11e9-983a-5dcc3de48227.jpg). - When batch size=512 (tutorial default),. - At ~3,900 steps, all memory (512 GB) was completely used. - At ~4,200 steps, all swap region was used (and killed). ![batch_size512 mem](https://user-images.githubusercontent.com/16730135/52196626-ddf14b00-289f-11e9-8910-67a7c64b324d.jpg). Are these swelling memory usage due to tensorflow itself or some memory leak? I want to know how to prevent these. Best,. Masaru.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:9,usability,memor,memory,9,"Swelling memory usage in training DeepVariant?; Hi,. I observed that CPU memory usage in training DeepVariant is very high and increased by proceeding training. I used DeepVariant v0.7.1 via docker. The tested server (standalone, in house server) has 512GB memory with 56CPU, 8GB swap region without GPU/TPU. I followed your tutorials and made tfrecords including 241,896 pileup images from our data. I tested the following 3 patterns of training and observed increasing memory usage in all cases:. - When batch size=32,. - Training was successfully finished (total 50,000 steps). - Memory usage at the end of this training was 268GB. ![batch_size32 mem](https://user-images.githubusercontent.com/16730135/52196627-ddf14b00-289f-11e9-90dc-05eff144191d.jpg). - When batch size=64 (source code default),. - At ~16,000 steps, all memory (512 GB) was completely used. - At ~19,000 steps, all swap region was used (and killed). ![batch_size64 mem](https://user-images.githubusercontent.com/16730135/52196624-ddf14b00-289f-11e9-983a-5dcc3de48227.jpg). - When batch size=512 (tutorial default),. - At ~3,900 steps, all memory (512 GB) was completely used. - At ~4,200 steps, all swap region was used (and killed). ![batch_size512 mem](https://user-images.githubusercontent.com/16730135/52196626-ddf14b00-289f-11e9-8910-67a7c64b324d.jpg). Are these swelling memory usage due to tensorflow itself or some memory leak? I want to know how to prevent these. Best,. Masaru.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:73,usability,memor,memory,73,"Swelling memory usage in training DeepVariant?; Hi,. I observed that CPU memory usage in training DeepVariant is very high and increased by proceeding training. I used DeepVariant v0.7.1 via docker. The tested server (standalone, in house server) has 512GB memory with 56CPU, 8GB swap region without GPU/TPU. I followed your tutorials and made tfrecords including 241,896 pileup images from our data. I tested the following 3 patterns of training and observed increasing memory usage in all cases:. - When batch size=32,. - Training was successfully finished (total 50,000 steps). - Memory usage at the end of this training was 268GB. ![batch_size32 mem](https://user-images.githubusercontent.com/16730135/52196627-ddf14b00-289f-11e9-90dc-05eff144191d.jpg). - When batch size=64 (source code default),. - At ~16,000 steps, all memory (512 GB) was completely used. - At ~19,000 steps, all swap region was used (and killed). ![batch_size64 mem](https://user-images.githubusercontent.com/16730135/52196624-ddf14b00-289f-11e9-983a-5dcc3de48227.jpg). - When batch size=512 (tutorial default),. - At ~3,900 steps, all memory (512 GB) was completely used. - At ~4,200 steps, all swap region was used (and killed). ![batch_size512 mem](https://user-images.githubusercontent.com/16730135/52196626-ddf14b00-289f-11e9-8910-67a7c64b324d.jpg). Are these swelling memory usage due to tensorflow itself or some memory leak? I want to know how to prevent these. Best,. Masaru.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:257,usability,memor,memory,257,"Swelling memory usage in training DeepVariant?; Hi,. I observed that CPU memory usage in training DeepVariant is very high and increased by proceeding training. I used DeepVariant v0.7.1 via docker. The tested server (standalone, in house server) has 512GB memory with 56CPU, 8GB swap region without GPU/TPU. I followed your tutorials and made tfrecords including 241,896 pileup images from our data. I tested the following 3 patterns of training and observed increasing memory usage in all cases:. - When batch size=32,. - Training was successfully finished (total 50,000 steps). - Memory usage at the end of this training was 268GB. ![batch_size32 mem](https://user-images.githubusercontent.com/16730135/52196627-ddf14b00-289f-11e9-90dc-05eff144191d.jpg). - When batch size=64 (source code default),. - At ~16,000 steps, all memory (512 GB) was completely used. - At ~19,000 steps, all swap region was used (and killed). ![batch_size64 mem](https://user-images.githubusercontent.com/16730135/52196624-ddf14b00-289f-11e9-983a-5dcc3de48227.jpg). - When batch size=512 (tutorial default),. - At ~3,900 steps, all memory (512 GB) was completely used. - At ~4,200 steps, all swap region was used (and killed). ![batch_size512 mem](https://user-images.githubusercontent.com/16730135/52196626-ddf14b00-289f-11e9-8910-67a7c64b324d.jpg). Are these swelling memory usage due to tensorflow itself or some memory leak? I want to know how to prevent these. Best,. Masaru.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:471,usability,memor,memory,471,"Swelling memory usage in training DeepVariant?; Hi,. I observed that CPU memory usage in training DeepVariant is very high and increased by proceeding training. I used DeepVariant v0.7.1 via docker. The tested server (standalone, in house server) has 512GB memory with 56CPU, 8GB swap region without GPU/TPU. I followed your tutorials and made tfrecords including 241,896 pileup images from our data. I tested the following 3 patterns of training and observed increasing memory usage in all cases:. - When batch size=32,. - Training was successfully finished (total 50,000 steps). - Memory usage at the end of this training was 268GB. ![batch_size32 mem](https://user-images.githubusercontent.com/16730135/52196627-ddf14b00-289f-11e9-90dc-05eff144191d.jpg). - When batch size=64 (source code default),. - At ~16,000 steps, all memory (512 GB) was completely used. - At ~19,000 steps, all swap region was used (and killed). ![batch_size64 mem](https://user-images.githubusercontent.com/16730135/52196624-ddf14b00-289f-11e9-983a-5dcc3de48227.jpg). - When batch size=512 (tutorial default),. - At ~3,900 steps, all memory (512 GB) was completely used. - At ~4,200 steps, all swap region was used (and killed). ![batch_size512 mem](https://user-images.githubusercontent.com/16730135/52196626-ddf14b00-289f-11e9-8910-67a7c64b324d.jpg). Are these swelling memory usage due to tensorflow itself or some memory leak? I want to know how to prevent these. Best,. Masaru.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:585,usability,Memor,Memory,585,"Swelling memory usage in training DeepVariant?; Hi,. I observed that CPU memory usage in training DeepVariant is very high and increased by proceeding training. I used DeepVariant v0.7.1 via docker. The tested server (standalone, in house server) has 512GB memory with 56CPU, 8GB swap region without GPU/TPU. I followed your tutorials and made tfrecords including 241,896 pileup images from our data. I tested the following 3 patterns of training and observed increasing memory usage in all cases:. - When batch size=32,. - Training was successfully finished (total 50,000 steps). - Memory usage at the end of this training was 268GB. ![batch_size32 mem](https://user-images.githubusercontent.com/16730135/52196627-ddf14b00-289f-11e9-90dc-05eff144191d.jpg). - When batch size=64 (source code default),. - At ~16,000 steps, all memory (512 GB) was completely used. - At ~19,000 steps, all swap region was used (and killed). ![batch_size64 mem](https://user-images.githubusercontent.com/16730135/52196624-ddf14b00-289f-11e9-983a-5dcc3de48227.jpg). - When batch size=512 (tutorial default),. - At ~3,900 steps, all memory (512 GB) was completely used. - At ~4,200 steps, all swap region was used (and killed). ![batch_size512 mem](https://user-images.githubusercontent.com/16730135/52196626-ddf14b00-289f-11e9-8910-67a7c64b324d.jpg). Are these swelling memory usage due to tensorflow itself or some memory leak? I want to know how to prevent these. Best,. Masaru.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:665,usability,user,user-images,665,"Swelling memory usage in training DeepVariant?; Hi,. I observed that CPU memory usage in training DeepVariant is very high and increased by proceeding training. I used DeepVariant v0.7.1 via docker. The tested server (standalone, in house server) has 512GB memory with 56CPU, 8GB swap region without GPU/TPU. I followed your tutorials and made tfrecords including 241,896 pileup images from our data. I tested the following 3 patterns of training and observed increasing memory usage in all cases:. - When batch size=32,. - Training was successfully finished (total 50,000 steps). - Memory usage at the end of this training was 268GB. ![batch_size32 mem](https://user-images.githubusercontent.com/16730135/52196627-ddf14b00-289f-11e9-90dc-05eff144191d.jpg). - When batch size=64 (source code default),. - At ~16,000 steps, all memory (512 GB) was completely used. - At ~19,000 steps, all swap region was used (and killed). ![batch_size64 mem](https://user-images.githubusercontent.com/16730135/52196624-ddf14b00-289f-11e9-983a-5dcc3de48227.jpg). - When batch size=512 (tutorial default),. - At ~3,900 steps, all memory (512 GB) was completely used. - At ~4,200 steps, all swap region was used (and killed). ![batch_size512 mem](https://user-images.githubusercontent.com/16730135/52196626-ddf14b00-289f-11e9-8910-67a7c64b324d.jpg). Are these swelling memory usage due to tensorflow itself or some memory leak? I want to know how to prevent these. Best,. Masaru.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:830,usability,memor,memory,830,"Swelling memory usage in training DeepVariant?; Hi,. I observed that CPU memory usage in training DeepVariant is very high and increased by proceeding training. I used DeepVariant v0.7.1 via docker. The tested server (standalone, in house server) has 512GB memory with 56CPU, 8GB swap region without GPU/TPU. I followed your tutorials and made tfrecords including 241,896 pileup images from our data. I tested the following 3 patterns of training and observed increasing memory usage in all cases:. - When batch size=32,. - Training was successfully finished (total 50,000 steps). - Memory usage at the end of this training was 268GB. ![batch_size32 mem](https://user-images.githubusercontent.com/16730135/52196627-ddf14b00-289f-11e9-90dc-05eff144191d.jpg). - When batch size=64 (source code default),. - At ~16,000 steps, all memory (512 GB) was completely used. - At ~19,000 steps, all swap region was used (and killed). ![batch_size64 mem](https://user-images.githubusercontent.com/16730135/52196624-ddf14b00-289f-11e9-983a-5dcc3de48227.jpg). - When batch size=512 (tutorial default),. - At ~3,900 steps, all memory (512 GB) was completely used. - At ~4,200 steps, all swap region was used (and killed). ![batch_size512 mem](https://user-images.githubusercontent.com/16730135/52196626-ddf14b00-289f-11e9-8910-67a7c64b324d.jpg). Are these swelling memory usage due to tensorflow itself or some memory leak? I want to know how to prevent these. Best,. Masaru.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:955,usability,user,user-images,955,"Swelling memory usage in training DeepVariant?; Hi,. I observed that CPU memory usage in training DeepVariant is very high and increased by proceeding training. I used DeepVariant v0.7.1 via docker. The tested server (standalone, in house server) has 512GB memory with 56CPU, 8GB swap region without GPU/TPU. I followed your tutorials and made tfrecords including 241,896 pileup images from our data. I tested the following 3 patterns of training and observed increasing memory usage in all cases:. - When batch size=32,. - Training was successfully finished (total 50,000 steps). - Memory usage at the end of this training was 268GB. ![batch_size32 mem](https://user-images.githubusercontent.com/16730135/52196627-ddf14b00-289f-11e9-90dc-05eff144191d.jpg). - When batch size=64 (source code default),. - At ~16,000 steps, all memory (512 GB) was completely used. - At ~19,000 steps, all swap region was used (and killed). ![batch_size64 mem](https://user-images.githubusercontent.com/16730135/52196624-ddf14b00-289f-11e9-983a-5dcc3de48227.jpg). - When batch size=512 (tutorial default),. - At ~3,900 steps, all memory (512 GB) was completely used. - At ~4,200 steps, all swap region was used (and killed). ![batch_size512 mem](https://user-images.githubusercontent.com/16730135/52196626-ddf14b00-289f-11e9-8910-67a7c64b324d.jpg). Are these swelling memory usage due to tensorflow itself or some memory leak? I want to know how to prevent these. Best,. Masaru.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:1117,usability,memor,memory,1117,"Swelling memory usage in training DeepVariant?; Hi,. I observed that CPU memory usage in training DeepVariant is very high and increased by proceeding training. I used DeepVariant v0.7.1 via docker. The tested server (standalone, in house server) has 512GB memory with 56CPU, 8GB swap region without GPU/TPU. I followed your tutorials and made tfrecords including 241,896 pileup images from our data. I tested the following 3 patterns of training and observed increasing memory usage in all cases:. - When batch size=32,. - Training was successfully finished (total 50,000 steps). - Memory usage at the end of this training was 268GB. ![batch_size32 mem](https://user-images.githubusercontent.com/16730135/52196627-ddf14b00-289f-11e9-90dc-05eff144191d.jpg). - When batch size=64 (source code default),. - At ~16,000 steps, all memory (512 GB) was completely used. - At ~19,000 steps, all swap region was used (and killed). ![batch_size64 mem](https://user-images.githubusercontent.com/16730135/52196624-ddf14b00-289f-11e9-983a-5dcc3de48227.jpg). - When batch size=512 (tutorial default),. - At ~3,900 steps, all memory (512 GB) was completely used. - At ~4,200 steps, all swap region was used (and killed). ![batch_size512 mem](https://user-images.githubusercontent.com/16730135/52196626-ddf14b00-289f-11e9-8910-67a7c64b324d.jpg). Are these swelling memory usage due to tensorflow itself or some memory leak? I want to know how to prevent these. Best,. Masaru.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:1242,usability,user,user-images,1242,"Swelling memory usage in training DeepVariant?; Hi,. I observed that CPU memory usage in training DeepVariant is very high and increased by proceeding training. I used DeepVariant v0.7.1 via docker. The tested server (standalone, in house server) has 512GB memory with 56CPU, 8GB swap region without GPU/TPU. I followed your tutorials and made tfrecords including 241,896 pileup images from our data. I tested the following 3 patterns of training and observed increasing memory usage in all cases:. - When batch size=32,. - Training was successfully finished (total 50,000 steps). - Memory usage at the end of this training was 268GB. ![batch_size32 mem](https://user-images.githubusercontent.com/16730135/52196627-ddf14b00-289f-11e9-90dc-05eff144191d.jpg). - When batch size=64 (source code default),. - At ~16,000 steps, all memory (512 GB) was completely used. - At ~19,000 steps, all swap region was used (and killed). ![batch_size64 mem](https://user-images.githubusercontent.com/16730135/52196624-ddf14b00-289f-11e9-983a-5dcc3de48227.jpg). - When batch size=512 (tutorial default),. - At ~3,900 steps, all memory (512 GB) was completely used. - At ~4,200 steps, all swap region was used (and killed). ![batch_size512 mem](https://user-images.githubusercontent.com/16730135/52196626-ddf14b00-289f-11e9-8910-67a7c64b324d.jpg). Are these swelling memory usage due to tensorflow itself or some memory leak? I want to know how to prevent these. Best,. Masaru.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:1356,usability,memor,memory,1356,"Swelling memory usage in training DeepVariant?; Hi,. I observed that CPU memory usage in training DeepVariant is very high and increased by proceeding training. I used DeepVariant v0.7.1 via docker. The tested server (standalone, in house server) has 512GB memory with 56CPU, 8GB swap region without GPU/TPU. I followed your tutorials and made tfrecords including 241,896 pileup images from our data. I tested the following 3 patterns of training and observed increasing memory usage in all cases:. - When batch size=32,. - Training was successfully finished (total 50,000 steps). - Memory usage at the end of this training was 268GB. ![batch_size32 mem](https://user-images.githubusercontent.com/16730135/52196627-ddf14b00-289f-11e9-90dc-05eff144191d.jpg). - When batch size=64 (source code default),. - At ~16,000 steps, all memory (512 GB) was completely used. - At ~19,000 steps, all swap region was used (and killed). ![batch_size64 mem](https://user-images.githubusercontent.com/16730135/52196624-ddf14b00-289f-11e9-983a-5dcc3de48227.jpg). - When batch size=512 (tutorial default),. - At ~3,900 steps, all memory (512 GB) was completely used. - At ~4,200 steps, all swap region was used (and killed). ![batch_size512 mem](https://user-images.githubusercontent.com/16730135/52196626-ddf14b00-289f-11e9-8910-67a7c64b324d.jpg). Are these swelling memory usage due to tensorflow itself or some memory leak? I want to know how to prevent these. Best,. Masaru.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/147:1402,usability,memor,memory,1402,"Swelling memory usage in training DeepVariant?; Hi,. I observed that CPU memory usage in training DeepVariant is very high and increased by proceeding training. I used DeepVariant v0.7.1 via docker. The tested server (standalone, in house server) has 512GB memory with 56CPU, 8GB swap region without GPU/TPU. I followed your tutorials and made tfrecords including 241,896 pileup images from our data. I tested the following 3 patterns of training and observed increasing memory usage in all cases:. - When batch size=32,. - Training was successfully finished (total 50,000 steps). - Memory usage at the end of this training was 268GB. ![batch_size32 mem](https://user-images.githubusercontent.com/16730135/52196627-ddf14b00-289f-11e9-90dc-05eff144191d.jpg). - When batch size=64 (source code default),. - At ~16,000 steps, all memory (512 GB) was completely used. - At ~19,000 steps, all swap region was used (and killed). ![batch_size64 mem](https://user-images.githubusercontent.com/16730135/52196624-ddf14b00-289f-11e9-983a-5dcc3de48227.jpg). - When batch size=512 (tutorial default),. - At ~3,900 steps, all memory (512 GB) was completely used. - At ~4,200 steps, all swap region was used (and killed). ![batch_size512 mem](https://user-images.githubusercontent.com/16730135/52196626-ddf14b00-289f-11e9-8910-67a7c64b324d.jpg). Are these swelling memory usage due to tensorflow itself or some memory leak? I want to know how to prevent these. Best,. Masaru.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/147
https://github.com/google/deepvariant/issues/148:263,availability,echo,echo,263,"make_examples bam index in different location from bam file; Hi,. Is there an argument to specify a bam index file's location in the make_examples step if it is different from the bam file? I am making symlinks for both, then running make_examples like so:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. And getting this error:. `ValueError: Not found: No index found for bamlink`. A little strange, because the bam index in question is indeed in the same location as the bam file-- these are the linking commands:. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam bamlink. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai bailink. ```. So two questions:. 1) Why doesn't this work? 2) Is there a way to specify the index file location separately, or am I going to have to simply copy the two files into a local folder together at the working directory level. This would be somewhat of a pain because the bam is hundreds of GB. Thanks! Seems like there might be based on [this link] (https://cloud.google.com/genomics/docs/tutorials/deepvariant#additional_configuration_options). But I can't find the equivalent just for the make_examples section.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/148
https://github.com/google/deepvariant/issues/148:707,availability,error,error,707,"make_examples bam index in different location from bam file; Hi,. Is there an argument to specify a bam index file's location in the make_examples step if it is different from the bam file? I am making symlinks for both, then running make_examples like so:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. And getting this error:. `ValueError: Not found: No index found for bamlink`. A little strange, because the bam index in question is indeed in the same location as the bam file-- these are the linking commands:. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam bamlink. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai bailink. ```. So two questions:. 1) Why doesn't this work? 2) Is there a way to specify the index file location separately, or am I going to have to simply copy the two files into a local folder together at the working directory level. This would be somewhat of a pain because the bam is hundreds of GB. Thanks! Seems like there might be based on [this link] (https://cloud.google.com/genomics/docs/tutorials/deepvariant#additional_configuration_options). But I can't find the equivalent just for the make_examples section.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/148
https://github.com/google/deepvariant/issues/148:299,deployability,Log,Log,299,"make_examples bam index in different location from bam file; Hi,. Is there an argument to specify a bam index file's location in the make_examples step if it is different from the bam file? I am making symlinks for both, then running make_examples like so:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. And getting this error:. `ValueError: Not found: No index found for bamlink`. A little strange, because the bam index in question is indeed in the same location as the bam file-- these are the linking commands:. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam bamlink. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai bailink. ```. So two questions:. 1) Why doesn't this work? 2) Is there a way to specify the index file location separately, or am I going to have to simply copy the two files into a local folder together at the working directory level. This would be somewhat of a pain because the bam is hundreds of GB. Thanks! Seems like there might be based on [this link] (https://cloud.google.com/genomics/docs/tutorials/deepvariant#additional_configuration_options). But I can't find the equivalent just for the make_examples section.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/148
https://github.com/google/deepvariant/issues/148:353,deployability,log,log,353,"make_examples bam index in different location from bam file; Hi,. Is there an argument to specify a bam index file's location in the make_examples step if it is different from the bam file? I am making symlinks for both, then running make_examples like so:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. And getting this error:. `ValueError: Not found: No index found for bamlink`. A little strange, because the bam index in question is indeed in the same location as the bam file-- these are the linking commands:. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam bamlink. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai bailink. ```. So two questions:. 1) Why doesn't this work? 2) Is there a way to specify the index file location separately, or am I going to have to simply copy the two files into a local folder together at the working directory level. This would be somewhat of a pain because the bam is hundreds of GB. Thanks! Seems like there might be based on [this link] (https://cloud.google.com/genomics/docs/tutorials/deepvariant#additional_configuration_options). But I can't find the equivalent just for the make_examples section.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/148
https://github.com/google/deepvariant/issues/148:679,deployability,log,log,679,"make_examples bam index in different location from bam file; Hi,. Is there an argument to specify a bam index file's location in the make_examples step if it is different from the bam file? I am making symlinks for both, then running make_examples like so:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. And getting this error:. `ValueError: Not found: No index found for bamlink`. A little strange, because the bam index in question is indeed in the same location as the bam file-- these are the linking commands:. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam bamlink. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai bailink. ```. So two questions:. 1) Why doesn't this work? 2) Is there a way to specify the index file location separately, or am I going to have to simply copy the two files into a local folder together at the working directory level. This would be somewhat of a pain because the bam is hundreds of GB. Thanks! Seems like there might be based on [this link] (https://cloud.google.com/genomics/docs/tutorials/deepvariant#additional_configuration_options). But I can't find the equivalent just for the make_examples section.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/148
https://github.com/google/deepvariant/issues/148:1444,energy efficiency,cloud,cloud,1444,"make_examples bam index in different location from bam file; Hi,. Is there an argument to specify a bam index file's location in the make_examples step if it is different from the bam file? I am making symlinks for both, then running make_examples like so:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. And getting this error:. `ValueError: Not found: No index found for bamlink`. A little strange, because the bam index in question is indeed in the same location as the bam file-- these are the linking commands:. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam bamlink. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai bailink. ```. So two questions:. 1) Why doesn't this work? 2) Is there a way to specify the index file location separately, or am I going to have to simply copy the two files into a local folder together at the working directory level. This would be somewhat of a pain because the bam is hundreds of GB. Thanks! Seems like there might be based on [this link] (https://cloud.google.com/genomics/docs/tutorials/deepvariant#additional_configuration_options). But I can't find the equivalent just for the make_examples section.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/148
https://github.com/google/deepvariant/issues/148:417,integrability,buffer,buffer,417,"make_examples bam index in different location from bam file; Hi,. Is there an argument to specify a bam index file's location in the make_examples step if it is different from the bam file? I am making symlinks for both, then running make_examples like so:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. And getting this error:. `ValueError: Not found: No index found for bamlink`. A little strange, because the bam index in question is indeed in the same location as the bam file-- these are the linking commands:. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam bamlink. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai bailink. ```. So two questions:. 1) Why doesn't this work? 2) Is there a way to specify the index file location separately, or am I going to have to simply copy the two files into a local folder together at the working directory level. This would be somewhat of a pain because the bam is hundreds of GB. Thanks! Seems like there might be based on [this link] (https://cloud.google.com/genomics/docs/tutorials/deepvariant#additional_configuration_options). But I can't find the equivalent just for the make_examples section.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/148
https://github.com/google/deepvariant/issues/148:90,interoperability,specif,specify,90,"make_examples bam index in different location from bam file; Hi,. Is there an argument to specify a bam index file's location in the make_examples step if it is different from the bam file? I am making symlinks for both, then running make_examples like so:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. And getting this error:. `ValueError: Not found: No index found for bamlink`. A little strange, because the bam index in question is indeed in the same location as the bam file-- these are the linking commands:. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam bamlink. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai bailink. ```. So two questions:. 1) Why doesn't this work? 2) Is there a way to specify the index file location separately, or am I going to have to simply copy the two files into a local folder together at the working directory level. This would be somewhat of a pain because the bam is hundreds of GB. Thanks! Seems like there might be based on [this link] (https://cloud.google.com/genomics/docs/tutorials/deepvariant#additional_configuration_options). But I can't find the equivalent just for the make_examples section.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/148
https://github.com/google/deepvariant/issues/148:1156,interoperability,specif,specify,1156,"make_examples bam index in different location from bam file; Hi,. Is there an argument to specify a bam index file's location in the make_examples step if it is different from the bam file? I am making symlinks for both, then running make_examples like so:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. And getting this error:. `ValueError: Not found: No index found for bamlink`. A little strange, because the bam index in question is indeed in the same location as the bam file-- these are the linking commands:. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam bamlink. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai bailink. ```. So two questions:. 1) Why doesn't this work? 2) Is there a way to specify the index file location separately, or am I going to have to simply copy the two files into a local folder together at the working directory level. This would be somewhat of a pain because the bam is hundreds of GB. Thanks! Seems like there might be based on [this link] (https://cloud.google.com/genomics/docs/tutorials/deepvariant#additional_configuration_options). But I can't find the equivalent just for the make_examples section.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/148
https://github.com/google/deepvariant/issues/148:362,performance,time,time,362,"make_examples bam index in different location from bam file; Hi,. Is there an argument to specify a bam index file's location in the make_examples step if it is different from the bam file? I am making symlinks for both, then running make_examples like so:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. And getting this error:. `ValueError: Not found: No index found for bamlink`. A little strange, because the bam index in question is indeed in the same location as the bam file-- these are the linking commands:. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam bamlink. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai bailink. ```. So two questions:. 1) Why doesn't this work? 2) Is there a way to specify the index file location separately, or am I going to have to simply copy the two files into a local folder together at the working directory level. This would be somewhat of a pain because the bam is hundreds of GB. Thanks! Seems like there might be based on [this link] (https://cloud.google.com/genomics/docs/tutorials/deepvariant#additional_configuration_options). But I can't find the equivalent just for the make_examples section.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/148
https://github.com/google/deepvariant/issues/148:398,performance,parallel,parallel,398,"make_examples bam index in different location from bam file; Hi,. Is there an argument to specify a bam index file's location in the make_examples step if it is different from the bam file? I am making symlinks for both, then running make_examples like so:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. And getting this error:. `ValueError: Not found: No index found for bamlink`. A little strange, because the bam index in question is indeed in the same location as the bam file-- these are the linking commands:. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam bamlink. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai bailink. ```. So two questions:. 1) Why doesn't this work? 2) Is there a way to specify the index file location separately, or am I going to have to simply copy the two files into a local folder together at the working directory level. This would be somewhat of a pain because the bam is hundreds of GB. Thanks! Seems like there might be based on [this link] (https://cloud.google.com/genomics/docs/tutorials/deepvariant#additional_configuration_options). But I can't find the equivalent just for the make_examples section.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/148
https://github.com/google/deepvariant/issues/148:707,performance,error,error,707,"make_examples bam index in different location from bam file; Hi,. Is there an argument to specify a bam index file's location in the make_examples step if it is different from the bam file? I am making symlinks for both, then running make_examples like so:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. And getting this error:. `ValueError: Not found: No index found for bamlink`. A little strange, because the bam index in question is indeed in the same location as the bam file-- these are the linking commands:. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam bamlink. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai bailink. ```. So two questions:. 1) Why doesn't this work? 2) Is there a way to specify the index file location separately, or am I going to have to simply copy the two files into a local folder together at the working directory level. This would be somewhat of a pain because the bam is hundreds of GB. Thanks! Seems like there might be based on [this link] (https://cloud.google.com/genomics/docs/tutorials/deepvariant#additional_configuration_options). But I can't find the equivalent just for the make_examples section.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/148
https://github.com/google/deepvariant/issues/148:1116,reliability,doe,doesn,1116,"make_examples bam index in different location from bam file; Hi,. Is there an argument to specify a bam index file's location in the make_examples step if it is different from the bam file? I am making symlinks for both, then running make_examples like so:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. And getting this error:. `ValueError: Not found: No index found for bamlink`. A little strange, because the bam index in question is indeed in the same location as the bam file-- these are the linking commands:. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam bamlink. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai bailink. ```. So two questions:. 1) Why doesn't this work? 2) Is there a way to specify the index file location separately, or am I going to have to simply copy the two files into a local folder together at the working directory level. This would be somewhat of a pain because the bam is hundreds of GB. Thanks! Seems like there might be based on [this link] (https://cloud.google.com/genomics/docs/tutorials/deepvariant#additional_configuration_options). But I can't find the equivalent just for the make_examples section.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/148
https://github.com/google/deepvariant/issues/148:299,safety,Log,Log,299,"make_examples bam index in different location from bam file; Hi,. Is there an argument to specify a bam index file's location in the make_examples step if it is different from the bam file? I am making symlinks for both, then running make_examples like so:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. And getting this error:. `ValueError: Not found: No index found for bamlink`. A little strange, because the bam index in question is indeed in the same location as the bam file-- these are the linking commands:. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam bamlink. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai bailink. ```. So two questions:. 1) Why doesn't this work? 2) Is there a way to specify the index file location separately, or am I going to have to simply copy the two files into a local folder together at the working directory level. This would be somewhat of a pain because the bam is hundreds of GB. Thanks! Seems like there might be based on [this link] (https://cloud.google.com/genomics/docs/tutorials/deepvariant#additional_configuration_options). But I can't find the equivalent just for the make_examples section.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/148
https://github.com/google/deepvariant/issues/148:353,safety,log,log,353,"make_examples bam index in different location from bam file; Hi,. Is there an argument to specify a bam index file's location in the make_examples step if it is different from the bam file? I am making symlinks for both, then running make_examples like so:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. And getting this error:. `ValueError: Not found: No index found for bamlink`. A little strange, because the bam index in question is indeed in the same location as the bam file-- these are the linking commands:. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam bamlink. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai bailink. ```. So two questions:. 1) Why doesn't this work? 2) Is there a way to specify the index file location separately, or am I going to have to simply copy the two files into a local folder together at the working directory level. This would be somewhat of a pain because the bam is hundreds of GB. Thanks! Seems like there might be based on [this link] (https://cloud.google.com/genomics/docs/tutorials/deepvariant#additional_configuration_options). But I can't find the equivalent just for the make_examples section.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/148
https://github.com/google/deepvariant/issues/148:679,safety,log,log,679,"make_examples bam index in different location from bam file; Hi,. Is there an argument to specify a bam index file's location in the make_examples step if it is different from the bam file? I am making symlinks for both, then running make_examples like so:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. And getting this error:. `ValueError: Not found: No index found for bamlink`. A little strange, because the bam index in question is indeed in the same location as the bam file-- these are the linking commands:. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam bamlink. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai bailink. ```. So two questions:. 1) Why doesn't this work? 2) Is there a way to specify the index file location separately, or am I going to have to simply copy the two files into a local folder together at the working directory level. This would be somewhat of a pain because the bam is hundreds of GB. Thanks! Seems like there might be based on [this link] (https://cloud.google.com/genomics/docs/tutorials/deepvariant#additional_configuration_options). But I can't find the equivalent just for the make_examples section.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/148
https://github.com/google/deepvariant/issues/148:707,safety,error,error,707,"make_examples bam index in different location from bam file; Hi,. Is there an argument to specify a bam index file's location in the make_examples step if it is different from the bam file? I am making symlinks for both, then running make_examples like so:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. And getting this error:. `ValueError: Not found: No index found for bamlink`. A little strange, because the bam index in question is indeed in the same location as the bam file-- these are the linking commands:. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam bamlink. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai bailink. ```. So two questions:. 1) Why doesn't this work? 2) Is there a way to specify the index file location separately, or am I going to have to simply copy the two files into a local folder together at the working directory level. This would be somewhat of a pain because the bam is hundreds of GB. Thanks! Seems like there might be based on [this link] (https://cloud.google.com/genomics/docs/tutorials/deepvariant#additional_configuration_options). But I can't find the equivalent just for the make_examples section.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/148
https://github.com/google/deepvariant/issues/148:299,security,Log,Log,299,"make_examples bam index in different location from bam file; Hi,. Is there an argument to specify a bam index file's location in the make_examples step if it is different from the bam file? I am making symlinks for both, then running make_examples like so:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. And getting this error:. `ValueError: Not found: No index found for bamlink`. A little strange, because the bam index in question is indeed in the same location as the bam file-- these are the linking commands:. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam bamlink. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai bailink. ```. So two questions:. 1) Why doesn't this work? 2) Is there a way to specify the index file location separately, or am I going to have to simply copy the two files into a local folder together at the working directory level. This would be somewhat of a pain because the bam is hundreds of GB. Thanks! Seems like there might be based on [this link] (https://cloud.google.com/genomics/docs/tutorials/deepvariant#additional_configuration_options). But I can't find the equivalent just for the make_examples section.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/148
https://github.com/google/deepvariant/issues/148:353,security,log,log,353,"make_examples bam index in different location from bam file; Hi,. Is there an argument to specify a bam index file's location in the make_examples step if it is different from the bam file? I am making symlinks for both, then running make_examples like so:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. And getting this error:. `ValueError: Not found: No index found for bamlink`. A little strange, because the bam index in question is indeed in the same location as the bam file-- these are the linking commands:. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam bamlink. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai bailink. ```. So two questions:. 1) Why doesn't this work? 2) Is there a way to specify the index file location separately, or am I going to have to simply copy the two files into a local folder together at the working directory level. This would be somewhat of a pain because the bam is hundreds of GB. Thanks! Seems like there might be based on [this link] (https://cloud.google.com/genomics/docs/tutorials/deepvariant#additional_configuration_options). But I can't find the equivalent just for the make_examples section.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/148
https://github.com/google/deepvariant/issues/148:679,security,log,log,679,"make_examples bam index in different location from bam file; Hi,. Is there an argument to specify a bam index file's location in the make_examples step if it is different from the bam file? I am making symlinks for both, then running make_examples like so:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. And getting this error:. `ValueError: Not found: No index found for bamlink`. A little strange, because the bam index in question is indeed in the same location as the bam file-- these are the linking commands:. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam bamlink. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai bailink. ```. So two questions:. 1) Why doesn't this work? 2) Is there a way to specify the index file location separately, or am I going to have to simply copy the two files into a local folder together at the working directory level. This would be somewhat of a pain because the bam is hundreds of GB. Thanks! Seems like there might be based on [this link] (https://cloud.google.com/genomics/docs/tutorials/deepvariant#additional_configuration_options). But I can't find the equivalent just for the make_examples section.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/148
https://github.com/google/deepvariant/issues/148:299,testability,Log,Log,299,"make_examples bam index in different location from bam file; Hi,. Is there an argument to specify a bam index file's location in the make_examples step if it is different from the bam file? I am making symlinks for both, then running make_examples like so:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. And getting this error:. `ValueError: Not found: No index found for bamlink`. A little strange, because the bam index in question is indeed in the same location as the bam file-- these are the linking commands:. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam bamlink. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai bailink. ```. So two questions:. 1) Why doesn't this work? 2) Is there a way to specify the index file location separately, or am I going to have to simply copy the two files into a local folder together at the working directory level. This would be somewhat of a pain because the bam is hundreds of GB. Thanks! Seems like there might be based on [this link] (https://cloud.google.com/genomics/docs/tutorials/deepvariant#additional_configuration_options). But I can't find the equivalent just for the make_examples section.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/148
https://github.com/google/deepvariant/issues/148:353,testability,log,log,353,"make_examples bam index in different location from bam file; Hi,. Is there an argument to specify a bam index file's location in the make_examples step if it is different from the bam file? I am making symlinks for both, then running make_examples like so:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. And getting this error:. `ValueError: Not found: No index found for bamlink`. A little strange, because the bam index in question is indeed in the same location as the bam file-- these are the linking commands:. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam bamlink. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai bailink. ```. So two questions:. 1) Why doesn't this work? 2) Is there a way to specify the index file location separately, or am I going to have to simply copy the two files into a local folder together at the working directory level. This would be somewhat of a pain because the bam is hundreds of GB. Thanks! Seems like there might be based on [this link] (https://cloud.google.com/genomics/docs/tutorials/deepvariant#additional_configuration_options). But I can't find the equivalent just for the make_examples section.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/148
https://github.com/google/deepvariant/issues/148:679,testability,log,log,679,"make_examples bam index in different location from bam file; Hi,. Is there an argument to specify a bam index file's location in the make_examples step if it is different from the bam file? I am making symlinks for both, then running make_examples like so:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. And getting this error:. `ValueError: Not found: No index found for bamlink`. A little strange, because the bam index in question is indeed in the same location as the bam file-- these are the linking commands:. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam bamlink. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai bailink. ```. So two questions:. 1) Why doesn't this work? 2) Is there a way to specify the index file location separately, or am I going to have to simply copy the two files into a local folder together at the working directory level. This would be somewhat of a pain because the bam is hundreds of GB. Thanks! Seems like there might be based on [this link] (https://cloud.google.com/genomics/docs/tutorials/deepvariant#additional_configuration_options). But I can't find the equivalent just for the make_examples section.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/148
https://github.com/google/deepvariant/issues/148:1225,testability,simpl,simply,1225,"make_examples bam index in different location from bam file; Hi,. Is there an argument to specify a bam index file's location in the make_examples step if it is different from the bam file? I am making symlinks for both, then running make_examples like so:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. And getting this error:. `ValueError: Not found: No index found for bamlink`. A little strange, because the bam index in question is indeed in the same location as the bam file-- these are the linking commands:. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam bamlink. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai bailink. ```. So two questions:. 1) Why doesn't this work? 2) Is there a way to specify the index file location separately, or am I going to have to simply copy the two files into a local folder together at the working directory level. This would be somewhat of a pain because the bam is hundreds of GB. Thanks! Seems like there might be based on [this link] (https://cloud.google.com/genomics/docs/tutorials/deepvariant#additional_configuration_options). But I can't find the equivalent just for the make_examples section.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/148
https://github.com/google/deepvariant/issues/148:707,usability,error,error,707,"make_examples bam index in different location from bam file; Hi,. Is there an argument to specify a bam index file's location in the make_examples step if it is different from the bam file? I am making symlinks for both, then running make_examples like so:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. And getting this error:. `ValueError: Not found: No index found for bamlink`. A little strange, because the bam index in question is indeed in the same location as the bam file-- these are the linking commands:. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam bamlink. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai bailink. ```. So two questions:. 1) Why doesn't this work? 2) Is there a way to specify the index file location separately, or am I going to have to simply copy the two files into a local folder together at the working directory level. This would be somewhat of a pain because the bam is hundreds of GB. Thanks! Seems like there might be based on [this link] (https://cloud.google.com/genomics/docs/tutorials/deepvariant#additional_configuration_options). But I can't find the equivalent just for the make_examples section.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/148
https://github.com/google/deepvariant/issues/148:891,usability,command,commands,891,"make_examples bam index in different location from bam file; Hi,. Is there an argument to specify a bam index file's location in the make_examples step if it is different from the bam file? I am making symlinks for both, then running make_examples like so:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. And getting this error:. `ValueError: Not found: No index found for bamlink`. A little strange, because the bam index in question is indeed in the same location as the bam file-- these are the linking commands:. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam bamlink. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai bailink. ```. So two questions:. 1) Why doesn't this work? 2) Is there a way to specify the index file location separately, or am I going to have to simply copy the two files into a local folder together at the working directory level. This would be somewhat of a pain because the bam is hundreds of GB. Thanks! Seems like there might be based on [this link] (https://cloud.google.com/genomics/docs/tutorials/deepvariant#additional_configuration_options). But I can't find the equivalent just for the make_examples section.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/148
https://github.com/google/deepvariant/issues/148:1225,usability,simpl,simply,1225,"make_examples bam index in different location from bam file; Hi,. Is there an argument to specify a bam index file's location in the make_examples step if it is different from the bam file? I am making symlinks for both, then running make_examples like so:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. And getting this error:. `ValueError: Not found: No index found for bamlink`. A little strange, because the bam index in question is indeed in the same location as the bam file-- these are the linking commands:. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam bamlink. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai bailink. ```. So two questions:. 1) Why doesn't this work? 2) Is there a way to specify the index file location separately, or am I going to have to simply copy the two files into a local folder together at the working directory level. This would be somewhat of a pain because the bam is hundreds of GB. Thanks! Seems like there might be based on [this link] (https://cloud.google.com/genomics/docs/tutorials/deepvariant#additional_configuration_options). But I can't find the equivalent just for the make_examples section.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/148
https://github.com/google/deepvariant/issues/149:289,availability,echo,echo,289,"bam index location issue for make_examples; Hi (accidentally closed old issue prematurely). Is there an argument to specify a bam index file's location in the make_examples step if it is different from the bam file? I am making symlinks for both, then running make_examples like so:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. And getting this error:. `ValueError: Not found: No index found for bamlink`. A little strange, because the bam index in question is indeed in the same location as the bam file-- these are the linking commands:. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam bamlink. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai bailink. ```. So two questions:. 1) Why doesn't this work? 2) Is there a way to specify the index file location separately, or am I going to have to simply copy the two files into a local folder together at the working directory level. This would be somewhat of a pain because the bam is hundreds of GB. Thanks! Seems like there might be based on [this link] (https://cloud.google.com/genomics/docs/tutorials/deepvariant#additional_configuration_options). But I can't find the equivalent just for the make_examples section.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/149
https://github.com/google/deepvariant/issues/149:733,availability,error,error,733,"bam index location issue for make_examples; Hi (accidentally closed old issue prematurely). Is there an argument to specify a bam index file's location in the make_examples step if it is different from the bam file? I am making symlinks for both, then running make_examples like so:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. And getting this error:. `ValueError: Not found: No index found for bamlink`. A little strange, because the bam index in question is indeed in the same location as the bam file-- these are the linking commands:. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam bamlink. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai bailink. ```. So two questions:. 1) Why doesn't this work? 2) Is there a way to specify the index file location separately, or am I going to have to simply copy the two files into a local folder together at the working directory level. This would be somewhat of a pain because the bam is hundreds of GB. Thanks! Seems like there might be based on [this link] (https://cloud.google.com/genomics/docs/tutorials/deepvariant#additional_configuration_options). But I can't find the equivalent just for the make_examples section.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/149
https://github.com/google/deepvariant/issues/149:325,deployability,Log,Log,325,"bam index location issue for make_examples; Hi (accidentally closed old issue prematurely). Is there an argument to specify a bam index file's location in the make_examples step if it is different from the bam file? I am making symlinks for both, then running make_examples like so:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. And getting this error:. `ValueError: Not found: No index found for bamlink`. A little strange, because the bam index in question is indeed in the same location as the bam file-- these are the linking commands:. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam bamlink. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai bailink. ```. So two questions:. 1) Why doesn't this work? 2) Is there a way to specify the index file location separately, or am I going to have to simply copy the two files into a local folder together at the working directory level. This would be somewhat of a pain because the bam is hundreds of GB. Thanks! Seems like there might be based on [this link] (https://cloud.google.com/genomics/docs/tutorials/deepvariant#additional_configuration_options). But I can't find the equivalent just for the make_examples section.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/149
https://github.com/google/deepvariant/issues/149:379,deployability,log,log,379,"bam index location issue for make_examples; Hi (accidentally closed old issue prematurely). Is there an argument to specify a bam index file's location in the make_examples step if it is different from the bam file? I am making symlinks for both, then running make_examples like so:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. And getting this error:. `ValueError: Not found: No index found for bamlink`. A little strange, because the bam index in question is indeed in the same location as the bam file-- these are the linking commands:. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam bamlink. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai bailink. ```. So two questions:. 1) Why doesn't this work? 2) Is there a way to specify the index file location separately, or am I going to have to simply copy the two files into a local folder together at the working directory level. This would be somewhat of a pain because the bam is hundreds of GB. Thanks! Seems like there might be based on [this link] (https://cloud.google.com/genomics/docs/tutorials/deepvariant#additional_configuration_options). But I can't find the equivalent just for the make_examples section.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/149
https://github.com/google/deepvariant/issues/149:705,deployability,log,log,705,"bam index location issue for make_examples; Hi (accidentally closed old issue prematurely). Is there an argument to specify a bam index file's location in the make_examples step if it is different from the bam file? I am making symlinks for both, then running make_examples like so:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. And getting this error:. `ValueError: Not found: No index found for bamlink`. A little strange, because the bam index in question is indeed in the same location as the bam file-- these are the linking commands:. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam bamlink. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai bailink. ```. So two questions:. 1) Why doesn't this work? 2) Is there a way to specify the index file location separately, or am I going to have to simply copy the two files into a local folder together at the working directory level. This would be somewhat of a pain because the bam is hundreds of GB. Thanks! Seems like there might be based on [this link] (https://cloud.google.com/genomics/docs/tutorials/deepvariant#additional_configuration_options). But I can't find the equivalent just for the make_examples section.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/149
https://github.com/google/deepvariant/issues/149:1470,energy efficiency,cloud,cloud,1470,"bam index location issue for make_examples; Hi (accidentally closed old issue prematurely). Is there an argument to specify a bam index file's location in the make_examples step if it is different from the bam file? I am making symlinks for both, then running make_examples like so:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. And getting this error:. `ValueError: Not found: No index found for bamlink`. A little strange, because the bam index in question is indeed in the same location as the bam file-- these are the linking commands:. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam bamlink. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai bailink. ```. So two questions:. 1) Why doesn't this work? 2) Is there a way to specify the index file location separately, or am I going to have to simply copy the two files into a local folder together at the working directory level. This would be somewhat of a pain because the bam is hundreds of GB. Thanks! Seems like there might be based on [this link] (https://cloud.google.com/genomics/docs/tutorials/deepvariant#additional_configuration_options). But I can't find the equivalent just for the make_examples section.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/149
https://github.com/google/deepvariant/issues/149:443,integrability,buffer,buffer,443,"bam index location issue for make_examples; Hi (accidentally closed old issue prematurely). Is there an argument to specify a bam index file's location in the make_examples step if it is different from the bam file? I am making symlinks for both, then running make_examples like so:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. And getting this error:. `ValueError: Not found: No index found for bamlink`. A little strange, because the bam index in question is indeed in the same location as the bam file-- these are the linking commands:. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam bamlink. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai bailink. ```. So two questions:. 1) Why doesn't this work? 2) Is there a way to specify the index file location separately, or am I going to have to simply copy the two files into a local folder together at the working directory level. This would be somewhat of a pain because the bam is hundreds of GB. Thanks! Seems like there might be based on [this link] (https://cloud.google.com/genomics/docs/tutorials/deepvariant#additional_configuration_options). But I can't find the equivalent just for the make_examples section.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/149
https://github.com/google/deepvariant/issues/149:116,interoperability,specif,specify,116,"bam index location issue for make_examples; Hi (accidentally closed old issue prematurely). Is there an argument to specify a bam index file's location in the make_examples step if it is different from the bam file? I am making symlinks for both, then running make_examples like so:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. And getting this error:. `ValueError: Not found: No index found for bamlink`. A little strange, because the bam index in question is indeed in the same location as the bam file-- these are the linking commands:. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam bamlink. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai bailink. ```. So two questions:. 1) Why doesn't this work? 2) Is there a way to specify the index file location separately, or am I going to have to simply copy the two files into a local folder together at the working directory level. This would be somewhat of a pain because the bam is hundreds of GB. Thanks! Seems like there might be based on [this link] (https://cloud.google.com/genomics/docs/tutorials/deepvariant#additional_configuration_options). But I can't find the equivalent just for the make_examples section.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/149
https://github.com/google/deepvariant/issues/149:1182,interoperability,specif,specify,1182,"bam index location issue for make_examples; Hi (accidentally closed old issue prematurely). Is there an argument to specify a bam index file's location in the make_examples step if it is different from the bam file? I am making symlinks for both, then running make_examples like so:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. And getting this error:. `ValueError: Not found: No index found for bamlink`. A little strange, because the bam index in question is indeed in the same location as the bam file-- these are the linking commands:. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam bamlink. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai bailink. ```. So two questions:. 1) Why doesn't this work? 2) Is there a way to specify the index file location separately, or am I going to have to simply copy the two files into a local folder together at the working directory level. This would be somewhat of a pain because the bam is hundreds of GB. Thanks! Seems like there might be based on [this link] (https://cloud.google.com/genomics/docs/tutorials/deepvariant#additional_configuration_options). But I can't find the equivalent just for the make_examples section.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/149
https://github.com/google/deepvariant/issues/149:388,performance,time,time,388,"bam index location issue for make_examples; Hi (accidentally closed old issue prematurely). Is there an argument to specify a bam index file's location in the make_examples step if it is different from the bam file? I am making symlinks for both, then running make_examples like so:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. And getting this error:. `ValueError: Not found: No index found for bamlink`. A little strange, because the bam index in question is indeed in the same location as the bam file-- these are the linking commands:. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam bamlink. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai bailink. ```. So two questions:. 1) Why doesn't this work? 2) Is there a way to specify the index file location separately, or am I going to have to simply copy the two files into a local folder together at the working directory level. This would be somewhat of a pain because the bam is hundreds of GB. Thanks! Seems like there might be based on [this link] (https://cloud.google.com/genomics/docs/tutorials/deepvariant#additional_configuration_options). But I can't find the equivalent just for the make_examples section.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/149
https://github.com/google/deepvariant/issues/149:424,performance,parallel,parallel,424,"bam index location issue for make_examples; Hi (accidentally closed old issue prematurely). Is there an argument to specify a bam index file's location in the make_examples step if it is different from the bam file? I am making symlinks for both, then running make_examples like so:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. And getting this error:. `ValueError: Not found: No index found for bamlink`. A little strange, because the bam index in question is indeed in the same location as the bam file-- these are the linking commands:. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam bamlink. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai bailink. ```. So two questions:. 1) Why doesn't this work? 2) Is there a way to specify the index file location separately, or am I going to have to simply copy the two files into a local folder together at the working directory level. This would be somewhat of a pain because the bam is hundreds of GB. Thanks! Seems like there might be based on [this link] (https://cloud.google.com/genomics/docs/tutorials/deepvariant#additional_configuration_options). But I can't find the equivalent just for the make_examples section.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/149
https://github.com/google/deepvariant/issues/149:733,performance,error,error,733,"bam index location issue for make_examples; Hi (accidentally closed old issue prematurely). Is there an argument to specify a bam index file's location in the make_examples step if it is different from the bam file? I am making symlinks for both, then running make_examples like so:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. And getting this error:. `ValueError: Not found: No index found for bamlink`. A little strange, because the bam index in question is indeed in the same location as the bam file-- these are the linking commands:. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam bamlink. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai bailink. ```. So two questions:. 1) Why doesn't this work? 2) Is there a way to specify the index file location separately, or am I going to have to simply copy the two files into a local folder together at the working directory level. This would be somewhat of a pain because the bam is hundreds of GB. Thanks! Seems like there might be based on [this link] (https://cloud.google.com/genomics/docs/tutorials/deepvariant#additional_configuration_options). But I can't find the equivalent just for the make_examples section.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/149
https://github.com/google/deepvariant/issues/149:1142,reliability,doe,doesn,1142,"bam index location issue for make_examples; Hi (accidentally closed old issue prematurely). Is there an argument to specify a bam index file's location in the make_examples step if it is different from the bam file? I am making symlinks for both, then running make_examples like so:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. And getting this error:. `ValueError: Not found: No index found for bamlink`. A little strange, because the bam index in question is indeed in the same location as the bam file-- these are the linking commands:. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam bamlink. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai bailink. ```. So two questions:. 1) Why doesn't this work? 2) Is there a way to specify the index file location separately, or am I going to have to simply copy the two files into a local folder together at the working directory level. This would be somewhat of a pain because the bam is hundreds of GB. Thanks! Seems like there might be based on [this link] (https://cloud.google.com/genomics/docs/tutorials/deepvariant#additional_configuration_options). But I can't find the equivalent just for the make_examples section.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/149
https://github.com/google/deepvariant/issues/149:48,safety,accid,accidentally,48,"bam index location issue for make_examples; Hi (accidentally closed old issue prematurely). Is there an argument to specify a bam index file's location in the make_examples step if it is different from the bam file? I am making symlinks for both, then running make_examples like so:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. And getting this error:. `ValueError: Not found: No index found for bamlink`. A little strange, because the bam index in question is indeed in the same location as the bam file-- these are the linking commands:. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam bamlink. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai bailink. ```. So two questions:. 1) Why doesn't this work? 2) Is there a way to specify the index file location separately, or am I going to have to simply copy the two files into a local folder together at the working directory level. This would be somewhat of a pain because the bam is hundreds of GB. Thanks! Seems like there might be based on [this link] (https://cloud.google.com/genomics/docs/tutorials/deepvariant#additional_configuration_options). But I can't find the equivalent just for the make_examples section.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/149
https://github.com/google/deepvariant/issues/149:325,safety,Log,Log,325,"bam index location issue for make_examples; Hi (accidentally closed old issue prematurely). Is there an argument to specify a bam index file's location in the make_examples step if it is different from the bam file? I am making symlinks for both, then running make_examples like so:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. And getting this error:. `ValueError: Not found: No index found for bamlink`. A little strange, because the bam index in question is indeed in the same location as the bam file-- these are the linking commands:. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam bamlink. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai bailink. ```. So two questions:. 1) Why doesn't this work? 2) Is there a way to specify the index file location separately, or am I going to have to simply copy the two files into a local folder together at the working directory level. This would be somewhat of a pain because the bam is hundreds of GB. Thanks! Seems like there might be based on [this link] (https://cloud.google.com/genomics/docs/tutorials/deepvariant#additional_configuration_options). But I can't find the equivalent just for the make_examples section.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/149
https://github.com/google/deepvariant/issues/149:379,safety,log,log,379,"bam index location issue for make_examples; Hi (accidentally closed old issue prematurely). Is there an argument to specify a bam index file's location in the make_examples step if it is different from the bam file? I am making symlinks for both, then running make_examples like so:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. And getting this error:. `ValueError: Not found: No index found for bamlink`. A little strange, because the bam index in question is indeed in the same location as the bam file-- these are the linking commands:. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam bamlink. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai bailink. ```. So two questions:. 1) Why doesn't this work? 2) Is there a way to specify the index file location separately, or am I going to have to simply copy the two files into a local folder together at the working directory level. This would be somewhat of a pain because the bam is hundreds of GB. Thanks! Seems like there might be based on [this link] (https://cloud.google.com/genomics/docs/tutorials/deepvariant#additional_configuration_options). But I can't find the equivalent just for the make_examples section.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/149
https://github.com/google/deepvariant/issues/149:705,safety,log,log,705,"bam index location issue for make_examples; Hi (accidentally closed old issue prematurely). Is there an argument to specify a bam index file's location in the make_examples step if it is different from the bam file? I am making symlinks for both, then running make_examples like so:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. And getting this error:. `ValueError: Not found: No index found for bamlink`. A little strange, because the bam index in question is indeed in the same location as the bam file-- these are the linking commands:. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam bamlink. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai bailink. ```. So two questions:. 1) Why doesn't this work? 2) Is there a way to specify the index file location separately, or am I going to have to simply copy the two files into a local folder together at the working directory level. This would be somewhat of a pain because the bam is hundreds of GB. Thanks! Seems like there might be based on [this link] (https://cloud.google.com/genomics/docs/tutorials/deepvariant#additional_configuration_options). But I can't find the equivalent just for the make_examples section.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/149
https://github.com/google/deepvariant/issues/149:733,safety,error,error,733,"bam index location issue for make_examples; Hi (accidentally closed old issue prematurely). Is there an argument to specify a bam index file's location in the make_examples step if it is different from the bam file? I am making symlinks for both, then running make_examples like so:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. And getting this error:. `ValueError: Not found: No index found for bamlink`. A little strange, because the bam index in question is indeed in the same location as the bam file-- these are the linking commands:. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam bamlink. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai bailink. ```. So two questions:. 1) Why doesn't this work? 2) Is there a way to specify the index file location separately, or am I going to have to simply copy the two files into a local folder together at the working directory level. This would be somewhat of a pain because the bam is hundreds of GB. Thanks! Seems like there might be based on [this link] (https://cloud.google.com/genomics/docs/tutorials/deepvariant#additional_configuration_options). But I can't find the equivalent just for the make_examples section.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/149
https://github.com/google/deepvariant/issues/149:325,security,Log,Log,325,"bam index location issue for make_examples; Hi (accidentally closed old issue prematurely). Is there an argument to specify a bam index file's location in the make_examples step if it is different from the bam file? I am making symlinks for both, then running make_examples like so:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. And getting this error:. `ValueError: Not found: No index found for bamlink`. A little strange, because the bam index in question is indeed in the same location as the bam file-- these are the linking commands:. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam bamlink. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai bailink. ```. So two questions:. 1) Why doesn't this work? 2) Is there a way to specify the index file location separately, or am I going to have to simply copy the two files into a local folder together at the working directory level. This would be somewhat of a pain because the bam is hundreds of GB. Thanks! Seems like there might be based on [this link] (https://cloud.google.com/genomics/docs/tutorials/deepvariant#additional_configuration_options). But I can't find the equivalent just for the make_examples section.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/149
https://github.com/google/deepvariant/issues/149:379,security,log,log,379,"bam index location issue for make_examples; Hi (accidentally closed old issue prematurely). Is there an argument to specify a bam index file's location in the make_examples step if it is different from the bam file? I am making symlinks for both, then running make_examples like so:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. And getting this error:. `ValueError: Not found: No index found for bamlink`. A little strange, because the bam index in question is indeed in the same location as the bam file-- these are the linking commands:. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam bamlink. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai bailink. ```. So two questions:. 1) Why doesn't this work? 2) Is there a way to specify the index file location separately, or am I going to have to simply copy the two files into a local folder together at the working directory level. This would be somewhat of a pain because the bam is hundreds of GB. Thanks! Seems like there might be based on [this link] (https://cloud.google.com/genomics/docs/tutorials/deepvariant#additional_configuration_options). But I can't find the equivalent just for the make_examples section.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/149
https://github.com/google/deepvariant/issues/149:705,security,log,log,705,"bam index location issue for make_examples; Hi (accidentally closed old issue prematurely). Is there an argument to specify a bam index file's location in the make_examples step if it is different from the bam file? I am making symlinks for both, then running make_examples like so:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. And getting this error:. `ValueError: Not found: No index found for bamlink`. A little strange, because the bam index in question is indeed in the same location as the bam file-- these are the linking commands:. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam bamlink. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai bailink. ```. So two questions:. 1) Why doesn't this work? 2) Is there a way to specify the index file location separately, or am I going to have to simply copy the two files into a local folder together at the working directory level. This would be somewhat of a pain because the bam is hundreds of GB. Thanks! Seems like there might be based on [this link] (https://cloud.google.com/genomics/docs/tutorials/deepvariant#additional_configuration_options). But I can't find the equivalent just for the make_examples section.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/149
https://github.com/google/deepvariant/issues/149:325,testability,Log,Log,325,"bam index location issue for make_examples; Hi (accidentally closed old issue prematurely). Is there an argument to specify a bam index file's location in the make_examples step if it is different from the bam file? I am making symlinks for both, then running make_examples like so:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. And getting this error:. `ValueError: Not found: No index found for bamlink`. A little strange, because the bam index in question is indeed in the same location as the bam file-- these are the linking commands:. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam bamlink. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai bailink. ```. So two questions:. 1) Why doesn't this work? 2) Is there a way to specify the index file location separately, or am I going to have to simply copy the two files into a local folder together at the working directory level. This would be somewhat of a pain because the bam is hundreds of GB. Thanks! Seems like there might be based on [this link] (https://cloud.google.com/genomics/docs/tutorials/deepvariant#additional_configuration_options). But I can't find the equivalent just for the make_examples section.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/149
https://github.com/google/deepvariant/issues/149:379,testability,log,log,379,"bam index location issue for make_examples; Hi (accidentally closed old issue prematurely). Is there an argument to specify a bam index file's location in the make_examples step if it is different from the bam file? I am making symlinks for both, then running make_examples like so:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. And getting this error:. `ValueError: Not found: No index found for bamlink`. A little strange, because the bam index in question is indeed in the same location as the bam file-- these are the linking commands:. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam bamlink. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai bailink. ```. So two questions:. 1) Why doesn't this work? 2) Is there a way to specify the index file location separately, or am I going to have to simply copy the two files into a local folder together at the working directory level. This would be somewhat of a pain because the bam is hundreds of GB. Thanks! Seems like there might be based on [this link] (https://cloud.google.com/genomics/docs/tutorials/deepvariant#additional_configuration_options). But I can't find the equivalent just for the make_examples section.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/149
https://github.com/google/deepvariant/issues/149:705,testability,log,log,705,"bam index location issue for make_examples; Hi (accidentally closed old issue prematurely). Is there an argument to specify a bam index file's location in the make_examples step if it is different from the bam file? I am making symlinks for both, then running make_examples like so:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. And getting this error:. `ValueError: Not found: No index found for bamlink`. A little strange, because the bam index in question is indeed in the same location as the bam file-- these are the linking commands:. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam bamlink. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai bailink. ```. So two questions:. 1) Why doesn't this work? 2) Is there a way to specify the index file location separately, or am I going to have to simply copy the two files into a local folder together at the working directory level. This would be somewhat of a pain because the bam is hundreds of GB. Thanks! Seems like there might be based on [this link] (https://cloud.google.com/genomics/docs/tutorials/deepvariant#additional_configuration_options). But I can't find the equivalent just for the make_examples section.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/149
https://github.com/google/deepvariant/issues/149:1251,testability,simpl,simply,1251,"bam index location issue for make_examples; Hi (accidentally closed old issue prematurely). Is there an argument to specify a bam index file's location in the make_examples step if it is different from the bam file? I am making symlinks for both, then running make_examples like so:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. And getting this error:. `ValueError: Not found: No index found for bamlink`. A little strange, because the bam index in question is indeed in the same location as the bam file-- these are the linking commands:. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam bamlink. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai bailink. ```. So two questions:. 1) Why doesn't this work? 2) Is there a way to specify the index file location separately, or am I going to have to simply copy the two files into a local folder together at the working directory level. This would be somewhat of a pain because the bam is hundreds of GB. Thanks! Seems like there might be based on [this link] (https://cloud.google.com/genomics/docs/tutorials/deepvariant#additional_configuration_options). But I can't find the equivalent just for the make_examples section.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/149
https://github.com/google/deepvariant/issues/149:61,usability,close,closed,61,"bam index location issue for make_examples; Hi (accidentally closed old issue prematurely). Is there an argument to specify a bam index file's location in the make_examples step if it is different from the bam file? I am making symlinks for both, then running make_examples like so:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. And getting this error:. `ValueError: Not found: No index found for bamlink`. A little strange, because the bam index in question is indeed in the same location as the bam file-- these are the linking commands:. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam bamlink. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai bailink. ```. So two questions:. 1) Why doesn't this work? 2) Is there a way to specify the index file location separately, or am I going to have to simply copy the two files into a local folder together at the working directory level. This would be somewhat of a pain because the bam is hundreds of GB. Thanks! Seems like there might be based on [this link] (https://cloud.google.com/genomics/docs/tutorials/deepvariant#additional_configuration_options). But I can't find the equivalent just for the make_examples section.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/149
https://github.com/google/deepvariant/issues/149:733,usability,error,error,733,"bam index location issue for make_examples; Hi (accidentally closed old issue prematurely). Is there an argument to specify a bam index file's location in the make_examples step if it is different from the bam file? I am making symlinks for both, then running make_examples like so:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. And getting this error:. `ValueError: Not found: No index found for bamlink`. A little strange, because the bam index in question is indeed in the same location as the bam file-- these are the linking commands:. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam bamlink. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai bailink. ```. So two questions:. 1) Why doesn't this work? 2) Is there a way to specify the index file location separately, or am I going to have to simply copy the two files into a local folder together at the working directory level. This would be somewhat of a pain because the bam is hundreds of GB. Thanks! Seems like there might be based on [this link] (https://cloud.google.com/genomics/docs/tutorials/deepvariant#additional_configuration_options). But I can't find the equivalent just for the make_examples section.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/149
https://github.com/google/deepvariant/issues/149:917,usability,command,commands,917,"bam index location issue for make_examples; Hi (accidentally closed old issue prematurely). Is there an argument to specify a bam index file's location in the make_examples step if it is different from the bam file? I am making symlinks for both, then running make_examples like so:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. And getting this error:. `ValueError: Not found: No index found for bamlink`. A little strange, because the bam index in question is indeed in the same location as the bam file-- these are the linking commands:. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam bamlink. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai bailink. ```. So two questions:. 1) Why doesn't this work? 2) Is there a way to specify the index file location separately, or am I going to have to simply copy the two files into a local folder together at the working directory level. This would be somewhat of a pain because the bam is hundreds of GB. Thanks! Seems like there might be based on [this link] (https://cloud.google.com/genomics/docs/tutorials/deepvariant#additional_configuration_options). But I can't find the equivalent just for the make_examples section.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/149
https://github.com/google/deepvariant/issues/149:1251,usability,simpl,simply,1251,"bam index location issue for make_examples; Hi (accidentally closed old issue prematurely). Is there an argument to specify a bam index file's location in the make_examples step if it is different from the bam file? I am making symlinks for both, then running make_examples like so:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". ```. And getting this error:. `ValueError: Not found: No index found for bamlink`. A little strange, because the bam index in question is indeed in the same location as the bam file-- these are the linking commands:. ```. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam bamlink. + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai bailink. ```. So two questions:. 1) Why doesn't this work? 2) Is there a way to specify the index file location separately, or am I going to have to simply copy the two files into a local folder together at the working directory level. This would be somewhat of a pain because the bam is hundreds of GB. Thanks! Seems like there might be based on [this link] (https://cloud.google.com/genomics/docs/tutorials/deepvariant#additional_configuration_options). But I can't find the equivalent just for the make_examples section.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/149
https://github.com/google/deepvariant/issues/150:311,availability,echo,echo,311,"Strange make_examples lag when trying to use 64 cores and shards for WGS; Hi, with WGS (mm10, but the documentation says WGS human model should work for this), I got make_examples running fine with 4 cores and shards, but as soon as I go above 4 I get a strange lag. I'm running this recommended command:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. With 4, I get a nice stdout update per shard, first a description of the contigs:. `I0201 17:01:29.167016 140183113676544 make_examples.py:946] Common contigs are [u'chr1', u'chr10', u'chr11', u'chr12', u'chr13', u'chr14', u'chr15', u'chr16', u'chr17', u'chr18', u'chr19', u'chr1_GL456210_random', u'chr1_GL456211_random', u'chr1_GL456212_random', u'chr1_GL456213_random', u'chr1_GL456221_random', u'chr2', u'chr3', u'chr4', u'chr4_GL456216_random', u'chr4_JH584292_random', u'chr4_GL456350_random', u'chr4_JH584293_random', u'chr4_JH584294_random', u'chr4_JH584295_random', u'chr5', u'chr5_JH584296_random', u'chr5_JH584297_random', u'chr5_JH584298_random', u'chr5_GL456354_random', u'chr5_JH584299_random', u'chr6', u'chr7', u'chr7_GL456219_random', u'chr8', u'chr9', u'chrX', u'chrX_GL456233_random', u'chrY', u'chrY_JH584300_random', u'chrY_JH584301_random', u'chrY_JH584302_random', u'chrY_JH584303_random', u'chrUn_GL456239', u'chrUn_GL456367', u'chrUn_GL456378', u'chrUn_GL456381', u'chrUn_GL456382', u'chrUn_GL456383', u'chrUn_GL456385', u'chrUn_GL456390', u'chrUn_GL456392', u'chrUn_GL456393', u'chrUn_GL456394', u'chrUn_GL456359', u'chrUn_GL456360', u'chrUn_GL456396', u'chrUn_GL456372', u'chrUn_GL456387', u'chrUn_GL456389', u'chrU",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:733,availability,echo,echo,733,"Strange make_examples lag when trying to use 64 cores and shards for WGS; Hi, with WGS (mm10, but the documentation says WGS human model should work for this), I got make_examples running fine with 4 cores and shards, but as soon as I go above 4 I get a strange lag. I'm running this recommended command:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. With 4, I get a nice stdout update per shard, first a description of the contigs:. `I0201 17:01:29.167016 140183113676544 make_examples.py:946] Common contigs are [u'chr1', u'chr10', u'chr11', u'chr12', u'chr13', u'chr14', u'chr15', u'chr16', u'chr17', u'chr18', u'chr19', u'chr1_GL456210_random', u'chr1_GL456211_random', u'chr1_GL456212_random', u'chr1_GL456213_random', u'chr1_GL456221_random', u'chr2', u'chr3', u'chr4', u'chr4_GL456216_random', u'chr4_JH584292_random', u'chr4_GL456350_random', u'chr4_JH584293_random', u'chr4_JH584294_random', u'chr4_JH584295_random', u'chr5', u'chr5_JH584296_random', u'chr5_JH584297_random', u'chr5_JH584298_random', u'chr5_GL456354_random', u'chr5_JH584299_random', u'chr6', u'chr7', u'chr7_GL456219_random', u'chr8', u'chr9', u'chrX', u'chrX_GL456233_random', u'chrY', u'chrY_JH584300_random', u'chrY_JH584301_random', u'chrY_JH584302_random', u'chrY_JH584303_random', u'chrUn_GL456239', u'chrUn_GL456367', u'chrUn_GL456378', u'chrUn_GL456381', u'chrUn_GL456382', u'chrUn_GL456383', u'chrUn_GL456385', u'chrUn_GL456390', u'chrUn_GL456392', u'chrUn_GL456393', u'chrUn_GL456394', u'chrUn_GL456359', u'chrUn_GL456360', u'chrUn_GL456396', u'chrUn_GL456372', u'chrUn_GL456387', u'chrUn_GL456389', u'chrU",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:747,availability,echo,echo,747,"Strange make_examples lag when trying to use 64 cores and shards for WGS; Hi, with WGS (mm10, but the documentation says WGS human model should work for this), I got make_examples running fine with 4 cores and shards, but as soon as I go above 4 I get a strange lag. I'm running this recommended command:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. With 4, I get a nice stdout update per shard, first a description of the contigs:. `I0201 17:01:29.167016 140183113676544 make_examples.py:946] Common contigs are [u'chr1', u'chr10', u'chr11', u'chr12', u'chr13', u'chr14', u'chr15', u'chr16', u'chr17', u'chr18', u'chr19', u'chr1_GL456210_random', u'chr1_GL456211_random', u'chr1_GL456212_random', u'chr1_GL456213_random', u'chr1_GL456221_random', u'chr2', u'chr3', u'chr4', u'chr4_GL456216_random', u'chr4_JH584292_random', u'chr4_GL456350_random', u'chr4_JH584293_random', u'chr4_JH584294_random', u'chr4_JH584295_random', u'chr5', u'chr5_JH584296_random', u'chr5_JH584297_random', u'chr5_JH584298_random', u'chr5_GL456354_random', u'chr5_JH584299_random', u'chr6', u'chr7', u'chr7_GL456219_random', u'chr8', u'chr9', u'chrX', u'chrX_GL456233_random', u'chrY', u'chrY_JH584300_random', u'chrY_JH584301_random', u'chrY_JH584302_random', u'chrY_JH584303_random', u'chrUn_GL456239', u'chrUn_GL456367', u'chrUn_GL456378', u'chrUn_GL456381', u'chrUn_GL456382', u'chrUn_GL456383', u'chrUn_GL456385', u'chrUn_GL456390', u'chrUn_GL456392', u'chrUn_GL456393', u'chrUn_GL456394', u'chrUn_GL456359', u'chrUn_GL456360', u'chrUn_GL456396', u'chrUn_GL456372', u'chrUn_GL456387', u'chrUn_GL456389', u'chrU",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:5998,availability,slo,slow,5998,"didates in chr1:10001-11000 [0.01s elapsed]. I0201 17:01:49.757262 140183113676544 make_examples.py:782] Found 0 candidates in chr1:14001-15000 [0.01s elapsed]. I0201 17:01:49.769794 140183113676544 make_examples.py:782] Found 0 candidates in chr1:18001-19000 [0.01s elapsed]`. However, when I go up in number of shards and cores, even to just 8, let alone the recommended 64, I don't get past this point:. `Common contigs are [u'chr1', u'chr10', u'chr11', u'chr12', u'chr13', u'chr14', u'chr15', u'chr16', u'chr17', u'chr18', u'chr19', u'chr1_GL456210_random', u'chr1_GL456211_random', u'chr1_GL456212_random', u'chr1_GL456213_random', u'chr1_GL456221_random', u'chr2', u'chr3', u'chr4', u'chr4_GL456216_random', u'chr4_JH584292_random', u'chr4_GL456350_random', u'chr4_JH584293_random', u'chr4_JH584294_random', u'chr4_JH584295_random', u'chr5', u'chr5_JH584296_random', u'chr5_JH584297_random', u'chr5_JH584298_random', u'chr5_GL456354_random', u'chr5_JH584299_random', u'chr6', u'chr7', u'chr7_GL456219_random', u'chr8', u'chr9', u'chrX', u'chrX_GL456233_random', u'chrY', u'chrY_JH584300_random', u'chrY_JH584301_random', u'chrY_JH584302_random', u'chrY_JH584303_random', u'chrUn_GL456239', u'chrUn_GL456367', u'chrUn_GL456378', u'chrUn_GL456381', u'chrUn_GL456382', u'chrUn_GL456383', u'chrUn_GL456385', u'chrUn_GL456390', u'chrUn_GL456392', u'chrUn_GL456393', u'chrUn_GL456394', u'chrUn_GL456359', u'chrUn_GL456360', u'chrUn_GL456396', u'chrUn_GL456372', u'chrUn_GL456387', u'chrUn_GL456389', u'chrUn_GL456370', u'chrUn_GL456379', u'chrUn_GL456366', u'chrUn_GL456368', u'chrUn_JH584304']`. Then it just seems to lag, for up to 24 hours before I aborted the job because of cost concerns. Any ideas of what might be happening under the hood such that 4 cores and shards seems to run immediately with no problem, but things just freeze and don't advance with more? 4 would be far too slow and expensive unfortunately for WGS, but I'm not sure how to get past this. Thanks in advance for any help.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:347,deployability,Log,Log,347,"Strange make_examples lag when trying to use 64 cores and shards for WGS; Hi, with WGS (mm10, but the documentation says WGS human model should work for this), I got make_examples running fine with 4 cores and shards, but as soon as I go above 4 I get a strange lag. I'm running this recommended command:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. With 4, I get a nice stdout update per shard, first a description of the contigs:. `I0201 17:01:29.167016 140183113676544 make_examples.py:946] Common contigs are [u'chr1', u'chr10', u'chr11', u'chr12', u'chr13', u'chr14', u'chr15', u'chr16', u'chr17', u'chr18', u'chr19', u'chr1_GL456210_random', u'chr1_GL456211_random', u'chr1_GL456212_random', u'chr1_GL456213_random', u'chr1_GL456221_random', u'chr2', u'chr3', u'chr4', u'chr4_GL456216_random', u'chr4_JH584292_random', u'chr4_GL456350_random', u'chr4_JH584293_random', u'chr4_JH584294_random', u'chr4_JH584295_random', u'chr5', u'chr5_JH584296_random', u'chr5_JH584297_random', u'chr5_JH584298_random', u'chr5_GL456354_random', u'chr5_JH584299_random', u'chr6', u'chr7', u'chr7_GL456219_random', u'chr8', u'chr9', u'chrX', u'chrX_GL456233_random', u'chrY', u'chrY_JH584300_random', u'chrY_JH584301_random', u'chrY_JH584302_random', u'chrY_JH584303_random', u'chrUn_GL456239', u'chrUn_GL456367', u'chrUn_GL456378', u'chrUn_GL456381', u'chrUn_GL456382', u'chrUn_GL456383', u'chrUn_GL456385', u'chrUn_GL456390', u'chrUn_GL456392', u'chrUn_GL456393', u'chrUn_GL456394', u'chrUn_GL456359', u'chrUn_GL456360', u'chrUn_GL456396', u'chrUn_GL456372', u'chrUn_GL456387', u'chrUn_GL456389', u'chrU",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:401,deployability,log,log,401,"Strange make_examples lag when trying to use 64 cores and shards for WGS; Hi, with WGS (mm10, but the documentation says WGS human model should work for this), I got make_examples running fine with 4 cores and shards, but as soon as I go above 4 I get a strange lag. I'm running this recommended command:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. With 4, I get a nice stdout update per shard, first a description of the contigs:. `I0201 17:01:29.167016 140183113676544 make_examples.py:946] Common contigs are [u'chr1', u'chr10', u'chr11', u'chr12', u'chr13', u'chr14', u'chr15', u'chr16', u'chr17', u'chr18', u'chr19', u'chr1_GL456210_random', u'chr1_GL456211_random', u'chr1_GL456212_random', u'chr1_GL456213_random', u'chr1_GL456221_random', u'chr2', u'chr3', u'chr4', u'chr4_GL456216_random', u'chr4_JH584292_random', u'chr4_GL456350_random', u'chr4_JH584293_random', u'chr4_JH584294_random', u'chr4_JH584295_random', u'chr5', u'chr5_JH584296_random', u'chr5_JH584297_random', u'chr5_JH584298_random', u'chr5_GL456354_random', u'chr5_JH584299_random', u'chr6', u'chr7', u'chr7_GL456219_random', u'chr8', u'chr9', u'chrX', u'chrX_GL456233_random', u'chrY', u'chrY_JH584300_random', u'chrY_JH584301_random', u'chrY_JH584302_random', u'chrY_JH584303_random', u'chrUn_GL456239', u'chrUn_GL456367', u'chrUn_GL456378', u'chrUn_GL456381', u'chrUn_GL456382', u'chrUn_GL456383', u'chrUn_GL456385', u'chrUn_GL456390', u'chrUn_GL456392', u'chrUn_GL456393', u'chrUn_GL456394', u'chrUn_GL456359', u'chrUn_GL456360', u'chrUn_GL456396', u'chrUn_GL456372', u'chrUn_GL456387', u'chrUn_GL456389', u'chrU",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:727,deployability,log,log,727,"Strange make_examples lag when trying to use 64 cores and shards for WGS; Hi, with WGS (mm10, but the documentation says WGS human model should work for this), I got make_examples running fine with 4 cores and shards, but as soon as I go above 4 I get a strange lag. I'm running this recommended command:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. With 4, I get a nice stdout update per shard, first a description of the contigs:. `I0201 17:01:29.167016 140183113676544 make_examples.py:946] Common contigs are [u'chr1', u'chr10', u'chr11', u'chr12', u'chr13', u'chr14', u'chr15', u'chr16', u'chr17', u'chr18', u'chr19', u'chr1_GL456210_random', u'chr1_GL456211_random', u'chr1_GL456212_random', u'chr1_GL456213_random', u'chr1_GL456221_random', u'chr2', u'chr3', u'chr4', u'chr4_GL456216_random', u'chr4_JH584292_random', u'chr4_GL456350_random', u'chr4_JH584293_random', u'chr4_JH584294_random', u'chr4_JH584295_random', u'chr5', u'chr5_JH584296_random', u'chr5_JH584297_random', u'chr5_JH584298_random', u'chr5_GL456354_random', u'chr5_JH584299_random', u'chr6', u'chr7', u'chr7_GL456219_random', u'chr8', u'chr9', u'chrX', u'chrX_GL456233_random', u'chrY', u'chrY_JH584300_random', u'chrY_JH584301_random', u'chrY_JH584302_random', u'chrY_JH584303_random', u'chrUn_GL456239', u'chrUn_GL456367', u'chrUn_GL456378', u'chrUn_GL456381', u'chrUn_GL456382', u'chrUn_GL456383', u'chrUn_GL456385', u'chrUn_GL456390', u'chrUn_GL456392', u'chrUn_GL456393', u'chrUn_GL456394', u'chrUn_GL456359', u'chrUn_GL456360', u'chrUn_GL456396', u'chrUn_GL456372', u'chrUn_GL456387', u'chrUn_GL456389', u'chrU",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:786,deployability,updat,update,786,"Strange make_examples lag when trying to use 64 cores and shards for WGS; Hi, with WGS (mm10, but the documentation says WGS human model should work for this), I got make_examples running fine with 4 cores and shards, but as soon as I go above 4 I get a strange lag. I'm running this recommended command:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. With 4, I get a nice stdout update per shard, first a description of the contigs:. `I0201 17:01:29.167016 140183113676544 make_examples.py:946] Common contigs are [u'chr1', u'chr10', u'chr11', u'chr12', u'chr13', u'chr14', u'chr15', u'chr16', u'chr17', u'chr18', u'chr19', u'chr1_GL456210_random', u'chr1_GL456211_random', u'chr1_GL456212_random', u'chr1_GL456213_random', u'chr1_GL456221_random', u'chr2', u'chr3', u'chr4', u'chr4_GL456216_random', u'chr4_JH584292_random', u'chr4_GL456350_random', u'chr4_JH584293_random', u'chr4_JH584294_random', u'chr4_JH584295_random', u'chr5', u'chr5_JH584296_random', u'chr5_JH584297_random', u'chr5_JH584298_random', u'chr5_GL456354_random', u'chr5_JH584299_random', u'chr6', u'chr7', u'chr7_GL456219_random', u'chr8', u'chr9', u'chrX', u'chrX_GL456233_random', u'chrY', u'chrY_JH584300_random', u'chrY_JH584301_random', u'chrY_JH584302_random', u'chrY_JH584303_random', u'chrUn_GL456239', u'chrUn_GL456367', u'chrUn_GL456378', u'chrUn_GL456381', u'chrUn_GL456382', u'chrUn_GL456383', u'chrUn_GL456385', u'chrUn_GL456390', u'chrUn_GL456392', u'chrUn_GL456393', u'chrUn_GL456394', u'chrUn_GL456359', u'chrUn_GL456360', u'chrUn_GL456396', u'chrUn_GL456372', u'chrUn_GL456387', u'chrUn_GL456389', u'chrU",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:48,energy efficiency,core,cores,48,"Strange make_examples lag when trying to use 64 cores and shards for WGS; Hi, with WGS (mm10, but the documentation says WGS human model should work for this), I got make_examples running fine with 4 cores and shards, but as soon as I go above 4 I get a strange lag. I'm running this recommended command:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. With 4, I get a nice stdout update per shard, first a description of the contigs:. `I0201 17:01:29.167016 140183113676544 make_examples.py:946] Common contigs are [u'chr1', u'chr10', u'chr11', u'chr12', u'chr13', u'chr14', u'chr15', u'chr16', u'chr17', u'chr18', u'chr19', u'chr1_GL456210_random', u'chr1_GL456211_random', u'chr1_GL456212_random', u'chr1_GL456213_random', u'chr1_GL456221_random', u'chr2', u'chr3', u'chr4', u'chr4_GL456216_random', u'chr4_JH584292_random', u'chr4_GL456350_random', u'chr4_JH584293_random', u'chr4_JH584294_random', u'chr4_JH584295_random', u'chr5', u'chr5_JH584296_random', u'chr5_JH584297_random', u'chr5_JH584298_random', u'chr5_GL456354_random', u'chr5_JH584299_random', u'chr6', u'chr7', u'chr7_GL456219_random', u'chr8', u'chr9', u'chrX', u'chrX_GL456233_random', u'chrY', u'chrY_JH584300_random', u'chrY_JH584301_random', u'chrY_JH584302_random', u'chrY_JH584303_random', u'chrUn_GL456239', u'chrUn_GL456367', u'chrUn_GL456378', u'chrUn_GL456381', u'chrUn_GL456382', u'chrUn_GL456383', u'chrUn_GL456385', u'chrUn_GL456390', u'chrUn_GL456392', u'chrUn_GL456393', u'chrUn_GL456394', u'chrUn_GL456359', u'chrUn_GL456360', u'chrUn_GL456396', u'chrUn_GL456372', u'chrUn_GL456387', u'chrUn_GL456389', u'chrU",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:131,energy efficiency,model,model,131,"Strange make_examples lag when trying to use 64 cores and shards for WGS; Hi, with WGS (mm10, but the documentation says WGS human model should work for this), I got make_examples running fine with 4 cores and shards, but as soon as I go above 4 I get a strange lag. I'm running this recommended command:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. With 4, I get a nice stdout update per shard, first a description of the contigs:. `I0201 17:01:29.167016 140183113676544 make_examples.py:946] Common contigs are [u'chr1', u'chr10', u'chr11', u'chr12', u'chr13', u'chr14', u'chr15', u'chr16', u'chr17', u'chr18', u'chr19', u'chr1_GL456210_random', u'chr1_GL456211_random', u'chr1_GL456212_random', u'chr1_GL456213_random', u'chr1_GL456221_random', u'chr2', u'chr3', u'chr4', u'chr4_GL456216_random', u'chr4_JH584292_random', u'chr4_GL456350_random', u'chr4_JH584293_random', u'chr4_JH584294_random', u'chr4_JH584295_random', u'chr5', u'chr5_JH584296_random', u'chr5_JH584297_random', u'chr5_JH584298_random', u'chr5_GL456354_random', u'chr5_JH584299_random', u'chr6', u'chr7', u'chr7_GL456219_random', u'chr8', u'chr9', u'chrX', u'chrX_GL456233_random', u'chrY', u'chrY_JH584300_random', u'chrY_JH584301_random', u'chrY_JH584302_random', u'chrY_JH584303_random', u'chrUn_GL456239', u'chrUn_GL456367', u'chrUn_GL456378', u'chrUn_GL456381', u'chrUn_GL456382', u'chrUn_GL456383', u'chrUn_GL456385', u'chrUn_GL456390', u'chrUn_GL456392', u'chrUn_GL456393', u'chrUn_GL456394', u'chrUn_GL456359', u'chrUn_GL456360', u'chrUn_GL456396', u'chrUn_GL456372', u'chrUn_GL456387', u'chrUn_GL456389', u'chrU",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:200,energy efficiency,core,cores,200,"Strange make_examples lag when trying to use 64 cores and shards for WGS; Hi, with WGS (mm10, but the documentation says WGS human model should work for this), I got make_examples running fine with 4 cores and shards, but as soon as I go above 4 I get a strange lag. I'm running this recommended command:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. With 4, I get a nice stdout update per shard, first a description of the contigs:. `I0201 17:01:29.167016 140183113676544 make_examples.py:946] Common contigs are [u'chr1', u'chr10', u'chr11', u'chr12', u'chr13', u'chr14', u'chr15', u'chr16', u'chr17', u'chr18', u'chr19', u'chr1_GL456210_random', u'chr1_GL456211_random', u'chr1_GL456212_random', u'chr1_GL456213_random', u'chr1_GL456221_random', u'chr2', u'chr3', u'chr4', u'chr4_GL456216_random', u'chr4_JH584292_random', u'chr4_GL456350_random', u'chr4_JH584293_random', u'chr4_JH584294_random', u'chr4_JH584295_random', u'chr5', u'chr5_JH584296_random', u'chr5_JH584297_random', u'chr5_JH584298_random', u'chr5_GL456354_random', u'chr5_JH584299_random', u'chr6', u'chr7', u'chr7_GL456219_random', u'chr8', u'chr9', u'chrX', u'chrX_GL456233_random', u'chrY', u'chrY_JH584300_random', u'chrY_JH584301_random', u'chrY_JH584302_random', u'chrY_JH584303_random', u'chrUn_GL456239', u'chrUn_GL456367', u'chrUn_GL456378', u'chrUn_GL456381', u'chrUn_GL456382', u'chrUn_GL456383', u'chrUn_GL456385', u'chrUn_GL456390', u'chrUn_GL456392', u'chrUn_GL456393', u'chrUn_GL456394', u'chrUn_GL456359', u'chrUn_GL456360', u'chrUn_GL456396', u'chrUn_GL456372', u'chrUn_GL456387', u'chrUn_GL456389', u'chrU",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:4434,energy efficiency,core,cores,4434,"JN_G2701-1_v2_WGS_OnPrem.examples.tfrecord-00000-of-00004.gz. I0201 17:01:44.952527 140546703472384 make_examples.py:1034] Writing gvcf records to RP-1735_JN_G2701-1_v2_WGS_OnPrem.gvcf.tfrecord-00000-of-00004.gz. 2019-02-01 17:01:44.953547: I third_party/nucleus/io/sam_reader.cc:565] Setting HTS_OPT_BLOCK_SIZE to 134217728. ```. It then starts outputting per site info:. `I0201 17:01:49.720201 140183113676544 make_examples.py:782] Found 0 candidates in chr1:2001-3000 [4.82s elapsed]. I0201 17:01:49.732466 140183113676544 make_examples.py:782] Found 0 candidates in chr1:6001-7000 [0.01s elapsed]. I0201 17:01:49.744833 140183113676544 make_examples.py:782] Found 0 candidates in chr1:10001-11000 [0.01s elapsed]. I0201 17:01:49.757262 140183113676544 make_examples.py:782] Found 0 candidates in chr1:14001-15000 [0.01s elapsed]. I0201 17:01:49.769794 140183113676544 make_examples.py:782] Found 0 candidates in chr1:18001-19000 [0.01s elapsed]`. However, when I go up in number of shards and cores, even to just 8, let alone the recommended 64, I don't get past this point:. `Common contigs are [u'chr1', u'chr10', u'chr11', u'chr12', u'chr13', u'chr14', u'chr15', u'chr16', u'chr17', u'chr18', u'chr19', u'chr1_GL456210_random', u'chr1_GL456211_random', u'chr1_GL456212_random', u'chr1_GL456213_random', u'chr1_GL456221_random', u'chr2', u'chr3', u'chr4', u'chr4_GL456216_random', u'chr4_JH584292_random', u'chr4_GL456350_random', u'chr4_JH584293_random', u'chr4_JH584294_random', u'chr4_JH584295_random', u'chr5', u'chr5_JH584296_random', u'chr5_JH584297_random', u'chr5_JH584298_random', u'chr5_GL456354_random', u'chr5_JH584299_random', u'chr6', u'chr7', u'chr7_GL456219_random', u'chr8', u'chr9', u'chrX', u'chrX_GL456233_random', u'chrY', u'chrY_JH584300_random', u'chrY_JH584301_random', u'chrY_JH584302_random', u'chrY_JH584303_random', u'chrUn_GL456239', u'chrUn_GL456367', u'chrUn_GL456378', u'chrUn_GL456381', u'chrUn_GL456382', u'chrUn_GL456383', u'chrUn_GL456385', u'chrUn_GL456390'",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:5868,energy efficiency,core,cores,5868,"didates in chr1:10001-11000 [0.01s elapsed]. I0201 17:01:49.757262 140183113676544 make_examples.py:782] Found 0 candidates in chr1:14001-15000 [0.01s elapsed]. I0201 17:01:49.769794 140183113676544 make_examples.py:782] Found 0 candidates in chr1:18001-19000 [0.01s elapsed]`. However, when I go up in number of shards and cores, even to just 8, let alone the recommended 64, I don't get past this point:. `Common contigs are [u'chr1', u'chr10', u'chr11', u'chr12', u'chr13', u'chr14', u'chr15', u'chr16', u'chr17', u'chr18', u'chr19', u'chr1_GL456210_random', u'chr1_GL456211_random', u'chr1_GL456212_random', u'chr1_GL456213_random', u'chr1_GL456221_random', u'chr2', u'chr3', u'chr4', u'chr4_GL456216_random', u'chr4_JH584292_random', u'chr4_GL456350_random', u'chr4_JH584293_random', u'chr4_JH584294_random', u'chr4_JH584295_random', u'chr5', u'chr5_JH584296_random', u'chr5_JH584297_random', u'chr5_JH584298_random', u'chr5_GL456354_random', u'chr5_JH584299_random', u'chr6', u'chr7', u'chr7_GL456219_random', u'chr8', u'chr9', u'chrX', u'chrX_GL456233_random', u'chrY', u'chrY_JH584300_random', u'chrY_JH584301_random', u'chrY_JH584302_random', u'chrY_JH584303_random', u'chrUn_GL456239', u'chrUn_GL456367', u'chrUn_GL456378', u'chrUn_GL456381', u'chrUn_GL456382', u'chrUn_GL456383', u'chrUn_GL456385', u'chrUn_GL456390', u'chrUn_GL456392', u'chrUn_GL456393', u'chrUn_GL456394', u'chrUn_GL456359', u'chrUn_GL456360', u'chrUn_GL456396', u'chrUn_GL456372', u'chrUn_GL456387', u'chrUn_GL456389', u'chrUn_GL456370', u'chrUn_GL456379', u'chrUn_GL456366', u'chrUn_GL456368', u'chrUn_JH584304']`. Then it just seems to lag, for up to 24 hours before I aborted the job because of cost concerns. Any ideas of what might be happening under the hood such that 4 cores and shards seems to run immediately with no problem, but things just freeze and don't advance with more? 4 would be far too slow and expensive unfortunately for WGS, but I'm not sure how to get past this. Thanks in advance for any help.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:465,integrability,buffer,buffer,465,"Strange make_examples lag when trying to use 64 cores and shards for WGS; Hi, with WGS (mm10, but the documentation says WGS human model should work for this), I got make_examples running fine with 4 cores and shards, but as soon as I go above 4 I get a strange lag. I'm running this recommended command:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. With 4, I get a nice stdout update per shard, first a description of the contigs:. `I0201 17:01:29.167016 140183113676544 make_examples.py:946] Common contigs are [u'chr1', u'chr10', u'chr11', u'chr12', u'chr13', u'chr14', u'chr15', u'chr16', u'chr17', u'chr18', u'chr19', u'chr1_GL456210_random', u'chr1_GL456211_random', u'chr1_GL456212_random', u'chr1_GL456213_random', u'chr1_GL456221_random', u'chr2', u'chr3', u'chr4', u'chr4_GL456216_random', u'chr4_JH584292_random', u'chr4_GL456350_random', u'chr4_JH584293_random', u'chr4_JH584294_random', u'chr4_JH584295_random', u'chr5', u'chr5_JH584296_random', u'chr5_JH584297_random', u'chr5_JH584298_random', u'chr5_GL456354_random', u'chr5_JH584299_random', u'chr6', u'chr7', u'chr7_GL456219_random', u'chr8', u'chr9', u'chrX', u'chrX_GL456233_random', u'chrY', u'chrY_JH584300_random', u'chrY_JH584301_random', u'chrY_JH584302_random', u'chrY_JH584303_random', u'chrUn_GL456239', u'chrUn_GL456367', u'chrUn_GL456378', u'chrUn_GL456381', u'chrUn_GL456382', u'chrUn_GL456383', u'chrUn_GL456385', u'chrUn_GL456390', u'chrUn_GL456392', u'chrUn_GL456393', u'chrUn_GL456394', u'chrUn_GL456359', u'chrUn_GL456360', u'chrUn_GL456396', u'chrUn_GL456372', u'chrUn_GL456387', u'chrUn_GL456389', u'chrU",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:5794,modifiability,concern,concerns,5794,"didates in chr1:10001-11000 [0.01s elapsed]. I0201 17:01:49.757262 140183113676544 make_examples.py:782] Found 0 candidates in chr1:14001-15000 [0.01s elapsed]. I0201 17:01:49.769794 140183113676544 make_examples.py:782] Found 0 candidates in chr1:18001-19000 [0.01s elapsed]`. However, when I go up in number of shards and cores, even to just 8, let alone the recommended 64, I don't get past this point:. `Common contigs are [u'chr1', u'chr10', u'chr11', u'chr12', u'chr13', u'chr14', u'chr15', u'chr16', u'chr17', u'chr18', u'chr19', u'chr1_GL456210_random', u'chr1_GL456211_random', u'chr1_GL456212_random', u'chr1_GL456213_random', u'chr1_GL456221_random', u'chr2', u'chr3', u'chr4', u'chr4_GL456216_random', u'chr4_JH584292_random', u'chr4_GL456350_random', u'chr4_JH584293_random', u'chr4_JH584294_random', u'chr4_JH584295_random', u'chr5', u'chr5_JH584296_random', u'chr5_JH584297_random', u'chr5_JH584298_random', u'chr5_GL456354_random', u'chr5_JH584299_random', u'chr6', u'chr7', u'chr7_GL456219_random', u'chr8', u'chr9', u'chrX', u'chrX_GL456233_random', u'chrY', u'chrY_JH584300_random', u'chrY_JH584301_random', u'chrY_JH584302_random', u'chrY_JH584303_random', u'chrUn_GL456239', u'chrUn_GL456367', u'chrUn_GL456378', u'chrUn_GL456381', u'chrUn_GL456382', u'chrUn_GL456383', u'chrUn_GL456385', u'chrUn_GL456390', u'chrUn_GL456392', u'chrUn_GL456393', u'chrUn_GL456394', u'chrUn_GL456359', u'chrUn_GL456360', u'chrUn_GL456396', u'chrUn_GL456372', u'chrUn_GL456387', u'chrUn_GL456389', u'chrUn_GL456370', u'chrUn_GL456379', u'chrUn_GL456366', u'chrUn_GL456368', u'chrUn_JH584304']`. Then it just seems to lag, for up to 24 hours before I aborted the job because of cost concerns. Any ideas of what might be happening under the hood such that 4 cores and shards seems to run immediately with no problem, but things just freeze and don't advance with more? 4 would be far too slow and expensive unfortunately for WGS, but I'm not sure how to get past this. Thanks in advance for any help.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:410,performance,time,time,410,"Strange make_examples lag when trying to use 64 cores and shards for WGS; Hi, with WGS (mm10, but the documentation says WGS human model should work for this), I got make_examples running fine with 4 cores and shards, but as soon as I go above 4 I get a strange lag. I'm running this recommended command:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. With 4, I get a nice stdout update per shard, first a description of the contigs:. `I0201 17:01:29.167016 140183113676544 make_examples.py:946] Common contigs are [u'chr1', u'chr10', u'chr11', u'chr12', u'chr13', u'chr14', u'chr15', u'chr16', u'chr17', u'chr18', u'chr19', u'chr1_GL456210_random', u'chr1_GL456211_random', u'chr1_GL456212_random', u'chr1_GL456213_random', u'chr1_GL456221_random', u'chr2', u'chr3', u'chr4', u'chr4_GL456216_random', u'chr4_JH584292_random', u'chr4_GL456350_random', u'chr4_JH584293_random', u'chr4_JH584294_random', u'chr4_JH584295_random', u'chr5', u'chr5_JH584296_random', u'chr5_JH584297_random', u'chr5_JH584298_random', u'chr5_GL456354_random', u'chr5_JH584299_random', u'chr6', u'chr7', u'chr7_GL456219_random', u'chr8', u'chr9', u'chrX', u'chrX_GL456233_random', u'chrY', u'chrY_JH584300_random', u'chrY_JH584301_random', u'chrY_JH584302_random', u'chrY_JH584303_random', u'chrUn_GL456239', u'chrUn_GL456367', u'chrUn_GL456378', u'chrUn_GL456381', u'chrUn_GL456382', u'chrUn_GL456383', u'chrUn_GL456385', u'chrUn_GL456390', u'chrUn_GL456392', u'chrUn_GL456393', u'chrUn_GL456394', u'chrUn_GL456359', u'chrUn_GL456360', u'chrUn_GL456396', u'chrUn_GL456372', u'chrUn_GL456387', u'chrUn_GL456389', u'chrU",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:446,performance,parallel,parallel,446,"Strange make_examples lag when trying to use 64 cores and shards for WGS; Hi, with WGS (mm10, but the documentation says WGS human model should work for this), I got make_examples running fine with 4 cores and shards, but as soon as I go above 4 I get a strange lag. I'm running this recommended command:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. With 4, I get a nice stdout update per shard, first a description of the contigs:. `I0201 17:01:29.167016 140183113676544 make_examples.py:946] Common contigs are [u'chr1', u'chr10', u'chr11', u'chr12', u'chr13', u'chr14', u'chr15', u'chr16', u'chr17', u'chr18', u'chr19', u'chr1_GL456210_random', u'chr1_GL456211_random', u'chr1_GL456212_random', u'chr1_GL456213_random', u'chr1_GL456221_random', u'chr2', u'chr3', u'chr4', u'chr4_GL456216_random', u'chr4_JH584292_random', u'chr4_GL456350_random', u'chr4_JH584293_random', u'chr4_JH584294_random', u'chr4_JH584295_random', u'chr5', u'chr5_JH584296_random', u'chr5_JH584297_random', u'chr5_JH584298_random', u'chr5_GL456354_random', u'chr5_JH584299_random', u'chr6', u'chr7', u'chr7_GL456219_random', u'chr8', u'chr9', u'chrX', u'chrX_GL456233_random', u'chrY', u'chrY_JH584300_random', u'chrY_JH584301_random', u'chrY_JH584302_random', u'chrY_JH584303_random', u'chrUn_GL456239', u'chrUn_GL456367', u'chrUn_GL456378', u'chrUn_GL456381', u'chrUn_GL456382', u'chrUn_GL456383', u'chrUn_GL456385', u'chrUn_GL456390', u'chrUn_GL456392', u'chrUn_GL456393', u'chrUn_GL456394', u'chrUn_GL456359', u'chrUn_GL456360', u'chrUn_GL456396', u'chrUn_GL456372', u'chrUn_GL456387', u'chrUn_GL456389', u'chrU",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:5998,reliability,slo,slow,5998,"didates in chr1:10001-11000 [0.01s elapsed]. I0201 17:01:49.757262 140183113676544 make_examples.py:782] Found 0 candidates in chr1:14001-15000 [0.01s elapsed]. I0201 17:01:49.769794 140183113676544 make_examples.py:782] Found 0 candidates in chr1:18001-19000 [0.01s elapsed]`. However, when I go up in number of shards and cores, even to just 8, let alone the recommended 64, I don't get past this point:. `Common contigs are [u'chr1', u'chr10', u'chr11', u'chr12', u'chr13', u'chr14', u'chr15', u'chr16', u'chr17', u'chr18', u'chr19', u'chr1_GL456210_random', u'chr1_GL456211_random', u'chr1_GL456212_random', u'chr1_GL456213_random', u'chr1_GL456221_random', u'chr2', u'chr3', u'chr4', u'chr4_GL456216_random', u'chr4_JH584292_random', u'chr4_GL456350_random', u'chr4_JH584293_random', u'chr4_JH584294_random', u'chr4_JH584295_random', u'chr5', u'chr5_JH584296_random', u'chr5_JH584297_random', u'chr5_JH584298_random', u'chr5_GL456354_random', u'chr5_JH584299_random', u'chr6', u'chr7', u'chr7_GL456219_random', u'chr8', u'chr9', u'chrX', u'chrX_GL456233_random', u'chrY', u'chrY_JH584300_random', u'chrY_JH584301_random', u'chrY_JH584302_random', u'chrY_JH584303_random', u'chrUn_GL456239', u'chrUn_GL456367', u'chrUn_GL456378', u'chrUn_GL456381', u'chrUn_GL456382', u'chrUn_GL456383', u'chrUn_GL456385', u'chrUn_GL456390', u'chrUn_GL456392', u'chrUn_GL456393', u'chrUn_GL456394', u'chrUn_GL456359', u'chrUn_GL456360', u'chrUn_GL456396', u'chrUn_GL456372', u'chrUn_GL456387', u'chrUn_GL456389', u'chrUn_GL456370', u'chrUn_GL456379', u'chrUn_GL456366', u'chrUn_GL456368', u'chrUn_JH584304']`. Then it just seems to lag, for up to 24 hours before I aborted the job because of cost concerns. Any ideas of what might be happening under the hood such that 4 cores and shards seems to run immediately with no problem, but things just freeze and don't advance with more? 4 would be far too slow and expensive unfortunately for WGS, but I'm not sure how to get past this. Thanks in advance for any help.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:347,safety,Log,Log,347,"Strange make_examples lag when trying to use 64 cores and shards for WGS; Hi, with WGS (mm10, but the documentation says WGS human model should work for this), I got make_examples running fine with 4 cores and shards, but as soon as I go above 4 I get a strange lag. I'm running this recommended command:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. With 4, I get a nice stdout update per shard, first a description of the contigs:. `I0201 17:01:29.167016 140183113676544 make_examples.py:946] Common contigs are [u'chr1', u'chr10', u'chr11', u'chr12', u'chr13', u'chr14', u'chr15', u'chr16', u'chr17', u'chr18', u'chr19', u'chr1_GL456210_random', u'chr1_GL456211_random', u'chr1_GL456212_random', u'chr1_GL456213_random', u'chr1_GL456221_random', u'chr2', u'chr3', u'chr4', u'chr4_GL456216_random', u'chr4_JH584292_random', u'chr4_GL456350_random', u'chr4_JH584293_random', u'chr4_JH584294_random', u'chr4_JH584295_random', u'chr5', u'chr5_JH584296_random', u'chr5_JH584297_random', u'chr5_JH584298_random', u'chr5_GL456354_random', u'chr5_JH584299_random', u'chr6', u'chr7', u'chr7_GL456219_random', u'chr8', u'chr9', u'chrX', u'chrX_GL456233_random', u'chrY', u'chrY_JH584300_random', u'chrY_JH584301_random', u'chrY_JH584302_random', u'chrY_JH584303_random', u'chrUn_GL456239', u'chrUn_GL456367', u'chrUn_GL456378', u'chrUn_GL456381', u'chrUn_GL456382', u'chrUn_GL456383', u'chrUn_GL456385', u'chrUn_GL456390', u'chrUn_GL456392', u'chrUn_GL456393', u'chrUn_GL456394', u'chrUn_GL456359', u'chrUn_GL456360', u'chrUn_GL456396', u'chrUn_GL456372', u'chrUn_GL456387', u'chrUn_GL456389', u'chrU",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:401,safety,log,log,401,"Strange make_examples lag when trying to use 64 cores and shards for WGS; Hi, with WGS (mm10, but the documentation says WGS human model should work for this), I got make_examples running fine with 4 cores and shards, but as soon as I go above 4 I get a strange lag. I'm running this recommended command:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. With 4, I get a nice stdout update per shard, first a description of the contigs:. `I0201 17:01:29.167016 140183113676544 make_examples.py:946] Common contigs are [u'chr1', u'chr10', u'chr11', u'chr12', u'chr13', u'chr14', u'chr15', u'chr16', u'chr17', u'chr18', u'chr19', u'chr1_GL456210_random', u'chr1_GL456211_random', u'chr1_GL456212_random', u'chr1_GL456213_random', u'chr1_GL456221_random', u'chr2', u'chr3', u'chr4', u'chr4_GL456216_random', u'chr4_JH584292_random', u'chr4_GL456350_random', u'chr4_JH584293_random', u'chr4_JH584294_random', u'chr4_JH584295_random', u'chr5', u'chr5_JH584296_random', u'chr5_JH584297_random', u'chr5_JH584298_random', u'chr5_GL456354_random', u'chr5_JH584299_random', u'chr6', u'chr7', u'chr7_GL456219_random', u'chr8', u'chr9', u'chrX', u'chrX_GL456233_random', u'chrY', u'chrY_JH584300_random', u'chrY_JH584301_random', u'chrY_JH584302_random', u'chrY_JH584303_random', u'chrUn_GL456239', u'chrUn_GL456367', u'chrUn_GL456378', u'chrUn_GL456381', u'chrUn_GL456382', u'chrUn_GL456383', u'chrUn_GL456385', u'chrUn_GL456390', u'chrUn_GL456392', u'chrUn_GL456393', u'chrUn_GL456394', u'chrUn_GL456359', u'chrUn_GL456360', u'chrUn_GL456396', u'chrUn_GL456372', u'chrUn_GL456387', u'chrUn_GL456389', u'chrU",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:727,safety,log,log,727,"Strange make_examples lag when trying to use 64 cores and shards for WGS; Hi, with WGS (mm10, but the documentation says WGS human model should work for this), I got make_examples running fine with 4 cores and shards, but as soon as I go above 4 I get a strange lag. I'm running this recommended command:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. With 4, I get a nice stdout update per shard, first a description of the contigs:. `I0201 17:01:29.167016 140183113676544 make_examples.py:946] Common contigs are [u'chr1', u'chr10', u'chr11', u'chr12', u'chr13', u'chr14', u'chr15', u'chr16', u'chr17', u'chr18', u'chr19', u'chr1_GL456210_random', u'chr1_GL456211_random', u'chr1_GL456212_random', u'chr1_GL456213_random', u'chr1_GL456221_random', u'chr2', u'chr3', u'chr4', u'chr4_GL456216_random', u'chr4_JH584292_random', u'chr4_GL456350_random', u'chr4_JH584293_random', u'chr4_JH584294_random', u'chr4_JH584295_random', u'chr5', u'chr5_JH584296_random', u'chr5_JH584297_random', u'chr5_JH584298_random', u'chr5_GL456354_random', u'chr5_JH584299_random', u'chr6', u'chr7', u'chr7_GL456219_random', u'chr8', u'chr9', u'chrX', u'chrX_GL456233_random', u'chrY', u'chrY_JH584300_random', u'chrY_JH584301_random', u'chrY_JH584302_random', u'chrY_JH584303_random', u'chrUn_GL456239', u'chrUn_GL456367', u'chrUn_GL456378', u'chrUn_GL456381', u'chrUn_GL456382', u'chrUn_GL456383', u'chrUn_GL456385', u'chrUn_GL456390', u'chrUn_GL456392', u'chrUn_GL456393', u'chrUn_GL456394', u'chrUn_GL456359', u'chrUn_GL456360', u'chrUn_GL456396', u'chrUn_GL456372', u'chrUn_GL456387', u'chrUn_GL456389', u'chrU",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:786,safety,updat,update,786,"Strange make_examples lag when trying to use 64 cores and shards for WGS; Hi, with WGS (mm10, but the documentation says WGS human model should work for this), I got make_examples running fine with 4 cores and shards, but as soon as I go above 4 I get a strange lag. I'm running this recommended command:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. With 4, I get a nice stdout update per shard, first a description of the contigs:. `I0201 17:01:29.167016 140183113676544 make_examples.py:946] Common contigs are [u'chr1', u'chr10', u'chr11', u'chr12', u'chr13', u'chr14', u'chr15', u'chr16', u'chr17', u'chr18', u'chr19', u'chr1_GL456210_random', u'chr1_GL456211_random', u'chr1_GL456212_random', u'chr1_GL456213_random', u'chr1_GL456221_random', u'chr2', u'chr3', u'chr4', u'chr4_GL456216_random', u'chr4_JH584292_random', u'chr4_GL456350_random', u'chr4_JH584293_random', u'chr4_JH584294_random', u'chr4_JH584295_random', u'chr5', u'chr5_JH584296_random', u'chr5_JH584297_random', u'chr5_JH584298_random', u'chr5_GL456354_random', u'chr5_JH584299_random', u'chr6', u'chr7', u'chr7_GL456219_random', u'chr8', u'chr9', u'chrX', u'chrX_GL456233_random', u'chrY', u'chrY_JH584300_random', u'chrY_JH584301_random', u'chrY_JH584302_random', u'chrY_JH584303_random', u'chrUn_GL456239', u'chrUn_GL456367', u'chrUn_GL456378', u'chrUn_GL456381', u'chrUn_GL456382', u'chrUn_GL456383', u'chrUn_GL456385', u'chrUn_GL456390', u'chrUn_GL456392', u'chrUn_GL456393', u'chrUn_GL456394', u'chrUn_GL456359', u'chrUn_GL456360', u'chrUn_GL456396', u'chrUn_GL456372', u'chrUn_GL456387', u'chrUn_GL456389', u'chrU",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:131,security,model,model,131,"Strange make_examples lag when trying to use 64 cores and shards for WGS; Hi, with WGS (mm10, but the documentation says WGS human model should work for this), I got make_examples running fine with 4 cores and shards, but as soon as I go above 4 I get a strange lag. I'm running this recommended command:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. With 4, I get a nice stdout update per shard, first a description of the contigs:. `I0201 17:01:29.167016 140183113676544 make_examples.py:946] Common contigs are [u'chr1', u'chr10', u'chr11', u'chr12', u'chr13', u'chr14', u'chr15', u'chr16', u'chr17', u'chr18', u'chr19', u'chr1_GL456210_random', u'chr1_GL456211_random', u'chr1_GL456212_random', u'chr1_GL456213_random', u'chr1_GL456221_random', u'chr2', u'chr3', u'chr4', u'chr4_GL456216_random', u'chr4_JH584292_random', u'chr4_GL456350_random', u'chr4_JH584293_random', u'chr4_JH584294_random', u'chr4_JH584295_random', u'chr5', u'chr5_JH584296_random', u'chr5_JH584297_random', u'chr5_JH584298_random', u'chr5_GL456354_random', u'chr5_JH584299_random', u'chr6', u'chr7', u'chr7_GL456219_random', u'chr8', u'chr9', u'chrX', u'chrX_GL456233_random', u'chrY', u'chrY_JH584300_random', u'chrY_JH584301_random', u'chrY_JH584302_random', u'chrY_JH584303_random', u'chrUn_GL456239', u'chrUn_GL456367', u'chrUn_GL456378', u'chrUn_GL456381', u'chrUn_GL456382', u'chrUn_GL456383', u'chrUn_GL456385', u'chrUn_GL456390', u'chrUn_GL456392', u'chrUn_GL456393', u'chrUn_GL456394', u'chrUn_GL456359', u'chrUn_GL456360', u'chrUn_GL456396', u'chrUn_GL456372', u'chrUn_GL456387', u'chrUn_GL456389', u'chrU",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:347,security,Log,Log,347,"Strange make_examples lag when trying to use 64 cores and shards for WGS; Hi, with WGS (mm10, but the documentation says WGS human model should work for this), I got make_examples running fine with 4 cores and shards, but as soon as I go above 4 I get a strange lag. I'm running this recommended command:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. With 4, I get a nice stdout update per shard, first a description of the contigs:. `I0201 17:01:29.167016 140183113676544 make_examples.py:946] Common contigs are [u'chr1', u'chr10', u'chr11', u'chr12', u'chr13', u'chr14', u'chr15', u'chr16', u'chr17', u'chr18', u'chr19', u'chr1_GL456210_random', u'chr1_GL456211_random', u'chr1_GL456212_random', u'chr1_GL456213_random', u'chr1_GL456221_random', u'chr2', u'chr3', u'chr4', u'chr4_GL456216_random', u'chr4_JH584292_random', u'chr4_GL456350_random', u'chr4_JH584293_random', u'chr4_JH584294_random', u'chr4_JH584295_random', u'chr5', u'chr5_JH584296_random', u'chr5_JH584297_random', u'chr5_JH584298_random', u'chr5_GL456354_random', u'chr5_JH584299_random', u'chr6', u'chr7', u'chr7_GL456219_random', u'chr8', u'chr9', u'chrX', u'chrX_GL456233_random', u'chrY', u'chrY_JH584300_random', u'chrY_JH584301_random', u'chrY_JH584302_random', u'chrY_JH584303_random', u'chrUn_GL456239', u'chrUn_GL456367', u'chrUn_GL456378', u'chrUn_GL456381', u'chrUn_GL456382', u'chrUn_GL456383', u'chrUn_GL456385', u'chrUn_GL456390', u'chrUn_GL456392', u'chrUn_GL456393', u'chrUn_GL456394', u'chrUn_GL456359', u'chrUn_GL456360', u'chrUn_GL456396', u'chrUn_GL456372', u'chrUn_GL456387', u'chrUn_GL456389', u'chrU",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:401,security,log,log,401,"Strange make_examples lag when trying to use 64 cores and shards for WGS; Hi, with WGS (mm10, but the documentation says WGS human model should work for this), I got make_examples running fine with 4 cores and shards, but as soon as I go above 4 I get a strange lag. I'm running this recommended command:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. With 4, I get a nice stdout update per shard, first a description of the contigs:. `I0201 17:01:29.167016 140183113676544 make_examples.py:946] Common contigs are [u'chr1', u'chr10', u'chr11', u'chr12', u'chr13', u'chr14', u'chr15', u'chr16', u'chr17', u'chr18', u'chr19', u'chr1_GL456210_random', u'chr1_GL456211_random', u'chr1_GL456212_random', u'chr1_GL456213_random', u'chr1_GL456221_random', u'chr2', u'chr3', u'chr4', u'chr4_GL456216_random', u'chr4_JH584292_random', u'chr4_GL456350_random', u'chr4_JH584293_random', u'chr4_JH584294_random', u'chr4_JH584295_random', u'chr5', u'chr5_JH584296_random', u'chr5_JH584297_random', u'chr5_JH584298_random', u'chr5_GL456354_random', u'chr5_JH584299_random', u'chr6', u'chr7', u'chr7_GL456219_random', u'chr8', u'chr9', u'chrX', u'chrX_GL456233_random', u'chrY', u'chrY_JH584300_random', u'chrY_JH584301_random', u'chrY_JH584302_random', u'chrY_JH584303_random', u'chrUn_GL456239', u'chrUn_GL456367', u'chrUn_GL456378', u'chrUn_GL456381', u'chrUn_GL456382', u'chrUn_GL456383', u'chrUn_GL456385', u'chrUn_GL456390', u'chrUn_GL456392', u'chrUn_GL456393', u'chrUn_GL456394', u'chrUn_GL456359', u'chrUn_GL456360', u'chrUn_GL456396', u'chrUn_GL456372', u'chrUn_GL456387', u'chrUn_GL456389', u'chrU",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:727,security,log,log,727,"Strange make_examples lag when trying to use 64 cores and shards for WGS; Hi, with WGS (mm10, but the documentation says WGS human model should work for this), I got make_examples running fine with 4 cores and shards, but as soon as I go above 4 I get a strange lag. I'm running this recommended command:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. With 4, I get a nice stdout update per shard, first a description of the contigs:. `I0201 17:01:29.167016 140183113676544 make_examples.py:946] Common contigs are [u'chr1', u'chr10', u'chr11', u'chr12', u'chr13', u'chr14', u'chr15', u'chr16', u'chr17', u'chr18', u'chr19', u'chr1_GL456210_random', u'chr1_GL456211_random', u'chr1_GL456212_random', u'chr1_GL456213_random', u'chr1_GL456221_random', u'chr2', u'chr3', u'chr4', u'chr4_GL456216_random', u'chr4_JH584292_random', u'chr4_GL456350_random', u'chr4_JH584293_random', u'chr4_JH584294_random', u'chr4_JH584295_random', u'chr5', u'chr5_JH584296_random', u'chr5_JH584297_random', u'chr5_JH584298_random', u'chr5_GL456354_random', u'chr5_JH584299_random', u'chr6', u'chr7', u'chr7_GL456219_random', u'chr8', u'chr9', u'chrX', u'chrX_GL456233_random', u'chrY', u'chrY_JH584300_random', u'chrY_JH584301_random', u'chrY_JH584302_random', u'chrY_JH584303_random', u'chrUn_GL456239', u'chrUn_GL456367', u'chrUn_GL456378', u'chrUn_GL456381', u'chrUn_GL456382', u'chrUn_GL456383', u'chrUn_GL456385', u'chrUn_GL456390', u'chrUn_GL456392', u'chrUn_GL456393', u'chrUn_GL456394', u'chrUn_GL456359', u'chrUn_GL456360', u'chrUn_GL456396', u'chrUn_GL456372', u'chrUn_GL456387', u'chrUn_GL456389', u'chrU",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:786,security,updat,update,786,"Strange make_examples lag when trying to use 64 cores and shards for WGS; Hi, with WGS (mm10, but the documentation says WGS human model should work for this), I got make_examples running fine with 4 cores and shards, but as soon as I go above 4 I get a strange lag. I'm running this recommended command:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. With 4, I get a nice stdout update per shard, first a description of the contigs:. `I0201 17:01:29.167016 140183113676544 make_examples.py:946] Common contigs are [u'chr1', u'chr10', u'chr11', u'chr12', u'chr13', u'chr14', u'chr15', u'chr16', u'chr17', u'chr18', u'chr19', u'chr1_GL456210_random', u'chr1_GL456211_random', u'chr1_GL456212_random', u'chr1_GL456213_random', u'chr1_GL456221_random', u'chr2', u'chr3', u'chr4', u'chr4_GL456216_random', u'chr4_JH584292_random', u'chr4_GL456350_random', u'chr4_JH584293_random', u'chr4_JH584294_random', u'chr4_JH584295_random', u'chr5', u'chr5_JH584296_random', u'chr5_JH584297_random', u'chr5_JH584298_random', u'chr5_GL456354_random', u'chr5_JH584299_random', u'chr6', u'chr7', u'chr7_GL456219_random', u'chr8', u'chr9', u'chrX', u'chrX_GL456233_random', u'chrY', u'chrY_JH584300_random', u'chrY_JH584301_random', u'chrY_JH584302_random', u'chrY_JH584303_random', u'chrUn_GL456239', u'chrUn_GL456367', u'chrUn_GL456378', u'chrUn_GL456381', u'chrUn_GL456382', u'chrUn_GL456383', u'chrUn_GL456385', u'chrUn_GL456390', u'chrUn_GL456392', u'chrUn_GL456393', u'chrUn_GL456394', u'chrUn_GL456359', u'chrUn_GL456360', u'chrUn_GL456396', u'chrUn_GL456372', u'chrUn_GL456387', u'chrUn_GL456389', u'chrU",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:347,testability,Log,Log,347,"Strange make_examples lag when trying to use 64 cores and shards for WGS; Hi, with WGS (mm10, but the documentation says WGS human model should work for this), I got make_examples running fine with 4 cores and shards, but as soon as I go above 4 I get a strange lag. I'm running this recommended command:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. With 4, I get a nice stdout update per shard, first a description of the contigs:. `I0201 17:01:29.167016 140183113676544 make_examples.py:946] Common contigs are [u'chr1', u'chr10', u'chr11', u'chr12', u'chr13', u'chr14', u'chr15', u'chr16', u'chr17', u'chr18', u'chr19', u'chr1_GL456210_random', u'chr1_GL456211_random', u'chr1_GL456212_random', u'chr1_GL456213_random', u'chr1_GL456221_random', u'chr2', u'chr3', u'chr4', u'chr4_GL456216_random', u'chr4_JH584292_random', u'chr4_GL456350_random', u'chr4_JH584293_random', u'chr4_JH584294_random', u'chr4_JH584295_random', u'chr5', u'chr5_JH584296_random', u'chr5_JH584297_random', u'chr5_JH584298_random', u'chr5_GL456354_random', u'chr5_JH584299_random', u'chr6', u'chr7', u'chr7_GL456219_random', u'chr8', u'chr9', u'chrX', u'chrX_GL456233_random', u'chrY', u'chrY_JH584300_random', u'chrY_JH584301_random', u'chrY_JH584302_random', u'chrY_JH584303_random', u'chrUn_GL456239', u'chrUn_GL456367', u'chrUn_GL456378', u'chrUn_GL456381', u'chrUn_GL456382', u'chrUn_GL456383', u'chrUn_GL456385', u'chrUn_GL456390', u'chrUn_GL456392', u'chrUn_GL456393', u'chrUn_GL456394', u'chrUn_GL456359', u'chrUn_GL456360', u'chrUn_GL456396', u'chrUn_GL456372', u'chrUn_GL456387', u'chrUn_GL456389', u'chrU",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:401,testability,log,log,401,"Strange make_examples lag when trying to use 64 cores and shards for WGS; Hi, with WGS (mm10, but the documentation says WGS human model should work for this), I got make_examples running fine with 4 cores and shards, but as soon as I go above 4 I get a strange lag. I'm running this recommended command:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. With 4, I get a nice stdout update per shard, first a description of the contigs:. `I0201 17:01:29.167016 140183113676544 make_examples.py:946] Common contigs are [u'chr1', u'chr10', u'chr11', u'chr12', u'chr13', u'chr14', u'chr15', u'chr16', u'chr17', u'chr18', u'chr19', u'chr1_GL456210_random', u'chr1_GL456211_random', u'chr1_GL456212_random', u'chr1_GL456213_random', u'chr1_GL456221_random', u'chr2', u'chr3', u'chr4', u'chr4_GL456216_random', u'chr4_JH584292_random', u'chr4_GL456350_random', u'chr4_JH584293_random', u'chr4_JH584294_random', u'chr4_JH584295_random', u'chr5', u'chr5_JH584296_random', u'chr5_JH584297_random', u'chr5_JH584298_random', u'chr5_GL456354_random', u'chr5_JH584299_random', u'chr6', u'chr7', u'chr7_GL456219_random', u'chr8', u'chr9', u'chrX', u'chrX_GL456233_random', u'chrY', u'chrY_JH584300_random', u'chrY_JH584301_random', u'chrY_JH584302_random', u'chrY_JH584303_random', u'chrUn_GL456239', u'chrUn_GL456367', u'chrUn_GL456378', u'chrUn_GL456381', u'chrUn_GL456382', u'chrUn_GL456383', u'chrUn_GL456385', u'chrUn_GL456390', u'chrUn_GL456392', u'chrUn_GL456393', u'chrUn_GL456394', u'chrUn_GL456359', u'chrUn_GL456360', u'chrUn_GL456396', u'chrUn_GL456372', u'chrUn_GL456387', u'chrUn_GL456389', u'chrU",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:727,testability,log,log,727,"Strange make_examples lag when trying to use 64 cores and shards for WGS; Hi, with WGS (mm10, but the documentation says WGS human model should work for this), I got make_examples running fine with 4 cores and shards, but as soon as I go above 4 I get a strange lag. I'm running this recommended command:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. With 4, I get a nice stdout update per shard, first a description of the contigs:. `I0201 17:01:29.167016 140183113676544 make_examples.py:946] Common contigs are [u'chr1', u'chr10', u'chr11', u'chr12', u'chr13', u'chr14', u'chr15', u'chr16', u'chr17', u'chr18', u'chr19', u'chr1_GL456210_random', u'chr1_GL456211_random', u'chr1_GL456212_random', u'chr1_GL456213_random', u'chr1_GL456221_random', u'chr2', u'chr3', u'chr4', u'chr4_GL456216_random', u'chr4_JH584292_random', u'chr4_GL456350_random', u'chr4_JH584293_random', u'chr4_JH584294_random', u'chr4_JH584295_random', u'chr5', u'chr5_JH584296_random', u'chr5_JH584297_random', u'chr5_JH584298_random', u'chr5_GL456354_random', u'chr5_JH584299_random', u'chr6', u'chr7', u'chr7_GL456219_random', u'chr8', u'chr9', u'chrX', u'chrX_GL456233_random', u'chrY', u'chrY_JH584300_random', u'chrY_JH584301_random', u'chrY_JH584302_random', u'chrY_JH584303_random', u'chrUn_GL456239', u'chrUn_GL456367', u'chrUn_GL456378', u'chrUn_GL456381', u'chrUn_GL456382', u'chrUn_GL456383', u'chrUn_GL456385', u'chrUn_GL456390', u'chrUn_GL456392', u'chrUn_GL456393', u'chrUn_GL456394', u'chrUn_GL456359', u'chrUn_GL456360', u'chrUn_GL456396', u'chrUn_GL456372', u'chrUn_GL456387', u'chrUn_GL456389', u'chrU",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:5794,testability,concern,concerns,5794,"didates in chr1:10001-11000 [0.01s elapsed]. I0201 17:01:49.757262 140183113676544 make_examples.py:782] Found 0 candidates in chr1:14001-15000 [0.01s elapsed]. I0201 17:01:49.769794 140183113676544 make_examples.py:782] Found 0 candidates in chr1:18001-19000 [0.01s elapsed]`. However, when I go up in number of shards and cores, even to just 8, let alone the recommended 64, I don't get past this point:. `Common contigs are [u'chr1', u'chr10', u'chr11', u'chr12', u'chr13', u'chr14', u'chr15', u'chr16', u'chr17', u'chr18', u'chr19', u'chr1_GL456210_random', u'chr1_GL456211_random', u'chr1_GL456212_random', u'chr1_GL456213_random', u'chr1_GL456221_random', u'chr2', u'chr3', u'chr4', u'chr4_GL456216_random', u'chr4_JH584292_random', u'chr4_GL456350_random', u'chr4_JH584293_random', u'chr4_JH584294_random', u'chr4_JH584295_random', u'chr5', u'chr5_JH584296_random', u'chr5_JH584297_random', u'chr5_JH584298_random', u'chr5_GL456354_random', u'chr5_JH584299_random', u'chr6', u'chr7', u'chr7_GL456219_random', u'chr8', u'chr9', u'chrX', u'chrX_GL456233_random', u'chrY', u'chrY_JH584300_random', u'chrY_JH584301_random', u'chrY_JH584302_random', u'chrY_JH584303_random', u'chrUn_GL456239', u'chrUn_GL456367', u'chrUn_GL456378', u'chrUn_GL456381', u'chrUn_GL456382', u'chrUn_GL456383', u'chrUn_GL456385', u'chrUn_GL456390', u'chrUn_GL456392', u'chrUn_GL456393', u'chrUn_GL456394', u'chrUn_GL456359', u'chrUn_GL456360', u'chrUn_GL456396', u'chrUn_GL456372', u'chrUn_GL456387', u'chrUn_GL456389', u'chrUn_GL456370', u'chrUn_GL456379', u'chrUn_GL456366', u'chrUn_GL456368', u'chrUn_JH584304']`. Then it just seems to lag, for up to 24 hours before I aborted the job because of cost concerns. Any ideas of what might be happening under the hood such that 4 cores and shards seems to run immediately with no problem, but things just freeze and don't advance with more? 4 would be far too slow and expensive unfortunately for WGS, but I'm not sure how to get past this. Thanks in advance for any help.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:102,usability,document,documentation,102,"Strange make_examples lag when trying to use 64 cores and shards for WGS; Hi, with WGS (mm10, but the documentation says WGS human model should work for this), I got make_examples running fine with 4 cores and shards, but as soon as I go above 4 I get a strange lag. I'm running this recommended command:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. With 4, I get a nice stdout update per shard, first a description of the contigs:. `I0201 17:01:29.167016 140183113676544 make_examples.py:946] Common contigs are [u'chr1', u'chr10', u'chr11', u'chr12', u'chr13', u'chr14', u'chr15', u'chr16', u'chr17', u'chr18', u'chr19', u'chr1_GL456210_random', u'chr1_GL456211_random', u'chr1_GL456212_random', u'chr1_GL456213_random', u'chr1_GL456221_random', u'chr2', u'chr3', u'chr4', u'chr4_GL456216_random', u'chr4_JH584292_random', u'chr4_GL456350_random', u'chr4_JH584293_random', u'chr4_JH584294_random', u'chr4_JH584295_random', u'chr5', u'chr5_JH584296_random', u'chr5_JH584297_random', u'chr5_JH584298_random', u'chr5_GL456354_random', u'chr5_JH584299_random', u'chr6', u'chr7', u'chr7_GL456219_random', u'chr8', u'chr9', u'chrX', u'chrX_GL456233_random', u'chrY', u'chrY_JH584300_random', u'chrY_JH584301_random', u'chrY_JH584302_random', u'chrY_JH584303_random', u'chrUn_GL456239', u'chrUn_GL456367', u'chrUn_GL456378', u'chrUn_GL456381', u'chrUn_GL456382', u'chrUn_GL456383', u'chrUn_GL456385', u'chrUn_GL456390', u'chrUn_GL456392', u'chrUn_GL456393', u'chrUn_GL456394', u'chrUn_GL456359', u'chrUn_GL456360', u'chrUn_GL456396', u'chrUn_GL456372', u'chrUn_GL456387', u'chrUn_GL456389', u'chrU",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:296,usability,command,command,296,"Strange make_examples lag when trying to use 64 cores and shards for WGS; Hi, with WGS (mm10, but the documentation says WGS human model should work for this), I got make_examples running fine with 4 cores and shards, but as soon as I go above 4 I get a strange lag. I'm running this recommended command:. ```. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads bamlink \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. With 4, I get a nice stdout update per shard, first a description of the contigs:. `I0201 17:01:29.167016 140183113676544 make_examples.py:946] Common contigs are [u'chr1', u'chr10', u'chr11', u'chr12', u'chr13', u'chr14', u'chr15', u'chr16', u'chr17', u'chr18', u'chr19', u'chr1_GL456210_random', u'chr1_GL456211_random', u'chr1_GL456212_random', u'chr1_GL456213_random', u'chr1_GL456221_random', u'chr2', u'chr3', u'chr4', u'chr4_GL456216_random', u'chr4_JH584292_random', u'chr4_GL456350_random', u'chr4_JH584293_random', u'chr4_JH584294_random', u'chr4_JH584295_random', u'chr5', u'chr5_JH584296_random', u'chr5_JH584297_random', u'chr5_JH584298_random', u'chr5_GL456354_random', u'chr5_JH584299_random', u'chr6', u'chr7', u'chr7_GL456219_random', u'chr8', u'chr9', u'chrX', u'chrX_GL456233_random', u'chrY', u'chrY_JH584300_random', u'chrY_JH584301_random', u'chrY_JH584302_random', u'chrY_JH584303_random', u'chrUn_GL456239', u'chrUn_GL456367', u'chrUn_GL456378', u'chrUn_GL456381', u'chrUn_GL456382', u'chrUn_GL456383', u'chrUn_GL456385', u'chrUn_GL456390', u'chrUn_GL456392', u'chrUn_GL456393', u'chrUn_GL456394', u'chrUn_GL456359', u'chrUn_GL456360', u'chrUn_GL456396', u'chrUn_GL456372', u'chrUn_GL456387', u'chrUn_GL456389', u'chrU",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/150:6105,usability,help,help,6105,"didates in chr1:10001-11000 [0.01s elapsed]. I0201 17:01:49.757262 140183113676544 make_examples.py:782] Found 0 candidates in chr1:14001-15000 [0.01s elapsed]. I0201 17:01:49.769794 140183113676544 make_examples.py:782] Found 0 candidates in chr1:18001-19000 [0.01s elapsed]`. However, when I go up in number of shards and cores, even to just 8, let alone the recommended 64, I don't get past this point:. `Common contigs are [u'chr1', u'chr10', u'chr11', u'chr12', u'chr13', u'chr14', u'chr15', u'chr16', u'chr17', u'chr18', u'chr19', u'chr1_GL456210_random', u'chr1_GL456211_random', u'chr1_GL456212_random', u'chr1_GL456213_random', u'chr1_GL456221_random', u'chr2', u'chr3', u'chr4', u'chr4_GL456216_random', u'chr4_JH584292_random', u'chr4_GL456350_random', u'chr4_JH584293_random', u'chr4_JH584294_random', u'chr4_JH584295_random', u'chr5', u'chr5_JH584296_random', u'chr5_JH584297_random', u'chr5_JH584298_random', u'chr5_GL456354_random', u'chr5_JH584299_random', u'chr6', u'chr7', u'chr7_GL456219_random', u'chr8', u'chr9', u'chrX', u'chrX_GL456233_random', u'chrY', u'chrY_JH584300_random', u'chrY_JH584301_random', u'chrY_JH584302_random', u'chrY_JH584303_random', u'chrUn_GL456239', u'chrUn_GL456367', u'chrUn_GL456378', u'chrUn_GL456381', u'chrUn_GL456382', u'chrUn_GL456383', u'chrUn_GL456385', u'chrUn_GL456390', u'chrUn_GL456392', u'chrUn_GL456393', u'chrUn_GL456394', u'chrUn_GL456359', u'chrUn_GL456360', u'chrUn_GL456396', u'chrUn_GL456372', u'chrUn_GL456387', u'chrUn_GL456389', u'chrUn_GL456370', u'chrUn_GL456379', u'chrUn_GL456366', u'chrUn_GL456368', u'chrUn_JH584304']`. Then it just seems to lag, for up to 24 hours before I aborted the job because of cost concerns. Any ideas of what might be happening under the hood such that 4 cores and shards seems to run immediately with no problem, but things just freeze and don't advance with more? 4 would be far too slow and expensive unfortunately for WGS, but I'm not sure how to get past this. Thanks in advance for any help.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/150
https://github.com/google/deepvariant/issues/151:2735,availability,echo,echo,2735,"4.gz. -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz. ... -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using the following command:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads reads.bam \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```. ## Run `call_variants`. echo ""Start running call_variants...Log will be in the terminal",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:3159,availability,echo,echo,3159,"- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using the following command:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads reads.bam \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```. ## Run `call_variants`. echo ""Start running call_variants...Log will be in the terminal and also to ${LOG_DIR}/call_variants.log."". ( time sudo docker run \. -v ""${BASE}"":""${BASE}"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${EXAMPLES}"" \. --checkpoint ""${MODEL}"". ) 2>&1 | tee ""${LOG_DIR}/call_variants.log"". echo ""Done."". echo. ```. Is there some magic pattern recognition that knows to look for files ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:3173,availability,echo,echo,3173,"5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using the following command:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads reads.bam \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```. ## Run `call_variants`. echo ""Start running call_variants...Log will be in the terminal and also to ${LOG_DIR}/call_variants.log."". ( time sudo docker run \. -v ""${BASE}"":""${BASE}"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${EXAMPLES}"" \. --checkpoint ""${MODEL}"". ) 2>&1 | tee ""${LOG_DIR}/call_variants.log"". echo ""Done."". echo. ```. Is there some magic pattern recognition that knows to look for files of the format ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:3674,availability,echo,echo,3674,"using the following command:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads reads.bam \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```. ## Run `call_variants`. echo ""Start running call_variants...Log will be in the terminal and also to ${LOG_DIR}/call_variants.log."". ( time sudo docker run \. -v ""${BASE}"":""${BASE}"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${EXAMPLES}"" \. --checkpoint ""${MODEL}"". ) 2>&1 | tee ""${LOG_DIR}/call_variants.log"". echo ""Done."". echo. ```. Is there some magic pattern recognition that knows to look for files of the format 000*-of-00064? Confused as to how I should do this; should I run call_variants on 64 separate machines, with each machine running a job on one of the sharded make_examples outputs? When I try incorporating the code recommended in the example workflow, I get the following error:. `ValueError: Cannot find matching files with the pattern ""test.examples.tfrecord@64.gz""`. So obviously not working out of the box as specified. But I'm not sure whether call_variants is intelligent to handle sharded examp",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:3999,availability,checkpoint,checkpoint,3999," ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```. ## Run `call_variants`. echo ""Start running call_variants...Log will be in the terminal and also to ${LOG_DIR}/call_variants.log."". ( time sudo docker run \. -v ""${BASE}"":""${BASE}"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${EXAMPLES}"" \. --checkpoint ""${MODEL}"". ) 2>&1 | tee ""${LOG_DIR}/call_variants.log"". echo ""Done."". echo. ```. Is there some magic pattern recognition that knows to look for files of the format 000*-of-00064? Confused as to how I should do this; should I run call_variants on 64 separate machines, with each machine running a job on one of the sharded make_examples outputs? When I try incorporating the code recommended in the example workflow, I get the following error:. `ValueError: Cannot find matching files with the pattern ""test.examples.tfrecord@64.gz""`. So obviously not working out of the box as specified. But I'm not sure whether call_variants is intelligent to handle sharded examples or if I should be explicitly only running it once on each shard and then somehow merging all the vcfs after or something. And where in this shading would post processing of variants fit in to generate the VCF -- can that be part of a reduce step pulling all sharded call_variants outputs together one one machine? Any recomm",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:4067,availability,echo,echo,4067,"${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```. ## Run `call_variants`. echo ""Start running call_variants...Log will be in the terminal and also to ${LOG_DIR}/call_variants.log."". ( time sudo docker run \. -v ""${BASE}"":""${BASE}"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${EXAMPLES}"" \. --checkpoint ""${MODEL}"". ) 2>&1 | tee ""${LOG_DIR}/call_variants.log"". echo ""Done."". echo. ```. Is there some magic pattern recognition that knows to look for files of the format 000*-of-00064? Confused as to how I should do this; should I run call_variants on 64 separate machines, with each machine running a job on one of the sharded make_examples outputs? When I try incorporating the code recommended in the example workflow, I get the following error:. `ValueError: Cannot find matching files with the pattern ""test.examples.tfrecord@64.gz""`. So obviously not working out of the box as specified. But I'm not sure whether call_variants is intelligent to handle sharded examples or if I should be explicitly only running it once on each shard and then somehow merging all the vcfs after or something. And where in this shading would post processing of variants fit in to generate the VCF -- can that be part of a reduce step pulling all sharded call_variants outputs together one one machine? Any recommendations @pichuan @akolesnikov ?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:4081,availability,echo,echo,4081,"${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```. ## Run `call_variants`. echo ""Start running call_variants...Log will be in the terminal and also to ${LOG_DIR}/call_variants.log."". ( time sudo docker run \. -v ""${BASE}"":""${BASE}"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${EXAMPLES}"" \. --checkpoint ""${MODEL}"". ) 2>&1 | tee ""${LOG_DIR}/call_variants.log"". echo ""Done."". echo. ```. Is there some magic pattern recognition that knows to look for files of the format 000*-of-00064? Confused as to how I should do this; should I run call_variants on 64 separate machines, with each machine running a job on one of the sharded make_examples outputs? When I try incorporating the code recommended in the example workflow, I get the following error:. `ValueError: Cannot find matching files with the pattern ""test.examples.tfrecord@64.gz""`. So obviously not working out of the box as specified. But I'm not sure whether call_variants is intelligent to handle sharded examples or if I should be explicitly only running it once on each shard and then somehow merging all the vcfs after or something. And where in this shading would post processing of variants fit in to generate the VCF -- can that be part of a reduce step pulling all sharded call_variants outputs together one one machine? Any recommendations @pichuan @akolesnikov ?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:4447,availability,error,error,4447,"${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```. ## Run `call_variants`. echo ""Start running call_variants...Log will be in the terminal and also to ${LOG_DIR}/call_variants.log."". ( time sudo docker run \. -v ""${BASE}"":""${BASE}"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${EXAMPLES}"" \. --checkpoint ""${MODEL}"". ) 2>&1 | tee ""${LOG_DIR}/call_variants.log"". echo ""Done."". echo. ```. Is there some magic pattern recognition that knows to look for files of the format 000*-of-00064? Confused as to how I should do this; should I run call_variants on 64 separate machines, with each machine running a job on one of the sharded make_examples outputs? When I try incorporating the code recommended in the example workflow, I get the following error:. `ValueError: Cannot find matching files with the pattern ""test.examples.tfrecord@64.gz""`. So obviously not working out of the box as specified. But I'm not sure whether call_variants is intelligent to handle sharded examples or if I should be explicitly only running it once on each shard and then somehow merging all the vcfs after or something. And where in this shading would post processing of variants fit in to generate the VCF -- can that be part of a reduce step pulling all sharded call_variants outputs together one one machine? Any recommendations @pichuan @akolesnikov ?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:2771,deployability,Log,Log,2771," Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz. ... -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using the following command:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads reads.bam \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```. ## Run `call_variants`. echo ""Start running call_variants...Log will be in the terminal and also to ${LOG_DIR}/call_variant",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:2825,deployability,log,log,2825," -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using the following command:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads reads.bam \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```. ## Run `call_variants`. echo ""Start running call_variants...Log will be in the terminal and also to ${LOG_DIR}/call_variants.log."". ( time sudo docker run \. -v ""${BASE}"":""${BAS",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:3153,deployability,log,log,3153,"-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using the following command:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads reads.bam \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```. ## Run `call_variants`. echo ""Start running call_variants...Log will be in the terminal and also to ${LOG_DIR}/call_variants.log."". ( time sudo docker run \. -v ""${BASE}"":""${BASE}"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${EXAMPLES}"" \. --checkpoint ""${MODEL}"". ) 2>&1 | tee ""${LOG_DIR}/call_variants.log"". echo ""Done."". echo. ```. Is there some magic pattern recognition that knows to look for ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:3710,deployability,Log,Log,3710,"# Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads reads.bam \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```. ## Run `call_variants`. echo ""Start running call_variants...Log will be in the terminal and also to ${LOG_DIR}/call_variants.log."". ( time sudo docker run \. -v ""${BASE}"":""${BASE}"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${EXAMPLES}"" \. --checkpoint ""${MODEL}"". ) 2>&1 | tee ""${LOG_DIR}/call_variants.log"". echo ""Done."". echo. ```. Is there some magic pattern recognition that knows to look for files of the format 000*-of-00064? Confused as to how I should do this; should I run call_variants on 64 separate machines, with each machine running a job on one of the sharded make_examples outputs? When I try incorporating the code recommended in the example workflow, I get the following error:. `ValueError: Cannot find matching files with the pattern ""test.examples.tfrecord@64.gz""`. So obviously not working out of the box as specified. But I'm not sure whether call_variants is intelligent to handle sharded examples or if I should be explicitly onl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:3775,deployability,log,log,3775,"ll be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads reads.bam \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```. ## Run `call_variants`. echo ""Start running call_variants...Log will be in the terminal and also to ${LOG_DIR}/call_variants.log."". ( time sudo docker run \. -v ""${BASE}"":""${BASE}"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${EXAMPLES}"" \. --checkpoint ""${MODEL}"". ) 2>&1 | tee ""${LOG_DIR}/call_variants.log"". echo ""Done."". echo. ```. Is there some magic pattern recognition that knows to look for files of the format 000*-of-00064? Confused as to how I should do this; should I run call_variants on 64 separate machines, with each machine running a job on one of the sharded make_examples outputs? When I try incorporating the code recommended in the example workflow, I get the following error:. `ValueError: Cannot find matching files with the pattern ""test.examples.tfrecord@64.gz""`. So obviously not working out of the box as specified. But I'm not sure whether call_variants is intelligent to handle sharded examples or if I should be explicitly only running it once on each shard and then somehow merging all the ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:4061,deployability,log,log,4061,"${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```. ## Run `call_variants`. echo ""Start running call_variants...Log will be in the terminal and also to ${LOG_DIR}/call_variants.log."". ( time sudo docker run \. -v ""${BASE}"":""${BASE}"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${EXAMPLES}"" \. --checkpoint ""${MODEL}"". ) 2>&1 | tee ""${LOG_DIR}/call_variants.log"". echo ""Done."". echo. ```. Is there some magic pattern recognition that knows to look for files of the format 000*-of-00064? Confused as to how I should do this; should I run call_variants on 64 separate machines, with each machine running a job on one of the sharded make_examples outputs? When I try incorporating the code recommended in the example workflow, I get the following error:. `ValueError: Cannot find matching files with the pattern ""test.examples.tfrecord@64.gz""`. So obviously not working out of the box as specified. But I'm not sure whether call_variants is intelligent to handle sharded examples or if I should be explicitly only running it once on each shard and then somehow merging all the vcfs after or something. And where in this shading would post processing of variants fit in to generate the VCF -- can that be part of a reduce step pulling all sharded call_variants outputs together one one machine? Any recommendations @pichuan @akolesnikov ?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:4013,energy efficiency,MODEL,MODEL,4013,"d}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```. ## Run `call_variants`. echo ""Start running call_variants...Log will be in the terminal and also to ${LOG_DIR}/call_variants.log."". ( time sudo docker run \. -v ""${BASE}"":""${BASE}"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${EXAMPLES}"" \. --checkpoint ""${MODEL}"". ) 2>&1 | tee ""${LOG_DIR}/call_variants.log"". echo ""Done."". echo. ```. Is there some magic pattern recognition that knows to look for files of the format 000*-of-00064? Confused as to how I should do this; should I run call_variants on 64 separate machines, with each machine running a job on one of the sharded make_examples outputs? When I try incorporating the code recommended in the example workflow, I get the following error:. `ValueError: Cannot find matching files with the pattern ""test.examples.tfrecord@64.gz""`. So obviously not working out of the box as specified. But I'm not sure whether call_variants is intelligent to handle sharded examples or if I should be explicitly only running it once on each shard and then somehow merging all the vcfs after or something. And where in this shading would post processing of variants fit in to generate the VCF -- can that be part of a reduce step pulling all sharded call_variants outputs together one one machine? Any recommendations @p",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:4914,energy efficiency,reduc,reduce,4914,"${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```. ## Run `call_variants`. echo ""Start running call_variants...Log will be in the terminal and also to ${LOG_DIR}/call_variants.log."". ( time sudo docker run \. -v ""${BASE}"":""${BASE}"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${EXAMPLES}"" \. --checkpoint ""${MODEL}"". ) 2>&1 | tee ""${LOG_DIR}/call_variants.log"". echo ""Done."". echo. ```. Is there some magic pattern recognition that knows to look for files of the format 000*-of-00064? Confused as to how I should do this; should I run call_variants on 64 separate machines, with each machine running a job on one of the sharded make_examples outputs? When I try incorporating the code recommended in the example workflow, I get the following error:. `ValueError: Cannot find matching files with the pattern ""test.examples.tfrecord@64.gz""`. So obviously not working out of the box as specified. But I'm not sure whether call_variants is intelligent to handle sharded examples or if I should be explicitly only running it once on each shard and then somehow merging all the vcfs after or something. And where in this shading would post processing of variants fit in to generate the VCF -- can that be part of a reduce step pulling all sharded call_variants outputs together one one machine? Any recommendations @pichuan @akolesnikov ?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:2889,integrability,buffer,buffer,2889,"054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using the following command:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads reads.bam \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```. ## Run `call_variants`. echo ""Start running call_variants...Log will be in the terminal and also to ${LOG_DIR}/call_variants.log."". ( time sudo docker run \. -v ""${BASE}"":""${BASE}"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:3366,interoperability,specif,specified,3366,"cord-00060-of-00064.gz. -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using the following command:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads reads.bam \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```. ## Run `call_variants`. echo ""Start running call_variants...Log will be in the terminal and also to ${LOG_DIR}/call_variants.log."". ( time sudo docker run \. -v ""${BASE}"":""${BASE}"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${EXAMPLES}"" \. --checkpoint ""${MODEL}"". ) 2>&1 | tee ""${LOG_DIR}/call_variants.log"". echo ""Done."". echo. ```. Is there some magic pattern recognition that knows to look for files of the format 000*-of-00064? Confused as to how I should do this; should I run call_variants on 64 separate machines, with each machine running a job on one of the sharded make_examples outputs? When I try inco",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:4168,interoperability,format,format,4168,"${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```. ## Run `call_variants`. echo ""Start running call_variants...Log will be in the terminal and also to ${LOG_DIR}/call_variants.log."". ( time sudo docker run \. -v ""${BASE}"":""${BASE}"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${EXAMPLES}"" \. --checkpoint ""${MODEL}"". ) 2>&1 | tee ""${LOG_DIR}/call_variants.log"". echo ""Done."". echo. ```. Is there some magic pattern recognition that knows to look for files of the format 000*-of-00064? Confused as to how I should do this; should I run call_variants on 64 separate machines, with each machine running a job on one of the sharded make_examples outputs? When I try incorporating the code recommended in the example workflow, I get the following error:. `ValueError: Cannot find matching files with the pattern ""test.examples.tfrecord@64.gz""`. So obviously not working out of the box as specified. But I'm not sure whether call_variants is intelligent to handle sharded examples or if I should be explicitly only running it once on each shard and then somehow merging all the vcfs after or something. And where in this shading would post processing of variants fit in to generate the VCF -- can that be part of a reduce step pulling all sharded call_variants outputs together one one machine? Any recommendations @pichuan @akolesnikov ?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:4588,interoperability,specif,specified,4588,"${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```. ## Run `call_variants`. echo ""Start running call_variants...Log will be in the terminal and also to ${LOG_DIR}/call_variants.log."". ( time sudo docker run \. -v ""${BASE}"":""${BASE}"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${EXAMPLES}"" \. --checkpoint ""${MODEL}"". ) 2>&1 | tee ""${LOG_DIR}/call_variants.log"". echo ""Done."". echo. ```. Is there some magic pattern recognition that knows to look for files of the format 000*-of-00064? Confused as to how I should do this; should I run call_variants on 64 separate machines, with each machine running a job on one of the sharded make_examples outputs? When I try incorporating the code recommended in the example workflow, I get the following error:. `ValueError: Cannot find matching files with the pattern ""test.examples.tfrecord@64.gz""`. So obviously not working out of the box as specified. But I'm not sure whether call_variants is intelligent to handle sharded examples or if I should be explicitly only running it once on each shard and then somehow merging all the vcfs after or something. And where in this shading would post processing of variants fit in to generate the VCF -- can that be part of a reduce step pulling all sharded call_variants outputs together one one machine? Any recommendations @pichuan @akolesnikov ?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:2834,performance,time,time,2834,"-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using the following command:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads reads.bam \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```. ## Run `call_variants`. echo ""Start running call_variants...Log will be in the terminal and also to ${LOG_DIR}/call_variants.log."". ( time sudo docker run \. -v ""${BASE}"":""${BASE}"" \. gc",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:2870,performance,parallel,parallel,2870,"t.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using the following command:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads reads.bam \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```. ## Run `call_variants`. echo ""Start running call_variants...Log will be in the terminal and also to ${LOG_DIR}/call_variants.log."". ( time sudo docker run \. -v ""${BASE}"":""${BASE}"" \. gcr.io/deepvariant-docker/deepvariant:""$",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:3784,performance,time,time,3784,"the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads reads.bam \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```. ## Run `call_variants`. echo ""Start running call_variants...Log will be in the terminal and also to ${LOG_DIR}/call_variants.log."". ( time sudo docker run \. -v ""${BASE}"":""${BASE}"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${EXAMPLES}"" \. --checkpoint ""${MODEL}"". ) 2>&1 | tee ""${LOG_DIR}/call_variants.log"". echo ""Done."". echo. ```. Is there some magic pattern recognition that knows to look for files of the format 000*-of-00064? Confused as to how I should do this; should I run call_variants on 64 separate machines, with each machine running a job on one of the sharded make_examples outputs? When I try incorporating the code recommended in the example workflow, I get the following error:. `ValueError: Cannot find matching files with the pattern ""test.examples.tfrecord@64.gz""`. So obviously not working out of the box as specified. But I'm not sure whether call_variants is intelligent to handle sharded examples or if I should be explicitly only running it once on each shard and then somehow merging all the vcfs afte",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:4447,performance,error,error,4447,"${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```. ## Run `call_variants`. echo ""Start running call_variants...Log will be in the terminal and also to ${LOG_DIR}/call_variants.log."". ( time sudo docker run \. -v ""${BASE}"":""${BASE}"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${EXAMPLES}"" \. --checkpoint ""${MODEL}"". ) 2>&1 | tee ""${LOG_DIR}/call_variants.log"". echo ""Done."". echo. ```. Is there some magic pattern recognition that knows to look for files of the format 000*-of-00064? Confused as to how I should do this; should I run call_variants on 64 separate machines, with each machine running a job on one of the sharded make_examples outputs? When I try incorporating the code recommended in the example workflow, I get the following error:. `ValueError: Cannot find matching files with the pattern ""test.examples.tfrecord@64.gz""`. So obviously not working out of the box as specified. But I'm not sure whether call_variants is intelligent to handle sharded examples or if I should be explicitly only running it once on each shard and then somehow merging all the vcfs after or something. And where in this shading would post processing of variants fit in to generate the VCF -- can that be part of a reduce step pulling all sharded call_variants outputs together one one machine? Any recommendations @pichuan @akolesnikov ?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:3999,reliability,checkpoint,checkpoint,3999," ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```. ## Run `call_variants`. echo ""Start running call_variants...Log will be in the terminal and also to ${LOG_DIR}/call_variants.log."". ( time sudo docker run \. -v ""${BASE}"":""${BASE}"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${EXAMPLES}"" \. --checkpoint ""${MODEL}"". ) 2>&1 | tee ""${LOG_DIR}/call_variants.log"". echo ""Done."". echo. ```. Is there some magic pattern recognition that knows to look for files of the format 000*-of-00064? Confused as to how I should do this; should I run call_variants on 64 separate machines, with each machine running a job on one of the sharded make_examples outputs? When I try incorporating the code recommended in the example workflow, I get the following error:. `ValueError: Cannot find matching files with the pattern ""test.examples.tfrecord@64.gz""`. So obviously not working out of the box as specified. But I'm not sure whether call_variants is intelligent to handle sharded examples or if I should be explicitly only running it once on each shard and then somehow merging all the vcfs after or something. And where in this shading would post processing of variants fit in to generate the VCF -- can that be part of a reduce step pulling all sharded call_variants outputs together one one machine? Any recomm",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:93,safety,test,test,93,"How to run/call call_variants when make_examples produces sharded outputs; Hi, I've gotten a test run successfully through the make_example step with 64 shards, and have produced 64 examples and gvcf files like so:. ```. -rw-r--r-- 1 root root 14394035 Feb 6 18:18 test.examples.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 16089657 Feb 6 18:18 test.examples.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 14238866 Feb 6 18:18 test.examples.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 14484530 Feb 6 18:19 test.examples.tfrecord-00003-of-00064.gz. ... -rw-r--r-- 1 root root 15225527 Feb 6 18:18 test.examples.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 14663343 Feb 6 18:19 test.examples.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 14571664 Feb 6 18:19 test.examples.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 13704439 Feb 6 18:19 test.examples.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 14383355 Feb 6 18:18 test.examples.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 13559255 Feb 6 18:19 test.examples.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz. -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz. -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz. -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz. -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz. ... -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r--",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:265,safety,test,test,265,"How to run/call call_variants when make_examples produces sharded outputs; Hi, I've gotten a test run successfully through the make_example step with 64 shards, and have produced 64 examples and gvcf files like so:. ```. -rw-r--r-- 1 root root 14394035 Feb 6 18:18 test.examples.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 16089657 Feb 6 18:18 test.examples.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 14238866 Feb 6 18:18 test.examples.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 14484530 Feb 6 18:19 test.examples.tfrecord-00003-of-00064.gz. ... -rw-r--r-- 1 root root 15225527 Feb 6 18:18 test.examples.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 14663343 Feb 6 18:19 test.examples.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 14571664 Feb 6 18:19 test.examples.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 13704439 Feb 6 18:19 test.examples.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 14383355 Feb 6 18:18 test.examples.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 13559255 Feb 6 18:19 test.examples.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz. -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz. -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz. -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz. -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz. ... -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r--",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:351,safety,test,test,351,"How to run/call call_variants when make_examples produces sharded outputs; Hi, I've gotten a test run successfully through the make_example step with 64 shards, and have produced 64 examples and gvcf files like so:. ```. -rw-r--r-- 1 root root 14394035 Feb 6 18:18 test.examples.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 16089657 Feb 6 18:18 test.examples.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 14238866 Feb 6 18:18 test.examples.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 14484530 Feb 6 18:19 test.examples.tfrecord-00003-of-00064.gz. ... -rw-r--r-- 1 root root 15225527 Feb 6 18:18 test.examples.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 14663343 Feb 6 18:19 test.examples.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 14571664 Feb 6 18:19 test.examples.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 13704439 Feb 6 18:19 test.examples.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 14383355 Feb 6 18:18 test.examples.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 13559255 Feb 6 18:19 test.examples.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz. -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz. -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz. -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz. -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz. ... -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r--",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:437,safety,test,test,437,"How to run/call call_variants when make_examples produces sharded outputs; Hi, I've gotten a test run successfully through the make_example step with 64 shards, and have produced 64 examples and gvcf files like so:. ```. -rw-r--r-- 1 root root 14394035 Feb 6 18:18 test.examples.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 16089657 Feb 6 18:18 test.examples.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 14238866 Feb 6 18:18 test.examples.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 14484530 Feb 6 18:19 test.examples.tfrecord-00003-of-00064.gz. ... -rw-r--r-- 1 root root 15225527 Feb 6 18:18 test.examples.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 14663343 Feb 6 18:19 test.examples.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 14571664 Feb 6 18:19 test.examples.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 13704439 Feb 6 18:19 test.examples.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 14383355 Feb 6 18:18 test.examples.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 13559255 Feb 6 18:19 test.examples.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz. -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz. -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz. -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz. -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz. ... -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r--",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:523,safety,test,test,523,"How to run/call call_variants when make_examples produces sharded outputs; Hi, I've gotten a test run successfully through the make_example step with 64 shards, and have produced 64 examples and gvcf files like so:. ```. -rw-r--r-- 1 root root 14394035 Feb 6 18:18 test.examples.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 16089657 Feb 6 18:18 test.examples.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 14238866 Feb 6 18:18 test.examples.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 14484530 Feb 6 18:19 test.examples.tfrecord-00003-of-00064.gz. ... -rw-r--r-- 1 root root 15225527 Feb 6 18:18 test.examples.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 14663343 Feb 6 18:19 test.examples.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 14571664 Feb 6 18:19 test.examples.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 13704439 Feb 6 18:19 test.examples.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 14383355 Feb 6 18:18 test.examples.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 13559255 Feb 6 18:19 test.examples.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz. -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz. -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz. -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz. -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz. ... -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r--",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:613,safety,test,test,613,"How to run/call call_variants when make_examples produces sharded outputs; Hi, I've gotten a test run successfully through the make_example step with 64 shards, and have produced 64 examples and gvcf files like so:. ```. -rw-r--r-- 1 root root 14394035 Feb 6 18:18 test.examples.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 16089657 Feb 6 18:18 test.examples.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 14238866 Feb 6 18:18 test.examples.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 14484530 Feb 6 18:19 test.examples.tfrecord-00003-of-00064.gz. ... -rw-r--r-- 1 root root 15225527 Feb 6 18:18 test.examples.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 14663343 Feb 6 18:19 test.examples.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 14571664 Feb 6 18:19 test.examples.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 13704439 Feb 6 18:19 test.examples.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 14383355 Feb 6 18:18 test.examples.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 13559255 Feb 6 18:19 test.examples.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz. -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz. -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz. -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz. -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz. ... -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r--",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:699,safety,test,test,699,"How to run/call call_variants when make_examples produces sharded outputs; Hi, I've gotten a test run successfully through the make_example step with 64 shards, and have produced 64 examples and gvcf files like so:. ```. -rw-r--r-- 1 root root 14394035 Feb 6 18:18 test.examples.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 16089657 Feb 6 18:18 test.examples.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 14238866 Feb 6 18:18 test.examples.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 14484530 Feb 6 18:19 test.examples.tfrecord-00003-of-00064.gz. ... -rw-r--r-- 1 root root 15225527 Feb 6 18:18 test.examples.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 14663343 Feb 6 18:19 test.examples.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 14571664 Feb 6 18:19 test.examples.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 13704439 Feb 6 18:19 test.examples.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 14383355 Feb 6 18:18 test.examples.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 13559255 Feb 6 18:19 test.examples.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz. -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz. -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz. -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz. -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz. ... -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r--",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:785,safety,test,test,785,"How to run/call call_variants when make_examples produces sharded outputs; Hi, I've gotten a test run successfully through the make_example step with 64 shards, and have produced 64 examples and gvcf files like so:. ```. -rw-r--r-- 1 root root 14394035 Feb 6 18:18 test.examples.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 16089657 Feb 6 18:18 test.examples.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 14238866 Feb 6 18:18 test.examples.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 14484530 Feb 6 18:19 test.examples.tfrecord-00003-of-00064.gz. ... -rw-r--r-- 1 root root 15225527 Feb 6 18:18 test.examples.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 14663343 Feb 6 18:19 test.examples.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 14571664 Feb 6 18:19 test.examples.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 13704439 Feb 6 18:19 test.examples.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 14383355 Feb 6 18:18 test.examples.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 13559255 Feb 6 18:19 test.examples.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz. -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz. -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz. -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz. -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz. ... -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r--",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:871,safety,test,test,871,"How to run/call call_variants when make_examples produces sharded outputs; Hi, I've gotten a test run successfully through the make_example step with 64 shards, and have produced 64 examples and gvcf files like so:. ```. -rw-r--r-- 1 root root 14394035 Feb 6 18:18 test.examples.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 16089657 Feb 6 18:18 test.examples.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 14238866 Feb 6 18:18 test.examples.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 14484530 Feb 6 18:19 test.examples.tfrecord-00003-of-00064.gz. ... -rw-r--r-- 1 root root 15225527 Feb 6 18:18 test.examples.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 14663343 Feb 6 18:19 test.examples.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 14571664 Feb 6 18:19 test.examples.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 13704439 Feb 6 18:19 test.examples.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 14383355 Feb 6 18:18 test.examples.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 13559255 Feb 6 18:19 test.examples.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz. -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz. -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz. -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz. -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz. ... -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r--",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:957,safety,test,test,957,"How to run/call call_variants when make_examples produces sharded outputs; Hi, I've gotten a test run successfully through the make_example step with 64 shards, and have produced 64 examples and gvcf files like so:. ```. -rw-r--r-- 1 root root 14394035 Feb 6 18:18 test.examples.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 16089657 Feb 6 18:18 test.examples.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 14238866 Feb 6 18:18 test.examples.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 14484530 Feb 6 18:19 test.examples.tfrecord-00003-of-00064.gz. ... -rw-r--r-- 1 root root 15225527 Feb 6 18:18 test.examples.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 14663343 Feb 6 18:19 test.examples.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 14571664 Feb 6 18:19 test.examples.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 13704439 Feb 6 18:19 test.examples.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 14383355 Feb 6 18:18 test.examples.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 13559255 Feb 6 18:19 test.examples.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz. -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz. -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz. -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz. -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz. ... -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r--",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1043,safety,test,test,1043,"les produces sharded outputs; Hi, I've gotten a test run successfully through the make_example step with 64 shards, and have produced 64 examples and gvcf files like so:. ```. -rw-r--r-- 1 root root 14394035 Feb 6 18:18 test.examples.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 16089657 Feb 6 18:18 test.examples.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 14238866 Feb 6 18:18 test.examples.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 14484530 Feb 6 18:19 test.examples.tfrecord-00003-of-00064.gz. ... -rw-r--r-- 1 root root 15225527 Feb 6 18:18 test.examples.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 14663343 Feb 6 18:19 test.examples.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 14571664 Feb 6 18:19 test.examples.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 13704439 Feb 6 18:19 test.examples.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 14383355 Feb 6 18:18 test.examples.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 13559255 Feb 6 18:19 test.examples.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz. -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz. -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz. -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz. -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz. ... -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tf",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1129,safety,test,test,1129,"_example step with 64 shards, and have produced 64 examples and gvcf files like so:. ```. -rw-r--r-- 1 root root 14394035 Feb 6 18:18 test.examples.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 16089657 Feb 6 18:18 test.examples.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 14238866 Feb 6 18:18 test.examples.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 14484530 Feb 6 18:19 test.examples.tfrecord-00003-of-00064.gz. ... -rw-r--r-- 1 root root 15225527 Feb 6 18:18 test.examples.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 14663343 Feb 6 18:19 test.examples.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 14571664 Feb 6 18:19 test.examples.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 13704439 Feb 6 18:19 test.examples.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 14383355 Feb 6 18:18 test.examples.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 13559255 Feb 6 18:19 test.examples.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz. -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz. -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz. -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz. -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz. ... -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1215,safety,test,test,1215,``. -rw-r--r-- 1 root root 14394035 Feb 6 18:18 test.examples.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 16089657 Feb 6 18:18 test.examples.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 14238866 Feb 6 18:18 test.examples.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 14484530 Feb 6 18:19 test.examples.tfrecord-00003-of-00064.gz. ... -rw-r--r-- 1 root root 15225527 Feb 6 18:18 test.examples.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 14663343 Feb 6 18:19 test.examples.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 14571664 Feb 6 18:19 test.examples.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 13704439 Feb 6 18:19 test.examples.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 14383355 Feb 6 18:18 test.examples.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 13559255 Feb 6 18:19 test.examples.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz. -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz. -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz. -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz. -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz. ... -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-000,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1300,safety,test,test,1300,.gz. -rw-r--r-- 1 root root 16089657 Feb 6 18:18 test.examples.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 14238866 Feb 6 18:18 test.examples.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 14484530 Feb 6 18:19 test.examples.tfrecord-00003-of-00064.gz. ... -rw-r--r-- 1 root root 15225527 Feb 6 18:18 test.examples.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 14663343 Feb 6 18:19 test.examples.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 14571664 Feb 6 18:19 test.examples.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 13704439 Feb 6 18:19 test.examples.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 14383355 Feb 6 18:18 test.examples.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 13559255 Feb 6 18:19 test.examples.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz. -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz. -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz. -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz. -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz. ... -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-o,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1381,safety,test,test,1381,00064.gz. -rw-r--r-- 1 root root 14238866 Feb 6 18:18 test.examples.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 14484530 Feb 6 18:19 test.examples.tfrecord-00003-of-00064.gz. ... -rw-r--r-- 1 root root 15225527 Feb 6 18:18 test.examples.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 14663343 Feb 6 18:19 test.examples.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 14571664 Feb 6 18:19 test.examples.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 13704439 Feb 6 18:19 test.examples.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 14383355 Feb 6 18:18 test.examples.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 13559255 Feb 6 18:19 test.examples.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz. -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz. -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz. -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz. -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz. ... -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-o,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1462,safety,test,test,1462,2-of-00064.gz. -rw-r--r-- 1 root root 14484530 Feb 6 18:19 test.examples.tfrecord-00003-of-00064.gz. ... -rw-r--r-- 1 root root 15225527 Feb 6 18:18 test.examples.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 14663343 Feb 6 18:19 test.examples.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 14571664 Feb 6 18:19 test.examples.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 13704439 Feb 6 18:19 test.examples.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 14383355 Feb 6 18:18 test.examples.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 13559255 Feb 6 18:19 test.examples.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz. -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz. -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz. -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz. -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz. ... -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-o,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1543,safety,test,test,1543,-00003-of-00064.gz. ... -rw-r--r-- 1 root root 15225527 Feb 6 18:18 test.examples.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 14663343 Feb 6 18:19 test.examples.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 14571664 Feb 6 18:19 test.examples.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 13704439 Feb 6 18:19 test.examples.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 14383355 Feb 6 18:18 test.examples.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 13559255 Feb 6 18:19 test.examples.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz. -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz. -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz. -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz. -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz. ... -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-o,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1624,safety,test,test,1624,.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 14663343 Feb 6 18:19 test.examples.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 14571664 Feb 6 18:19 test.examples.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 13704439 Feb 6 18:19 test.examples.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 14383355 Feb 6 18:18 test.examples.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 13559255 Feb 6 18:19 test.examples.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz. -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz. -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz. -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz. -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz. ... -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-o,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1705,safety,test,test,1705,"mples.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 14571664 Feb 6 18:19 test.examples.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 13704439 Feb 6 18:19 test.examples.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 14383355 Feb 6 18:18 test.examples.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 13559255 Feb 6 18:19 test.examples.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz. -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz. -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz. -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz. -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz. ... -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using the following command:. `",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1786,safety,test,test,1786,"t.examples.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 13704439 Feb 6 18:19 test.examples.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 14383355 Feb 6 18:18 test.examples.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 13559255 Feb 6 18:19 test.examples.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz. -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz. -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz. -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz. -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz. ... -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using the following command:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in th",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1871,safety,test,test,1871,"st.examples.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 14383355 Feb 6 18:18 test.examples.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 13559255 Feb 6 18:19 test.examples.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz. -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz. -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz. -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz. -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz. ... -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using the following command:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. par",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1952,safety,test,test,1952,"18 test.examples.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 13559255 Feb 6 18:19 test.examples.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz. -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz. -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz. -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz. -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz. ... -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using the following command:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:2033,safety,test,test,2033,"6 18:19 test.examples.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz. -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz. -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz. -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz. -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz. ... -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using the following command:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads reads.bam \. --examples ""${sample_id}.examples.tfrecor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:2114,safety,test,test,2114," Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz. -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz. -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz. -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz. -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz. ... -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using the following command:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads reads.bam \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --ta",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:2195,safety,test,test,2195,"76769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz. -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz. -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz. -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz. -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz. ... -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using the following command:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads reads.bam \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. Which was bas",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:2276,safety,test,test,2276,"t 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz. -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz. -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz. -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz. ... -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using the following command:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads reads.bam \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_w",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:2357,safety,test,test,2357,"t 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz. -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz. -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz. -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz. ... -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using the following command:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads reads.bam \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pat",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:2438,safety,test,test,2438,"t 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz. -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz. -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz. -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz. ... -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using the following command:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads reads.bam \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:2519,safety,test,test,2519,"t 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz. -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz. -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz. -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz. ... -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using the following command:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads reads.bam \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with thes",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:2600,safety,test,test,2600,"t 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz. -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz. -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz. ... -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using the following command:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads reads.bam \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_varian",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:2771,safety,Log,Log,2771," Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz. ... -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using the following command:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads reads.bam \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```. ## Run `call_variants`. echo ""Start running call_variants...Log will be in the terminal and also to ${LOG_DIR}/call_variant",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:2825,safety,log,log,2825," -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using the following command:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads reads.bam \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```. ## Run `call_variants`. echo ""Start running call_variants...Log will be in the terminal and also to ${LOG_DIR}/call_variants.log."". ( time sudo docker run \. -v ""${BASE}"":""${BAS",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:3153,safety,log,log,3153,"-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using the following command:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads reads.bam \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```. ## Run `call_variants`. echo ""Start running call_variants...Log will be in the terminal and also to ${LOG_DIR}/call_variants.log."". ( time sudo docker run \. -v ""${BASE}"":""${BASE}"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${EXAMPLES}"" \. --checkpoint ""${MODEL}"". ) 2>&1 | tee ""${LOG_DIR}/call_variants.log"". echo ""Done."". echo. ```. Is there some magic pattern recognition that knows to look for ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:3577,safety,input,inputs,3577,"5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using the following command:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads reads.bam \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```. ## Run `call_variants`. echo ""Start running call_variants...Log will be in the terminal and also to ${LOG_DIR}/call_variants.log."". ( time sudo docker run \. -v ""${BASE}"":""${BASE}"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${EXAMPLES}"" \. --checkpoint ""${MODEL}"". ) 2>&1 | tee ""${LOG_DIR}/call_variants.log"". echo ""Done."". echo. ```. Is there some magic pattern recognition that knows to look for files of the format 000*-of-00064? Confused as to how I should do this; should I run call_variants on 64 separate machines, with each machine running a job on one of the sharded make_examples outputs? When I try incorporating the code recommended in the example workflow, I get the following error:. `ValueError: Cannot find matching files with the pattern ""test.examples.tfrecord@64.gz""`. So obviously not working out of the",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:3710,safety,Log,Log,3710,"# Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads reads.bam \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```. ## Run `call_variants`. echo ""Start running call_variants...Log will be in the terminal and also to ${LOG_DIR}/call_variants.log."". ( time sudo docker run \. -v ""${BASE}"":""${BASE}"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${EXAMPLES}"" \. --checkpoint ""${MODEL}"". ) 2>&1 | tee ""${LOG_DIR}/call_variants.log"". echo ""Done."". echo. ```. Is there some magic pattern recognition that knows to look for files of the format 000*-of-00064? Confused as to how I should do this; should I run call_variants on 64 separate machines, with each machine running a job on one of the sharded make_examples outputs? When I try incorporating the code recommended in the example workflow, I get the following error:. `ValueError: Cannot find matching files with the pattern ""test.examples.tfrecord@64.gz""`. So obviously not working out of the box as specified. But I'm not sure whether call_variants is intelligent to handle sharded examples or if I should be explicitly onl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:3775,safety,log,log,3775,"ll be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads reads.bam \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```. ## Run `call_variants`. echo ""Start running call_variants...Log will be in the terminal and also to ${LOG_DIR}/call_variants.log."". ( time sudo docker run \. -v ""${BASE}"":""${BASE}"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${EXAMPLES}"" \. --checkpoint ""${MODEL}"". ) 2>&1 | tee ""${LOG_DIR}/call_variants.log"". echo ""Done."". echo. ```. Is there some magic pattern recognition that knows to look for files of the format 000*-of-00064? Confused as to how I should do this; should I run call_variants on 64 separate machines, with each machine running a job on one of the sharded make_examples outputs? When I try incorporating the code recommended in the example workflow, I get the following error:. `ValueError: Cannot find matching files with the pattern ""test.examples.tfrecord@64.gz""`. So obviously not working out of the box as specified. But I'm not sure whether call_variants is intelligent to handle sharded examples or if I should be explicitly only running it once on each shard and then somehow merging all the ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:4061,safety,log,log,4061,"${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```. ## Run `call_variants`. echo ""Start running call_variants...Log will be in the terminal and also to ${LOG_DIR}/call_variants.log."". ( time sudo docker run \. -v ""${BASE}"":""${BASE}"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${EXAMPLES}"" \. --checkpoint ""${MODEL}"". ) 2>&1 | tee ""${LOG_DIR}/call_variants.log"". echo ""Done."". echo. ```. Is there some magic pattern recognition that knows to look for files of the format 000*-of-00064? Confused as to how I should do this; should I run call_variants on 64 separate machines, with each machine running a job on one of the sharded make_examples outputs? When I try incorporating the code recommended in the example workflow, I get the following error:. `ValueError: Cannot find matching files with the pattern ""test.examples.tfrecord@64.gz""`. So obviously not working out of the box as specified. But I'm not sure whether call_variants is intelligent to handle sharded examples or if I should be explicitly only running it once on each shard and then somehow merging all the vcfs after or something. And where in this shading would post processing of variants fit in to generate the VCF -- can that be part of a reduce step pulling all sharded call_variants outputs together one one machine? Any recommendations @pichuan @akolesnikov ?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:4447,safety,error,error,4447,"${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```. ## Run `call_variants`. echo ""Start running call_variants...Log will be in the terminal and also to ${LOG_DIR}/call_variants.log."". ( time sudo docker run \. -v ""${BASE}"":""${BASE}"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${EXAMPLES}"" \. --checkpoint ""${MODEL}"". ) 2>&1 | tee ""${LOG_DIR}/call_variants.log"". echo ""Done."". echo. ```. Is there some magic pattern recognition that knows to look for files of the format 000*-of-00064? Confused as to how I should do this; should I run call_variants on 64 separate machines, with each machine running a job on one of the sharded make_examples outputs? When I try incorporating the code recommended in the example workflow, I get the following error:. `ValueError: Cannot find matching files with the pattern ""test.examples.tfrecord@64.gz""`. So obviously not working out of the box as specified. But I'm not sure whether call_variants is intelligent to handle sharded examples or if I should be explicitly only running it once on each shard and then somehow merging all the vcfs after or something. And where in this shading would post processing of variants fit in to generate the VCF -- can that be part of a reduce step pulling all sharded call_variants outputs together one one machine? Any recommendations @pichuan @akolesnikov ?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:4513,safety,test,test,4513,"${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```. ## Run `call_variants`. echo ""Start running call_variants...Log will be in the terminal and also to ${LOG_DIR}/call_variants.log."". ( time sudo docker run \. -v ""${BASE}"":""${BASE}"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${EXAMPLES}"" \. --checkpoint ""${MODEL}"". ) 2>&1 | tee ""${LOG_DIR}/call_variants.log"". echo ""Done."". echo. ```. Is there some magic pattern recognition that knows to look for files of the format 000*-of-00064? Confused as to how I should do this; should I run call_variants on 64 separate machines, with each machine running a job on one of the sharded make_examples outputs? When I try incorporating the code recommended in the example workflow, I get the following error:. `ValueError: Cannot find matching files with the pattern ""test.examples.tfrecord@64.gz""`. So obviously not working out of the box as specified. But I'm not sure whether call_variants is intelligent to handle sharded examples or if I should be explicitly only running it once on each shard and then somehow merging all the vcfs after or something. And where in this shading would post processing of variants fit in to generate the VCF -- can that be part of a reduce step pulling all sharded call_variants outputs together one one machine? Any recommendations @pichuan @akolesnikov ?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:2771,security,Log,Log,2771," Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz. ... -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using the following command:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads reads.bam \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```. ## Run `call_variants`. echo ""Start running call_variants...Log will be in the terminal and also to ${LOG_DIR}/call_variant",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:2825,security,log,log,2825," -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using the following command:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads reads.bam \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```. ## Run `call_variants`. echo ""Start running call_variants...Log will be in the terminal and also to ${LOG_DIR}/call_variants.log."". ( time sudo docker run \. -v ""${BASE}"":""${BAS",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:3153,security,log,log,3153,"-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using the following command:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads reads.bam \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```. ## Run `call_variants`. echo ""Start running call_variants...Log will be in the terminal and also to ${LOG_DIR}/call_variants.log."". ( time sudo docker run \. -v ""${BASE}"":""${BASE}"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${EXAMPLES}"" \. --checkpoint ""${MODEL}"". ) 2>&1 | tee ""${LOG_DIR}/call_variants.log"". echo ""Done."". echo. ```. Is there some magic pattern recognition that knows to look for ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:3710,security,Log,Log,3710,"# Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads reads.bam \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```. ## Run `call_variants`. echo ""Start running call_variants...Log will be in the terminal and also to ${LOG_DIR}/call_variants.log."". ( time sudo docker run \. -v ""${BASE}"":""${BASE}"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${EXAMPLES}"" \. --checkpoint ""${MODEL}"". ) 2>&1 | tee ""${LOG_DIR}/call_variants.log"". echo ""Done."". echo. ```. Is there some magic pattern recognition that knows to look for files of the format 000*-of-00064? Confused as to how I should do this; should I run call_variants on 64 separate machines, with each machine running a job on one of the sharded make_examples outputs? When I try incorporating the code recommended in the example workflow, I get the following error:. `ValueError: Cannot find matching files with the pattern ""test.examples.tfrecord@64.gz""`. So obviously not working out of the box as specified. But I'm not sure whether call_variants is intelligent to handle sharded examples or if I should be explicitly onl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:3775,security,log,log,3775,"ll be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads reads.bam \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```. ## Run `call_variants`. echo ""Start running call_variants...Log will be in the terminal and also to ${LOG_DIR}/call_variants.log."". ( time sudo docker run \. -v ""${BASE}"":""${BASE}"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${EXAMPLES}"" \. --checkpoint ""${MODEL}"". ) 2>&1 | tee ""${LOG_DIR}/call_variants.log"". echo ""Done."". echo. ```. Is there some magic pattern recognition that knows to look for files of the format 000*-of-00064? Confused as to how I should do this; should I run call_variants on 64 separate machines, with each machine running a job on one of the sharded make_examples outputs? When I try incorporating the code recommended in the example workflow, I get the following error:. `ValueError: Cannot find matching files with the pattern ""test.examples.tfrecord@64.gz""`. So obviously not working out of the box as specified. But I'm not sure whether call_variants is intelligent to handle sharded examples or if I should be explicitly only running it once on each shard and then somehow merging all the ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:4013,security,MODEL,MODEL,4013,"d}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```. ## Run `call_variants`. echo ""Start running call_variants...Log will be in the terminal and also to ${LOG_DIR}/call_variants.log."". ( time sudo docker run \. -v ""${BASE}"":""${BASE}"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${EXAMPLES}"" \. --checkpoint ""${MODEL}"". ) 2>&1 | tee ""${LOG_DIR}/call_variants.log"". echo ""Done."". echo. ```. Is there some magic pattern recognition that knows to look for files of the format 000*-of-00064? Confused as to how I should do this; should I run call_variants on 64 separate machines, with each machine running a job on one of the sharded make_examples outputs? When I try incorporating the code recommended in the example workflow, I get the following error:. `ValueError: Cannot find matching files with the pattern ""test.examples.tfrecord@64.gz""`. So obviously not working out of the box as specified. But I'm not sure whether call_variants is intelligent to handle sharded examples or if I should be explicitly only running it once on each shard and then somehow merging all the vcfs after or something. And where in this shading would post processing of variants fit in to generate the VCF -- can that be part of a reduce step pulling all sharded call_variants outputs together one one machine? Any recommendations @p",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:4061,security,log,log,4061,"${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```. ## Run `call_variants`. echo ""Start running call_variants...Log will be in the terminal and also to ${LOG_DIR}/call_variants.log."". ( time sudo docker run \. -v ""${BASE}"":""${BASE}"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${EXAMPLES}"" \. --checkpoint ""${MODEL}"". ) 2>&1 | tee ""${LOG_DIR}/call_variants.log"". echo ""Done."". echo. ```. Is there some magic pattern recognition that knows to look for files of the format 000*-of-00064? Confused as to how I should do this; should I run call_variants on 64 separate machines, with each machine running a job on one of the sharded make_examples outputs? When I try incorporating the code recommended in the example workflow, I get the following error:. `ValueError: Cannot find matching files with the pattern ""test.examples.tfrecord@64.gz""`. So obviously not working out of the box as specified. But I'm not sure whether call_variants is intelligent to handle sharded examples or if I should be explicitly only running it once on each shard and then somehow merging all the vcfs after or something. And where in this shading would post processing of variants fit in to generate the VCF -- can that be part of a reduce step pulling all sharded call_variants outputs together one one machine? Any recommendations @pichuan @akolesnikov ?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:93,testability,test,test,93,"How to run/call call_variants when make_examples produces sharded outputs; Hi, I've gotten a test run successfully through the make_example step with 64 shards, and have produced 64 examples and gvcf files like so:. ```. -rw-r--r-- 1 root root 14394035 Feb 6 18:18 test.examples.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 16089657 Feb 6 18:18 test.examples.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 14238866 Feb 6 18:18 test.examples.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 14484530 Feb 6 18:19 test.examples.tfrecord-00003-of-00064.gz. ... -rw-r--r-- 1 root root 15225527 Feb 6 18:18 test.examples.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 14663343 Feb 6 18:19 test.examples.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 14571664 Feb 6 18:19 test.examples.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 13704439 Feb 6 18:19 test.examples.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 14383355 Feb 6 18:18 test.examples.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 13559255 Feb 6 18:19 test.examples.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz. -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz. -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz. -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz. -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz. ... -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r--",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:265,testability,test,test,265,"How to run/call call_variants when make_examples produces sharded outputs; Hi, I've gotten a test run successfully through the make_example step with 64 shards, and have produced 64 examples and gvcf files like so:. ```. -rw-r--r-- 1 root root 14394035 Feb 6 18:18 test.examples.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 16089657 Feb 6 18:18 test.examples.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 14238866 Feb 6 18:18 test.examples.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 14484530 Feb 6 18:19 test.examples.tfrecord-00003-of-00064.gz. ... -rw-r--r-- 1 root root 15225527 Feb 6 18:18 test.examples.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 14663343 Feb 6 18:19 test.examples.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 14571664 Feb 6 18:19 test.examples.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 13704439 Feb 6 18:19 test.examples.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 14383355 Feb 6 18:18 test.examples.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 13559255 Feb 6 18:19 test.examples.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz. -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz. -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz. -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz. -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz. ... -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r--",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:351,testability,test,test,351,"How to run/call call_variants when make_examples produces sharded outputs; Hi, I've gotten a test run successfully through the make_example step with 64 shards, and have produced 64 examples and gvcf files like so:. ```. -rw-r--r-- 1 root root 14394035 Feb 6 18:18 test.examples.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 16089657 Feb 6 18:18 test.examples.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 14238866 Feb 6 18:18 test.examples.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 14484530 Feb 6 18:19 test.examples.tfrecord-00003-of-00064.gz. ... -rw-r--r-- 1 root root 15225527 Feb 6 18:18 test.examples.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 14663343 Feb 6 18:19 test.examples.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 14571664 Feb 6 18:19 test.examples.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 13704439 Feb 6 18:19 test.examples.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 14383355 Feb 6 18:18 test.examples.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 13559255 Feb 6 18:19 test.examples.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz. -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz. -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz. -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz. -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz. ... -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r--",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:437,testability,test,test,437,"How to run/call call_variants when make_examples produces sharded outputs; Hi, I've gotten a test run successfully through the make_example step with 64 shards, and have produced 64 examples and gvcf files like so:. ```. -rw-r--r-- 1 root root 14394035 Feb 6 18:18 test.examples.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 16089657 Feb 6 18:18 test.examples.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 14238866 Feb 6 18:18 test.examples.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 14484530 Feb 6 18:19 test.examples.tfrecord-00003-of-00064.gz. ... -rw-r--r-- 1 root root 15225527 Feb 6 18:18 test.examples.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 14663343 Feb 6 18:19 test.examples.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 14571664 Feb 6 18:19 test.examples.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 13704439 Feb 6 18:19 test.examples.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 14383355 Feb 6 18:18 test.examples.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 13559255 Feb 6 18:19 test.examples.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz. -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz. -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz. -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz. -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz. ... -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r--",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:523,testability,test,test,523,"How to run/call call_variants when make_examples produces sharded outputs; Hi, I've gotten a test run successfully through the make_example step with 64 shards, and have produced 64 examples and gvcf files like so:. ```. -rw-r--r-- 1 root root 14394035 Feb 6 18:18 test.examples.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 16089657 Feb 6 18:18 test.examples.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 14238866 Feb 6 18:18 test.examples.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 14484530 Feb 6 18:19 test.examples.tfrecord-00003-of-00064.gz. ... -rw-r--r-- 1 root root 15225527 Feb 6 18:18 test.examples.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 14663343 Feb 6 18:19 test.examples.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 14571664 Feb 6 18:19 test.examples.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 13704439 Feb 6 18:19 test.examples.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 14383355 Feb 6 18:18 test.examples.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 13559255 Feb 6 18:19 test.examples.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz. -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz. -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz. -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz. -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz. ... -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r--",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:613,testability,test,test,613,"How to run/call call_variants when make_examples produces sharded outputs; Hi, I've gotten a test run successfully through the make_example step with 64 shards, and have produced 64 examples and gvcf files like so:. ```. -rw-r--r-- 1 root root 14394035 Feb 6 18:18 test.examples.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 16089657 Feb 6 18:18 test.examples.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 14238866 Feb 6 18:18 test.examples.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 14484530 Feb 6 18:19 test.examples.tfrecord-00003-of-00064.gz. ... -rw-r--r-- 1 root root 15225527 Feb 6 18:18 test.examples.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 14663343 Feb 6 18:19 test.examples.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 14571664 Feb 6 18:19 test.examples.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 13704439 Feb 6 18:19 test.examples.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 14383355 Feb 6 18:18 test.examples.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 13559255 Feb 6 18:19 test.examples.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz. -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz. -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz. -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz. -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz. ... -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r--",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:699,testability,test,test,699,"How to run/call call_variants when make_examples produces sharded outputs; Hi, I've gotten a test run successfully through the make_example step with 64 shards, and have produced 64 examples and gvcf files like so:. ```. -rw-r--r-- 1 root root 14394035 Feb 6 18:18 test.examples.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 16089657 Feb 6 18:18 test.examples.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 14238866 Feb 6 18:18 test.examples.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 14484530 Feb 6 18:19 test.examples.tfrecord-00003-of-00064.gz. ... -rw-r--r-- 1 root root 15225527 Feb 6 18:18 test.examples.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 14663343 Feb 6 18:19 test.examples.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 14571664 Feb 6 18:19 test.examples.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 13704439 Feb 6 18:19 test.examples.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 14383355 Feb 6 18:18 test.examples.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 13559255 Feb 6 18:19 test.examples.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz. -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz. -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz. -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz. -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz. ... -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r--",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:785,testability,test,test,785,"How to run/call call_variants when make_examples produces sharded outputs; Hi, I've gotten a test run successfully through the make_example step with 64 shards, and have produced 64 examples and gvcf files like so:. ```. -rw-r--r-- 1 root root 14394035 Feb 6 18:18 test.examples.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 16089657 Feb 6 18:18 test.examples.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 14238866 Feb 6 18:18 test.examples.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 14484530 Feb 6 18:19 test.examples.tfrecord-00003-of-00064.gz. ... -rw-r--r-- 1 root root 15225527 Feb 6 18:18 test.examples.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 14663343 Feb 6 18:19 test.examples.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 14571664 Feb 6 18:19 test.examples.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 13704439 Feb 6 18:19 test.examples.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 14383355 Feb 6 18:18 test.examples.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 13559255 Feb 6 18:19 test.examples.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz. -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz. -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz. -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz. -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz. ... -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r--",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:871,testability,test,test,871,"How to run/call call_variants when make_examples produces sharded outputs; Hi, I've gotten a test run successfully through the make_example step with 64 shards, and have produced 64 examples and gvcf files like so:. ```. -rw-r--r-- 1 root root 14394035 Feb 6 18:18 test.examples.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 16089657 Feb 6 18:18 test.examples.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 14238866 Feb 6 18:18 test.examples.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 14484530 Feb 6 18:19 test.examples.tfrecord-00003-of-00064.gz. ... -rw-r--r-- 1 root root 15225527 Feb 6 18:18 test.examples.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 14663343 Feb 6 18:19 test.examples.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 14571664 Feb 6 18:19 test.examples.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 13704439 Feb 6 18:19 test.examples.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 14383355 Feb 6 18:18 test.examples.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 13559255 Feb 6 18:19 test.examples.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz. -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz. -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz. -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz. -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz. ... -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r--",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:957,testability,test,test,957,"How to run/call call_variants when make_examples produces sharded outputs; Hi, I've gotten a test run successfully through the make_example step with 64 shards, and have produced 64 examples and gvcf files like so:. ```. -rw-r--r-- 1 root root 14394035 Feb 6 18:18 test.examples.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 16089657 Feb 6 18:18 test.examples.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 14238866 Feb 6 18:18 test.examples.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 14484530 Feb 6 18:19 test.examples.tfrecord-00003-of-00064.gz. ... -rw-r--r-- 1 root root 15225527 Feb 6 18:18 test.examples.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 14663343 Feb 6 18:19 test.examples.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 14571664 Feb 6 18:19 test.examples.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 13704439 Feb 6 18:19 test.examples.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 14383355 Feb 6 18:18 test.examples.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 13559255 Feb 6 18:19 test.examples.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz. -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz. -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz. -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz. -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz. ... -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r--",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1043,testability,test,test,1043,"les produces sharded outputs; Hi, I've gotten a test run successfully through the make_example step with 64 shards, and have produced 64 examples and gvcf files like so:. ```. -rw-r--r-- 1 root root 14394035 Feb 6 18:18 test.examples.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 16089657 Feb 6 18:18 test.examples.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 14238866 Feb 6 18:18 test.examples.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 14484530 Feb 6 18:19 test.examples.tfrecord-00003-of-00064.gz. ... -rw-r--r-- 1 root root 15225527 Feb 6 18:18 test.examples.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 14663343 Feb 6 18:19 test.examples.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 14571664 Feb 6 18:19 test.examples.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 13704439 Feb 6 18:19 test.examples.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 14383355 Feb 6 18:18 test.examples.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 13559255 Feb 6 18:19 test.examples.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz. -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz. -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz. -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz. -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz. ... -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tf",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1129,testability,test,test,1129,"_example step with 64 shards, and have produced 64 examples and gvcf files like so:. ```. -rw-r--r-- 1 root root 14394035 Feb 6 18:18 test.examples.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 16089657 Feb 6 18:18 test.examples.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 14238866 Feb 6 18:18 test.examples.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 14484530 Feb 6 18:19 test.examples.tfrecord-00003-of-00064.gz. ... -rw-r--r-- 1 root root 15225527 Feb 6 18:18 test.examples.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 14663343 Feb 6 18:19 test.examples.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 14571664 Feb 6 18:19 test.examples.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 13704439 Feb 6 18:19 test.examples.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 14383355 Feb 6 18:18 test.examples.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 13559255 Feb 6 18:19 test.examples.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz. -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz. -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz. -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz. -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz. ... -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1215,testability,test,test,1215,``. -rw-r--r-- 1 root root 14394035 Feb 6 18:18 test.examples.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 16089657 Feb 6 18:18 test.examples.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 14238866 Feb 6 18:18 test.examples.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 14484530 Feb 6 18:19 test.examples.tfrecord-00003-of-00064.gz. ... -rw-r--r-- 1 root root 15225527 Feb 6 18:18 test.examples.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 14663343 Feb 6 18:19 test.examples.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 14571664 Feb 6 18:19 test.examples.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 13704439 Feb 6 18:19 test.examples.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 14383355 Feb 6 18:18 test.examples.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 13559255 Feb 6 18:19 test.examples.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz. -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz. -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz. -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz. -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz. ... -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-000,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1300,testability,test,test,1300,.gz. -rw-r--r-- 1 root root 16089657 Feb 6 18:18 test.examples.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 14238866 Feb 6 18:18 test.examples.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 14484530 Feb 6 18:19 test.examples.tfrecord-00003-of-00064.gz. ... -rw-r--r-- 1 root root 15225527 Feb 6 18:18 test.examples.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 14663343 Feb 6 18:19 test.examples.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 14571664 Feb 6 18:19 test.examples.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 13704439 Feb 6 18:19 test.examples.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 14383355 Feb 6 18:18 test.examples.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 13559255 Feb 6 18:19 test.examples.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz. -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz. -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz. -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz. -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz. ... -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-o,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1381,testability,test,test,1381,00064.gz. -rw-r--r-- 1 root root 14238866 Feb 6 18:18 test.examples.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 14484530 Feb 6 18:19 test.examples.tfrecord-00003-of-00064.gz. ... -rw-r--r-- 1 root root 15225527 Feb 6 18:18 test.examples.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 14663343 Feb 6 18:19 test.examples.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 14571664 Feb 6 18:19 test.examples.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 13704439 Feb 6 18:19 test.examples.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 14383355 Feb 6 18:18 test.examples.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 13559255 Feb 6 18:19 test.examples.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz. -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz. -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz. -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz. -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz. ... -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-o,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1462,testability,test,test,1462,2-of-00064.gz. -rw-r--r-- 1 root root 14484530 Feb 6 18:19 test.examples.tfrecord-00003-of-00064.gz. ... -rw-r--r-- 1 root root 15225527 Feb 6 18:18 test.examples.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 14663343 Feb 6 18:19 test.examples.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 14571664 Feb 6 18:19 test.examples.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 13704439 Feb 6 18:19 test.examples.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 14383355 Feb 6 18:18 test.examples.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 13559255 Feb 6 18:19 test.examples.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz. -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz. -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz. -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz. -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz. ... -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-o,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1543,testability,test,test,1543,-00003-of-00064.gz. ... -rw-r--r-- 1 root root 15225527 Feb 6 18:18 test.examples.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 14663343 Feb 6 18:19 test.examples.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 14571664 Feb 6 18:19 test.examples.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 13704439 Feb 6 18:19 test.examples.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 14383355 Feb 6 18:18 test.examples.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 13559255 Feb 6 18:19 test.examples.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz. -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz. -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz. -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz. -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz. ... -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-o,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1624,testability,test,test,1624,.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 14663343 Feb 6 18:19 test.examples.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 14571664 Feb 6 18:19 test.examples.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 13704439 Feb 6 18:19 test.examples.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 14383355 Feb 6 18:18 test.examples.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 13559255 Feb 6 18:19 test.examples.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz. -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz. -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz. -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz. -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz. ... -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-o,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1705,testability,test,test,1705,"mples.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 14571664 Feb 6 18:19 test.examples.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 13704439 Feb 6 18:19 test.examples.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 14383355 Feb 6 18:18 test.examples.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 13559255 Feb 6 18:19 test.examples.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz. -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz. -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz. -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz. -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz. ... -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using the following command:. `",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1786,testability,test,test,1786,"t.examples.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 13704439 Feb 6 18:19 test.examples.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 14383355 Feb 6 18:18 test.examples.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 13559255 Feb 6 18:19 test.examples.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz. -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz. -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz. -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz. -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz. ... -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using the following command:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in th",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1871,testability,test,test,1871,"st.examples.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 14383355 Feb 6 18:18 test.examples.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 13559255 Feb 6 18:19 test.examples.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz. -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz. -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz. -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz. -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz. ... -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using the following command:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. par",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:1952,testability,test,test,1952,"18 test.examples.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 13559255 Feb 6 18:19 test.examples.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz. -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz. -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz. -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz. -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz. ... -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using the following command:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:2033,testability,test,test,2033,"6 18:19 test.examples.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz. -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz. -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz. -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz. -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz. ... -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using the following command:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads reads.bam \. --examples ""${sample_id}.examples.tfrecor",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:2114,testability,test,test,2114," Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz. -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz. -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz. -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz. -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz. ... -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using the following command:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads reads.bam \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --ta",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:2195,testability,test,test,2195,"76769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz. -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz. -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz. -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz. -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz. ... -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using the following command:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads reads.bam \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. Which was bas",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:2276,testability,test,test,2276,"t 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz. -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz. -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz. -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz. -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz. ... -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using the following command:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads reads.bam \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_w",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:2357,testability,test,test,2357,"t 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz. -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz. -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz. -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz. -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz. ... -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using the following command:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads reads.bam \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pat",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:2438,testability,test,test,2438,"t 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz. -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz. -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz. -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz. -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz. ... -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using the following command:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads reads.bam \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:2519,testability,test,test,2519,"t 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz. -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz. -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz. -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz. ... -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using the following command:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads reads.bam \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with thes",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:2600,testability,test,test,2600,"t 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz. -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz. -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz. ... -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using the following command:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads reads.bam \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_varian",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:2771,testability,Log,Log,2771," Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz. ... -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using the following command:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads reads.bam \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```. ## Run `call_variants`. echo ""Start running call_variants...Log will be in the terminal and also to ${LOG_DIR}/call_variant",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:2825,testability,log,log,2825," -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using the following command:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads reads.bam \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```. ## Run `call_variants`. echo ""Start running call_variants...Log will be in the terminal and also to ${LOG_DIR}/call_variants.log."". ( time sudo docker run \. -v ""${BASE}"":""${BAS",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:3153,testability,log,log,3153,"-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using the following command:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads reads.bam \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```. ## Run `call_variants`. echo ""Start running call_variants...Log will be in the terminal and also to ${LOG_DIR}/call_variants.log."". ( time sudo docker run \. -v ""${BASE}"":""${BASE}"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${EXAMPLES}"" \. --checkpoint ""${MODEL}"". ) 2>&1 | tee ""${LOG_DIR}/call_variants.log"". echo ""Done."". echo. ```. Is there some magic pattern recognition that knows to look for ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:3710,testability,Log,Log,3710,"# Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads reads.bam \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```. ## Run `call_variants`. echo ""Start running call_variants...Log will be in the terminal and also to ${LOG_DIR}/call_variants.log."". ( time sudo docker run \. -v ""${BASE}"":""${BASE}"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${EXAMPLES}"" \. --checkpoint ""${MODEL}"". ) 2>&1 | tee ""${LOG_DIR}/call_variants.log"". echo ""Done."". echo. ```. Is there some magic pattern recognition that knows to look for files of the format 000*-of-00064? Confused as to how I should do this; should I run call_variants on 64 separate machines, with each machine running a job on one of the sharded make_examples outputs? When I try incorporating the code recommended in the example workflow, I get the following error:. `ValueError: Cannot find matching files with the pattern ""test.examples.tfrecord@64.gz""`. So obviously not working out of the box as specified. But I'm not sure whether call_variants is intelligent to handle sharded examples or if I should be explicitly onl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:3775,testability,log,log,3775,"ll be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads reads.bam \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```. ## Run `call_variants`. echo ""Start running call_variants...Log will be in the terminal and also to ${LOG_DIR}/call_variants.log."". ( time sudo docker run \. -v ""${BASE}"":""${BASE}"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${EXAMPLES}"" \. --checkpoint ""${MODEL}"". ) 2>&1 | tee ""${LOG_DIR}/call_variants.log"". echo ""Done."". echo. ```. Is there some magic pattern recognition that knows to look for files of the format 000*-of-00064? Confused as to how I should do this; should I run call_variants on 64 separate machines, with each machine running a job on one of the sharded make_examples outputs? When I try incorporating the code recommended in the example workflow, I get the following error:. `ValueError: Cannot find matching files with the pattern ""test.examples.tfrecord@64.gz""`. So obviously not working out of the box as specified. But I'm not sure whether call_variants is intelligent to handle sharded examples or if I should be explicitly only running it once on each shard and then somehow merging all the ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:4061,testability,log,log,4061,"${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```. ## Run `call_variants`. echo ""Start running call_variants...Log will be in the terminal and also to ${LOG_DIR}/call_variants.log."". ( time sudo docker run \. -v ""${BASE}"":""${BASE}"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${EXAMPLES}"" \. --checkpoint ""${MODEL}"". ) 2>&1 | tee ""${LOG_DIR}/call_variants.log"". echo ""Done."". echo. ```. Is there some magic pattern recognition that knows to look for files of the format 000*-of-00064? Confused as to how I should do this; should I run call_variants on 64 separate machines, with each machine running a job on one of the sharded make_examples outputs? When I try incorporating the code recommended in the example workflow, I get the following error:. `ValueError: Cannot find matching files with the pattern ""test.examples.tfrecord@64.gz""`. So obviously not working out of the box as specified. But I'm not sure whether call_variants is intelligent to handle sharded examples or if I should be explicitly only running it once on each shard and then somehow merging all the vcfs after or something. And where in this shading would post processing of variants fit in to generate the VCF -- can that be part of a reduce step pulling all sharded call_variants outputs together one one machine? Any recommendations @pichuan @akolesnikov ?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:4513,testability,test,test,4513,"${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```. ## Run `call_variants`. echo ""Start running call_variants...Log will be in the terminal and also to ${LOG_DIR}/call_variants.log."". ( time sudo docker run \. -v ""${BASE}"":""${BASE}"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${EXAMPLES}"" \. --checkpoint ""${MODEL}"". ) 2>&1 | tee ""${LOG_DIR}/call_variants.log"". echo ""Done."". echo. ```. Is there some magic pattern recognition that knows to look for files of the format 000*-of-00064? Confused as to how I should do this; should I run call_variants on 64 separate machines, with each machine running a job on one of the sharded make_examples outputs? When I try incorporating the code recommended in the example workflow, I get the following error:. `ValueError: Cannot find matching files with the pattern ""test.examples.tfrecord@64.gz""`. So obviously not working out of the box as specified. But I'm not sure whether call_variants is intelligent to handle sharded examples or if I should be explicitly only running it once on each shard and then somehow merging all the vcfs after or something. And where in this shading would post processing of variants fit in to generate the VCF -- can that be part of a reduce step pulling all sharded call_variants outputs together one one machine? Any recommendations @pichuan @akolesnikov ?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:2696,usability,command,command,2696,"8:18 test.gvcf.tfrecord-00005-of-00064.gz. -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz. ... -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz. -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz. -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz. -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz. -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz. -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz. -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz. -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz. -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz. -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using the following command:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads reads.bam \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```. ## Run `call_variants`. echo ""Start running call_v",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:3577,usability,input,inputs,3577,"5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using the following command:. ```. ## Run `make_examples`. echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log."". ( time seq 0 $((${numShards}-1)) | \. parallel -k --line-buffer \. /opt/deepvariant/bin/make_examples \. --mode calling \. --ref ${Fasta} \. --reads reads.bam \. --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```. ## Run `call_variants`. echo ""Start running call_variants...Log will be in the terminal and also to ${LOG_DIR}/call_variants.log."". ( time sudo docker run \. -v ""${BASE}"":""${BASE}"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${EXAMPLES}"" \. --checkpoint ""${MODEL}"". ) 2>&1 | tee ""${LOG_DIR}/call_variants.log"". echo ""Done."". echo. ```. Is there some magic pattern recognition that knows to look for files of the format 000*-of-00064? Confused as to how I should do this; should I run call_variants on 64 separate machines, with each machine running a job on one of the sharded make_examples outputs? When I try incorporating the code recommended in the example workflow, I get the following error:. `ValueError: Cannot find matching files with the pattern ""test.examples.tfrecord@64.gz""`. So obviously not working out of the",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:4417,usability,workflow,workflow,4417,"${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```. ## Run `call_variants`. echo ""Start running call_variants...Log will be in the terminal and also to ${LOG_DIR}/call_variants.log."". ( time sudo docker run \. -v ""${BASE}"":""${BASE}"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${EXAMPLES}"" \. --checkpoint ""${MODEL}"". ) 2>&1 | tee ""${LOG_DIR}/call_variants.log"". echo ""Done."". echo. ```. Is there some magic pattern recognition that knows to look for files of the format 000*-of-00064? Confused as to how I should do this; should I run call_variants on 64 separate machines, with each machine running a job on one of the sharded make_examples outputs? When I try incorporating the code recommended in the example workflow, I get the following error:. `ValueError: Cannot find matching files with the pattern ""test.examples.tfrecord@64.gz""`. So obviously not working out of the box as specified. But I'm not sure whether call_variants is intelligent to handle sharded examples or if I should be explicitly only running it once on each shard and then somehow merging all the vcfs after or something. And where in this shading would post processing of variants fit in to generate the VCF -- can that be part of a reduce step pulling all sharded call_variants outputs together one one machine? Any recommendations @pichuan @akolesnikov ?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/issues/151:4447,usability,error,error,4447,"${numShards}.gz"" \. --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \. --task {} \. ) 2>&1 | tee ""make_examples.log"". echo ""Done."". echo. ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```. ## Run `call_variants`. echo ""Start running call_variants...Log will be in the terminal and also to ${LOG_DIR}/call_variants.log."". ( time sudo docker run \. -v ""${BASE}"":""${BASE}"" \. gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${EXAMPLES}"" \. --checkpoint ""${MODEL}"". ) 2>&1 | tee ""${LOG_DIR}/call_variants.log"". echo ""Done."". echo. ```. Is there some magic pattern recognition that knows to look for files of the format 000*-of-00064? Confused as to how I should do this; should I run call_variants on 64 separate machines, with each machine running a job on one of the sharded make_examples outputs? When I try incorporating the code recommended in the example workflow, I get the following error:. `ValueError: Cannot find matching files with the pattern ""test.examples.tfrecord@64.gz""`. So obviously not working out of the box as specified. But I'm not sure whether call_variants is intelligent to handle sharded examples or if I should be explicitly only running it once on each shard and then somehow merging all the vcfs after or something. And where in this shading would post processing of variants fit in to generate the VCF -- can that be part of a reduce step pulling all sharded call_variants outputs together one one machine? Any recommendations @pichuan @akolesnikov ?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/151
https://github.com/google/deepvariant/pull/152:22,performance,perform,performance,22,improve call_variants performance; improve call_variants performance,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/152
https://github.com/google/deepvariant/pull/152:57,performance,perform,performance,57,improve call_variants performance; improve call_variants performance,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/152
https://github.com/google/deepvariant/pull/152:22,usability,perform,performance,22,improve call_variants performance; improve call_variants performance,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/152
https://github.com/google/deepvariant/pull/152:57,usability,perform,performance,57,improve call_variants performance; improve call_variants performance,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/152
https://github.com/google/deepvariant/issues/153:213,energy efficiency,load,load,213,"Validating WGS outputs; Hi, for a 168GB bam the make_examples step produces a total of 48 GB of tf records (sharded, so this is split over 64 different files). Does this seem like a reasonable intermediate output load to pass to call_variants?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/153
https://github.com/google/deepvariant/issues/153:193,modifiability,interm,intermediate,193,"Validating WGS outputs; Hi, for a 168GB bam the make_examples step produces a total of 48 GB of tf records (sharded, so this is split over 64 different files). Does this seem like a reasonable intermediate output load to pass to call_variants?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/153
https://github.com/google/deepvariant/issues/153:213,performance,load,load,213,"Validating WGS outputs; Hi, for a 168GB bam the make_examples step produces a total of 48 GB of tf records (sharded, so this is split over 64 different files). Does this seem like a reasonable intermediate output load to pass to call_variants?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/153
https://github.com/google/deepvariant/issues/153:160,reliability,Doe,Does,160,"Validating WGS outputs; Hi, for a 168GB bam the make_examples step produces a total of 48 GB of tf records (sharded, so this is split over 64 different files). Does this seem like a reasonable intermediate output load to pass to call_variants?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/153
https://github.com/google/deepvariant/issues/153:0,safety,Valid,Validating,0,"Validating WGS outputs; Hi, for a 168GB bam the make_examples step produces a total of 48 GB of tf records (sharded, so this is split over 64 different files). Does this seem like a reasonable intermediate output load to pass to call_variants?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/153
https://github.com/google/deepvariant/issues/153:0,security,Validat,Validating,0,"Validating WGS outputs; Hi, for a 168GB bam the make_examples step produces a total of 48 GB of tf records (sharded, so this is split over 64 different files). Does this seem like a reasonable intermediate output load to pass to call_variants?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/153
https://github.com/google/deepvariant/issues/154:441,availability,error,error,441,"test_make_labeler_ref fail due to mock data; Hi There,. I compiled the whole deepvariant on ppc64le with IBM Advance Toolchain 11.0 with all its dependencies on RHEL 7.5. And most test cases passed, but only 2 test cases failed. Found one root cause today of ""//deepvariant/labeler:haplotype_labeler_test"" as following. While suppose this is not related to platform/environment issue? Would you please kindly help to comment how to fix this error? The detailed root cause please refer to the comments inline in the code, thanks in advance :). In the test file of ""deepvariant/labeler/haplotype_labeler_test.py"", the function of ""test_make_labeler_ref"". ```python. def test_make_labeler_ref(self, candidates, truths, expected_start,. expected_end, bufsize):. expected_bases = 'A' * (expected_end - expected_start). ## generate a Mock object instead of real object of InMemoryFastaReader. labeler = _make_labeler(). labeler._ref_reader.query.return_value = expected_bases. labeler_ref = labeler.make_labeler_ref(candidates, truths, bufsize=bufsize). labeler._ref_reader.query.assert_called_once_with(. ranges.make_range('20', expected_start, expected_end)). self.assertEqual(labeler_ref.start, expected_start). self.assertEqual(labeler_ref.end, expected_end). self.assertEqual(. labeler_ref.bases(expected_start, expected_end), expected_bases). ```. So when in the file of ""deepvariant/labeler/haplotype_labeler.py"", the function of ""make_labeler_ref"" will generate an incorrect output as ""self._ref_reader"" is mock. ```python. def make_labeler_ref(self, candidates, true_variants, bufsize=20):. all_variants = candidates + true_variants. contig = all_variants[0].reference_name. start = min(x.start for x in all_variants). end = max(x.end for x in all_variants). ## always output contig_nbp = 1, as self._ref_reader is Mock object. ## in fact contig_nbp=[<MagicMock name='mock.contig().n_bases' id='70366068929488'>]. ## change the above type to int becomes ""1"", then the region.end will be 1 to cause",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:22,deployability,fail,fail,22,"test_make_labeler_ref fail due to mock data; Hi There,. I compiled the whole deepvariant on ppc64le with IBM Advance Toolchain 11.0 with all its dependencies on RHEL 7.5. And most test cases passed, but only 2 test cases failed. Found one root cause today of ""//deepvariant/labeler:haplotype_labeler_test"" as following. While suppose this is not related to platform/environment issue? Would you please kindly help to comment how to fix this error? The detailed root cause please refer to the comments inline in the code, thanks in advance :). In the test file of ""deepvariant/labeler/haplotype_labeler_test.py"", the function of ""test_make_labeler_ref"". ```python. def test_make_labeler_ref(self, candidates, truths, expected_start,. expected_end, bufsize):. expected_bases = 'A' * (expected_end - expected_start). ## generate a Mock object instead of real object of InMemoryFastaReader. labeler = _make_labeler(). labeler._ref_reader.query.return_value = expected_bases. labeler_ref = labeler.make_labeler_ref(candidates, truths, bufsize=bufsize). labeler._ref_reader.query.assert_called_once_with(. ranges.make_range('20', expected_start, expected_end)). self.assertEqual(labeler_ref.start, expected_start). self.assertEqual(labeler_ref.end, expected_end). self.assertEqual(. labeler_ref.bases(expected_start, expected_end), expected_bases). ```. So when in the file of ""deepvariant/labeler/haplotype_labeler.py"", the function of ""make_labeler_ref"" will generate an incorrect output as ""self._ref_reader"" is mock. ```python. def make_labeler_ref(self, candidates, true_variants, bufsize=20):. all_variants = candidates + true_variants. contig = all_variants[0].reference_name. start = min(x.start for x in all_variants). end = max(x.end for x in all_variants). ## always output contig_nbp = 1, as self._ref_reader is Mock object. ## in fact contig_nbp=[<MagicMock name='mock.contig().n_bases' id='70366068929488'>]. ## change the above type to int becomes ""1"", then the region.end will be 1 to cause",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:117,deployability,Toolchain,Toolchain,117,"test_make_labeler_ref fail due to mock data; Hi There,. I compiled the whole deepvariant on ppc64le with IBM Advance Toolchain 11.0 with all its dependencies on RHEL 7.5. And most test cases passed, but only 2 test cases failed. Found one root cause today of ""//deepvariant/labeler:haplotype_labeler_test"" as following. While suppose this is not related to platform/environment issue? Would you please kindly help to comment how to fix this error? The detailed root cause please refer to the comments inline in the code, thanks in advance :). In the test file of ""deepvariant/labeler/haplotype_labeler_test.py"", the function of ""test_make_labeler_ref"". ```python. def test_make_labeler_ref(self, candidates, truths, expected_start,. expected_end, bufsize):. expected_bases = 'A' * (expected_end - expected_start). ## generate a Mock object instead of real object of InMemoryFastaReader. labeler = _make_labeler(). labeler._ref_reader.query.return_value = expected_bases. labeler_ref = labeler.make_labeler_ref(candidates, truths, bufsize=bufsize). labeler._ref_reader.query.assert_called_once_with(. ranges.make_range('20', expected_start, expected_end)). self.assertEqual(labeler_ref.start, expected_start). self.assertEqual(labeler_ref.end, expected_end). self.assertEqual(. labeler_ref.bases(expected_start, expected_end), expected_bases). ```. So when in the file of ""deepvariant/labeler/haplotype_labeler.py"", the function of ""make_labeler_ref"" will generate an incorrect output as ""self._ref_reader"" is mock. ```python. def make_labeler_ref(self, candidates, true_variants, bufsize=20):. all_variants = candidates + true_variants. contig = all_variants[0].reference_name. start = min(x.start for x in all_variants). end = max(x.end for x in all_variants). ## always output contig_nbp = 1, as self._ref_reader is Mock object. ## in fact contig_nbp=[<MagicMock name='mock.contig().n_bases' id='70366068929488'>]. ## change the above type to int becomes ""1"", then the region.end will be 1 to cause",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:145,deployability,depend,dependencies,145,"test_make_labeler_ref fail due to mock data; Hi There,. I compiled the whole deepvariant on ppc64le with IBM Advance Toolchain 11.0 with all its dependencies on RHEL 7.5. And most test cases passed, but only 2 test cases failed. Found one root cause today of ""//deepvariant/labeler:haplotype_labeler_test"" as following. While suppose this is not related to platform/environment issue? Would you please kindly help to comment how to fix this error? The detailed root cause please refer to the comments inline in the code, thanks in advance :). In the test file of ""deepvariant/labeler/haplotype_labeler_test.py"", the function of ""test_make_labeler_ref"". ```python. def test_make_labeler_ref(self, candidates, truths, expected_start,. expected_end, bufsize):. expected_bases = 'A' * (expected_end - expected_start). ## generate a Mock object instead of real object of InMemoryFastaReader. labeler = _make_labeler(). labeler._ref_reader.query.return_value = expected_bases. labeler_ref = labeler.make_labeler_ref(candidates, truths, bufsize=bufsize). labeler._ref_reader.query.assert_called_once_with(. ranges.make_range('20', expected_start, expected_end)). self.assertEqual(labeler_ref.start, expected_start). self.assertEqual(labeler_ref.end, expected_end). self.assertEqual(. labeler_ref.bases(expected_start, expected_end), expected_bases). ```. So when in the file of ""deepvariant/labeler/haplotype_labeler.py"", the function of ""make_labeler_ref"" will generate an incorrect output as ""self._ref_reader"" is mock. ```python. def make_labeler_ref(self, candidates, true_variants, bufsize=20):. all_variants = candidates + true_variants. contig = all_variants[0].reference_name. start = min(x.start for x in all_variants). end = max(x.end for x in all_variants). ## always output contig_nbp = 1, as self._ref_reader is Mock object. ## in fact contig_nbp=[<MagicMock name='mock.contig().n_bases' id='70366068929488'>]. ## change the above type to int becomes ""1"", then the region.end will be 1 to cause",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:221,deployability,fail,failed,221,"test_make_labeler_ref fail due to mock data; Hi There,. I compiled the whole deepvariant on ppc64le with IBM Advance Toolchain 11.0 with all its dependencies on RHEL 7.5. And most test cases passed, but only 2 test cases failed. Found one root cause today of ""//deepvariant/labeler:haplotype_labeler_test"" as following. While suppose this is not related to platform/environment issue? Would you please kindly help to comment how to fix this error? The detailed root cause please refer to the comments inline in the code, thanks in advance :). In the test file of ""deepvariant/labeler/haplotype_labeler_test.py"", the function of ""test_make_labeler_ref"". ```python. def test_make_labeler_ref(self, candidates, truths, expected_start,. expected_end, bufsize):. expected_bases = 'A' * (expected_end - expected_start). ## generate a Mock object instead of real object of InMemoryFastaReader. labeler = _make_labeler(). labeler._ref_reader.query.return_value = expected_bases. labeler_ref = labeler.make_labeler_ref(candidates, truths, bufsize=bufsize). labeler._ref_reader.query.assert_called_once_with(. ranges.make_range('20', expected_start, expected_end)). self.assertEqual(labeler_ref.start, expected_start). self.assertEqual(labeler_ref.end, expected_end). self.assertEqual(. labeler_ref.bases(expected_start, expected_end), expected_bases). ```. So when in the file of ""deepvariant/labeler/haplotype_labeler.py"", the function of ""make_labeler_ref"" will generate an incorrect output as ""self._ref_reader"" is mock. ```python. def make_labeler_ref(self, candidates, true_variants, bufsize=20):. all_variants = candidates + true_variants. contig = all_variants[0].reference_name. start = min(x.start for x in all_variants). end = max(x.end for x in all_variants). ## always output contig_nbp = 1, as self._ref_reader is Mock object. ## in fact contig_nbp=[<MagicMock name='mock.contig().n_bases' id='70366068929488'>]. ## change the above type to int becomes ""1"", then the region.end will be 1 to cause",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:2006,deployability,fail,fail,2006," ""//deepvariant/labeler:haplotype_labeler_test"" as following. While suppose this is not related to platform/environment issue? Would you please kindly help to comment how to fix this error? The detailed root cause please refer to the comments inline in the code, thanks in advance :). In the test file of ""deepvariant/labeler/haplotype_labeler_test.py"", the function of ""test_make_labeler_ref"". ```python. def test_make_labeler_ref(self, candidates, truths, expected_start,. expected_end, bufsize):. expected_bases = 'A' * (expected_end - expected_start). ## generate a Mock object instead of real object of InMemoryFastaReader. labeler = _make_labeler(). labeler._ref_reader.query.return_value = expected_bases. labeler_ref = labeler.make_labeler_ref(candidates, truths, bufsize=bufsize). labeler._ref_reader.query.assert_called_once_with(. ranges.make_range('20', expected_start, expected_end)). self.assertEqual(labeler_ref.start, expected_start). self.assertEqual(labeler_ref.end, expected_end). self.assertEqual(. labeler_ref.bases(expected_start, expected_end), expected_bases). ```. So when in the file of ""deepvariant/labeler/haplotype_labeler.py"", the function of ""make_labeler_ref"" will generate an incorrect output as ""self._ref_reader"" is mock. ```python. def make_labeler_ref(self, candidates, true_variants, bufsize=20):. all_variants = candidates + true_variants. contig = all_variants[0].reference_name. start = min(x.start for x in all_variants). end = max(x.end for x in all_variants). ## always output contig_nbp = 1, as self._ref_reader is Mock object. ## in fact contig_nbp=[<MagicMock name='mock.contig().n_bases' id='70366068929488'>]. ## change the above type to int becomes ""1"", then the region.end will be 1 to cause test fail. contig_nbp = self._ref_reader.contig(contig).n_bases . region = ranges.make_range(contig, max(start - 1, 0),. min(end + bufsize, contig_nbp)). ref_bases = self._ref_reader.query(region). return ReferenceRegion(ref_bases, start=region.start). ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:145,integrability,depend,dependencies,145,"test_make_labeler_ref fail due to mock data; Hi There,. I compiled the whole deepvariant on ppc64le with IBM Advance Toolchain 11.0 with all its dependencies on RHEL 7.5. And most test cases passed, but only 2 test cases failed. Found one root cause today of ""//deepvariant/labeler:haplotype_labeler_test"" as following. While suppose this is not related to platform/environment issue? Would you please kindly help to comment how to fix this error? The detailed root cause please refer to the comments inline in the code, thanks in advance :). In the test file of ""deepvariant/labeler/haplotype_labeler_test.py"", the function of ""test_make_labeler_ref"". ```python. def test_make_labeler_ref(self, candidates, truths, expected_start,. expected_end, bufsize):. expected_bases = 'A' * (expected_end - expected_start). ## generate a Mock object instead of real object of InMemoryFastaReader. labeler = _make_labeler(). labeler._ref_reader.query.return_value = expected_bases. labeler_ref = labeler.make_labeler_ref(candidates, truths, bufsize=bufsize). labeler._ref_reader.query.assert_called_once_with(. ranges.make_range('20', expected_start, expected_end)). self.assertEqual(labeler_ref.start, expected_start). self.assertEqual(labeler_ref.end, expected_end). self.assertEqual(. labeler_ref.bases(expected_start, expected_end), expected_bases). ```. So when in the file of ""deepvariant/labeler/haplotype_labeler.py"", the function of ""make_labeler_ref"" will generate an incorrect output as ""self._ref_reader"" is mock. ```python. def make_labeler_ref(self, candidates, true_variants, bufsize=20):. all_variants = candidates + true_variants. contig = all_variants[0].reference_name. start = min(x.start for x in all_variants). end = max(x.end for x in all_variants). ## always output contig_nbp = 1, as self._ref_reader is Mock object. ## in fact contig_nbp=[<MagicMock name='mock.contig().n_bases' id='70366068929488'>]. ## change the above type to int becomes ""1"", then the region.end will be 1 to cause",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:357,interoperability,platform,platform,357,"test_make_labeler_ref fail due to mock data; Hi There,. I compiled the whole deepvariant on ppc64le with IBM Advance Toolchain 11.0 with all its dependencies on RHEL 7.5. And most test cases passed, but only 2 test cases failed. Found one root cause today of ""//deepvariant/labeler:haplotype_labeler_test"" as following. While suppose this is not related to platform/environment issue? Would you please kindly help to comment how to fix this error? The detailed root cause please refer to the comments inline in the code, thanks in advance :). In the test file of ""deepvariant/labeler/haplotype_labeler_test.py"", the function of ""test_make_labeler_ref"". ```python. def test_make_labeler_ref(self, candidates, truths, expected_start,. expected_end, bufsize):. expected_bases = 'A' * (expected_end - expected_start). ## generate a Mock object instead of real object of InMemoryFastaReader. labeler = _make_labeler(). labeler._ref_reader.query.return_value = expected_bases. labeler_ref = labeler.make_labeler_ref(candidates, truths, bufsize=bufsize). labeler._ref_reader.query.assert_called_once_with(. ranges.make_range('20', expected_start, expected_end)). self.assertEqual(labeler_ref.start, expected_start). self.assertEqual(labeler_ref.end, expected_end). self.assertEqual(. labeler_ref.bases(expected_start, expected_end), expected_bases). ```. So when in the file of ""deepvariant/labeler/haplotype_labeler.py"", the function of ""make_labeler_ref"" will generate an incorrect output as ""self._ref_reader"" is mock. ```python. def make_labeler_ref(self, candidates, true_variants, bufsize=20):. all_variants = candidates + true_variants. contig = all_variants[0].reference_name. start = min(x.start for x in all_variants). end = max(x.end for x in all_variants). ## always output contig_nbp = 1, as self._ref_reader is Mock object. ## in fact contig_nbp=[<MagicMock name='mock.contig().n_bases' id='70366068929488'>]. ## change the above type to int becomes ""1"", then the region.end will be 1 to cause",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:145,modifiability,depend,dependencies,145,"test_make_labeler_ref fail due to mock data; Hi There,. I compiled the whole deepvariant on ppc64le with IBM Advance Toolchain 11.0 with all its dependencies on RHEL 7.5. And most test cases passed, but only 2 test cases failed. Found one root cause today of ""//deepvariant/labeler:haplotype_labeler_test"" as following. While suppose this is not related to platform/environment issue? Would you please kindly help to comment how to fix this error? The detailed root cause please refer to the comments inline in the code, thanks in advance :). In the test file of ""deepvariant/labeler/haplotype_labeler_test.py"", the function of ""test_make_labeler_ref"". ```python. def test_make_labeler_ref(self, candidates, truths, expected_start,. expected_end, bufsize):. expected_bases = 'A' * (expected_end - expected_start). ## generate a Mock object instead of real object of InMemoryFastaReader. labeler = _make_labeler(). labeler._ref_reader.query.return_value = expected_bases. labeler_ref = labeler.make_labeler_ref(candidates, truths, bufsize=bufsize). labeler._ref_reader.query.assert_called_once_with(. ranges.make_range('20', expected_start, expected_end)). self.assertEqual(labeler_ref.start, expected_start). self.assertEqual(labeler_ref.end, expected_end). self.assertEqual(. labeler_ref.bases(expected_start, expected_end), expected_bases). ```. So when in the file of ""deepvariant/labeler/haplotype_labeler.py"", the function of ""make_labeler_ref"" will generate an incorrect output as ""self._ref_reader"" is mock. ```python. def make_labeler_ref(self, candidates, true_variants, bufsize=20):. all_variants = candidates + true_variants. contig = all_variants[0].reference_name. start = min(x.start for x in all_variants). end = max(x.end for x in all_variants). ## always output contig_nbp = 1, as self._ref_reader is Mock object. ## in fact contig_nbp=[<MagicMock name='mock.contig().n_bases' id='70366068929488'>]. ## change the above type to int becomes ""1"", then the region.end will be 1 to cause",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:441,performance,error,error,441,"test_make_labeler_ref fail due to mock data; Hi There,. I compiled the whole deepvariant on ppc64le with IBM Advance Toolchain 11.0 with all its dependencies on RHEL 7.5. And most test cases passed, but only 2 test cases failed. Found one root cause today of ""//deepvariant/labeler:haplotype_labeler_test"" as following. While suppose this is not related to platform/environment issue? Would you please kindly help to comment how to fix this error? The detailed root cause please refer to the comments inline in the code, thanks in advance :). In the test file of ""deepvariant/labeler/haplotype_labeler_test.py"", the function of ""test_make_labeler_ref"". ```python. def test_make_labeler_ref(self, candidates, truths, expected_start,. expected_end, bufsize):. expected_bases = 'A' * (expected_end - expected_start). ## generate a Mock object instead of real object of InMemoryFastaReader. labeler = _make_labeler(). labeler._ref_reader.query.return_value = expected_bases. labeler_ref = labeler.make_labeler_ref(candidates, truths, bufsize=bufsize). labeler._ref_reader.query.assert_called_once_with(. ranges.make_range('20', expected_start, expected_end)). self.assertEqual(labeler_ref.start, expected_start). self.assertEqual(labeler_ref.end, expected_end). self.assertEqual(. labeler_ref.bases(expected_start, expected_end), expected_bases). ```. So when in the file of ""deepvariant/labeler/haplotype_labeler.py"", the function of ""make_labeler_ref"" will generate an incorrect output as ""self._ref_reader"" is mock. ```python. def make_labeler_ref(self, candidates, true_variants, bufsize=20):. all_variants = candidates + true_variants. contig = all_variants[0].reference_name. start = min(x.start for x in all_variants). end = max(x.end for x in all_variants). ## always output contig_nbp = 1, as self._ref_reader is Mock object. ## in fact contig_nbp=[<MagicMock name='mock.contig().n_bases' id='70366068929488'>]. ## change the above type to int becomes ""1"", then the region.end will be 1 to cause",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:22,reliability,fail,fail,22,"test_make_labeler_ref fail due to mock data; Hi There,. I compiled the whole deepvariant on ppc64le with IBM Advance Toolchain 11.0 with all its dependencies on RHEL 7.5. And most test cases passed, but only 2 test cases failed. Found one root cause today of ""//deepvariant/labeler:haplotype_labeler_test"" as following. While suppose this is not related to platform/environment issue? Would you please kindly help to comment how to fix this error? The detailed root cause please refer to the comments inline in the code, thanks in advance :). In the test file of ""deepvariant/labeler/haplotype_labeler_test.py"", the function of ""test_make_labeler_ref"". ```python. def test_make_labeler_ref(self, candidates, truths, expected_start,. expected_end, bufsize):. expected_bases = 'A' * (expected_end - expected_start). ## generate a Mock object instead of real object of InMemoryFastaReader. labeler = _make_labeler(). labeler._ref_reader.query.return_value = expected_bases. labeler_ref = labeler.make_labeler_ref(candidates, truths, bufsize=bufsize). labeler._ref_reader.query.assert_called_once_with(. ranges.make_range('20', expected_start, expected_end)). self.assertEqual(labeler_ref.start, expected_start). self.assertEqual(labeler_ref.end, expected_end). self.assertEqual(. labeler_ref.bases(expected_start, expected_end), expected_bases). ```. So when in the file of ""deepvariant/labeler/haplotype_labeler.py"", the function of ""make_labeler_ref"" will generate an incorrect output as ""self._ref_reader"" is mock. ```python. def make_labeler_ref(self, candidates, true_variants, bufsize=20):. all_variants = candidates + true_variants. contig = all_variants[0].reference_name. start = min(x.start for x in all_variants). end = max(x.end for x in all_variants). ## always output contig_nbp = 1, as self._ref_reader is Mock object. ## in fact contig_nbp=[<MagicMock name='mock.contig().n_bases' id='70366068929488'>]. ## change the above type to int becomes ""1"", then the region.end will be 1 to cause",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:221,reliability,fail,failed,221,"test_make_labeler_ref fail due to mock data; Hi There,. I compiled the whole deepvariant on ppc64le with IBM Advance Toolchain 11.0 with all its dependencies on RHEL 7.5. And most test cases passed, but only 2 test cases failed. Found one root cause today of ""//deepvariant/labeler:haplotype_labeler_test"" as following. While suppose this is not related to platform/environment issue? Would you please kindly help to comment how to fix this error? The detailed root cause please refer to the comments inline in the code, thanks in advance :). In the test file of ""deepvariant/labeler/haplotype_labeler_test.py"", the function of ""test_make_labeler_ref"". ```python. def test_make_labeler_ref(self, candidates, truths, expected_start,. expected_end, bufsize):. expected_bases = 'A' * (expected_end - expected_start). ## generate a Mock object instead of real object of InMemoryFastaReader. labeler = _make_labeler(). labeler._ref_reader.query.return_value = expected_bases. labeler_ref = labeler.make_labeler_ref(candidates, truths, bufsize=bufsize). labeler._ref_reader.query.assert_called_once_with(. ranges.make_range('20', expected_start, expected_end)). self.assertEqual(labeler_ref.start, expected_start). self.assertEqual(labeler_ref.end, expected_end). self.assertEqual(. labeler_ref.bases(expected_start, expected_end), expected_bases). ```. So when in the file of ""deepvariant/labeler/haplotype_labeler.py"", the function of ""make_labeler_ref"" will generate an incorrect output as ""self._ref_reader"" is mock. ```python. def make_labeler_ref(self, candidates, true_variants, bufsize=20):. all_variants = candidates + true_variants. contig = all_variants[0].reference_name. start = min(x.start for x in all_variants). end = max(x.end for x in all_variants). ## always output contig_nbp = 1, as self._ref_reader is Mock object. ## in fact contig_nbp=[<MagicMock name='mock.contig().n_bases' id='70366068929488'>]. ## change the above type to int becomes ""1"", then the region.end will be 1 to cause",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:2006,reliability,fail,fail,2006," ""//deepvariant/labeler:haplotype_labeler_test"" as following. While suppose this is not related to platform/environment issue? Would you please kindly help to comment how to fix this error? The detailed root cause please refer to the comments inline in the code, thanks in advance :). In the test file of ""deepvariant/labeler/haplotype_labeler_test.py"", the function of ""test_make_labeler_ref"". ```python. def test_make_labeler_ref(self, candidates, truths, expected_start,. expected_end, bufsize):. expected_bases = 'A' * (expected_end - expected_start). ## generate a Mock object instead of real object of InMemoryFastaReader. labeler = _make_labeler(). labeler._ref_reader.query.return_value = expected_bases. labeler_ref = labeler.make_labeler_ref(candidates, truths, bufsize=bufsize). labeler._ref_reader.query.assert_called_once_with(. ranges.make_range('20', expected_start, expected_end)). self.assertEqual(labeler_ref.start, expected_start). self.assertEqual(labeler_ref.end, expected_end). self.assertEqual(. labeler_ref.bases(expected_start, expected_end), expected_bases). ```. So when in the file of ""deepvariant/labeler/haplotype_labeler.py"", the function of ""make_labeler_ref"" will generate an incorrect output as ""self._ref_reader"" is mock. ```python. def make_labeler_ref(self, candidates, true_variants, bufsize=20):. all_variants = candidates + true_variants. contig = all_variants[0].reference_name. start = min(x.start for x in all_variants). end = max(x.end for x in all_variants). ## always output contig_nbp = 1, as self._ref_reader is Mock object. ## in fact contig_nbp=[<MagicMock name='mock.contig().n_bases' id='70366068929488'>]. ## change the above type to int becomes ""1"", then the region.end will be 1 to cause test fail. contig_nbp = self._ref_reader.contig(contig).n_bases . region = ranges.make_range(contig, max(start - 1, 0),. min(end + bufsize, contig_nbp)). ref_bases = self._ref_reader.query(region). return ReferenceRegion(ref_bases, start=region.start). ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:145,safety,depend,dependencies,145,"test_make_labeler_ref fail due to mock data; Hi There,. I compiled the whole deepvariant on ppc64le with IBM Advance Toolchain 11.0 with all its dependencies on RHEL 7.5. And most test cases passed, but only 2 test cases failed. Found one root cause today of ""//deepvariant/labeler:haplotype_labeler_test"" as following. While suppose this is not related to platform/environment issue? Would you please kindly help to comment how to fix this error? The detailed root cause please refer to the comments inline in the code, thanks in advance :). In the test file of ""deepvariant/labeler/haplotype_labeler_test.py"", the function of ""test_make_labeler_ref"". ```python. def test_make_labeler_ref(self, candidates, truths, expected_start,. expected_end, bufsize):. expected_bases = 'A' * (expected_end - expected_start). ## generate a Mock object instead of real object of InMemoryFastaReader. labeler = _make_labeler(). labeler._ref_reader.query.return_value = expected_bases. labeler_ref = labeler.make_labeler_ref(candidates, truths, bufsize=bufsize). labeler._ref_reader.query.assert_called_once_with(. ranges.make_range('20', expected_start, expected_end)). self.assertEqual(labeler_ref.start, expected_start). self.assertEqual(labeler_ref.end, expected_end). self.assertEqual(. labeler_ref.bases(expected_start, expected_end), expected_bases). ```. So when in the file of ""deepvariant/labeler/haplotype_labeler.py"", the function of ""make_labeler_ref"" will generate an incorrect output as ""self._ref_reader"" is mock. ```python. def make_labeler_ref(self, candidates, true_variants, bufsize=20):. all_variants = candidates + true_variants. contig = all_variants[0].reference_name. start = min(x.start for x in all_variants). end = max(x.end for x in all_variants). ## always output contig_nbp = 1, as self._ref_reader is Mock object. ## in fact contig_nbp=[<MagicMock name='mock.contig().n_bases' id='70366068929488'>]. ## change the above type to int becomes ""1"", then the region.end will be 1 to cause",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:180,safety,test,test,180,"test_make_labeler_ref fail due to mock data; Hi There,. I compiled the whole deepvariant on ppc64le with IBM Advance Toolchain 11.0 with all its dependencies on RHEL 7.5. And most test cases passed, but only 2 test cases failed. Found one root cause today of ""//deepvariant/labeler:haplotype_labeler_test"" as following. While suppose this is not related to platform/environment issue? Would you please kindly help to comment how to fix this error? The detailed root cause please refer to the comments inline in the code, thanks in advance :). In the test file of ""deepvariant/labeler/haplotype_labeler_test.py"", the function of ""test_make_labeler_ref"". ```python. def test_make_labeler_ref(self, candidates, truths, expected_start,. expected_end, bufsize):. expected_bases = 'A' * (expected_end - expected_start). ## generate a Mock object instead of real object of InMemoryFastaReader. labeler = _make_labeler(). labeler._ref_reader.query.return_value = expected_bases. labeler_ref = labeler.make_labeler_ref(candidates, truths, bufsize=bufsize). labeler._ref_reader.query.assert_called_once_with(. ranges.make_range('20', expected_start, expected_end)). self.assertEqual(labeler_ref.start, expected_start). self.assertEqual(labeler_ref.end, expected_end). self.assertEqual(. labeler_ref.bases(expected_start, expected_end), expected_bases). ```. So when in the file of ""deepvariant/labeler/haplotype_labeler.py"", the function of ""make_labeler_ref"" will generate an incorrect output as ""self._ref_reader"" is mock. ```python. def make_labeler_ref(self, candidates, true_variants, bufsize=20):. all_variants = candidates + true_variants. contig = all_variants[0].reference_name. start = min(x.start for x in all_variants). end = max(x.end for x in all_variants). ## always output contig_nbp = 1, as self._ref_reader is Mock object. ## in fact contig_nbp=[<MagicMock name='mock.contig().n_bases' id='70366068929488'>]. ## change the above type to int becomes ""1"", then the region.end will be 1 to cause",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:210,safety,test,test,210,"test_make_labeler_ref fail due to mock data; Hi There,. I compiled the whole deepvariant on ppc64le with IBM Advance Toolchain 11.0 with all its dependencies on RHEL 7.5. And most test cases passed, but only 2 test cases failed. Found one root cause today of ""//deepvariant/labeler:haplotype_labeler_test"" as following. While suppose this is not related to platform/environment issue? Would you please kindly help to comment how to fix this error? The detailed root cause please refer to the comments inline in the code, thanks in advance :). In the test file of ""deepvariant/labeler/haplotype_labeler_test.py"", the function of ""test_make_labeler_ref"". ```python. def test_make_labeler_ref(self, candidates, truths, expected_start,. expected_end, bufsize):. expected_bases = 'A' * (expected_end - expected_start). ## generate a Mock object instead of real object of InMemoryFastaReader. labeler = _make_labeler(). labeler._ref_reader.query.return_value = expected_bases. labeler_ref = labeler.make_labeler_ref(candidates, truths, bufsize=bufsize). labeler._ref_reader.query.assert_called_once_with(. ranges.make_range('20', expected_start, expected_end)). self.assertEqual(labeler_ref.start, expected_start). self.assertEqual(labeler_ref.end, expected_end). self.assertEqual(. labeler_ref.bases(expected_start, expected_end), expected_bases). ```. So when in the file of ""deepvariant/labeler/haplotype_labeler.py"", the function of ""make_labeler_ref"" will generate an incorrect output as ""self._ref_reader"" is mock. ```python. def make_labeler_ref(self, candidates, true_variants, bufsize=20):. all_variants = candidates + true_variants. contig = all_variants[0].reference_name. start = min(x.start for x in all_variants). end = max(x.end for x in all_variants). ## always output contig_nbp = 1, as self._ref_reader is Mock object. ## in fact contig_nbp=[<MagicMock name='mock.contig().n_bases' id='70366068929488'>]. ## change the above type to int becomes ""1"", then the region.end will be 1 to cause",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:441,safety,error,error,441,"test_make_labeler_ref fail due to mock data; Hi There,. I compiled the whole deepvariant on ppc64le with IBM Advance Toolchain 11.0 with all its dependencies on RHEL 7.5. And most test cases passed, but only 2 test cases failed. Found one root cause today of ""//deepvariant/labeler:haplotype_labeler_test"" as following. While suppose this is not related to platform/environment issue? Would you please kindly help to comment how to fix this error? The detailed root cause please refer to the comments inline in the code, thanks in advance :). In the test file of ""deepvariant/labeler/haplotype_labeler_test.py"", the function of ""test_make_labeler_ref"". ```python. def test_make_labeler_ref(self, candidates, truths, expected_start,. expected_end, bufsize):. expected_bases = 'A' * (expected_end - expected_start). ## generate a Mock object instead of real object of InMemoryFastaReader. labeler = _make_labeler(). labeler._ref_reader.query.return_value = expected_bases. labeler_ref = labeler.make_labeler_ref(candidates, truths, bufsize=bufsize). labeler._ref_reader.query.assert_called_once_with(. ranges.make_range('20', expected_start, expected_end)). self.assertEqual(labeler_ref.start, expected_start). self.assertEqual(labeler_ref.end, expected_end). self.assertEqual(. labeler_ref.bases(expected_start, expected_end), expected_bases). ```. So when in the file of ""deepvariant/labeler/haplotype_labeler.py"", the function of ""make_labeler_ref"" will generate an incorrect output as ""self._ref_reader"" is mock. ```python. def make_labeler_ref(self, candidates, true_variants, bufsize=20):. all_variants = candidates + true_variants. contig = all_variants[0].reference_name. start = min(x.start for x in all_variants). end = max(x.end for x in all_variants). ## always output contig_nbp = 1, as self._ref_reader is Mock object. ## in fact contig_nbp=[<MagicMock name='mock.contig().n_bases' id='70366068929488'>]. ## change the above type to int becomes ""1"", then the region.end will be 1 to cause",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:550,safety,test,test,550,"test_make_labeler_ref fail due to mock data; Hi There,. I compiled the whole deepvariant on ppc64le with IBM Advance Toolchain 11.0 with all its dependencies on RHEL 7.5. And most test cases passed, but only 2 test cases failed. Found one root cause today of ""//deepvariant/labeler:haplotype_labeler_test"" as following. While suppose this is not related to platform/environment issue? Would you please kindly help to comment how to fix this error? The detailed root cause please refer to the comments inline in the code, thanks in advance :). In the test file of ""deepvariant/labeler/haplotype_labeler_test.py"", the function of ""test_make_labeler_ref"". ```python. def test_make_labeler_ref(self, candidates, truths, expected_start,. expected_end, bufsize):. expected_bases = 'A' * (expected_end - expected_start). ## generate a Mock object instead of real object of InMemoryFastaReader. labeler = _make_labeler(). labeler._ref_reader.query.return_value = expected_bases. labeler_ref = labeler.make_labeler_ref(candidates, truths, bufsize=bufsize). labeler._ref_reader.query.assert_called_once_with(. ranges.make_range('20', expected_start, expected_end)). self.assertEqual(labeler_ref.start, expected_start). self.assertEqual(labeler_ref.end, expected_end). self.assertEqual(. labeler_ref.bases(expected_start, expected_end), expected_bases). ```. So when in the file of ""deepvariant/labeler/haplotype_labeler.py"", the function of ""make_labeler_ref"" will generate an incorrect output as ""self._ref_reader"" is mock. ```python. def make_labeler_ref(self, candidates, true_variants, bufsize=20):. all_variants = candidates + true_variants. contig = all_variants[0].reference_name. start = min(x.start for x in all_variants). end = max(x.end for x in all_variants). ## always output contig_nbp = 1, as self._ref_reader is Mock object. ## in fact contig_nbp=[<MagicMock name='mock.contig().n_bases' id='70366068929488'>]. ## change the above type to int becomes ""1"", then the region.end will be 1 to cause",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:2001,safety,test,test,2001," ""//deepvariant/labeler:haplotype_labeler_test"" as following. While suppose this is not related to platform/environment issue? Would you please kindly help to comment how to fix this error? The detailed root cause please refer to the comments inline in the code, thanks in advance :). In the test file of ""deepvariant/labeler/haplotype_labeler_test.py"", the function of ""test_make_labeler_ref"". ```python. def test_make_labeler_ref(self, candidates, truths, expected_start,. expected_end, bufsize):. expected_bases = 'A' * (expected_end - expected_start). ## generate a Mock object instead of real object of InMemoryFastaReader. labeler = _make_labeler(). labeler._ref_reader.query.return_value = expected_bases. labeler_ref = labeler.make_labeler_ref(candidates, truths, bufsize=bufsize). labeler._ref_reader.query.assert_called_once_with(. ranges.make_range('20', expected_start, expected_end)). self.assertEqual(labeler_ref.start, expected_start). self.assertEqual(labeler_ref.end, expected_end). self.assertEqual(. labeler_ref.bases(expected_start, expected_end), expected_bases). ```. So when in the file of ""deepvariant/labeler/haplotype_labeler.py"", the function of ""make_labeler_ref"" will generate an incorrect output as ""self._ref_reader"" is mock. ```python. def make_labeler_ref(self, candidates, true_variants, bufsize=20):. all_variants = candidates + true_variants. contig = all_variants[0].reference_name. start = min(x.start for x in all_variants). end = max(x.end for x in all_variants). ## always output contig_nbp = 1, as self._ref_reader is Mock object. ## in fact contig_nbp=[<MagicMock name='mock.contig().n_bases' id='70366068929488'>]. ## change the above type to int becomes ""1"", then the region.end will be 1 to cause test fail. contig_nbp = self._ref_reader.contig(contig).n_bases . region = ranges.make_range(contig, max(start - 1, 0),. min(end + bufsize, contig_nbp)). ref_bases = self._ref_reader.query(region). return ReferenceRegion(ref_bases, start=region.start). ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:34,testability,mock,mock,34,"test_make_labeler_ref fail due to mock data; Hi There,. I compiled the whole deepvariant on ppc64le with IBM Advance Toolchain 11.0 with all its dependencies on RHEL 7.5. And most test cases passed, but only 2 test cases failed. Found one root cause today of ""//deepvariant/labeler:haplotype_labeler_test"" as following. While suppose this is not related to platform/environment issue? Would you please kindly help to comment how to fix this error? The detailed root cause please refer to the comments inline in the code, thanks in advance :). In the test file of ""deepvariant/labeler/haplotype_labeler_test.py"", the function of ""test_make_labeler_ref"". ```python. def test_make_labeler_ref(self, candidates, truths, expected_start,. expected_end, bufsize):. expected_bases = 'A' * (expected_end - expected_start). ## generate a Mock object instead of real object of InMemoryFastaReader. labeler = _make_labeler(). labeler._ref_reader.query.return_value = expected_bases. labeler_ref = labeler.make_labeler_ref(candidates, truths, bufsize=bufsize). labeler._ref_reader.query.assert_called_once_with(. ranges.make_range('20', expected_start, expected_end)). self.assertEqual(labeler_ref.start, expected_start). self.assertEqual(labeler_ref.end, expected_end). self.assertEqual(. labeler_ref.bases(expected_start, expected_end), expected_bases). ```. So when in the file of ""deepvariant/labeler/haplotype_labeler.py"", the function of ""make_labeler_ref"" will generate an incorrect output as ""self._ref_reader"" is mock. ```python. def make_labeler_ref(self, candidates, true_variants, bufsize=20):. all_variants = candidates + true_variants. contig = all_variants[0].reference_name. start = min(x.start for x in all_variants). end = max(x.end for x in all_variants). ## always output contig_nbp = 1, as self._ref_reader is Mock object. ## in fact contig_nbp=[<MagicMock name='mock.contig().n_bases' id='70366068929488'>]. ## change the above type to int becomes ""1"", then the region.end will be 1 to cause",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:145,testability,depend,dependencies,145,"test_make_labeler_ref fail due to mock data; Hi There,. I compiled the whole deepvariant on ppc64le with IBM Advance Toolchain 11.0 with all its dependencies on RHEL 7.5. And most test cases passed, but only 2 test cases failed. Found one root cause today of ""//deepvariant/labeler:haplotype_labeler_test"" as following. While suppose this is not related to platform/environment issue? Would you please kindly help to comment how to fix this error? The detailed root cause please refer to the comments inline in the code, thanks in advance :). In the test file of ""deepvariant/labeler/haplotype_labeler_test.py"", the function of ""test_make_labeler_ref"". ```python. def test_make_labeler_ref(self, candidates, truths, expected_start,. expected_end, bufsize):. expected_bases = 'A' * (expected_end - expected_start). ## generate a Mock object instead of real object of InMemoryFastaReader. labeler = _make_labeler(). labeler._ref_reader.query.return_value = expected_bases. labeler_ref = labeler.make_labeler_ref(candidates, truths, bufsize=bufsize). labeler._ref_reader.query.assert_called_once_with(. ranges.make_range('20', expected_start, expected_end)). self.assertEqual(labeler_ref.start, expected_start). self.assertEqual(labeler_ref.end, expected_end). self.assertEqual(. labeler_ref.bases(expected_start, expected_end), expected_bases). ```. So when in the file of ""deepvariant/labeler/haplotype_labeler.py"", the function of ""make_labeler_ref"" will generate an incorrect output as ""self._ref_reader"" is mock. ```python. def make_labeler_ref(self, candidates, true_variants, bufsize=20):. all_variants = candidates + true_variants. contig = all_variants[0].reference_name. start = min(x.start for x in all_variants). end = max(x.end for x in all_variants). ## always output contig_nbp = 1, as self._ref_reader is Mock object. ## in fact contig_nbp=[<MagicMock name='mock.contig().n_bases' id='70366068929488'>]. ## change the above type to int becomes ""1"", then the region.end will be 1 to cause",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:180,testability,test,test,180,"test_make_labeler_ref fail due to mock data; Hi There,. I compiled the whole deepvariant on ppc64le with IBM Advance Toolchain 11.0 with all its dependencies on RHEL 7.5. And most test cases passed, but only 2 test cases failed. Found one root cause today of ""//deepvariant/labeler:haplotype_labeler_test"" as following. While suppose this is not related to platform/environment issue? Would you please kindly help to comment how to fix this error? The detailed root cause please refer to the comments inline in the code, thanks in advance :). In the test file of ""deepvariant/labeler/haplotype_labeler_test.py"", the function of ""test_make_labeler_ref"". ```python. def test_make_labeler_ref(self, candidates, truths, expected_start,. expected_end, bufsize):. expected_bases = 'A' * (expected_end - expected_start). ## generate a Mock object instead of real object of InMemoryFastaReader. labeler = _make_labeler(). labeler._ref_reader.query.return_value = expected_bases. labeler_ref = labeler.make_labeler_ref(candidates, truths, bufsize=bufsize). labeler._ref_reader.query.assert_called_once_with(. ranges.make_range('20', expected_start, expected_end)). self.assertEqual(labeler_ref.start, expected_start). self.assertEqual(labeler_ref.end, expected_end). self.assertEqual(. labeler_ref.bases(expected_start, expected_end), expected_bases). ```. So when in the file of ""deepvariant/labeler/haplotype_labeler.py"", the function of ""make_labeler_ref"" will generate an incorrect output as ""self._ref_reader"" is mock. ```python. def make_labeler_ref(self, candidates, true_variants, bufsize=20):. all_variants = candidates + true_variants. contig = all_variants[0].reference_name. start = min(x.start for x in all_variants). end = max(x.end for x in all_variants). ## always output contig_nbp = 1, as self._ref_reader is Mock object. ## in fact contig_nbp=[<MagicMock name='mock.contig().n_bases' id='70366068929488'>]. ## change the above type to int becomes ""1"", then the region.end will be 1 to cause",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:210,testability,test,test,210,"test_make_labeler_ref fail due to mock data; Hi There,. I compiled the whole deepvariant on ppc64le with IBM Advance Toolchain 11.0 with all its dependencies on RHEL 7.5. And most test cases passed, but only 2 test cases failed. Found one root cause today of ""//deepvariant/labeler:haplotype_labeler_test"" as following. While suppose this is not related to platform/environment issue? Would you please kindly help to comment how to fix this error? The detailed root cause please refer to the comments inline in the code, thanks in advance :). In the test file of ""deepvariant/labeler/haplotype_labeler_test.py"", the function of ""test_make_labeler_ref"". ```python. def test_make_labeler_ref(self, candidates, truths, expected_start,. expected_end, bufsize):. expected_bases = 'A' * (expected_end - expected_start). ## generate a Mock object instead of real object of InMemoryFastaReader. labeler = _make_labeler(). labeler._ref_reader.query.return_value = expected_bases. labeler_ref = labeler.make_labeler_ref(candidates, truths, bufsize=bufsize). labeler._ref_reader.query.assert_called_once_with(. ranges.make_range('20', expected_start, expected_end)). self.assertEqual(labeler_ref.start, expected_start). self.assertEqual(labeler_ref.end, expected_end). self.assertEqual(. labeler_ref.bases(expected_start, expected_end), expected_bases). ```. So when in the file of ""deepvariant/labeler/haplotype_labeler.py"", the function of ""make_labeler_ref"" will generate an incorrect output as ""self._ref_reader"" is mock. ```python. def make_labeler_ref(self, candidates, true_variants, bufsize=20):. all_variants = candidates + true_variants. contig = all_variants[0].reference_name. start = min(x.start for x in all_variants). end = max(x.end for x in all_variants). ## always output contig_nbp = 1, as self._ref_reader is Mock object. ## in fact contig_nbp=[<MagicMock name='mock.contig().n_bases' id='70366068929488'>]. ## change the above type to int becomes ""1"", then the region.end will be 1 to cause",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:550,testability,test,test,550,"test_make_labeler_ref fail due to mock data; Hi There,. I compiled the whole deepvariant on ppc64le with IBM Advance Toolchain 11.0 with all its dependencies on RHEL 7.5. And most test cases passed, but only 2 test cases failed. Found one root cause today of ""//deepvariant/labeler:haplotype_labeler_test"" as following. While suppose this is not related to platform/environment issue? Would you please kindly help to comment how to fix this error? The detailed root cause please refer to the comments inline in the code, thanks in advance :). In the test file of ""deepvariant/labeler/haplotype_labeler_test.py"", the function of ""test_make_labeler_ref"". ```python. def test_make_labeler_ref(self, candidates, truths, expected_start,. expected_end, bufsize):. expected_bases = 'A' * (expected_end - expected_start). ## generate a Mock object instead of real object of InMemoryFastaReader. labeler = _make_labeler(). labeler._ref_reader.query.return_value = expected_bases. labeler_ref = labeler.make_labeler_ref(candidates, truths, bufsize=bufsize). labeler._ref_reader.query.assert_called_once_with(. ranges.make_range('20', expected_start, expected_end)). self.assertEqual(labeler_ref.start, expected_start). self.assertEqual(labeler_ref.end, expected_end). self.assertEqual(. labeler_ref.bases(expected_start, expected_end), expected_bases). ```. So when in the file of ""deepvariant/labeler/haplotype_labeler.py"", the function of ""make_labeler_ref"" will generate an incorrect output as ""self._ref_reader"" is mock. ```python. def make_labeler_ref(self, candidates, true_variants, bufsize=20):. all_variants = candidates + true_variants. contig = all_variants[0].reference_name. start = min(x.start for x in all_variants). end = max(x.end for x in all_variants). ## always output contig_nbp = 1, as self._ref_reader is Mock object. ## in fact contig_nbp=[<MagicMock name='mock.contig().n_bases' id='70366068929488'>]. ## change the above type to int becomes ""1"", then the region.end will be 1 to cause",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:828,testability,Mock,Mock,828,"test_make_labeler_ref fail due to mock data; Hi There,. I compiled the whole deepvariant on ppc64le with IBM Advance Toolchain 11.0 with all its dependencies on RHEL 7.5. And most test cases passed, but only 2 test cases failed. Found one root cause today of ""//deepvariant/labeler:haplotype_labeler_test"" as following. While suppose this is not related to platform/environment issue? Would you please kindly help to comment how to fix this error? The detailed root cause please refer to the comments inline in the code, thanks in advance :). In the test file of ""deepvariant/labeler/haplotype_labeler_test.py"", the function of ""test_make_labeler_ref"". ```python. def test_make_labeler_ref(self, candidates, truths, expected_start,. expected_end, bufsize):. expected_bases = 'A' * (expected_end - expected_start). ## generate a Mock object instead of real object of InMemoryFastaReader. labeler = _make_labeler(). labeler._ref_reader.query.return_value = expected_bases. labeler_ref = labeler.make_labeler_ref(candidates, truths, bufsize=bufsize). labeler._ref_reader.query.assert_called_once_with(. ranges.make_range('20', expected_start, expected_end)). self.assertEqual(labeler_ref.start, expected_start). self.assertEqual(labeler_ref.end, expected_end). self.assertEqual(. labeler_ref.bases(expected_start, expected_end), expected_bases). ```. So when in the file of ""deepvariant/labeler/haplotype_labeler.py"", the function of ""make_labeler_ref"" will generate an incorrect output as ""self._ref_reader"" is mock. ```python. def make_labeler_ref(self, candidates, true_variants, bufsize=20):. all_variants = candidates + true_variants. contig = all_variants[0].reference_name. start = min(x.start for x in all_variants). end = max(x.end for x in all_variants). ## always output contig_nbp = 1, as self._ref_reader is Mock object. ## in fact contig_nbp=[<MagicMock name='mock.contig().n_bases' id='70366068929488'>]. ## change the above type to int becomes ""1"", then the region.end will be 1 to cause",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:1161,testability,assert,assertEqual,1161,".5. And most test cases passed, but only 2 test cases failed. Found one root cause today of ""//deepvariant/labeler:haplotype_labeler_test"" as following. While suppose this is not related to platform/environment issue? Would you please kindly help to comment how to fix this error? The detailed root cause please refer to the comments inline in the code, thanks in advance :). In the test file of ""deepvariant/labeler/haplotype_labeler_test.py"", the function of ""test_make_labeler_ref"". ```python. def test_make_labeler_ref(self, candidates, truths, expected_start,. expected_end, bufsize):. expected_bases = 'A' * (expected_end - expected_start). ## generate a Mock object instead of real object of InMemoryFastaReader. labeler = _make_labeler(). labeler._ref_reader.query.return_value = expected_bases. labeler_ref = labeler.make_labeler_ref(candidates, truths, bufsize=bufsize). labeler._ref_reader.query.assert_called_once_with(. ranges.make_range('20', expected_start, expected_end)). self.assertEqual(labeler_ref.start, expected_start). self.assertEqual(labeler_ref.end, expected_end). self.assertEqual(. labeler_ref.bases(expected_start, expected_end), expected_bases). ```. So when in the file of ""deepvariant/labeler/haplotype_labeler.py"", the function of ""make_labeler_ref"" will generate an incorrect output as ""self._ref_reader"" is mock. ```python. def make_labeler_ref(self, candidates, true_variants, bufsize=20):. all_variants = candidates + true_variants. contig = all_variants[0].reference_name. start = min(x.start for x in all_variants). end = max(x.end for x in all_variants). ## always output contig_nbp = 1, as self._ref_reader is Mock object. ## in fact contig_nbp=[<MagicMock name='mock.contig().n_bases' id='70366068929488'>]. ## change the above type to int becomes ""1"", then the region.end will be 1 to cause test fail. contig_nbp = self._ref_reader.contig(contig).n_bases . region = ranges.make_range(contig, max(start - 1, 0),. min(end + bufsize, contig_nbp)). ref_bases = ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:1214,testability,assert,assertEqual,1214," failed. Found one root cause today of ""//deepvariant/labeler:haplotype_labeler_test"" as following. While suppose this is not related to platform/environment issue? Would you please kindly help to comment how to fix this error? The detailed root cause please refer to the comments inline in the code, thanks in advance :). In the test file of ""deepvariant/labeler/haplotype_labeler_test.py"", the function of ""test_make_labeler_ref"". ```python. def test_make_labeler_ref(self, candidates, truths, expected_start,. expected_end, bufsize):. expected_bases = 'A' * (expected_end - expected_start). ## generate a Mock object instead of real object of InMemoryFastaReader. labeler = _make_labeler(). labeler._ref_reader.query.return_value = expected_bases. labeler_ref = labeler.make_labeler_ref(candidates, truths, bufsize=bufsize). labeler._ref_reader.query.assert_called_once_with(. ranges.make_range('20', expected_start, expected_end)). self.assertEqual(labeler_ref.start, expected_start). self.assertEqual(labeler_ref.end, expected_end). self.assertEqual(. labeler_ref.bases(expected_start, expected_end), expected_bases). ```. So when in the file of ""deepvariant/labeler/haplotype_labeler.py"", the function of ""make_labeler_ref"" will generate an incorrect output as ""self._ref_reader"" is mock. ```python. def make_labeler_ref(self, candidates, true_variants, bufsize=20):. all_variants = candidates + true_variants. contig = all_variants[0].reference_name. start = min(x.start for x in all_variants). end = max(x.end for x in all_variants). ## always output contig_nbp = 1, as self._ref_reader is Mock object. ## in fact contig_nbp=[<MagicMock name='mock.contig().n_bases' id='70366068929488'>]. ## change the above type to int becomes ""1"", then the region.end will be 1 to cause test fail. contig_nbp = self._ref_reader.contig(contig).n_bases . region = ranges.make_range(contig, max(start - 1, 0),. min(end + bufsize, contig_nbp)). ref_bases = self._ref_reader.query(region). return ReferenceRegio",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:1263,testability,assert,assertEqual,1263," ""//deepvariant/labeler:haplotype_labeler_test"" as following. While suppose this is not related to platform/environment issue? Would you please kindly help to comment how to fix this error? The detailed root cause please refer to the comments inline in the code, thanks in advance :). In the test file of ""deepvariant/labeler/haplotype_labeler_test.py"", the function of ""test_make_labeler_ref"". ```python. def test_make_labeler_ref(self, candidates, truths, expected_start,. expected_end, bufsize):. expected_bases = 'A' * (expected_end - expected_start). ## generate a Mock object instead of real object of InMemoryFastaReader. labeler = _make_labeler(). labeler._ref_reader.query.return_value = expected_bases. labeler_ref = labeler.make_labeler_ref(candidates, truths, bufsize=bufsize). labeler._ref_reader.query.assert_called_once_with(. ranges.make_range('20', expected_start, expected_end)). self.assertEqual(labeler_ref.start, expected_start). self.assertEqual(labeler_ref.end, expected_end). self.assertEqual(. labeler_ref.bases(expected_start, expected_end), expected_bases). ```. So when in the file of ""deepvariant/labeler/haplotype_labeler.py"", the function of ""make_labeler_ref"" will generate an incorrect output as ""self._ref_reader"" is mock. ```python. def make_labeler_ref(self, candidates, true_variants, bufsize=20):. all_variants = candidates + true_variants. contig = all_variants[0].reference_name. start = min(x.start for x in all_variants). end = max(x.end for x in all_variants). ## always output contig_nbp = 1, as self._ref_reader is Mock object. ## in fact contig_nbp=[<MagicMock name='mock.contig().n_bases' id='70366068929488'>]. ## change the above type to int becomes ""1"", then the region.end will be 1 to cause test fail. contig_nbp = self._ref_reader.contig(contig).n_bases . region = ranges.make_range(contig, max(start - 1, 0),. min(end + bufsize, contig_nbp)). ref_bases = self._ref_reader.query(region). return ReferenceRegion(ref_bases, start=region.start). ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:1509,testability,mock,mock,1509," ""//deepvariant/labeler:haplotype_labeler_test"" as following. While suppose this is not related to platform/environment issue? Would you please kindly help to comment how to fix this error? The detailed root cause please refer to the comments inline in the code, thanks in advance :). In the test file of ""deepvariant/labeler/haplotype_labeler_test.py"", the function of ""test_make_labeler_ref"". ```python. def test_make_labeler_ref(self, candidates, truths, expected_start,. expected_end, bufsize):. expected_bases = 'A' * (expected_end - expected_start). ## generate a Mock object instead of real object of InMemoryFastaReader. labeler = _make_labeler(). labeler._ref_reader.query.return_value = expected_bases. labeler_ref = labeler.make_labeler_ref(candidates, truths, bufsize=bufsize). labeler._ref_reader.query.assert_called_once_with(. ranges.make_range('20', expected_start, expected_end)). self.assertEqual(labeler_ref.start, expected_start). self.assertEqual(labeler_ref.end, expected_end). self.assertEqual(. labeler_ref.bases(expected_start, expected_end), expected_bases). ```. So when in the file of ""deepvariant/labeler/haplotype_labeler.py"", the function of ""make_labeler_ref"" will generate an incorrect output as ""self._ref_reader"" is mock. ```python. def make_labeler_ref(self, candidates, true_variants, bufsize=20):. all_variants = candidates + true_variants. contig = all_variants[0].reference_name. start = min(x.start for x in all_variants). end = max(x.end for x in all_variants). ## always output contig_nbp = 1, as self._ref_reader is Mock object. ## in fact contig_nbp=[<MagicMock name='mock.contig().n_bases' id='70366068929488'>]. ## change the above type to int becomes ""1"", then the region.end will be 1 to cause test fail. contig_nbp = self._ref_reader.contig(contig).n_bases . region = ranges.make_range(contig, max(start - 1, 0),. min(end + bufsize, contig_nbp)). ref_bases = self._ref_reader.query(region). return ReferenceRegion(ref_bases, start=region.start). ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:1818,testability,Mock,Mock,1818," ""//deepvariant/labeler:haplotype_labeler_test"" as following. While suppose this is not related to platform/environment issue? Would you please kindly help to comment how to fix this error? The detailed root cause please refer to the comments inline in the code, thanks in advance :). In the test file of ""deepvariant/labeler/haplotype_labeler_test.py"", the function of ""test_make_labeler_ref"". ```python. def test_make_labeler_ref(self, candidates, truths, expected_start,. expected_end, bufsize):. expected_bases = 'A' * (expected_end - expected_start). ## generate a Mock object instead of real object of InMemoryFastaReader. labeler = _make_labeler(). labeler._ref_reader.query.return_value = expected_bases. labeler_ref = labeler.make_labeler_ref(candidates, truths, bufsize=bufsize). labeler._ref_reader.query.assert_called_once_with(. ranges.make_range('20', expected_start, expected_end)). self.assertEqual(labeler_ref.start, expected_start). self.assertEqual(labeler_ref.end, expected_end). self.assertEqual(. labeler_ref.bases(expected_start, expected_end), expected_bases). ```. So when in the file of ""deepvariant/labeler/haplotype_labeler.py"", the function of ""make_labeler_ref"" will generate an incorrect output as ""self._ref_reader"" is mock. ```python. def make_labeler_ref(self, candidates, true_variants, bufsize=20):. all_variants = candidates + true_variants. contig = all_variants[0].reference_name. start = min(x.start for x in all_variants). end = max(x.end for x in all_variants). ## always output contig_nbp = 1, as self._ref_reader is Mock object. ## in fact contig_nbp=[<MagicMock name='mock.contig().n_bases' id='70366068929488'>]. ## change the above type to int becomes ""1"", then the region.end will be 1 to cause test fail. contig_nbp = self._ref_reader.contig(contig).n_bases . region = ranges.make_range(contig, max(start - 1, 0),. min(end + bufsize, contig_nbp)). ref_bases = self._ref_reader.query(region). return ReferenceRegion(ref_bases, start=region.start). ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:1871,testability,mock,mock,1871," ""//deepvariant/labeler:haplotype_labeler_test"" as following. While suppose this is not related to platform/environment issue? Would you please kindly help to comment how to fix this error? The detailed root cause please refer to the comments inline in the code, thanks in advance :). In the test file of ""deepvariant/labeler/haplotype_labeler_test.py"", the function of ""test_make_labeler_ref"". ```python. def test_make_labeler_ref(self, candidates, truths, expected_start,. expected_end, bufsize):. expected_bases = 'A' * (expected_end - expected_start). ## generate a Mock object instead of real object of InMemoryFastaReader. labeler = _make_labeler(). labeler._ref_reader.query.return_value = expected_bases. labeler_ref = labeler.make_labeler_ref(candidates, truths, bufsize=bufsize). labeler._ref_reader.query.assert_called_once_with(. ranges.make_range('20', expected_start, expected_end)). self.assertEqual(labeler_ref.start, expected_start). self.assertEqual(labeler_ref.end, expected_end). self.assertEqual(. labeler_ref.bases(expected_start, expected_end), expected_bases). ```. So when in the file of ""deepvariant/labeler/haplotype_labeler.py"", the function of ""make_labeler_ref"" will generate an incorrect output as ""self._ref_reader"" is mock. ```python. def make_labeler_ref(self, candidates, true_variants, bufsize=20):. all_variants = candidates + true_variants. contig = all_variants[0].reference_name. start = min(x.start for x in all_variants). end = max(x.end for x in all_variants). ## always output contig_nbp = 1, as self._ref_reader is Mock object. ## in fact contig_nbp=[<MagicMock name='mock.contig().n_bases' id='70366068929488'>]. ## change the above type to int becomes ""1"", then the region.end will be 1 to cause test fail. contig_nbp = self._ref_reader.contig(contig).n_bases . region = ranges.make_range(contig, max(start - 1, 0),. min(end + bufsize, contig_nbp)). ref_bases = self._ref_reader.query(region). return ReferenceRegion(ref_bases, start=region.start). ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:2001,testability,test,test,2001," ""//deepvariant/labeler:haplotype_labeler_test"" as following. While suppose this is not related to platform/environment issue? Would you please kindly help to comment how to fix this error? The detailed root cause please refer to the comments inline in the code, thanks in advance :). In the test file of ""deepvariant/labeler/haplotype_labeler_test.py"", the function of ""test_make_labeler_ref"". ```python. def test_make_labeler_ref(self, candidates, truths, expected_start,. expected_end, bufsize):. expected_bases = 'A' * (expected_end - expected_start). ## generate a Mock object instead of real object of InMemoryFastaReader. labeler = _make_labeler(). labeler._ref_reader.query.return_value = expected_bases. labeler_ref = labeler.make_labeler_ref(candidates, truths, bufsize=bufsize). labeler._ref_reader.query.assert_called_once_with(. ranges.make_range('20', expected_start, expected_end)). self.assertEqual(labeler_ref.start, expected_start). self.assertEqual(labeler_ref.end, expected_end). self.assertEqual(. labeler_ref.bases(expected_start, expected_end), expected_bases). ```. So when in the file of ""deepvariant/labeler/haplotype_labeler.py"", the function of ""make_labeler_ref"" will generate an incorrect output as ""self._ref_reader"" is mock. ```python. def make_labeler_ref(self, candidates, true_variants, bufsize=20):. all_variants = candidates + true_variants. contig = all_variants[0].reference_name. start = min(x.start for x in all_variants). end = max(x.end for x in all_variants). ## always output contig_nbp = 1, as self._ref_reader is Mock object. ## in fact contig_nbp=[<MagicMock name='mock.contig().n_bases' id='70366068929488'>]. ## change the above type to int becomes ""1"", then the region.end will be 1 to cause test fail. contig_nbp = self._ref_reader.contig(contig).n_bases . region = ranges.make_range(contig, max(start - 1, 0),. min(end + bufsize, contig_nbp)). ref_bases = self._ref_reader.query(region). return ReferenceRegion(ref_bases, start=region.start). ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:117,usability,Tool,Toolchain,117,"test_make_labeler_ref fail due to mock data; Hi There,. I compiled the whole deepvariant on ppc64le with IBM Advance Toolchain 11.0 with all its dependencies on RHEL 7.5. And most test cases passed, but only 2 test cases failed. Found one root cause today of ""//deepvariant/labeler:haplotype_labeler_test"" as following. While suppose this is not related to platform/environment issue? Would you please kindly help to comment how to fix this error? The detailed root cause please refer to the comments inline in the code, thanks in advance :). In the test file of ""deepvariant/labeler/haplotype_labeler_test.py"", the function of ""test_make_labeler_ref"". ```python. def test_make_labeler_ref(self, candidates, truths, expected_start,. expected_end, bufsize):. expected_bases = 'A' * (expected_end - expected_start). ## generate a Mock object instead of real object of InMemoryFastaReader. labeler = _make_labeler(). labeler._ref_reader.query.return_value = expected_bases. labeler_ref = labeler.make_labeler_ref(candidates, truths, bufsize=bufsize). labeler._ref_reader.query.assert_called_once_with(. ranges.make_range('20', expected_start, expected_end)). self.assertEqual(labeler_ref.start, expected_start). self.assertEqual(labeler_ref.end, expected_end). self.assertEqual(. labeler_ref.bases(expected_start, expected_end), expected_bases). ```. So when in the file of ""deepvariant/labeler/haplotype_labeler.py"", the function of ""make_labeler_ref"" will generate an incorrect output as ""self._ref_reader"" is mock. ```python. def make_labeler_ref(self, candidates, true_variants, bufsize=20):. all_variants = candidates + true_variants. contig = all_variants[0].reference_name. start = min(x.start for x in all_variants). end = max(x.end for x in all_variants). ## always output contig_nbp = 1, as self._ref_reader is Mock object. ## in fact contig_nbp=[<MagicMock name='mock.contig().n_bases' id='70366068929488'>]. ## change the above type to int becomes ""1"", then the region.end will be 1 to cause",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:409,usability,help,help,409,"test_make_labeler_ref fail due to mock data; Hi There,. I compiled the whole deepvariant on ppc64le with IBM Advance Toolchain 11.0 with all its dependencies on RHEL 7.5. And most test cases passed, but only 2 test cases failed. Found one root cause today of ""//deepvariant/labeler:haplotype_labeler_test"" as following. While suppose this is not related to platform/environment issue? Would you please kindly help to comment how to fix this error? The detailed root cause please refer to the comments inline in the code, thanks in advance :). In the test file of ""deepvariant/labeler/haplotype_labeler_test.py"", the function of ""test_make_labeler_ref"". ```python. def test_make_labeler_ref(self, candidates, truths, expected_start,. expected_end, bufsize):. expected_bases = 'A' * (expected_end - expected_start). ## generate a Mock object instead of real object of InMemoryFastaReader. labeler = _make_labeler(). labeler._ref_reader.query.return_value = expected_bases. labeler_ref = labeler.make_labeler_ref(candidates, truths, bufsize=bufsize). labeler._ref_reader.query.assert_called_once_with(. ranges.make_range('20', expected_start, expected_end)). self.assertEqual(labeler_ref.start, expected_start). self.assertEqual(labeler_ref.end, expected_end). self.assertEqual(. labeler_ref.bases(expected_start, expected_end), expected_bases). ```. So when in the file of ""deepvariant/labeler/haplotype_labeler.py"", the function of ""make_labeler_ref"" will generate an incorrect output as ""self._ref_reader"" is mock. ```python. def make_labeler_ref(self, candidates, true_variants, bufsize=20):. all_variants = candidates + true_variants. contig = all_variants[0].reference_name. start = min(x.start for x in all_variants). end = max(x.end for x in all_variants). ## always output contig_nbp = 1, as self._ref_reader is Mock object. ## in fact contig_nbp=[<MagicMock name='mock.contig().n_bases' id='70366068929488'>]. ## change the above type to int becomes ""1"", then the region.end will be 1 to cause",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/154:441,usability,error,error,441,"test_make_labeler_ref fail due to mock data; Hi There,. I compiled the whole deepvariant on ppc64le with IBM Advance Toolchain 11.0 with all its dependencies on RHEL 7.5. And most test cases passed, but only 2 test cases failed. Found one root cause today of ""//deepvariant/labeler:haplotype_labeler_test"" as following. While suppose this is not related to platform/environment issue? Would you please kindly help to comment how to fix this error? The detailed root cause please refer to the comments inline in the code, thanks in advance :). In the test file of ""deepvariant/labeler/haplotype_labeler_test.py"", the function of ""test_make_labeler_ref"". ```python. def test_make_labeler_ref(self, candidates, truths, expected_start,. expected_end, bufsize):. expected_bases = 'A' * (expected_end - expected_start). ## generate a Mock object instead of real object of InMemoryFastaReader. labeler = _make_labeler(). labeler._ref_reader.query.return_value = expected_bases. labeler_ref = labeler.make_labeler_ref(candidates, truths, bufsize=bufsize). labeler._ref_reader.query.assert_called_once_with(. ranges.make_range('20', expected_start, expected_end)). self.assertEqual(labeler_ref.start, expected_start). self.assertEqual(labeler_ref.end, expected_end). self.assertEqual(. labeler_ref.bases(expected_start, expected_end), expected_bases). ```. So when in the file of ""deepvariant/labeler/haplotype_labeler.py"", the function of ""make_labeler_ref"" will generate an incorrect output as ""self._ref_reader"" is mock. ```python. def make_labeler_ref(self, candidates, true_variants, bufsize=20):. all_variants = candidates + true_variants. contig = all_variants[0].reference_name. start = min(x.start for x in all_variants). end = max(x.end for x in all_variants). ## always output contig_nbp = 1, as self._ref_reader is Mock object. ## in fact contig_nbp=[<MagicMock name='mock.contig().n_bases' id='70366068929488'>]. ## change the above type to int becomes ""1"", then the region.end will be 1 to cause",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/154
https://github.com/google/deepvariant/issues/155:112,availability,error,error,112,"AttributeError: 'IntervalTree' object has no attribute 'merge_overlaps'; Hi Deepvariant developer:. I paste the error report when I run ""make_examples"" for testing dataset:. python /opt/deepvariant/bin/make_examples.zip \. --mode calling \. --ref /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \. --reads /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \. --examples /usit/abel/u1/senz/DeepVariant0.7.2/quickstart_output/examples.tfrecord.gz. 2019-02-17 16:15:42.409205: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring: . I0217 16:15:42.409567 139914391602944 genomics_reader.py:213] Reading /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0217 16:15:42.417557 139914391602944 make_examples.py:1080] Preparing inputs. 2019-02-17 16:15:42.422480: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring: . I0217 16:15:42.422776 139914391602944 genomics_reader.py:213] Reading /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0217 16:15:42.424446 139914391602944 make_examples.py:996] Common contigs are [u'chr20']. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1205, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1195, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1081, in make_examples_runner. regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", lin",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:1425,deployability,modul,module,1425,"bam \. --examples /usit/abel/u1/senz/DeepVariant0.7.2/quickstart_output/examples.tfrecord.gz. 2019-02-17 16:15:42.409205: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring: . I0217 16:15:42.409567 139914391602944 genomics_reader.py:213] Reading /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0217 16:15:42.417557 139914391602944 make_examples.py:1080] Preparing inputs. 2019-02-17 16:15:42.422480: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring: . I0217 16:15:42.422776 139914391602944 genomics_reader.py:213] Reading /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0217 16:15:42.424446 139914391602944 make_examples.py:996] Common contigs are [u'chr20']. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1205, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1195, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1081, in make_examples_runner. regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 998, in processing_regions_from_options. options.exclude_calling_regions). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 583, in build_calling_regions. regions = ranges.RangeSet.from_contigs(contigs). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 168, in from_contigs. contigs). File ""/tmp/Bazel.runfiles_DVDpl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:1511,interoperability,platform,platform,1511,"rd.gz. 2019-02-17 16:15:42.409205: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring: . I0217 16:15:42.409567 139914391602944 genomics_reader.py:213] Reading /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0217 16:15:42.417557 139914391602944 make_examples.py:1080] Preparing inputs. 2019-02-17 16:15:42.422480: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring: . I0217 16:15:42.422776 139914391602944 genomics_reader.py:213] Reading /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0217 16:15:42.424446 139914391602944 make_examples.py:996] Common contigs are [u'chr20']. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1205, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1195, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1081, in make_examples_runner. regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 998, in processing_regions_from_options. options.exclude_calling_regions). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 583, in build_calling_regions. regions = ranges.RangeSet.from_contigs(contigs). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 168, in from_contigs. contigs). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 125, in __i",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:1425,modifiability,modul,module,1425,"bam \. --examples /usit/abel/u1/senz/DeepVariant0.7.2/quickstart_output/examples.tfrecord.gz. 2019-02-17 16:15:42.409205: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring: . I0217 16:15:42.409567 139914391602944 genomics_reader.py:213] Reading /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0217 16:15:42.417557 139914391602944 make_examples.py:1080] Preparing inputs. 2019-02-17 16:15:42.422480: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring: . I0217 16:15:42.422776 139914391602944 genomics_reader.py:213] Reading /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0217 16:15:42.424446 139914391602944 make_examples.py:996] Common contigs are [u'chr20']. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1205, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1195, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1081, in make_examples_runner. regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 998, in processing_regions_from_options. options.exclude_calling_regions). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 583, in build_calling_regions. regions = ranges.RangeSet.from_contigs(contigs). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 168, in from_contigs. contigs). File ""/tmp/Bazel.runfiles_DVDpl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:1484,modifiability,pac,packages,1484,"tart_output/examples.tfrecord.gz. 2019-02-17 16:15:42.409205: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring: . I0217 16:15:42.409567 139914391602944 genomics_reader.py:213] Reading /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0217 16:15:42.417557 139914391602944 make_examples.py:1080] Preparing inputs. 2019-02-17 16:15:42.422480: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring: . I0217 16:15:42.422776 139914391602944 genomics_reader.py:213] Reading /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0217 16:15:42.424446 139914391602944 make_examples.py:996] Common contigs are [u'chr20']. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1205, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1195, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1081, in make_examples_runner. regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 998, in processing_regions_from_options. options.exclude_calling_regions). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 583, in build_calling_regions. regions = ranges.RangeSet.from_contigs(contigs). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 168, in from_contigs. contigs). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/third_party/nucleus/util/r",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:2651,modifiability,pac,package,2651,"2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0217 16:15:42.417557 139914391602944 make_examples.py:1080] Preparing inputs. 2019-02-17 16:15:42.422480: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring: . I0217 16:15:42.422776 139914391602944 genomics_reader.py:213] Reading /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0217 16:15:42.424446 139914391602944 make_examples.py:996] Common contigs are [u'chr20']. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1205, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1195, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1081, in make_examples_runner. regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 998, in processing_regions_from_options. options.exclude_calling_regions). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 583, in build_calling_regions. regions = ranges.RangeSet.from_contigs(contigs). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 168, in from_contigs. contigs). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 125, in __init__. tree.merge_overlaps(). AttributeError: 'IntervalTree' object has no attribute 'merge_overlaps'. I attempted to change the python package intervaltree as v2.1.0, but still received such report. Does anybody can help me?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:112,performance,error,error,112,"AttributeError: 'IntervalTree' object has no attribute 'merge_overlaps'; Hi Deepvariant developer:. I paste the error report when I run ""make_examples"" for testing dataset:. python /opt/deepvariant/bin/make_examples.zip \. --mode calling \. --ref /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \. --reads /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \. --examples /usit/abel/u1/senz/DeepVariant0.7.2/quickstart_output/examples.tfrecord.gz. 2019-02-17 16:15:42.409205: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring: . I0217 16:15:42.409567 139914391602944 genomics_reader.py:213] Reading /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0217 16:15:42.417557 139914391602944 make_examples.py:1080] Preparing inputs. 2019-02-17 16:15:42.422480: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring: . I0217 16:15:42.422776 139914391602944 genomics_reader.py:213] Reading /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0217 16:15:42.424446 139914391602944 make_examples.py:996] Common contigs are [u'chr20']. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1205, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1195, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1081, in make_examples_runner. regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", lin",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:2715,reliability,Doe,Does,2715,"2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0217 16:15:42.417557 139914391602944 make_examples.py:1080] Preparing inputs. 2019-02-17 16:15:42.422480: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring: . I0217 16:15:42.422776 139914391602944 genomics_reader.py:213] Reading /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0217 16:15:42.424446 139914391602944 make_examples.py:996] Common contigs are [u'chr20']. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1205, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1195, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1081, in make_examples_runner. regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 998, in processing_regions_from_options. options.exclude_calling_regions). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 583, in build_calling_regions. regions = ranges.RangeSet.from_contigs(contigs). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 168, in from_contigs. contigs). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 125, in __init__. tree.merge_overlaps(). AttributeError: 'IntervalTree' object has no attribute 'merge_overlaps'. I attempted to change the python package intervaltree as v2.1.0, but still received such report. Does anybody can help me?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:112,safety,error,error,112,"AttributeError: 'IntervalTree' object has no attribute 'merge_overlaps'; Hi Deepvariant developer:. I paste the error report when I run ""make_examples"" for testing dataset:. python /opt/deepvariant/bin/make_examples.zip \. --mode calling \. --ref /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \. --reads /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \. --examples /usit/abel/u1/senz/DeepVariant0.7.2/quickstart_output/examples.tfrecord.gz. 2019-02-17 16:15:42.409205: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring: . I0217 16:15:42.409567 139914391602944 genomics_reader.py:213] Reading /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0217 16:15:42.417557 139914391602944 make_examples.py:1080] Preparing inputs. 2019-02-17 16:15:42.422480: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring: . I0217 16:15:42.422776 139914391602944 genomics_reader.py:213] Reading /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0217 16:15:42.424446 139914391602944 make_examples.py:996] Common contigs are [u'chr20']. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1205, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1195, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1081, in make_examples_runner. regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", lin",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:156,safety,test,testing,156,"AttributeError: 'IntervalTree' object has no attribute 'merge_overlaps'; Hi Deepvariant developer:. I paste the error report when I run ""make_examples"" for testing dataset:. python /opt/deepvariant/bin/make_examples.zip \. --mode calling \. --ref /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \. --reads /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \. --examples /usit/abel/u1/senz/DeepVariant0.7.2/quickstart_output/examples.tfrecord.gz. 2019-02-17 16:15:42.409205: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring: . I0217 16:15:42.409567 139914391602944 genomics_reader.py:213] Reading /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0217 16:15:42.417557 139914391602944 make_examples.py:1080] Preparing inputs. 2019-02-17 16:15:42.422480: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring: . I0217 16:15:42.422776 139914391602944 genomics_reader.py:213] Reading /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0217 16:15:42.424446 139914391602944 make_examples.py:996] Common contigs are [u'chr20']. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1205, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1195, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1081, in make_examples_runner. regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", lin",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:294,safety,test,testdata,294,"AttributeError: 'IntervalTree' object has no attribute 'merge_overlaps'; Hi Deepvariant developer:. I paste the error report when I run ""make_examples"" for testing dataset:. python /opt/deepvariant/bin/make_examples.zip \. --mode calling \. --ref /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \. --reads /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \. --examples /usit/abel/u1/senz/DeepVariant0.7.2/quickstart_output/examples.tfrecord.gz. 2019-02-17 16:15:42.409205: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring: . I0217 16:15:42.409567 139914391602944 genomics_reader.py:213] Reading /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0217 16:15:42.417557 139914391602944 make_examples.py:1080] Preparing inputs. 2019-02-17 16:15:42.422480: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring: . I0217 16:15:42.422776 139914391602944 genomics_reader.py:213] Reading /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0217 16:15:42.424446 139914391602944 make_examples.py:996] Common contigs are [u'chr20']. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1205, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1195, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1081, in make_examples_runner. regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", lin",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:392,safety,test,testdata,392,"AttributeError: 'IntervalTree' object has no attribute 'merge_overlaps'; Hi Deepvariant developer:. I paste the error report when I run ""make_examples"" for testing dataset:. python /opt/deepvariant/bin/make_examples.zip \. --mode calling \. --ref /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \. --reads /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \. --examples /usit/abel/u1/senz/DeepVariant0.7.2/quickstart_output/examples.tfrecord.gz. 2019-02-17 16:15:42.409205: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring: . I0217 16:15:42.409567 139914391602944 genomics_reader.py:213] Reading /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0217 16:15:42.417557 139914391602944 make_examples.py:1080] Preparing inputs. 2019-02-17 16:15:42.422480: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring: . I0217 16:15:42.422776 139914391602944 genomics_reader.py:213] Reading /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0217 16:15:42.424446 139914391602944 make_examples.py:996] Common contigs are [u'chr20']. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1205, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1195, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1081, in make_examples_runner. regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", lin",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:753,safety,test,testdata,753,"AttributeError: 'IntervalTree' object has no attribute 'merge_overlaps'; Hi Deepvariant developer:. I paste the error report when I run ""make_examples"" for testing dataset:. python /opt/deepvariant/bin/make_examples.zip \. --mode calling \. --ref /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \. --reads /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \. --examples /usit/abel/u1/senz/DeepVariant0.7.2/quickstart_output/examples.tfrecord.gz. 2019-02-17 16:15:42.409205: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring: . I0217 16:15:42.409567 139914391602944 genomics_reader.py:213] Reading /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0217 16:15:42.417557 139914391602944 make_examples.py:1080] Preparing inputs. 2019-02-17 16:15:42.422480: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring: . I0217 16:15:42.422776 139914391602944 genomics_reader.py:213] Reading /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0217 16:15:42.424446 139914391602944 make_examples.py:996] Common contigs are [u'chr20']. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1205, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1195, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1081, in make_examples_runner. regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", lin",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:886,safety,input,inputs,886,"AttributeError: 'IntervalTree' object has no attribute 'merge_overlaps'; Hi Deepvariant developer:. I paste the error report when I run ""make_examples"" for testing dataset:. python /opt/deepvariant/bin/make_examples.zip \. --mode calling \. --ref /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \. --reads /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \. --examples /usit/abel/u1/senz/DeepVariant0.7.2/quickstart_output/examples.tfrecord.gz. 2019-02-17 16:15:42.409205: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring: . I0217 16:15:42.409567 139914391602944 genomics_reader.py:213] Reading /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0217 16:15:42.417557 139914391602944 make_examples.py:1080] Preparing inputs. 2019-02-17 16:15:42.422480: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring: . I0217 16:15:42.422776 139914391602944 genomics_reader.py:213] Reading /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0217 16:15:42.424446 139914391602944 make_examples.py:996] Common contigs are [u'chr20']. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1205, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1195, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1081, in make_examples_runner. regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", lin",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:1125,safety,test,testdata,1125," I run ""make_examples"" for testing dataset:. python /opt/deepvariant/bin/make_examples.zip \. --mode calling \. --ref /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \. --reads /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \. --examples /usit/abel/u1/senz/DeepVariant0.7.2/quickstart_output/examples.tfrecord.gz. 2019-02-17 16:15:42.409205: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring: . I0217 16:15:42.409567 139914391602944 genomics_reader.py:213] Reading /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0217 16:15:42.417557 139914391602944 make_examples.py:1080] Preparing inputs. 2019-02-17 16:15:42.422480: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring: . I0217 16:15:42.422776 139914391602944 genomics_reader.py:213] Reading /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0217 16:15:42.424446 139914391602944 make_examples.py:996] Common contigs are [u'chr20']. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1205, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1195, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1081, in make_examples_runner. regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 998, in processing_regions_from_options. options.exclude_calling_regions). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:1425,safety,modul,module,1425,"bam \. --examples /usit/abel/u1/senz/DeepVariant0.7.2/quickstart_output/examples.tfrecord.gz. 2019-02-17 16:15:42.409205: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring: . I0217 16:15:42.409567 139914391602944 genomics_reader.py:213] Reading /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0217 16:15:42.417557 139914391602944 make_examples.py:1080] Preparing inputs. 2019-02-17 16:15:42.422480: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring: . I0217 16:15:42.422776 139914391602944 genomics_reader.py:213] Reading /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0217 16:15:42.424446 139914391602944 make_examples.py:996] Common contigs are [u'chr20']. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1205, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1195, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1081, in make_examples_runner. regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 998, in processing_regions_from_options. options.exclude_calling_regions). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 583, in build_calling_regions. regions = ranges.RangeSet.from_contigs(contigs). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 168, in from_contigs. contigs). File ""/tmp/Bazel.runfiles_DVDpl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:156,testability,test,testing,156,"AttributeError: 'IntervalTree' object has no attribute 'merge_overlaps'; Hi Deepvariant developer:. I paste the error report when I run ""make_examples"" for testing dataset:. python /opt/deepvariant/bin/make_examples.zip \. --mode calling \. --ref /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \. --reads /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \. --examples /usit/abel/u1/senz/DeepVariant0.7.2/quickstart_output/examples.tfrecord.gz. 2019-02-17 16:15:42.409205: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring: . I0217 16:15:42.409567 139914391602944 genomics_reader.py:213] Reading /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0217 16:15:42.417557 139914391602944 make_examples.py:1080] Preparing inputs. 2019-02-17 16:15:42.422480: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring: . I0217 16:15:42.422776 139914391602944 genomics_reader.py:213] Reading /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0217 16:15:42.424446 139914391602944 make_examples.py:996] Common contigs are [u'chr20']. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1205, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1195, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1081, in make_examples_runner. regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", lin",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:294,testability,test,testdata,294,"AttributeError: 'IntervalTree' object has no attribute 'merge_overlaps'; Hi Deepvariant developer:. I paste the error report when I run ""make_examples"" for testing dataset:. python /opt/deepvariant/bin/make_examples.zip \. --mode calling \. --ref /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \. --reads /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \. --examples /usit/abel/u1/senz/DeepVariant0.7.2/quickstart_output/examples.tfrecord.gz. 2019-02-17 16:15:42.409205: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring: . I0217 16:15:42.409567 139914391602944 genomics_reader.py:213] Reading /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0217 16:15:42.417557 139914391602944 make_examples.py:1080] Preparing inputs. 2019-02-17 16:15:42.422480: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring: . I0217 16:15:42.422776 139914391602944 genomics_reader.py:213] Reading /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0217 16:15:42.424446 139914391602944 make_examples.py:996] Common contigs are [u'chr20']. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1205, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1195, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1081, in make_examples_runner. regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", lin",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:319,testability,unit,unittest,319,"AttributeError: 'IntervalTree' object has no attribute 'merge_overlaps'; Hi Deepvariant developer:. I paste the error report when I run ""make_examples"" for testing dataset:. python /opt/deepvariant/bin/make_examples.zip \. --mode calling \. --ref /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \. --reads /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \. --examples /usit/abel/u1/senz/DeepVariant0.7.2/quickstart_output/examples.tfrecord.gz. 2019-02-17 16:15:42.409205: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring: . I0217 16:15:42.409567 139914391602944 genomics_reader.py:213] Reading /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0217 16:15:42.417557 139914391602944 make_examples.py:1080] Preparing inputs. 2019-02-17 16:15:42.422480: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring: . I0217 16:15:42.422776 139914391602944 genomics_reader.py:213] Reading /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0217 16:15:42.424446 139914391602944 make_examples.py:996] Common contigs are [u'chr20']. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1205, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1195, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1081, in make_examples_runner. regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", lin",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:392,testability,test,testdata,392,"AttributeError: 'IntervalTree' object has no attribute 'merge_overlaps'; Hi Deepvariant developer:. I paste the error report when I run ""make_examples"" for testing dataset:. python /opt/deepvariant/bin/make_examples.zip \. --mode calling \. --ref /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \. --reads /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \. --examples /usit/abel/u1/senz/DeepVariant0.7.2/quickstart_output/examples.tfrecord.gz. 2019-02-17 16:15:42.409205: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring: . I0217 16:15:42.409567 139914391602944 genomics_reader.py:213] Reading /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0217 16:15:42.417557 139914391602944 make_examples.py:1080] Preparing inputs. 2019-02-17 16:15:42.422480: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring: . I0217 16:15:42.422776 139914391602944 genomics_reader.py:213] Reading /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0217 16:15:42.424446 139914391602944 make_examples.py:996] Common contigs are [u'chr20']. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1205, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1195, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1081, in make_examples_runner. regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", lin",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:753,testability,test,testdata,753,"AttributeError: 'IntervalTree' object has no attribute 'merge_overlaps'; Hi Deepvariant developer:. I paste the error report when I run ""make_examples"" for testing dataset:. python /opt/deepvariant/bin/make_examples.zip \. --mode calling \. --ref /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \. --reads /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \. --examples /usit/abel/u1/senz/DeepVariant0.7.2/quickstart_output/examples.tfrecord.gz. 2019-02-17 16:15:42.409205: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring: . I0217 16:15:42.409567 139914391602944 genomics_reader.py:213] Reading /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0217 16:15:42.417557 139914391602944 make_examples.py:1080] Preparing inputs. 2019-02-17 16:15:42.422480: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring: . I0217 16:15:42.422776 139914391602944 genomics_reader.py:213] Reading /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0217 16:15:42.424446 139914391602944 make_examples.py:996] Common contigs are [u'chr20']. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1205, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1195, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1081, in make_examples_runner. regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", lin",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:1125,testability,test,testdata,1125," I run ""make_examples"" for testing dataset:. python /opt/deepvariant/bin/make_examples.zip \. --mode calling \. --ref /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \. --reads /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \. --examples /usit/abel/u1/senz/DeepVariant0.7.2/quickstart_output/examples.tfrecord.gz. 2019-02-17 16:15:42.409205: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring: . I0217 16:15:42.409567 139914391602944 genomics_reader.py:213] Reading /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0217 16:15:42.417557 139914391602944 make_examples.py:1080] Preparing inputs. 2019-02-17 16:15:42.422480: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring: . I0217 16:15:42.422776 139914391602944 genomics_reader.py:213] Reading /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0217 16:15:42.424446 139914391602944 make_examples.py:996] Common contigs are [u'chr20']. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1205, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1195, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1081, in make_examples_runner. regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 998, in processing_regions_from_options. options.exclude_calling_regions). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:1278,testability,Trace,Traceback,1278,"quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \. --reads /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \. --examples /usit/abel/u1/senz/DeepVariant0.7.2/quickstart_output/examples.tfrecord.gz. 2019-02-17 16:15:42.409205: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring: . I0217 16:15:42.409567 139914391602944 genomics_reader.py:213] Reading /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0217 16:15:42.417557 139914391602944 make_examples.py:1080] Preparing inputs. 2019-02-17 16:15:42.422480: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring: . I0217 16:15:42.422776 139914391602944 genomics_reader.py:213] Reading /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0217 16:15:42.424446 139914391602944 make_examples.py:996] Common contigs are [u'chr20']. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1205, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1195, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1081, in make_examples_runner. regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 998, in processing_regions_from_options. options.exclude_calling_regions). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 583, in build_calling_regions. regions = ranges.RangeSet.from_contigs(contigs). File ""/tmp/Bazel.runfiles",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:112,usability,error,error,112,"AttributeError: 'IntervalTree' object has no attribute 'merge_overlaps'; Hi Deepvariant developer:. I paste the error report when I run ""make_examples"" for testing dataset:. python /opt/deepvariant/bin/make_examples.zip \. --mode calling \. --ref /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \. --reads /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \. --examples /usit/abel/u1/senz/DeepVariant0.7.2/quickstart_output/examples.tfrecord.gz. 2019-02-17 16:15:42.409205: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring: . I0217 16:15:42.409567 139914391602944 genomics_reader.py:213] Reading /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0217 16:15:42.417557 139914391602944 make_examples.py:1080] Preparing inputs. 2019-02-17 16:15:42.422480: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring: . I0217 16:15:42.422776 139914391602944 genomics_reader.py:213] Reading /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0217 16:15:42.424446 139914391602944 make_examples.py:996] Common contigs are [u'chr20']. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1205, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1195, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1081, in make_examples_runner. regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", lin",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:886,usability,input,inputs,886,"AttributeError: 'IntervalTree' object has no attribute 'merge_overlaps'; Hi Deepvariant developer:. I paste the error report when I run ""make_examples"" for testing dataset:. python /opt/deepvariant/bin/make_examples.zip \. --mode calling \. --ref /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \. --reads /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \. --examples /usit/abel/u1/senz/DeepVariant0.7.2/quickstart_output/examples.tfrecord.gz. 2019-02-17 16:15:42.409205: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring: . I0217 16:15:42.409567 139914391602944 genomics_reader.py:213] Reading /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0217 16:15:42.417557 139914391602944 make_examples.py:1080] Preparing inputs. 2019-02-17 16:15:42.422480: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring: . I0217 16:15:42.422776 139914391602944 genomics_reader.py:213] Reading /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0217 16:15:42.424446 139914391602944 make_examples.py:996] Common contigs are [u'chr20']. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1205, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1195, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1081, in make_examples_runner. regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", lin",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/155:2732,usability,help,help,2732,"2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0217 16:15:42.417557 139914391602944 make_examples.py:1080] Preparing inputs. 2019-02-17 16:15:42.422480: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring: . I0217 16:15:42.422776 139914391602944 genomics_reader.py:213] Reading /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader. I0217 16:15:42.424446 139914391602944 make_examples.py:996] Common contigs are [u'chr20']. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1205, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1195, in main. make_examples_runner(options). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1081, in make_examples_runner. regions = processing_regions_from_options(options). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 998, in processing_regions_from_options. options.exclude_calling_regions). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 583, in build_calling_regions. regions = ranges.RangeSet.from_contigs(contigs). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 168, in from_contigs. contigs). File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 125, in __init__. tree.merge_overlaps(). AttributeError: 'IntervalTree' object has no attribute 'merge_overlaps'. I attempted to change the python package intervaltree as v2.1.0, but still received such report. Does anybody can help me?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/155
https://github.com/google/deepvariant/issues/157:285,deployability,upgrad,upgrade,285,"Nvidia RTX 2070 with Tensor cores; Hello, I am a pharmacy student and I am trying to use your tool. . I have in my possession a pc with i7-6700, 32GB RAM and a RTX 2070 with CUDA cores and Tensor Cores. 1) Can I somehow use the Tensor cores of my GPU and how? 2) Is 32 GB enough? If I upgrade to 64gb will be better?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:28,energy efficiency,core,cores,28,"Nvidia RTX 2070 with Tensor cores; Hello, I am a pharmacy student and I am trying to use your tool. . I have in my possession a pc with i7-6700, 32GB RAM and a RTX 2070 with CUDA cores and Tensor Cores. 1) Can I somehow use the Tensor cores of my GPU and how? 2) Is 32 GB enough? If I upgrade to 64gb will be better?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:179,energy efficiency,core,cores,179,"Nvidia RTX 2070 with Tensor cores; Hello, I am a pharmacy student and I am trying to use your tool. . I have in my possession a pc with i7-6700, 32GB RAM and a RTX 2070 with CUDA cores and Tensor Cores. 1) Can I somehow use the Tensor cores of my GPU and how? 2) Is 32 GB enough? If I upgrade to 64gb will be better?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:196,energy efficiency,Core,Cores,196,"Nvidia RTX 2070 with Tensor cores; Hello, I am a pharmacy student and I am trying to use your tool. . I have in my possession a pc with i7-6700, 32GB RAM and a RTX 2070 with CUDA cores and Tensor Cores. 1) Can I somehow use the Tensor cores of my GPU and how? 2) Is 32 GB enough? If I upgrade to 64gb will be better?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:235,energy efficiency,core,cores,235,"Nvidia RTX 2070 with Tensor cores; Hello, I am a pharmacy student and I am trying to use your tool. . I have in my possession a pc with i7-6700, 32GB RAM and a RTX 2070 with CUDA cores and Tensor Cores. 1) Can I somehow use the Tensor cores of my GPU and how? 2) Is 32 GB enough? If I upgrade to 64gb will be better?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:247,energy efficiency,GPU,GPU,247,"Nvidia RTX 2070 with Tensor cores; Hello, I am a pharmacy student and I am trying to use your tool. . I have in my possession a pc with i7-6700, 32GB RAM and a RTX 2070 with CUDA cores and Tensor Cores. 1) Can I somehow use the Tensor cores of my GPU and how? 2) Is 32 GB enough? If I upgrade to 64gb will be better?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:285,modifiability,upgrad,upgrade,285,"Nvidia RTX 2070 with Tensor cores; Hello, I am a pharmacy student and I am trying to use your tool. . I have in my possession a pc with i7-6700, 32GB RAM and a RTX 2070 with CUDA cores and Tensor Cores. 1) Can I somehow use the Tensor cores of my GPU and how? 2) Is 32 GB enough? If I upgrade to 64gb will be better?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:247,performance,GPU,GPU,247,"Nvidia RTX 2070 with Tensor cores; Hello, I am a pharmacy student and I am trying to use your tool. . I have in my possession a pc with i7-6700, 32GB RAM and a RTX 2070 with CUDA cores and Tensor Cores. 1) Can I somehow use the Tensor cores of my GPU and how? 2) Is 32 GB enough? If I upgrade to 64gb will be better?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/157:94,usability,tool,tool,94,"Nvidia RTX 2070 with Tensor cores; Hello, I am a pharmacy student and I am trying to use your tool. . I have in my possession a pc with i7-6700, 32GB RAM and a RTX 2070 with CUDA cores and Tensor Cores. 1) Can I somehow use the Tensor cores of my GPU and how? 2) Is 32 GB enough? If I upgrade to 64gb will be better?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/157
https://github.com/google/deepvariant/issues/158:1136,availability,checkpoint,checkpoint,1136,"Less variants were called than expected with deepvariant 0.5.0 exome model ; Hi deepvariant developer,. I used deepvariant v0.5.0 for calling variants in exome target regions of a exemplary sequenced sample by using similar commands as described in the exome case study. After successfully executing make_variants, call_variants and post process_variants I ended up with a dataset of about 12,000 called variants in the exome of my sample, rather than the expected 20,000 to 30,000 possible variants in the human exome. I used the following commands in docker:. python ../binaries/make_examples.zip \. --mode calling \. --ref /Volumes/workspace/output_mac8/Homo_sapiens_hg38.fa \. --reads /Volumes/workspace/output_mac8/Sample_NoIndex_R1.recal.bam \. --examples /Volumes/workspace/output_mac8/Sample.tfrecord@8.gz \. --regions /Volumes/workspace/output_mac8/S07604514_Regions_nochr_noheader.bed \. --gvcf /Volumes/workspace/output_mac8/Sample.gvcf.tfrecord@8.gz \. --task 0. python ../bin/call_variants.zip \. --outfile /Volumes/workspace/output_mac8/Sample.vcf.gz \. --examples /Volumes/workspace/output_mac8/Sample.tfrecord@8.gz \. --checkpoint ../models/model.ckpt \. --batch_size 32. python ../bin/postprocess_variants.zip \. --ref /Volumes/workspace/output_mac8/Homo_sapiens_hg38.fa \. --infile /Volumes/workspace/output_mac8/Sample.vcf.gz \. --outfile /Volumes/workspace/output_mac8/Sample.final.vcf.gz. How can I improve the variant calling or what could be a possible mistake, while executing the commands? Could be the reason for receiving less variants, that I used an older deepvariant version? Many Thanks in advance. Ferdinand.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/158
https://github.com/google/deepvariant/issues/158:1597,deployability,version,version,1597,"Less variants were called than expected with deepvariant 0.5.0 exome model ; Hi deepvariant developer,. I used deepvariant v0.5.0 for calling variants in exome target regions of a exemplary sequenced sample by using similar commands as described in the exome case study. After successfully executing make_variants, call_variants and post process_variants I ended up with a dataset of about 12,000 called variants in the exome of my sample, rather than the expected 20,000 to 30,000 possible variants in the human exome. I used the following commands in docker:. python ../binaries/make_examples.zip \. --mode calling \. --ref /Volumes/workspace/output_mac8/Homo_sapiens_hg38.fa \. --reads /Volumes/workspace/output_mac8/Sample_NoIndex_R1.recal.bam \. --examples /Volumes/workspace/output_mac8/Sample.tfrecord@8.gz \. --regions /Volumes/workspace/output_mac8/S07604514_Regions_nochr_noheader.bed \. --gvcf /Volumes/workspace/output_mac8/Sample.gvcf.tfrecord@8.gz \. --task 0. python ../bin/call_variants.zip \. --outfile /Volumes/workspace/output_mac8/Sample.vcf.gz \. --examples /Volumes/workspace/output_mac8/Sample.tfrecord@8.gz \. --checkpoint ../models/model.ckpt \. --batch_size 32. python ../bin/postprocess_variants.zip \. --ref /Volumes/workspace/output_mac8/Homo_sapiens_hg38.fa \. --infile /Volumes/workspace/output_mac8/Sample.vcf.gz \. --outfile /Volumes/workspace/output_mac8/Sample.final.vcf.gz. How can I improve the variant calling or what could be a possible mistake, while executing the commands? Could be the reason for receiving less variants, that I used an older deepvariant version? Many Thanks in advance. Ferdinand.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/158
https://github.com/google/deepvariant/issues/158:69,energy efficiency,model,model,69,"Less variants were called than expected with deepvariant 0.5.0 exome model ; Hi deepvariant developer,. I used deepvariant v0.5.0 for calling variants in exome target regions of a exemplary sequenced sample by using similar commands as described in the exome case study. After successfully executing make_variants, call_variants and post process_variants I ended up with a dataset of about 12,000 called variants in the exome of my sample, rather than the expected 20,000 to 30,000 possible variants in the human exome. I used the following commands in docker:. python ../binaries/make_examples.zip \. --mode calling \. --ref /Volumes/workspace/output_mac8/Homo_sapiens_hg38.fa \. --reads /Volumes/workspace/output_mac8/Sample_NoIndex_R1.recal.bam \. --examples /Volumes/workspace/output_mac8/Sample.tfrecord@8.gz \. --regions /Volumes/workspace/output_mac8/S07604514_Regions_nochr_noheader.bed \. --gvcf /Volumes/workspace/output_mac8/Sample.gvcf.tfrecord@8.gz \. --task 0. python ../bin/call_variants.zip \. --outfile /Volumes/workspace/output_mac8/Sample.vcf.gz \. --examples /Volumes/workspace/output_mac8/Sample.tfrecord@8.gz \. --checkpoint ../models/model.ckpt \. --batch_size 32. python ../bin/postprocess_variants.zip \. --ref /Volumes/workspace/output_mac8/Homo_sapiens_hg38.fa \. --infile /Volumes/workspace/output_mac8/Sample.vcf.gz \. --outfile /Volumes/workspace/output_mac8/Sample.final.vcf.gz. How can I improve the variant calling or what could be a possible mistake, while executing the commands? Could be the reason for receiving less variants, that I used an older deepvariant version? Many Thanks in advance. Ferdinand.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/158
https://github.com/google/deepvariant/issues/158:1150,energy efficiency,model,models,1150,"Less variants were called than expected with deepvariant 0.5.0 exome model ; Hi deepvariant developer,. I used deepvariant v0.5.0 for calling variants in exome target regions of a exemplary sequenced sample by using similar commands as described in the exome case study. After successfully executing make_variants, call_variants and post process_variants I ended up with a dataset of about 12,000 called variants in the exome of my sample, rather than the expected 20,000 to 30,000 possible variants in the human exome. I used the following commands in docker:. python ../binaries/make_examples.zip \. --mode calling \. --ref /Volumes/workspace/output_mac8/Homo_sapiens_hg38.fa \. --reads /Volumes/workspace/output_mac8/Sample_NoIndex_R1.recal.bam \. --examples /Volumes/workspace/output_mac8/Sample.tfrecord@8.gz \. --regions /Volumes/workspace/output_mac8/S07604514_Regions_nochr_noheader.bed \. --gvcf /Volumes/workspace/output_mac8/Sample.gvcf.tfrecord@8.gz \. --task 0. python ../bin/call_variants.zip \. --outfile /Volumes/workspace/output_mac8/Sample.vcf.gz \. --examples /Volumes/workspace/output_mac8/Sample.tfrecord@8.gz \. --checkpoint ../models/model.ckpt \. --batch_size 32. python ../bin/postprocess_variants.zip \. --ref /Volumes/workspace/output_mac8/Homo_sapiens_hg38.fa \. --infile /Volumes/workspace/output_mac8/Sample.vcf.gz \. --outfile /Volumes/workspace/output_mac8/Sample.final.vcf.gz. How can I improve the variant calling or what could be a possible mistake, while executing the commands? Could be the reason for receiving less variants, that I used an older deepvariant version? Many Thanks in advance. Ferdinand.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/158
https://github.com/google/deepvariant/issues/158:1157,energy efficiency,model,model,1157,"Less variants were called than expected with deepvariant 0.5.0 exome model ; Hi deepvariant developer,. I used deepvariant v0.5.0 for calling variants in exome target regions of a exemplary sequenced sample by using similar commands as described in the exome case study. After successfully executing make_variants, call_variants and post process_variants I ended up with a dataset of about 12,000 called variants in the exome of my sample, rather than the expected 20,000 to 30,000 possible variants in the human exome. I used the following commands in docker:. python ../binaries/make_examples.zip \. --mode calling \. --ref /Volumes/workspace/output_mac8/Homo_sapiens_hg38.fa \. --reads /Volumes/workspace/output_mac8/Sample_NoIndex_R1.recal.bam \. --examples /Volumes/workspace/output_mac8/Sample.tfrecord@8.gz \. --regions /Volumes/workspace/output_mac8/S07604514_Regions_nochr_noheader.bed \. --gvcf /Volumes/workspace/output_mac8/Sample.gvcf.tfrecord@8.gz \. --task 0. python ../bin/call_variants.zip \. --outfile /Volumes/workspace/output_mac8/Sample.vcf.gz \. --examples /Volumes/workspace/output_mac8/Sample.tfrecord@8.gz \. --checkpoint ../models/model.ckpt \. --batch_size 32. python ../bin/postprocess_variants.zip \. --ref /Volumes/workspace/output_mac8/Homo_sapiens_hg38.fa \. --infile /Volumes/workspace/output_mac8/Sample.vcf.gz \. --outfile /Volumes/workspace/output_mac8/Sample.final.vcf.gz. How can I improve the variant calling or what could be a possible mistake, while executing the commands? Could be the reason for receiving less variants, that I used an older deepvariant version? Many Thanks in advance. Ferdinand.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/158
https://github.com/google/deepvariant/issues/158:1597,integrability,version,version,1597,"Less variants were called than expected with deepvariant 0.5.0 exome model ; Hi deepvariant developer,. I used deepvariant v0.5.0 for calling variants in exome target regions of a exemplary sequenced sample by using similar commands as described in the exome case study. After successfully executing make_variants, call_variants and post process_variants I ended up with a dataset of about 12,000 called variants in the exome of my sample, rather than the expected 20,000 to 30,000 possible variants in the human exome. I used the following commands in docker:. python ../binaries/make_examples.zip \. --mode calling \. --ref /Volumes/workspace/output_mac8/Homo_sapiens_hg38.fa \. --reads /Volumes/workspace/output_mac8/Sample_NoIndex_R1.recal.bam \. --examples /Volumes/workspace/output_mac8/Sample.tfrecord@8.gz \. --regions /Volumes/workspace/output_mac8/S07604514_Regions_nochr_noheader.bed \. --gvcf /Volumes/workspace/output_mac8/Sample.gvcf.tfrecord@8.gz \. --task 0. python ../bin/call_variants.zip \. --outfile /Volumes/workspace/output_mac8/Sample.vcf.gz \. --examples /Volumes/workspace/output_mac8/Sample.tfrecord@8.gz \. --checkpoint ../models/model.ckpt \. --batch_size 32. python ../bin/postprocess_variants.zip \. --ref /Volumes/workspace/output_mac8/Homo_sapiens_hg38.fa \. --infile /Volumes/workspace/output_mac8/Sample.vcf.gz \. --outfile /Volumes/workspace/output_mac8/Sample.final.vcf.gz. How can I improve the variant calling or what could be a possible mistake, while executing the commands? Could be the reason for receiving less variants, that I used an older deepvariant version? Many Thanks in advance. Ferdinand.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/158
https://github.com/google/deepvariant/issues/158:1597,modifiability,version,version,1597,"Less variants were called than expected with deepvariant 0.5.0 exome model ; Hi deepvariant developer,. I used deepvariant v0.5.0 for calling variants in exome target regions of a exemplary sequenced sample by using similar commands as described in the exome case study. After successfully executing make_variants, call_variants and post process_variants I ended up with a dataset of about 12,000 called variants in the exome of my sample, rather than the expected 20,000 to 30,000 possible variants in the human exome. I used the following commands in docker:. python ../binaries/make_examples.zip \. --mode calling \. --ref /Volumes/workspace/output_mac8/Homo_sapiens_hg38.fa \. --reads /Volumes/workspace/output_mac8/Sample_NoIndex_R1.recal.bam \. --examples /Volumes/workspace/output_mac8/Sample.tfrecord@8.gz \. --regions /Volumes/workspace/output_mac8/S07604514_Regions_nochr_noheader.bed \. --gvcf /Volumes/workspace/output_mac8/Sample.gvcf.tfrecord@8.gz \. --task 0. python ../bin/call_variants.zip \. --outfile /Volumes/workspace/output_mac8/Sample.vcf.gz \. --examples /Volumes/workspace/output_mac8/Sample.tfrecord@8.gz \. --checkpoint ../models/model.ckpt \. --batch_size 32. python ../bin/postprocess_variants.zip \. --ref /Volumes/workspace/output_mac8/Homo_sapiens_hg38.fa \. --infile /Volumes/workspace/output_mac8/Sample.vcf.gz \. --outfile /Volumes/workspace/output_mac8/Sample.final.vcf.gz. How can I improve the variant calling or what could be a possible mistake, while executing the commands? Could be the reason for receiving less variants, that I used an older deepvariant version? Many Thanks in advance. Ferdinand.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/158
https://github.com/google/deepvariant/issues/158:1136,reliability,checkpoint,checkpoint,1136,"Less variants were called than expected with deepvariant 0.5.0 exome model ; Hi deepvariant developer,. I used deepvariant v0.5.0 for calling variants in exome target regions of a exemplary sequenced sample by using similar commands as described in the exome case study. After successfully executing make_variants, call_variants and post process_variants I ended up with a dataset of about 12,000 called variants in the exome of my sample, rather than the expected 20,000 to 30,000 possible variants in the human exome. I used the following commands in docker:. python ../binaries/make_examples.zip \. --mode calling \. --ref /Volumes/workspace/output_mac8/Homo_sapiens_hg38.fa \. --reads /Volumes/workspace/output_mac8/Sample_NoIndex_R1.recal.bam \. --examples /Volumes/workspace/output_mac8/Sample.tfrecord@8.gz \. --regions /Volumes/workspace/output_mac8/S07604514_Regions_nochr_noheader.bed \. --gvcf /Volumes/workspace/output_mac8/Sample.gvcf.tfrecord@8.gz \. --task 0. python ../bin/call_variants.zip \. --outfile /Volumes/workspace/output_mac8/Sample.vcf.gz \. --examples /Volumes/workspace/output_mac8/Sample.tfrecord@8.gz \. --checkpoint ../models/model.ckpt \. --batch_size 32. python ../bin/postprocess_variants.zip \. --ref /Volumes/workspace/output_mac8/Homo_sapiens_hg38.fa \. --infile /Volumes/workspace/output_mac8/Sample.vcf.gz \. --outfile /Volumes/workspace/output_mac8/Sample.final.vcf.gz. How can I improve the variant calling or what could be a possible mistake, while executing the commands? Could be the reason for receiving less variants, that I used an older deepvariant version? Many Thanks in advance. Ferdinand.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/158
https://github.com/google/deepvariant/issues/158:69,security,model,model,69,"Less variants were called than expected with deepvariant 0.5.0 exome model ; Hi deepvariant developer,. I used deepvariant v0.5.0 for calling variants in exome target regions of a exemplary sequenced sample by using similar commands as described in the exome case study. After successfully executing make_variants, call_variants and post process_variants I ended up with a dataset of about 12,000 called variants in the exome of my sample, rather than the expected 20,000 to 30,000 possible variants in the human exome. I used the following commands in docker:. python ../binaries/make_examples.zip \. --mode calling \. --ref /Volumes/workspace/output_mac8/Homo_sapiens_hg38.fa \. --reads /Volumes/workspace/output_mac8/Sample_NoIndex_R1.recal.bam \. --examples /Volumes/workspace/output_mac8/Sample.tfrecord@8.gz \. --regions /Volumes/workspace/output_mac8/S07604514_Regions_nochr_noheader.bed \. --gvcf /Volumes/workspace/output_mac8/Sample.gvcf.tfrecord@8.gz \. --task 0. python ../bin/call_variants.zip \. --outfile /Volumes/workspace/output_mac8/Sample.vcf.gz \. --examples /Volumes/workspace/output_mac8/Sample.tfrecord@8.gz \. --checkpoint ../models/model.ckpt \. --batch_size 32. python ../bin/postprocess_variants.zip \. --ref /Volumes/workspace/output_mac8/Homo_sapiens_hg38.fa \. --infile /Volumes/workspace/output_mac8/Sample.vcf.gz \. --outfile /Volumes/workspace/output_mac8/Sample.final.vcf.gz. How can I improve the variant calling or what could be a possible mistake, while executing the commands? Could be the reason for receiving less variants, that I used an older deepvariant version? Many Thanks in advance. Ferdinand.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/158
https://github.com/google/deepvariant/issues/158:1150,security,model,models,1150,"Less variants were called than expected with deepvariant 0.5.0 exome model ; Hi deepvariant developer,. I used deepvariant v0.5.0 for calling variants in exome target regions of a exemplary sequenced sample by using similar commands as described in the exome case study. After successfully executing make_variants, call_variants and post process_variants I ended up with a dataset of about 12,000 called variants in the exome of my sample, rather than the expected 20,000 to 30,000 possible variants in the human exome. I used the following commands in docker:. python ../binaries/make_examples.zip \. --mode calling \. --ref /Volumes/workspace/output_mac8/Homo_sapiens_hg38.fa \. --reads /Volumes/workspace/output_mac8/Sample_NoIndex_R1.recal.bam \. --examples /Volumes/workspace/output_mac8/Sample.tfrecord@8.gz \. --regions /Volumes/workspace/output_mac8/S07604514_Regions_nochr_noheader.bed \. --gvcf /Volumes/workspace/output_mac8/Sample.gvcf.tfrecord@8.gz \. --task 0. python ../bin/call_variants.zip \. --outfile /Volumes/workspace/output_mac8/Sample.vcf.gz \. --examples /Volumes/workspace/output_mac8/Sample.tfrecord@8.gz \. --checkpoint ../models/model.ckpt \. --batch_size 32. python ../bin/postprocess_variants.zip \. --ref /Volumes/workspace/output_mac8/Homo_sapiens_hg38.fa \. --infile /Volumes/workspace/output_mac8/Sample.vcf.gz \. --outfile /Volumes/workspace/output_mac8/Sample.final.vcf.gz. How can I improve the variant calling or what could be a possible mistake, while executing the commands? Could be the reason for receiving less variants, that I used an older deepvariant version? Many Thanks in advance. Ferdinand.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/158
https://github.com/google/deepvariant/issues/158:1157,security,model,model,1157,"Less variants were called than expected with deepvariant 0.5.0 exome model ; Hi deepvariant developer,. I used deepvariant v0.5.0 for calling variants in exome target regions of a exemplary sequenced sample by using similar commands as described in the exome case study. After successfully executing make_variants, call_variants and post process_variants I ended up with a dataset of about 12,000 called variants in the exome of my sample, rather than the expected 20,000 to 30,000 possible variants in the human exome. I used the following commands in docker:. python ../binaries/make_examples.zip \. --mode calling \. --ref /Volumes/workspace/output_mac8/Homo_sapiens_hg38.fa \. --reads /Volumes/workspace/output_mac8/Sample_NoIndex_R1.recal.bam \. --examples /Volumes/workspace/output_mac8/Sample.tfrecord@8.gz \. --regions /Volumes/workspace/output_mac8/S07604514_Regions_nochr_noheader.bed \. --gvcf /Volumes/workspace/output_mac8/Sample.gvcf.tfrecord@8.gz \. --task 0. python ../bin/call_variants.zip \. --outfile /Volumes/workspace/output_mac8/Sample.vcf.gz \. --examples /Volumes/workspace/output_mac8/Sample.tfrecord@8.gz \. --checkpoint ../models/model.ckpt \. --batch_size 32. python ../bin/postprocess_variants.zip \. --ref /Volumes/workspace/output_mac8/Homo_sapiens_hg38.fa \. --infile /Volumes/workspace/output_mac8/Sample.vcf.gz \. --outfile /Volumes/workspace/output_mac8/Sample.final.vcf.gz. How can I improve the variant calling or what could be a possible mistake, while executing the commands? Could be the reason for receiving less variants, that I used an older deepvariant version? Many Thanks in advance. Ferdinand.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/158
https://github.com/google/deepvariant/issues/158:224,usability,command,commands,224,"Less variants were called than expected with deepvariant 0.5.0 exome model ; Hi deepvariant developer,. I used deepvariant v0.5.0 for calling variants in exome target regions of a exemplary sequenced sample by using similar commands as described in the exome case study. After successfully executing make_variants, call_variants and post process_variants I ended up with a dataset of about 12,000 called variants in the exome of my sample, rather than the expected 20,000 to 30,000 possible variants in the human exome. I used the following commands in docker:. python ../binaries/make_examples.zip \. --mode calling \. --ref /Volumes/workspace/output_mac8/Homo_sapiens_hg38.fa \. --reads /Volumes/workspace/output_mac8/Sample_NoIndex_R1.recal.bam \. --examples /Volumes/workspace/output_mac8/Sample.tfrecord@8.gz \. --regions /Volumes/workspace/output_mac8/S07604514_Regions_nochr_noheader.bed \. --gvcf /Volumes/workspace/output_mac8/Sample.gvcf.tfrecord@8.gz \. --task 0. python ../bin/call_variants.zip \. --outfile /Volumes/workspace/output_mac8/Sample.vcf.gz \. --examples /Volumes/workspace/output_mac8/Sample.tfrecord@8.gz \. --checkpoint ../models/model.ckpt \. --batch_size 32. python ../bin/postprocess_variants.zip \. --ref /Volumes/workspace/output_mac8/Homo_sapiens_hg38.fa \. --infile /Volumes/workspace/output_mac8/Sample.vcf.gz \. --outfile /Volumes/workspace/output_mac8/Sample.final.vcf.gz. How can I improve the variant calling or what could be a possible mistake, while executing the commands? Could be the reason for receiving less variants, that I used an older deepvariant version? Many Thanks in advance. Ferdinand.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/158
https://github.com/google/deepvariant/issues/158:541,usability,command,commands,541,"Less variants were called than expected with deepvariant 0.5.0 exome model ; Hi deepvariant developer,. I used deepvariant v0.5.0 for calling variants in exome target regions of a exemplary sequenced sample by using similar commands as described in the exome case study. After successfully executing make_variants, call_variants and post process_variants I ended up with a dataset of about 12,000 called variants in the exome of my sample, rather than the expected 20,000 to 30,000 possible variants in the human exome. I used the following commands in docker:. python ../binaries/make_examples.zip \. --mode calling \. --ref /Volumes/workspace/output_mac8/Homo_sapiens_hg38.fa \. --reads /Volumes/workspace/output_mac8/Sample_NoIndex_R1.recal.bam \. --examples /Volumes/workspace/output_mac8/Sample.tfrecord@8.gz \. --regions /Volumes/workspace/output_mac8/S07604514_Regions_nochr_noheader.bed \. --gvcf /Volumes/workspace/output_mac8/Sample.gvcf.tfrecord@8.gz \. --task 0. python ../bin/call_variants.zip \. --outfile /Volumes/workspace/output_mac8/Sample.vcf.gz \. --examples /Volumes/workspace/output_mac8/Sample.tfrecord@8.gz \. --checkpoint ../models/model.ckpt \. --batch_size 32. python ../bin/postprocess_variants.zip \. --ref /Volumes/workspace/output_mac8/Homo_sapiens_hg38.fa \. --infile /Volumes/workspace/output_mac8/Sample.vcf.gz \. --outfile /Volumes/workspace/output_mac8/Sample.final.vcf.gz. How can I improve the variant calling or what could be a possible mistake, while executing the commands? Could be the reason for receiving less variants, that I used an older deepvariant version? Many Thanks in advance. Ferdinand.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/158
https://github.com/google/deepvariant/issues/158:1505,usability,command,commands,1505,"Less variants were called than expected with deepvariant 0.5.0 exome model ; Hi deepvariant developer,. I used deepvariant v0.5.0 for calling variants in exome target regions of a exemplary sequenced sample by using similar commands as described in the exome case study. After successfully executing make_variants, call_variants and post process_variants I ended up with a dataset of about 12,000 called variants in the exome of my sample, rather than the expected 20,000 to 30,000 possible variants in the human exome. I used the following commands in docker:. python ../binaries/make_examples.zip \. --mode calling \. --ref /Volumes/workspace/output_mac8/Homo_sapiens_hg38.fa \. --reads /Volumes/workspace/output_mac8/Sample_NoIndex_R1.recal.bam \. --examples /Volumes/workspace/output_mac8/Sample.tfrecord@8.gz \. --regions /Volumes/workspace/output_mac8/S07604514_Regions_nochr_noheader.bed \. --gvcf /Volumes/workspace/output_mac8/Sample.gvcf.tfrecord@8.gz \. --task 0. python ../bin/call_variants.zip \. --outfile /Volumes/workspace/output_mac8/Sample.vcf.gz \. --examples /Volumes/workspace/output_mac8/Sample.tfrecord@8.gz \. --checkpoint ../models/model.ckpt \. --batch_size 32. python ../bin/postprocess_variants.zip \. --ref /Volumes/workspace/output_mac8/Homo_sapiens_hg38.fa \. --infile /Volumes/workspace/output_mac8/Sample.vcf.gz \. --outfile /Volumes/workspace/output_mac8/Sample.final.vcf.gz. How can I improve the variant calling or what could be a possible mistake, while executing the commands? Could be the reason for receiving less variants, that I used an older deepvariant version? Many Thanks in advance. Ferdinand.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/158
https://github.com/google/deepvariant/pull/159:101,deployability,resourc,resource,101,"Enable configurable GPU memory allocation of call_variants; To port DeepVariant to Apache Spark, the resource allocation should be configurable, including CPU vCores, main memory and GPU memory. In this pull request, we add two more parameters (enable_configurable_gpu and per_process_gpu_memory_fraction) to let users have more flexibility to allocate the resource of GPU memory. If enable_configurable_gpu is true, each call_variants process will allocate GPU memory by the value of per_process_gpu_memory_fraction. (Default is 100) Please help to check whether the modification is proper or not. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:357,deployability,resourc,resource,357,"Enable configurable GPU memory allocation of call_variants; To port DeepVariant to Apache Spark, the resource allocation should be configurable, including CPU vCores, main memory and GPU memory. In this pull request, we add two more parameters (enable_configurable_gpu and per_process_gpu_memory_fraction) to let users have more flexibility to allocate the resource of GPU memory. If enable_configurable_gpu is true, each call_variants process will allocate GPU memory by the value of per_process_gpu_memory_fraction. (Default is 100) Please help to check whether the modification is proper or not. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:20,energy efficiency,GPU,GPU,20,"Enable configurable GPU memory allocation of call_variants; To port DeepVariant to Apache Spark, the resource allocation should be configurable, including CPU vCores, main memory and GPU memory. In this pull request, we add two more parameters (enable_configurable_gpu and per_process_gpu_memory_fraction) to let users have more flexibility to allocate the resource of GPU memory. If enable_configurable_gpu is true, each call_variants process will allocate GPU memory by the value of per_process_gpu_memory_fraction. (Default is 100) Please help to check whether the modification is proper or not. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:31,energy efficiency,alloc,allocation,31,"Enable configurable GPU memory allocation of call_variants; To port DeepVariant to Apache Spark, the resource allocation should be configurable, including CPU vCores, main memory and GPU memory. In this pull request, we add two more parameters (enable_configurable_gpu and per_process_gpu_memory_fraction) to let users have more flexibility to allocate the resource of GPU memory. If enable_configurable_gpu is true, each call_variants process will allocate GPU memory by the value of per_process_gpu_memory_fraction. (Default is 100) Please help to check whether the modification is proper or not. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:101,energy efficiency,resourc,resource,101,"Enable configurable GPU memory allocation of call_variants; To port DeepVariant to Apache Spark, the resource allocation should be configurable, including CPU vCores, main memory and GPU memory. In this pull request, we add two more parameters (enable_configurable_gpu and per_process_gpu_memory_fraction) to let users have more flexibility to allocate the resource of GPU memory. If enable_configurable_gpu is true, each call_variants process will allocate GPU memory by the value of per_process_gpu_memory_fraction. (Default is 100) Please help to check whether the modification is proper or not. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:110,energy efficiency,alloc,allocation,110,"Enable configurable GPU memory allocation of call_variants; To port DeepVariant to Apache Spark, the resource allocation should be configurable, including CPU vCores, main memory and GPU memory. In this pull request, we add two more parameters (enable_configurable_gpu and per_process_gpu_memory_fraction) to let users have more flexibility to allocate the resource of GPU memory. If enable_configurable_gpu is true, each call_variants process will allocate GPU memory by the value of per_process_gpu_memory_fraction. (Default is 100) Please help to check whether the modification is proper or not. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:155,energy efficiency,CPU,CPU,155,"Enable configurable GPU memory allocation of call_variants; To port DeepVariant to Apache Spark, the resource allocation should be configurable, including CPU vCores, main memory and GPU memory. In this pull request, we add two more parameters (enable_configurable_gpu and per_process_gpu_memory_fraction) to let users have more flexibility to allocate the resource of GPU memory. If enable_configurable_gpu is true, each call_variants process will allocate GPU memory by the value of per_process_gpu_memory_fraction. (Default is 100) Please help to check whether the modification is proper or not. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:183,energy efficiency,GPU,GPU,183,"Enable configurable GPU memory allocation of call_variants; To port DeepVariant to Apache Spark, the resource allocation should be configurable, including CPU vCores, main memory and GPU memory. In this pull request, we add two more parameters (enable_configurable_gpu and per_process_gpu_memory_fraction) to let users have more flexibility to allocate the resource of GPU memory. If enable_configurable_gpu is true, each call_variants process will allocate GPU memory by the value of per_process_gpu_memory_fraction. (Default is 100) Please help to check whether the modification is proper or not. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:344,energy efficiency,alloc,allocate,344,"Enable configurable GPU memory allocation of call_variants; To port DeepVariant to Apache Spark, the resource allocation should be configurable, including CPU vCores, main memory and GPU memory. In this pull request, we add two more parameters (enable_configurable_gpu and per_process_gpu_memory_fraction) to let users have more flexibility to allocate the resource of GPU memory. If enable_configurable_gpu is true, each call_variants process will allocate GPU memory by the value of per_process_gpu_memory_fraction. (Default is 100) Please help to check whether the modification is proper or not. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:357,energy efficiency,resourc,resource,357,"Enable configurable GPU memory allocation of call_variants; To port DeepVariant to Apache Spark, the resource allocation should be configurable, including CPU vCores, main memory and GPU memory. In this pull request, we add two more parameters (enable_configurable_gpu and per_process_gpu_memory_fraction) to let users have more flexibility to allocate the resource of GPU memory. If enable_configurable_gpu is true, each call_variants process will allocate GPU memory by the value of per_process_gpu_memory_fraction. (Default is 100) Please help to check whether the modification is proper or not. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:369,energy efficiency,GPU,GPU,369,"Enable configurable GPU memory allocation of call_variants; To port DeepVariant to Apache Spark, the resource allocation should be configurable, including CPU vCores, main memory and GPU memory. In this pull request, we add two more parameters (enable_configurable_gpu and per_process_gpu_memory_fraction) to let users have more flexibility to allocate the resource of GPU memory. If enable_configurable_gpu is true, each call_variants process will allocate GPU memory by the value of per_process_gpu_memory_fraction. (Default is 100) Please help to check whether the modification is proper or not. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:449,energy efficiency,alloc,allocate,449,"Enable configurable GPU memory allocation of call_variants; To port DeepVariant to Apache Spark, the resource allocation should be configurable, including CPU vCores, main memory and GPU memory. In this pull request, we add two more parameters (enable_configurable_gpu and per_process_gpu_memory_fraction) to let users have more flexibility to allocate the resource of GPU memory. If enable_configurable_gpu is true, each call_variants process will allocate GPU memory by the value of per_process_gpu_memory_fraction. (Default is 100) Please help to check whether the modification is proper or not. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:458,energy efficiency,GPU,GPU,458,"Enable configurable GPU memory allocation of call_variants; To port DeepVariant to Apache Spark, the resource allocation should be configurable, including CPU vCores, main memory and GPU memory. In this pull request, we add two more parameters (enable_configurable_gpu and per_process_gpu_memory_fraction) to let users have more flexibility to allocate the resource of GPU memory. If enable_configurable_gpu is true, each call_variants process will allocate GPU memory by the value of per_process_gpu_memory_fraction. (Default is 100) Please help to check whether the modification is proper or not. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:7,integrability,configur,configurable,7,"Enable configurable GPU memory allocation of call_variants; To port DeepVariant to Apache Spark, the resource allocation should be configurable, including CPU vCores, main memory and GPU memory. In this pull request, we add two more parameters (enable_configurable_gpu and per_process_gpu_memory_fraction) to let users have more flexibility to allocate the resource of GPU memory. If enable_configurable_gpu is true, each call_variants process will allocate GPU memory by the value of per_process_gpu_memory_fraction. (Default is 100) Please help to check whether the modification is proper or not. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:131,integrability,configur,configurable,131,"Enable configurable GPU memory allocation of call_variants; To port DeepVariant to Apache Spark, the resource allocation should be configurable, including CPU vCores, main memory and GPU memory. In this pull request, we add two more parameters (enable_configurable_gpu and per_process_gpu_memory_fraction) to let users have more flexibility to allocate the resource of GPU memory. If enable_configurable_gpu is true, each call_variants process will allocate GPU memory by the value of per_process_gpu_memory_fraction. (Default is 100) Please help to check whether the modification is proper or not. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:7,modifiability,configur,configurable,7,"Enable configurable GPU memory allocation of call_variants; To port DeepVariant to Apache Spark, the resource allocation should be configurable, including CPU vCores, main memory and GPU memory. In this pull request, we add two more parameters (enable_configurable_gpu and per_process_gpu_memory_fraction) to let users have more flexibility to allocate the resource of GPU memory. If enable_configurable_gpu is true, each call_variants process will allocate GPU memory by the value of per_process_gpu_memory_fraction. (Default is 100) Please help to check whether the modification is proper or not. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:131,modifiability,configur,configurable,131,"Enable configurable GPU memory allocation of call_variants; To port DeepVariant to Apache Spark, the resource allocation should be configurable, including CPU vCores, main memory and GPU memory. In this pull request, we add two more parameters (enable_configurable_gpu and per_process_gpu_memory_fraction) to let users have more flexibility to allocate the resource of GPU memory. If enable_configurable_gpu is true, each call_variants process will allocate GPU memory by the value of per_process_gpu_memory_fraction. (Default is 100) Please help to check whether the modification is proper or not. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:233,modifiability,paramet,parameters,233,"Enable configurable GPU memory allocation of call_variants; To port DeepVariant to Apache Spark, the resource allocation should be configurable, including CPU vCores, main memory and GPU memory. In this pull request, we add two more parameters (enable_configurable_gpu and per_process_gpu_memory_fraction) to let users have more flexibility to allocate the resource of GPU memory. If enable_configurable_gpu is true, each call_variants process will allocate GPU memory by the value of per_process_gpu_memory_fraction. (Default is 100) Please help to check whether the modification is proper or not. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:20,performance,GPU,GPU,20,"Enable configurable GPU memory allocation of call_variants; To port DeepVariant to Apache Spark, the resource allocation should be configurable, including CPU vCores, main memory and GPU memory. In this pull request, we add two more parameters (enable_configurable_gpu and per_process_gpu_memory_fraction) to let users have more flexibility to allocate the resource of GPU memory. If enable_configurable_gpu is true, each call_variants process will allocate GPU memory by the value of per_process_gpu_memory_fraction. (Default is 100) Please help to check whether the modification is proper or not. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:24,performance,memor,memory,24,"Enable configurable GPU memory allocation of call_variants; To port DeepVariant to Apache Spark, the resource allocation should be configurable, including CPU vCores, main memory and GPU memory. In this pull request, we add two more parameters (enable_configurable_gpu and per_process_gpu_memory_fraction) to let users have more flexibility to allocate the resource of GPU memory. If enable_configurable_gpu is true, each call_variants process will allocate GPU memory by the value of per_process_gpu_memory_fraction. (Default is 100) Please help to check whether the modification is proper or not. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:101,performance,resourc,resource,101,"Enable configurable GPU memory allocation of call_variants; To port DeepVariant to Apache Spark, the resource allocation should be configurable, including CPU vCores, main memory and GPU memory. In this pull request, we add two more parameters (enable_configurable_gpu and per_process_gpu_memory_fraction) to let users have more flexibility to allocate the resource of GPU memory. If enable_configurable_gpu is true, each call_variants process will allocate GPU memory by the value of per_process_gpu_memory_fraction. (Default is 100) Please help to check whether the modification is proper or not. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:155,performance,CPU,CPU,155,"Enable configurable GPU memory allocation of call_variants; To port DeepVariant to Apache Spark, the resource allocation should be configurable, including CPU vCores, main memory and GPU memory. In this pull request, we add two more parameters (enable_configurable_gpu and per_process_gpu_memory_fraction) to let users have more flexibility to allocate the resource of GPU memory. If enable_configurable_gpu is true, each call_variants process will allocate GPU memory by the value of per_process_gpu_memory_fraction. (Default is 100) Please help to check whether the modification is proper or not. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:172,performance,memor,memory,172,"Enable configurable GPU memory allocation of call_variants; To port DeepVariant to Apache Spark, the resource allocation should be configurable, including CPU vCores, main memory and GPU memory. In this pull request, we add two more parameters (enable_configurable_gpu and per_process_gpu_memory_fraction) to let users have more flexibility to allocate the resource of GPU memory. If enable_configurable_gpu is true, each call_variants process will allocate GPU memory by the value of per_process_gpu_memory_fraction. (Default is 100) Please help to check whether the modification is proper or not. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:183,performance,GPU,GPU,183,"Enable configurable GPU memory allocation of call_variants; To port DeepVariant to Apache Spark, the resource allocation should be configurable, including CPU vCores, main memory and GPU memory. In this pull request, we add two more parameters (enable_configurable_gpu and per_process_gpu_memory_fraction) to let users have more flexibility to allocate the resource of GPU memory. If enable_configurable_gpu is true, each call_variants process will allocate GPU memory by the value of per_process_gpu_memory_fraction. (Default is 100) Please help to check whether the modification is proper or not. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:187,performance,memor,memory,187,"Enable configurable GPU memory allocation of call_variants; To port DeepVariant to Apache Spark, the resource allocation should be configurable, including CPU vCores, main memory and GPU memory. In this pull request, we add two more parameters (enable_configurable_gpu and per_process_gpu_memory_fraction) to let users have more flexibility to allocate the resource of GPU memory. If enable_configurable_gpu is true, each call_variants process will allocate GPU memory by the value of per_process_gpu_memory_fraction. (Default is 100) Please help to check whether the modification is proper or not. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:357,performance,resourc,resource,357,"Enable configurable GPU memory allocation of call_variants; To port DeepVariant to Apache Spark, the resource allocation should be configurable, including CPU vCores, main memory and GPU memory. In this pull request, we add two more parameters (enable_configurable_gpu and per_process_gpu_memory_fraction) to let users have more flexibility to allocate the resource of GPU memory. If enable_configurable_gpu is true, each call_variants process will allocate GPU memory by the value of per_process_gpu_memory_fraction. (Default is 100) Please help to check whether the modification is proper or not. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:369,performance,GPU,GPU,369,"Enable configurable GPU memory allocation of call_variants; To port DeepVariant to Apache Spark, the resource allocation should be configurable, including CPU vCores, main memory and GPU memory. In this pull request, we add two more parameters (enable_configurable_gpu and per_process_gpu_memory_fraction) to let users have more flexibility to allocate the resource of GPU memory. If enable_configurable_gpu is true, each call_variants process will allocate GPU memory by the value of per_process_gpu_memory_fraction. (Default is 100) Please help to check whether the modification is proper or not. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:373,performance,memor,memory,373,"Enable configurable GPU memory allocation of call_variants; To port DeepVariant to Apache Spark, the resource allocation should be configurable, including CPU vCores, main memory and GPU memory. In this pull request, we add two more parameters (enable_configurable_gpu and per_process_gpu_memory_fraction) to let users have more flexibility to allocate the resource of GPU memory. If enable_configurable_gpu is true, each call_variants process will allocate GPU memory by the value of per_process_gpu_memory_fraction. (Default is 100) Please help to check whether the modification is proper or not. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:458,performance,GPU,GPU,458,"Enable configurable GPU memory allocation of call_variants; To port DeepVariant to Apache Spark, the resource allocation should be configurable, including CPU vCores, main memory and GPU memory. In this pull request, we add two more parameters (enable_configurable_gpu and per_process_gpu_memory_fraction) to let users have more flexibility to allocate the resource of GPU memory. If enable_configurable_gpu is true, each call_variants process will allocate GPU memory by the value of per_process_gpu_memory_fraction. (Default is 100) Please help to check whether the modification is proper or not. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:462,performance,memor,memory,462,"Enable configurable GPU memory allocation of call_variants; To port DeepVariant to Apache Spark, the resource allocation should be configurable, including CPU vCores, main memory and GPU memory. In this pull request, we add two more parameters (enable_configurable_gpu and per_process_gpu_memory_fraction) to let users have more flexibility to allocate the resource of GPU memory. If enable_configurable_gpu is true, each call_variants process will allocate GPU memory by the value of per_process_gpu_memory_fraction. (Default is 100) Please help to check whether the modification is proper or not. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:101,safety,resourc,resource,101,"Enable configurable GPU memory allocation of call_variants; To port DeepVariant to Apache Spark, the resource allocation should be configurable, including CPU vCores, main memory and GPU memory. In this pull request, we add two more parameters (enable_configurable_gpu and per_process_gpu_memory_fraction) to let users have more flexibility to allocate the resource of GPU memory. If enable_configurable_gpu is true, each call_variants process will allocate GPU memory by the value of per_process_gpu_memory_fraction. (Default is 100) Please help to check whether the modification is proper or not. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:357,safety,resourc,resource,357,"Enable configurable GPU memory allocation of call_variants; To port DeepVariant to Apache Spark, the resource allocation should be configurable, including CPU vCores, main memory and GPU memory. In this pull request, we add two more parameters (enable_configurable_gpu and per_process_gpu_memory_fraction) to let users have more flexibility to allocate the resource of GPU memory. If enable_configurable_gpu is true, each call_variants process will allocate GPU memory by the value of per_process_gpu_memory_fraction. (Default is 100) Please help to check whether the modification is proper or not. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:7,security,configur,configurable,7,"Enable configurable GPU memory allocation of call_variants; To port DeepVariant to Apache Spark, the resource allocation should be configurable, including CPU vCores, main memory and GPU memory. In this pull request, we add two more parameters (enable_configurable_gpu and per_process_gpu_memory_fraction) to let users have more flexibility to allocate the resource of GPU memory. If enable_configurable_gpu is true, each call_variants process will allocate GPU memory by the value of per_process_gpu_memory_fraction. (Default is 100) Please help to check whether the modification is proper or not. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:131,security,configur,configurable,131,"Enable configurable GPU memory allocation of call_variants; To port DeepVariant to Apache Spark, the resource allocation should be configurable, including CPU vCores, main memory and GPU memory. In this pull request, we add two more parameters (enable_configurable_gpu and per_process_gpu_memory_fraction) to let users have more flexibility to allocate the resource of GPU memory. If enable_configurable_gpu is true, each call_variants process will allocate GPU memory by the value of per_process_gpu_memory_fraction. (Default is 100) Please help to check whether the modification is proper or not. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:568,security,modif,modification,568,"Enable configurable GPU memory allocation of call_variants; To port DeepVariant to Apache Spark, the resource allocation should be configurable, including CPU vCores, main memory and GPU memory. In this pull request, we add two more parameters (enable_configurable_gpu and per_process_gpu_memory_fraction) to let users have more flexibility to allocate the resource of GPU memory. If enable_configurable_gpu is true, each call_variants process will allocate GPU memory by the value of per_process_gpu_memory_fraction. (Default is 100) Please help to check whether the modification is proper or not. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:101,testability,resourc,resource,101,"Enable configurable GPU memory allocation of call_variants; To port DeepVariant to Apache Spark, the resource allocation should be configurable, including CPU vCores, main memory and GPU memory. In this pull request, we add two more parameters (enable_configurable_gpu and per_process_gpu_memory_fraction) to let users have more flexibility to allocate the resource of GPU memory. If enable_configurable_gpu is true, each call_variants process will allocate GPU memory by the value of per_process_gpu_memory_fraction. (Default is 100) Please help to check whether the modification is proper or not. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:357,testability,resourc,resource,357,"Enable configurable GPU memory allocation of call_variants; To port DeepVariant to Apache Spark, the resource allocation should be configurable, including CPU vCores, main memory and GPU memory. In this pull request, we add two more parameters (enable_configurable_gpu and per_process_gpu_memory_fraction) to let users have more flexibility to allocate the resource of GPU memory. If enable_configurable_gpu is true, each call_variants process will allocate GPU memory by the value of per_process_gpu_memory_fraction. (Default is 100) Please help to check whether the modification is proper or not. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:24,usability,memor,memory,24,"Enable configurable GPU memory allocation of call_variants; To port DeepVariant to Apache Spark, the resource allocation should be configurable, including CPU vCores, main memory and GPU memory. In this pull request, we add two more parameters (enable_configurable_gpu and per_process_gpu_memory_fraction) to let users have more flexibility to allocate the resource of GPU memory. If enable_configurable_gpu is true, each call_variants process will allocate GPU memory by the value of per_process_gpu_memory_fraction. (Default is 100) Please help to check whether the modification is proper or not. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:172,usability,memor,memory,172,"Enable configurable GPU memory allocation of call_variants; To port DeepVariant to Apache Spark, the resource allocation should be configurable, including CPU vCores, main memory and GPU memory. In this pull request, we add two more parameters (enable_configurable_gpu and per_process_gpu_memory_fraction) to let users have more flexibility to allocate the resource of GPU memory. If enable_configurable_gpu is true, each call_variants process will allocate GPU memory by the value of per_process_gpu_memory_fraction. (Default is 100) Please help to check whether the modification is proper or not. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:187,usability,memor,memory,187,"Enable configurable GPU memory allocation of call_variants; To port DeepVariant to Apache Spark, the resource allocation should be configurable, including CPU vCores, main memory and GPU memory. In this pull request, we add two more parameters (enable_configurable_gpu and per_process_gpu_memory_fraction) to let users have more flexibility to allocate the resource of GPU memory. If enable_configurable_gpu is true, each call_variants process will allocate GPU memory by the value of per_process_gpu_memory_fraction. (Default is 100) Please help to check whether the modification is proper or not. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:313,usability,user,users,313,"Enable configurable GPU memory allocation of call_variants; To port DeepVariant to Apache Spark, the resource allocation should be configurable, including CPU vCores, main memory and GPU memory. In this pull request, we add two more parameters (enable_configurable_gpu and per_process_gpu_memory_fraction) to let users have more flexibility to allocate the resource of GPU memory. If enable_configurable_gpu is true, each call_variants process will allocate GPU memory by the value of per_process_gpu_memory_fraction. (Default is 100) Please help to check whether the modification is proper or not. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:373,usability,memor,memory,373,"Enable configurable GPU memory allocation of call_variants; To port DeepVariant to Apache Spark, the resource allocation should be configurable, including CPU vCores, main memory and GPU memory. In this pull request, we add two more parameters (enable_configurable_gpu and per_process_gpu_memory_fraction) to let users have more flexibility to allocate the resource of GPU memory. If enable_configurable_gpu is true, each call_variants process will allocate GPU memory by the value of per_process_gpu_memory_fraction. (Default is 100) Please help to check whether the modification is proper or not. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:462,usability,memor,memory,462,"Enable configurable GPU memory allocation of call_variants; To port DeepVariant to Apache Spark, the resource allocation should be configurable, including CPU vCores, main memory and GPU memory. In this pull request, we add two more parameters (enable_configurable_gpu and per_process_gpu_memory_fraction) to let users have more flexibility to allocate the resource of GPU memory. If enable_configurable_gpu is true, each call_variants process will allocate GPU memory by the value of per_process_gpu_memory_fraction. (Default is 100) Please help to check whether the modification is proper or not. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/pull/159:542,usability,help,help,542,"Enable configurable GPU memory allocation of call_variants; To port DeepVariant to Apache Spark, the resource allocation should be configurable, including CPU vCores, main memory and GPU memory. In this pull request, we add two more parameters (enable_configurable_gpu and per_process_gpu_memory_fraction) to let users have more flexibility to allocate the resource of GPU memory. If enable_configurable_gpu is true, each call_variants process will allocate GPU memory by the value of per_process_gpu_memory_fraction. (Default is 100) Please help to check whether the modification is proper or not. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/pull/159
https://github.com/google/deepvariant/issues/160:620,availability,error,error,620,"build-prereq.sh only checks for a single location of PYCLIF; I am currently trying to install DeepVariant on Pop!_OS 18.10, which is based on Ubuntu 18.10. I installed Google CLIF from sources but the ""build-prereq.sh"" kept complaining that the PYCLIF isn't installed. On looking into the code of ""build-prereq.sh"", I found that the code only looks for a single location of PYCLIF:. `if [[ -e /usr/local/clif/bin/pyclif ]];`. While my PYCLIF was installed by default to:. `/usr/local/bin/pyclif`. I suggest, maybe, you could use something similar to Python disutils.spawn ? Additionally, there is a small mistake in the error message output:. `""CLIF is not installed on this machine and a prebuilt binary is not. unavailable for this platform. Please install CLIF at. https://github.com/google/clif before continuing.""`. The error message says, **""not unavailable""**, maybe make it **""unavailable""** or **""not available""**. A tiny cosmetic error, but can be easily fixed. Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/160
https://github.com/google/deepvariant/issues/160:713,availability,unavail,unavailable,713,"build-prereq.sh only checks for a single location of PYCLIF; I am currently trying to install DeepVariant on Pop!_OS 18.10, which is based on Ubuntu 18.10. I installed Google CLIF from sources but the ""build-prereq.sh"" kept complaining that the PYCLIF isn't installed. On looking into the code of ""build-prereq.sh"", I found that the code only looks for a single location of PYCLIF:. `if [[ -e /usr/local/clif/bin/pyclif ]];`. While my PYCLIF was installed by default to:. `/usr/local/bin/pyclif`. I suggest, maybe, you could use something similar to Python disutils.spawn ? Additionally, there is a small mistake in the error message output:. `""CLIF is not installed on this machine and a prebuilt binary is not. unavailable for this platform. Please install CLIF at. https://github.com/google/clif before continuing.""`. The error message says, **""not unavailable""**, maybe make it **""unavailable""** or **""not available""**. A tiny cosmetic error, but can be easily fixed. Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/160
https://github.com/google/deepvariant/issues/160:825,availability,error,error,825,"build-prereq.sh only checks for a single location of PYCLIF; I am currently trying to install DeepVariant on Pop!_OS 18.10, which is based on Ubuntu 18.10. I installed Google CLIF from sources but the ""build-prereq.sh"" kept complaining that the PYCLIF isn't installed. On looking into the code of ""build-prereq.sh"", I found that the code only looks for a single location of PYCLIF:. `if [[ -e /usr/local/clif/bin/pyclif ]];`. While my PYCLIF was installed by default to:. `/usr/local/bin/pyclif`. I suggest, maybe, you could use something similar to Python disutils.spawn ? Additionally, there is a small mistake in the error message output:. `""CLIF is not installed on this machine and a prebuilt binary is not. unavailable for this platform. Please install CLIF at. https://github.com/google/clif before continuing.""`. The error message says, **""not unavailable""**, maybe make it **""unavailable""** or **""not available""**. A tiny cosmetic error, but can be easily fixed. Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/160
https://github.com/google/deepvariant/issues/160:852,availability,unavail,unavailable,852,"build-prereq.sh only checks for a single location of PYCLIF; I am currently trying to install DeepVariant on Pop!_OS 18.10, which is based on Ubuntu 18.10. I installed Google CLIF from sources but the ""build-prereq.sh"" kept complaining that the PYCLIF isn't installed. On looking into the code of ""build-prereq.sh"", I found that the code only looks for a single location of PYCLIF:. `if [[ -e /usr/local/clif/bin/pyclif ]];`. While my PYCLIF was installed by default to:. `/usr/local/bin/pyclif`. I suggest, maybe, you could use something similar to Python disutils.spawn ? Additionally, there is a small mistake in the error message output:. `""CLIF is not installed on this machine and a prebuilt binary is not. unavailable for this platform. Please install CLIF at. https://github.com/google/clif before continuing.""`. The error message says, **""not unavailable""**, maybe make it **""unavailable""** or **""not available""**. A tiny cosmetic error, but can be easily fixed. Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/160
https://github.com/google/deepvariant/issues/160:885,availability,unavail,unavailable,885,"build-prereq.sh only checks for a single location of PYCLIF; I am currently trying to install DeepVariant on Pop!_OS 18.10, which is based on Ubuntu 18.10. I installed Google CLIF from sources but the ""build-prereq.sh"" kept complaining that the PYCLIF isn't installed. On looking into the code of ""build-prereq.sh"", I found that the code only looks for a single location of PYCLIF:. `if [[ -e /usr/local/clif/bin/pyclif ]];`. While my PYCLIF was installed by default to:. `/usr/local/bin/pyclif`. I suggest, maybe, you could use something similar to Python disutils.spawn ? Additionally, there is a small mistake in the error message output:. `""CLIF is not installed on this machine and a prebuilt binary is not. unavailable for this platform. Please install CLIF at. https://github.com/google/clif before continuing.""`. The error message says, **""not unavailable""**, maybe make it **""unavailable""** or **""not available""**. A tiny cosmetic error, but can be easily fixed. Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/160
https://github.com/google/deepvariant/issues/160:910,availability,avail,available,910,"build-prereq.sh only checks for a single location of PYCLIF; I am currently trying to install DeepVariant on Pop!_OS 18.10, which is based on Ubuntu 18.10. I installed Google CLIF from sources but the ""build-prereq.sh"" kept complaining that the PYCLIF isn't installed. On looking into the code of ""build-prereq.sh"", I found that the code only looks for a single location of PYCLIF:. `if [[ -e /usr/local/clif/bin/pyclif ]];`. While my PYCLIF was installed by default to:. `/usr/local/bin/pyclif`. I suggest, maybe, you could use something similar to Python disutils.spawn ? Additionally, there is a small mistake in the error message output:. `""CLIF is not installed on this machine and a prebuilt binary is not. unavailable for this platform. Please install CLIF at. https://github.com/google/clif before continuing.""`. The error message says, **""not unavailable""**, maybe make it **""unavailable""** or **""not available""**. A tiny cosmetic error, but can be easily fixed. Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/160
https://github.com/google/deepvariant/issues/160:940,availability,error,error,940,"build-prereq.sh only checks for a single location of PYCLIF; I am currently trying to install DeepVariant on Pop!_OS 18.10, which is based on Ubuntu 18.10. I installed Google CLIF from sources but the ""build-prereq.sh"" kept complaining that the PYCLIF isn't installed. On looking into the code of ""build-prereq.sh"", I found that the code only looks for a single location of PYCLIF:. `if [[ -e /usr/local/clif/bin/pyclif ]];`. While my PYCLIF was installed by default to:. `/usr/local/bin/pyclif`. I suggest, maybe, you could use something similar to Python disutils.spawn ? Additionally, there is a small mistake in the error message output:. `""CLIF is not installed on this machine and a prebuilt binary is not. unavailable for this platform. Please install CLIF at. https://github.com/google/clif before continuing.""`. The error message says, **""not unavailable""**, maybe make it **""unavailable""** or **""not available""**. A tiny cosmetic error, but can be easily fixed. Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/160
https://github.com/google/deepvariant/issues/160:0,deployability,build,build-prereq,0,"build-prereq.sh only checks for a single location of PYCLIF; I am currently trying to install DeepVariant on Pop!_OS 18.10, which is based on Ubuntu 18.10. I installed Google CLIF from sources but the ""build-prereq.sh"" kept complaining that the PYCLIF isn't installed. On looking into the code of ""build-prereq.sh"", I found that the code only looks for a single location of PYCLIF:. `if [[ -e /usr/local/clif/bin/pyclif ]];`. While my PYCLIF was installed by default to:. `/usr/local/bin/pyclif`. I suggest, maybe, you could use something similar to Python disutils.spawn ? Additionally, there is a small mistake in the error message output:. `""CLIF is not installed on this machine and a prebuilt binary is not. unavailable for this platform. Please install CLIF at. https://github.com/google/clif before continuing.""`. The error message says, **""not unavailable""**, maybe make it **""unavailable""** or **""not available""**. A tiny cosmetic error, but can be easily fixed. Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/160
https://github.com/google/deepvariant/issues/160:86,deployability,instal,install,86,"build-prereq.sh only checks for a single location of PYCLIF; I am currently trying to install DeepVariant on Pop!_OS 18.10, which is based on Ubuntu 18.10. I installed Google CLIF from sources but the ""build-prereq.sh"" kept complaining that the PYCLIF isn't installed. On looking into the code of ""build-prereq.sh"", I found that the code only looks for a single location of PYCLIF:. `if [[ -e /usr/local/clif/bin/pyclif ]];`. While my PYCLIF was installed by default to:. `/usr/local/bin/pyclif`. I suggest, maybe, you could use something similar to Python disutils.spawn ? Additionally, there is a small mistake in the error message output:. `""CLIF is not installed on this machine and a prebuilt binary is not. unavailable for this platform. Please install CLIF at. https://github.com/google/clif before continuing.""`. The error message says, **""not unavailable""**, maybe make it **""unavailable""** or **""not available""**. A tiny cosmetic error, but can be easily fixed. Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/160
https://github.com/google/deepvariant/issues/160:158,deployability,instal,installed,158,"build-prereq.sh only checks for a single location of PYCLIF; I am currently trying to install DeepVariant on Pop!_OS 18.10, which is based on Ubuntu 18.10. I installed Google CLIF from sources but the ""build-prereq.sh"" kept complaining that the PYCLIF isn't installed. On looking into the code of ""build-prereq.sh"", I found that the code only looks for a single location of PYCLIF:. `if [[ -e /usr/local/clif/bin/pyclif ]];`. While my PYCLIF was installed by default to:. `/usr/local/bin/pyclif`. I suggest, maybe, you could use something similar to Python disutils.spawn ? Additionally, there is a small mistake in the error message output:. `""CLIF is not installed on this machine and a prebuilt binary is not. unavailable for this platform. Please install CLIF at. https://github.com/google/clif before continuing.""`. The error message says, **""not unavailable""**, maybe make it **""unavailable""** or **""not available""**. A tiny cosmetic error, but can be easily fixed. Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/160
https://github.com/google/deepvariant/issues/160:202,deployability,build,build-prereq,202,"build-prereq.sh only checks for a single location of PYCLIF; I am currently trying to install DeepVariant on Pop!_OS 18.10, which is based on Ubuntu 18.10. I installed Google CLIF from sources but the ""build-prereq.sh"" kept complaining that the PYCLIF isn't installed. On looking into the code of ""build-prereq.sh"", I found that the code only looks for a single location of PYCLIF:. `if [[ -e /usr/local/clif/bin/pyclif ]];`. While my PYCLIF was installed by default to:. `/usr/local/bin/pyclif`. I suggest, maybe, you could use something similar to Python disutils.spawn ? Additionally, there is a small mistake in the error message output:. `""CLIF is not installed on this machine and a prebuilt binary is not. unavailable for this platform. Please install CLIF at. https://github.com/google/clif before continuing.""`. The error message says, **""not unavailable""**, maybe make it **""unavailable""** or **""not available""**. A tiny cosmetic error, but can be easily fixed. Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/160
https://github.com/google/deepvariant/issues/160:258,deployability,instal,installed,258,"build-prereq.sh only checks for a single location of PYCLIF; I am currently trying to install DeepVariant on Pop!_OS 18.10, which is based on Ubuntu 18.10. I installed Google CLIF from sources but the ""build-prereq.sh"" kept complaining that the PYCLIF isn't installed. On looking into the code of ""build-prereq.sh"", I found that the code only looks for a single location of PYCLIF:. `if [[ -e /usr/local/clif/bin/pyclif ]];`. While my PYCLIF was installed by default to:. `/usr/local/bin/pyclif`. I suggest, maybe, you could use something similar to Python disutils.spawn ? Additionally, there is a small mistake in the error message output:. `""CLIF is not installed on this machine and a prebuilt binary is not. unavailable for this platform. Please install CLIF at. https://github.com/google/clif before continuing.""`. The error message says, **""not unavailable""**, maybe make it **""unavailable""** or **""not available""**. A tiny cosmetic error, but can be easily fixed. Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/160
https://github.com/google/deepvariant/issues/160:298,deployability,build,build-prereq,298,"build-prereq.sh only checks for a single location of PYCLIF; I am currently trying to install DeepVariant on Pop!_OS 18.10, which is based on Ubuntu 18.10. I installed Google CLIF from sources but the ""build-prereq.sh"" kept complaining that the PYCLIF isn't installed. On looking into the code of ""build-prereq.sh"", I found that the code only looks for a single location of PYCLIF:. `if [[ -e /usr/local/clif/bin/pyclif ]];`. While my PYCLIF was installed by default to:. `/usr/local/bin/pyclif`. I suggest, maybe, you could use something similar to Python disutils.spawn ? Additionally, there is a small mistake in the error message output:. `""CLIF is not installed on this machine and a prebuilt binary is not. unavailable for this platform. Please install CLIF at. https://github.com/google/clif before continuing.""`. The error message says, **""not unavailable""**, maybe make it **""unavailable""** or **""not available""**. A tiny cosmetic error, but can be easily fixed. Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/160
https://github.com/google/deepvariant/issues/160:446,deployability,instal,installed,446,"build-prereq.sh only checks for a single location of PYCLIF; I am currently trying to install DeepVariant on Pop!_OS 18.10, which is based on Ubuntu 18.10. I installed Google CLIF from sources but the ""build-prereq.sh"" kept complaining that the PYCLIF isn't installed. On looking into the code of ""build-prereq.sh"", I found that the code only looks for a single location of PYCLIF:. `if [[ -e /usr/local/clif/bin/pyclif ]];`. While my PYCLIF was installed by default to:. `/usr/local/bin/pyclif`. I suggest, maybe, you could use something similar to Python disutils.spawn ? Additionally, there is a small mistake in the error message output:. `""CLIF is not installed on this machine and a prebuilt binary is not. unavailable for this platform. Please install CLIF at. https://github.com/google/clif before continuing.""`. The error message says, **""not unavailable""**, maybe make it **""unavailable""** or **""not available""**. A tiny cosmetic error, but can be easily fixed. Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/160
https://github.com/google/deepvariant/issues/160:657,deployability,instal,installed,657,"build-prereq.sh only checks for a single location of PYCLIF; I am currently trying to install DeepVariant on Pop!_OS 18.10, which is based on Ubuntu 18.10. I installed Google CLIF from sources but the ""build-prereq.sh"" kept complaining that the PYCLIF isn't installed. On looking into the code of ""build-prereq.sh"", I found that the code only looks for a single location of PYCLIF:. `if [[ -e /usr/local/clif/bin/pyclif ]];`. While my PYCLIF was installed by default to:. `/usr/local/bin/pyclif`. I suggest, maybe, you could use something similar to Python disutils.spawn ? Additionally, there is a small mistake in the error message output:. `""CLIF is not installed on this machine and a prebuilt binary is not. unavailable for this platform. Please install CLIF at. https://github.com/google/clif before continuing.""`. The error message says, **""not unavailable""**, maybe make it **""unavailable""** or **""not available""**. A tiny cosmetic error, but can be easily fixed. Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/160
https://github.com/google/deepvariant/issues/160:751,deployability,instal,install,751,"build-prereq.sh only checks for a single location of PYCLIF; I am currently trying to install DeepVariant on Pop!_OS 18.10, which is based on Ubuntu 18.10. I installed Google CLIF from sources but the ""build-prereq.sh"" kept complaining that the PYCLIF isn't installed. On looking into the code of ""build-prereq.sh"", I found that the code only looks for a single location of PYCLIF:. `if [[ -e /usr/local/clif/bin/pyclif ]];`. While my PYCLIF was installed by default to:. `/usr/local/bin/pyclif`. I suggest, maybe, you could use something similar to Python disutils.spawn ? Additionally, there is a small mistake in the error message output:. `""CLIF is not installed on this machine and a prebuilt binary is not. unavailable for this platform. Please install CLIF at. https://github.com/google/clif before continuing.""`. The error message says, **""not unavailable""**, maybe make it **""unavailable""** or **""not available""**. A tiny cosmetic error, but can be easily fixed. Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/160
https://github.com/google/deepvariant/issues/160:806,deployability,continu,continuing,806,"build-prereq.sh only checks for a single location of PYCLIF; I am currently trying to install DeepVariant on Pop!_OS 18.10, which is based on Ubuntu 18.10. I installed Google CLIF from sources but the ""build-prereq.sh"" kept complaining that the PYCLIF isn't installed. On looking into the code of ""build-prereq.sh"", I found that the code only looks for a single location of PYCLIF:. `if [[ -e /usr/local/clif/bin/pyclif ]];`. While my PYCLIF was installed by default to:. `/usr/local/bin/pyclif`. I suggest, maybe, you could use something similar to Python disutils.spawn ? Additionally, there is a small mistake in the error message output:. `""CLIF is not installed on this machine and a prebuilt binary is not. unavailable for this platform. Please install CLIF at. https://github.com/google/clif before continuing.""`. The error message says, **""not unavailable""**, maybe make it **""unavailable""** or **""not available""**. A tiny cosmetic error, but can be easily fixed. Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/160
https://github.com/google/deepvariant/issues/160:66,energy efficiency,current,currently,66,"build-prereq.sh only checks for a single location of PYCLIF; I am currently trying to install DeepVariant on Pop!_OS 18.10, which is based on Ubuntu 18.10. I installed Google CLIF from sources but the ""build-prereq.sh"" kept complaining that the PYCLIF isn't installed. On looking into the code of ""build-prereq.sh"", I found that the code only looks for a single location of PYCLIF:. `if [[ -e /usr/local/clif/bin/pyclif ]];`. While my PYCLIF was installed by default to:. `/usr/local/bin/pyclif`. I suggest, maybe, you could use something similar to Python disutils.spawn ? Additionally, there is a small mistake in the error message output:. `""CLIF is not installed on this machine and a prebuilt binary is not. unavailable for this platform. Please install CLIF at. https://github.com/google/clif before continuing.""`. The error message says, **""not unavailable""**, maybe make it **""unavailable""** or **""not available""**. A tiny cosmetic error, but can be easily fixed. Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/160
https://github.com/google/deepvariant/issues/160:626,integrability,messag,message,626,"build-prereq.sh only checks for a single location of PYCLIF; I am currently trying to install DeepVariant on Pop!_OS 18.10, which is based on Ubuntu 18.10. I installed Google CLIF from sources but the ""build-prereq.sh"" kept complaining that the PYCLIF isn't installed. On looking into the code of ""build-prereq.sh"", I found that the code only looks for a single location of PYCLIF:. `if [[ -e /usr/local/clif/bin/pyclif ]];`. While my PYCLIF was installed by default to:. `/usr/local/bin/pyclif`. I suggest, maybe, you could use something similar to Python disutils.spawn ? Additionally, there is a small mistake in the error message output:. `""CLIF is not installed on this machine and a prebuilt binary is not. unavailable for this platform. Please install CLIF at. https://github.com/google/clif before continuing.""`. The error message says, **""not unavailable""**, maybe make it **""unavailable""** or **""not available""**. A tiny cosmetic error, but can be easily fixed. Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/160
https://github.com/google/deepvariant/issues/160:831,integrability,messag,message,831,"build-prereq.sh only checks for a single location of PYCLIF; I am currently trying to install DeepVariant on Pop!_OS 18.10, which is based on Ubuntu 18.10. I installed Google CLIF from sources but the ""build-prereq.sh"" kept complaining that the PYCLIF isn't installed. On looking into the code of ""build-prereq.sh"", I found that the code only looks for a single location of PYCLIF:. `if [[ -e /usr/local/clif/bin/pyclif ]];`. While my PYCLIF was installed by default to:. `/usr/local/bin/pyclif`. I suggest, maybe, you could use something similar to Python disutils.spawn ? Additionally, there is a small mistake in the error message output:. `""CLIF is not installed on this machine and a prebuilt binary is not. unavailable for this platform. Please install CLIF at. https://github.com/google/clif before continuing.""`. The error message says, **""not unavailable""**, maybe make it **""unavailable""** or **""not available""**. A tiny cosmetic error, but can be easily fixed. Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/160
https://github.com/google/deepvariant/issues/160:626,interoperability,messag,message,626,"build-prereq.sh only checks for a single location of PYCLIF; I am currently trying to install DeepVariant on Pop!_OS 18.10, which is based on Ubuntu 18.10. I installed Google CLIF from sources but the ""build-prereq.sh"" kept complaining that the PYCLIF isn't installed. On looking into the code of ""build-prereq.sh"", I found that the code only looks for a single location of PYCLIF:. `if [[ -e /usr/local/clif/bin/pyclif ]];`. While my PYCLIF was installed by default to:. `/usr/local/bin/pyclif`. I suggest, maybe, you could use something similar to Python disutils.spawn ? Additionally, there is a small mistake in the error message output:. `""CLIF is not installed on this machine and a prebuilt binary is not. unavailable for this platform. Please install CLIF at. https://github.com/google/clif before continuing.""`. The error message says, **""not unavailable""**, maybe make it **""unavailable""** or **""not available""**. A tiny cosmetic error, but can be easily fixed. Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/160
https://github.com/google/deepvariant/issues/160:734,interoperability,platform,platform,734,"build-prereq.sh only checks for a single location of PYCLIF; I am currently trying to install DeepVariant on Pop!_OS 18.10, which is based on Ubuntu 18.10. I installed Google CLIF from sources but the ""build-prereq.sh"" kept complaining that the PYCLIF isn't installed. On looking into the code of ""build-prereq.sh"", I found that the code only looks for a single location of PYCLIF:. `if [[ -e /usr/local/clif/bin/pyclif ]];`. While my PYCLIF was installed by default to:. `/usr/local/bin/pyclif`. I suggest, maybe, you could use something similar to Python disutils.spawn ? Additionally, there is a small mistake in the error message output:. `""CLIF is not installed on this machine and a prebuilt binary is not. unavailable for this platform. Please install CLIF at. https://github.com/google/clif before continuing.""`. The error message says, **""not unavailable""**, maybe make it **""unavailable""** or **""not available""**. A tiny cosmetic error, but can be easily fixed. Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/160
https://github.com/google/deepvariant/issues/160:831,interoperability,messag,message,831,"build-prereq.sh only checks for a single location of PYCLIF; I am currently trying to install DeepVariant on Pop!_OS 18.10, which is based on Ubuntu 18.10. I installed Google CLIF from sources but the ""build-prereq.sh"" kept complaining that the PYCLIF isn't installed. On looking into the code of ""build-prereq.sh"", I found that the code only looks for a single location of PYCLIF:. `if [[ -e /usr/local/clif/bin/pyclif ]];`. While my PYCLIF was installed by default to:. `/usr/local/bin/pyclif`. I suggest, maybe, you could use something similar to Python disutils.spawn ? Additionally, there is a small mistake in the error message output:. `""CLIF is not installed on this machine and a prebuilt binary is not. unavailable for this platform. Please install CLIF at. https://github.com/google/clif before continuing.""`. The error message says, **""not unavailable""**, maybe make it **""unavailable""** or **""not available""**. A tiny cosmetic error, but can be easily fixed. Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/160
https://github.com/google/deepvariant/issues/160:620,performance,error,error,620,"build-prereq.sh only checks for a single location of PYCLIF; I am currently trying to install DeepVariant on Pop!_OS 18.10, which is based on Ubuntu 18.10. I installed Google CLIF from sources but the ""build-prereq.sh"" kept complaining that the PYCLIF isn't installed. On looking into the code of ""build-prereq.sh"", I found that the code only looks for a single location of PYCLIF:. `if [[ -e /usr/local/clif/bin/pyclif ]];`. While my PYCLIF was installed by default to:. `/usr/local/bin/pyclif`. I suggest, maybe, you could use something similar to Python disutils.spawn ? Additionally, there is a small mistake in the error message output:. `""CLIF is not installed on this machine and a prebuilt binary is not. unavailable for this platform. Please install CLIF at. https://github.com/google/clif before continuing.""`. The error message says, **""not unavailable""**, maybe make it **""unavailable""** or **""not available""**. A tiny cosmetic error, but can be easily fixed. Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/160
https://github.com/google/deepvariant/issues/160:825,performance,error,error,825,"build-prereq.sh only checks for a single location of PYCLIF; I am currently trying to install DeepVariant on Pop!_OS 18.10, which is based on Ubuntu 18.10. I installed Google CLIF from sources but the ""build-prereq.sh"" kept complaining that the PYCLIF isn't installed. On looking into the code of ""build-prereq.sh"", I found that the code only looks for a single location of PYCLIF:. `if [[ -e /usr/local/clif/bin/pyclif ]];`. While my PYCLIF was installed by default to:. `/usr/local/bin/pyclif`. I suggest, maybe, you could use something similar to Python disutils.spawn ? Additionally, there is a small mistake in the error message output:. `""CLIF is not installed on this machine and a prebuilt binary is not. unavailable for this platform. Please install CLIF at. https://github.com/google/clif before continuing.""`. The error message says, **""not unavailable""**, maybe make it **""unavailable""** or **""not available""**. A tiny cosmetic error, but can be easily fixed. Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/160
https://github.com/google/deepvariant/issues/160:940,performance,error,error,940,"build-prereq.sh only checks for a single location of PYCLIF; I am currently trying to install DeepVariant on Pop!_OS 18.10, which is based on Ubuntu 18.10. I installed Google CLIF from sources but the ""build-prereq.sh"" kept complaining that the PYCLIF isn't installed. On looking into the code of ""build-prereq.sh"", I found that the code only looks for a single location of PYCLIF:. `if [[ -e /usr/local/clif/bin/pyclif ]];`. While my PYCLIF was installed by default to:. `/usr/local/bin/pyclif`. I suggest, maybe, you could use something similar to Python disutils.spawn ? Additionally, there is a small mistake in the error message output:. `""CLIF is not installed on this machine and a prebuilt binary is not. unavailable for this platform. Please install CLIF at. https://github.com/google/clif before continuing.""`. The error message says, **""not unavailable""**, maybe make it **""unavailable""** or **""not available""**. A tiny cosmetic error, but can be easily fixed. Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/160
https://github.com/google/deepvariant/issues/160:910,reliability,availab,available,910,"build-prereq.sh only checks for a single location of PYCLIF; I am currently trying to install DeepVariant on Pop!_OS 18.10, which is based on Ubuntu 18.10. I installed Google CLIF from sources but the ""build-prereq.sh"" kept complaining that the PYCLIF isn't installed. On looking into the code of ""build-prereq.sh"", I found that the code only looks for a single location of PYCLIF:. `if [[ -e /usr/local/clif/bin/pyclif ]];`. While my PYCLIF was installed by default to:. `/usr/local/bin/pyclif`. I suggest, maybe, you could use something similar to Python disutils.spawn ? Additionally, there is a small mistake in the error message output:. `""CLIF is not installed on this machine and a prebuilt binary is not. unavailable for this platform. Please install CLIF at. https://github.com/google/clif before continuing.""`. The error message says, **""not unavailable""**, maybe make it **""unavailable""** or **""not available""**. A tiny cosmetic error, but can be easily fixed. Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/160
https://github.com/google/deepvariant/issues/160:224,safety,compl,complaining,224,"build-prereq.sh only checks for a single location of PYCLIF; I am currently trying to install DeepVariant on Pop!_OS 18.10, which is based on Ubuntu 18.10. I installed Google CLIF from sources but the ""build-prereq.sh"" kept complaining that the PYCLIF isn't installed. On looking into the code of ""build-prereq.sh"", I found that the code only looks for a single location of PYCLIF:. `if [[ -e /usr/local/clif/bin/pyclif ]];`. While my PYCLIF was installed by default to:. `/usr/local/bin/pyclif`. I suggest, maybe, you could use something similar to Python disutils.spawn ? Additionally, there is a small mistake in the error message output:. `""CLIF is not installed on this machine and a prebuilt binary is not. unavailable for this platform. Please install CLIF at. https://github.com/google/clif before continuing.""`. The error message says, **""not unavailable""**, maybe make it **""unavailable""** or **""not available""**. A tiny cosmetic error, but can be easily fixed. Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/160
https://github.com/google/deepvariant/issues/160:620,safety,error,error,620,"build-prereq.sh only checks for a single location of PYCLIF; I am currently trying to install DeepVariant on Pop!_OS 18.10, which is based on Ubuntu 18.10. I installed Google CLIF from sources but the ""build-prereq.sh"" kept complaining that the PYCLIF isn't installed. On looking into the code of ""build-prereq.sh"", I found that the code only looks for a single location of PYCLIF:. `if [[ -e /usr/local/clif/bin/pyclif ]];`. While my PYCLIF was installed by default to:. `/usr/local/bin/pyclif`. I suggest, maybe, you could use something similar to Python disutils.spawn ? Additionally, there is a small mistake in the error message output:. `""CLIF is not installed on this machine and a prebuilt binary is not. unavailable for this platform. Please install CLIF at. https://github.com/google/clif before continuing.""`. The error message says, **""not unavailable""**, maybe make it **""unavailable""** or **""not available""**. A tiny cosmetic error, but can be easily fixed. Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/160
https://github.com/google/deepvariant/issues/160:825,safety,error,error,825,"build-prereq.sh only checks for a single location of PYCLIF; I am currently trying to install DeepVariant on Pop!_OS 18.10, which is based on Ubuntu 18.10. I installed Google CLIF from sources but the ""build-prereq.sh"" kept complaining that the PYCLIF isn't installed. On looking into the code of ""build-prereq.sh"", I found that the code only looks for a single location of PYCLIF:. `if [[ -e /usr/local/clif/bin/pyclif ]];`. While my PYCLIF was installed by default to:. `/usr/local/bin/pyclif`. I suggest, maybe, you could use something similar to Python disutils.spawn ? Additionally, there is a small mistake in the error message output:. `""CLIF is not installed on this machine and a prebuilt binary is not. unavailable for this platform. Please install CLIF at. https://github.com/google/clif before continuing.""`. The error message says, **""not unavailable""**, maybe make it **""unavailable""** or **""not available""**. A tiny cosmetic error, but can be easily fixed. Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/160
https://github.com/google/deepvariant/issues/160:910,safety,avail,available,910,"build-prereq.sh only checks for a single location of PYCLIF; I am currently trying to install DeepVariant on Pop!_OS 18.10, which is based on Ubuntu 18.10. I installed Google CLIF from sources but the ""build-prereq.sh"" kept complaining that the PYCLIF isn't installed. On looking into the code of ""build-prereq.sh"", I found that the code only looks for a single location of PYCLIF:. `if [[ -e /usr/local/clif/bin/pyclif ]];`. While my PYCLIF was installed by default to:. `/usr/local/bin/pyclif`. I suggest, maybe, you could use something similar to Python disutils.spawn ? Additionally, there is a small mistake in the error message output:. `""CLIF is not installed on this machine and a prebuilt binary is not. unavailable for this platform. Please install CLIF at. https://github.com/google/clif before continuing.""`. The error message says, **""not unavailable""**, maybe make it **""unavailable""** or **""not available""**. A tiny cosmetic error, but can be easily fixed. Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/160
https://github.com/google/deepvariant/issues/160:940,safety,error,error,940,"build-prereq.sh only checks for a single location of PYCLIF; I am currently trying to install DeepVariant on Pop!_OS 18.10, which is based on Ubuntu 18.10. I installed Google CLIF from sources but the ""build-prereq.sh"" kept complaining that the PYCLIF isn't installed. On looking into the code of ""build-prereq.sh"", I found that the code only looks for a single location of PYCLIF:. `if [[ -e /usr/local/clif/bin/pyclif ]];`. While my PYCLIF was installed by default to:. `/usr/local/bin/pyclif`. I suggest, maybe, you could use something similar to Python disutils.spawn ? Additionally, there is a small mistake in the error message output:. `""CLIF is not installed on this machine and a prebuilt binary is not. unavailable for this platform. Please install CLIF at. https://github.com/google/clif before continuing.""`. The error message says, **""not unavailable""**, maybe make it **""unavailable""** or **""not available""**. A tiny cosmetic error, but can be easily fixed. Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/160
https://github.com/google/deepvariant/issues/160:224,security,compl,complaining,224,"build-prereq.sh only checks for a single location of PYCLIF; I am currently trying to install DeepVariant on Pop!_OS 18.10, which is based on Ubuntu 18.10. I installed Google CLIF from sources but the ""build-prereq.sh"" kept complaining that the PYCLIF isn't installed. On looking into the code of ""build-prereq.sh"", I found that the code only looks for a single location of PYCLIF:. `if [[ -e /usr/local/clif/bin/pyclif ]];`. While my PYCLIF was installed by default to:. `/usr/local/bin/pyclif`. I suggest, maybe, you could use something similar to Python disutils.spawn ? Additionally, there is a small mistake in the error message output:. `""CLIF is not installed on this machine and a prebuilt binary is not. unavailable for this platform. Please install CLIF at. https://github.com/google/clif before continuing.""`. The error message says, **""not unavailable""**, maybe make it **""unavailable""** or **""not available""**. A tiny cosmetic error, but can be easily fixed. Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/160
https://github.com/google/deepvariant/issues/160:910,security,availab,available,910,"build-prereq.sh only checks for a single location of PYCLIF; I am currently trying to install DeepVariant on Pop!_OS 18.10, which is based on Ubuntu 18.10. I installed Google CLIF from sources but the ""build-prereq.sh"" kept complaining that the PYCLIF isn't installed. On looking into the code of ""build-prereq.sh"", I found that the code only looks for a single location of PYCLIF:. `if [[ -e /usr/local/clif/bin/pyclif ]];`. While my PYCLIF was installed by default to:. `/usr/local/bin/pyclif`. I suggest, maybe, you could use something similar to Python disutils.spawn ? Additionally, there is a small mistake in the error message output:. `""CLIF is not installed on this machine and a prebuilt binary is not. unavailable for this platform. Please install CLIF at. https://github.com/google/clif before continuing.""`. The error message says, **""not unavailable""**, maybe make it **""unavailable""** or **""not available""**. A tiny cosmetic error, but can be easily fixed. Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/160
https://github.com/google/deepvariant/issues/160:620,usability,error,error,620,"build-prereq.sh only checks for a single location of PYCLIF; I am currently trying to install DeepVariant on Pop!_OS 18.10, which is based on Ubuntu 18.10. I installed Google CLIF from sources but the ""build-prereq.sh"" kept complaining that the PYCLIF isn't installed. On looking into the code of ""build-prereq.sh"", I found that the code only looks for a single location of PYCLIF:. `if [[ -e /usr/local/clif/bin/pyclif ]];`. While my PYCLIF was installed by default to:. `/usr/local/bin/pyclif`. I suggest, maybe, you could use something similar to Python disutils.spawn ? Additionally, there is a small mistake in the error message output:. `""CLIF is not installed on this machine and a prebuilt binary is not. unavailable for this platform. Please install CLIF at. https://github.com/google/clif before continuing.""`. The error message says, **""not unavailable""**, maybe make it **""unavailable""** or **""not available""**. A tiny cosmetic error, but can be easily fixed. Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/160
https://github.com/google/deepvariant/issues/160:825,usability,error,error,825,"build-prereq.sh only checks for a single location of PYCLIF; I am currently trying to install DeepVariant on Pop!_OS 18.10, which is based on Ubuntu 18.10. I installed Google CLIF from sources but the ""build-prereq.sh"" kept complaining that the PYCLIF isn't installed. On looking into the code of ""build-prereq.sh"", I found that the code only looks for a single location of PYCLIF:. `if [[ -e /usr/local/clif/bin/pyclif ]];`. While my PYCLIF was installed by default to:. `/usr/local/bin/pyclif`. I suggest, maybe, you could use something similar to Python disutils.spawn ? Additionally, there is a small mistake in the error message output:. `""CLIF is not installed on this machine and a prebuilt binary is not. unavailable for this platform. Please install CLIF at. https://github.com/google/clif before continuing.""`. The error message says, **""not unavailable""**, maybe make it **""unavailable""** or **""not available""**. A tiny cosmetic error, but can be easily fixed. Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/160
https://github.com/google/deepvariant/issues/160:940,usability,error,error,940,"build-prereq.sh only checks for a single location of PYCLIF; I am currently trying to install DeepVariant on Pop!_OS 18.10, which is based on Ubuntu 18.10. I installed Google CLIF from sources but the ""build-prereq.sh"" kept complaining that the PYCLIF isn't installed. On looking into the code of ""build-prereq.sh"", I found that the code only looks for a single location of PYCLIF:. `if [[ -e /usr/local/clif/bin/pyclif ]];`. While my PYCLIF was installed by default to:. `/usr/local/bin/pyclif`. I suggest, maybe, you could use something similar to Python disutils.spawn ? Additionally, there is a small mistake in the error message output:. `""CLIF is not installed on this machine and a prebuilt binary is not. unavailable for this platform. Please install CLIF at. https://github.com/google/clif before continuing.""`. The error message says, **""not unavailable""**, maybe make it **""unavailable""** or **""not available""**. A tiny cosmetic error, but can be easily fixed. Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/160
https://github.com/google/deepvariant/issues/162:81,availability,slo,slow,81,"Training model with GPU; I tried to train the DeepVariant on CPU and it was very slow. I now have access to a google cloud instance with GPUs. I am new with google cloud and I am not sure how I can accelerate the training process with GPUs. According to the [Case Study](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md), I can use the use_tpu flag to accelerate the training process with TPUs, but I don't know if I can do the same thing with GPUs. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/162
https://github.com/google/deepvariant/issues/162:9,energy efficiency,model,model,9,"Training model with GPU; I tried to train the DeepVariant on CPU and it was very slow. I now have access to a google cloud instance with GPUs. I am new with google cloud and I am not sure how I can accelerate the training process with GPUs. According to the [Case Study](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md), I can use the use_tpu flag to accelerate the training process with TPUs, but I don't know if I can do the same thing with GPUs. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/162
https://github.com/google/deepvariant/issues/162:20,energy efficiency,GPU,GPU,20,"Training model with GPU; I tried to train the DeepVariant on CPU and it was very slow. I now have access to a google cloud instance with GPUs. I am new with google cloud and I am not sure how I can accelerate the training process with GPUs. According to the [Case Study](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md), I can use the use_tpu flag to accelerate the training process with TPUs, but I don't know if I can do the same thing with GPUs. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/162
https://github.com/google/deepvariant/issues/162:61,energy efficiency,CPU,CPU,61,"Training model with GPU; I tried to train the DeepVariant on CPU and it was very slow. I now have access to a google cloud instance with GPUs. I am new with google cloud and I am not sure how I can accelerate the training process with GPUs. According to the [Case Study](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md), I can use the use_tpu flag to accelerate the training process with TPUs, but I don't know if I can do the same thing with GPUs. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/162
https://github.com/google/deepvariant/issues/162:117,energy efficiency,cloud,cloud,117,"Training model with GPU; I tried to train the DeepVariant on CPU and it was very slow. I now have access to a google cloud instance with GPUs. I am new with google cloud and I am not sure how I can accelerate the training process with GPUs. According to the [Case Study](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md), I can use the use_tpu flag to accelerate the training process with TPUs, but I don't know if I can do the same thing with GPUs. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/162
https://github.com/google/deepvariant/issues/162:137,energy efficiency,GPU,GPUs,137,"Training model with GPU; I tried to train the DeepVariant on CPU and it was very slow. I now have access to a google cloud instance with GPUs. I am new with google cloud and I am not sure how I can accelerate the training process with GPUs. According to the [Case Study](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md), I can use the use_tpu flag to accelerate the training process with TPUs, but I don't know if I can do the same thing with GPUs. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/162
https://github.com/google/deepvariant/issues/162:164,energy efficiency,cloud,cloud,164,"Training model with GPU; I tried to train the DeepVariant on CPU and it was very slow. I now have access to a google cloud instance with GPUs. I am new with google cloud and I am not sure how I can accelerate the training process with GPUs. According to the [Case Study](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md), I can use the use_tpu flag to accelerate the training process with TPUs, but I don't know if I can do the same thing with GPUs. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/162
https://github.com/google/deepvariant/issues/162:235,energy efficiency,GPU,GPUs,235,"Training model with GPU; I tried to train the DeepVariant on CPU and it was very slow. I now have access to a google cloud instance with GPUs. I am new with google cloud and I am not sure how I can accelerate the training process with GPUs. According to the [Case Study](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md), I can use the use_tpu flag to accelerate the training process with TPUs, but I don't know if I can do the same thing with GPUs. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/162
https://github.com/google/deepvariant/issues/162:487,energy efficiency,GPU,GPUs,487,"Training model with GPU; I tried to train the DeepVariant on CPU and it was very slow. I now have access to a google cloud instance with GPUs. I am new with google cloud and I am not sure how I can accelerate the training process with GPUs. According to the [Case Study](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md), I can use the use_tpu flag to accelerate the training process with TPUs, but I don't know if I can do the same thing with GPUs. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/162
https://github.com/google/deepvariant/issues/162:20,performance,GPU,GPU,20,"Training model with GPU; I tried to train the DeepVariant on CPU and it was very slow. I now have access to a google cloud instance with GPUs. I am new with google cloud and I am not sure how I can accelerate the training process with GPUs. According to the [Case Study](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md), I can use the use_tpu flag to accelerate the training process with TPUs, but I don't know if I can do the same thing with GPUs. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/162
https://github.com/google/deepvariant/issues/162:61,performance,CPU,CPU,61,"Training model with GPU; I tried to train the DeepVariant on CPU and it was very slow. I now have access to a google cloud instance with GPUs. I am new with google cloud and I am not sure how I can accelerate the training process with GPUs. According to the [Case Study](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md), I can use the use_tpu flag to accelerate the training process with TPUs, but I don't know if I can do the same thing with GPUs. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/162
https://github.com/google/deepvariant/issues/162:137,performance,GPU,GPUs,137,"Training model with GPU; I tried to train the DeepVariant on CPU and it was very slow. I now have access to a google cloud instance with GPUs. I am new with google cloud and I am not sure how I can accelerate the training process with GPUs. According to the [Case Study](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md), I can use the use_tpu flag to accelerate the training process with TPUs, but I don't know if I can do the same thing with GPUs. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/162
https://github.com/google/deepvariant/issues/162:235,performance,GPU,GPUs,235,"Training model with GPU; I tried to train the DeepVariant on CPU and it was very slow. I now have access to a google cloud instance with GPUs. I am new with google cloud and I am not sure how I can accelerate the training process with GPUs. According to the [Case Study](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md), I can use the use_tpu flag to accelerate the training process with TPUs, but I don't know if I can do the same thing with GPUs. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/162
https://github.com/google/deepvariant/issues/162:487,performance,GPU,GPUs,487,"Training model with GPU; I tried to train the DeepVariant on CPU and it was very slow. I now have access to a google cloud instance with GPUs. I am new with google cloud and I am not sure how I can accelerate the training process with GPUs. According to the [Case Study](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md), I can use the use_tpu flag to accelerate the training process with TPUs, but I don't know if I can do the same thing with GPUs. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/162
https://github.com/google/deepvariant/issues/162:81,reliability,slo,slow,81,"Training model with GPU; I tried to train the DeepVariant on CPU and it was very slow. I now have access to a google cloud instance with GPUs. I am new with google cloud and I am not sure how I can accelerate the training process with GPUs. According to the [Case Study](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md), I can use the use_tpu flag to accelerate the training process with TPUs, but I don't know if I can do the same thing with GPUs. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/162
https://github.com/google/deepvariant/issues/162:9,security,model,model,9,"Training model with GPU; I tried to train the DeepVariant on CPU and it was very slow. I now have access to a google cloud instance with GPUs. I am new with google cloud and I am not sure how I can accelerate the training process with GPUs. According to the [Case Study](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md), I can use the use_tpu flag to accelerate the training process with TPUs, but I don't know if I can do the same thing with GPUs. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/162
https://github.com/google/deepvariant/issues/162:98,security,access,access,98,"Training model with GPU; I tried to train the DeepVariant on CPU and it was very slow. I now have access to a google cloud instance with GPUs. I am new with google cloud and I am not sure how I can accelerate the training process with GPUs. According to the [Case Study](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md), I can use the use_tpu flag to accelerate the training process with TPUs, but I don't know if I can do the same thing with GPUs. .",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/162
https://github.com/google/deepvariant/issues/163:6,energy efficiency,gpu,gpu,6,multi-gpu support?; Is there any multi-gpu support? Would it be a waste to run DV on instances with multiple GPUs?,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/163
https://github.com/google/deepvariant/issues/163:39,energy efficiency,gpu,gpu,39,multi-gpu support?; Is there any multi-gpu support? Would it be a waste to run DV on instances with multiple GPUs?,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/163
https://github.com/google/deepvariant/issues/163:109,energy efficiency,GPU,GPUs,109,multi-gpu support?; Is there any multi-gpu support? Would it be a waste to run DV on instances with multiple GPUs?,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/163
https://github.com/google/deepvariant/issues/163:6,performance,gpu,gpu,6,multi-gpu support?; Is there any multi-gpu support? Would it be a waste to run DV on instances with multiple GPUs?,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/163
https://github.com/google/deepvariant/issues/163:39,performance,gpu,gpu,39,multi-gpu support?; Is there any multi-gpu support? Would it be a waste to run DV on instances with multiple GPUs?,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/163
https://github.com/google/deepvariant/issues/163:109,performance,GPU,GPUs,109,multi-gpu support?; Is there any multi-gpu support? Would it be a waste to run DV on instances with multiple GPUs?,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/163
https://github.com/google/deepvariant/issues/163:10,usability,support,support,10,multi-gpu support?; Is there any multi-gpu support? Would it be a waste to run DV on instances with multiple GPUs?,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/163
https://github.com/google/deepvariant/issues/163:43,usability,support,support,43,multi-gpu support?; Is there any multi-gpu support? Would it be a waste to run DV on instances with multiple GPUs?,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/163
https://github.com/google/deepvariant/issues/164:26,availability,error,error,26,"Failed to retrieve block: error reading file for gvcf postprocess variants for WGS; Hi, I'm getting the following error when trying to postprocess the gvcf (generation of the normal vcf works fine). Any ideas why this is happening? It's happening across several different samples, but always at the same site. Could it be something missing in the reference fasta file? If so is there a setting to have it fail less spectacularly and skip over variants it for some reason cannot find? ```. I0320 10:18:29.648770 140566999074560 postprocess_variants.py:593] Writing output to VCF file: UFC100105-Normal-SM-CUCI1.gvcf. I0320 10:18:29.649548 140566999074560 genomics_writer.py:118] Writing UFC100105-Normal-SM-CUCI1.gvcf with NativeVcfWriter. [E::fai_retrieve] Failed to retrieve block: error reading file. Traceback (most recent call last):. File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 871, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 866, in main. header=header). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 596, in write_variants_to_vcf. for variant in variant_generator:. File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 798, in merge_variants_and_nonvariants. nonvariant.end, fasta_reader). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 712, in _create_record_from_template. ranges.make_range(retval.reference_name, start, start + 1)). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/ru",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/164
https://github.com/google/deepvariant/issues/164:114,availability,error,error,114,"Failed to retrieve block: error reading file for gvcf postprocess variants for WGS; Hi, I'm getting the following error when trying to postprocess the gvcf (generation of the normal vcf works fine). Any ideas why this is happening? It's happening across several different samples, but always at the same site. Could it be something missing in the reference fasta file? If so is there a setting to have it fail less spectacularly and skip over variants it for some reason cannot find? ```. I0320 10:18:29.648770 140566999074560 postprocess_variants.py:593] Writing output to VCF file: UFC100105-Normal-SM-CUCI1.gvcf. I0320 10:18:29.649548 140566999074560 genomics_writer.py:118] Writing UFC100105-Normal-SM-CUCI1.gvcf with NativeVcfWriter. [E::fai_retrieve] Failed to retrieve block: error reading file. Traceback (most recent call last):. File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 871, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 866, in main. header=header). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 596, in write_variants_to_vcf. for variant in variant_generator:. File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 798, in merge_variants_and_nonvariants. nonvariant.end, fasta_reader). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 712, in _create_record_from_template. ranges.make_range(retval.reference_name, start, start + 1)). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/ru",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/164
https://github.com/google/deepvariant/issues/164:783,availability,error,error,783,"Failed to retrieve block: error reading file for gvcf postprocess variants for WGS; Hi, I'm getting the following error when trying to postprocess the gvcf (generation of the normal vcf works fine). Any ideas why this is happening? It's happening across several different samples, but always at the same site. Could it be something missing in the reference fasta file? If so is there a setting to have it fail less spectacularly and skip over variants it for some reason cannot find? ```. I0320 10:18:29.648770 140566999074560 postprocess_variants.py:593] Writing output to VCF file: UFC100105-Normal-SM-CUCI1.gvcf. I0320 10:18:29.649548 140566999074560 genomics_writer.py:118] Writing UFC100105-Normal-SM-CUCI1.gvcf with NativeVcfWriter. [E::fai_retrieve] Failed to retrieve block: error reading file. Traceback (most recent call last):. File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 871, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 866, in main. header=header). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 596, in write_variants_to_vcf. for variant in variant_generator:. File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 798, in merge_variants_and_nonvariants. nonvariant.end, fasta_reader). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 712, in _create_record_from_template. ranges.make_range(retval.reference_name, start, start + 1)). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/ru",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/164
https://github.com/google/deepvariant/issues/164:2220,availability,echo,echo,2220,"cularly and skip over variants it for some reason cannot find? ```. I0320 10:18:29.648770 140566999074560 postprocess_variants.py:593] Writing output to VCF file: UFC100105-Normal-SM-CUCI1.gvcf. I0320 10:18:29.649548 140566999074560 genomics_writer.py:118] Writing UFC100105-Normal-SM-CUCI1.gvcf with NativeVcfWriter. [E::fai_retrieve] Failed to retrieve block: error reading file. Traceback (most recent call last):. File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 871, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 866, in main. header=header). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 596, in write_variants_to_vcf. for variant in variant_generator:. File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 798, in merge_variants_and_nonvariants. nonvariant.end, fasta_reader). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 712, in _create_record_from_template. ranges.make_range(retval.reference_name, start, start + 1)). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 90, in query. return self._reader.bases(region). ValueError: Invalid argument: Couldn't fetch bases for reference_name: ""1"" start: 10147 end: 10148. + echo 'grep -v -E '\''RefCall[[:space:]]'\'' '\''UFC100105-Normal-SM-CUCI1.vcf'\'' > '\''UFC100105-Normal-SM-CUCI1.filtered.vcf'\'''. + grep -v -E 'RefCall[[:space:]]' UFC100105-Normal-SM-CUCI1.vcf. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/164
https://github.com/google/deepvariant/issues/164:0,deployability,Fail,Failed,0,"Failed to retrieve block: error reading file for gvcf postprocess variants for WGS; Hi, I'm getting the following error when trying to postprocess the gvcf (generation of the normal vcf works fine). Any ideas why this is happening? It's happening across several different samples, but always at the same site. Could it be something missing in the reference fasta file? If so is there a setting to have it fail less spectacularly and skip over variants it for some reason cannot find? ```. I0320 10:18:29.648770 140566999074560 postprocess_variants.py:593] Writing output to VCF file: UFC100105-Normal-SM-CUCI1.gvcf. I0320 10:18:29.649548 140566999074560 genomics_writer.py:118] Writing UFC100105-Normal-SM-CUCI1.gvcf with NativeVcfWriter. [E::fai_retrieve] Failed to retrieve block: error reading file. Traceback (most recent call last):. File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 871, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 866, in main. header=header). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 596, in write_variants_to_vcf. for variant in variant_generator:. File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 798, in merge_variants_and_nonvariants. nonvariant.end, fasta_reader). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 712, in _create_record_from_template. ranges.make_range(retval.reference_name, start, start + 1)). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/ru",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/164
https://github.com/google/deepvariant/issues/164:405,deployability,fail,fail,405,"Failed to retrieve block: error reading file for gvcf postprocess variants for WGS; Hi, I'm getting the following error when trying to postprocess the gvcf (generation of the normal vcf works fine). Any ideas why this is happening? It's happening across several different samples, but always at the same site. Could it be something missing in the reference fasta file? If so is there a setting to have it fail less spectacularly and skip over variants it for some reason cannot find? ```. I0320 10:18:29.648770 140566999074560 postprocess_variants.py:593] Writing output to VCF file: UFC100105-Normal-SM-CUCI1.gvcf. I0320 10:18:29.649548 140566999074560 genomics_writer.py:118] Writing UFC100105-Normal-SM-CUCI1.gvcf with NativeVcfWriter. [E::fai_retrieve] Failed to retrieve block: error reading file. Traceback (most recent call last):. File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 871, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 866, in main. header=header). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 596, in write_variants_to_vcf. for variant in variant_generator:. File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 798, in merge_variants_and_nonvariants. nonvariant.end, fasta_reader). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 712, in _create_record_from_template. ranges.make_range(retval.reference_name, start, start + 1)). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/ru",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/164
https://github.com/google/deepvariant/issues/164:757,deployability,Fail,Failed,757,"Failed to retrieve block: error reading file for gvcf postprocess variants for WGS; Hi, I'm getting the following error when trying to postprocess the gvcf (generation of the normal vcf works fine). Any ideas why this is happening? It's happening across several different samples, but always at the same site. Could it be something missing in the reference fasta file? If so is there a setting to have it fail less spectacularly and skip over variants it for some reason cannot find? ```. I0320 10:18:29.648770 140566999074560 postprocess_variants.py:593] Writing output to VCF file: UFC100105-Normal-SM-CUCI1.gvcf. I0320 10:18:29.649548 140566999074560 genomics_writer.py:118] Writing UFC100105-Normal-SM-CUCI1.gvcf with NativeVcfWriter. [E::fai_retrieve] Failed to retrieve block: error reading file. Traceback (most recent call last):. File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 871, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 866, in main. header=header). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 596, in write_variants_to_vcf. for variant in variant_generator:. File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 798, in merge_variants_and_nonvariants. nonvariant.end, fasta_reader). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 712, in _create_record_from_template. ranges.make_range(retval.reference_name, start, start + 1)). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/ru",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/164
https://github.com/google/deepvariant/issues/164:979,deployability,modul,module,979,"Failed to retrieve block: error reading file for gvcf postprocess variants for WGS; Hi, I'm getting the following error when trying to postprocess the gvcf (generation of the normal vcf works fine). Any ideas why this is happening? It's happening across several different samples, but always at the same site. Could it be something missing in the reference fasta file? If so is there a setting to have it fail less spectacularly and skip over variants it for some reason cannot find? ```. I0320 10:18:29.648770 140566999074560 postprocess_variants.py:593] Writing output to VCF file: UFC100105-Normal-SM-CUCI1.gvcf. I0320 10:18:29.649548 140566999074560 genomics_writer.py:118] Writing UFC100105-Normal-SM-CUCI1.gvcf with NativeVcfWriter. [E::fai_retrieve] Failed to retrieve block: error reading file. Traceback (most recent call last):. File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 871, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 866, in main. header=header). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 596, in write_variants_to_vcf. for variant in variant_generator:. File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 798, in merge_variants_and_nonvariants. nonvariant.end, fasta_reader). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 712, in _create_record_from_template. ranges.make_range(retval.reference_name, start, start + 1)). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/ru",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/164
https://github.com/google/deepvariant/issues/164:2334,integrability,filter,filtered,2334,"cularly and skip over variants it for some reason cannot find? ```. I0320 10:18:29.648770 140566999074560 postprocess_variants.py:593] Writing output to VCF file: UFC100105-Normal-SM-CUCI1.gvcf. I0320 10:18:29.649548 140566999074560 genomics_writer.py:118] Writing UFC100105-Normal-SM-CUCI1.gvcf with NativeVcfWriter. [E::fai_retrieve] Failed to retrieve block: error reading file. Traceback (most recent call last):. File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 871, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 866, in main. header=header). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 596, in write_variants_to_vcf. for variant in variant_generator:. File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 798, in merge_variants_and_nonvariants. nonvariant.end, fasta_reader). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 712, in _create_record_from_template. ranges.make_range(retval.reference_name, start, start + 1)). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 90, in query. return self._reader.bases(region). ValueError: Invalid argument: Couldn't fetch bases for reference_name: ""1"" start: 10147 end: 10148. + echo 'grep -v -E '\''RefCall[[:space:]]'\'' '\''UFC100105-Normal-SM-CUCI1.vcf'\'' > '\''UFC100105-Normal-SM-CUCI1.filtered.vcf'\'''. + grep -v -E 'RefCall[[:space:]]' UFC100105-Normal-SM-CUCI1.vcf. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/164
https://github.com/google/deepvariant/issues/164:1065,interoperability,platform,platform,1065,"iants for WGS; Hi, I'm getting the following error when trying to postprocess the gvcf (generation of the normal vcf works fine). Any ideas why this is happening? It's happening across several different samples, but always at the same site. Could it be something missing in the reference fasta file? If so is there a setting to have it fail less spectacularly and skip over variants it for some reason cannot find? ```. I0320 10:18:29.648770 140566999074560 postprocess_variants.py:593] Writing output to VCF file: UFC100105-Normal-SM-CUCI1.gvcf. I0320 10:18:29.649548 140566999074560 genomics_writer.py:118] Writing UFC100105-Normal-SM-CUCI1.gvcf with NativeVcfWriter. [E::fai_retrieve] Failed to retrieve block: error reading file. Traceback (most recent call last):. File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 871, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 866, in main. header=header). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 596, in write_variants_to_vcf. for variant in variant_generator:. File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 798, in merge_variants_and_nonvariants. nonvariant.end, fasta_reader). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 712, in _create_record_from_template. ranges.make_range(retval.reference_name, start, start + 1)). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/164
https://github.com/google/deepvariant/issues/164:979,modifiability,modul,module,979,"Failed to retrieve block: error reading file for gvcf postprocess variants for WGS; Hi, I'm getting the following error when trying to postprocess the gvcf (generation of the normal vcf works fine). Any ideas why this is happening? It's happening across several different samples, but always at the same site. Could it be something missing in the reference fasta file? If so is there a setting to have it fail less spectacularly and skip over variants it for some reason cannot find? ```. I0320 10:18:29.648770 140566999074560 postprocess_variants.py:593] Writing output to VCF file: UFC100105-Normal-SM-CUCI1.gvcf. I0320 10:18:29.649548 140566999074560 genomics_writer.py:118] Writing UFC100105-Normal-SM-CUCI1.gvcf with NativeVcfWriter. [E::fai_retrieve] Failed to retrieve block: error reading file. Traceback (most recent call last):. File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 871, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 866, in main. header=header). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 596, in write_variants_to_vcf. for variant in variant_generator:. File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 798, in merge_variants_and_nonvariants. nonvariant.end, fasta_reader). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 712, in _create_record_from_template. ranges.make_range(retval.reference_name, start, start + 1)). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/ru",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/164
https://github.com/google/deepvariant/issues/164:1038,modifiability,pac,packages,1038,"le for gvcf postprocess variants for WGS; Hi, I'm getting the following error when trying to postprocess the gvcf (generation of the normal vcf works fine). Any ideas why this is happening? It's happening across several different samples, but always at the same site. Could it be something missing in the reference fasta file? If so is there a setting to have it fail less spectacularly and skip over variants it for some reason cannot find? ```. I0320 10:18:29.648770 140566999074560 postprocess_variants.py:593] Writing output to VCF file: UFC100105-Normal-SM-CUCI1.gvcf. I0320 10:18:29.649548 140566999074560 genomics_writer.py:118] Writing UFC100105-Normal-SM-CUCI1.gvcf with NativeVcfWriter. [E::fai_retrieve] Failed to retrieve block: error reading file. Traceback (most recent call last):. File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 871, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 866, in main. header=header). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 596, in write_variants_to_vcf. for variant in variant_generator:. File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 798, in merge_variants_and_nonvariants. nonvariant.end, fasta_reader). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 712, in _create_record_from_template. ranges.make_range(retval.reference_name, start, start + 1)). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/third_party/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/164
https://github.com/google/deepvariant/issues/164:26,performance,error,error,26,"Failed to retrieve block: error reading file for gvcf postprocess variants for WGS; Hi, I'm getting the following error when trying to postprocess the gvcf (generation of the normal vcf works fine). Any ideas why this is happening? It's happening across several different samples, but always at the same site. Could it be something missing in the reference fasta file? If so is there a setting to have it fail less spectacularly and skip over variants it for some reason cannot find? ```. I0320 10:18:29.648770 140566999074560 postprocess_variants.py:593] Writing output to VCF file: UFC100105-Normal-SM-CUCI1.gvcf. I0320 10:18:29.649548 140566999074560 genomics_writer.py:118] Writing UFC100105-Normal-SM-CUCI1.gvcf with NativeVcfWriter. [E::fai_retrieve] Failed to retrieve block: error reading file. Traceback (most recent call last):. File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 871, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 866, in main. header=header). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 596, in write_variants_to_vcf. for variant in variant_generator:. File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 798, in merge_variants_and_nonvariants. nonvariant.end, fasta_reader). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 712, in _create_record_from_template. ranges.make_range(retval.reference_name, start, start + 1)). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/ru",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/164
https://github.com/google/deepvariant/issues/164:114,performance,error,error,114,"Failed to retrieve block: error reading file for gvcf postprocess variants for WGS; Hi, I'm getting the following error when trying to postprocess the gvcf (generation of the normal vcf works fine). Any ideas why this is happening? It's happening across several different samples, but always at the same site. Could it be something missing in the reference fasta file? If so is there a setting to have it fail less spectacularly and skip over variants it for some reason cannot find? ```. I0320 10:18:29.648770 140566999074560 postprocess_variants.py:593] Writing output to VCF file: UFC100105-Normal-SM-CUCI1.gvcf. I0320 10:18:29.649548 140566999074560 genomics_writer.py:118] Writing UFC100105-Normal-SM-CUCI1.gvcf with NativeVcfWriter. [E::fai_retrieve] Failed to retrieve block: error reading file. Traceback (most recent call last):. File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 871, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 866, in main. header=header). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 596, in write_variants_to_vcf. for variant in variant_generator:. File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 798, in merge_variants_and_nonvariants. nonvariant.end, fasta_reader). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 712, in _create_record_from_template. ranges.make_range(retval.reference_name, start, start + 1)). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/ru",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/164
https://github.com/google/deepvariant/issues/164:783,performance,error,error,783,"Failed to retrieve block: error reading file for gvcf postprocess variants for WGS; Hi, I'm getting the following error when trying to postprocess the gvcf (generation of the normal vcf works fine). Any ideas why this is happening? It's happening across several different samples, but always at the same site. Could it be something missing in the reference fasta file? If so is there a setting to have it fail less spectacularly and skip over variants it for some reason cannot find? ```. I0320 10:18:29.648770 140566999074560 postprocess_variants.py:593] Writing output to VCF file: UFC100105-Normal-SM-CUCI1.gvcf. I0320 10:18:29.649548 140566999074560 genomics_writer.py:118] Writing UFC100105-Normal-SM-CUCI1.gvcf with NativeVcfWriter. [E::fai_retrieve] Failed to retrieve block: error reading file. Traceback (most recent call last):. File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 871, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 866, in main. header=header). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 596, in write_variants_to_vcf. for variant in variant_generator:. File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 798, in merge_variants_and_nonvariants. nonvariant.end, fasta_reader). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 712, in _create_record_from_template. ranges.make_range(retval.reference_name, start, start + 1)). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/ru",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/164
https://github.com/google/deepvariant/issues/164:0,reliability,Fail,Failed,0,"Failed to retrieve block: error reading file for gvcf postprocess variants for WGS; Hi, I'm getting the following error when trying to postprocess the gvcf (generation of the normal vcf works fine). Any ideas why this is happening? It's happening across several different samples, but always at the same site. Could it be something missing in the reference fasta file? If so is there a setting to have it fail less spectacularly and skip over variants it for some reason cannot find? ```. I0320 10:18:29.648770 140566999074560 postprocess_variants.py:593] Writing output to VCF file: UFC100105-Normal-SM-CUCI1.gvcf. I0320 10:18:29.649548 140566999074560 genomics_writer.py:118] Writing UFC100105-Normal-SM-CUCI1.gvcf with NativeVcfWriter. [E::fai_retrieve] Failed to retrieve block: error reading file. Traceback (most recent call last):. File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 871, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 866, in main. header=header). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 596, in write_variants_to_vcf. for variant in variant_generator:. File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 798, in merge_variants_and_nonvariants. nonvariant.end, fasta_reader). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 712, in _create_record_from_template. ranges.make_range(retval.reference_name, start, start + 1)). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/ru",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/164
https://github.com/google/deepvariant/issues/164:405,reliability,fail,fail,405,"Failed to retrieve block: error reading file for gvcf postprocess variants for WGS; Hi, I'm getting the following error when trying to postprocess the gvcf (generation of the normal vcf works fine). Any ideas why this is happening? It's happening across several different samples, but always at the same site. Could it be something missing in the reference fasta file? If so is there a setting to have it fail less spectacularly and skip over variants it for some reason cannot find? ```. I0320 10:18:29.648770 140566999074560 postprocess_variants.py:593] Writing output to VCF file: UFC100105-Normal-SM-CUCI1.gvcf. I0320 10:18:29.649548 140566999074560 genomics_writer.py:118] Writing UFC100105-Normal-SM-CUCI1.gvcf with NativeVcfWriter. [E::fai_retrieve] Failed to retrieve block: error reading file. Traceback (most recent call last):. File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 871, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 866, in main. header=header). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 596, in write_variants_to_vcf. for variant in variant_generator:. File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 798, in merge_variants_and_nonvariants. nonvariant.end, fasta_reader). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 712, in _create_record_from_template. ranges.make_range(retval.reference_name, start, start + 1)). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/ru",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/164
https://github.com/google/deepvariant/issues/164:757,reliability,Fail,Failed,757,"Failed to retrieve block: error reading file for gvcf postprocess variants for WGS; Hi, I'm getting the following error when trying to postprocess the gvcf (generation of the normal vcf works fine). Any ideas why this is happening? It's happening across several different samples, but always at the same site. Could it be something missing in the reference fasta file? If so is there a setting to have it fail less spectacularly and skip over variants it for some reason cannot find? ```. I0320 10:18:29.648770 140566999074560 postprocess_variants.py:593] Writing output to VCF file: UFC100105-Normal-SM-CUCI1.gvcf. I0320 10:18:29.649548 140566999074560 genomics_writer.py:118] Writing UFC100105-Normal-SM-CUCI1.gvcf with NativeVcfWriter. [E::fai_retrieve] Failed to retrieve block: error reading file. Traceback (most recent call last):. File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 871, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 866, in main. header=header). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 596, in write_variants_to_vcf. for variant in variant_generator:. File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 798, in merge_variants_and_nonvariants. nonvariant.end, fasta_reader). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 712, in _create_record_from_template. ranges.make_range(retval.reference_name, start, start + 1)). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/ru",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/164
https://github.com/google/deepvariant/issues/164:26,safety,error,error,26,"Failed to retrieve block: error reading file for gvcf postprocess variants for WGS; Hi, I'm getting the following error when trying to postprocess the gvcf (generation of the normal vcf works fine). Any ideas why this is happening? It's happening across several different samples, but always at the same site. Could it be something missing in the reference fasta file? If so is there a setting to have it fail less spectacularly and skip over variants it for some reason cannot find? ```. I0320 10:18:29.648770 140566999074560 postprocess_variants.py:593] Writing output to VCF file: UFC100105-Normal-SM-CUCI1.gvcf. I0320 10:18:29.649548 140566999074560 genomics_writer.py:118] Writing UFC100105-Normal-SM-CUCI1.gvcf with NativeVcfWriter. [E::fai_retrieve] Failed to retrieve block: error reading file. Traceback (most recent call last):. File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 871, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 866, in main. header=header). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 596, in write_variants_to_vcf. for variant in variant_generator:. File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 798, in merge_variants_and_nonvariants. nonvariant.end, fasta_reader). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 712, in _create_record_from_template. ranges.make_range(retval.reference_name, start, start + 1)). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/ru",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/164
https://github.com/google/deepvariant/issues/164:114,safety,error,error,114,"Failed to retrieve block: error reading file for gvcf postprocess variants for WGS; Hi, I'm getting the following error when trying to postprocess the gvcf (generation of the normal vcf works fine). Any ideas why this is happening? It's happening across several different samples, but always at the same site. Could it be something missing in the reference fasta file? If so is there a setting to have it fail less spectacularly and skip over variants it for some reason cannot find? ```. I0320 10:18:29.648770 140566999074560 postprocess_variants.py:593] Writing output to VCF file: UFC100105-Normal-SM-CUCI1.gvcf. I0320 10:18:29.649548 140566999074560 genomics_writer.py:118] Writing UFC100105-Normal-SM-CUCI1.gvcf with NativeVcfWriter. [E::fai_retrieve] Failed to retrieve block: error reading file. Traceback (most recent call last):. File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 871, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 866, in main. header=header). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 596, in write_variants_to_vcf. for variant in variant_generator:. File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 798, in merge_variants_and_nonvariants. nonvariant.end, fasta_reader). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 712, in _create_record_from_template. ranges.make_range(retval.reference_name, start, start + 1)). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/ru",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/164
https://github.com/google/deepvariant/issues/164:783,safety,error,error,783,"Failed to retrieve block: error reading file for gvcf postprocess variants for WGS; Hi, I'm getting the following error when trying to postprocess the gvcf (generation of the normal vcf works fine). Any ideas why this is happening? It's happening across several different samples, but always at the same site. Could it be something missing in the reference fasta file? If so is there a setting to have it fail less spectacularly and skip over variants it for some reason cannot find? ```. I0320 10:18:29.648770 140566999074560 postprocess_variants.py:593] Writing output to VCF file: UFC100105-Normal-SM-CUCI1.gvcf. I0320 10:18:29.649548 140566999074560 genomics_writer.py:118] Writing UFC100105-Normal-SM-CUCI1.gvcf with NativeVcfWriter. [E::fai_retrieve] Failed to retrieve block: error reading file. Traceback (most recent call last):. File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 871, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 866, in main. header=header). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 596, in write_variants_to_vcf. for variant in variant_generator:. File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 798, in merge_variants_and_nonvariants. nonvariant.end, fasta_reader). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 712, in _create_record_from_template. ranges.make_range(retval.reference_name, start, start + 1)). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/ru",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/164
https://github.com/google/deepvariant/issues/164:979,safety,modul,module,979,"Failed to retrieve block: error reading file for gvcf postprocess variants for WGS; Hi, I'm getting the following error when trying to postprocess the gvcf (generation of the normal vcf works fine). Any ideas why this is happening? It's happening across several different samples, but always at the same site. Could it be something missing in the reference fasta file? If so is there a setting to have it fail less spectacularly and skip over variants it for some reason cannot find? ```. I0320 10:18:29.648770 140566999074560 postprocess_variants.py:593] Writing output to VCF file: UFC100105-Normal-SM-CUCI1.gvcf. I0320 10:18:29.649548 140566999074560 genomics_writer.py:118] Writing UFC100105-Normal-SM-CUCI1.gvcf with NativeVcfWriter. [E::fai_retrieve] Failed to retrieve block: error reading file. Traceback (most recent call last):. File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 871, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 866, in main. header=header). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 596, in write_variants_to_vcf. for variant in variant_generator:. File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 798, in merge_variants_and_nonvariants. nonvariant.end, fasta_reader). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 712, in _create_record_from_template. ranges.make_range(retval.reference_name, start, start + 1)). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/ru",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/164
https://github.com/google/deepvariant/issues/164:803,testability,Trace,Traceback,803,"Failed to retrieve block: error reading file for gvcf postprocess variants for WGS; Hi, I'm getting the following error when trying to postprocess the gvcf (generation of the normal vcf works fine). Any ideas why this is happening? It's happening across several different samples, but always at the same site. Could it be something missing in the reference fasta file? If so is there a setting to have it fail less spectacularly and skip over variants it for some reason cannot find? ```. I0320 10:18:29.648770 140566999074560 postprocess_variants.py:593] Writing output to VCF file: UFC100105-Normal-SM-CUCI1.gvcf. I0320 10:18:29.649548 140566999074560 genomics_writer.py:118] Writing UFC100105-Normal-SM-CUCI1.gvcf with NativeVcfWriter. [E::fai_retrieve] Failed to retrieve block: error reading file. Traceback (most recent call last):. File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 871, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 866, in main. header=header). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 596, in write_variants_to_vcf. for variant in variant_generator:. File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 798, in merge_variants_and_nonvariants. nonvariant.end, fasta_reader). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 712, in _create_record_from_template. ranges.make_range(retval.reference_name, start, start + 1)). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/ru",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/164
https://github.com/google/deepvariant/issues/164:26,usability,error,error,26,"Failed to retrieve block: error reading file for gvcf postprocess variants for WGS; Hi, I'm getting the following error when trying to postprocess the gvcf (generation of the normal vcf works fine). Any ideas why this is happening? It's happening across several different samples, but always at the same site. Could it be something missing in the reference fasta file? If so is there a setting to have it fail less spectacularly and skip over variants it for some reason cannot find? ```. I0320 10:18:29.648770 140566999074560 postprocess_variants.py:593] Writing output to VCF file: UFC100105-Normal-SM-CUCI1.gvcf. I0320 10:18:29.649548 140566999074560 genomics_writer.py:118] Writing UFC100105-Normal-SM-CUCI1.gvcf with NativeVcfWriter. [E::fai_retrieve] Failed to retrieve block: error reading file. Traceback (most recent call last):. File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 871, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 866, in main. header=header). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 596, in write_variants_to_vcf. for variant in variant_generator:. File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 798, in merge_variants_and_nonvariants. nonvariant.end, fasta_reader). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 712, in _create_record_from_template. ranges.make_range(retval.reference_name, start, start + 1)). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/ru",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/164
https://github.com/google/deepvariant/issues/164:114,usability,error,error,114,"Failed to retrieve block: error reading file for gvcf postprocess variants for WGS; Hi, I'm getting the following error when trying to postprocess the gvcf (generation of the normal vcf works fine). Any ideas why this is happening? It's happening across several different samples, but always at the same site. Could it be something missing in the reference fasta file? If so is there a setting to have it fail less spectacularly and skip over variants it for some reason cannot find? ```. I0320 10:18:29.648770 140566999074560 postprocess_variants.py:593] Writing output to VCF file: UFC100105-Normal-SM-CUCI1.gvcf. I0320 10:18:29.649548 140566999074560 genomics_writer.py:118] Writing UFC100105-Normal-SM-CUCI1.gvcf with NativeVcfWriter. [E::fai_retrieve] Failed to retrieve block: error reading file. Traceback (most recent call last):. File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 871, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 866, in main. header=header). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 596, in write_variants_to_vcf. for variant in variant_generator:. File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 798, in merge_variants_and_nonvariants. nonvariant.end, fasta_reader). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 712, in _create_record_from_template. ranges.make_range(retval.reference_name, start, start + 1)). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/ru",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/164
https://github.com/google/deepvariant/issues/164:783,usability,error,error,783,"Failed to retrieve block: error reading file for gvcf postprocess variants for WGS; Hi, I'm getting the following error when trying to postprocess the gvcf (generation of the normal vcf works fine). Any ideas why this is happening? It's happening across several different samples, but always at the same site. Could it be something missing in the reference fasta file? If so is there a setting to have it fail less spectacularly and skip over variants it for some reason cannot find? ```. I0320 10:18:29.648770 140566999074560 postprocess_variants.py:593] Writing output to VCF file: UFC100105-Normal-SM-CUCI1.gvcf. I0320 10:18:29.649548 140566999074560 genomics_writer.py:118] Writing UFC100105-Normal-SM-CUCI1.gvcf with NativeVcfWriter. [E::fai_retrieve] Failed to retrieve block: error reading file. Traceback (most recent call last):. File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 871, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run. _sys.exit(main(_sys.argv[:1] + flags_passthrough)). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 866, in main. header=header). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 596, in write_variants_to_vcf. for variant in variant_generator:. File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 798, in merge_variants_and_nonvariants. nonvariant.end, fasta_reader). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 712, in _create_record_from_template. ranges.make_range(retval.reference_name, start, start + 1)). File ""/cromwell_root/tmp.1b831160/Bazel.runfiles_bAePPc/ru",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/164
https://github.com/google/deepvariant/issues/165:1385,deployability,pipelin,pipelines,1385," the term ""DeepVariant"" or ""Google"" any where. I apologize for missing something that a lot of people might know, but would you mind explaining how the results in the [PrecisionFDA results table](https://precision.fda.gov/challenges/truth/results) match DeepVariant? **2)** While it has been hard for me explain myself precisely, I have been concerned that there was somehow over-fitting for some datasets reporting extremely high accuracy. In the PrecisionFDA challenge, I think it should probably be mentioned that there are multiple programs with high percentages, including different workflows that actually use the same variant caller (like GATK) and no strategy was ""best"" for all the criteria defined. **3)** While I don't know the precise way in which you could have a lack of independence for training versus test datasets, I think there are other benchmarks that better match what I would expect for more typical results. For example, [in this paper](https://www.nature.com/articles/s41587-019-0054-x), it says ""_In the high-confidence regions, when comparing these pipelines to each other (https://precision.fda.gov/jobs/job-FJpqBP80F3YyfJG02bQzPJBj, link immediately accessible by requesting an account), they agreed on 99.7% of SNVs and 98.7% of indels. Outside the high-confidence regions (https://precision.fda.gov/jobs/job-FJpqJF80F3YyXqz6Kv8Q1BQK), they agreed with each other on only 76.5% of SNVs and 78.7% of indels_."". **4)** You mention ""_No filtering is needed beyond setting your preferred minimum quality threshold_."". While I don't currently have much first-hand experience (although that is on my long-term ""to-do"" list), I didn't think this was true. For example, unless I am missing something, this table reports a very high ""Failed Filters"" count for DeepVariant versus GATK:. https://www.nature.com/articles/s41598-018-36177-7/tables/1. Do you have other metrics with comparisons on completely independent samples? Thank you very much for your help! Sincerely,. Charles",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/165
https://github.com/google/deepvariant/issues/165:2064,deployability,Fail,Failed,2064," the term ""DeepVariant"" or ""Google"" any where. I apologize for missing something that a lot of people might know, but would you mind explaining how the results in the [PrecisionFDA results table](https://precision.fda.gov/challenges/truth/results) match DeepVariant? **2)** While it has been hard for me explain myself precisely, I have been concerned that there was somehow over-fitting for some datasets reporting extremely high accuracy. In the PrecisionFDA challenge, I think it should probably be mentioned that there are multiple programs with high percentages, including different workflows that actually use the same variant caller (like GATK) and no strategy was ""best"" for all the criteria defined. **3)** While I don't know the precise way in which you could have a lack of independence for training versus test datasets, I think there are other benchmarks that better match what I would expect for more typical results. For example, [in this paper](https://www.nature.com/articles/s41587-019-0054-x), it says ""_In the high-confidence regions, when comparing these pipelines to each other (https://precision.fda.gov/jobs/job-FJpqBP80F3YyfJG02bQzPJBj, link immediately accessible by requesting an account), they agreed on 99.7% of SNVs and 98.7% of indels. Outside the high-confidence regions (https://precision.fda.gov/jobs/job-FJpqJF80F3YyXqz6Kv8Q1BQK), they agreed with each other on only 76.5% of SNVs and 78.7% of indels_."". **4)** You mention ""_No filtering is needed beyond setting your preferred minimum quality threshold_."". While I don't currently have much first-hand experience (although that is on my long-term ""to-do"" list), I didn't think this was true. For example, unless I am missing something, this table reports a very high ""Failed Filters"" count for DeepVariant versus GATK:. https://www.nature.com/articles/s41598-018-36177-7/tables/1. Do you have other metrics with comparisons on completely independent samples? Thank you very much for your help! Sincerely,. Charles",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/165
https://github.com/google/deepvariant/issues/165:1867,energy efficiency,current,currently,1867," the term ""DeepVariant"" or ""Google"" any where. I apologize for missing something that a lot of people might know, but would you mind explaining how the results in the [PrecisionFDA results table](https://precision.fda.gov/challenges/truth/results) match DeepVariant? **2)** While it has been hard for me explain myself precisely, I have been concerned that there was somehow over-fitting for some datasets reporting extremely high accuracy. In the PrecisionFDA challenge, I think it should probably be mentioned that there are multiple programs with high percentages, including different workflows that actually use the same variant caller (like GATK) and no strategy was ""best"" for all the criteria defined. **3)** While I don't know the precise way in which you could have a lack of independence for training versus test datasets, I think there are other benchmarks that better match what I would expect for more typical results. For example, [in this paper](https://www.nature.com/articles/s41587-019-0054-x), it says ""_In the high-confidence regions, when comparing these pipelines to each other (https://precision.fda.gov/jobs/job-FJpqBP80F3YyfJG02bQzPJBj, link immediately accessible by requesting an account), they agreed on 99.7% of SNVs and 98.7% of indels. Outside the high-confidence regions (https://precision.fda.gov/jobs/job-FJpqJF80F3YyXqz6Kv8Q1BQK), they agreed with each other on only 76.5% of SNVs and 78.7% of indels_."". **4)** You mention ""_No filtering is needed beyond setting your preferred minimum quality threshold_."". While I don't currently have much first-hand experience (although that is on my long-term ""to-do"" list), I didn't think this was true. For example, unless I am missing something, this table reports a very high ""Failed Filters"" count for DeepVariant versus GATK:. https://www.nature.com/articles/s41598-018-36177-7/tables/1. Do you have other metrics with comparisons on completely independent samples? Thank you very much for your help! Sincerely,. Charles",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/165
https://github.com/google/deepvariant/issues/165:1385,integrability,pipelin,pipelines,1385," the term ""DeepVariant"" or ""Google"" any where. I apologize for missing something that a lot of people might know, but would you mind explaining how the results in the [PrecisionFDA results table](https://precision.fda.gov/challenges/truth/results) match DeepVariant? **2)** While it has been hard for me explain myself precisely, I have been concerned that there was somehow over-fitting for some datasets reporting extremely high accuracy. In the PrecisionFDA challenge, I think it should probably be mentioned that there are multiple programs with high percentages, including different workflows that actually use the same variant caller (like GATK) and no strategy was ""best"" for all the criteria defined. **3)** While I don't know the precise way in which you could have a lack of independence for training versus test datasets, I think there are other benchmarks that better match what I would expect for more typical results. For example, [in this paper](https://www.nature.com/articles/s41587-019-0054-x), it says ""_In the high-confidence regions, when comparing these pipelines to each other (https://precision.fda.gov/jobs/job-FJpqBP80F3YyfJG02bQzPJBj, link immediately accessible by requesting an account), they agreed on 99.7% of SNVs and 98.7% of indels. Outside the high-confidence regions (https://precision.fda.gov/jobs/job-FJpqJF80F3YyXqz6Kv8Q1BQK), they agreed with each other on only 76.5% of SNVs and 78.7% of indels_."". **4)** You mention ""_No filtering is needed beyond setting your preferred minimum quality threshold_."". While I don't currently have much first-hand experience (although that is on my long-term ""to-do"" list), I didn't think this was true. For example, unless I am missing something, this table reports a very high ""Failed Filters"" count for DeepVariant versus GATK:. https://www.nature.com/articles/s41598-018-36177-7/tables/1. Do you have other metrics with comparisons on completely independent samples? Thank you very much for your help! Sincerely,. Charles",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/165
https://github.com/google/deepvariant/issues/165:1773,integrability,filter,filtering,1773," the term ""DeepVariant"" or ""Google"" any where. I apologize for missing something that a lot of people might know, but would you mind explaining how the results in the [PrecisionFDA results table](https://precision.fda.gov/challenges/truth/results) match DeepVariant? **2)** While it has been hard for me explain myself precisely, I have been concerned that there was somehow over-fitting for some datasets reporting extremely high accuracy. In the PrecisionFDA challenge, I think it should probably be mentioned that there are multiple programs with high percentages, including different workflows that actually use the same variant caller (like GATK) and no strategy was ""best"" for all the criteria defined. **3)** While I don't know the precise way in which you could have a lack of independence for training versus test datasets, I think there are other benchmarks that better match what I would expect for more typical results. For example, [in this paper](https://www.nature.com/articles/s41587-019-0054-x), it says ""_In the high-confidence regions, when comparing these pipelines to each other (https://precision.fda.gov/jobs/job-FJpqBP80F3YyfJG02bQzPJBj, link immediately accessible by requesting an account), they agreed on 99.7% of SNVs and 98.7% of indels. Outside the high-confidence regions (https://precision.fda.gov/jobs/job-FJpqJF80F3YyXqz6Kv8Q1BQK), they agreed with each other on only 76.5% of SNVs and 78.7% of indels_."". **4)** You mention ""_No filtering is needed beyond setting your preferred minimum quality threshold_."". While I don't currently have much first-hand experience (although that is on my long-term ""to-do"" list), I didn't think this was true. For example, unless I am missing something, this table reports a very high ""Failed Filters"" count for DeepVariant versus GATK:. https://www.nature.com/articles/s41598-018-36177-7/tables/1. Do you have other metrics with comparisons on completely independent samples? Thank you very much for your help! Sincerely,. Charles",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/165
https://github.com/google/deepvariant/issues/165:2071,integrability,Filter,Filters,2071," the term ""DeepVariant"" or ""Google"" any where. I apologize for missing something that a lot of people might know, but would you mind explaining how the results in the [PrecisionFDA results table](https://precision.fda.gov/challenges/truth/results) match DeepVariant? **2)** While it has been hard for me explain myself precisely, I have been concerned that there was somehow over-fitting for some datasets reporting extremely high accuracy. In the PrecisionFDA challenge, I think it should probably be mentioned that there are multiple programs with high percentages, including different workflows that actually use the same variant caller (like GATK) and no strategy was ""best"" for all the criteria defined. **3)** While I don't know the precise way in which you could have a lack of independence for training versus test datasets, I think there are other benchmarks that better match what I would expect for more typical results. For example, [in this paper](https://www.nature.com/articles/s41587-019-0054-x), it says ""_In the high-confidence regions, when comparing these pipelines to each other (https://precision.fda.gov/jobs/job-FJpqBP80F3YyfJG02bQzPJBj, link immediately accessible by requesting an account), they agreed on 99.7% of SNVs and 98.7% of indels. Outside the high-confidence regions (https://precision.fda.gov/jobs/job-FJpqJF80F3YyXqz6Kv8Q1BQK), they agreed with each other on only 76.5% of SNVs and 78.7% of indels_."". **4)** You mention ""_No filtering is needed beyond setting your preferred minimum quality threshold_."". While I don't currently have much first-hand experience (although that is on my long-term ""to-do"" list), I didn't think this was true. For example, unless I am missing something, this table reports a very high ""Failed Filters"" count for DeepVariant versus GATK:. https://www.nature.com/articles/s41598-018-36177-7/tables/1. Do you have other metrics with comparisons on completely independent samples? Thank you very much for your help! Sincerely,. Charles",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/165
https://github.com/google/deepvariant/issues/165:222,modifiability,concern,concerns,222,"High-Level Questions about ""Why DeepVariant"" Section; Hi,. I genuinely appreciate that you are providing a lot of detailed information for your free program. However, as I understand it, I have some high-level questions / concerns:. **1)** You mention scoring high on the PrecisionFDA calling, but I don't see the term ""DeepVariant"" or ""Google"" any where. I apologize for missing something that a lot of people might know, but would you mind explaining how the results in the [PrecisionFDA results table](https://precision.fda.gov/challenges/truth/results) match DeepVariant? **2)** While it has been hard for me explain myself precisely, I have been concerned that there was somehow over-fitting for some datasets reporting extremely high accuracy. In the PrecisionFDA challenge, I think it should probably be mentioned that there are multiple programs with high percentages, including different workflows that actually use the same variant caller (like GATK) and no strategy was ""best"" for all the criteria defined. **3)** While I don't know the precise way in which you could have a lack of independence for training versus test datasets, I think there are other benchmarks that better match what I would expect for more typical results. For example, [in this paper](https://www.nature.com/articles/s41587-019-0054-x), it says ""_In the high-confidence regions, when comparing these pipelines to each other (https://precision.fda.gov/jobs/job-FJpqBP80F3YyfJG02bQzPJBj, link immediately accessible by requesting an account), they agreed on 99.7% of SNVs and 98.7% of indels. Outside the high-confidence regions (https://precision.fda.gov/jobs/job-FJpqJF80F3YyXqz6Kv8Q1BQK), they agreed with each other on only 76.5% of SNVs and 78.7% of indels_."". **4)** You mention ""_No filtering is needed beyond setting your preferred minimum quality threshold_."". While I don't currently have much first-hand experience (although that is on my long-term ""to-do"" list), I didn't think this was true. For example,",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/165
https://github.com/google/deepvariant/issues/165:651,modifiability,concern,concerned,651,"High-Level Questions about ""Why DeepVariant"" Section; Hi,. I genuinely appreciate that you are providing a lot of detailed information for your free program. However, as I understand it, I have some high-level questions / concerns:. **1)** You mention scoring high on the PrecisionFDA calling, but I don't see the term ""DeepVariant"" or ""Google"" any where. I apologize for missing something that a lot of people might know, but would you mind explaining how the results in the [PrecisionFDA results table](https://precision.fda.gov/challenges/truth/results) match DeepVariant? **2)** While it has been hard for me explain myself precisely, I have been concerned that there was somehow over-fitting for some datasets reporting extremely high accuracy. In the PrecisionFDA challenge, I think it should probably be mentioned that there are multiple programs with high percentages, including different workflows that actually use the same variant caller (like GATK) and no strategy was ""best"" for all the criteria defined. **3)** While I don't know the precise way in which you could have a lack of independence for training versus test datasets, I think there are other benchmarks that better match what I would expect for more typical results. For example, [in this paper](https://www.nature.com/articles/s41587-019-0054-x), it says ""_In the high-confidence regions, when comparing these pipelines to each other (https://precision.fda.gov/jobs/job-FJpqBP80F3YyfJG02bQzPJBj, link immediately accessible by requesting an account), they agreed on 99.7% of SNVs and 98.7% of indels. Outside the high-confidence regions (https://precision.fda.gov/jobs/job-FJpqJF80F3YyXqz6Kv8Q1BQK), they agreed with each other on only 76.5% of SNVs and 78.7% of indels_."". **4)** You mention ""_No filtering is needed beyond setting your preferred minimum quality threshold_."". While I don't currently have much first-hand experience (although that is on my long-term ""to-do"" list), I didn't think this was true. For example,",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/165
https://github.com/google/deepvariant/issues/165:2064,reliability,Fail,Failed,2064," the term ""DeepVariant"" or ""Google"" any where. I apologize for missing something that a lot of people might know, but would you mind explaining how the results in the [PrecisionFDA results table](https://precision.fda.gov/challenges/truth/results) match DeepVariant? **2)** While it has been hard for me explain myself precisely, I have been concerned that there was somehow over-fitting for some datasets reporting extremely high accuracy. In the PrecisionFDA challenge, I think it should probably be mentioned that there are multiple programs with high percentages, including different workflows that actually use the same variant caller (like GATK) and no strategy was ""best"" for all the criteria defined. **3)** While I don't know the precise way in which you could have a lack of independence for training versus test datasets, I think there are other benchmarks that better match what I would expect for more typical results. For example, [in this paper](https://www.nature.com/articles/s41587-019-0054-x), it says ""_In the high-confidence regions, when comparing these pipelines to each other (https://precision.fda.gov/jobs/job-FJpqBP80F3YyfJG02bQzPJBj, link immediately accessible by requesting an account), they agreed on 99.7% of SNVs and 98.7% of indels. Outside the high-confidence regions (https://precision.fda.gov/jobs/job-FJpqJF80F3YyXqz6Kv8Q1BQK), they agreed with each other on only 76.5% of SNVs and 78.7% of indels_."". **4)** You mention ""_No filtering is needed beyond setting your preferred minimum quality threshold_."". While I don't currently have much first-hand experience (although that is on my long-term ""to-do"" list), I didn't think this was true. For example, unless I am missing something, this table reports a very high ""Failed Filters"" count for DeepVariant versus GATK:. https://www.nature.com/articles/s41598-018-36177-7/tables/1. Do you have other metrics with comparisons on completely independent samples? Thank you very much for your help! Sincerely,. Charles",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/165
https://github.com/google/deepvariant/issues/165:1127,safety,test,test,1127,"ation for your free program. However, as I understand it, I have some high-level questions / concerns:. **1)** You mention scoring high on the PrecisionFDA calling, but I don't see the term ""DeepVariant"" or ""Google"" any where. I apologize for missing something that a lot of people might know, but would you mind explaining how the results in the [PrecisionFDA results table](https://precision.fda.gov/challenges/truth/results) match DeepVariant? **2)** While it has been hard for me explain myself precisely, I have been concerned that there was somehow over-fitting for some datasets reporting extremely high accuracy. In the PrecisionFDA challenge, I think it should probably be mentioned that there are multiple programs with high percentages, including different workflows that actually use the same variant caller (like GATK) and no strategy was ""best"" for all the criteria defined. **3)** While I don't know the precise way in which you could have a lack of independence for training versus test datasets, I think there are other benchmarks that better match what I would expect for more typical results. For example, [in this paper](https://www.nature.com/articles/s41587-019-0054-x), it says ""_In the high-confidence regions, when comparing these pipelines to each other (https://precision.fda.gov/jobs/job-FJpqBP80F3YyfJG02bQzPJBj, link immediately accessible by requesting an account), they agreed on 99.7% of SNVs and 98.7% of indels. Outside the high-confidence regions (https://precision.fda.gov/jobs/job-FJpqJF80F3YyXqz6Kv8Q1BQK), they agreed with each other on only 76.5% of SNVs and 78.7% of indels_."". **4)** You mention ""_No filtering is needed beyond setting your preferred minimum quality threshold_."". While I don't currently have much first-hand experience (although that is on my long-term ""to-do"" list), I didn't think this was true. For example, unless I am missing something, this table reports a very high ""Failed Filters"" count for DeepVariant versus GATK:. https://www.n",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/165
https://github.com/google/deepvariant/issues/165:2223,safety,compl,completely,2223," the term ""DeepVariant"" or ""Google"" any where. I apologize for missing something that a lot of people might know, but would you mind explaining how the results in the [PrecisionFDA results table](https://precision.fda.gov/challenges/truth/results) match DeepVariant? **2)** While it has been hard for me explain myself precisely, I have been concerned that there was somehow over-fitting for some datasets reporting extremely high accuracy. In the PrecisionFDA challenge, I think it should probably be mentioned that there are multiple programs with high percentages, including different workflows that actually use the same variant caller (like GATK) and no strategy was ""best"" for all the criteria defined. **3)** While I don't know the precise way in which you could have a lack of independence for training versus test datasets, I think there are other benchmarks that better match what I would expect for more typical results. For example, [in this paper](https://www.nature.com/articles/s41587-019-0054-x), it says ""_In the high-confidence regions, when comparing these pipelines to each other (https://precision.fda.gov/jobs/job-FJpqBP80F3YyfJG02bQzPJBj, link immediately accessible by requesting an account), they agreed on 99.7% of SNVs and 98.7% of indels. Outside the high-confidence regions (https://precision.fda.gov/jobs/job-FJpqJF80F3YyXqz6Kv8Q1BQK), they agreed with each other on only 76.5% of SNVs and 78.7% of indels_."". **4)** You mention ""_No filtering is needed beyond setting your preferred minimum quality threshold_."". While I don't currently have much first-hand experience (although that is on my long-term ""to-do"" list), I didn't think this was true. For example, unless I am missing something, this table reports a very high ""Failed Filters"" count for DeepVariant versus GATK:. https://www.nature.com/articles/s41598-018-36177-7/tables/1. Do you have other metrics with comparisons on completely independent samples? Thank you very much for your help! Sincerely,. Charles",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/165
https://github.com/google/deepvariant/issues/165:1488,security,access,accessible,1488," the term ""DeepVariant"" or ""Google"" any where. I apologize for missing something that a lot of people might know, but would you mind explaining how the results in the [PrecisionFDA results table](https://precision.fda.gov/challenges/truth/results) match DeepVariant? **2)** While it has been hard for me explain myself precisely, I have been concerned that there was somehow over-fitting for some datasets reporting extremely high accuracy. In the PrecisionFDA challenge, I think it should probably be mentioned that there are multiple programs with high percentages, including different workflows that actually use the same variant caller (like GATK) and no strategy was ""best"" for all the criteria defined. **3)** While I don't know the precise way in which you could have a lack of independence for training versus test datasets, I think there are other benchmarks that better match what I would expect for more typical results. For example, [in this paper](https://www.nature.com/articles/s41587-019-0054-x), it says ""_In the high-confidence regions, when comparing these pipelines to each other (https://precision.fda.gov/jobs/job-FJpqBP80F3YyfJG02bQzPJBj, link immediately accessible by requesting an account), they agreed on 99.7% of SNVs and 98.7% of indels. Outside the high-confidence regions (https://precision.fda.gov/jobs/job-FJpqJF80F3YyXqz6Kv8Q1BQK), they agreed with each other on only 76.5% of SNVs and 78.7% of indels_."". **4)** You mention ""_No filtering is needed beyond setting your preferred minimum quality threshold_."". While I don't currently have much first-hand experience (although that is on my long-term ""to-do"" list), I didn't think this was true. For example, unless I am missing something, this table reports a very high ""Failed Filters"" count for DeepVariant versus GATK:. https://www.nature.com/articles/s41598-018-36177-7/tables/1. Do you have other metrics with comparisons on completely independent samples? Thank you very much for your help! Sincerely,. Charles",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/165
https://github.com/google/deepvariant/issues/165:2223,security,compl,completely,2223," the term ""DeepVariant"" or ""Google"" any where. I apologize for missing something that a lot of people might know, but would you mind explaining how the results in the [PrecisionFDA results table](https://precision.fda.gov/challenges/truth/results) match DeepVariant? **2)** While it has been hard for me explain myself precisely, I have been concerned that there was somehow over-fitting for some datasets reporting extremely high accuracy. In the PrecisionFDA challenge, I think it should probably be mentioned that there are multiple programs with high percentages, including different workflows that actually use the same variant caller (like GATK) and no strategy was ""best"" for all the criteria defined. **3)** While I don't know the precise way in which you could have a lack of independence for training versus test datasets, I think there are other benchmarks that better match what I would expect for more typical results. For example, [in this paper](https://www.nature.com/articles/s41587-019-0054-x), it says ""_In the high-confidence regions, when comparing these pipelines to each other (https://precision.fda.gov/jobs/job-FJpqBP80F3YyfJG02bQzPJBj, link immediately accessible by requesting an account), they agreed on 99.7% of SNVs and 98.7% of indels. Outside the high-confidence regions (https://precision.fda.gov/jobs/job-FJpqJF80F3YyXqz6Kv8Q1BQK), they agreed with each other on only 76.5% of SNVs and 78.7% of indels_."". **4)** You mention ""_No filtering is needed beyond setting your preferred minimum quality threshold_."". While I don't currently have much first-hand experience (although that is on my long-term ""to-do"" list), I didn't think this was true. For example, unless I am missing something, this table reports a very high ""Failed Filters"" count for DeepVariant versus GATK:. https://www.nature.com/articles/s41598-018-36177-7/tables/1. Do you have other metrics with comparisons on completely independent samples? Thank you very much for your help! Sincerely,. Charles",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/165
https://github.com/google/deepvariant/issues/165:172,testability,understand,understand,172,"High-Level Questions about ""Why DeepVariant"" Section; Hi,. I genuinely appreciate that you are providing a lot of detailed information for your free program. However, as I understand it, I have some high-level questions / concerns:. **1)** You mention scoring high on the PrecisionFDA calling, but I don't see the term ""DeepVariant"" or ""Google"" any where. I apologize for missing something that a lot of people might know, but would you mind explaining how the results in the [PrecisionFDA results table](https://precision.fda.gov/challenges/truth/results) match DeepVariant? **2)** While it has been hard for me explain myself precisely, I have been concerned that there was somehow over-fitting for some datasets reporting extremely high accuracy. In the PrecisionFDA challenge, I think it should probably be mentioned that there are multiple programs with high percentages, including different workflows that actually use the same variant caller (like GATK) and no strategy was ""best"" for all the criteria defined. **3)** While I don't know the precise way in which you could have a lack of independence for training versus test datasets, I think there are other benchmarks that better match what I would expect for more typical results. For example, [in this paper](https://www.nature.com/articles/s41587-019-0054-x), it says ""_In the high-confidence regions, when comparing these pipelines to each other (https://precision.fda.gov/jobs/job-FJpqBP80F3YyfJG02bQzPJBj, link immediately accessible by requesting an account), they agreed on 99.7% of SNVs and 98.7% of indels. Outside the high-confidence regions (https://precision.fda.gov/jobs/job-FJpqJF80F3YyXqz6Kv8Q1BQK), they agreed with each other on only 76.5% of SNVs and 78.7% of indels_."". **4)** You mention ""_No filtering is needed beyond setting your preferred minimum quality threshold_."". While I don't currently have much first-hand experience (although that is on my long-term ""to-do"" list), I didn't think this was true. For example,",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/165
https://github.com/google/deepvariant/issues/165:222,testability,concern,concerns,222,"High-Level Questions about ""Why DeepVariant"" Section; Hi,. I genuinely appreciate that you are providing a lot of detailed information for your free program. However, as I understand it, I have some high-level questions / concerns:. **1)** You mention scoring high on the PrecisionFDA calling, but I don't see the term ""DeepVariant"" or ""Google"" any where. I apologize for missing something that a lot of people might know, but would you mind explaining how the results in the [PrecisionFDA results table](https://precision.fda.gov/challenges/truth/results) match DeepVariant? **2)** While it has been hard for me explain myself precisely, I have been concerned that there was somehow over-fitting for some datasets reporting extremely high accuracy. In the PrecisionFDA challenge, I think it should probably be mentioned that there are multiple programs with high percentages, including different workflows that actually use the same variant caller (like GATK) and no strategy was ""best"" for all the criteria defined. **3)** While I don't know the precise way in which you could have a lack of independence for training versus test datasets, I think there are other benchmarks that better match what I would expect for more typical results. For example, [in this paper](https://www.nature.com/articles/s41587-019-0054-x), it says ""_In the high-confidence regions, when comparing these pipelines to each other (https://precision.fda.gov/jobs/job-FJpqBP80F3YyfJG02bQzPJBj, link immediately accessible by requesting an account), they agreed on 99.7% of SNVs and 98.7% of indels. Outside the high-confidence regions (https://precision.fda.gov/jobs/job-FJpqJF80F3YyXqz6Kv8Q1BQK), they agreed with each other on only 76.5% of SNVs and 78.7% of indels_."". **4)** You mention ""_No filtering is needed beyond setting your preferred minimum quality threshold_."". While I don't currently have much first-hand experience (although that is on my long-term ""to-do"" list), I didn't think this was true. For example,",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/165
https://github.com/google/deepvariant/issues/165:651,testability,concern,concerned,651,"High-Level Questions about ""Why DeepVariant"" Section; Hi,. I genuinely appreciate that you are providing a lot of detailed information for your free program. However, as I understand it, I have some high-level questions / concerns:. **1)** You mention scoring high on the PrecisionFDA calling, but I don't see the term ""DeepVariant"" or ""Google"" any where. I apologize for missing something that a lot of people might know, but would you mind explaining how the results in the [PrecisionFDA results table](https://precision.fda.gov/challenges/truth/results) match DeepVariant? **2)** While it has been hard for me explain myself precisely, I have been concerned that there was somehow over-fitting for some datasets reporting extremely high accuracy. In the PrecisionFDA challenge, I think it should probably be mentioned that there are multiple programs with high percentages, including different workflows that actually use the same variant caller (like GATK) and no strategy was ""best"" for all the criteria defined. **3)** While I don't know the precise way in which you could have a lack of independence for training versus test datasets, I think there are other benchmarks that better match what I would expect for more typical results. For example, [in this paper](https://www.nature.com/articles/s41587-019-0054-x), it says ""_In the high-confidence regions, when comparing these pipelines to each other (https://precision.fda.gov/jobs/job-FJpqBP80F3YyfJG02bQzPJBj, link immediately accessible by requesting an account), they agreed on 99.7% of SNVs and 98.7% of indels. Outside the high-confidence regions (https://precision.fda.gov/jobs/job-FJpqJF80F3YyXqz6Kv8Q1BQK), they agreed with each other on only 76.5% of SNVs and 78.7% of indels_."". **4)** You mention ""_No filtering is needed beyond setting your preferred minimum quality threshold_."". While I don't currently have much first-hand experience (although that is on my long-term ""to-do"" list), I didn't think this was true. For example,",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/165
https://github.com/google/deepvariant/issues/165:1127,testability,test,test,1127,"ation for your free program. However, as I understand it, I have some high-level questions / concerns:. **1)** You mention scoring high on the PrecisionFDA calling, but I don't see the term ""DeepVariant"" or ""Google"" any where. I apologize for missing something that a lot of people might know, but would you mind explaining how the results in the [PrecisionFDA results table](https://precision.fda.gov/challenges/truth/results) match DeepVariant? **2)** While it has been hard for me explain myself precisely, I have been concerned that there was somehow over-fitting for some datasets reporting extremely high accuracy. In the PrecisionFDA challenge, I think it should probably be mentioned that there are multiple programs with high percentages, including different workflows that actually use the same variant caller (like GATK) and no strategy was ""best"" for all the criteria defined. **3)** While I don't know the precise way in which you could have a lack of independence for training versus test datasets, I think there are other benchmarks that better match what I would expect for more typical results. For example, [in this paper](https://www.nature.com/articles/s41587-019-0054-x), it says ""_In the high-confidence regions, when comparing these pipelines to each other (https://precision.fda.gov/jobs/job-FJpqBP80F3YyfJG02bQzPJBj, link immediately accessible by requesting an account), they agreed on 99.7% of SNVs and 98.7% of indels. Outside the high-confidence regions (https://precision.fda.gov/jobs/job-FJpqJF80F3YyXqz6Kv8Q1BQK), they agreed with each other on only 76.5% of SNVs and 78.7% of indels_."". **4)** You mention ""_No filtering is needed beyond setting your preferred minimum quality threshold_."". While I don't currently have much first-hand experience (although that is on my long-term ""to-do"" list), I didn't think this was true. For example, unless I am missing something, this table reports a very high ""Failed Filters"" count for DeepVariant versus GATK:. https://www.n",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/165
https://github.com/google/deepvariant/issues/165:897,usability,workflow,workflows,897,"High-Level Questions about ""Why DeepVariant"" Section; Hi,. I genuinely appreciate that you are providing a lot of detailed information for your free program. However, as I understand it, I have some high-level questions / concerns:. **1)** You mention scoring high on the PrecisionFDA calling, but I don't see the term ""DeepVariant"" or ""Google"" any where. I apologize for missing something that a lot of people might know, but would you mind explaining how the results in the [PrecisionFDA results table](https://precision.fda.gov/challenges/truth/results) match DeepVariant? **2)** While it has been hard for me explain myself precisely, I have been concerned that there was somehow over-fitting for some datasets reporting extremely high accuracy. In the PrecisionFDA challenge, I think it should probably be mentioned that there are multiple programs with high percentages, including different workflows that actually use the same variant caller (like GATK) and no strategy was ""best"" for all the criteria defined. **3)** While I don't know the precise way in which you could have a lack of independence for training versus test datasets, I think there are other benchmarks that better match what I would expect for more typical results. For example, [in this paper](https://www.nature.com/articles/s41587-019-0054-x), it says ""_In the high-confidence regions, when comparing these pipelines to each other (https://precision.fda.gov/jobs/job-FJpqBP80F3YyfJG02bQzPJBj, link immediately accessible by requesting an account), they agreed on 99.7% of SNVs and 98.7% of indels. Outside the high-confidence regions (https://precision.fda.gov/jobs/job-FJpqJF80F3YyXqz6Kv8Q1BQK), they agreed with each other on only 76.5% of SNVs and 78.7% of indels_."". **4)** You mention ""_No filtering is needed beyond setting your preferred minimum quality threshold_."". While I don't currently have much first-hand experience (although that is on my long-term ""to-do"" list), I didn't think this was true. For example,",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/165
https://github.com/google/deepvariant/issues/165:1813,usability,prefer,preferred,1813," the term ""DeepVariant"" or ""Google"" any where. I apologize for missing something that a lot of people might know, but would you mind explaining how the results in the [PrecisionFDA results table](https://precision.fda.gov/challenges/truth/results) match DeepVariant? **2)** While it has been hard for me explain myself precisely, I have been concerned that there was somehow over-fitting for some datasets reporting extremely high accuracy. In the PrecisionFDA challenge, I think it should probably be mentioned that there are multiple programs with high percentages, including different workflows that actually use the same variant caller (like GATK) and no strategy was ""best"" for all the criteria defined. **3)** While I don't know the precise way in which you could have a lack of independence for training versus test datasets, I think there are other benchmarks that better match what I would expect for more typical results. For example, [in this paper](https://www.nature.com/articles/s41587-019-0054-x), it says ""_In the high-confidence regions, when comparing these pipelines to each other (https://precision.fda.gov/jobs/job-FJpqBP80F3YyfJG02bQzPJBj, link immediately accessible by requesting an account), they agreed on 99.7% of SNVs and 98.7% of indels. Outside the high-confidence regions (https://precision.fda.gov/jobs/job-FJpqJF80F3YyXqz6Kv8Q1BQK), they agreed with each other on only 76.5% of SNVs and 78.7% of indels_."". **4)** You mention ""_No filtering is needed beyond setting your preferred minimum quality threshold_."". While I don't currently have much first-hand experience (although that is on my long-term ""to-do"" list), I didn't think this was true. For example, unless I am missing something, this table reports a very high ""Failed Filters"" count for DeepVariant versus GATK:. https://www.nature.com/articles/s41598-018-36177-7/tables/1. Do you have other metrics with comparisons on completely independent samples? Thank you very much for your help! Sincerely,. Charles",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/165
https://github.com/google/deepvariant/issues/165:1823,usability,minim,minimum,1823," the term ""DeepVariant"" or ""Google"" any where. I apologize for missing something that a lot of people might know, but would you mind explaining how the results in the [PrecisionFDA results table](https://precision.fda.gov/challenges/truth/results) match DeepVariant? **2)** While it has been hard for me explain myself precisely, I have been concerned that there was somehow over-fitting for some datasets reporting extremely high accuracy. In the PrecisionFDA challenge, I think it should probably be mentioned that there are multiple programs with high percentages, including different workflows that actually use the same variant caller (like GATK) and no strategy was ""best"" for all the criteria defined. **3)** While I don't know the precise way in which you could have a lack of independence for training versus test datasets, I think there are other benchmarks that better match what I would expect for more typical results. For example, [in this paper](https://www.nature.com/articles/s41587-019-0054-x), it says ""_In the high-confidence regions, when comparing these pipelines to each other (https://precision.fda.gov/jobs/job-FJpqBP80F3YyfJG02bQzPJBj, link immediately accessible by requesting an account), they agreed on 99.7% of SNVs and 98.7% of indels. Outside the high-confidence regions (https://precision.fda.gov/jobs/job-FJpqJF80F3YyXqz6Kv8Q1BQK), they agreed with each other on only 76.5% of SNVs and 78.7% of indels_."". **4)** You mention ""_No filtering is needed beyond setting your preferred minimum quality threshold_."". While I don't currently have much first-hand experience (although that is on my long-term ""to-do"" list), I didn't think this was true. For example, unless I am missing something, this table reports a very high ""Failed Filters"" count for DeepVariant versus GATK:. https://www.nature.com/articles/s41598-018-36177-7/tables/1. Do you have other metrics with comparisons on completely independent samples? Thank you very much for your help! Sincerely,. Charles",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/165
https://github.com/google/deepvariant/issues/165:1898,usability,experien,experience,1898," the term ""DeepVariant"" or ""Google"" any where. I apologize for missing something that a lot of people might know, but would you mind explaining how the results in the [PrecisionFDA results table](https://precision.fda.gov/challenges/truth/results) match DeepVariant? **2)** While it has been hard for me explain myself precisely, I have been concerned that there was somehow over-fitting for some datasets reporting extremely high accuracy. In the PrecisionFDA challenge, I think it should probably be mentioned that there are multiple programs with high percentages, including different workflows that actually use the same variant caller (like GATK) and no strategy was ""best"" for all the criteria defined. **3)** While I don't know the precise way in which you could have a lack of independence for training versus test datasets, I think there are other benchmarks that better match what I would expect for more typical results. For example, [in this paper](https://www.nature.com/articles/s41587-019-0054-x), it says ""_In the high-confidence regions, when comparing these pipelines to each other (https://precision.fda.gov/jobs/job-FJpqBP80F3YyfJG02bQzPJBj, link immediately accessible by requesting an account), they agreed on 99.7% of SNVs and 98.7% of indels. Outside the high-confidence regions (https://precision.fda.gov/jobs/job-FJpqJF80F3YyXqz6Kv8Q1BQK), they agreed with each other on only 76.5% of SNVs and 78.7% of indels_."". **4)** You mention ""_No filtering is needed beyond setting your preferred minimum quality threshold_."". While I don't currently have much first-hand experience (although that is on my long-term ""to-do"" list), I didn't think this was true. For example, unless I am missing something, this table reports a very high ""Failed Filters"" count for DeepVariant versus GATK:. https://www.nature.com/articles/s41598-018-36177-7/tables/1. Do you have other metrics with comparisons on completely independent samples? Thank you very much for your help! Sincerely,. Charles",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/165
https://github.com/google/deepvariant/issues/165:2284,usability,help,help,2284," the term ""DeepVariant"" or ""Google"" any where. I apologize for missing something that a lot of people might know, but would you mind explaining how the results in the [PrecisionFDA results table](https://precision.fda.gov/challenges/truth/results) match DeepVariant? **2)** While it has been hard for me explain myself precisely, I have been concerned that there was somehow over-fitting for some datasets reporting extremely high accuracy. In the PrecisionFDA challenge, I think it should probably be mentioned that there are multiple programs with high percentages, including different workflows that actually use the same variant caller (like GATK) and no strategy was ""best"" for all the criteria defined. **3)** While I don't know the precise way in which you could have a lack of independence for training versus test datasets, I think there are other benchmarks that better match what I would expect for more typical results. For example, [in this paper](https://www.nature.com/articles/s41587-019-0054-x), it says ""_In the high-confidence regions, when comparing these pipelines to each other (https://precision.fda.gov/jobs/job-FJpqBP80F3YyfJG02bQzPJBj, link immediately accessible by requesting an account), they agreed on 99.7% of SNVs and 98.7% of indels. Outside the high-confidence regions (https://precision.fda.gov/jobs/job-FJpqJF80F3YyXqz6Kv8Q1BQK), they agreed with each other on only 76.5% of SNVs and 78.7% of indels_."". **4)** You mention ""_No filtering is needed beyond setting your preferred minimum quality threshold_."". While I don't currently have much first-hand experience (although that is on my long-term ""to-do"" list), I didn't think this was true. For example, unless I am missing something, this table reports a very high ""Failed Filters"" count for DeepVariant versus GATK:. https://www.nature.com/articles/s41598-018-36177-7/tables/1. Do you have other metrics with comparisons on completely independent samples? Thank you very much for your help! Sincerely,. Charles",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/165
https://github.com/google/deepvariant/issues/166:770,availability,down,download,770,"DataLossError / Failed precondition when running WES call_variant; Hello,. I have been trying to do some testing of DeepVariant on my own Exome (WES) and WGS data. As a starting point, I was trying to work with calling variants from the .bam file provided for my WES data. I started running from within a Docker container on my local computer but that was taking a long time (and, ultimately, the _make_examples_ step did not run to completion). I started learning more about the AWS options for analysis, and I was able to run the _make_examples_ much quicker (and successfully) on an AWS m5.xlarge ECS instance (although I am admittedly well over the ~25 minutes and $0.20 time/cost mentioned for Google Cloud, just for the _make_examples_, without considering upload/download, long-term storage, etc.). While I was hoping to eventually compare running things on Google Cloud (and I think my experience so far probably helps me ask better questions), I was wondering if you could help me troubleshoot something that I think is probably close to working:. Essentially, I am currently at the **call_variant** step of DeepVariant, with WES data. This is the error message that I am currently receiving:. ```. sudo sh run_deepvariant.sh. I0331 18:31:22.446569 140549764839168 call_variants.py:292] Set KMP_BLOCKTIME to 0. 2019-03-31 18:31:22.486802: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2 AVX512F FMA. 2019-03-31 18:31:22.489180: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. I0331 18:31:22.527594 140549764839168 modeling.py:351] Initializing model with random parameters. W0331 18:31:22.529449 140549764839168 tf_logging.py:125] Using temporary folder as model directory: /tmp/tmpuBleAQ. I0331 18:31:22.529786 140549764839168 tf_logging.py:115] Using config: {'_sav",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:1157,availability,error,error,1157,"ta. As a starting point, I was trying to work with calling variants from the .bam file provided for my WES data. I started running from within a Docker container on my local computer but that was taking a long time (and, ultimately, the _make_examples_ step did not run to completion). I started learning more about the AWS options for analysis, and I was able to run the _make_examples_ much quicker (and successfully) on an AWS m5.xlarge ECS instance (although I am admittedly well over the ~25 minutes and $0.20 time/cost mentioned for Google Cloud, just for the _make_examples_, without considering upload/download, long-term storage, etc.). While I was hoping to eventually compare running things on Google Cloud (and I think my experience so far probably helps me ask better questions), I was wondering if you could help me troubleshoot something that I think is probably close to working:. Essentially, I am currently at the **call_variant** step of DeepVariant, with WES data. This is the error message that I am currently receiving:. ```. sudo sh run_deepvariant.sh. I0331 18:31:22.446569 140549764839168 call_variants.py:292] Set KMP_BLOCKTIME to 0. 2019-03-31 18:31:22.486802: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2 AVX512F FMA. 2019-03-31 18:31:22.489180: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. I0331 18:31:22.527594 140549764839168 modeling.py:351] Initializing model with random parameters. W0331 18:31:22.529449 140549764839168 tf_logging.py:125] Using temporary folder as model directory: /tmp/tmpuBleAQ. I0331 18:31:22.529786 140549764839168 tf_logging.py:115] Using config: {'_save_checkpoints_secs': 1000, '_num_ps_replicas': 0, '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_clust",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:2209,availability,Cluster,ClusterSpec,2209," run_deepvariant.sh. I0331 18:31:22.446569 140549764839168 call_variants.py:292] Set KMP_BLOCKTIME to 0. 2019-03-31 18:31:22.486802: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2 AVX512F FMA. 2019-03-31 18:31:22.489180: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. I0331 18:31:22.527594 140549764839168 modeling.py:351] Initializing model with random parameters. W0331 18:31:22.529449 140549764839168 tf_logging.py:125] Using temporary folder as model directory: /tmp/tmpuBleAQ. I0331 18:31:22.529786 140549764839168 tf_logging.py:115] Using config: {'_save_checkpoints_secs': 1000, '_num_ps_replicas': 0, '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd40e875e50>, '_model_dir': '/tmp/tmpuBleAQ', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_train_distribute': None, '_master': ''}. I0331 18:31:22.529983 140549764839168 call_variants.py:350] Writing calls to /mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant/call_variants_output.tfrecord.gz. I0331 18:31:22.610783 140549764839168 tf_logging.py:115] Calling model_fn. I0331 18:31:24.905831 140549764839168 tf_logging.py:115] Done calling model_fn. I0331 18:31:25.996718 140549764839168 tf_logging.py:115] Graph was finalized. I0331 18:31:2",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:3259,availability,Restor,Restoring,3259,"tmp/tmpuBleAQ', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_train_distribute': None, '_master': ''}. I0331 18:31:22.529983 140549764839168 call_variants.py:350] Writing calls to /mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant/call_variants_output.tfrecord.gz. I0331 18:31:22.610783 140549764839168 tf_logging.py:115] Calling model_fn. I0331 18:31:24.905831 140549764839168 tf_logging.py:115] Done calling model_fn. I0331 18:31:25.996718 140549764839168 tf_logging.py:115] Graph was finalized. I0331 18:31:26.008368 140549764839168 tf_logging.py:115] Restoring parameters from /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. 2019-03-31 18:31:26.447697: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? 2019-03-31 18:31:26.450035: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? 2019-03-31 18:31:26.450061: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_ten",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:3741,availability,restor,restore,3741,"'_train_distribute': None, '_master': ''}. I0331 18:31:22.529983 140549764839168 call_variants.py:350] Writing calls to /mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant/call_variants_output.tfrecord.gz. I0331 18:31:22.610783 140549764839168 tf_logging.py:115] Calling model_fn. I0331 18:31:24.905831 140549764839168 tf_logging.py:115] Done calling model_fn. I0331 18:31:25.996718 140549764839168 tf_logging.py:115] Graph was finalized. I0331 18:31:26.008368 140549764839168 tf_logging.py:115] Restoring parameters from /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. 2019-03-31 18:31:26.447697: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? 2019-03-31 18:31:26.450035: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? 2019-03-31 18:31:26.450061: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:172 : Data loss: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:3749,availability,operat,operator,3749,"distribute': None, '_master': ''}. I0331 18:31:22.529983 140549764839168 call_variants.py:350] Writing calls to /mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant/call_variants_output.tfrecord.gz. I0331 18:31:22.610783 140549764839168 tf_logging.py:115] Calling model_fn. I0331 18:31:24.905831 140549764839168 tf_logging.py:115] Done calling model_fn. I0331 18:31:25.996718 140549764839168 tf_logging.py:115] Graph was finalized. I0331 18:31:26.008368 140549764839168 tf_logging.py:115] Restoring parameters from /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. 2019-03-31 18:31:26.447697: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? 2019-03-31 18:31:26.450035: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? 2019-03-31 18:31:26.450061: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:172 : Data loss: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:4133,availability,restor,restore,4133,"5.996718 140549764839168 tf_logging.py:115] Graph was finalized. I0331 18:31:26.008368 140549764839168 tf_logging.py:115] Restoring parameters from /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. 2019-03-31 18:31:26.447697: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? 2019-03-31 18:31:26.450035: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? 2019-03-31 18:31:26.450061: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:172 : Data loss: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>. tf.app.run(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 401, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.p",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:4141,availability,operat,operator,4141," 140549764839168 tf_logging.py:115] Graph was finalized. I0331 18:31:26.008368 140549764839168 tf_logging.py:115] Restoring parameters from /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. 2019-03-31 18:31:26.447697: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? 2019-03-31 18:31:26.450035: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? 2019-03-31 18:31:26.450061: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:172 : Data loss: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>. tf.app.run(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 401, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:4595,availability,restor,restore,4595,"Variant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? 2019-03-31 18:31:26.450035: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? 2019-03-31 18:31:26.450061: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:172 : Data loss: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>. tf.app.run(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 401, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 357, in call_variants. prediction = next(predictions). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 567, in predict. hooks=all_hooks) as mon_sess:. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 921, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_ses",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:4603,availability,operat,operator,4603,"inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? 2019-03-31 18:31:26.450035: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? 2019-03-31 18:31:26.450061: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:172 : Data loss: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>. tf.app.run(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 401, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 357, in call_variants. prediction = next(predictions). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 567, in predict. hooks=all_hooks) as mon_sess:. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 921, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py""",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:6658,availability,restor,restore,6658,"on(self._coordinated_creator). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1107, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1112, in _create_session. return self._sess_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 566, in create_session. init_fn=self._scaffold.init_fn). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session. config=config). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 195, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1538, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 887, in run. run_metadata_ptr). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1110, in _run. feed_dict_tensor, options, run_metadata). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1286, in _do_run. run_metadata). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1308, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.DataLossError: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_stan",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:6803,availability,restor,restore,6803,"nit__. _WrappedSession.__init__(self, self._create_session()). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1112, in _create_session. return self._sess_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 566, in create_session. init_fn=self._scaffold.init_fn). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session. config=config). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 195, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1538, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 887, in run. run_metadata_ptr). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1110, in _run. feed_dict_tensor, options, run_metadata). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1286, in _do_run. run_metadata). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1308, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.DataLossError: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:7764,availability,restor,restore,7764,"/training/saver.py"", line 1538, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 887, in run. run_metadata_ptr). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1110, in _run. feed_dict_tensor, options, run_metadata). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1286, in _do_run. run_metadata). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1308, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.DataLossError: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. Caused by op u'save_1/RestoreV2', defined at:. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>. tf.app.run(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 401, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 357, in call_variants. prediction = next(predictions). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:7772,availability,operat,operator,7772,"g/saver.py"", line 1538, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 887, in run. run_metadata_ptr). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1110, in _run. feed_dict_tensor, options, run_metadata). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1286, in _do_run. run_metadata). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1308, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.DataLossError: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. Caused by op u'save_1/RestoreV2', defined at:. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>. tf.app.run(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 401, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 357, in call_variants. prediction = next(predictions). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", lin",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:7960,availability,replic,replica,7960,". run_metadata_ptr). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1110, in _run. feed_dict_tensor, options, run_metadata). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1286, in _do_run. run_metadata). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1308, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.DataLossError: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. Caused by op u'save_1/RestoreV2', defined at:. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>. tf.app.run(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 401, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 357, in call_variants. prediction = next(predictions). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 567, in predict. hooks=all_hooks) as mon_sess:. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 921, in __init__. stop_grace_period_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:11280,availability,sli,slices,11280,"ensorflow/python/training/saver.py"", line 1094, in __init__. self.build(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1106, in build. self._build(self._filename, build_save=True, build_restore=True). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1143, in _build. build_save=build_save, build_restore=build_restore). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 781, in _build_internal. restore_sequentially, reshape). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 459, in _AddShardedRestoreOps. name=""restore_shard"")). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 406, in _AddRestoreOps. restore_sequentially). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 854, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 1466, in restore_v2. shape_and_slices=shape_and_slices, dtypes=dtypes, name=name). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper. op_def=op_def). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func. return func(*args, **kwargs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 3272, in create_op. op_def=op_def). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1768, in __init__. self._traceback = tf_stack.extract_stack(). DataLossError (see above for traceback): Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:12377,availability,restor,restore,12377," line 1466, in restore_v2. shape_and_slices=shape_and_slices, dtypes=dtypes, name=name). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper. op_def=op_def). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func. return func(*args, **kwargs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 3272, in create_op. op_def=op_def). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1768, in __init__. self._traceback = tf_stack.extract_stack(). DataLossError (see above for traceback): Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. ```. This is the script that I am running DeepVariant:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. BAM=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/82651510240740.mapped.sorted.markdup.realn.recal.bam. MODEL=/mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. ## step #1. LOGDIR=logs. N_SHARDS=4. #mkdir -p ""${LOGDIR}"". #time seq 0 $((N_SHARDS-1)) | \. # parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" \. # sudo docker run \. # -v /mnt/efs-genome:/mnt/efs-genome \. # gcr.io/deepvariant-docker/deepvariant \. # /opt/deepvariant/bin/make",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:12385,availability,operat,operator,12385,"66, in restore_v2. shape_and_slices=shape_and_slices, dtypes=dtypes, name=name). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper. op_def=op_def). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func. return func(*args, **kwargs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 3272, in create_op. op_def=op_def). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1768, in __init__. self._traceback = tf_stack.extract_stack(). DataLossError (see above for traceback): Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. ```. This is the script that I am running DeepVariant:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. BAM=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/82651510240740.mapped.sorted.markdup.realn.recal.bam. MODEL=/mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. ## step #1. LOGDIR=logs. N_SHARDS=4. #mkdir -p ""${LOGDIR}"". #time seq 0 $((N_SHARDS-1)) | \. # parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" \. # sudo docker run \. # -v /mnt/efs-genome:/mnt/efs-genome \. # gcr.io/deepvariant-docker/deepvariant \. # /opt/deepvariant/bin/make_example",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:12573,availability,replic,replica,12573," _apply_op_helper. op_def=op_def). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func. return func(*args, **kwargs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 3272, in create_op. op_def=op_def). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1768, in __init__. self._traceback = tf_stack.extract_stack(). DataLossError (see above for traceback): Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. ```. This is the script that I am running DeepVariant:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. BAM=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/82651510240740.mapped.sorted.markdup.realn.recal.bam. MODEL=/mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. ## step #1. LOGDIR=logs. N_SHARDS=4. #mkdir -p ""${LOGDIR}"". #time seq 0 $((N_SHARDS-1)) | \. # parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" \. # sudo docker run \. # -v /mnt/efs-genome:/mnt/efs-genome \. # gcr.io/deepvariant-docker/deepvariant \. # /opt/deepvariant/bin/make_examples \. # --mode calling \. # --ref ""${REF}"" \. # --reads ""${BAM}"" \. # --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. # --task {}. ## step #2. CALL_VARIANTS_OUTPUT=""${OUTPUT_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:13857,availability,checkpoint,checkpoint,13857,"3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. ```. This is the script that I am running DeepVariant:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. BAM=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/82651510240740.mapped.sorted.markdup.realn.recal.bam. MODEL=/mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. ## step #1. LOGDIR=logs. N_SHARDS=4. #mkdir -p ""${LOGDIR}"". #time seq 0 $((N_SHARDS-1)) | \. # parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" \. # sudo docker run \. # -v /mnt/efs-genome:/mnt/efs-genome \. # gcr.io/deepvariant-docker/deepvariant \. # /opt/deepvariant/bin/make_examples \. # --mode calling \. # --ref ""${REF}"" \. # --reads ""${BAM}"" \. # --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. # --task {}. ## step #2. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". sudo docker run \. -v /mnt/efs-genome:/mnt/efs-genome \. gcr.io/deepvariant-docker/deepvariant \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""${MODEL}"". ```. Can you please help me troubleshoot? I thought it might be something simple, like [this question](https://github.com/google/deepvariant/issues/129). However, that particular solution is not working for me. Thank you very much for your assistance. Sincerely,. Charles",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:16,deployability,Fail,Failed,16,"DataLossError / Failed precondition when running WES call_variant; Hello,. I have been trying to do some testing of DeepVariant on my own Exome (WES) and WGS data. As a starting point, I was trying to work with calling variants from the .bam file provided for my WES data. I started running from within a Docker container on my local computer but that was taking a long time (and, ultimately, the _make_examples_ step did not run to completion). I started learning more about the AWS options for analysis, and I was able to run the _make_examples_ much quicker (and successfully) on an AWS m5.xlarge ECS instance (although I am admittedly well over the ~25 minutes and $0.20 time/cost mentioned for Google Cloud, just for the _make_examples_, without considering upload/download, long-term storage, etc.). While I was hoping to eventually compare running things on Google Cloud (and I think my experience so far probably helps me ask better questions), I was wondering if you could help me troubleshoot something that I think is probably close to working:. Essentially, I am currently at the **call_variant** step of DeepVariant, with WES data. This is the error message that I am currently receiving:. ```. sudo sh run_deepvariant.sh. I0331 18:31:22.446569 140549764839168 call_variants.py:292] Set KMP_BLOCKTIME to 0. 2019-03-31 18:31:22.486802: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2 AVX512F FMA. 2019-03-31 18:31:22.489180: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. I0331 18:31:22.527594 140549764839168 modeling.py:351] Initializing model with random parameters. W0331 18:31:22.529449 140549764839168 tf_logging.py:125] Using temporary folder as model directory: /tmp/tmpuBleAQ. I0331 18:31:22.529786 140549764839168 tf_logging.py:115] Using config: {'_sav",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:312,deployability,contain,container,312,"DataLossError / Failed precondition when running WES call_variant; Hello,. I have been trying to do some testing of DeepVariant on my own Exome (WES) and WGS data. As a starting point, I was trying to work with calling variants from the .bam file provided for my WES data. I started running from within a Docker container on my local computer but that was taking a long time (and, ultimately, the _make_examples_ step did not run to completion). I started learning more about the AWS options for analysis, and I was able to run the _make_examples_ much quicker (and successfully) on an AWS m5.xlarge ECS instance (although I am admittedly well over the ~25 minutes and $0.20 time/cost mentioned for Google Cloud, just for the _make_examples_, without considering upload/download, long-term storage, etc.). While I was hoping to eventually compare running things on Google Cloud (and I think my experience so far probably helps me ask better questions), I was wondering if you could help me troubleshoot something that I think is probably close to working:. Essentially, I am currently at the **call_variant** step of DeepVariant, with WES data. This is the error message that I am currently receiving:. ```. sudo sh run_deepvariant.sh. I0331 18:31:22.446569 140549764839168 call_variants.py:292] Set KMP_BLOCKTIME to 0. 2019-03-31 18:31:22.486802: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2 AVX512F FMA. 2019-03-31 18:31:22.489180: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. I0331 18:31:22.527594 140549764839168 modeling.py:351] Initializing model with random parameters. W0331 18:31:22.529449 140549764839168 tf_logging.py:125] Using temporary folder as model directory: /tmp/tmpuBleAQ. I0331 18:31:22.529786 140549764839168 tf_logging.py:115] Using config: {'_sav",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:2209,deployability,Cluster,ClusterSpec,2209," run_deepvariant.sh. I0331 18:31:22.446569 140549764839168 call_variants.py:292] Set KMP_BLOCKTIME to 0. 2019-03-31 18:31:22.486802: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2 AVX512F FMA. 2019-03-31 18:31:22.489180: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. I0331 18:31:22.527594 140549764839168 modeling.py:351] Initializing model with random parameters. W0331 18:31:22.529449 140549764839168 tf_logging.py:125] Using temporary folder as model directory: /tmp/tmpuBleAQ. I0331 18:31:22.529786 140549764839168 tf_logging.py:115] Using config: {'_save_checkpoints_secs': 1000, '_num_ps_replicas': 0, '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd40e875e50>, '_model_dir': '/tmp/tmpuBleAQ', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_train_distribute': None, '_master': ''}. I0331 18:31:22.529983 140549764839168 call_variants.py:350] Writing calls to /mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant/call_variants_output.tfrecord.gz. I0331 18:31:22.610783 140549764839168 tf_logging.py:115] Calling model_fn. I0331 18:31:24.905831 140549764839168 tf_logging.py:115] Done calling model_fn. I0331 18:31:25.996718 140549764839168 tf_logging.py:115] Graph was finalized. I0331 18:31:2",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:3542,deployability,Fail,Failed,3542,"_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_train_distribute': None, '_master': ''}. I0331 18:31:22.529983 140549764839168 call_variants.py:350] Writing calls to /mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant/call_variants_output.tfrecord.gz. I0331 18:31:22.610783 140549764839168 tf_logging.py:115] Calling model_fn. I0331 18:31:24.905831 140549764839168 tf_logging.py:115] Done calling model_fn. I0331 18:31:25.996718 140549764839168 tf_logging.py:115] Graph was finalized. I0331 18:31:26.008368 140549764839168 tf_logging.py:115] Restoring parameters from /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. 2019-03-31 18:31:26.447697: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? 2019-03-31 18:31:26.450035: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? 2019-03-31 18:31:26.450061: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:172 : Data loss: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a diff",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:3934,deployability,Fail,Failed,3934,"ll_variants_output.tfrecord.gz. I0331 18:31:22.610783 140549764839168 tf_logging.py:115] Calling model_fn. I0331 18:31:24.905831 140549764839168 tf_logging.py:115] Done calling model_fn. I0331 18:31:25.996718 140549764839168 tf_logging.py:115] Graph was finalized. I0331 18:31:26.008368 140549764839168 tf_logging.py:115] Restoring parameters from /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. 2019-03-31 18:31:26.447697: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? 2019-03-31 18:31:26.450035: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? 2019-03-31 18:31:26.450061: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:172 : Data loss: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>. tf.app.run(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_9uQwM",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:4238,deployability,fail,failed,4238,"f_logging.py:115] Restoring parameters from /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. 2019-03-31 18:31:26.447697: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? 2019-03-31 18:31:26.450035: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? 2019-03-31 18:31:26.450061: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:172 : Data loss: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>. tf.app.run(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 401, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 357, in call_variants. prediction = next(predictions). File ""/root/.local/lib/python2.7/site-pa",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:4396,deployability,Fail,Failed,4396,"nsorflow/core/util/tensor_slice_reader.cc:95] Could not open /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? 2019-03-31 18:31:26.450035: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? 2019-03-31 18:31:26.450061: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:172 : Data loss: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>. tf.app.run(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 401, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 357, in call_variants. prediction = next(predictions). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 567, in predict. hooks=all_hooks) as mon_sess:. File ""/root/.local/lib/python2.7/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:4759,deployability,modul,module,4759,"9-03-31 18:31:26.450035: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? 2019-03-31 18:31:26.450061: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:172 : Data loss: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>. tf.app.run(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 401, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 357, in call_variants. prediction = next(predictions). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 567, in predict. hooks=all_hooks) as mon_sess:. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 921, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 643, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/trai",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:7565,deployability,Fail,Failed,7565,"s/tensorflow/python/training/session_manager.py"", line 195, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1538, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 887, in run. run_metadata_ptr). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1110, in _run. feed_dict_tensor, options, run_metadata). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1286, in _do_run. run_metadata). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1308, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.DataLossError: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. Caused by op u'save_1/RestoreV2', defined at:. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>. tf.app.run(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 401, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:8241,deployability,modul,module,8241,"metadata). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1308, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.DataLossError: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. Caused by op u'save_1/RestoreV2', defined at:. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>. tf.app.run(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 401, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 357, in call_variants. prediction = next(predictions). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 567, in predict. hooks=all_hooks) as mon_sess:. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 921, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 643, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/trai",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:10349,deployability,build,build,10349,"File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1112, in _create_session. return self._sess_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 557, in create_session. self._scaffold.finalize(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 213, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 878, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1094, in __init__. self.build(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1106, in build. self._build(self._filename, build_save=True, build_restore=True). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1143, in _build. build_save=build_save, build_restore=build_restore). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 781, in _build_internal. restore_sequentially, reshape). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 459, in _AddShardedRestoreOps. name=""restore_shard"")). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 406, in _AddRestoreOps. restore_sequentially). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 854, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""/root/.local/lib/python2.7/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:10457,deployability,build,build,10457,"in _create_session. return self._sess_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 557, in create_session. self._scaffold.finalize(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 213, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 878, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1094, in __init__. self.build(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1106, in build. self._build(self._filename, build_save=True, build_restore=True). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1143, in _build. build_save=build_save, build_restore=build_restore). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 781, in _build_internal. restore_sequentially, reshape). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 459, in _AddShardedRestoreOps. name=""restore_shard"")). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 406, in _AddRestoreOps. restore_sequentially). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 854, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 1466, in restore_v2. shape_and_slices=shape_and_slices, dtypes=dtypes, na",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:12178,deployability,Fail,Failed,12178,"on/training/saver.py"", line 854, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 1466, in restore_v2. shape_and_slices=shape_and_slices, dtypes=dtypes, name=name). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper. op_def=op_def). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func. return func(*args, **kwargs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 3272, in create_op. op_def=op_def). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1768, in __init__. self._traceback = tf_stack.extract_stack(). DataLossError (see above for traceback): Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. ```. This is the script that I am running DeepVariant:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. BAM=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/82651510240740.mapped.sorted.markdup.realn.recal.bam. MODEL=/mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. ## step #1. LOGDIR=logs. N_SHARDS=4. #mkdir -p ""${LOGDIR}"". #time seq 0 $((N_SHARDS-1)) | \. # p",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:13097,deployability,LOG,LOGDIR,13097,"/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. ```. This is the script that I am running DeepVariant:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. BAM=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/82651510240740.mapped.sorted.markdup.realn.recal.bam. MODEL=/mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. ## step #1. LOGDIR=logs. N_SHARDS=4. #mkdir -p ""${LOGDIR}"". #time seq 0 $((N_SHARDS-1)) | \. # parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" \. # sudo docker run \. # -v /mnt/efs-genome:/mnt/efs-genome \. # gcr.io/deepvariant-docker/deepvariant \. # /opt/deepvariant/bin/make_examples \. # --mode calling \. # --ref ""${REF}"" \. # --reads ""${BAM}"" \. # --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. # --task {}. ## step #2. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". sudo docker run \. -v /mnt/efs-genome:/mnt/efs-genome \. gcr.io/deepvariant-docker/deepvariant \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""${MODEL}"". ```. Can you please help me troubleshoot? I thought it might be something simple, like [this question](https://github.com/google/deepvariant/issues/129). However, that particular solution is not working for me. Thank you",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:13104,deployability,log,logs,13104,"enome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. ```. This is the script that I am running DeepVariant:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. BAM=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/82651510240740.mapped.sorted.markdup.realn.recal.bam. MODEL=/mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. ## step #1. LOGDIR=logs. N_SHARDS=4. #mkdir -p ""${LOGDIR}"". #time seq 0 $((N_SHARDS-1)) | \. # parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" \. # sudo docker run \. # -v /mnt/efs-genome:/mnt/efs-genome \. # gcr.io/deepvariant-docker/deepvariant \. # /opt/deepvariant/bin/make_examples \. # --mode calling \. # --ref ""${REF}"" \. # --reads ""${BAM}"" \. # --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. # --task {}. ## step #2. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". sudo docker run \. -v /mnt/efs-genome:/mnt/efs-genome \. gcr.io/deepvariant-docker/deepvariant \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""${MODEL}"". ```. Can you please help me troubleshoot? I thought it might be something simple, like [this question](https://github.com/google/deepvariant/issues/129). However, that particular solution is not working for me. Thank you very ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:13135,deployability,LOG,LOGDIR,13135,"t-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. ```. This is the script that I am running DeepVariant:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. BAM=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/82651510240740.mapped.sorted.markdup.realn.recal.bam. MODEL=/mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. ## step #1. LOGDIR=logs. N_SHARDS=4. #mkdir -p ""${LOGDIR}"". #time seq 0 $((N_SHARDS-1)) | \. # parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" \. # sudo docker run \. # -v /mnt/efs-genome:/mnt/efs-genome \. # gcr.io/deepvariant-docker/deepvariant \. # /opt/deepvariant/bin/make_examples \. # --mode calling \. # --ref ""${REF}"" \. # --reads ""${BAM}"" \. # --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. # --task {}. ## step #2. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". sudo docker run \. -v /mnt/efs-genome:/mnt/efs-genome \. gcr.io/deepvariant-docker/deepvariant \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""${MODEL}"". ```. Can you please help me troubleshoot? I thought it might be something simple, like [this question](https://github.com/google/deepvariant/issues/129). However, that particular solution is not working for me. Thank you very much for your assistance. Sincer",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:13216,deployability,LOG,LOGDIR,13216,"3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. ```. This is the script that I am running DeepVariant:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. BAM=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/82651510240740.mapped.sorted.markdup.realn.recal.bam. MODEL=/mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. ## step #1. LOGDIR=logs. N_SHARDS=4. #mkdir -p ""${LOGDIR}"". #time seq 0 $((N_SHARDS-1)) | \. # parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" \. # sudo docker run \. # -v /mnt/efs-genome:/mnt/efs-genome \. # gcr.io/deepvariant-docker/deepvariant \. # /opt/deepvariant/bin/make_examples \. # --mode calling \. # --ref ""${REF}"" \. # --reads ""${BAM}"" \. # --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. # --task {}. ## step #2. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". sudo docker run \. -v /mnt/efs-genome:/mnt/efs-genome \. gcr.io/deepvariant-docker/deepvariant \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""${MODEL}"". ```. Can you please help me troubleshoot? I thought it might be something simple, like [this question](https://github.com/google/deepvariant/issues/129). However, that particular solution is not working for me. Thank you very much for your assistance. Sincerely,. Charles",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:13224,deployability,log,log,13224,"3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. ```. This is the script that I am running DeepVariant:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. BAM=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/82651510240740.mapped.sorted.markdup.realn.recal.bam. MODEL=/mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. ## step #1. LOGDIR=logs. N_SHARDS=4. #mkdir -p ""${LOGDIR}"". #time seq 0 $((N_SHARDS-1)) | \. # parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" \. # sudo docker run \. # -v /mnt/efs-genome:/mnt/efs-genome \. # gcr.io/deepvariant-docker/deepvariant \. # /opt/deepvariant/bin/make_examples \. # --mode calling \. # --ref ""${REF}"" \. # --reads ""${BAM}"" \. # --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. # --task {}. ## step #2. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". sudo docker run \. -v /mnt/efs-genome:/mnt/efs-genome \. gcr.io/deepvariant-docker/deepvariant \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""${MODEL}"". ```. Can you please help me troubleshoot? I thought it might be something simple, like [this question](https://github.com/google/deepvariant/issues/129). However, that particular solution is not working for me. Thank you very much for your assistance. Sincerely,. Charles",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:13238,deployability,LOG,LOGDIR,13238,"3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. ```. This is the script that I am running DeepVariant:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. BAM=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/82651510240740.mapped.sorted.markdup.realn.recal.bam. MODEL=/mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. ## step #1. LOGDIR=logs. N_SHARDS=4. #mkdir -p ""${LOGDIR}"". #time seq 0 $((N_SHARDS-1)) | \. # parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" \. # sudo docker run \. # -v /mnt/efs-genome:/mnt/efs-genome \. # gcr.io/deepvariant-docker/deepvariant \. # /opt/deepvariant/bin/make_examples \. # --mode calling \. # --ref ""${REF}"" \. # --reads ""${BAM}"" \. # --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. # --task {}. ## step #2. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". sudo docker run \. -v /mnt/efs-genome:/mnt/efs-genome \. gcr.io/deepvariant-docker/deepvariant \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""${MODEL}"". ```. Can you please help me troubleshoot? I thought it might be something simple, like [this question](https://github.com/google/deepvariant/issues/129). However, that particular solution is not working for me. Thank you very much for your assistance. Sincerely,. Charles",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:706,energy efficiency,Cloud,Cloud,706,"DataLossError / Failed precondition when running WES call_variant; Hello,. I have been trying to do some testing of DeepVariant on my own Exome (WES) and WGS data. As a starting point, I was trying to work with calling variants from the .bam file provided for my WES data. I started running from within a Docker container on my local computer but that was taking a long time (and, ultimately, the _make_examples_ step did not run to completion). I started learning more about the AWS options for analysis, and I was able to run the _make_examples_ much quicker (and successfully) on an AWS m5.xlarge ECS instance (although I am admittedly well over the ~25 minutes and $0.20 time/cost mentioned for Google Cloud, just for the _make_examples_, without considering upload/download, long-term storage, etc.). While I was hoping to eventually compare running things on Google Cloud (and I think my experience so far probably helps me ask better questions), I was wondering if you could help me troubleshoot something that I think is probably close to working:. Essentially, I am currently at the **call_variant** step of DeepVariant, with WES data. This is the error message that I am currently receiving:. ```. sudo sh run_deepvariant.sh. I0331 18:31:22.446569 140549764839168 call_variants.py:292] Set KMP_BLOCKTIME to 0. 2019-03-31 18:31:22.486802: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2 AVX512F FMA. 2019-03-31 18:31:22.489180: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. I0331 18:31:22.527594 140549764839168 modeling.py:351] Initializing model with random parameters. W0331 18:31:22.529449 140549764839168 tf_logging.py:125] Using temporary folder as model directory: /tmp/tmpuBleAQ. I0331 18:31:22.529786 140549764839168 tf_logging.py:115] Using config: {'_sav",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:872,energy efficiency,Cloud,Cloud,872,"DataLossError / Failed precondition when running WES call_variant; Hello,. I have been trying to do some testing of DeepVariant on my own Exome (WES) and WGS data. As a starting point, I was trying to work with calling variants from the .bam file provided for my WES data. I started running from within a Docker container on my local computer but that was taking a long time (and, ultimately, the _make_examples_ step did not run to completion). I started learning more about the AWS options for analysis, and I was able to run the _make_examples_ much quicker (and successfully) on an AWS m5.xlarge ECS instance (although I am admittedly well over the ~25 minutes and $0.20 time/cost mentioned for Google Cloud, just for the _make_examples_, without considering upload/download, long-term storage, etc.). While I was hoping to eventually compare running things on Google Cloud (and I think my experience so far probably helps me ask better questions), I was wondering if you could help me troubleshoot something that I think is probably close to working:. Essentially, I am currently at the **call_variant** step of DeepVariant, with WES data. This is the error message that I am currently receiving:. ```. sudo sh run_deepvariant.sh. I0331 18:31:22.446569 140549764839168 call_variants.py:292] Set KMP_BLOCKTIME to 0. 2019-03-31 18:31:22.486802: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2 AVX512F FMA. 2019-03-31 18:31:22.489180: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. I0331 18:31:22.527594 140549764839168 modeling.py:351] Initializing model with random parameters. W0331 18:31:22.529449 140549764839168 tf_logging.py:125] Using temporary folder as model directory: /tmp/tmpuBleAQ. I0331 18:31:22.529786 140549764839168 tf_logging.py:115] Using config: {'_sav",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:1075,energy efficiency,current,currently,1075,"e been trying to do some testing of DeepVariant on my own Exome (WES) and WGS data. As a starting point, I was trying to work with calling variants from the .bam file provided for my WES data. I started running from within a Docker container on my local computer but that was taking a long time (and, ultimately, the _make_examples_ step did not run to completion). I started learning more about the AWS options for analysis, and I was able to run the _make_examples_ much quicker (and successfully) on an AWS m5.xlarge ECS instance (although I am admittedly well over the ~25 minutes and $0.20 time/cost mentioned for Google Cloud, just for the _make_examples_, without considering upload/download, long-term storage, etc.). While I was hoping to eventually compare running things on Google Cloud (and I think my experience so far probably helps me ask better questions), I was wondering if you could help me troubleshoot something that I think is probably close to working:. Essentially, I am currently at the **call_variant** step of DeepVariant, with WES data. This is the error message that I am currently receiving:. ```. sudo sh run_deepvariant.sh. I0331 18:31:22.446569 140549764839168 call_variants.py:292] Set KMP_BLOCKTIME to 0. 2019-03-31 18:31:22.486802: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2 AVX512F FMA. 2019-03-31 18:31:22.489180: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. I0331 18:31:22.527594 140549764839168 modeling.py:351] Initializing model with random parameters. W0331 18:31:22.529449 140549764839168 tf_logging.py:125] Using temporary folder as model directory: /tmp/tmpuBleAQ. I0331 18:31:22.529786 140549764839168 tf_logging.py:115] Using config: {'_save_checkpoints_secs': 1000, '_num_ps_replicas': 0, '_keep_checkpoint_max': 100000",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:1181,energy efficiency,current,currently,1181," was trying to work with calling variants from the .bam file provided for my WES data. I started running from within a Docker container on my local computer but that was taking a long time (and, ultimately, the _make_examples_ step did not run to completion). I started learning more about the AWS options for analysis, and I was able to run the _make_examples_ much quicker (and successfully) on an AWS m5.xlarge ECS instance (although I am admittedly well over the ~25 minutes and $0.20 time/cost mentioned for Google Cloud, just for the _make_examples_, without considering upload/download, long-term storage, etc.). While I was hoping to eventually compare running things on Google Cloud (and I think my experience so far probably helps me ask better questions), I was wondering if you could help me troubleshoot something that I think is probably close to working:. Essentially, I am currently at the **call_variant** step of DeepVariant, with WES data. This is the error message that I am currently receiving:. ```. sudo sh run_deepvariant.sh. I0331 18:31:22.446569 140549764839168 call_variants.py:292] Set KMP_BLOCKTIME to 0. 2019-03-31 18:31:22.486802: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2 AVX512F FMA. 2019-03-31 18:31:22.489180: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. I0331 18:31:22.527594 140549764839168 modeling.py:351] Initializing model with random parameters. W0331 18:31:22.529449 140549764839168 tf_logging.py:125] Using temporary folder as model directory: /tmp/tmpuBleAQ. I0331 18:31:22.529786 140549764839168 tf_logging.py:115] Using config: {'_save_checkpoints_secs': 1000, '_num_ps_replicas': 0, '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.pyth",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:1361,energy efficiency,core,core,1361,"a long time (and, ultimately, the _make_examples_ step did not run to completion). I started learning more about the AWS options for analysis, and I was able to run the _make_examples_ much quicker (and successfully) on an AWS m5.xlarge ECS instance (although I am admittedly well over the ~25 minutes and $0.20 time/cost mentioned for Google Cloud, just for the _make_examples_, without considering upload/download, long-term storage, etc.). While I was hoping to eventually compare running things on Google Cloud (and I think my experience so far probably helps me ask better questions), I was wondering if you could help me troubleshoot something that I think is probably close to working:. Essentially, I am currently at the **call_variant** step of DeepVariant, with WES data. This is the error message that I am currently receiving:. ```. sudo sh run_deepvariant.sh. I0331 18:31:22.446569 140549764839168 call_variants.py:292] Set KMP_BLOCKTIME to 0. 2019-03-31 18:31:22.486802: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2 AVX512F FMA. 2019-03-31 18:31:22.489180: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. I0331 18:31:22.527594 140549764839168 modeling.py:351] Initializing model with random parameters. W0331 18:31:22.529449 140549764839168 tf_logging.py:125] Using temporary folder as model directory: /tmp/tmpuBleAQ. I0331 18:31:22.529786 140549764839168 tf_logging.py:115] Using config: {'_save_checkpoints_secs': 1000, '_num_ps_replicas': 0, '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd40e875e50>, '_model_dir': '/tmp/tmpuBleAQ', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:1406,energy efficiency,CPU,CPU,1406,"les_ step did not run to completion). I started learning more about the AWS options for analysis, and I was able to run the _make_examples_ much quicker (and successfully) on an AWS m5.xlarge ECS instance (although I am admittedly well over the ~25 minutes and $0.20 time/cost mentioned for Google Cloud, just for the _make_examples_, without considering upload/download, long-term storage, etc.). While I was hoping to eventually compare running things on Google Cloud (and I think my experience so far probably helps me ask better questions), I was wondering if you could help me troubleshoot something that I think is probably close to working:. Essentially, I am currently at the **call_variant** step of DeepVariant, with WES data. This is the error message that I am currently receiving:. ```. sudo sh run_deepvariant.sh. I0331 18:31:22.446569 140549764839168 call_variants.py:292] Set KMP_BLOCKTIME to 0. 2019-03-31 18:31:22.486802: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2 AVX512F FMA. 2019-03-31 18:31:22.489180: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. I0331 18:31:22.527594 140549764839168 modeling.py:351] Initializing model with random parameters. W0331 18:31:22.529449 140549764839168 tf_logging.py:125] Using temporary folder as model directory: /tmp/tmpuBleAQ. I0331 18:31:22.529786 140549764839168 tf_logging.py:115] Using config: {'_save_checkpoints_secs': 1000, '_num_ps_replicas': 0, '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd40e875e50>, '_model_dir': '/tmp/tmpuBleAQ', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_session_config'",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:1548,energy efficiency,core,core,1548,"ch quicker (and successfully) on an AWS m5.xlarge ECS instance (although I am admittedly well over the ~25 minutes and $0.20 time/cost mentioned for Google Cloud, just for the _make_examples_, without considering upload/download, long-term storage, etc.). While I was hoping to eventually compare running things on Google Cloud (and I think my experience so far probably helps me ask better questions), I was wondering if you could help me troubleshoot something that I think is probably close to working:. Essentially, I am currently at the **call_variant** step of DeepVariant, with WES data. This is the error message that I am currently receiving:. ```. sudo sh run_deepvariant.sh. I0331 18:31:22.446569 140549764839168 call_variants.py:292] Set KMP_BLOCKTIME to 0. 2019-03-31 18:31:22.486802: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2 AVX512F FMA. 2019-03-31 18:31:22.489180: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. I0331 18:31:22.527594 140549764839168 modeling.py:351] Initializing model with random parameters. W0331 18:31:22.529449 140549764839168 tf_logging.py:125] Using temporary folder as model directory: /tmp/tmpuBleAQ. I0331 18:31:22.529786 140549764839168 tf_logging.py:115] Using config: {'_save_checkpoints_secs': 1000, '_num_ps_replicas': 0, '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd40e875e50>, '_model_dir': '/tmp/tmpuBleAQ', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_tf_random_seed': None, '_save_summ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:1747,energy efficiency,model,modeling,1747,"considering upload/download, long-term storage, etc.). While I was hoping to eventually compare running things on Google Cloud (and I think my experience so far probably helps me ask better questions), I was wondering if you could help me troubleshoot something that I think is probably close to working:. Essentially, I am currently at the **call_variant** step of DeepVariant, with WES data. This is the error message that I am currently receiving:. ```. sudo sh run_deepvariant.sh. I0331 18:31:22.446569 140549764839168 call_variants.py:292] Set KMP_BLOCKTIME to 0. 2019-03-31 18:31:22.486802: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2 AVX512F FMA. 2019-03-31 18:31:22.489180: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. I0331 18:31:22.527594 140549764839168 modeling.py:351] Initializing model with random parameters. W0331 18:31:22.529449 140549764839168 tf_logging.py:125] Using temporary folder as model directory: /tmp/tmpuBleAQ. I0331 18:31:22.529786 140549764839168 tf_logging.py:115] Using config: {'_save_checkpoints_secs': 1000, '_num_ps_replicas': 0, '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd40e875e50>, '_model_dir': '/tmp/tmpuBleAQ', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_trai",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:1777,energy efficiency,model,model,1777,"long-term storage, etc.). While I was hoping to eventually compare running things on Google Cloud (and I think my experience so far probably helps me ask better questions), I was wondering if you could help me troubleshoot something that I think is probably close to working:. Essentially, I am currently at the **call_variant** step of DeepVariant, with WES data. This is the error message that I am currently receiving:. ```. sudo sh run_deepvariant.sh. I0331 18:31:22.446569 140549764839168 call_variants.py:292] Set KMP_BLOCKTIME to 0. 2019-03-31 18:31:22.486802: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2 AVX512F FMA. 2019-03-31 18:31:22.489180: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. I0331 18:31:22.527594 140549764839168 modeling.py:351] Initializing model with random parameters. W0331 18:31:22.529449 140549764839168 tf_logging.py:125] Using temporary folder as model directory: /tmp/tmpuBleAQ. I0331 18:31:22.529786 140549764839168 tf_logging.py:115] Using config: {'_save_checkpoints_secs': 1000, '_num_ps_replicas': 0, '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd40e875e50>, '_model_dir': '/tmp/tmpuBleAQ', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_train_distribute': None, '_master",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:1890,energy efficiency,model,model,1890," experience so far probably helps me ask better questions), I was wondering if you could help me troubleshoot something that I think is probably close to working:. Essentially, I am currently at the **call_variant** step of DeepVariant, with WES data. This is the error message that I am currently receiving:. ```. sudo sh run_deepvariant.sh. I0331 18:31:22.446569 140549764839168 call_variants.py:292] Set KMP_BLOCKTIME to 0. 2019-03-31 18:31:22.486802: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2 AVX512F FMA. 2019-03-31 18:31:22.489180: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. I0331 18:31:22.527594 140549764839168 modeling.py:351] Initializing model with random parameters. W0331 18:31:22.529449 140549764839168 tf_logging.py:125] Using temporary folder as model directory: /tmp/tmpuBleAQ. I0331 18:31:22.529786 140549764839168 tf_logging.py:115] Using config: {'_save_checkpoints_secs': 1000, '_num_ps_replicas': 0, '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd40e875e50>, '_model_dir': '/tmp/tmpuBleAQ', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_train_distribute': None, '_master': ''}. I0331 18:31:22.529983 140549764839168 call_variants.py:350] Writing calls to /mnt/efs-genome/Charles_Huma",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:3408,energy efficiency,core,core,3408,"allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_train_distribute': None, '_master': ''}. I0331 18:31:22.529983 140549764839168 call_variants.py:350] Writing calls to /mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant/call_variants_output.tfrecord.gz. I0331 18:31:22.610783 140549764839168 tf_logging.py:115] Calling model_fn. I0331 18:31:24.905831 140549764839168 tf_logging.py:115] Done calling model_fn. I0331 18:31:25.996718 140549764839168 tf_logging.py:115] Graph was finalized. I0331 18:31:26.008368 140549764839168 tf_logging.py:115] Restoring parameters from /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. 2019-03-31 18:31:26.447697: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? 2019-03-31 18:31:26.450035: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? 2019-03-31 18:31:26.450061: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:172 : Data loss: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precond",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:3800,energy efficiency,core,core,3800,".529983 140549764839168 call_variants.py:350] Writing calls to /mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant/call_variants_output.tfrecord.gz. I0331 18:31:22.610783 140549764839168 tf_logging.py:115] Calling model_fn. I0331 18:31:24.905831 140549764839168 tf_logging.py:115] Done calling model_fn. I0331 18:31:25.996718 140549764839168 tf_logging.py:115] Graph was finalized. I0331 18:31:26.008368 140549764839168 tf_logging.py:115] Restoring parameters from /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. 2019-03-31 18:31:26.447697: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? 2019-03-31 18:31:26.450035: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? 2019-03-31 18:31:26.450061: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:172 : Data loss: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>. tf.app.run(). File ""/root/.local/l",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:4192,energy efficiency,core,core,4192,"alized. I0331 18:31:26.008368 140549764839168 tf_logging.py:115] Restoring parameters from /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. 2019-03-31 18:31:26.447697: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? 2019-03-31 18:31:26.450035: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? 2019-03-31 18:31:26.450061: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:172 : Data loss: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>. tf.app.run(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 401, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 357, in call_variants. prediction = next(predict",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:5169,energy efficiency,predict,prediction,5169,"061: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:172 : Data loss: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>. tf.app.run(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 401, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 357, in call_variants. prediction = next(predictions). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 567, in predict. hooks=all_hooks) as mon_sess:. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 921, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 643, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1107, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1112, in _create_session. return self._sess_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session. self.t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:5187,energy efficiency,predict,predictions,5187,"ore/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:172 : Data loss: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>. tf.app.run(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 401, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 357, in call_variants. prediction = next(predictions). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 567, in predict. hooks=all_hooks) as mon_sess:. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 921, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 643, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1107, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1112, in _create_session. return self._sess_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session. self.tf_sess = self._sess",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:5266,energy efficiency,estimat,estimator,5266,"172 : Data loss: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>. tf.app.run(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 401, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 357, in call_variants. prediction = next(predictions). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 567, in predict. hooks=all_hooks) as mon_sess:. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 921, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 643, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1107, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1112, in _create_session. return self._sess_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:5276,energy efficiency,estimat,estimator,5276," loss: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>. tf.app.run(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 401, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 357, in call_variants. prediction = next(predictions). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 567, in predict. hooks=all_hooks) as mon_sess:. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 921, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 643, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1107, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1112, in _create_session. return self._sess_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:5304,energy efficiency,predict,predict,5304," file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>. tf.app.run(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 401, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 357, in call_variants. prediction = next(predictions). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 567, in predict. hooks=all_hooks) as mon_sess:. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 921, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 643, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1107, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1112, in _create_session. return self._sess_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_s",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:7984,energy efficiency,CPU,CPU,7984,"ile ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1110, in _run. feed_dict_tensor, options, run_metadata). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1286, in _do_run. run_metadata). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1308, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.DataLossError: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. Caused by op u'save_1/RestoreV2', defined at:. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>. tf.app.run(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 401, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 357, in call_variants. prediction = next(predictions). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 567, in predict. hooks=all_hooks) as mon_sess:. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 921, in __init__. stop_grace_period_secs=stop_grace_period",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:8651,energy efficiency,predict,prediction,8651,"s_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. Caused by op u'save_1/RestoreV2', defined at:. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>. tf.app.run(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 401, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 357, in call_variants. prediction = next(predictions). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 567, in predict. hooks=all_hooks) as mon_sess:. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 921, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 643, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1107, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1112, in _create_session. return self._sess_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session. self.t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:8669,energy efficiency,predict,predictions,8669,"rectory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. Caused by op u'save_1/RestoreV2', defined at:. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>. tf.app.run(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 401, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 357, in call_variants. prediction = next(predictions). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 567, in predict. hooks=all_hooks) as mon_sess:. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 921, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 643, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1107, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1112, in _create_session. return self._sess_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session. self.tf_sess = self._sess",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:8748,energy efficiency,estimat,estimator,8748," different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. Caused by op u'save_1/RestoreV2', defined at:. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>. tf.app.run(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 401, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 357, in call_variants. prediction = next(predictions). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 567, in predict. hooks=all_hooks) as mon_sess:. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 921, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 643, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1107, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1112, in _create_session. return self._sess_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:8758,energy efficiency,estimat,estimator,8758," restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. Caused by op u'save_1/RestoreV2', defined at:. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>. tf.app.run(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 401, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 357, in call_variants. prediction = next(predictions). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 567, in predict. hooks=all_hooks) as mon_sess:. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 921, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 643, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1107, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1112, in _create_session. return self._sess_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:8786,energy efficiency,predict,predict,8786," save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. Caused by op u'save_1/RestoreV2', defined at:. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>. tf.app.run(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 401, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 357, in call_variants. prediction = next(predictions). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 567, in predict. hooks=all_hooks) as mon_sess:. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 921, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 643, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1107, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1112, in _create_session. return self._sess_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_s",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:12597,energy efficiency,CPU,CPU,12597,"def=op_def). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func. return func(*args, **kwargs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 3272, in create_op. op_def=op_def). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1768, in __init__. self._traceback = tf_stack.extract_stack(). DataLossError (see above for traceback): Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. ```. This is the script that I am running DeepVariant:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. BAM=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/82651510240740.mapped.sorted.markdup.realn.recal.bam. MODEL=/mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. ## step #1. LOGDIR=logs. N_SHARDS=4. #mkdir -p ""${LOGDIR}"". #time seq 0 $((N_SHARDS-1)) | \. # parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" \. # sudo docker run \. # -v /mnt/efs-genome:/mnt/efs-genome \. # gcr.io/deepvariant-docker/deepvariant \. # /opt/deepvariant/bin/make_examples \. # --mode calling \. # --ref ""${REF}"" \. # --reads ""${BAM}"" \. # --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. # --task {}. ## step #2. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_out",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:12997,energy efficiency,MODEL,MODEL,12997," = tf_stack.extract_stack(). DataLossError (see above for traceback): Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. ```. This is the script that I am running DeepVariant:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. BAM=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/82651510240740.mapped.sorted.markdup.realn.recal.bam. MODEL=/mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. ## step #1. LOGDIR=logs. N_SHARDS=4. #mkdir -p ""${LOGDIR}"". #time seq 0 $((N_SHARDS-1)) | \. # parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" \. # sudo docker run \. # -v /mnt/efs-genome:/mnt/efs-genome \. # gcr.io/deepvariant-docker/deepvariant \. # /opt/deepvariant/bin/make_examples \. # --mode calling \. # --ref ""${REF}"" \. # --reads ""${BAM}"" \. # --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. # --task {}. ## step #2. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". sudo docker run \. -v /mnt/efs-genome:/mnt/efs-genome \. gcr.io/deepvariant-docker/deepvariant \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""${MODEL}"". ```. Can you please help me troubleshoot? I thought it might be something simple, like [this question](https://github.co",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:13871,energy efficiency,MODEL,MODEL,13871,"3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. ```. This is the script that I am running DeepVariant:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. BAM=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/82651510240740.mapped.sorted.markdup.realn.recal.bam. MODEL=/mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. ## step #1. LOGDIR=logs. N_SHARDS=4. #mkdir -p ""${LOGDIR}"". #time seq 0 $((N_SHARDS-1)) | \. # parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" \. # sudo docker run \. # -v /mnt/efs-genome:/mnt/efs-genome \. # gcr.io/deepvariant-docker/deepvariant \. # /opt/deepvariant/bin/make_examples \. # --mode calling \. # --ref ""${REF}"" \. # --reads ""${BAM}"" \. # --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. # --task {}. ## step #2. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". sudo docker run \. -v /mnt/efs-genome:/mnt/efs-genome \. gcr.io/deepvariant-docker/deepvariant \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""${MODEL}"". ```. Can you please help me troubleshoot? I thought it might be something simple, like [this question](https://github.com/google/deepvariant/issues/129). However, that particular solution is not working for me. Thank you very much for your assistance. Sincerely,. Charles",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:828,integrability,event,eventually,828,"DataLossError / Failed precondition when running WES call_variant; Hello,. I have been trying to do some testing of DeepVariant on my own Exome (WES) and WGS data. As a starting point, I was trying to work with calling variants from the .bam file provided for my WES data. I started running from within a Docker container on my local computer but that was taking a long time (and, ultimately, the _make_examples_ step did not run to completion). I started learning more about the AWS options for analysis, and I was able to run the _make_examples_ much quicker (and successfully) on an AWS m5.xlarge ECS instance (although I am admittedly well over the ~25 minutes and $0.20 time/cost mentioned for Google Cloud, just for the _make_examples_, without considering upload/download, long-term storage, etc.). While I was hoping to eventually compare running things on Google Cloud (and I think my experience so far probably helps me ask better questions), I was wondering if you could help me troubleshoot something that I think is probably close to working:. Essentially, I am currently at the **call_variant** step of DeepVariant, with WES data. This is the error message that I am currently receiving:. ```. sudo sh run_deepvariant.sh. I0331 18:31:22.446569 140549764839168 call_variants.py:292] Set KMP_BLOCKTIME to 0. 2019-03-31 18:31:22.486802: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2 AVX512F FMA. 2019-03-31 18:31:22.489180: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. I0331 18:31:22.527594 140549764839168 modeling.py:351] Initializing model with random parameters. W0331 18:31:22.529449 140549764839168 tf_logging.py:125] Using temporary folder as model directory: /tmp/tmpuBleAQ. I0331 18:31:22.529786 140549764839168 tf_logging.py:115] Using config: {'_sav",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:1163,integrability,messag,message,1163,"a starting point, I was trying to work with calling variants from the .bam file provided for my WES data. I started running from within a Docker container on my local computer but that was taking a long time (and, ultimately, the _make_examples_ step did not run to completion). I started learning more about the AWS options for analysis, and I was able to run the _make_examples_ much quicker (and successfully) on an AWS m5.xlarge ECS instance (although I am admittedly well over the ~25 minutes and $0.20 time/cost mentioned for Google Cloud, just for the _make_examples_, without considering upload/download, long-term storage, etc.). While I was hoping to eventually compare running things on Google Cloud (and I think my experience so far probably helps me ask better questions), I was wondering if you could help me troubleshoot something that I think is probably close to working:. Essentially, I am currently at the **call_variant** step of DeepVariant, with WES data. This is the error message that I am currently receiving:. ```. sudo sh run_deepvariant.sh. I0331 18:31:22.446569 140549764839168 call_variants.py:292] Set KMP_BLOCKTIME to 0. 2019-03-31 18:31:22.486802: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2 AVX512F FMA. 2019-03-31 18:31:22.489180: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. I0331 18:31:22.527594 140549764839168 modeling.py:351] Initializing model with random parameters. W0331 18:31:22.529449 140549764839168 tf_logging.py:125] Using temporary folder as model directory: /tmp/tmpuBleAQ. I0331 18:31:22.529786 140549764839168 tf_logging.py:115] Using config: {'_save_checkpoints_secs': 1000, '_num_ps_replicas': 0, '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:7392,integrability,messag,message,7392,"ocal/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session. config=config). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 195, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1538, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 887, in run. run_metadata_ptr). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1110, in _run. feed_dict_tensor, options, run_metadata). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1286, in _do_run. run_metadata). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1308, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.DataLossError: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. Caused by op u'save_1/RestoreV2', defined at:. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>. tf.app.run(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:1163,interoperability,messag,message,1163,"a starting point, I was trying to work with calling variants from the .bam file provided for my WES data. I started running from within a Docker container on my local computer but that was taking a long time (and, ultimately, the _make_examples_ step did not run to completion). I started learning more about the AWS options for analysis, and I was able to run the _make_examples_ much quicker (and successfully) on an AWS m5.xlarge ECS instance (although I am admittedly well over the ~25 minutes and $0.20 time/cost mentioned for Google Cloud, just for the _make_examples_, without considering upload/download, long-term storage, etc.). While I was hoping to eventually compare running things on Google Cloud (and I think my experience so far probably helps me ask better questions), I was wondering if you could help me troubleshoot something that I think is probably close to working:. Essentially, I am currently at the **call_variant** step of DeepVariant, with WES data. This is the error message that I am currently receiving:. ```. sudo sh run_deepvariant.sh. I0331 18:31:22.446569 140549764839168 call_variants.py:292] Set KMP_BLOCKTIME to 0. 2019-03-31 18:31:22.486802: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2 AVX512F FMA. 2019-03-31 18:31:22.489180: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. I0331 18:31:22.527594 140549764839168 modeling.py:351] Initializing model with random parameters. W0331 18:31:22.529449 140549764839168 tf_logging.py:125] Using temporary folder as model directory: /tmp/tmpuBleAQ. I0331 18:31:22.529786 140549764839168 tf_logging.py:115] Using config: {'_save_checkpoints_secs': 1000, '_num_ps_replicas': 0, '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:1366,interoperability,platform,platform,1366,"time (and, ultimately, the _make_examples_ step did not run to completion). I started learning more about the AWS options for analysis, and I was able to run the _make_examples_ much quicker (and successfully) on an AWS m5.xlarge ECS instance (although I am admittedly well over the ~25 minutes and $0.20 time/cost mentioned for Google Cloud, just for the _make_examples_, without considering upload/download, long-term storage, etc.). While I was hoping to eventually compare running things on Google Cloud (and I think my experience so far probably helps me ask better questions), I was wondering if you could help me troubleshoot something that I think is probably close to working:. Essentially, I am currently at the **call_variant** step of DeepVariant, with WES data. This is the error message that I am currently receiving:. ```. sudo sh run_deepvariant.sh. I0331 18:31:22.446569 140549764839168 call_variants.py:292] Set KMP_BLOCKTIME to 0. 2019-03-31 18:31:22.486802: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2 AVX512F FMA. 2019-03-31 18:31:22.489180: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. I0331 18:31:22.527594 140549764839168 modeling.py:351] Initializing model with random parameters. W0331 18:31:22.529449 140549764839168 tf_logging.py:125] Using temporary folder as model directory: /tmp/tmpuBleAQ. I0331 18:31:22.529786 140549764839168 tf_logging.py:115] Using config: {'_save_checkpoints_secs': 1000, '_num_ps_replicas': 0, '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd40e875e50>, '_model_dir': '/tmp/tmpuBleAQ', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 1000",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:3702,interoperability,format,format,3702,"_master': '', '_eval_distribute': None, '_train_distribute': None, '_master': ''}. I0331 18:31:22.529983 140549764839168 call_variants.py:350] Writing calls to /mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant/call_variants_output.tfrecord.gz. I0331 18:31:22.610783 140549764839168 tf_logging.py:115] Calling model_fn. I0331 18:31:24.905831 140549764839168 tf_logging.py:115] Done calling model_fn. I0331 18:31:25.996718 140549764839168 tf_logging.py:115] Graph was finalized. I0331 18:31:26.008368 140549764839168 tf_logging.py:115] Restoring parameters from /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. 2019-03-31 18:31:26.447697: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? 2019-03-31 18:31:26.450035: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? 2019-03-31 18:31:26.450061: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:172 : Data loss: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_dee",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:4094,interoperability,format,format,4094,"15] Done calling model_fn. I0331 18:31:25.996718 140549764839168 tf_logging.py:115] Graph was finalized. I0331 18:31:26.008368 140549764839168 tf_logging.py:115] Restoring parameters from /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. 2019-03-31 18:31:26.447697: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? 2019-03-31 18:31:26.450035: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? 2019-03-31 18:31:26.450061: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:172 : Data loss: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>. tf.app.run(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 401, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:4556,interoperability,format,format,4556,"on: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? 2019-03-31 18:31:26.450035: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? 2019-03-31 18:31:26.450061: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:172 : Data loss: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>. tf.app.run(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 401, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 357, in call_variants. prediction = next(predictions). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 567, in predict. hooks=all_hooks) as mon_sess:. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 921, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/root/.local/lib/python2.7/site-packages/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:4847,interoperability,platform,platform,4847," /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? 2019-03-31 18:31:26.450061: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:172 : Data loss: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>. tf.app.run(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 401, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 357, in call_variants. prediction = next(predictions). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 567, in predict. hooks=all_hooks) as mon_sess:. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 921, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 643, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1107, in __init__. _WrappedSession.__init__(self, self._",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:7392,interoperability,messag,message,7392,"ocal/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session. config=config). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 195, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1538, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 887, in run. run_metadata_ptr). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1110, in _run. feed_dict_tensor, options, run_metadata). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1286, in _do_run. run_metadata). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1308, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.DataLossError: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. Caused by op u'save_1/RestoreV2', defined at:. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>. tf.app.run(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:7725,interoperability,format,format,7725,"ython2.7/site-packages/tensorflow/python/training/saver.py"", line 1538, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 887, in run. run_metadata_ptr). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1110, in _run. feed_dict_tensor, options, run_metadata). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1286, in _do_run. run_metadata). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1308, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.DataLossError: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. Caused by op u'save_1/RestoreV2', defined at:. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>. tf.app.run(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 401, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 357, in call_variants. prediction = next(predictions). File ""/root/.local/lib/python2.7/site-package",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:8329,interoperability,platform,platform,8329,"n.py"", line 1308, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.DataLossError: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. Caused by op u'save_1/RestoreV2', defined at:. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>. tf.app.run(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 401, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 357, in call_variants. prediction = next(predictions). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 567, in predict. hooks=all_hooks) as mon_sess:. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 921, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 643, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1107, in __init__. _WrappedSession.__init__(self, self._",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:12338,interoperability,format,format,12338,"es/tensorflow/python/ops/gen_io_ops.py"", line 1466, in restore_v2. shape_and_slices=shape_and_slices, dtypes=dtypes, name=name). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper. op_def=op_def). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func. return func(*args, **kwargs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 3272, in create_op. op_def=op_def). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1768, in __init__. self._traceback = tf_stack.extract_stack(). DataLossError (see above for traceback): Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. ```. This is the script that I am running DeepVariant:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. BAM=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/82651510240740.mapped.sorted.markdup.realn.recal.bam. MODEL=/mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. ## step #1. LOGDIR=logs. N_SHARDS=4. #mkdir -p ""${LOGDIR}"". #time seq 0 $((N_SHARDS-1)) | \. # parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" \. # sudo docker run \. # -v /mnt/efs-genome:/mnt/efs-genome \. # gcr.io/deepvariant-docker/de",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:1795,modifiability,paramet,parameters,1795,"tc.). While I was hoping to eventually compare running things on Google Cloud (and I think my experience so far probably helps me ask better questions), I was wondering if you could help me troubleshoot something that I think is probably close to working:. Essentially, I am currently at the **call_variant** step of DeepVariant, with WES data. This is the error message that I am currently receiving:. ```. sudo sh run_deepvariant.sh. I0331 18:31:22.446569 140549764839168 call_variants.py:292] Set KMP_BLOCKTIME to 0. 2019-03-31 18:31:22.486802: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2 AVX512F FMA. 2019-03-31 18:31:22.489180: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. I0331 18:31:22.527594 140549764839168 modeling.py:351] Initializing model with random parameters. W0331 18:31:22.529449 140549764839168 tf_logging.py:125] Using temporary folder as model directory: /tmp/tmpuBleAQ. I0331 18:31:22.529786 140549764839168 tf_logging.py:115] Using config: {'_save_checkpoints_secs': 1000, '_num_ps_replicas': 0, '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd40e875e50>, '_model_dir': '/tmp/tmpuBleAQ', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_train_distribute': None, '_master': ''}. I0331 18:31:",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:3269,modifiability,paramet,parameters,3269,"eAQ', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_train_distribute': None, '_master': ''}. I0331 18:31:22.529983 140549764839168 call_variants.py:350] Writing calls to /mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant/call_variants_output.tfrecord.gz. I0331 18:31:22.610783 140549764839168 tf_logging.py:115] Calling model_fn. I0331 18:31:24.905831 140549764839168 tf_logging.py:115] Done calling model_fn. I0331 18:31:25.996718 140549764839168 tf_logging.py:115] Graph was finalized. I0331 18:31:26.008368 140549764839168 tf_logging.py:115] Restoring parameters from /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. 2019-03-31 18:31:26.447697: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? 2019-03-31 18:31:26.450035: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? 2019-03-31 18:31:26.450061: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:172",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:4759,modifiability,modul,module,4759,"9-03-31 18:31:26.450035: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? 2019-03-31 18:31:26.450061: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:172 : Data loss: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>. tf.app.run(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 401, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 357, in call_variants. prediction = next(predictions). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 567, in predict. hooks=all_hooks) as mon_sess:. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 921, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 643, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/trai",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:4820,modifiability,pac,packages,4820,"eader.cc:95] Could not open /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? 2019-03-31 18:31:26.450061: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:172 : Data loss: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>. tf.app.run(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 401, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 357, in call_variants. prediction = next(predictions). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 567, in predict. hooks=all_hooks) as mon_sess:. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 921, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 643, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1107, in __init__. _WrappedSe",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:5239,modifiability,pac,packages,5239,"d at save_restore_tensor.cc:172 : Data loss: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>. tf.app.run(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 401, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 357, in call_variants. prediction = next(predictions). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 567, in predict. hooks=all_hooks) as mon_sess:. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 921, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 643, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1107, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1112, in _create_session. return self._sess_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/root/.local/l",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:5382,modifiability,pac,packages,5382,"standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>. tf.app.run(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 401, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 357, in call_variants. prediction = next(predictions). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 567, in predict. hooks=all_hooks) as mon_sess:. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 921, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 643, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1107, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1112, in _create_session. return self._sess_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 566, in create_session. init_fn=self._scaffold.init_fn). File",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:5550,modifiability,pac,packages,5550,"e format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>. tf.app.run(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 401, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 357, in call_variants. prediction = next(predictions). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 567, in predict. hooks=all_hooks) as mon_sess:. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 921, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 643, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1107, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1112, in _create_session. return self._sess_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 566, in create_session. init_fn=self._scaffold.init_fn). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session. config=config). File ""/root/.local/lib/python2.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:5731,modifiability,pac,packages,5731,"ants.py"", line 411, in <module>. tf.app.run(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 401, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 357, in call_variants. prediction = next(predictions). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 567, in predict. hooks=all_hooks) as mon_sess:. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 921, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 643, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1107, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1112, in _create_session. return self._sess_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 566, in create_session. init_fn=self._scaffold.init_fn). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session. config=config). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 195, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/root/.local/lib/python2.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:5908,modifiability,pac,packages,5908,"/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 401, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 357, in call_variants. prediction = next(predictions). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 567, in predict. hooks=all_hooks) as mon_sess:. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 921, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 643, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1107, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1112, in _create_session. return self._sess_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 566, in create_session. init_fn=self._scaffold.init_fn). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session. config=config). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 195, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1538, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/root/.local/lib/python2.7/site-packages/te",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:6080,modifiability,pac,packages,6080,"es/com_google_deepvariant/deepvariant/call_variants.py"", line 357, in call_variants. prediction = next(predictions). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 567, in predict. hooks=all_hooks) as mon_sess:. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 921, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 643, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1107, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1112, in _create_session. return self._sess_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 566, in create_session. init_fn=self._scaffold.init_fn). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session. config=config). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 195, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1538, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 887, in run. run_metadata_ptr). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1110, in ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:6261,modifiability,pac,packages,6261,"/estimator/estimator.py"", line 567, in predict. hooks=all_hooks) as mon_sess:. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 921, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 643, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1107, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1112, in _create_session. return self._sess_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 566, in create_session. init_fn=self._scaffold.init_fn). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session. config=config). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 195, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1538, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 887, in run. run_metadata_ptr). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1110, in _run. feed_dict_tensor, options, run_metadata). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1286, in _do_run. run_metadata). File ""/roo",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:6420,modifiability,pac,packages,6420,"red_session.py"", line 921, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 643, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1107, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1112, in _create_session. return self._sess_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 566, in create_session. init_fn=self._scaffold.init_fn). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session. config=config). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 195, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1538, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 887, in run. run_metadata_ptr). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1110, in _run. feed_dict_tensor, options, run_metadata). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1286, in _do_run. run_metadata). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1308, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.fram",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:6561,modifiability,pac,packages,6561,"flow/python/training/monitored_session.py"", line 643, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1107, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1112, in _create_session. return self._sess_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 566, in create_session. init_fn=self._scaffold.init_fn). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session. config=config). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 195, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1538, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 887, in run. run_metadata_ptr). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1110, in _run. feed_dict_tensor, options, run_metadata). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1286, in _do_run. run_metadata). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1308, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.DataLossError: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:6742,modifiability,pac,packages,6742,"flow/python/training/monitored_session.py"", line 1107, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1112, in _create_session. return self._sess_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 566, in create_session. init_fn=self._scaffold.init_fn). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session. config=config). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 195, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1538, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 887, in run. run_metadata_ptr). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1110, in _run. feed_dict_tensor, options, run_metadata). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1286, in _do_run. run_metadata). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1308, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.DataLossError: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:6901,modifiability,pac,packages,6901,"te-packages/tensorflow/python/training/monitored_session.py"", line 1112, in _create_session. return self._sess_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 566, in create_session. init_fn=self._scaffold.init_fn). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session. config=config). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 195, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1538, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 887, in run. run_metadata_ptr). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1110, in _run. feed_dict_tensor, options, run_metadata). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1286, in _do_run. run_metadata). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1308, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.DataLossError: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:7023,modifiability,pac,packages,7023,"ate_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 566, in create_session. init_fn=self._scaffold.init_fn). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session. config=config). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 195, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1538, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 887, in run. run_metadata_ptr). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1110, in _run. feed_dict_tensor, options, run_metadata). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1286, in _do_run. run_metadata). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1308, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.DataLossError: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/Resto",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:7170,modifiability,pac,packages,7170,"f_sess = self._session_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 566, in create_session. init_fn=self._scaffold.init_fn). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session. config=config). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 195, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1538, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 887, in run. run_metadata_ptr). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1110, in _run. feed_dict_tensor, options, run_metadata). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1286, in _do_run. run_metadata). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1308, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.DataLossError: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. Caused by op u'save_1/RestoreV2', defined at:. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/c",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:7293,modifiability,pac,packages,7293,"monitored_session.py"", line 566, in create_session. init_fn=self._scaffold.init_fn). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session. config=config). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 195, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1538, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 887, in run. run_metadata_ptr). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1110, in _run. feed_dict_tensor, options, run_metadata). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1286, in _do_run. run_metadata). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1308, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.DataLossError: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. Caused by op u'save_1/RestoreV2', defined at:. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>. tf.app.run(). File ""/root/.local/lib/python2.7/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:8241,modifiability,modul,module,8241,"metadata). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1308, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.DataLossError: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. Caused by op u'save_1/RestoreV2', defined at:. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>. tf.app.run(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 401, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 357, in call_variants. prediction = next(predictions). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 567, in predict. hooks=all_hooks) as mon_sess:. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 921, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 643, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/trai",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:8302,modifiability,pac,packages,8302,"orflow/python/client/session.py"", line 1308, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.DataLossError: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. Caused by op u'save_1/RestoreV2', defined at:. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>. tf.app.run(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 401, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 357, in call_variants. prediction = next(predictions). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 567, in predict. hooks=all_hooks) as mon_sess:. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 921, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 643, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1107, in __init__. _WrappedSe",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:8721,modifiability,pac,packages,8721,"format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. Caused by op u'save_1/RestoreV2', defined at:. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>. tf.app.run(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 401, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 357, in call_variants. prediction = next(predictions). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 567, in predict. hooks=all_hooks) as mon_sess:. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 921, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 643, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1107, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1112, in _create_session. return self._sess_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/root/.local/l",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:8864,modifiability,pac,packages,8864,", DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. Caused by op u'save_1/RestoreV2', defined at:. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>. tf.app.run(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 401, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 357, in call_variants. prediction = next(predictions). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 567, in predict. hooks=all_hooks) as mon_sess:. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 921, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 643, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1107, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1112, in _create_session. return self._sess_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 557, in create_session. self._scaffold.finalize(). File ""/roo",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:9032,modifiability,pac,packages,9032,"or_names, save_1/RestoreV2/shape_and_slices)]]. Caused by op u'save_1/RestoreV2', defined at:. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>. tf.app.run(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 401, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 357, in call_variants. prediction = next(predictions). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 567, in predict. hooks=all_hooks) as mon_sess:. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 921, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 643, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1107, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1112, in _create_session. return self._sess_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 557, in create_session. self._scaffold.finalize(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 213, in finalize. self._saver = training_saver._get_saver_or_default() # pyl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:9213,modifiability,pac,packages,9213,"ants.py"", line 411, in <module>. tf.app.run(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 401, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 357, in call_variants. prediction = next(predictions). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 567, in predict. hooks=all_hooks) as mon_sess:. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 921, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 643, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1107, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1112, in _create_session. return self._sess_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 557, in create_session. self._scaffold.finalize(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 213, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 878, in _get_saver_or_default. saver = Saver(sharded=True, a",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:9390,modifiability,pac,packages,9390,"/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 401, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 357, in call_variants. prediction = next(predictions). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 567, in predict. hooks=all_hooks) as mon_sess:. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 921, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 643, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1107, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1112, in _create_session. return self._sess_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 557, in create_session. self._scaffold.finalize(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 213, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 878, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1094, in __init__. self.build(). File ""/root/.local/lib/python2.7/sit",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:9562,modifiability,pac,packages,9562,"es/com_google_deepvariant/deepvariant/call_variants.py"", line 357, in call_variants. prediction = next(predictions). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 567, in predict. hooks=all_hooks) as mon_sess:. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 921, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 643, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1107, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1112, in _create_session. return self._sess_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 557, in create_session. self._scaffold.finalize(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 213, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 878, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1094, in __init__. self.build(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1106, in build. self._build(self._filename, build_save=True, build_restore=True). File ""/root/.local/lib/python2.7/sit",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:9743,modifiability,pac,packages,9743,"/estimator/estimator.py"", line 567, in predict. hooks=all_hooks) as mon_sess:. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 921, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 643, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1107, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1112, in _create_session. return self._sess_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 557, in create_session. self._scaffold.finalize(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 213, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 878, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1094, in __init__. self.build(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1106, in build. self._build(self._filename, build_save=True, build_restore=True). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1143, in _build. build_save=build_save, build_restore=build_restore). File ""/root/.local/lib/python2.7/site-packages/tensorflow",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:9896,modifiability,pac,packages,9896,"monitored_session.py"", line 921, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 643, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1107, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1112, in _create_session. return self._sess_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 557, in create_session. self._scaffold.finalize(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 213, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 878, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1094, in __init__. self.build(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1106, in build. self._build(self._filename, build_save=True, build_restore=True). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1143, in _build. build_save=build_save, build_restore=build_restore). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 781, in _build_internal. restore_sequentially, reshape). File ""/root/.local/lib/python2.7/site-packages/tensorflow/pytho",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:10105,modifiability,pac,packages,10105,"t__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1107, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1112, in _create_session. return self._sess_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 557, in create_session. self._scaffold.finalize(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 213, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 878, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1094, in __init__. self.build(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1106, in build. self._build(self._filename, build_save=True, build_restore=True). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1143, in _build. build_save=build_save, build_restore=build_restore). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 781, in _build_internal. restore_sequentially, reshape). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 459, in _AddShardedRestoreOps. name=""restore_shard"")). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 406, in _AddRestoreOps. restore_seque",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:10273,modifiability,pac,packages,10273,"1107, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1112, in _create_session. return self._sess_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 557, in create_session. self._scaffold.finalize(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 213, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 878, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1094, in __init__. self.build(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1106, in build. self._build(self._filename, build_save=True, build_restore=True). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1143, in _build. build_save=build_save, build_restore=build_restore). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 781, in _build_internal. restore_sequentially, reshape). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 459, in _AddShardedRestoreOps. name=""restore_shard"")). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 406, in _AddRestoreOps. restore_sequentially). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 854, in bulk_restore. return io_ops.restore_v2(filename_tensor, name",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:10396,modifiability,pac,packages,10396,"ensorflow/python/training/monitored_session.py"", line 1112, in _create_session. return self._sess_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 557, in create_session. self._scaffold.finalize(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 213, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 878, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1094, in __init__. self.build(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1106, in build. self._build(self._filename, build_save=True, build_restore=True). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1143, in _build. build_save=build_save, build_restore=build_restore). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 781, in _build_internal. restore_sequentially, reshape). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 459, in _AddShardedRestoreOps. name=""restore_shard"")). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 406, in _AddRestoreOps. restore_sequentially). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 854, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 1466, in rest",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:10568,modifiability,pac,packages,10568,"ensorflow/python/training/monitored_session.py"", line 800, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 557, in create_session. self._scaffold.finalize(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 213, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 878, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1094, in __init__. self.build(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1106, in build. self._build(self._filename, build_save=True, build_restore=True). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1143, in _build. build_save=build_save, build_restore=build_restore). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 781, in _build_internal. restore_sequentially, reshape). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 459, in _AddShardedRestoreOps. name=""restore_shard"")). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 406, in _AddRestoreOps. restore_sequentially). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 854, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 1466, in restore_v2. shape_and_slices=shape_and_slices, dtypes=dtypes, name=name). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 78",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:10728,modifiability,pac,packages,10728,"on2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 557, in create_session. self._scaffold.finalize(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 213, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 878, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1094, in __init__. self.build(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1106, in build. self._build(self._filename, build_save=True, build_restore=True). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1143, in _build. build_save=build_save, build_restore=build_restore). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 781, in _build_internal. restore_sequentially, reshape). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 459, in _AddShardedRestoreOps. name=""restore_shard"")). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 406, in _AddRestoreOps. restore_sequentially). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 854, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 1466, in restore_v2. shape_and_slices=shape_and_slices, dtypes=dtypes, name=name). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper. op_def=op_def). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func. return fun",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:10875,modifiability,pac,packages,10875,"b/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 213, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 878, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1094, in __init__. self.build(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1106, in build. self._build(self._filename, build_save=True, build_restore=True). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1143, in _build. build_save=build_save, build_restore=build_restore). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 781, in _build_internal. restore_sequentially, reshape). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 459, in _AddShardedRestoreOps. name=""restore_shard"")). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 406, in _AddRestoreOps. restore_sequentially). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 854, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 1466, in restore_v2. shape_and_slices=shape_and_slices, dtypes=dtypes, name=name). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper. op_def=op_def). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func. return func(*args, **kwargs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 3272, in create_op. op_def=op_def). Fi",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:11020,modifiability,pac,packages,11020,"ault() # pylint: disable=protected-access. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 878, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1094, in __init__. self.build(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1106, in build. self._build(self._filename, build_save=True, build_restore=True). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1143, in _build. build_save=build_save, build_restore=build_restore). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 781, in _build_internal. restore_sequentially, reshape). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 459, in _AddShardedRestoreOps. name=""restore_shard"")). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 406, in _AddRestoreOps. restore_sequentially). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 854, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 1466, in restore_v2. shape_and_slices=shape_and_slices, dtypes=dtypes, name=name). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper. op_def=op_def). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func. return func(*args, **kwargs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 3272, in create_op. op_def=op_def). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1768, in __init__. self._traceback = tf_stack.extract_stac",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:11157,modifiability,pac,packages,11157," in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1094, in __init__. self.build(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1106, in build. self._build(self._filename, build_save=True, build_restore=True). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1143, in _build. build_save=build_save, build_restore=build_restore). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 781, in _build_internal. restore_sequentially, reshape). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 459, in _AddShardedRestoreOps. name=""restore_shard"")). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 406, in _AddRestoreOps. restore_sequentially). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 854, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 1466, in restore_v2. shape_and_slices=shape_and_slices, dtypes=dtypes, name=name). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper. op_def=op_def). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func. return func(*args, **kwargs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 3272, in create_op. op_def=op_def). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1768, in __init__. self._traceback = tf_stack.extract_stack(). DataLossError (see above for traceback): Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+da",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:11335,modifiability,pac,packages,11335,"t__. self.build(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1106, in build. self._build(self._filename, build_save=True, build_restore=True). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1143, in _build. build_save=build_save, build_restore=build_restore). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 781, in _build_internal. restore_sequentially, reshape). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 459, in _AddShardedRestoreOps. name=""restore_shard"")). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 406, in _AddRestoreOps. restore_sequentially). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 854, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 1466, in restore_v2. shape_and_slices=shape_and_slices, dtypes=dtypes, name=name). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper. op_def=op_def). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func. return func(*args, **kwargs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 3272, in create_op. op_def=op_def). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1768, in __init__. self._traceback = tf_stack.extract_stack(). DataLossError (see above for traceback): Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file f",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:11508,modifiability,pac,packages,11508,"ld_restore=True). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1143, in _build. build_save=build_save, build_restore=build_restore). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 781, in _build_internal. restore_sequentially, reshape). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 459, in _AddShardedRestoreOps. name=""restore_shard"")). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 406, in _AddRestoreOps. restore_sequentially). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 854, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 1466, in restore_v2. shape_and_slices=shape_and_slices, dtypes=dtypes, name=name). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper. op_def=op_def). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func. return func(*args, **kwargs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 3272, in create_op. op_def=op_def). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1768, in __init__. self._traceback = tf_stack.extract_stack(). DataLossError (see above for traceback): Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_F",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:11650,modifiability,pac,packages,11650,"save, build_restore=build_restore). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 781, in _build_internal. restore_sequentially, reshape). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 459, in _AddShardedRestoreOps. name=""restore_shard"")). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 406, in _AddRestoreOps. restore_sequentially). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 854, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 1466, in restore_v2. shape_and_slices=shape_and_slices, dtypes=dtypes, name=name). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper. op_def=op_def). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func. return func(*args, **kwargs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 3272, in create_op. op_def=op_def). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1768, in __init__. self._traceback = tf_stack.extract_stack(). DataLossError (see above for traceback): Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_na",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:11790,modifiability,pac,packages,11790,"_internal. restore_sequentially, reshape). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 459, in _AddShardedRestoreOps. name=""restore_shard"")). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 406, in _AddRestoreOps. restore_sequentially). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 854, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 1466, in restore_v2. shape_and_slices=shape_and_slices, dtypes=dtypes, name=name). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper. op_def=op_def). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func. return func(*args, **kwargs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 3272, in create_op. op_def=op_def). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1768, in __init__. self._traceback = tf_stack.extract_stack(). DataLossError (see above for traceback): Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. ```. This is the script that I am running DeepVariant:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_H",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:11915,modifiability,pac,packages,11915,""", line 459, in _AddShardedRestoreOps. name=""restore_shard"")). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 406, in _AddRestoreOps. restore_sequentially). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 854, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 1466, in restore_v2. shape_and_slices=shape_and_slices, dtypes=dtypes, name=name). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper. op_def=op_def). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func. return func(*args, **kwargs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 3272, in create_op. op_def=op_def). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1768, in __init__. self._traceback = tf_stack.extract_stack(). DataLossError (see above for traceback): Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. ```. This is the script that I am running DeepVariant:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. BAM=/mnt/efs-genome/Charles_Human/Geno",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:370,performance,time,time,370,"DataLossError / Failed precondition when running WES call_variant; Hello,. I have been trying to do some testing of DeepVariant on my own Exome (WES) and WGS data. As a starting point, I was trying to work with calling variants from the .bam file provided for my WES data. I started running from within a Docker container on my local computer but that was taking a long time (and, ultimately, the _make_examples_ step did not run to completion). I started learning more about the AWS options for analysis, and I was able to run the _make_examples_ much quicker (and successfully) on an AWS m5.xlarge ECS instance (although I am admittedly well over the ~25 minutes and $0.20 time/cost mentioned for Google Cloud, just for the _make_examples_, without considering upload/download, long-term storage, etc.). While I was hoping to eventually compare running things on Google Cloud (and I think my experience so far probably helps me ask better questions), I was wondering if you could help me troubleshoot something that I think is probably close to working:. Essentially, I am currently at the **call_variant** step of DeepVariant, with WES data. This is the error message that I am currently receiving:. ```. sudo sh run_deepvariant.sh. I0331 18:31:22.446569 140549764839168 call_variants.py:292] Set KMP_BLOCKTIME to 0. 2019-03-31 18:31:22.486802: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2 AVX512F FMA. 2019-03-31 18:31:22.489180: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. I0331 18:31:22.527594 140549764839168 modeling.py:351] Initializing model with random parameters. W0331 18:31:22.529449 140549764839168 tf_logging.py:125] Using temporary folder as model directory: /tmp/tmpuBleAQ. I0331 18:31:22.529786 140549764839168 tf_logging.py:115] Using config: {'_sav",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:675,performance,time,time,675,"DataLossError / Failed precondition when running WES call_variant; Hello,. I have been trying to do some testing of DeepVariant on my own Exome (WES) and WGS data. As a starting point, I was trying to work with calling variants from the .bam file provided for my WES data. I started running from within a Docker container on my local computer but that was taking a long time (and, ultimately, the _make_examples_ step did not run to completion). I started learning more about the AWS options for analysis, and I was able to run the _make_examples_ much quicker (and successfully) on an AWS m5.xlarge ECS instance (although I am admittedly well over the ~25 minutes and $0.20 time/cost mentioned for Google Cloud, just for the _make_examples_, without considering upload/download, long-term storage, etc.). While I was hoping to eventually compare running things on Google Cloud (and I think my experience so far probably helps me ask better questions), I was wondering if you could help me troubleshoot something that I think is probably close to working:. Essentially, I am currently at the **call_variant** step of DeepVariant, with WES data. This is the error message that I am currently receiving:. ```. sudo sh run_deepvariant.sh. I0331 18:31:22.446569 140549764839168 call_variants.py:292] Set KMP_BLOCKTIME to 0. 2019-03-31 18:31:22.486802: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2 AVX512F FMA. 2019-03-31 18:31:22.489180: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. I0331 18:31:22.527594 140549764839168 modeling.py:351] Initializing model with random parameters. W0331 18:31:22.529449 140549764839168 tf_logging.py:125] Using temporary folder as model directory: /tmp/tmpuBleAQ. I0331 18:31:22.529786 140549764839168 tf_logging.py:115] Using config: {'_sav",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:1157,performance,error,error,1157,"ta. As a starting point, I was trying to work with calling variants from the .bam file provided for my WES data. I started running from within a Docker container on my local computer but that was taking a long time (and, ultimately, the _make_examples_ step did not run to completion). I started learning more about the AWS options for analysis, and I was able to run the _make_examples_ much quicker (and successfully) on an AWS m5.xlarge ECS instance (although I am admittedly well over the ~25 minutes and $0.20 time/cost mentioned for Google Cloud, just for the _make_examples_, without considering upload/download, long-term storage, etc.). While I was hoping to eventually compare running things on Google Cloud (and I think my experience so far probably helps me ask better questions), I was wondering if you could help me troubleshoot something that I think is probably close to working:. Essentially, I am currently at the **call_variant** step of DeepVariant, with WES data. This is the error message that I am currently receiving:. ```. sudo sh run_deepvariant.sh. I0331 18:31:22.446569 140549764839168 call_variants.py:292] Set KMP_BLOCKTIME to 0. 2019-03-31 18:31:22.486802: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2 AVX512F FMA. 2019-03-31 18:31:22.489180: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. I0331 18:31:22.527594 140549764839168 modeling.py:351] Initializing model with random parameters. W0331 18:31:22.529449 140549764839168 tf_logging.py:125] Using temporary folder as model directory: /tmp/tmpuBleAQ. I0331 18:31:22.529786 140549764839168 tf_logging.py:115] Using config: {'_save_checkpoints_secs': 1000, '_num_ps_replicas': 0, '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_clust",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:1406,performance,CPU,CPU,1406,"les_ step did not run to completion). I started learning more about the AWS options for analysis, and I was able to run the _make_examples_ much quicker (and successfully) on an AWS m5.xlarge ECS instance (although I am admittedly well over the ~25 minutes and $0.20 time/cost mentioned for Google Cloud, just for the _make_examples_, without considering upload/download, long-term storage, etc.). While I was hoping to eventually compare running things on Google Cloud (and I think my experience so far probably helps me ask better questions), I was wondering if you could help me troubleshoot something that I think is probably close to working:. Essentially, I am currently at the **call_variant** step of DeepVariant, with WES data. This is the error message that I am currently receiving:. ```. sudo sh run_deepvariant.sh. I0331 18:31:22.446569 140549764839168 call_variants.py:292] Set KMP_BLOCKTIME to 0. 2019-03-31 18:31:22.486802: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2 AVX512F FMA. 2019-03-31 18:31:22.489180: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. I0331 18:31:22.527594 140549764839168 modeling.py:351] Initializing model with random parameters. W0331 18:31:22.529449 140549764839168 tf_logging.py:125] Using temporary folder as model directory: /tmp/tmpuBleAQ. I0331 18:31:22.529786 140549764839168 tf_logging.py:115] Using config: {'_save_checkpoints_secs': 1000, '_num_ps_replicas': 0, '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd40e875e50>, '_model_dir': '/tmp/tmpuBleAQ', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_session_config'",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:1647,performance,Tune,Tune,1647,"the ~25 minutes and $0.20 time/cost mentioned for Google Cloud, just for the _make_examples_, without considering upload/download, long-term storage, etc.). While I was hoping to eventually compare running things on Google Cloud (and I think my experience so far probably helps me ask better questions), I was wondering if you could help me troubleshoot something that I think is probably close to working:. Essentially, I am currently at the **call_variant** step of DeepVariant, with WES data. This is the error message that I am currently receiving:. ```. sudo sh run_deepvariant.sh. I0331 18:31:22.446569 140549764839168 call_variants.py:292] Set KMP_BLOCKTIME to 0. 2019-03-31 18:31:22.486802: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2 AVX512F FMA. 2019-03-31 18:31:22.489180: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. I0331 18:31:22.527594 140549764839168 modeling.py:351] Initializing model with random parameters. W0331 18:31:22.529449 140549764839168 tf_logging.py:125] Using temporary folder as model directory: /tmp/tmpuBleAQ. I0331 18:31:22.529786 140549764839168 tf_logging.py:115] Using config: {'_save_checkpoints_secs': 1000, '_num_ps_replicas': 0, '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd40e875e50>, '_model_dir': '/tmp/tmpuBleAQ', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:1696,performance,perform,performance,1696,"gle Cloud, just for the _make_examples_, without considering upload/download, long-term storage, etc.). While I was hoping to eventually compare running things on Google Cloud (and I think my experience so far probably helps me ask better questions), I was wondering if you could help me troubleshoot something that I think is probably close to working:. Essentially, I am currently at the **call_variant** step of DeepVariant, with WES data. This is the error message that I am currently receiving:. ```. sudo sh run_deepvariant.sh. I0331 18:31:22.446569 140549764839168 call_variants.py:292] Set KMP_BLOCKTIME to 0. 2019-03-31 18:31:22.486802: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2 AVX512F FMA. 2019-03-31 18:31:22.489180: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. I0331 18:31:22.527594 140549764839168 modeling.py:351] Initializing model with random parameters. W0331 18:31:22.529449 140549764839168 tf_logging.py:125] Using temporary folder as model directory: /tmp/tmpuBleAQ. I0331 18:31:22.529786 140549764839168 tf_logging.py:115] Using config: {'_save_checkpoints_secs': 1000, '_num_ps_replicas': 0, '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd40e875e50>, '_model_dir': '/tmp/tmpuBleAQ', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluat",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:7984,performance,CPU,CPU,7984,"ile ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1110, in _run. feed_dict_tensor, options, run_metadata). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1286, in _do_run. run_metadata). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1308, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.DataLossError: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. Caused by op u'save_1/RestoreV2', defined at:. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>. tf.app.run(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 401, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 357, in call_variants. prediction = next(predictions). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 567, in predict. hooks=all_hooks) as mon_sess:. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 921, in __init__. stop_grace_period_secs=stop_grace_period",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:12597,performance,CPU,CPU,12597,"def=op_def). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func. return func(*args, **kwargs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 3272, in create_op. op_def=op_def). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1768, in __init__. self._traceback = tf_stack.extract_stack(). DataLossError (see above for traceback): Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. ```. This is the script that I am running DeepVariant:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. BAM=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/82651510240740.mapped.sorted.markdup.realn.recal.bam. MODEL=/mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. ## step #1. LOGDIR=logs. N_SHARDS=4. #mkdir -p ""${LOGDIR}"". #time seq 0 $((N_SHARDS-1)) | \. # parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" \. # sudo docker run \. # -v /mnt/efs-genome:/mnt/efs-genome \. # gcr.io/deepvariant-docker/deepvariant \. # /opt/deepvariant/bin/make_examples \. # --mode calling \. # --ref ""${REF}"" \. # --reads ""${BAM}"" \. # --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. # --task {}. ## step #2. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_out",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:13146,performance,time,time,13146,"n_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. ```. This is the script that I am running DeepVariant:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. BAM=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/82651510240740.mapped.sorted.markdup.realn.recal.bam. MODEL=/mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. ## step #1. LOGDIR=logs. N_SHARDS=4. #mkdir -p ""${LOGDIR}"". #time seq 0 $((N_SHARDS-1)) | \. # parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" \. # sudo docker run \. # -v /mnt/efs-genome:/mnt/efs-genome \. # gcr.io/deepvariant-docker/deepvariant \. # /opt/deepvariant/bin/make_examples \. # --mode calling \. # --ref ""${REF}"" \. # --reads ""${BAM}"" \. # --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. # --task {}. ## step #2. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". sudo docker run \. -v /mnt/efs-genome:/mnt/efs-genome \. gcr.io/deepvariant-docker/deepvariant \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""${MODEL}"". ```. Can you please help me troubleshoot? I thought it might be something simple, like [this question](https://github.com/google/deepvariant/issues/129). However, that particular solution is not working for me. Thank you very much for your assistance. Sincerely,. Char",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:13180,performance,parallel,parallel,13180,"3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. ```. This is the script that I am running DeepVariant:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. BAM=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/82651510240740.mapped.sorted.markdup.realn.recal.bam. MODEL=/mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. ## step #1. LOGDIR=logs. N_SHARDS=4. #mkdir -p ""${LOGDIR}"". #time seq 0 $((N_SHARDS-1)) | \. # parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" \. # sudo docker run \. # -v /mnt/efs-genome:/mnt/efs-genome \. # gcr.io/deepvariant-docker/deepvariant \. # /opt/deepvariant/bin/make_examples \. # --mode calling \. # --ref ""${REF}"" \. # --reads ""${BAM}"" \. # --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. # --task {}. ## step #2. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". sudo docker run \. -v /mnt/efs-genome:/mnt/efs-genome \. gcr.io/deepvariant-docker/deepvariant \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""${MODEL}"". ```. Can you please help me troubleshoot? I thought it might be something simple, like [this question](https://github.com/google/deepvariant/issues/129). However, that particular solution is not working for me. Thank you very much for your assistance. Sincerely,. Charles",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:16,reliability,Fail,Failed,16,"DataLossError / Failed precondition when running WES call_variant; Hello,. I have been trying to do some testing of DeepVariant on my own Exome (WES) and WGS data. As a starting point, I was trying to work with calling variants from the .bam file provided for my WES data. I started running from within a Docker container on my local computer but that was taking a long time (and, ultimately, the _make_examples_ step did not run to completion). I started learning more about the AWS options for analysis, and I was able to run the _make_examples_ much quicker (and successfully) on an AWS m5.xlarge ECS instance (although I am admittedly well over the ~25 minutes and $0.20 time/cost mentioned for Google Cloud, just for the _make_examples_, without considering upload/download, long-term storage, etc.). While I was hoping to eventually compare running things on Google Cloud (and I think my experience so far probably helps me ask better questions), I was wondering if you could help me troubleshoot something that I think is probably close to working:. Essentially, I am currently at the **call_variant** step of DeepVariant, with WES data. This is the error message that I am currently receiving:. ```. sudo sh run_deepvariant.sh. I0331 18:31:22.446569 140549764839168 call_variants.py:292] Set KMP_BLOCKTIME to 0. 2019-03-31 18:31:22.486802: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2 AVX512F FMA. 2019-03-31 18:31:22.489180: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. I0331 18:31:22.527594 140549764839168 modeling.py:351] Initializing model with random parameters. W0331 18:31:22.529449 140549764839168 tf_logging.py:125] Using temporary folder as model directory: /tmp/tmpuBleAQ. I0331 18:31:22.529786 140549764839168 tf_logging.py:115] Using config: {'_sav",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:3259,reliability,Restor,Restoring,3259,"tmp/tmpuBleAQ', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_train_distribute': None, '_master': ''}. I0331 18:31:22.529983 140549764839168 call_variants.py:350] Writing calls to /mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant/call_variants_output.tfrecord.gz. I0331 18:31:22.610783 140549764839168 tf_logging.py:115] Calling model_fn. I0331 18:31:24.905831 140549764839168 tf_logging.py:115] Done calling model_fn. I0331 18:31:25.996718 140549764839168 tf_logging.py:115] Graph was finalized. I0331 18:31:26.008368 140549764839168 tf_logging.py:115] Restoring parameters from /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. 2019-03-31 18:31:26.447697: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? 2019-03-31 18:31:26.450035: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? 2019-03-31 18:31:26.450061: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_ten",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:3542,reliability,Fail,Failed,3542,"_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_train_distribute': None, '_master': ''}. I0331 18:31:22.529983 140549764839168 call_variants.py:350] Writing calls to /mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant/call_variants_output.tfrecord.gz. I0331 18:31:22.610783 140549764839168 tf_logging.py:115] Calling model_fn. I0331 18:31:24.905831 140549764839168 tf_logging.py:115] Done calling model_fn. I0331 18:31:25.996718 140549764839168 tf_logging.py:115] Graph was finalized. I0331 18:31:26.008368 140549764839168 tf_logging.py:115] Restoring parameters from /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. 2019-03-31 18:31:26.447697: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? 2019-03-31 18:31:26.450035: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? 2019-03-31 18:31:26.450061: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:172 : Data loss: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a diff",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:3741,reliability,restor,restore,3741,"'_train_distribute': None, '_master': ''}. I0331 18:31:22.529983 140549764839168 call_variants.py:350] Writing calls to /mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant/call_variants_output.tfrecord.gz. I0331 18:31:22.610783 140549764839168 tf_logging.py:115] Calling model_fn. I0331 18:31:24.905831 140549764839168 tf_logging.py:115] Done calling model_fn. I0331 18:31:25.996718 140549764839168 tf_logging.py:115] Graph was finalized. I0331 18:31:26.008368 140549764839168 tf_logging.py:115] Restoring parameters from /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. 2019-03-31 18:31:26.447697: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? 2019-03-31 18:31:26.450035: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? 2019-03-31 18:31:26.450061: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:172 : Data loss: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:3934,reliability,Fail,Failed,3934,"ll_variants_output.tfrecord.gz. I0331 18:31:22.610783 140549764839168 tf_logging.py:115] Calling model_fn. I0331 18:31:24.905831 140549764839168 tf_logging.py:115] Done calling model_fn. I0331 18:31:25.996718 140549764839168 tf_logging.py:115] Graph was finalized. I0331 18:31:26.008368 140549764839168 tf_logging.py:115] Restoring parameters from /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. 2019-03-31 18:31:26.447697: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? 2019-03-31 18:31:26.450035: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? 2019-03-31 18:31:26.450061: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:172 : Data loss: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>. tf.app.run(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_9uQwM",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:4133,reliability,restor,restore,4133,"5.996718 140549764839168 tf_logging.py:115] Graph was finalized. I0331 18:31:26.008368 140549764839168 tf_logging.py:115] Restoring parameters from /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. 2019-03-31 18:31:26.447697: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? 2019-03-31 18:31:26.450035: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? 2019-03-31 18:31:26.450061: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:172 : Data loss: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>. tf.app.run(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 401, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.p",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:4238,reliability,fail,failed,4238,"f_logging.py:115] Restoring parameters from /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. 2019-03-31 18:31:26.447697: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? 2019-03-31 18:31:26.450035: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? 2019-03-31 18:31:26.450061: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:172 : Data loss: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>. tf.app.run(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 401, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 357, in call_variants. prediction = next(predictions). File ""/root/.local/lib/python2.7/site-pa",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:4396,reliability,Fail,Failed,4396,"nsorflow/core/util/tensor_slice_reader.cc:95] Could not open /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? 2019-03-31 18:31:26.450035: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? 2019-03-31 18:31:26.450061: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:172 : Data loss: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>. tf.app.run(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 401, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 357, in call_variants. prediction = next(predictions). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 567, in predict. hooks=all_hooks) as mon_sess:. File ""/root/.local/lib/python2.7/site-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:4595,reliability,restor,restore,4595,"Variant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? 2019-03-31 18:31:26.450035: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? 2019-03-31 18:31:26.450061: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:172 : Data loss: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>. tf.app.run(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 401, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 357, in call_variants. prediction = next(predictions). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 567, in predict. hooks=all_hooks) as mon_sess:. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 921, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_ses",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:6658,reliability,restor,restore,6658,"on(self._coordinated_creator). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1107, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1112, in _create_session. return self._sess_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 566, in create_session. init_fn=self._scaffold.init_fn). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session. config=config). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 195, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1538, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 887, in run. run_metadata_ptr). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1110, in _run. feed_dict_tensor, options, run_metadata). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1286, in _do_run. run_metadata). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1308, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.DataLossError: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_stan",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:6803,reliability,restor,restore,6803,"nit__. _WrappedSession.__init__(self, self._create_session()). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1112, in _create_session. return self._sess_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 566, in create_session. init_fn=self._scaffold.init_fn). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session. config=config). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 195, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1538, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 887, in run. run_metadata_ptr). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1110, in _run. feed_dict_tensor, options, run_metadata). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1286, in _do_run. run_metadata). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1308, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.DataLossError: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:7565,reliability,Fail,Failed,7565,"s/tensorflow/python/training/session_manager.py"", line 195, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1538, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 887, in run. run_metadata_ptr). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1110, in _run. feed_dict_tensor, options, run_metadata). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1286, in _do_run. run_metadata). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1308, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.DataLossError: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. Caused by op u'save_1/RestoreV2', defined at:. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>. tf.app.run(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 401, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:7764,reliability,restor,restore,7764,"/training/saver.py"", line 1538, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 887, in run. run_metadata_ptr). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1110, in _run. feed_dict_tensor, options, run_metadata). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1286, in _do_run. run_metadata). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1308, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.DataLossError: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. Caused by op u'save_1/RestoreV2', defined at:. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>. tf.app.run(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 401, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 357, in call_variants. prediction = next(predictions). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:11280,reliability,sli,slices,11280,"ensorflow/python/training/saver.py"", line 1094, in __init__. self.build(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1106, in build. self._build(self._filename, build_save=True, build_restore=True). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1143, in _build. build_save=build_save, build_restore=build_restore). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 781, in _build_internal. restore_sequentially, reshape). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 459, in _AddShardedRestoreOps. name=""restore_shard"")). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 406, in _AddRestoreOps. restore_sequentially). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 854, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 1466, in restore_v2. shape_and_slices=shape_and_slices, dtypes=dtypes, name=name). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper. op_def=op_def). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func. return func(*args, **kwargs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 3272, in create_op. op_def=op_def). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1768, in __init__. self._traceback = tf_stack.extract_stack(). DataLossError (see above for traceback): Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:12178,reliability,Fail,Failed,12178,"on/training/saver.py"", line 854, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 1466, in restore_v2. shape_and_slices=shape_and_slices, dtypes=dtypes, name=name). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper. op_def=op_def). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func. return func(*args, **kwargs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 3272, in create_op. op_def=op_def). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1768, in __init__. self._traceback = tf_stack.extract_stack(). DataLossError (see above for traceback): Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. ```. This is the script that I am running DeepVariant:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. BAM=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/82651510240740.mapped.sorted.markdup.realn.recal.bam. MODEL=/mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. ## step #1. LOGDIR=logs. N_SHARDS=4. #mkdir -p ""${LOGDIR}"". #time seq 0 $((N_SHARDS-1)) | \. # p",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:12377,reliability,restor,restore,12377," line 1466, in restore_v2. shape_and_slices=shape_and_slices, dtypes=dtypes, name=name). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper. op_def=op_def). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func. return func(*args, **kwargs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 3272, in create_op. op_def=op_def). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1768, in __init__. self._traceback = tf_stack.extract_stack(). DataLossError (see above for traceback): Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. ```. This is the script that I am running DeepVariant:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. BAM=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/82651510240740.mapped.sorted.markdup.realn.recal.bam. MODEL=/mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. ## step #1. LOGDIR=logs. N_SHARDS=4. #mkdir -p ""${LOGDIR}"". #time seq 0 $((N_SHARDS-1)) | \. # parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" \. # sudo docker run \. # -v /mnt/efs-genome:/mnt/efs-genome \. # gcr.io/deepvariant-docker/deepvariant \. # /opt/deepvariant/bin/make",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:13857,reliability,checkpoint,checkpoint,13857,"3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. ```. This is the script that I am running DeepVariant:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. BAM=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/82651510240740.mapped.sorted.markdup.realn.recal.bam. MODEL=/mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. ## step #1. LOGDIR=logs. N_SHARDS=4. #mkdir -p ""${LOGDIR}"". #time seq 0 $((N_SHARDS-1)) | \. # parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" \. # sudo docker run \. # -v /mnt/efs-genome:/mnt/efs-genome \. # gcr.io/deepvariant-docker/deepvariant \. # /opt/deepvariant/bin/make_examples \. # --mode calling \. # --ref ""${REF}"" \. # --reads ""${BAM}"" \. # --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. # --task {}. ## step #2. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". sudo docker run \. -v /mnt/efs-genome:/mnt/efs-genome \. gcr.io/deepvariant-docker/deepvariant \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""${MODEL}"". ```. Can you please help me troubleshoot? I thought it might be something simple, like [this question](https://github.com/google/deepvariant/issues/129). However, that particular solution is not working for me. Thank you very much for your assistance. Sincerely,. Charles",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:105,safety,test,testing,105,"DataLossError / Failed precondition when running WES call_variant; Hello,. I have been trying to do some testing of DeepVariant on my own Exome (WES) and WGS data. As a starting point, I was trying to work with calling variants from the .bam file provided for my WES data. I started running from within a Docker container on my local computer but that was taking a long time (and, ultimately, the _make_examples_ step did not run to completion). I started learning more about the AWS options for analysis, and I was able to run the _make_examples_ much quicker (and successfully) on an AWS m5.xlarge ECS instance (although I am admittedly well over the ~25 minutes and $0.20 time/cost mentioned for Google Cloud, just for the _make_examples_, without considering upload/download, long-term storage, etc.). While I was hoping to eventually compare running things on Google Cloud (and I think my experience so far probably helps me ask better questions), I was wondering if you could help me troubleshoot something that I think is probably close to working:. Essentially, I am currently at the **call_variant** step of DeepVariant, with WES data. This is the error message that I am currently receiving:. ```. sudo sh run_deepvariant.sh. I0331 18:31:22.446569 140549764839168 call_variants.py:292] Set KMP_BLOCKTIME to 0. 2019-03-31 18:31:22.486802: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2 AVX512F FMA. 2019-03-31 18:31:22.489180: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. I0331 18:31:22.527594 140549764839168 modeling.py:351] Initializing model with random parameters. W0331 18:31:22.529449 140549764839168 tf_logging.py:125] Using temporary folder as model directory: /tmp/tmpuBleAQ. I0331 18:31:22.529786 140549764839168 tf_logging.py:115] Using config: {'_sav",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:433,safety,compl,completion,433,"DataLossError / Failed precondition when running WES call_variant; Hello,. I have been trying to do some testing of DeepVariant on my own Exome (WES) and WGS data. As a starting point, I was trying to work with calling variants from the .bam file provided for my WES data. I started running from within a Docker container on my local computer but that was taking a long time (and, ultimately, the _make_examples_ step did not run to completion). I started learning more about the AWS options for analysis, and I was able to run the _make_examples_ much quicker (and successfully) on an AWS m5.xlarge ECS instance (although I am admittedly well over the ~25 minutes and $0.20 time/cost mentioned for Google Cloud, just for the _make_examples_, without considering upload/download, long-term storage, etc.). While I was hoping to eventually compare running things on Google Cloud (and I think my experience so far probably helps me ask better questions), I was wondering if you could help me troubleshoot something that I think is probably close to working:. Essentially, I am currently at the **call_variant** step of DeepVariant, with WES data. This is the error message that I am currently receiving:. ```. sudo sh run_deepvariant.sh. I0331 18:31:22.446569 140549764839168 call_variants.py:292] Set KMP_BLOCKTIME to 0. 2019-03-31 18:31:22.486802: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2 AVX512F FMA. 2019-03-31 18:31:22.489180: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. I0331 18:31:22.527594 140549764839168 modeling.py:351] Initializing model with random parameters. W0331 18:31:22.529449 140549764839168 tf_logging.py:125] Using temporary folder as model directory: /tmp/tmpuBleAQ. I0331 18:31:22.529786 140549764839168 tf_logging.py:115] Using config: {'_sav",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:1157,safety,error,error,1157,"ta. As a starting point, I was trying to work with calling variants from the .bam file provided for my WES data. I started running from within a Docker container on my local computer but that was taking a long time (and, ultimately, the _make_examples_ step did not run to completion). I started learning more about the AWS options for analysis, and I was able to run the _make_examples_ much quicker (and successfully) on an AWS m5.xlarge ECS instance (although I am admittedly well over the ~25 minutes and $0.20 time/cost mentioned for Google Cloud, just for the _make_examples_, without considering upload/download, long-term storage, etc.). While I was hoping to eventually compare running things on Google Cloud (and I think my experience so far probably helps me ask better questions), I was wondering if you could help me troubleshoot something that I think is probably close to working:. Essentially, I am currently at the **call_variant** step of DeepVariant, with WES data. This is the error message that I am currently receiving:. ```. sudo sh run_deepvariant.sh. I0331 18:31:22.446569 140549764839168 call_variants.py:292] Set KMP_BLOCKTIME to 0. 2019-03-31 18:31:22.486802: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2 AVX512F FMA. 2019-03-31 18:31:22.489180: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. I0331 18:31:22.527594 140549764839168 modeling.py:351] Initializing model with random parameters. W0331 18:31:22.529449 140549764839168 tf_logging.py:125] Using temporary folder as model directory: /tmp/tmpuBleAQ. I0331 18:31:22.529786 140549764839168 tf_logging.py:115] Using config: {'_save_checkpoints_secs': 1000, '_num_ps_replicas': 0, '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_clust",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:4759,safety,modul,module,4759,"9-03-31 18:31:26.450035: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? 2019-03-31 18:31:26.450061: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:172 : Data loss: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>. tf.app.run(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 401, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 357, in call_variants. prediction = next(predictions). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 567, in predict. hooks=all_hooks) as mon_sess:. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 921, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 643, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/trai",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:5169,safety,predict,prediction,5169,"061: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:172 : Data loss: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>. tf.app.run(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 401, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 357, in call_variants. prediction = next(predictions). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 567, in predict. hooks=all_hooks) as mon_sess:. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 921, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 643, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1107, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1112, in _create_session. return self._sess_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session. self.t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:5187,safety,predict,predictions,5187,"ore/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:172 : Data loss: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>. tf.app.run(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 401, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 357, in call_variants. prediction = next(predictions). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 567, in predict. hooks=all_hooks) as mon_sess:. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 921, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 643, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1107, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1112, in _create_session. return self._sess_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session. self.tf_sess = self._sess",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:5304,safety,predict,predict,5304," file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>. tf.app.run(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 401, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 357, in call_variants. prediction = next(predictions). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 567, in predict. hooks=all_hooks) as mon_sess:. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 921, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 643, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1107, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1112, in _create_session. return self._sess_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_s",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:8241,safety,modul,module,8241,"metadata). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1308, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.DataLossError: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. Caused by op u'save_1/RestoreV2', defined at:. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>. tf.app.run(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 401, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 357, in call_variants. prediction = next(predictions). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 567, in predict. hooks=all_hooks) as mon_sess:. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 921, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 643, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/trai",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:8651,safety,predict,prediction,8651,"s_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. Caused by op u'save_1/RestoreV2', defined at:. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>. tf.app.run(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 401, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 357, in call_variants. prediction = next(predictions). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 567, in predict. hooks=all_hooks) as mon_sess:. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 921, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 643, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1107, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1112, in _create_session. return self._sess_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session. self.t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:8669,safety,predict,predictions,8669,"rectory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. Caused by op u'save_1/RestoreV2', defined at:. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>. tf.app.run(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 401, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 357, in call_variants. prediction = next(predictions). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 567, in predict. hooks=all_hooks) as mon_sess:. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 921, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 643, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1107, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1112, in _create_session. return self._sess_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session. self.tf_sess = self._sess",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:8786,safety,predict,predict,8786," save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. Caused by op u'save_1/RestoreV2', defined at:. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>. tf.app.run(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 401, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 357, in call_variants. prediction = next(predictions). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 567, in predict. hooks=all_hooks) as mon_sess:. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 921, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 643, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1107, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1112, in _create_session. return self._sess_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_s",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:13097,safety,LOG,LOGDIR,13097,"/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. ```. This is the script that I am running DeepVariant:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. BAM=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/82651510240740.mapped.sorted.markdup.realn.recal.bam. MODEL=/mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. ## step #1. LOGDIR=logs. N_SHARDS=4. #mkdir -p ""${LOGDIR}"". #time seq 0 $((N_SHARDS-1)) | \. # parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" \. # sudo docker run \. # -v /mnt/efs-genome:/mnt/efs-genome \. # gcr.io/deepvariant-docker/deepvariant \. # /opt/deepvariant/bin/make_examples \. # --mode calling \. # --ref ""${REF}"" \. # --reads ""${BAM}"" \. # --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. # --task {}. ## step #2. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". sudo docker run \. -v /mnt/efs-genome:/mnt/efs-genome \. gcr.io/deepvariant-docker/deepvariant \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""${MODEL}"". ```. Can you please help me troubleshoot? I thought it might be something simple, like [this question](https://github.com/google/deepvariant/issues/129). However, that particular solution is not working for me. Thank you",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:13104,safety,log,logs,13104,"enome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. ```. This is the script that I am running DeepVariant:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. BAM=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/82651510240740.mapped.sorted.markdup.realn.recal.bam. MODEL=/mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. ## step #1. LOGDIR=logs. N_SHARDS=4. #mkdir -p ""${LOGDIR}"". #time seq 0 $((N_SHARDS-1)) | \. # parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" \. # sudo docker run \. # -v /mnt/efs-genome:/mnt/efs-genome \. # gcr.io/deepvariant-docker/deepvariant \. # /opt/deepvariant/bin/make_examples \. # --mode calling \. # --ref ""${REF}"" \. # --reads ""${BAM}"" \. # --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. # --task {}. ## step #2. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". sudo docker run \. -v /mnt/efs-genome:/mnt/efs-genome \. gcr.io/deepvariant-docker/deepvariant \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""${MODEL}"". ```. Can you please help me troubleshoot? I thought it might be something simple, like [this question](https://github.com/google/deepvariant/issues/129). However, that particular solution is not working for me. Thank you very ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:13135,safety,LOG,LOGDIR,13135,"t-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. ```. This is the script that I am running DeepVariant:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. BAM=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/82651510240740.mapped.sorted.markdup.realn.recal.bam. MODEL=/mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. ## step #1. LOGDIR=logs. N_SHARDS=4. #mkdir -p ""${LOGDIR}"". #time seq 0 $((N_SHARDS-1)) | \. # parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" \. # sudo docker run \. # -v /mnt/efs-genome:/mnt/efs-genome \. # gcr.io/deepvariant-docker/deepvariant \. # /opt/deepvariant/bin/make_examples \. # --mode calling \. # --ref ""${REF}"" \. # --reads ""${BAM}"" \. # --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. # --task {}. ## step #2. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". sudo docker run \. -v /mnt/efs-genome:/mnt/efs-genome \. gcr.io/deepvariant-docker/deepvariant \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""${MODEL}"". ```. Can you please help me troubleshoot? I thought it might be something simple, like [this question](https://github.com/google/deepvariant/issues/129). However, that particular solution is not working for me. Thank you very much for your assistance. Sincer",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:13216,safety,LOG,LOGDIR,13216,"3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. ```. This is the script that I am running DeepVariant:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. BAM=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/82651510240740.mapped.sorted.markdup.realn.recal.bam. MODEL=/mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. ## step #1. LOGDIR=logs. N_SHARDS=4. #mkdir -p ""${LOGDIR}"". #time seq 0 $((N_SHARDS-1)) | \. # parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" \. # sudo docker run \. # -v /mnt/efs-genome:/mnt/efs-genome \. # gcr.io/deepvariant-docker/deepvariant \. # /opt/deepvariant/bin/make_examples \. # --mode calling \. # --ref ""${REF}"" \. # --reads ""${BAM}"" \. # --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. # --task {}. ## step #2. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". sudo docker run \. -v /mnt/efs-genome:/mnt/efs-genome \. gcr.io/deepvariant-docker/deepvariant \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""${MODEL}"". ```. Can you please help me troubleshoot? I thought it might be something simple, like [this question](https://github.com/google/deepvariant/issues/129). However, that particular solution is not working for me. Thank you very much for your assistance. Sincerely,. Charles",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:13224,safety,log,log,13224,"3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. ```. This is the script that I am running DeepVariant:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. BAM=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/82651510240740.mapped.sorted.markdup.realn.recal.bam. MODEL=/mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. ## step #1. LOGDIR=logs. N_SHARDS=4. #mkdir -p ""${LOGDIR}"". #time seq 0 $((N_SHARDS-1)) | \. # parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" \. # sudo docker run \. # -v /mnt/efs-genome:/mnt/efs-genome \. # gcr.io/deepvariant-docker/deepvariant \. # /opt/deepvariant/bin/make_examples \. # --mode calling \. # --ref ""${REF}"" \. # --reads ""${BAM}"" \. # --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. # --task {}. ## step #2. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". sudo docker run \. -v /mnt/efs-genome:/mnt/efs-genome \. gcr.io/deepvariant-docker/deepvariant \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""${MODEL}"". ```. Can you please help me troubleshoot? I thought it might be something simple, like [this question](https://github.com/google/deepvariant/issues/129). However, that particular solution is not working for me. Thank you very much for your assistance. Sincerely,. Charles",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:13238,safety,LOG,LOGDIR,13238,"3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. ```. This is the script that I am running DeepVariant:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. BAM=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/82651510240740.mapped.sorted.markdup.realn.recal.bam. MODEL=/mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. ## step #1. LOGDIR=logs. N_SHARDS=4. #mkdir -p ""${LOGDIR}"". #time seq 0 $((N_SHARDS-1)) | \. # parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" \. # sudo docker run \. # -v /mnt/efs-genome:/mnt/efs-genome \. # gcr.io/deepvariant-docker/deepvariant \. # /opt/deepvariant/bin/make_examples \. # --mode calling \. # --ref ""${REF}"" \. # --reads ""${BAM}"" \. # --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. # --task {}. ## step #2. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". sudo docker run \. -v /mnt/efs-genome:/mnt/efs-genome \. gcr.io/deepvariant-docker/deepvariant \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""${MODEL}"". ```. Can you please help me troubleshoot? I thought it might be something simple, like [this question](https://github.com/google/deepvariant/issues/129). However, that particular solution is not working for me. Thank you very much for your assistance. Sincerely,. Charles",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:433,security,compl,completion,433,"DataLossError / Failed precondition when running WES call_variant; Hello,. I have been trying to do some testing of DeepVariant on my own Exome (WES) and WGS data. As a starting point, I was trying to work with calling variants from the .bam file provided for my WES data. I started running from within a Docker container on my local computer but that was taking a long time (and, ultimately, the _make_examples_ step did not run to completion). I started learning more about the AWS options for analysis, and I was able to run the _make_examples_ much quicker (and successfully) on an AWS m5.xlarge ECS instance (although I am admittedly well over the ~25 minutes and $0.20 time/cost mentioned for Google Cloud, just for the _make_examples_, without considering upload/download, long-term storage, etc.). While I was hoping to eventually compare running things on Google Cloud (and I think my experience so far probably helps me ask better questions), I was wondering if you could help me troubleshoot something that I think is probably close to working:. Essentially, I am currently at the **call_variant** step of DeepVariant, with WES data. This is the error message that I am currently receiving:. ```. sudo sh run_deepvariant.sh. I0331 18:31:22.446569 140549764839168 call_variants.py:292] Set KMP_BLOCKTIME to 0. 2019-03-31 18:31:22.486802: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2 AVX512F FMA. 2019-03-31 18:31:22.489180: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. I0331 18:31:22.527594 140549764839168 modeling.py:351] Initializing model with random parameters. W0331 18:31:22.529449 140549764839168 tf_logging.py:125] Using temporary folder as model directory: /tmp/tmpuBleAQ. I0331 18:31:22.529786 140549764839168 tf_logging.py:115] Using config: {'_sav",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:1747,security,model,modeling,1747,"considering upload/download, long-term storage, etc.). While I was hoping to eventually compare running things on Google Cloud (and I think my experience so far probably helps me ask better questions), I was wondering if you could help me troubleshoot something that I think is probably close to working:. Essentially, I am currently at the **call_variant** step of DeepVariant, with WES data. This is the error message that I am currently receiving:. ```. sudo sh run_deepvariant.sh. I0331 18:31:22.446569 140549764839168 call_variants.py:292] Set KMP_BLOCKTIME to 0. 2019-03-31 18:31:22.486802: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2 AVX512F FMA. 2019-03-31 18:31:22.489180: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. I0331 18:31:22.527594 140549764839168 modeling.py:351] Initializing model with random parameters. W0331 18:31:22.529449 140549764839168 tf_logging.py:125] Using temporary folder as model directory: /tmp/tmpuBleAQ. I0331 18:31:22.529786 140549764839168 tf_logging.py:115] Using config: {'_save_checkpoints_secs': 1000, '_num_ps_replicas': 0, '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd40e875e50>, '_model_dir': '/tmp/tmpuBleAQ', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_trai",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:1777,security,model,model,1777,"long-term storage, etc.). While I was hoping to eventually compare running things on Google Cloud (and I think my experience so far probably helps me ask better questions), I was wondering if you could help me troubleshoot something that I think is probably close to working:. Essentially, I am currently at the **call_variant** step of DeepVariant, with WES data. This is the error message that I am currently receiving:. ```. sudo sh run_deepvariant.sh. I0331 18:31:22.446569 140549764839168 call_variants.py:292] Set KMP_BLOCKTIME to 0. 2019-03-31 18:31:22.486802: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2 AVX512F FMA. 2019-03-31 18:31:22.489180: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. I0331 18:31:22.527594 140549764839168 modeling.py:351] Initializing model with random parameters. W0331 18:31:22.529449 140549764839168 tf_logging.py:125] Using temporary folder as model directory: /tmp/tmpuBleAQ. I0331 18:31:22.529786 140549764839168 tf_logging.py:115] Using config: {'_save_checkpoints_secs': 1000, '_num_ps_replicas': 0, '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd40e875e50>, '_model_dir': '/tmp/tmpuBleAQ', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_train_distribute': None, '_master",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:1890,security,model,model,1890," experience so far probably helps me ask better questions), I was wondering if you could help me troubleshoot something that I think is probably close to working:. Essentially, I am currently at the **call_variant** step of DeepVariant, with WES data. This is the error message that I am currently receiving:. ```. sudo sh run_deepvariant.sh. I0331 18:31:22.446569 140549764839168 call_variants.py:292] Set KMP_BLOCKTIME to 0. 2019-03-31 18:31:22.486802: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2 AVX512F FMA. 2019-03-31 18:31:22.489180: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. I0331 18:31:22.527594 140549764839168 modeling.py:351] Initializing model with random parameters. W0331 18:31:22.529449 140549764839168 tf_logging.py:125] Using temporary folder as model directory: /tmp/tmpuBleAQ. I0331 18:31:22.529786 140549764839168 tf_logging.py:115] Using config: {'_save_checkpoints_secs': 1000, '_num_ps_replicas': 0, '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd40e875e50>, '_model_dir': '/tmp/tmpuBleAQ', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_train_distribute': None, '_master': ''}. I0331 18:31:22.529983 140549764839168 call_variants.py:350] Writing calls to /mnt/efs-genome/Charles_Huma",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:4282,security,loss,loss,4282," /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. 2019-03-31 18:31:26.447697: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? 2019-03-31 18:31:26.450035: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? 2019-03-31 18:31:26.450061: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:172 : Data loss: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>. tf.app.run(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 401, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 357, in call_variants. prediction = next(predictions). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimato",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:6935,security,session,session,6935,"ning/monitored_session.py"", line 1112, in _create_session. return self._sess_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 566, in create_session. init_fn=self._scaffold.init_fn). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session. config=config). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 195, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1538, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 887, in run. run_metadata_ptr). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1110, in _run. feed_dict_tensor, options, run_metadata). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1286, in _do_run. run_metadata). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1308, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.DataLossError: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _de",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:7057,security,session,session,7057,"lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 566, in create_session. init_fn=self._scaffold.init_fn). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session. config=config). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 195, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1538, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 887, in run. run_metadata_ptr). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1110, in _run. feed_dict_tensor, options, run_metadata). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1286, in _do_run. run_metadata). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1308, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.DataLossError: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:7204,security,session,session,7204,"ate_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 566, in create_session. init_fn=self._scaffold.init_fn). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session. config=config). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 195, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1538, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 887, in run. run_metadata_ptr). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1110, in _run. feed_dict_tensor, options, run_metadata). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1286, in _do_run. run_metadata). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1308, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.DataLossError: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. Caused by op u'save_1/RestoreV2', defined at:. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:7327,security,session,session,7327,"n create_session. init_fn=self._scaffold.init_fn). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session. config=config). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 195, in _restore_checkpoint. saver.restore(sess, checkpoint_filename_with_path). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1538, in restore. {self.saver_def.filename_tensor_name: save_path}). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 887, in run. run_metadata_ptr). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1110, in _run. feed_dict_tensor, options, run_metadata). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1286, in _do_run. run_metadata). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1308, in _do_call. raise type(e)(node_def, op, message). tensorflow.python.framework.errors_impl.DataLossError: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. Caused by op u'save_1/RestoreV2', defined at:. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>. tf.app.run(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:10059,security,access,access,10059,"ining/monitored_session.py"", line 643, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1107, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1112, in _create_session. return self._sess_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 557, in create_session. self._scaffold.finalize(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 213, in finalize. self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 878, in _get_saver_or_default. saver = Saver(sharded=True, allow_empty=True). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1094, in __init__. self.build(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1106, in build. self._build(self._filename, build_save=True, build_restore=True). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1143, in _build. build_save=build_save, build_restore=build_restore). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 781, in _build_internal. restore_sequentially, reshape). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 459, in _AddShardedRestoreOps. name=""restore_shard"")). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:12997,security,MODEL,MODEL,12997," = tf_stack.extract_stack(). DataLossError (see above for traceback): Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. ```. This is the script that I am running DeepVariant:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. BAM=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/82651510240740.mapped.sorted.markdup.realn.recal.bam. MODEL=/mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. ## step #1. LOGDIR=logs. N_SHARDS=4. #mkdir -p ""${LOGDIR}"". #time seq 0 $((N_SHARDS-1)) | \. # parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" \. # sudo docker run \. # -v /mnt/efs-genome:/mnt/efs-genome \. # gcr.io/deepvariant-docker/deepvariant \. # /opt/deepvariant/bin/make_examples \. # --mode calling \. # --ref ""${REF}"" \. # --reads ""${BAM}"" \. # --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. # --task {}. ## step #2. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". sudo docker run \. -v /mnt/efs-genome:/mnt/efs-genome \. gcr.io/deepvariant-docker/deepvariant \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""${MODEL}"". ```. Can you please help me troubleshoot? I thought it might be something simple, like [this question](https://github.co",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:13097,security,LOG,LOGDIR,13097,"/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. ```. This is the script that I am running DeepVariant:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. BAM=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/82651510240740.mapped.sorted.markdup.realn.recal.bam. MODEL=/mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. ## step #1. LOGDIR=logs. N_SHARDS=4. #mkdir -p ""${LOGDIR}"". #time seq 0 $((N_SHARDS-1)) | \. # parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" \. # sudo docker run \. # -v /mnt/efs-genome:/mnt/efs-genome \. # gcr.io/deepvariant-docker/deepvariant \. # /opt/deepvariant/bin/make_examples \. # --mode calling \. # --ref ""${REF}"" \. # --reads ""${BAM}"" \. # --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. # --task {}. ## step #2. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". sudo docker run \. -v /mnt/efs-genome:/mnt/efs-genome \. gcr.io/deepvariant-docker/deepvariant \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""${MODEL}"". ```. Can you please help me troubleshoot? I thought it might be something simple, like [this question](https://github.com/google/deepvariant/issues/129). However, that particular solution is not working for me. Thank you",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:13104,security,log,logs,13104,"enome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. ```. This is the script that I am running DeepVariant:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. BAM=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/82651510240740.mapped.sorted.markdup.realn.recal.bam. MODEL=/mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. ## step #1. LOGDIR=logs. N_SHARDS=4. #mkdir -p ""${LOGDIR}"". #time seq 0 $((N_SHARDS-1)) | \. # parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" \. # sudo docker run \. # -v /mnt/efs-genome:/mnt/efs-genome \. # gcr.io/deepvariant-docker/deepvariant \. # /opt/deepvariant/bin/make_examples \. # --mode calling \. # --ref ""${REF}"" \. # --reads ""${BAM}"" \. # --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. # --task {}. ## step #2. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". sudo docker run \. -v /mnt/efs-genome:/mnt/efs-genome \. gcr.io/deepvariant-docker/deepvariant \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""${MODEL}"". ```. Can you please help me troubleshoot? I thought it might be something simple, like [this question](https://github.com/google/deepvariant/issues/129). However, that particular solution is not working for me. Thank you very ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:13135,security,LOG,LOGDIR,13135,"t-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. ```. This is the script that I am running DeepVariant:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. BAM=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/82651510240740.mapped.sorted.markdup.realn.recal.bam. MODEL=/mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. ## step #1. LOGDIR=logs. N_SHARDS=4. #mkdir -p ""${LOGDIR}"". #time seq 0 $((N_SHARDS-1)) | \. # parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" \. # sudo docker run \. # -v /mnt/efs-genome:/mnt/efs-genome \. # gcr.io/deepvariant-docker/deepvariant \. # /opt/deepvariant/bin/make_examples \. # --mode calling \. # --ref ""${REF}"" \. # --reads ""${BAM}"" \. # --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. # --task {}. ## step #2. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". sudo docker run \. -v /mnt/efs-genome:/mnt/efs-genome \. gcr.io/deepvariant-docker/deepvariant \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""${MODEL}"". ```. Can you please help me troubleshoot? I thought it might be something simple, like [this question](https://github.com/google/deepvariant/issues/129). However, that particular solution is not working for me. Thank you very much for your assistance. Sincer",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:13216,security,LOG,LOGDIR,13216,"3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. ```. This is the script that I am running DeepVariant:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. BAM=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/82651510240740.mapped.sorted.markdup.realn.recal.bam. MODEL=/mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. ## step #1. LOGDIR=logs. N_SHARDS=4. #mkdir -p ""${LOGDIR}"". #time seq 0 $((N_SHARDS-1)) | \. # parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" \. # sudo docker run \. # -v /mnt/efs-genome:/mnt/efs-genome \. # gcr.io/deepvariant-docker/deepvariant \. # /opt/deepvariant/bin/make_examples \. # --mode calling \. # --ref ""${REF}"" \. # --reads ""${BAM}"" \. # --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. # --task {}. ## step #2. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". sudo docker run \. -v /mnt/efs-genome:/mnt/efs-genome \. gcr.io/deepvariant-docker/deepvariant \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""${MODEL}"". ```. Can you please help me troubleshoot? I thought it might be something simple, like [this question](https://github.com/google/deepvariant/issues/129). However, that particular solution is not working for me. Thank you very much for your assistance. Sincerely,. Charles",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:13224,security,log,log,13224,"3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. ```. This is the script that I am running DeepVariant:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. BAM=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/82651510240740.mapped.sorted.markdup.realn.recal.bam. MODEL=/mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. ## step #1. LOGDIR=logs. N_SHARDS=4. #mkdir -p ""${LOGDIR}"". #time seq 0 $((N_SHARDS-1)) | \. # parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" \. # sudo docker run \. # -v /mnt/efs-genome:/mnt/efs-genome \. # gcr.io/deepvariant-docker/deepvariant \. # /opt/deepvariant/bin/make_examples \. # --mode calling \. # --ref ""${REF}"" \. # --reads ""${BAM}"" \. # --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. # --task {}. ## step #2. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". sudo docker run \. -v /mnt/efs-genome:/mnt/efs-genome \. gcr.io/deepvariant-docker/deepvariant \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""${MODEL}"". ```. Can you please help me troubleshoot? I thought it might be something simple, like [this question](https://github.com/google/deepvariant/issues/129). However, that particular solution is not working for me. Thank you very much for your assistance. Sincerely,. Charles",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:13238,security,LOG,LOGDIR,13238,"3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. ```. This is the script that I am running DeepVariant:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. BAM=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/82651510240740.mapped.sorted.markdup.realn.recal.bam. MODEL=/mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. ## step #1. LOGDIR=logs. N_SHARDS=4. #mkdir -p ""${LOGDIR}"". #time seq 0 $((N_SHARDS-1)) | \. # parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" \. # sudo docker run \. # -v /mnt/efs-genome:/mnt/efs-genome \. # gcr.io/deepvariant-docker/deepvariant \. # /opt/deepvariant/bin/make_examples \. # --mode calling \. # --ref ""${REF}"" \. # --reads ""${BAM}"" \. # --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. # --task {}. ## step #2. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". sudo docker run \. -v /mnt/efs-genome:/mnt/efs-genome \. gcr.io/deepvariant-docker/deepvariant \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""${MODEL}"". ```. Can you please help me troubleshoot? I thought it might be something simple, like [this question](https://github.com/google/deepvariant/issues/129). However, that particular solution is not working for me. Thank you very much for your assistance. Sincerely,. Charles",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:13871,security,MODEL,MODEL,13871,"3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. ```. This is the script that I am running DeepVariant:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. BAM=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/82651510240740.mapped.sorted.markdup.realn.recal.bam. MODEL=/mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. ## step #1. LOGDIR=logs. N_SHARDS=4. #mkdir -p ""${LOGDIR}"". #time seq 0 $((N_SHARDS-1)) | \. # parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" \. # sudo docker run \. # -v /mnt/efs-genome:/mnt/efs-genome \. # gcr.io/deepvariant-docker/deepvariant \. # /opt/deepvariant/bin/make_examples \. # --mode calling \. # --ref ""${REF}"" \. # --reads ""${BAM}"" \. # --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. # --task {}. ## step #2. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". sudo docker run \. -v /mnt/efs-genome:/mnt/efs-genome \. gcr.io/deepvariant-docker/deepvariant \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""${MODEL}"". ```. Can you please help me troubleshoot? I thought it might be something simple, like [this question](https://github.com/google/deepvariant/issues/129). However, that particular solution is not working for me. Thank you very much for your assistance. Sincerely,. Charles",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:105,testability,test,testing,105,"DataLossError / Failed precondition when running WES call_variant; Hello,. I have been trying to do some testing of DeepVariant on my own Exome (WES) and WGS data. As a starting point, I was trying to work with calling variants from the .bam file provided for my WES data. I started running from within a Docker container on my local computer but that was taking a long time (and, ultimately, the _make_examples_ step did not run to completion). I started learning more about the AWS options for analysis, and I was able to run the _make_examples_ much quicker (and successfully) on an AWS m5.xlarge ECS instance (although I am admittedly well over the ~25 minutes and $0.20 time/cost mentioned for Google Cloud, just for the _make_examples_, without considering upload/download, long-term storage, etc.). While I was hoping to eventually compare running things on Google Cloud (and I think my experience so far probably helps me ask better questions), I was wondering if you could help me troubleshoot something that I think is probably close to working:. Essentially, I am currently at the **call_variant** step of DeepVariant, with WES data. This is the error message that I am currently receiving:. ```. sudo sh run_deepvariant.sh. I0331 18:31:22.446569 140549764839168 call_variants.py:292] Set KMP_BLOCKTIME to 0. 2019-03-31 18:31:22.486802: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2 AVX512F FMA. 2019-03-31 18:31:22.489180: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. I0331 18:31:22.527594 140549764839168 modeling.py:351] Initializing model with random parameters. W0331 18:31:22.529449 140549764839168 tf_logging.py:125] Using temporary folder as model directory: /tmp/tmpuBleAQ. I0331 18:31:22.529786 140549764839168 tf_logging.py:115] Using config: {'_sav",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:4613,testability,Trace,Traceback,4613,"3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? 2019-03-31 18:31:26.450035: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? 2019-03-31 18:31:26.450061: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:172 : Data loss: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>. tf.app.run(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 401, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 357, in call_variants. prediction = next(predictions). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 567, in predict. hooks=all_hooks) as mon_sess:. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 921, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 643,",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:5313,testability,hook,hooks,5313,"nt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>. tf.app.run(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 401, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 357, in call_variants. prediction = next(predictions). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 567, in predict. hooks=all_hooks) as mon_sess:. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 921, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 643, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1107, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1112, in _create_session. return self._sess_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.p",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:8795,testability,hook,hooks,8795,"RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. Caused by op u'save_1/RestoreV2', defined at:. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>. tf.app.run(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 401, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 357, in call_variants. prediction = next(predictions). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 567, in predict. hooks=all_hooks) as mon_sess:. File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 921, in __init__. stop_grace_period_secs=stop_grace_period_secs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 643, in __init__. self._sess = _RecoverableSession(self._coordinated_creator). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1107, in __init__. _WrappedSession.__init__(self, self._create_session()). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1112, in _create_session. return self._sess_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session. self.tf_sess = self._session_creator.create_session(). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.p",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:12058,testability,trace,traceback,12058,"y"", line 406, in _AddRestoreOps. restore_sequentially). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 854, in bulk_restore. return io_ops.restore_v2(filename_tensor, names, slices, dtypes). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 1466, in restore_v2. shape_and_slices=shape_and_slices, dtypes=dtypes, name=name). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper. op_def=op_def). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func. return func(*args, **kwargs). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 3272, in create_op. op_def=op_def). File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1768, in __init__. self._traceback = tf_stack.extract_stack(). DataLossError (see above for traceback): Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. ```. This is the script that I am running DeepVariant:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. BAM=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/82651510240740.mapped.sorted.markdup.realn.recal.bam. MODEL=/mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:13097,testability,LOG,LOGDIR,13097,"/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. ```. This is the script that I am running DeepVariant:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. BAM=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/82651510240740.mapped.sorted.markdup.realn.recal.bam. MODEL=/mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. ## step #1. LOGDIR=logs. N_SHARDS=4. #mkdir -p ""${LOGDIR}"". #time seq 0 $((N_SHARDS-1)) | \. # parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" \. # sudo docker run \. # -v /mnt/efs-genome:/mnt/efs-genome \. # gcr.io/deepvariant-docker/deepvariant \. # /opt/deepvariant/bin/make_examples \. # --mode calling \. # --ref ""${REF}"" \. # --reads ""${BAM}"" \. # --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. # --task {}. ## step #2. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". sudo docker run \. -v /mnt/efs-genome:/mnt/efs-genome \. gcr.io/deepvariant-docker/deepvariant \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""${MODEL}"". ```. Can you please help me troubleshoot? I thought it might be something simple, like [this question](https://github.com/google/deepvariant/issues/129). However, that particular solution is not working for me. Thank you",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:13104,testability,log,logs,13104,"enome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. ```. This is the script that I am running DeepVariant:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. BAM=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/82651510240740.mapped.sorted.markdup.realn.recal.bam. MODEL=/mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. ## step #1. LOGDIR=logs. N_SHARDS=4. #mkdir -p ""${LOGDIR}"". #time seq 0 $((N_SHARDS-1)) | \. # parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" \. # sudo docker run \. # -v /mnt/efs-genome:/mnt/efs-genome \. # gcr.io/deepvariant-docker/deepvariant \. # /opt/deepvariant/bin/make_examples \. # --mode calling \. # --ref ""${REF}"" \. # --reads ""${BAM}"" \. # --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. # --task {}. ## step #2. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". sudo docker run \. -v /mnt/efs-genome:/mnt/efs-genome \. gcr.io/deepvariant-docker/deepvariant \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""${MODEL}"". ```. Can you please help me troubleshoot? I thought it might be something simple, like [this question](https://github.com/google/deepvariant/issues/129). However, that particular solution is not working for me. Thank you very ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:13135,testability,LOG,LOGDIR,13135,"t-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. ```. This is the script that I am running DeepVariant:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. BAM=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/82651510240740.mapped.sorted.markdup.realn.recal.bam. MODEL=/mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. ## step #1. LOGDIR=logs. N_SHARDS=4. #mkdir -p ""${LOGDIR}"". #time seq 0 $((N_SHARDS-1)) | \. # parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" \. # sudo docker run \. # -v /mnt/efs-genome:/mnt/efs-genome \. # gcr.io/deepvariant-docker/deepvariant \. # /opt/deepvariant/bin/make_examples \. # --mode calling \. # --ref ""${REF}"" \. # --reads ""${BAM}"" \. # --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. # --task {}. ## step #2. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". sudo docker run \. -v /mnt/efs-genome:/mnt/efs-genome \. gcr.io/deepvariant-docker/deepvariant \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""${MODEL}"". ```. Can you please help me troubleshoot? I thought it might be something simple, like [this question](https://github.com/google/deepvariant/issues/129). However, that particular solution is not working for me. Thank you very much for your assistance. Sincer",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:13216,testability,LOG,LOGDIR,13216,"3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. ```. This is the script that I am running DeepVariant:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. BAM=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/82651510240740.mapped.sorted.markdup.realn.recal.bam. MODEL=/mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. ## step #1. LOGDIR=logs. N_SHARDS=4. #mkdir -p ""${LOGDIR}"". #time seq 0 $((N_SHARDS-1)) | \. # parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" \. # sudo docker run \. # -v /mnt/efs-genome:/mnt/efs-genome \. # gcr.io/deepvariant-docker/deepvariant \. # /opt/deepvariant/bin/make_examples \. # --mode calling \. # --ref ""${REF}"" \. # --reads ""${BAM}"" \. # --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. # --task {}. ## step #2. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". sudo docker run \. -v /mnt/efs-genome:/mnt/efs-genome \. gcr.io/deepvariant-docker/deepvariant \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""${MODEL}"". ```. Can you please help me troubleshoot? I thought it might be something simple, like [this question](https://github.com/google/deepvariant/issues/129). However, that particular solution is not working for me. Thank you very much for your assistance. Sincerely,. Charles",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:13224,testability,log,log,13224,"3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. ```. This is the script that I am running DeepVariant:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. BAM=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/82651510240740.mapped.sorted.markdup.realn.recal.bam. MODEL=/mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. ## step #1. LOGDIR=logs. N_SHARDS=4. #mkdir -p ""${LOGDIR}"". #time seq 0 $((N_SHARDS-1)) | \. # parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" \. # sudo docker run \. # -v /mnt/efs-genome:/mnt/efs-genome \. # gcr.io/deepvariant-docker/deepvariant \. # /opt/deepvariant/bin/make_examples \. # --mode calling \. # --ref ""${REF}"" \. # --reads ""${BAM}"" \. # --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. # --task {}. ## step #2. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". sudo docker run \. -v /mnt/efs-genome:/mnt/efs-genome \. gcr.io/deepvariant-docker/deepvariant \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""${MODEL}"". ```. Can you please help me troubleshoot? I thought it might be something simple, like [this question](https://github.com/google/deepvariant/issues/129). However, that particular solution is not working for me. Thank you very much for your assistance. Sincerely,. Charles",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:13238,testability,LOG,LOGDIR,13238,"3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. ```. This is the script that I am running DeepVariant:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. BAM=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/82651510240740.mapped.sorted.markdup.realn.recal.bam. MODEL=/mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. ## step #1. LOGDIR=logs. N_SHARDS=4. #mkdir -p ""${LOGDIR}"". #time seq 0 $((N_SHARDS-1)) | \. # parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" \. # sudo docker run \. # -v /mnt/efs-genome:/mnt/efs-genome \. # gcr.io/deepvariant-docker/deepvariant \. # /opt/deepvariant/bin/make_examples \. # --mode calling \. # --ref ""${REF}"" \. # --reads ""${BAM}"" \. # --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. # --task {}. ## step #2. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". sudo docker run \. -v /mnt/efs-genome:/mnt/efs-genome \. gcr.io/deepvariant-docker/deepvariant \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""${MODEL}"". ```. Can you please help me troubleshoot? I thought it might be something simple, like [this question](https://github.com/google/deepvariant/issues/129). However, that particular solution is not working for me. Thank you very much for your assistance. Sincerely,. Charles",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:13954,testability,simpl,simple,13954,"3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. ```. This is the script that I am running DeepVariant:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. BAM=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/82651510240740.mapped.sorted.markdup.realn.recal.bam. MODEL=/mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. ## step #1. LOGDIR=logs. N_SHARDS=4. #mkdir -p ""${LOGDIR}"". #time seq 0 $((N_SHARDS-1)) | \. # parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" \. # sudo docker run \. # -v /mnt/efs-genome:/mnt/efs-genome \. # gcr.io/deepvariant-docker/deepvariant \. # /opt/deepvariant/bin/make_examples \. # --mode calling \. # --ref ""${REF}"" \. # --reads ""${BAM}"" \. # --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. # --task {}. ## step #2. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". sudo docker run \. -v /mnt/efs-genome:/mnt/efs-genome \. gcr.io/deepvariant-docker/deepvariant \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""${MODEL}"". ```. Can you please help me troubleshoot? I thought it might be something simple, like [this question](https://github.com/google/deepvariant/issues/129). However, that particular solution is not working for me. Thank you very much for your assistance. Sincerely,. Charles",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:456,usability,learn,learning,456,"DataLossError / Failed precondition when running WES call_variant; Hello,. I have been trying to do some testing of DeepVariant on my own Exome (WES) and WGS data. As a starting point, I was trying to work with calling variants from the .bam file provided for my WES data. I started running from within a Docker container on my local computer but that was taking a long time (and, ultimately, the _make_examples_ step did not run to completion). I started learning more about the AWS options for analysis, and I was able to run the _make_examples_ much quicker (and successfully) on an AWS m5.xlarge ECS instance (although I am admittedly well over the ~25 minutes and $0.20 time/cost mentioned for Google Cloud, just for the _make_examples_, without considering upload/download, long-term storage, etc.). While I was hoping to eventually compare running things on Google Cloud (and I think my experience so far probably helps me ask better questions), I was wondering if you could help me troubleshoot something that I think is probably close to working:. Essentially, I am currently at the **call_variant** step of DeepVariant, with WES data. This is the error message that I am currently receiving:. ```. sudo sh run_deepvariant.sh. I0331 18:31:22.446569 140549764839168 call_variants.py:292] Set KMP_BLOCKTIME to 0. 2019-03-31 18:31:22.486802: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2 AVX512F FMA. 2019-03-31 18:31:22.489180: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. I0331 18:31:22.527594 140549764839168 modeling.py:351] Initializing model with random parameters. W0331 18:31:22.529449 140549764839168 tf_logging.py:125] Using temporary folder as model directory: /tmp/tmpuBleAQ. I0331 18:31:22.529786 140549764839168 tf_logging.py:115] Using config: {'_sav",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:894,usability,experien,experience,894,"DataLossError / Failed precondition when running WES call_variant; Hello,. I have been trying to do some testing of DeepVariant on my own Exome (WES) and WGS data. As a starting point, I was trying to work with calling variants from the .bam file provided for my WES data. I started running from within a Docker container on my local computer but that was taking a long time (and, ultimately, the _make_examples_ step did not run to completion). I started learning more about the AWS options for analysis, and I was able to run the _make_examples_ much quicker (and successfully) on an AWS m5.xlarge ECS instance (although I am admittedly well over the ~25 minutes and $0.20 time/cost mentioned for Google Cloud, just for the _make_examples_, without considering upload/download, long-term storage, etc.). While I was hoping to eventually compare running things on Google Cloud (and I think my experience so far probably helps me ask better questions), I was wondering if you could help me troubleshoot something that I think is probably close to working:. Essentially, I am currently at the **call_variant** step of DeepVariant, with WES data. This is the error message that I am currently receiving:. ```. sudo sh run_deepvariant.sh. I0331 18:31:22.446569 140549764839168 call_variants.py:292] Set KMP_BLOCKTIME to 0. 2019-03-31 18:31:22.486802: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2 AVX512F FMA. 2019-03-31 18:31:22.489180: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. I0331 18:31:22.527594 140549764839168 modeling.py:351] Initializing model with random parameters. W0331 18:31:22.529449 140549764839168 tf_logging.py:125] Using temporary folder as model directory: /tmp/tmpuBleAQ. I0331 18:31:22.529786 140549764839168 tf_logging.py:115] Using config: {'_sav",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:921,usability,help,helps,921,"DataLossError / Failed precondition when running WES call_variant; Hello,. I have been trying to do some testing of DeepVariant on my own Exome (WES) and WGS data. As a starting point, I was trying to work with calling variants from the .bam file provided for my WES data. I started running from within a Docker container on my local computer but that was taking a long time (and, ultimately, the _make_examples_ step did not run to completion). I started learning more about the AWS options for analysis, and I was able to run the _make_examples_ much quicker (and successfully) on an AWS m5.xlarge ECS instance (although I am admittedly well over the ~25 minutes and $0.20 time/cost mentioned for Google Cloud, just for the _make_examples_, without considering upload/download, long-term storage, etc.). While I was hoping to eventually compare running things on Google Cloud (and I think my experience so far probably helps me ask better questions), I was wondering if you could help me troubleshoot something that I think is probably close to working:. Essentially, I am currently at the **call_variant** step of DeepVariant, with WES data. This is the error message that I am currently receiving:. ```. sudo sh run_deepvariant.sh. I0331 18:31:22.446569 140549764839168 call_variants.py:292] Set KMP_BLOCKTIME to 0. 2019-03-31 18:31:22.486802: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2 AVX512F FMA. 2019-03-31 18:31:22.489180: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. I0331 18:31:22.527594 140549764839168 modeling.py:351] Initializing model with random parameters. W0331 18:31:22.529449 140549764839168 tf_logging.py:125] Using temporary folder as model directory: /tmp/tmpuBleAQ. I0331 18:31:22.529786 140549764839168 tf_logging.py:115] Using config: {'_sav",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:982,usability,help,help,982,"DataLossError / Failed precondition when running WES call_variant; Hello,. I have been trying to do some testing of DeepVariant on my own Exome (WES) and WGS data. As a starting point, I was trying to work with calling variants from the .bam file provided for my WES data. I started running from within a Docker container on my local computer but that was taking a long time (and, ultimately, the _make_examples_ step did not run to completion). I started learning more about the AWS options for analysis, and I was able to run the _make_examples_ much quicker (and successfully) on an AWS m5.xlarge ECS instance (although I am admittedly well over the ~25 minutes and $0.20 time/cost mentioned for Google Cloud, just for the _make_examples_, without considering upload/download, long-term storage, etc.). While I was hoping to eventually compare running things on Google Cloud (and I think my experience so far probably helps me ask better questions), I was wondering if you could help me troubleshoot something that I think is probably close to working:. Essentially, I am currently at the **call_variant** step of DeepVariant, with WES data. This is the error message that I am currently receiving:. ```. sudo sh run_deepvariant.sh. I0331 18:31:22.446569 140549764839168 call_variants.py:292] Set KMP_BLOCKTIME to 0. 2019-03-31 18:31:22.486802: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2 AVX512F FMA. 2019-03-31 18:31:22.489180: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. I0331 18:31:22.527594 140549764839168 modeling.py:351] Initializing model with random parameters. W0331 18:31:22.529449 140549764839168 tf_logging.py:125] Using temporary folder as model directory: /tmp/tmpuBleAQ. I0331 18:31:22.529786 140549764839168 tf_logging.py:115] Using config: {'_sav",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:1038,usability,close,close,1038,"running WES call_variant; Hello,. I have been trying to do some testing of DeepVariant on my own Exome (WES) and WGS data. As a starting point, I was trying to work with calling variants from the .bam file provided for my WES data. I started running from within a Docker container on my local computer but that was taking a long time (and, ultimately, the _make_examples_ step did not run to completion). I started learning more about the AWS options for analysis, and I was able to run the _make_examples_ much quicker (and successfully) on an AWS m5.xlarge ECS instance (although I am admittedly well over the ~25 minutes and $0.20 time/cost mentioned for Google Cloud, just for the _make_examples_, without considering upload/download, long-term storage, etc.). While I was hoping to eventually compare running things on Google Cloud (and I think my experience so far probably helps me ask better questions), I was wondering if you could help me troubleshoot something that I think is probably close to working:. Essentially, I am currently at the **call_variant** step of DeepVariant, with WES data. This is the error message that I am currently receiving:. ```. sudo sh run_deepvariant.sh. I0331 18:31:22.446569 140549764839168 call_variants.py:292] Set KMP_BLOCKTIME to 0. 2019-03-31 18:31:22.486802: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2 AVX512F FMA. 2019-03-31 18:31:22.489180: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. I0331 18:31:22.527594 140549764839168 modeling.py:351] Initializing model with random parameters. W0331 18:31:22.529449 140549764839168 tf_logging.py:125] Using temporary folder as model directory: /tmp/tmpuBleAQ. I0331 18:31:22.529786 140549764839168 tf_logging.py:115] Using config: {'_save_checkpoints_secs': 1000, '_num_ps_repli",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:1157,usability,error,error,1157,"ta. As a starting point, I was trying to work with calling variants from the .bam file provided for my WES data. I started running from within a Docker container on my local computer but that was taking a long time (and, ultimately, the _make_examples_ step did not run to completion). I started learning more about the AWS options for analysis, and I was able to run the _make_examples_ much quicker (and successfully) on an AWS m5.xlarge ECS instance (although I am admittedly well over the ~25 minutes and $0.20 time/cost mentioned for Google Cloud, just for the _make_examples_, without considering upload/download, long-term storage, etc.). While I was hoping to eventually compare running things on Google Cloud (and I think my experience so far probably helps me ask better questions), I was wondering if you could help me troubleshoot something that I think is probably close to working:. Essentially, I am currently at the **call_variant** step of DeepVariant, with WES data. This is the error message that I am currently receiving:. ```. sudo sh run_deepvariant.sh. I0331 18:31:22.446569 140549764839168 call_variants.py:292] Set KMP_BLOCKTIME to 0. 2019-03-31 18:31:22.486802: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2 AVX512F FMA. 2019-03-31 18:31:22.489180: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. I0331 18:31:22.527594 140549764839168 modeling.py:351] Initializing model with random parameters. W0331 18:31:22.529449 140549764839168 tf_logging.py:125] Using temporary folder as model directory: /tmp/tmpuBleAQ. I0331 18:31:22.529786 140549764839168 tf_logging.py:115] Using config: {'_save_checkpoints_secs': 1000, '_num_ps_replicas': 0, '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_clust",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:1410,usability,support,supports,1410,"tep did not run to completion). I started learning more about the AWS options for analysis, and I was able to run the _make_examples_ much quicker (and successfully) on an AWS m5.xlarge ECS instance (although I am admittedly well over the ~25 minutes and $0.20 time/cost mentioned for Google Cloud, just for the _make_examples_, without considering upload/download, long-term storage, etc.). While I was hoping to eventually compare running things on Google Cloud (and I think my experience so far probably helps me ask better questions), I was wondering if you could help me troubleshoot something that I think is probably close to working:. Essentially, I am currently at the **call_variant** step of DeepVariant, with WES data. This is the error message that I am currently receiving:. ```. sudo sh run_deepvariant.sh. I0331 18:31:22.446569 140549764839168 call_variants.py:292] Set KMP_BLOCKTIME to 0. 2019-03-31 18:31:22.486802: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2 AVX512F FMA. 2019-03-31 18:31:22.489180: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. I0331 18:31:22.527594 140549764839168 modeling.py:351] Initializing model with random parameters. W0331 18:31:22.529449 140549764839168 tf_logging.py:125] Using temporary folder as model directory: /tmp/tmpuBleAQ. I0331 18:31:22.529786 140549764839168 tf_logging.py:115] Using config: {'_save_checkpoints_secs': 1000, '_num_ps_replicas': 0, '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd40e875e50>, '_model_dir': '/tmp/tmpuBleAQ', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_session_config': allo",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:1696,usability,perform,performance,1696,"gle Cloud, just for the _make_examples_, without considering upload/download, long-term storage, etc.). While I was hoping to eventually compare running things on Google Cloud (and I think my experience so far probably helps me ask better questions), I was wondering if you could help me troubleshoot something that I think is probably close to working:. Essentially, I am currently at the **call_variant** step of DeepVariant, with WES data. This is the error message that I am currently receiving:. ```. sudo sh run_deepvariant.sh. I0331 18:31:22.446569 140549764839168 call_variants.py:292] Set KMP_BLOCKTIME to 0. 2019-03-31 18:31:22.486802: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2 AVX512F FMA. 2019-03-31 18:31:22.489180: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance. I0331 18:31:22.527594 140549764839168 modeling.py:351] Initializing model with random parameters. W0331 18:31:22.529449 140549764839168 tf_logging.py:125] Using temporary folder as model directory: /tmp/tmpuBleAQ. I0331 18:31:22.529786 140549764839168 tf_logging.py:115] Using config: {'_save_checkpoints_secs': 1000, '_num_ps_replicas': 0, '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd40e875e50>, '_model_dir': '/tmp/tmpuBleAQ', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluat",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:13900,usability,help,help,13900,"3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. ```. This is the script that I am running DeepVariant:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. BAM=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/82651510240740.mapped.sorted.markdup.realn.recal.bam. MODEL=/mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. ## step #1. LOGDIR=logs. N_SHARDS=4. #mkdir -p ""${LOGDIR}"". #time seq 0 $((N_SHARDS-1)) | \. # parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" \. # sudo docker run \. # -v /mnt/efs-genome:/mnt/efs-genome \. # gcr.io/deepvariant-docker/deepvariant \. # /opt/deepvariant/bin/make_examples \. # --mode calling \. # --ref ""${REF}"" \. # --reads ""${BAM}"" \. # --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. # --task {}. ## step #2. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". sudo docker run \. -v /mnt/efs-genome:/mnt/efs-genome \. gcr.io/deepvariant-docker/deepvariant \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""${MODEL}"". ```. Can you please help me troubleshoot? I thought it might be something simple, like [this question](https://github.com/google/deepvariant/issues/129). However, that particular solution is not working for me. Thank you very much for your assistance. Sincerely,. Charles",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/166:13954,usability,simpl,simple,13954,"3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator? [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. ```. This is the script that I am running DeepVariant:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. BAM=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/82651510240740.mapped.sorted.markdup.realn.recal.bam. MODEL=/mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. ## step #1. LOGDIR=logs. N_SHARDS=4. #mkdir -p ""${LOGDIR}"". #time seq 0 $((N_SHARDS-1)) | \. # parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" \. # sudo docker run \. # -v /mnt/efs-genome:/mnt/efs-genome \. # gcr.io/deepvariant-docker/deepvariant \. # /opt/deepvariant/bin/make_examples \. # --mode calling \. # --ref ""${REF}"" \. # --reads ""${BAM}"" \. # --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. # --task {}. ## step #2. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". sudo docker run \. -v /mnt/efs-genome:/mnt/efs-genome \. gcr.io/deepvariant-docker/deepvariant \. /opt/deepvariant/bin/call_variants \. --outfile ""${CALL_VARIANTS_OUTPUT}"" \. --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \. --checkpoint ""${MODEL}"". ```. Can you please help me troubleshoot? I thought it might be something simple, like [this question](https://github.com/google/deepvariant/issues/129). However, that particular solution is not working for me. Thank you very much for your assistance. Sincerely,. Charles",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/166
https://github.com/google/deepvariant/issues/167:288,availability,error,error,288,"bad_alloc (memory?) issue postprocess_variants; Hi,. As a follow-up to my [previous question](https://github.com/google/deepvariant/issues/166), I am trying to run the **postprocess_variants** command for DeepVariant from a Docker container on an AWS instance. I am getting the following error message:. ```. terminate called after throwing an instance of 'std::bad_alloc'. what(): std::bad_alloc. ```. I thought this might have been an issue with memory allocation, but I have been testing the same command with increasing computational resources (I am currently using a m5.4xlarge instance). **1)** Am I totally wrong about the underlying cause? If so, is there anything you can suggest to troubleshoot this issue? **2)** Based upon my Google searches, it seemed like this might have something to do with using TensorFlow. Is it easy to tell if that is correct? If so, does that mean you are still doing variant calling/prediction from the **postprocess_variants** command? This is the command that I am running:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". FINAL_OUTPUT_VCF=""${OUTPUT_DIR}/output.vcf.gz"". sudo docker run \. -v /mnt/efs-genome:/mnt/efs-genome \. gcr.io/deepvariant-docker/deepvariant \. /opt/deepvariant/bin/postprocess_variants \. --ref ""${REF}"" \. --infile ""${CALL_VARIANTS_OUTPUT}"" \. --outfile ""${FINAL_OUTPUT_VCF}"". ```. Thank you very much for your assistance! Sincerely,. Charles.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167
https://github.com/google/deepvariant/issues/167:231,deployability,contain,container,231,"bad_alloc (memory?) issue postprocess_variants; Hi,. As a follow-up to my [previous question](https://github.com/google/deepvariant/issues/166), I am trying to run the **postprocess_variants** command for DeepVariant from a Docker container on an AWS instance. I am getting the following error message:. ```. terminate called after throwing an instance of 'std::bad_alloc'. what(): std::bad_alloc. ```. I thought this might have been an issue with memory allocation, but I have been testing the same command with increasing computational resources (I am currently using a m5.4xlarge instance). **1)** Am I totally wrong about the underlying cause? If so, is there anything you can suggest to troubleshoot this issue? **2)** Based upon my Google searches, it seemed like this might have something to do with using TensorFlow. Is it easy to tell if that is correct? If so, does that mean you are still doing variant calling/prediction from the **postprocess_variants** command? This is the command that I am running:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". FINAL_OUTPUT_VCF=""${OUTPUT_DIR}/output.vcf.gz"". sudo docker run \. -v /mnt/efs-genome:/mnt/efs-genome \. gcr.io/deepvariant-docker/deepvariant \. /opt/deepvariant/bin/postprocess_variants \. --ref ""${REF}"" \. --infile ""${CALL_VARIANTS_OUTPUT}"" \. --outfile ""${FINAL_OUTPUT_VCF}"". ```. Thank you very much for your assistance! Sincerely,. Charles.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167
https://github.com/google/deepvariant/issues/167:538,deployability,resourc,resources,538,"bad_alloc (memory?) issue postprocess_variants; Hi,. As a follow-up to my [previous question](https://github.com/google/deepvariant/issues/166), I am trying to run the **postprocess_variants** command for DeepVariant from a Docker container on an AWS instance. I am getting the following error message:. ```. terminate called after throwing an instance of 'std::bad_alloc'. what(): std::bad_alloc. ```. I thought this might have been an issue with memory allocation, but I have been testing the same command with increasing computational resources (I am currently using a m5.4xlarge instance). **1)** Am I totally wrong about the underlying cause? If so, is there anything you can suggest to troubleshoot this issue? **2)** Based upon my Google searches, it seemed like this might have something to do with using TensorFlow. Is it easy to tell if that is correct? If so, does that mean you are still doing variant calling/prediction from the **postprocess_variants** command? This is the command that I am running:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". FINAL_OUTPUT_VCF=""${OUTPUT_DIR}/output.vcf.gz"". sudo docker run \. -v /mnt/efs-genome:/mnt/efs-genome \. gcr.io/deepvariant-docker/deepvariant \. /opt/deepvariant/bin/postprocess_variants \. --ref ""${REF}"" \. --infile ""${CALL_VARIANTS_OUTPUT}"" \. --outfile ""${FINAL_OUTPUT_VCF}"". ```. Thank you very much for your assistance! Sincerely,. Charles.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167
https://github.com/google/deepvariant/issues/167:455,energy efficiency,alloc,allocation,455,"bad_alloc (memory?) issue postprocess_variants; Hi,. As a follow-up to my [previous question](https://github.com/google/deepvariant/issues/166), I am trying to run the **postprocess_variants** command for DeepVariant from a Docker container on an AWS instance. I am getting the following error message:. ```. terminate called after throwing an instance of 'std::bad_alloc'. what(): std::bad_alloc. ```. I thought this might have been an issue with memory allocation, but I have been testing the same command with increasing computational resources (I am currently using a m5.4xlarge instance). **1)** Am I totally wrong about the underlying cause? If so, is there anything you can suggest to troubleshoot this issue? **2)** Based upon my Google searches, it seemed like this might have something to do with using TensorFlow. Is it easy to tell if that is correct? If so, does that mean you are still doing variant calling/prediction from the **postprocess_variants** command? This is the command that I am running:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". FINAL_OUTPUT_VCF=""${OUTPUT_DIR}/output.vcf.gz"". sudo docker run \. -v /mnt/efs-genome:/mnt/efs-genome \. gcr.io/deepvariant-docker/deepvariant \. /opt/deepvariant/bin/postprocess_variants \. --ref ""${REF}"" \. --infile ""${CALL_VARIANTS_OUTPUT}"" \. --outfile ""${FINAL_OUTPUT_VCF}"". ```. Thank you very much for your assistance! Sincerely,. Charles.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167
https://github.com/google/deepvariant/issues/167:538,energy efficiency,resourc,resources,538,"bad_alloc (memory?) issue postprocess_variants; Hi,. As a follow-up to my [previous question](https://github.com/google/deepvariant/issues/166), I am trying to run the **postprocess_variants** command for DeepVariant from a Docker container on an AWS instance. I am getting the following error message:. ```. terminate called after throwing an instance of 'std::bad_alloc'. what(): std::bad_alloc. ```. I thought this might have been an issue with memory allocation, but I have been testing the same command with increasing computational resources (I am currently using a m5.4xlarge instance). **1)** Am I totally wrong about the underlying cause? If so, is there anything you can suggest to troubleshoot this issue? **2)** Based upon my Google searches, it seemed like this might have something to do with using TensorFlow. Is it easy to tell if that is correct? If so, does that mean you are still doing variant calling/prediction from the **postprocess_variants** command? This is the command that I am running:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". FINAL_OUTPUT_VCF=""${OUTPUT_DIR}/output.vcf.gz"". sudo docker run \. -v /mnt/efs-genome:/mnt/efs-genome \. gcr.io/deepvariant-docker/deepvariant \. /opt/deepvariant/bin/postprocess_variants \. --ref ""${REF}"" \. --infile ""${CALL_VARIANTS_OUTPUT}"" \. --outfile ""${FINAL_OUTPUT_VCF}"". ```. Thank you very much for your assistance! Sincerely,. Charles.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167
https://github.com/google/deepvariant/issues/167:554,energy efficiency,current,currently,554,"bad_alloc (memory?) issue postprocess_variants; Hi,. As a follow-up to my [previous question](https://github.com/google/deepvariant/issues/166), I am trying to run the **postprocess_variants** command for DeepVariant from a Docker container on an AWS instance. I am getting the following error message:. ```. terminate called after throwing an instance of 'std::bad_alloc'. what(): std::bad_alloc. ```. I thought this might have been an issue with memory allocation, but I have been testing the same command with increasing computational resources (I am currently using a m5.4xlarge instance). **1)** Am I totally wrong about the underlying cause? If so, is there anything you can suggest to troubleshoot this issue? **2)** Based upon my Google searches, it seemed like this might have something to do with using TensorFlow. Is it easy to tell if that is correct? If so, does that mean you are still doing variant calling/prediction from the **postprocess_variants** command? This is the command that I am running:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". FINAL_OUTPUT_VCF=""${OUTPUT_DIR}/output.vcf.gz"". sudo docker run \. -v /mnt/efs-genome:/mnt/efs-genome \. gcr.io/deepvariant-docker/deepvariant \. /opt/deepvariant/bin/postprocess_variants \. --ref ""${REF}"" \. --infile ""${CALL_VARIANTS_OUTPUT}"" \. --outfile ""${FINAL_OUTPUT_VCF}"". ```. Thank you very much for your assistance! Sincerely,. Charles.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167
https://github.com/google/deepvariant/issues/167:922,energy efficiency,predict,prediction,922,"bad_alloc (memory?) issue postprocess_variants; Hi,. As a follow-up to my [previous question](https://github.com/google/deepvariant/issues/166), I am trying to run the **postprocess_variants** command for DeepVariant from a Docker container on an AWS instance. I am getting the following error message:. ```. terminate called after throwing an instance of 'std::bad_alloc'. what(): std::bad_alloc. ```. I thought this might have been an issue with memory allocation, but I have been testing the same command with increasing computational resources (I am currently using a m5.4xlarge instance). **1)** Am I totally wrong about the underlying cause? If so, is there anything you can suggest to troubleshoot this issue? **2)** Based upon my Google searches, it seemed like this might have something to do with using TensorFlow. Is it easy to tell if that is correct? If so, does that mean you are still doing variant calling/prediction from the **postprocess_variants** command? This is the command that I am running:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". FINAL_OUTPUT_VCF=""${OUTPUT_DIR}/output.vcf.gz"". sudo docker run \. -v /mnt/efs-genome:/mnt/efs-genome \. gcr.io/deepvariant-docker/deepvariant \. /opt/deepvariant/bin/postprocess_variants \. --ref ""${REF}"" \. --infile ""${CALL_VARIANTS_OUTPUT}"" \. --outfile ""${FINAL_OUTPUT_VCF}"". ```. Thank you very much for your assistance! Sincerely,. Charles.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167
https://github.com/google/deepvariant/issues/167:294,integrability,messag,message,294,"bad_alloc (memory?) issue postprocess_variants; Hi,. As a follow-up to my [previous question](https://github.com/google/deepvariant/issues/166), I am trying to run the **postprocess_variants** command for DeepVariant from a Docker container on an AWS instance. I am getting the following error message:. ```. terminate called after throwing an instance of 'std::bad_alloc'. what(): std::bad_alloc. ```. I thought this might have been an issue with memory allocation, but I have been testing the same command with increasing computational resources (I am currently using a m5.4xlarge instance). **1)** Am I totally wrong about the underlying cause? If so, is there anything you can suggest to troubleshoot this issue? **2)** Based upon my Google searches, it seemed like this might have something to do with using TensorFlow. Is it easy to tell if that is correct? If so, does that mean you are still doing variant calling/prediction from the **postprocess_variants** command? This is the command that I am running:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". FINAL_OUTPUT_VCF=""${OUTPUT_DIR}/output.vcf.gz"". sudo docker run \. -v /mnt/efs-genome:/mnt/efs-genome \. gcr.io/deepvariant-docker/deepvariant \. /opt/deepvariant/bin/postprocess_variants \. --ref ""${REF}"" \. --infile ""${CALL_VARIANTS_OUTPUT}"" \. --outfile ""${FINAL_OUTPUT_VCF}"". ```. Thank you very much for your assistance! Sincerely,. Charles.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167
https://github.com/google/deepvariant/issues/167:294,interoperability,messag,message,294,"bad_alloc (memory?) issue postprocess_variants; Hi,. As a follow-up to my [previous question](https://github.com/google/deepvariant/issues/166), I am trying to run the **postprocess_variants** command for DeepVariant from a Docker container on an AWS instance. I am getting the following error message:. ```. terminate called after throwing an instance of 'std::bad_alloc'. what(): std::bad_alloc. ```. I thought this might have been an issue with memory allocation, but I have been testing the same command with increasing computational resources (I am currently using a m5.4xlarge instance). **1)** Am I totally wrong about the underlying cause? If so, is there anything you can suggest to troubleshoot this issue? **2)** Based upon my Google searches, it seemed like this might have something to do with using TensorFlow. Is it easy to tell if that is correct? If so, does that mean you are still doing variant calling/prediction from the **postprocess_variants** command? This is the command that I am running:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". FINAL_OUTPUT_VCF=""${OUTPUT_DIR}/output.vcf.gz"". sudo docker run \. -v /mnt/efs-genome:/mnt/efs-genome \. gcr.io/deepvariant-docker/deepvariant \. /opt/deepvariant/bin/postprocess_variants \. --ref ""${REF}"" \. --infile ""${CALL_VARIANTS_OUTPUT}"" \. --outfile ""${FINAL_OUTPUT_VCF}"". ```. Thank you very much for your assistance! Sincerely,. Charles.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167
https://github.com/google/deepvariant/issues/167:11,performance,memor,memory,11,"bad_alloc (memory?) issue postprocess_variants; Hi,. As a follow-up to my [previous question](https://github.com/google/deepvariant/issues/166), I am trying to run the **postprocess_variants** command for DeepVariant from a Docker container on an AWS instance. I am getting the following error message:. ```. terminate called after throwing an instance of 'std::bad_alloc'. what(): std::bad_alloc. ```. I thought this might have been an issue with memory allocation, but I have been testing the same command with increasing computational resources (I am currently using a m5.4xlarge instance). **1)** Am I totally wrong about the underlying cause? If so, is there anything you can suggest to troubleshoot this issue? **2)** Based upon my Google searches, it seemed like this might have something to do with using TensorFlow. Is it easy to tell if that is correct? If so, does that mean you are still doing variant calling/prediction from the **postprocess_variants** command? This is the command that I am running:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". FINAL_OUTPUT_VCF=""${OUTPUT_DIR}/output.vcf.gz"". sudo docker run \. -v /mnt/efs-genome:/mnt/efs-genome \. gcr.io/deepvariant-docker/deepvariant \. /opt/deepvariant/bin/postprocess_variants \. --ref ""${REF}"" \. --infile ""${CALL_VARIANTS_OUTPUT}"" \. --outfile ""${FINAL_OUTPUT_VCF}"". ```. Thank you very much for your assistance! Sincerely,. Charles.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167
https://github.com/google/deepvariant/issues/167:288,performance,error,error,288,"bad_alloc (memory?) issue postprocess_variants; Hi,. As a follow-up to my [previous question](https://github.com/google/deepvariant/issues/166), I am trying to run the **postprocess_variants** command for DeepVariant from a Docker container on an AWS instance. I am getting the following error message:. ```. terminate called after throwing an instance of 'std::bad_alloc'. what(): std::bad_alloc. ```. I thought this might have been an issue with memory allocation, but I have been testing the same command with increasing computational resources (I am currently using a m5.4xlarge instance). **1)** Am I totally wrong about the underlying cause? If so, is there anything you can suggest to troubleshoot this issue? **2)** Based upon my Google searches, it seemed like this might have something to do with using TensorFlow. Is it easy to tell if that is correct? If so, does that mean you are still doing variant calling/prediction from the **postprocess_variants** command? This is the command that I am running:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". FINAL_OUTPUT_VCF=""${OUTPUT_DIR}/output.vcf.gz"". sudo docker run \. -v /mnt/efs-genome:/mnt/efs-genome \. gcr.io/deepvariant-docker/deepvariant \. /opt/deepvariant/bin/postprocess_variants \. --ref ""${REF}"" \. --infile ""${CALL_VARIANTS_OUTPUT}"" \. --outfile ""${FINAL_OUTPUT_VCF}"". ```. Thank you very much for your assistance! Sincerely,. Charles.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167
https://github.com/google/deepvariant/issues/167:448,performance,memor,memory,448,"bad_alloc (memory?) issue postprocess_variants; Hi,. As a follow-up to my [previous question](https://github.com/google/deepvariant/issues/166), I am trying to run the **postprocess_variants** command for DeepVariant from a Docker container on an AWS instance. I am getting the following error message:. ```. terminate called after throwing an instance of 'std::bad_alloc'. what(): std::bad_alloc. ```. I thought this might have been an issue with memory allocation, but I have been testing the same command with increasing computational resources (I am currently using a m5.4xlarge instance). **1)** Am I totally wrong about the underlying cause? If so, is there anything you can suggest to troubleshoot this issue? **2)** Based upon my Google searches, it seemed like this might have something to do with using TensorFlow. Is it easy to tell if that is correct? If so, does that mean you are still doing variant calling/prediction from the **postprocess_variants** command? This is the command that I am running:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". FINAL_OUTPUT_VCF=""${OUTPUT_DIR}/output.vcf.gz"". sudo docker run \. -v /mnt/efs-genome:/mnt/efs-genome \. gcr.io/deepvariant-docker/deepvariant \. /opt/deepvariant/bin/postprocess_variants \. --ref ""${REF}"" \. --infile ""${CALL_VARIANTS_OUTPUT}"" \. --outfile ""${FINAL_OUTPUT_VCF}"". ```. Thank you very much for your assistance! Sincerely,. Charles.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167
https://github.com/google/deepvariant/issues/167:524,performance,computational resourc,computational resources,524,"bad_alloc (memory?) issue postprocess_variants; Hi,. As a follow-up to my [previous question](https://github.com/google/deepvariant/issues/166), I am trying to run the **postprocess_variants** command for DeepVariant from a Docker container on an AWS instance. I am getting the following error message:. ```. terminate called after throwing an instance of 'std::bad_alloc'. what(): std::bad_alloc. ```. I thought this might have been an issue with memory allocation, but I have been testing the same command with increasing computational resources (I am currently using a m5.4xlarge instance). **1)** Am I totally wrong about the underlying cause? If so, is there anything you can suggest to troubleshoot this issue? **2)** Based upon my Google searches, it seemed like this might have something to do with using TensorFlow. Is it easy to tell if that is correct? If so, does that mean you are still doing variant calling/prediction from the **postprocess_variants** command? This is the command that I am running:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". FINAL_OUTPUT_VCF=""${OUTPUT_DIR}/output.vcf.gz"". sudo docker run \. -v /mnt/efs-genome:/mnt/efs-genome \. gcr.io/deepvariant-docker/deepvariant \. /opt/deepvariant/bin/postprocess_variants \. --ref ""${REF}"" \. --infile ""${CALL_VARIANTS_OUTPUT}"" \. --outfile ""${FINAL_OUTPUT_VCF}"". ```. Thank you very much for your assistance! Sincerely,. Charles.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167
https://github.com/google/deepvariant/issues/167:871,reliability,doe,does,871,"bad_alloc (memory?) issue postprocess_variants; Hi,. As a follow-up to my [previous question](https://github.com/google/deepvariant/issues/166), I am trying to run the **postprocess_variants** command for DeepVariant from a Docker container on an AWS instance. I am getting the following error message:. ```. terminate called after throwing an instance of 'std::bad_alloc'. what(): std::bad_alloc. ```. I thought this might have been an issue with memory allocation, but I have been testing the same command with increasing computational resources (I am currently using a m5.4xlarge instance). **1)** Am I totally wrong about the underlying cause? If so, is there anything you can suggest to troubleshoot this issue? **2)** Based upon my Google searches, it seemed like this might have something to do with using TensorFlow. Is it easy to tell if that is correct? If so, does that mean you are still doing variant calling/prediction from the **postprocess_variants** command? This is the command that I am running:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". FINAL_OUTPUT_VCF=""${OUTPUT_DIR}/output.vcf.gz"". sudo docker run \. -v /mnt/efs-genome:/mnt/efs-genome \. gcr.io/deepvariant-docker/deepvariant \. /opt/deepvariant/bin/postprocess_variants \. --ref ""${REF}"" \. --infile ""${CALL_VARIANTS_OUTPUT}"" \. --outfile ""${FINAL_OUTPUT_VCF}"". ```. Thank you very much for your assistance! Sincerely,. Charles.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167
https://github.com/google/deepvariant/issues/167:288,safety,error,error,288,"bad_alloc (memory?) issue postprocess_variants; Hi,. As a follow-up to my [previous question](https://github.com/google/deepvariant/issues/166), I am trying to run the **postprocess_variants** command for DeepVariant from a Docker container on an AWS instance. I am getting the following error message:. ```. terminate called after throwing an instance of 'std::bad_alloc'. what(): std::bad_alloc. ```. I thought this might have been an issue with memory allocation, but I have been testing the same command with increasing computational resources (I am currently using a m5.4xlarge instance). **1)** Am I totally wrong about the underlying cause? If so, is there anything you can suggest to troubleshoot this issue? **2)** Based upon my Google searches, it seemed like this might have something to do with using TensorFlow. Is it easy to tell if that is correct? If so, does that mean you are still doing variant calling/prediction from the **postprocess_variants** command? This is the command that I am running:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". FINAL_OUTPUT_VCF=""${OUTPUT_DIR}/output.vcf.gz"". sudo docker run \. -v /mnt/efs-genome:/mnt/efs-genome \. gcr.io/deepvariant-docker/deepvariant \. /opt/deepvariant/bin/postprocess_variants \. --ref ""${REF}"" \. --infile ""${CALL_VARIANTS_OUTPUT}"" \. --outfile ""${FINAL_OUTPUT_VCF}"". ```. Thank you very much for your assistance! Sincerely,. Charles.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167
https://github.com/google/deepvariant/issues/167:483,safety,test,testing,483,"bad_alloc (memory?) issue postprocess_variants; Hi,. As a follow-up to my [previous question](https://github.com/google/deepvariant/issues/166), I am trying to run the **postprocess_variants** command for DeepVariant from a Docker container on an AWS instance. I am getting the following error message:. ```. terminate called after throwing an instance of 'std::bad_alloc'. what(): std::bad_alloc. ```. I thought this might have been an issue with memory allocation, but I have been testing the same command with increasing computational resources (I am currently using a m5.4xlarge instance). **1)** Am I totally wrong about the underlying cause? If so, is there anything you can suggest to troubleshoot this issue? **2)** Based upon my Google searches, it seemed like this might have something to do with using TensorFlow. Is it easy to tell if that is correct? If so, does that mean you are still doing variant calling/prediction from the **postprocess_variants** command? This is the command that I am running:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". FINAL_OUTPUT_VCF=""${OUTPUT_DIR}/output.vcf.gz"". sudo docker run \. -v /mnt/efs-genome:/mnt/efs-genome \. gcr.io/deepvariant-docker/deepvariant \. /opt/deepvariant/bin/postprocess_variants \. --ref ""${REF}"" \. --infile ""${CALL_VARIANTS_OUTPUT}"" \. --outfile ""${FINAL_OUTPUT_VCF}"". ```. Thank you very much for your assistance! Sincerely,. Charles.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167
https://github.com/google/deepvariant/issues/167:538,safety,resourc,resources,538,"bad_alloc (memory?) issue postprocess_variants; Hi,. As a follow-up to my [previous question](https://github.com/google/deepvariant/issues/166), I am trying to run the **postprocess_variants** command for DeepVariant from a Docker container on an AWS instance. I am getting the following error message:. ```. terminate called after throwing an instance of 'std::bad_alloc'. what(): std::bad_alloc. ```. I thought this might have been an issue with memory allocation, but I have been testing the same command with increasing computational resources (I am currently using a m5.4xlarge instance). **1)** Am I totally wrong about the underlying cause? If so, is there anything you can suggest to troubleshoot this issue? **2)** Based upon my Google searches, it seemed like this might have something to do with using TensorFlow. Is it easy to tell if that is correct? If so, does that mean you are still doing variant calling/prediction from the **postprocess_variants** command? This is the command that I am running:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". FINAL_OUTPUT_VCF=""${OUTPUT_DIR}/output.vcf.gz"". sudo docker run \. -v /mnt/efs-genome:/mnt/efs-genome \. gcr.io/deepvariant-docker/deepvariant \. /opt/deepvariant/bin/postprocess_variants \. --ref ""${REF}"" \. --infile ""${CALL_VARIANTS_OUTPUT}"" \. --outfile ""${FINAL_OUTPUT_VCF}"". ```. Thank you very much for your assistance! Sincerely,. Charles.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167
https://github.com/google/deepvariant/issues/167:922,safety,predict,prediction,922,"bad_alloc (memory?) issue postprocess_variants; Hi,. As a follow-up to my [previous question](https://github.com/google/deepvariant/issues/166), I am trying to run the **postprocess_variants** command for DeepVariant from a Docker container on an AWS instance. I am getting the following error message:. ```. terminate called after throwing an instance of 'std::bad_alloc'. what(): std::bad_alloc. ```. I thought this might have been an issue with memory allocation, but I have been testing the same command with increasing computational resources (I am currently using a m5.4xlarge instance). **1)** Am I totally wrong about the underlying cause? If so, is there anything you can suggest to troubleshoot this issue? **2)** Based upon my Google searches, it seemed like this might have something to do with using TensorFlow. Is it easy to tell if that is correct? If so, does that mean you are still doing variant calling/prediction from the **postprocess_variants** command? This is the command that I am running:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". FINAL_OUTPUT_VCF=""${OUTPUT_DIR}/output.vcf.gz"". sudo docker run \. -v /mnt/efs-genome:/mnt/efs-genome \. gcr.io/deepvariant-docker/deepvariant \. /opt/deepvariant/bin/postprocess_variants \. --ref ""${REF}"" \. --infile ""${CALL_VARIANTS_OUTPUT}"" \. --outfile ""${FINAL_OUTPUT_VCF}"". ```. Thank you very much for your assistance! Sincerely,. Charles.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167
https://github.com/google/deepvariant/issues/167:483,testability,test,testing,483,"bad_alloc (memory?) issue postprocess_variants; Hi,. As a follow-up to my [previous question](https://github.com/google/deepvariant/issues/166), I am trying to run the **postprocess_variants** command for DeepVariant from a Docker container on an AWS instance. I am getting the following error message:. ```. terminate called after throwing an instance of 'std::bad_alloc'. what(): std::bad_alloc. ```. I thought this might have been an issue with memory allocation, but I have been testing the same command with increasing computational resources (I am currently using a m5.4xlarge instance). **1)** Am I totally wrong about the underlying cause? If so, is there anything you can suggest to troubleshoot this issue? **2)** Based upon my Google searches, it seemed like this might have something to do with using TensorFlow. Is it easy to tell if that is correct? If so, does that mean you are still doing variant calling/prediction from the **postprocess_variants** command? This is the command that I am running:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". FINAL_OUTPUT_VCF=""${OUTPUT_DIR}/output.vcf.gz"". sudo docker run \. -v /mnt/efs-genome:/mnt/efs-genome \. gcr.io/deepvariant-docker/deepvariant \. /opt/deepvariant/bin/postprocess_variants \. --ref ""${REF}"" \. --infile ""${CALL_VARIANTS_OUTPUT}"" \. --outfile ""${FINAL_OUTPUT_VCF}"". ```. Thank you very much for your assistance! Sincerely,. Charles.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167
https://github.com/google/deepvariant/issues/167:538,testability,resourc,resources,538,"bad_alloc (memory?) issue postprocess_variants; Hi,. As a follow-up to my [previous question](https://github.com/google/deepvariant/issues/166), I am trying to run the **postprocess_variants** command for DeepVariant from a Docker container on an AWS instance. I am getting the following error message:. ```. terminate called after throwing an instance of 'std::bad_alloc'. what(): std::bad_alloc. ```. I thought this might have been an issue with memory allocation, but I have been testing the same command with increasing computational resources (I am currently using a m5.4xlarge instance). **1)** Am I totally wrong about the underlying cause? If so, is there anything you can suggest to troubleshoot this issue? **2)** Based upon my Google searches, it seemed like this might have something to do with using TensorFlow. Is it easy to tell if that is correct? If so, does that mean you are still doing variant calling/prediction from the **postprocess_variants** command? This is the command that I am running:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". FINAL_OUTPUT_VCF=""${OUTPUT_DIR}/output.vcf.gz"". sudo docker run \. -v /mnt/efs-genome:/mnt/efs-genome \. gcr.io/deepvariant-docker/deepvariant \. /opt/deepvariant/bin/postprocess_variants \. --ref ""${REF}"" \. --infile ""${CALL_VARIANTS_OUTPUT}"" \. --outfile ""${FINAL_OUTPUT_VCF}"". ```. Thank you very much for your assistance! Sincerely,. Charles.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167
https://github.com/google/deepvariant/issues/167:11,usability,memor,memory,11,"bad_alloc (memory?) issue postprocess_variants; Hi,. As a follow-up to my [previous question](https://github.com/google/deepvariant/issues/166), I am trying to run the **postprocess_variants** command for DeepVariant from a Docker container on an AWS instance. I am getting the following error message:. ```. terminate called after throwing an instance of 'std::bad_alloc'. what(): std::bad_alloc. ```. I thought this might have been an issue with memory allocation, but I have been testing the same command with increasing computational resources (I am currently using a m5.4xlarge instance). **1)** Am I totally wrong about the underlying cause? If so, is there anything you can suggest to troubleshoot this issue? **2)** Based upon my Google searches, it seemed like this might have something to do with using TensorFlow. Is it easy to tell if that is correct? If so, does that mean you are still doing variant calling/prediction from the **postprocess_variants** command? This is the command that I am running:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". FINAL_OUTPUT_VCF=""${OUTPUT_DIR}/output.vcf.gz"". sudo docker run \. -v /mnt/efs-genome:/mnt/efs-genome \. gcr.io/deepvariant-docker/deepvariant \. /opt/deepvariant/bin/postprocess_variants \. --ref ""${REF}"" \. --infile ""${CALL_VARIANTS_OUTPUT}"" \. --outfile ""${FINAL_OUTPUT_VCF}"". ```. Thank you very much for your assistance! Sincerely,. Charles.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167
https://github.com/google/deepvariant/issues/167:193,usability,command,command,193,"bad_alloc (memory?) issue postprocess_variants; Hi,. As a follow-up to my [previous question](https://github.com/google/deepvariant/issues/166), I am trying to run the **postprocess_variants** command for DeepVariant from a Docker container on an AWS instance. I am getting the following error message:. ```. terminate called after throwing an instance of 'std::bad_alloc'. what(): std::bad_alloc. ```. I thought this might have been an issue with memory allocation, but I have been testing the same command with increasing computational resources (I am currently using a m5.4xlarge instance). **1)** Am I totally wrong about the underlying cause? If so, is there anything you can suggest to troubleshoot this issue? **2)** Based upon my Google searches, it seemed like this might have something to do with using TensorFlow. Is it easy to tell if that is correct? If so, does that mean you are still doing variant calling/prediction from the **postprocess_variants** command? This is the command that I am running:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". FINAL_OUTPUT_VCF=""${OUTPUT_DIR}/output.vcf.gz"". sudo docker run \. -v /mnt/efs-genome:/mnt/efs-genome \. gcr.io/deepvariant-docker/deepvariant \. /opt/deepvariant/bin/postprocess_variants \. --ref ""${REF}"" \. --infile ""${CALL_VARIANTS_OUTPUT}"" \. --outfile ""${FINAL_OUTPUT_VCF}"". ```. Thank you very much for your assistance! Sincerely,. Charles.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167
https://github.com/google/deepvariant/issues/167:288,usability,error,error,288,"bad_alloc (memory?) issue postprocess_variants; Hi,. As a follow-up to my [previous question](https://github.com/google/deepvariant/issues/166), I am trying to run the **postprocess_variants** command for DeepVariant from a Docker container on an AWS instance. I am getting the following error message:. ```. terminate called after throwing an instance of 'std::bad_alloc'. what(): std::bad_alloc. ```. I thought this might have been an issue with memory allocation, but I have been testing the same command with increasing computational resources (I am currently using a m5.4xlarge instance). **1)** Am I totally wrong about the underlying cause? If so, is there anything you can suggest to troubleshoot this issue? **2)** Based upon my Google searches, it seemed like this might have something to do with using TensorFlow. Is it easy to tell if that is correct? If so, does that mean you are still doing variant calling/prediction from the **postprocess_variants** command? This is the command that I am running:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". FINAL_OUTPUT_VCF=""${OUTPUT_DIR}/output.vcf.gz"". sudo docker run \. -v /mnt/efs-genome:/mnt/efs-genome \. gcr.io/deepvariant-docker/deepvariant \. /opt/deepvariant/bin/postprocess_variants \. --ref ""${REF}"" \. --infile ""${CALL_VARIANTS_OUTPUT}"" \. --outfile ""${FINAL_OUTPUT_VCF}"". ```. Thank you very much for your assistance! Sincerely,. Charles.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167
https://github.com/google/deepvariant/issues/167:448,usability,memor,memory,448,"bad_alloc (memory?) issue postprocess_variants; Hi,. As a follow-up to my [previous question](https://github.com/google/deepvariant/issues/166), I am trying to run the **postprocess_variants** command for DeepVariant from a Docker container on an AWS instance. I am getting the following error message:. ```. terminate called after throwing an instance of 'std::bad_alloc'. what(): std::bad_alloc. ```. I thought this might have been an issue with memory allocation, but I have been testing the same command with increasing computational resources (I am currently using a m5.4xlarge instance). **1)** Am I totally wrong about the underlying cause? If so, is there anything you can suggest to troubleshoot this issue? **2)** Based upon my Google searches, it seemed like this might have something to do with using TensorFlow. Is it easy to tell if that is correct? If so, does that mean you are still doing variant calling/prediction from the **postprocess_variants** command? This is the command that I am running:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". FINAL_OUTPUT_VCF=""${OUTPUT_DIR}/output.vcf.gz"". sudo docker run \. -v /mnt/efs-genome:/mnt/efs-genome \. gcr.io/deepvariant-docker/deepvariant \. /opt/deepvariant/bin/postprocess_variants \. --ref ""${REF}"" \. --infile ""${CALL_VARIANTS_OUTPUT}"" \. --outfile ""${FINAL_OUTPUT_VCF}"". ```. Thank you very much for your assistance! Sincerely,. Charles.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167
https://github.com/google/deepvariant/issues/167:500,usability,command,command,500,"bad_alloc (memory?) issue postprocess_variants; Hi,. As a follow-up to my [previous question](https://github.com/google/deepvariant/issues/166), I am trying to run the **postprocess_variants** command for DeepVariant from a Docker container on an AWS instance. I am getting the following error message:. ```. terminate called after throwing an instance of 'std::bad_alloc'. what(): std::bad_alloc. ```. I thought this might have been an issue with memory allocation, but I have been testing the same command with increasing computational resources (I am currently using a m5.4xlarge instance). **1)** Am I totally wrong about the underlying cause? If so, is there anything you can suggest to troubleshoot this issue? **2)** Based upon my Google searches, it seemed like this might have something to do with using TensorFlow. Is it easy to tell if that is correct? If so, does that mean you are still doing variant calling/prediction from the **postprocess_variants** command? This is the command that I am running:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". FINAL_OUTPUT_VCF=""${OUTPUT_DIR}/output.vcf.gz"". sudo docker run \. -v /mnt/efs-genome:/mnt/efs-genome \. gcr.io/deepvariant-docker/deepvariant \. /opt/deepvariant/bin/postprocess_variants \. --ref ""${REF}"" \. --infile ""${CALL_VARIANTS_OUTPUT}"" \. --outfile ""${FINAL_OUTPUT_VCF}"". ```. Thank you very much for your assistance! Sincerely,. Charles.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167
https://github.com/google/deepvariant/issues/167:967,usability,command,command,967,"bad_alloc (memory?) issue postprocess_variants; Hi,. As a follow-up to my [previous question](https://github.com/google/deepvariant/issues/166), I am trying to run the **postprocess_variants** command for DeepVariant from a Docker container on an AWS instance. I am getting the following error message:. ```. terminate called after throwing an instance of 'std::bad_alloc'. what(): std::bad_alloc. ```. I thought this might have been an issue with memory allocation, but I have been testing the same command with increasing computational resources (I am currently using a m5.4xlarge instance). **1)** Am I totally wrong about the underlying cause? If so, is there anything you can suggest to troubleshoot this issue? **2)** Based upon my Google searches, it seemed like this might have something to do with using TensorFlow. Is it easy to tell if that is correct? If so, does that mean you are still doing variant calling/prediction from the **postprocess_variants** command? This is the command that I am running:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". FINAL_OUTPUT_VCF=""${OUTPUT_DIR}/output.vcf.gz"". sudo docker run \. -v /mnt/efs-genome:/mnt/efs-genome \. gcr.io/deepvariant-docker/deepvariant \. /opt/deepvariant/bin/postprocess_variants \. --ref ""${REF}"" \. --infile ""${CALL_VARIANTS_OUTPUT}"" \. --outfile ""${FINAL_OUTPUT_VCF}"". ```. Thank you very much for your assistance! Sincerely,. Charles.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167
https://github.com/google/deepvariant/issues/167:988,usability,command,command,988,"bad_alloc (memory?) issue postprocess_variants; Hi,. As a follow-up to my [previous question](https://github.com/google/deepvariant/issues/166), I am trying to run the **postprocess_variants** command for DeepVariant from a Docker container on an AWS instance. I am getting the following error message:. ```. terminate called after throwing an instance of 'std::bad_alloc'. what(): std::bad_alloc. ```. I thought this might have been an issue with memory allocation, but I have been testing the same command with increasing computational resources (I am currently using a m5.4xlarge instance). **1)** Am I totally wrong about the underlying cause? If so, is there anything you can suggest to troubleshoot this issue? **2)** Based upon my Google searches, it seemed like this might have something to do with using TensorFlow. Is it easy to tell if that is correct? If so, does that mean you are still doing variant calling/prediction from the **postprocess_variants** command? This is the command that I am running:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant. REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". FINAL_OUTPUT_VCF=""${OUTPUT_DIR}/output.vcf.gz"". sudo docker run \. -v /mnt/efs-genome:/mnt/efs-genome \. gcr.io/deepvariant-docker/deepvariant \. /opt/deepvariant/bin/postprocess_variants \. --ref ""${REF}"" \. --infile ""${CALL_VARIANTS_OUTPUT}"" \. --outfile ""${FINAL_OUTPUT_VCF}"". ```. Thank you very much for your assistance! Sincerely,. Charles.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/167
https://github.com/google/deepvariant/issues/168:98,deployability,manag,managed,98,"Make_examples using GPU?; Hi, I am using DeepVariant 0.7.2 on my local machine (nvidia-docker). I managed to significantly accelerate call_variants using GPU (5-10x faster using relatively cheap graphics card). I wonder if it is possible to accelerate make_examples using GPU - for now I am using parallel CPU threads as suggested: seq 0 $((N_SHARDS-1)) | parallel (...). --. Tomasz Stokowy.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/168
https://github.com/google/deepvariant/issues/168:20,energy efficiency,GPU,GPU,20,"Make_examples using GPU?; Hi, I am using DeepVariant 0.7.2 on my local machine (nvidia-docker). I managed to significantly accelerate call_variants using GPU (5-10x faster using relatively cheap graphics card). I wonder if it is possible to accelerate make_examples using GPU - for now I am using parallel CPU threads as suggested: seq 0 $((N_SHARDS-1)) | parallel (...). --. Tomasz Stokowy.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/168
https://github.com/google/deepvariant/issues/168:98,energy efficiency,manag,managed,98,"Make_examples using GPU?; Hi, I am using DeepVariant 0.7.2 on my local machine (nvidia-docker). I managed to significantly accelerate call_variants using GPU (5-10x faster using relatively cheap graphics card). I wonder if it is possible to accelerate make_examples using GPU - for now I am using parallel CPU threads as suggested: seq 0 $((N_SHARDS-1)) | parallel (...). --. Tomasz Stokowy.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/168
https://github.com/google/deepvariant/issues/168:154,energy efficiency,GPU,GPU,154,"Make_examples using GPU?; Hi, I am using DeepVariant 0.7.2 on my local machine (nvidia-docker). I managed to significantly accelerate call_variants using GPU (5-10x faster using relatively cheap graphics card). I wonder if it is possible to accelerate make_examples using GPU - for now I am using parallel CPU threads as suggested: seq 0 $((N_SHARDS-1)) | parallel (...). --. Tomasz Stokowy.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/168
https://github.com/google/deepvariant/issues/168:272,energy efficiency,GPU,GPU,272,"Make_examples using GPU?; Hi, I am using DeepVariant 0.7.2 on my local machine (nvidia-docker). I managed to significantly accelerate call_variants using GPU (5-10x faster using relatively cheap graphics card). I wonder if it is possible to accelerate make_examples using GPU - for now I am using parallel CPU threads as suggested: seq 0 $((N_SHARDS-1)) | parallel (...). --. Tomasz Stokowy.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/168
https://github.com/google/deepvariant/issues/168:306,energy efficiency,CPU,CPU,306,"Make_examples using GPU?; Hi, I am using DeepVariant 0.7.2 on my local machine (nvidia-docker). I managed to significantly accelerate call_variants using GPU (5-10x faster using relatively cheap graphics card). I wonder if it is possible to accelerate make_examples using GPU - for now I am using parallel CPU threads as suggested: seq 0 $((N_SHARDS-1)) | parallel (...). --. Tomasz Stokowy.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/168
https://github.com/google/deepvariant/issues/168:20,performance,GPU,GPU,20,"Make_examples using GPU?; Hi, I am using DeepVariant 0.7.2 on my local machine (nvidia-docker). I managed to significantly accelerate call_variants using GPU (5-10x faster using relatively cheap graphics card). I wonder if it is possible to accelerate make_examples using GPU - for now I am using parallel CPU threads as suggested: seq 0 $((N_SHARDS-1)) | parallel (...). --. Tomasz Stokowy.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/168
https://github.com/google/deepvariant/issues/168:154,performance,GPU,GPU,154,"Make_examples using GPU?; Hi, I am using DeepVariant 0.7.2 on my local machine (nvidia-docker). I managed to significantly accelerate call_variants using GPU (5-10x faster using relatively cheap graphics card). I wonder if it is possible to accelerate make_examples using GPU - for now I am using parallel CPU threads as suggested: seq 0 $((N_SHARDS-1)) | parallel (...). --. Tomasz Stokowy.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/168
https://github.com/google/deepvariant/issues/168:272,performance,GPU,GPU,272,"Make_examples using GPU?; Hi, I am using DeepVariant 0.7.2 on my local machine (nvidia-docker). I managed to significantly accelerate call_variants using GPU (5-10x faster using relatively cheap graphics card). I wonder if it is possible to accelerate make_examples using GPU - for now I am using parallel CPU threads as suggested: seq 0 $((N_SHARDS-1)) | parallel (...). --. Tomasz Stokowy.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/168
https://github.com/google/deepvariant/issues/168:297,performance,parallel,parallel,297,"Make_examples using GPU?; Hi, I am using DeepVariant 0.7.2 on my local machine (nvidia-docker). I managed to significantly accelerate call_variants using GPU (5-10x faster using relatively cheap graphics card). I wonder if it is possible to accelerate make_examples using GPU - for now I am using parallel CPU threads as suggested: seq 0 $((N_SHARDS-1)) | parallel (...). --. Tomasz Stokowy.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/168
https://github.com/google/deepvariant/issues/168:306,performance,CPU,CPU,306,"Make_examples using GPU?; Hi, I am using DeepVariant 0.7.2 on my local machine (nvidia-docker). I managed to significantly accelerate call_variants using GPU (5-10x faster using relatively cheap graphics card). I wonder if it is possible to accelerate make_examples using GPU - for now I am using parallel CPU threads as suggested: seq 0 $((N_SHARDS-1)) | parallel (...). --. Tomasz Stokowy.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/168
https://github.com/google/deepvariant/issues/168:356,performance,parallel,parallel,356,"Make_examples using GPU?; Hi, I am using DeepVariant 0.7.2 on my local machine (nvidia-docker). I managed to significantly accelerate call_variants using GPU (5-10x faster using relatively cheap graphics card). I wonder if it is possible to accelerate make_examples using GPU - for now I am using parallel CPU threads as suggested: seq 0 $((N_SHARDS-1)) | parallel (...). --. Tomasz Stokowy.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/168
https://github.com/google/deepvariant/issues/168:98,safety,manag,managed,98,"Make_examples using GPU?; Hi, I am using DeepVariant 0.7.2 on my local machine (nvidia-docker). I managed to significantly accelerate call_variants using GPU (5-10x faster using relatively cheap graphics card). I wonder if it is possible to accelerate make_examples using GPU - for now I am using parallel CPU threads as suggested: seq 0 $((N_SHARDS-1)) | parallel (...). --. Tomasz Stokowy.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/168
https://github.com/google/deepvariant/issues/168:109,security,sign,significantly,109,"Make_examples using GPU?; Hi, I am using DeepVariant 0.7.2 on my local machine (nvidia-docker). I managed to significantly accelerate call_variants using GPU (5-10x faster using relatively cheap graphics card). I wonder if it is possible to accelerate make_examples using GPU - for now I am using parallel CPU threads as suggested: seq 0 $((N_SHARDS-1)) | parallel (...). --. Tomasz Stokowy.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/168
https://github.com/google/deepvariant/issues/169:14,deployability,instal,install,14,Soooo hard to install ; . I had tried install the soft using conda but it was failed finally. Besides dose GPU necessary ,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/169
https://github.com/google/deepvariant/issues/169:38,deployability,instal,install,38,Soooo hard to install ; . I had tried install the soft using conda but it was failed finally. Besides dose GPU necessary ,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/169
https://github.com/google/deepvariant/issues/169:79,deployability,fail,failed,79,Soooo hard to install ; . I had tried install the soft using conda but it was failed finally. Besides dose GPU necessary ,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/169
https://github.com/google/deepvariant/issues/169:108,energy efficiency,GPU,GPU,108,Soooo hard to install ; . I had tried install the soft using conda but it was failed finally. Besides dose GPU necessary ,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/169
https://github.com/google/deepvariant/issues/169:108,performance,GPU,GPU,108,Soooo hard to install ; . I had tried install the soft using conda but it was failed finally. Besides dose GPU necessary ,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/169
https://github.com/google/deepvariant/issues/169:79,reliability,fail,failed,79,Soooo hard to install ; . I had tried install the soft using conda but it was failed finally. Besides dose GPU necessary ,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/169
https://github.com/google/deepvariant/issues/170:111,availability,error,error,111,"Running batch analysis; Hi there! I tried to combine the deepvariant's gvcf files using GATK and it returned a error because of the ""NON_REF"". Do you have any other recommendation? Or did you tested your gvcf in GATK? This is the error that I got using GATK:. A USER ERROR has occurred: The list of input alleles must contain <NON_REF> as an allele but that is not the case at position 10325413; please use the Haplotype Caller with gVCF output to generate appropriate records. The DeepVariant ""<NON_REF>"" allele is ""<*>"". _Originally posted by @jaqueytw in https://github.com/google/deepvariant/issues/45#issuecomment-481414559_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/170
https://github.com/google/deepvariant/issues/170:230,availability,error,error,230,"Running batch analysis; Hi there! I tried to combine the deepvariant's gvcf files using GATK and it returned a error because of the ""NON_REF"". Do you have any other recommendation? Or did you tested your gvcf in GATK? This is the error that I got using GATK:. A USER ERROR has occurred: The list of input alleles must contain <NON_REF> as an allele but that is not the case at position 10325413; please use the Haplotype Caller with gVCF output to generate appropriate records. The DeepVariant ""<NON_REF>"" allele is ""<*>"". _Originally posted by @jaqueytw in https://github.com/google/deepvariant/issues/45#issuecomment-481414559_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/170
https://github.com/google/deepvariant/issues/170:267,availability,ERROR,ERROR,267,"Running batch analysis; Hi there! I tried to combine the deepvariant's gvcf files using GATK and it returned a error because of the ""NON_REF"". Do you have any other recommendation? Or did you tested your gvcf in GATK? This is the error that I got using GATK:. A USER ERROR has occurred: The list of input alleles must contain <NON_REF> as an allele but that is not the case at position 10325413; please use the Haplotype Caller with gVCF output to generate appropriate records. The DeepVariant ""<NON_REF>"" allele is ""<*>"". _Originally posted by @jaqueytw in https://github.com/google/deepvariant/issues/45#issuecomment-481414559_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/170
https://github.com/google/deepvariant/issues/170:318,deployability,contain,contain,318,"Running batch analysis; Hi there! I tried to combine the deepvariant's gvcf files using GATK and it returned a error because of the ""NON_REF"". Do you have any other recommendation? Or did you tested your gvcf in GATK? This is the error that I got using GATK:. A USER ERROR has occurred: The list of input alleles must contain <NON_REF> as an allele but that is not the case at position 10325413; please use the Haplotype Caller with gVCF output to generate appropriate records. The DeepVariant ""<NON_REF>"" allele is ""<*>"". _Originally posted by @jaqueytw in https://github.com/google/deepvariant/issues/45#issuecomment-481414559_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/170
https://github.com/google/deepvariant/issues/170:8,integrability,batch,batch,8,"Running batch analysis; Hi there! I tried to combine the deepvariant's gvcf files using GATK and it returned a error because of the ""NON_REF"". Do you have any other recommendation? Or did you tested your gvcf in GATK? This is the error that I got using GATK:. A USER ERROR has occurred: The list of input alleles must contain <NON_REF> as an allele but that is not the case at position 10325413; please use the Haplotype Caller with gVCF output to generate appropriate records. The DeepVariant ""<NON_REF>"" allele is ""<*>"". _Originally posted by @jaqueytw in https://github.com/google/deepvariant/issues/45#issuecomment-481414559_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/170
https://github.com/google/deepvariant/issues/170:8,performance,batch,batch,8,"Running batch analysis; Hi there! I tried to combine the deepvariant's gvcf files using GATK and it returned a error because of the ""NON_REF"". Do you have any other recommendation? Or did you tested your gvcf in GATK? This is the error that I got using GATK:. A USER ERROR has occurred: The list of input alleles must contain <NON_REF> as an allele but that is not the case at position 10325413; please use the Haplotype Caller with gVCF output to generate appropriate records. The DeepVariant ""<NON_REF>"" allele is ""<*>"". _Originally posted by @jaqueytw in https://github.com/google/deepvariant/issues/45#issuecomment-481414559_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/170
https://github.com/google/deepvariant/issues/170:111,performance,error,error,111,"Running batch analysis; Hi there! I tried to combine the deepvariant's gvcf files using GATK and it returned a error because of the ""NON_REF"". Do you have any other recommendation? Or did you tested your gvcf in GATK? This is the error that I got using GATK:. A USER ERROR has occurred: The list of input alleles must contain <NON_REF> as an allele but that is not the case at position 10325413; please use the Haplotype Caller with gVCF output to generate appropriate records. The DeepVariant ""<NON_REF>"" allele is ""<*>"". _Originally posted by @jaqueytw in https://github.com/google/deepvariant/issues/45#issuecomment-481414559_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/170
https://github.com/google/deepvariant/issues/170:230,performance,error,error,230,"Running batch analysis; Hi there! I tried to combine the deepvariant's gvcf files using GATK and it returned a error because of the ""NON_REF"". Do you have any other recommendation? Or did you tested your gvcf in GATK? This is the error that I got using GATK:. A USER ERROR has occurred: The list of input alleles must contain <NON_REF> as an allele but that is not the case at position 10325413; please use the Haplotype Caller with gVCF output to generate appropriate records. The DeepVariant ""<NON_REF>"" allele is ""<*>"". _Originally posted by @jaqueytw in https://github.com/google/deepvariant/issues/45#issuecomment-481414559_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/170
https://github.com/google/deepvariant/issues/170:267,performance,ERROR,ERROR,267,"Running batch analysis; Hi there! I tried to combine the deepvariant's gvcf files using GATK and it returned a error because of the ""NON_REF"". Do you have any other recommendation? Or did you tested your gvcf in GATK? This is the error that I got using GATK:. A USER ERROR has occurred: The list of input alleles must contain <NON_REF> as an allele but that is not the case at position 10325413; please use the Haplotype Caller with gVCF output to generate appropriate records. The DeepVariant ""<NON_REF>"" allele is ""<*>"". _Originally posted by @jaqueytw in https://github.com/google/deepvariant/issues/45#issuecomment-481414559_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/170
https://github.com/google/deepvariant/issues/170:111,safety,error,error,111,"Running batch analysis; Hi there! I tried to combine the deepvariant's gvcf files using GATK and it returned a error because of the ""NON_REF"". Do you have any other recommendation? Or did you tested your gvcf in GATK? This is the error that I got using GATK:. A USER ERROR has occurred: The list of input alleles must contain <NON_REF> as an allele but that is not the case at position 10325413; please use the Haplotype Caller with gVCF output to generate appropriate records. The DeepVariant ""<NON_REF>"" allele is ""<*>"". _Originally posted by @jaqueytw in https://github.com/google/deepvariant/issues/45#issuecomment-481414559_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/170
https://github.com/google/deepvariant/issues/170:192,safety,test,tested,192,"Running batch analysis; Hi there! I tried to combine the deepvariant's gvcf files using GATK and it returned a error because of the ""NON_REF"". Do you have any other recommendation? Or did you tested your gvcf in GATK? This is the error that I got using GATK:. A USER ERROR has occurred: The list of input alleles must contain <NON_REF> as an allele but that is not the case at position 10325413; please use the Haplotype Caller with gVCF output to generate appropriate records. The DeepVariant ""<NON_REF>"" allele is ""<*>"". _Originally posted by @jaqueytw in https://github.com/google/deepvariant/issues/45#issuecomment-481414559_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/170
https://github.com/google/deepvariant/issues/170:230,safety,error,error,230,"Running batch analysis; Hi there! I tried to combine the deepvariant's gvcf files using GATK and it returned a error because of the ""NON_REF"". Do you have any other recommendation? Or did you tested your gvcf in GATK? This is the error that I got using GATK:. A USER ERROR has occurred: The list of input alleles must contain <NON_REF> as an allele but that is not the case at position 10325413; please use the Haplotype Caller with gVCF output to generate appropriate records. The DeepVariant ""<NON_REF>"" allele is ""<*>"". _Originally posted by @jaqueytw in https://github.com/google/deepvariant/issues/45#issuecomment-481414559_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/170
https://github.com/google/deepvariant/issues/170:267,safety,ERROR,ERROR,267,"Running batch analysis; Hi there! I tried to combine the deepvariant's gvcf files using GATK and it returned a error because of the ""NON_REF"". Do you have any other recommendation? Or did you tested your gvcf in GATK? This is the error that I got using GATK:. A USER ERROR has occurred: The list of input alleles must contain <NON_REF> as an allele but that is not the case at position 10325413; please use the Haplotype Caller with gVCF output to generate appropriate records. The DeepVariant ""<NON_REF>"" allele is ""<*>"". _Originally posted by @jaqueytw in https://github.com/google/deepvariant/issues/45#issuecomment-481414559_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/170
https://github.com/google/deepvariant/issues/170:299,safety,input,input,299,"Running batch analysis; Hi there! I tried to combine the deepvariant's gvcf files using GATK and it returned a error because of the ""NON_REF"". Do you have any other recommendation? Or did you tested your gvcf in GATK? This is the error that I got using GATK:. A USER ERROR has occurred: The list of input alleles must contain <NON_REF> as an allele but that is not the case at position 10325413; please use the Haplotype Caller with gVCF output to generate appropriate records. The DeepVariant ""<NON_REF>"" allele is ""<*>"". _Originally posted by @jaqueytw in https://github.com/google/deepvariant/issues/45#issuecomment-481414559_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/170
https://github.com/google/deepvariant/issues/170:192,testability,test,tested,192,"Running batch analysis; Hi there! I tried to combine the deepvariant's gvcf files using GATK and it returned a error because of the ""NON_REF"". Do you have any other recommendation? Or did you tested your gvcf in GATK? This is the error that I got using GATK:. A USER ERROR has occurred: The list of input alleles must contain <NON_REF> as an allele but that is not the case at position 10325413; please use the Haplotype Caller with gVCF output to generate appropriate records. The DeepVariant ""<NON_REF>"" allele is ""<*>"". _Originally posted by @jaqueytw in https://github.com/google/deepvariant/issues/45#issuecomment-481414559_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/170
https://github.com/google/deepvariant/issues/170:111,usability,error,error,111,"Running batch analysis; Hi there! I tried to combine the deepvariant's gvcf files using GATK and it returned a error because of the ""NON_REF"". Do you have any other recommendation? Or did you tested your gvcf in GATK? This is the error that I got using GATK:. A USER ERROR has occurred: The list of input alleles must contain <NON_REF> as an allele but that is not the case at position 10325413; please use the Haplotype Caller with gVCF output to generate appropriate records. The DeepVariant ""<NON_REF>"" allele is ""<*>"". _Originally posted by @jaqueytw in https://github.com/google/deepvariant/issues/45#issuecomment-481414559_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/170
https://github.com/google/deepvariant/issues/170:230,usability,error,error,230,"Running batch analysis; Hi there! I tried to combine the deepvariant's gvcf files using GATK and it returned a error because of the ""NON_REF"". Do you have any other recommendation? Or did you tested your gvcf in GATK? This is the error that I got using GATK:. A USER ERROR has occurred: The list of input alleles must contain <NON_REF> as an allele but that is not the case at position 10325413; please use the Haplotype Caller with gVCF output to generate appropriate records. The DeepVariant ""<NON_REF>"" allele is ""<*>"". _Originally posted by @jaqueytw in https://github.com/google/deepvariant/issues/45#issuecomment-481414559_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/170
https://github.com/google/deepvariant/issues/170:262,usability,USER,USER,262,"Running batch analysis; Hi there! I tried to combine the deepvariant's gvcf files using GATK and it returned a error because of the ""NON_REF"". Do you have any other recommendation? Or did you tested your gvcf in GATK? This is the error that I got using GATK:. A USER ERROR has occurred: The list of input alleles must contain <NON_REF> as an allele but that is not the case at position 10325413; please use the Haplotype Caller with gVCF output to generate appropriate records. The DeepVariant ""<NON_REF>"" allele is ""<*>"". _Originally posted by @jaqueytw in https://github.com/google/deepvariant/issues/45#issuecomment-481414559_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/170
https://github.com/google/deepvariant/issues/170:267,usability,ERROR,ERROR,267,"Running batch analysis; Hi there! I tried to combine the deepvariant's gvcf files using GATK and it returned a error because of the ""NON_REF"". Do you have any other recommendation? Or did you tested your gvcf in GATK? This is the error that I got using GATK:. A USER ERROR has occurred: The list of input alleles must contain <NON_REF> as an allele but that is not the case at position 10325413; please use the Haplotype Caller with gVCF output to generate appropriate records. The DeepVariant ""<NON_REF>"" allele is ""<*>"". _Originally posted by @jaqueytw in https://github.com/google/deepvariant/issues/45#issuecomment-481414559_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/170
https://github.com/google/deepvariant/issues/170:299,usability,input,input,299,"Running batch analysis; Hi there! I tried to combine the deepvariant's gvcf files using GATK and it returned a error because of the ""NON_REF"". Do you have any other recommendation? Or did you tested your gvcf in GATK? This is the error that I got using GATK:. A USER ERROR has occurred: The list of input alleles must contain <NON_REF> as an allele but that is not the case at position 10325413; please use the Haplotype Caller with gVCF output to generate appropriate records. The DeepVariant ""<NON_REF>"" allele is ""<*>"". _Originally posted by @jaqueytw in https://github.com/google/deepvariant/issues/45#issuecomment-481414559_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/170
https://github.com/google/deepvariant/issues/171:4888,availability,error,error,4888," 3 steps run separately on Google Cloud (instead of using `gcloud alpha genomics pipelines`). I have some notes on this [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WO",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:5021,availability,state,state,5021,"ow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: r",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:5226,availability,state,state,5226,"d. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/1",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:6246,availability,Error,Error,6246,"ne. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alter",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:6274,availability,error,error,6274,"newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:6420,availability,error,error,6420,"ission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:6431,availability,error,error,6431,"ed. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), but I only ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:6913,availability,Error,Error,6913,"ou pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), but I only copied one representative example above, for the repeated part. If I try to run the DeepVariant container in interactive mode (to try and understand what is going on), I get the following message (which is a note, without actually going into interactive mode):. ```. docker run -it -v /home/user/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant. See https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md. ```. I do have the `gcloud alpha ge",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:6941,availability,error,error,6941,"feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), but I only copied one representative example above, for the repeated part. If I try to run the DeepVariant container in interactive mode (to try and understand what is going on), I get the following message (which is a note, without actually going into interactive mode):. ```. docker run -it -v /home/user/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant. See https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md. ```. I do have the `gcloud alpha genomics pipelines` example wo",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:7087,availability,error,error,7087," 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), but I only copied one representative example above, for the repeated part. If I try to run the DeepVariant container in interactive mode (to try and understand what is going on), I get the following message (which is a note, without actually going into interactive mode):. ```. docker run -it -v /home/user/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant. See https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md. ```. I do have the `gcloud alpha genomics pipelines` example working, so this isnt absolutely essential for running DeepVariant on Google Cloud. However, if you can help provide me some guidance for running t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:7098,availability,error,error,7098,"8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), but I only copied one representative example above, for the repeated part. If I try to run the DeepVariant container in interactive mode (to try and understand what is going on), I get the following message (which is a note, without actually going into interactive mode):. ```. docker run -it -v /home/user/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant. See https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md. ```. I do have the `gcloud alpha genomics pipelines` example working, so this isnt absolutely essential for running DeepVariant on Google Cloud. However, if you can help provide me some guidance for running the [linked ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:7379,availability,error,error,7379,"ile creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), but I only copied one representative example above, for the repeated part. If I try to run the DeepVariant container in interactive mode (to try and understand what is going on), I get the following message (which is a note, without actually going into interactive mode):. ```. docker run -it -v /home/user/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant. See https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md. ```. I do have the `gcloud alpha genomics pipelines` example working, so this isnt absolutely essential for running DeepVariant on Google Cloud. However, if you can help provide me some guidance for running the [linked script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud, I would very much appreciate it. Thank you very much,. Charles.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:3627,deployability,depend,depending,3627,"about using the Cloud (versus local server + storage) for processing 100s or 1000s of samples (or even more, although I assume that would probably be for more than most individual labs or citizen scientists). I think 24 hour run-time was similar to running GATK on my local computer (with 8 GB of RAM and 4 cores), so Im not really complaining about the Cloud run-time that I encountered (I am just saying that the estimates provided on the README didnt match my own experience, even with an almost identical command on Google Cloud). **1b)** I realize that it would take some time (and Im not sure what would be the benefits versus other projects). However, have you considered allowing users to upload their run-time information (and estimated costs) to a program that might be able to help estimate run-time and cost (to possible help with topic **1a)**, **in the long-term**)? Since `gcp_deepvariant_runner` avoids the possibility of delays between running steps (and has an exist status depending upon whether variant calling was successful), perhaps some sort of optional reporting to an anonymized database could be provided as a parameter for that? **2)** While I realize it could be considered a cross-post, I am trying to test running each of the 3 steps run separately on Google Cloud (instead of using `gcloud alpha genomics pipelines`). I have some notes on this [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/ma",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:3972,deployability,pipelin,pipelines,3972,"about the Cloud run-time that I encountered (I am just saying that the estimates provided on the README didnt match my own experience, even with an almost identical command on Google Cloud). **1b)** I realize that it would take some time (and Im not sure what would be the benefits versus other projects). However, have you considered allowing users to upload their run-time information (and estimated costs) to a program that might be able to help estimate run-time and cost (to possible help with topic **1a)**, **in the long-term**)? Since `gcp_deepvariant_runner` avoids the possibility of delays between running steps (and has an exist status depending upon whether variant calling was successful), perhaps some sort of optional reporting to an anonymized database could be provided as a parameter for that? **2)** While I realize it could be considered a cross-post, I am trying to test running each of the 3 steps run separately on Google Cloud (instead of using `gcloud alpha genomics pipelines`). I have some notes on this [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:4012,deployability,Stack,Stack,4012,"tered (I am just saying that the estimates provided on the README didnt match my own experience, even with an almost identical command on Google Cloud). **1b)** I realize that it would take some time (and Im not sure what would be the benefits versus other projects). However, have you considered allowing users to upload their run-time information (and estimated costs) to a program that might be able to help estimate run-time and cost (to possible help with topic **1a)**, **in the long-term**)? Since `gcp_deepvariant_runner` avoids the possibility of delays between running steps (and has an exist status depending upon whether variant calling was successful), perhaps some sort of optional reporting to an anonymized database could be provided as a parameter for that? **2)** While I realize it could be considered a cross-post, I am trying to test running each of the 3 steps run separately on Google Cloud (instead of using `gcloud alpha genomics pipelines`). I have some notes on this [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Re",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:4042,deployability,stack,stackoverflow,4042,"stimates provided on the README didnt match my own experience, even with an almost identical command on Google Cloud). **1b)** I realize that it would take some time (and Im not sure what would be the benefits versus other projects). However, have you considered allowing users to upload their run-time information (and estimated costs) to a program that might be able to help estimate run-time and cost (to possible help with topic **1a)**, **in the long-term**)? Since `gcp_deepvariant_runner` avoids the possibility of delays between running steps (and has an exist status depending upon whether variant calling was successful), perhaps some sort of optional reporting to an anonymized database could be provided as a parameter for that? **2)** While I realize it could be considered a cross-post, I am trying to test running each of the 3 steps run separately on Google Cloud (instead of using `gcloud alpha genomics pipelines`). I have some notes on this [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:4180,deployability,instal,installation,4180," that it would take some time (and Im not sure what would be the benefits versus other projects). However, have you considered allowing users to upload their run-time information (and estimated costs) to a program that might be able to help estimate run-time and cost (to possible help with topic **1a)**, **in the long-term**)? Since `gcp_deepvariant_runner` avoids the possibility of delays between running steps (and has an exist status depending upon whether variant calling was successful), perhaps some sort of optional reporting to an anonymized database could be provided as a parameter for that? **2)** While I realize it could be considered a cross-post, I am trying to test running each of the 3 steps run separately on Google Cloud (instead of using `gcloud alpha genomics pipelines`). I have some notes on this [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:4467,deployability,contain,container,4467," with topic **1a)**, **in the long-term**)? Since `gcp_deepvariant_runner` avoids the possibility of delays between running steps (and has an exist status depending upon whether variant calling was successful), perhaps some sort of optional reporting to an anonymized database could be provided as a parameter for that? **2)** While I realize it could be considered a cross-post, I am trying to test running each of the 3 steps run separately on Google Cloud (instead of using `gcloud alpha genomics pipelines`). I have some notes on this [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:4491,deployability,contain,container,4491,"n the long-term**)? Since `gcp_deepvariant_runner` avoids the possibility of delays between running steps (and has an exist status depending upon whether variant calling was successful), perhaps some sort of optional reporting to an anonymized database could be provided as a parameter for that? **2)** While I realize it could be considered a cross-post, I am trying to test running each of the 3 steps run separately on Google Cloud (instead of using `gcloud alpha genomics pipelines`). I have some notes on this [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdat",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:4987,deployability,Build,Building,4987," some notes on this [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. T",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:4996,deployability,depend,dependency,4996,"s on this [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:5075,deployability,version,version,5075,"running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max j",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:5100,deployability,upgrad,upgraded,5100,"cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 /",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:5118,deployability,instal,installed,5118,"-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs r",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:5151,deployability,upgrad,upgraded,5151," about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of starte",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:5192,deployability,Build,Building,5192," running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:5201,deployability,depend,dependency,5201,"f Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:5284,deployability,version,version,5284,"ed to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while cr",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:5308,deployability,upgrad,upgraded,5308,"rms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:5326,deployability,instal,installed,5326,"er on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:5359,deployability,upgrad,upgraded,5359,"stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cd",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:5793,deployability,log,login,5793,"om/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --example",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:6449,deployability,contain,container,6449,"tem 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), but I only copied one represent",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:6498,deployability,fail,failed,6498,"2maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), but I only copied one representative example above, for the repeated part. If ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:7116,deployability,contain,container,7116," running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), but I only copied one representative example above, for the repeated part. If I try to run the DeepVariant container in interactive mode (to try and understand what is going on), I get the following message (which is a note, without actually going into interactive mode):. ```. docker run -it -v /home/user/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant. See https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md. ```. I do have the `gcloud alpha genomics pipelines` example working, so this isnt absolutely essential for running DeepVariant on Google Cloud. However, if you can help provide me some guidance for running the [linked script]( https://git",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:7530,deployability,contain,container,7530,"ile creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), but I only copied one representative example above, for the repeated part. If I try to run the DeepVariant container in interactive mode (to try and understand what is going on), I get the following message (which is a note, without actually going into interactive mode):. ```. docker run -it -v /home/user/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant. See https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md. ```. I do have the `gcloud alpha genomics pipelines` example working, so this isnt absolutely essential for running DeepVariant on Google Cloud. However, if you can help provide me some guidance for running the [linked script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud, I would very much appreciate it. Thank you very much,. Charles.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:7923,deployability,pipelin,pipelines,7923,"ile creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), but I only copied one representative example above, for the repeated part. If I try to run the DeepVariant container in interactive mode (to try and understand what is going on), I get the following message (which is a note, without actually going into interactive mode):. ```. docker run -it -v /home/user/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant. See https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md. ```. I do have the `gcloud alpha genomics pipelines` example working, so this isnt absolutely essential for running DeepVariant on Google Cloud. However, if you can help provide me some guidance for running the [linked script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud, I would very much appreciate it. Thank you very much,. Charles.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:0,energy efficiency,Cloud,Cloud,0,"Cloud Run-Time and Cost (including Docker on Google Cloud with Data in Bucket); Hi Again,. Thank you again for your help (on previous issues like #166 and #167). I have successfully run one Exome sample and one WGS sample (on AWS and Google Cloud). The WGS sample has ~250 million reads (paired-end, 150 bp x 150 bp), so it has ~25x coverage. Using the code based upon the [Google Cloud DeepVariant example]( https://cloud.google.com/genomics/docs/tutorials/deepvariant), I could process this 25X WGS samples in almost exactly 24 hours at a cost of approximately $10. On AWS, I probably should have kept more complete notes, but I believe it completed in ~18 hours (with a cost of approximately $15). This includes some cost of storage for the exome sample (and the WGS .fastq.gz files), although read storage would need to be taken into consideration if performing all analysis on the clould. Nevertheless, I previously reported [a much larger number]( https://github.com/google/deepvariant/issues/167#issuecomment-479522653) when I was learning how to use AWS, and these cost figures are now much more similar (as probably should be expected). One reason that I found running each step separately on AWS to be helpful was that I could figure out that I needed to add an extra parameter (`--sample_name VeritasProvided `) for the WGS dataset (for the provided alignment from Veritas) that wasnt necessary for the Exome dataset (for the provided alignment from Genos). Ive re-processed each sample locally, so I would also like to compare variant calls from a BWA-MEM alignment. Plus, I would like to make my comparison to AWS as fair as possible. So, here are my thoughts moving forward:. **1a)** I think it is good that you have changed the example WGS run time from [70 minutes]( https://github.com/google/deepvariant/blob/9d24133fc83e0423b3d5cf125a710bbefa864bbb/README.md) minutes to [5 hours](https://github.com/google/deepvariant/blob/r0.8/README.md), but this is still quite different than ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:52,energy efficiency,Cloud,Cloud,52,"Cloud Run-Time and Cost (including Docker on Google Cloud with Data in Bucket); Hi Again,. Thank you again for your help (on previous issues like #166 and #167). I have successfully run one Exome sample and one WGS sample (on AWS and Google Cloud). The WGS sample has ~250 million reads (paired-end, 150 bp x 150 bp), so it has ~25x coverage. Using the code based upon the [Google Cloud DeepVariant example]( https://cloud.google.com/genomics/docs/tutorials/deepvariant), I could process this 25X WGS samples in almost exactly 24 hours at a cost of approximately $10. On AWS, I probably should have kept more complete notes, but I believe it completed in ~18 hours (with a cost of approximately $15). This includes some cost of storage for the exome sample (and the WGS .fastq.gz files), although read storage would need to be taken into consideration if performing all analysis on the clould. Nevertheless, I previously reported [a much larger number]( https://github.com/google/deepvariant/issues/167#issuecomment-479522653) when I was learning how to use AWS, and these cost figures are now much more similar (as probably should be expected). One reason that I found running each step separately on AWS to be helpful was that I could figure out that I needed to add an extra parameter (`--sample_name VeritasProvided `) for the WGS dataset (for the provided alignment from Veritas) that wasnt necessary for the Exome dataset (for the provided alignment from Genos). Ive re-processed each sample locally, so I would also like to compare variant calls from a BWA-MEM alignment. Plus, I would like to make my comparison to AWS as fair as possible. So, here are my thoughts moving forward:. **1a)** I think it is good that you have changed the example WGS run time from [70 minutes]( https://github.com/google/deepvariant/blob/9d24133fc83e0423b3d5cf125a710bbefa864bbb/README.md) minutes to [5 hours](https://github.com/google/deepvariant/blob/r0.8/README.md), but this is still quite different than ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:241,energy efficiency,Cloud,Cloud,241,"Cloud Run-Time and Cost (including Docker on Google Cloud with Data in Bucket); Hi Again,. Thank you again for your help (on previous issues like #166 and #167). I have successfully run one Exome sample and one WGS sample (on AWS and Google Cloud). The WGS sample has ~250 million reads (paired-end, 150 bp x 150 bp), so it has ~25x coverage. Using the code based upon the [Google Cloud DeepVariant example]( https://cloud.google.com/genomics/docs/tutorials/deepvariant), I could process this 25X WGS samples in almost exactly 24 hours at a cost of approximately $10. On AWS, I probably should have kept more complete notes, but I believe it completed in ~18 hours (with a cost of approximately $15). This includes some cost of storage for the exome sample (and the WGS .fastq.gz files), although read storage would need to be taken into consideration if performing all analysis on the clould. Nevertheless, I previously reported [a much larger number]( https://github.com/google/deepvariant/issues/167#issuecomment-479522653) when I was learning how to use AWS, and these cost figures are now much more similar (as probably should be expected). One reason that I found running each step separately on AWS to be helpful was that I could figure out that I needed to add an extra parameter (`--sample_name VeritasProvided `) for the WGS dataset (for the provided alignment from Veritas) that wasnt necessary for the Exome dataset (for the provided alignment from Genos). Ive re-processed each sample locally, so I would also like to compare variant calls from a BWA-MEM alignment. Plus, I would like to make my comparison to AWS as fair as possible. So, here are my thoughts moving forward:. **1a)** I think it is good that you have changed the example WGS run time from [70 minutes]( https://github.com/google/deepvariant/blob/9d24133fc83e0423b3d5cf125a710bbefa864bbb/README.md) minutes to [5 hours](https://github.com/google/deepvariant/blob/r0.8/README.md), but this is still quite different than ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:381,energy efficiency,Cloud,Cloud,381,"Cloud Run-Time and Cost (including Docker on Google Cloud with Data in Bucket); Hi Again,. Thank you again for your help (on previous issues like #166 and #167). I have successfully run one Exome sample and one WGS sample (on AWS and Google Cloud). The WGS sample has ~250 million reads (paired-end, 150 bp x 150 bp), so it has ~25x coverage. Using the code based upon the [Google Cloud DeepVariant example]( https://cloud.google.com/genomics/docs/tutorials/deepvariant), I could process this 25X WGS samples in almost exactly 24 hours at a cost of approximately $10. On AWS, I probably should have kept more complete notes, but I believe it completed in ~18 hours (with a cost of approximately $15). This includes some cost of storage for the exome sample (and the WGS .fastq.gz files), although read storage would need to be taken into consideration if performing all analysis on the clould. Nevertheless, I previously reported [a much larger number]( https://github.com/google/deepvariant/issues/167#issuecomment-479522653) when I was learning how to use AWS, and these cost figures are now much more similar (as probably should be expected). One reason that I found running each step separately on AWS to be helpful was that I could figure out that I needed to add an extra parameter (`--sample_name VeritasProvided `) for the WGS dataset (for the provided alignment from Veritas) that wasnt necessary for the Exome dataset (for the provided alignment from Genos). Ive re-processed each sample locally, so I would also like to compare variant calls from a BWA-MEM alignment. Plus, I would like to make my comparison to AWS as fair as possible. So, here are my thoughts moving forward:. **1a)** I think it is good that you have changed the example WGS run time from [70 minutes]( https://github.com/google/deepvariant/blob/9d24133fc83e0423b3d5cf125a710bbefa864bbb/README.md) minutes to [5 hours](https://github.com/google/deepvariant/blob/r0.8/README.md), but this is still quite different than ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:417,energy efficiency,cloud,cloud,417,"Cloud Run-Time and Cost (including Docker on Google Cloud with Data in Bucket); Hi Again,. Thank you again for your help (on previous issues like #166 and #167). I have successfully run one Exome sample and one WGS sample (on AWS and Google Cloud). The WGS sample has ~250 million reads (paired-end, 150 bp x 150 bp), so it has ~25x coverage. Using the code based upon the [Google Cloud DeepVariant example]( https://cloud.google.com/genomics/docs/tutorials/deepvariant), I could process this 25X WGS samples in almost exactly 24 hours at a cost of approximately $10. On AWS, I probably should have kept more complete notes, but I believe it completed in ~18 hours (with a cost of approximately $15). This includes some cost of storage for the exome sample (and the WGS .fastq.gz files), although read storage would need to be taken into consideration if performing all analysis on the clould. Nevertheless, I previously reported [a much larger number]( https://github.com/google/deepvariant/issues/167#issuecomment-479522653) when I was learning how to use AWS, and these cost figures are now much more similar (as probably should be expected). One reason that I found running each step separately on AWS to be helpful was that I could figure out that I needed to add an extra parameter (`--sample_name VeritasProvided `) for the WGS dataset (for the provided alignment from Veritas) that wasnt necessary for the Exome dataset (for the provided alignment from Genos). Ive re-processed each sample locally, so I would also like to compare variant calls from a BWA-MEM alignment. Plus, I would like to make my comparison to AWS as fair as possible. So, here are my thoughts moving forward:. **1a)** I think it is good that you have changed the example WGS run time from [70 minutes]( https://github.com/google/deepvariant/blob/9d24133fc83e0423b3d5cf125a710bbefa864bbb/README.md) minutes to [5 hours](https://github.com/google/deepvariant/blob/r0.8/README.md), but this is still quite different than ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:2147,energy efficiency,cloud,cloud,2147,"reason that I found running each step separately on AWS to be helpful was that I could figure out that I needed to add an extra parameter (`--sample_name VeritasProvided `) for the WGS dataset (for the provided alignment from Veritas) that wasnt necessary for the Exome dataset (for the provided alignment from Genos). Ive re-processed each sample locally, so I would also like to compare variant calls from a BWA-MEM alignment. Plus, I would like to make my comparison to AWS as fair as possible. So, here are my thoughts moving forward:. **1a)** I think it is good that you have changed the example WGS run time from [70 minutes]( https://github.com/google/deepvariant/blob/9d24133fc83e0423b3d5cf125a710bbefa864bbb/README.md) minutes to [5 hours](https://github.com/google/deepvariant/blob/r0.8/README.md), but this is still quite different than my own experience (**24 hours**). I believe my upload times for my WGS datasets was also about a day. However, I also noticed that the upload from cloud servers to the precisionFDA was much quicker (so, maybe part of this is my apartment internet connection). To be fair, with the $300 credit, DeepVariant is essentially free to use on Google Cloud. However, if I was evaluating Cloud options in the long term (prior to conducting my own test), I think the examples of $0.20 per Exome (my own was <$1.00, but I think it was more than $0.20) and $2-3 per WGS (compared to $10+) may not be representative for somebody who is thinking about using the Cloud (versus local server + storage) for processing 100s or 1000s of samples (or even more, although I assume that would probably be for more than most individual labs or citizen scientists). I think 24 hour run-time was similar to running GATK on my local computer (with 8 GB of RAM and 4 cores), so Im not really complaining about the Cloud run-time that I encountered (I am just saying that the estimates provided on the README didnt match my own experience, even with an almost identical command",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:2343,energy efficiency,Cloud,Cloud,2343,"r the provided alignment from Veritas) that wasnt necessary for the Exome dataset (for the provided alignment from Genos). Ive re-processed each sample locally, so I would also like to compare variant calls from a BWA-MEM alignment. Plus, I would like to make my comparison to AWS as fair as possible. So, here are my thoughts moving forward:. **1a)** I think it is good that you have changed the example WGS run time from [70 minutes]( https://github.com/google/deepvariant/blob/9d24133fc83e0423b3d5cf125a710bbefa864bbb/README.md) minutes to [5 hours](https://github.com/google/deepvariant/blob/r0.8/README.md), but this is still quite different than my own experience (**24 hours**). I believe my upload times for my WGS datasets was also about a day. However, I also noticed that the upload from cloud servers to the precisionFDA was much quicker (so, maybe part of this is my apartment internet connection). To be fair, with the $300 credit, DeepVariant is essentially free to use on Google Cloud. However, if I was evaluating Cloud options in the long term (prior to conducting my own test), I think the examples of $0.20 per Exome (my own was <$1.00, but I think it was more than $0.20) and $2-3 per WGS (compared to $10+) may not be representative for somebody who is thinking about using the Cloud (versus local server + storage) for processing 100s or 1000s of samples (or even more, although I assume that would probably be for more than most individual labs or citizen scientists). I think 24 hour run-time was similar to running GATK on my local computer (with 8 GB of RAM and 4 cores), so Im not really complaining about the Cloud run-time that I encountered (I am just saying that the estimates provided on the README didnt match my own experience, even with an almost identical command on Google Cloud). **1b)** I realize that it would take some time (and Im not sure what would be the benefits versus other projects). However, have you considered allowing users to upload their r",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:2379,energy efficiency,Cloud,Cloud,2379,"s) that wasnt necessary for the Exome dataset (for the provided alignment from Genos). Ive re-processed each sample locally, so I would also like to compare variant calls from a BWA-MEM alignment. Plus, I would like to make my comparison to AWS as fair as possible. So, here are my thoughts moving forward:. **1a)** I think it is good that you have changed the example WGS run time from [70 minutes]( https://github.com/google/deepvariant/blob/9d24133fc83e0423b3d5cf125a710bbefa864bbb/README.md) minutes to [5 hours](https://github.com/google/deepvariant/blob/r0.8/README.md), but this is still quite different than my own experience (**24 hours**). I believe my upload times for my WGS datasets was also about a day. However, I also noticed that the upload from cloud servers to the precisionFDA was much quicker (so, maybe part of this is my apartment internet connection). To be fair, with the $300 credit, DeepVariant is essentially free to use on Google Cloud. However, if I was evaluating Cloud options in the long term (prior to conducting my own test), I think the examples of $0.20 per Exome (my own was <$1.00, but I think it was more than $0.20) and $2-3 per WGS (compared to $10+) may not be representative for somebody who is thinking about using the Cloud (versus local server + storage) for processing 100s or 1000s of samples (or even more, although I assume that would probably be for more than most individual labs or citizen scientists). I think 24 hour run-time was similar to running GATK on my local computer (with 8 GB of RAM and 4 cores), so Im not really complaining about the Cloud run-time that I encountered (I am just saying that the estimates provided on the README didnt match my own experience, even with an almost identical command on Google Cloud). **1b)** I realize that it would take some time (and Im not sure what would be the benefits versus other projects). However, have you considered allowing users to upload their run-time information (and estimated c",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:2648,energy efficiency,Cloud,Cloud,2648,"o, here are my thoughts moving forward:. **1a)** I think it is good that you have changed the example WGS run time from [70 minutes]( https://github.com/google/deepvariant/blob/9d24133fc83e0423b3d5cf125a710bbefa864bbb/README.md) minutes to [5 hours](https://github.com/google/deepvariant/blob/r0.8/README.md), but this is still quite different than my own experience (**24 hours**). I believe my upload times for my WGS datasets was also about a day. However, I also noticed that the upload from cloud servers to the precisionFDA was much quicker (so, maybe part of this is my apartment internet connection). To be fair, with the $300 credit, DeepVariant is essentially free to use on Google Cloud. However, if I was evaluating Cloud options in the long term (prior to conducting my own test), I think the examples of $0.20 per Exome (my own was <$1.00, but I think it was more than $0.20) and $2-3 per WGS (compared to $10+) may not be representative for somebody who is thinking about using the Cloud (versus local server + storage) for processing 100s or 1000s of samples (or even more, although I assume that would probably be for more than most individual labs or citizen scientists). I think 24 hour run-time was similar to running GATK on my local computer (with 8 GB of RAM and 4 cores), so Im not really complaining about the Cloud run-time that I encountered (I am just saying that the estimates provided on the README didnt match my own experience, even with an almost identical command on Google Cloud). **1b)** I realize that it would take some time (and Im not sure what would be the benefits versus other projects). However, have you considered allowing users to upload their run-time information (and estimated costs) to a program that might be able to help estimate run-time and cost (to possible help with topic **1a)**, **in the long-term**)? Since `gcp_deepvariant_runner` avoids the possibility of delays between running steps (and has an exist status depending upon whether v",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:2939,energy efficiency,core,cores,2939,"b/r0.8/README.md), but this is still quite different than my own experience (**24 hours**). I believe my upload times for my WGS datasets was also about a day. However, I also noticed that the upload from cloud servers to the precisionFDA was much quicker (so, maybe part of this is my apartment internet connection). To be fair, with the $300 credit, DeepVariant is essentially free to use on Google Cloud. However, if I was evaluating Cloud options in the long term (prior to conducting my own test), I think the examples of $0.20 per Exome (my own was <$1.00, but I think it was more than $0.20) and $2-3 per WGS (compared to $10+) may not be representative for somebody who is thinking about using the Cloud (versus local server + storage) for processing 100s or 1000s of samples (or even more, although I assume that would probably be for more than most individual labs or citizen scientists). I think 24 hour run-time was similar to running GATK on my local computer (with 8 GB of RAM and 4 cores), so Im not really complaining about the Cloud run-time that I encountered (I am just saying that the estimates provided on the README didnt match my own experience, even with an almost identical command on Google Cloud). **1b)** I realize that it would take some time (and Im not sure what would be the benefits versus other projects). However, have you considered allowing users to upload their run-time information (and estimated costs) to a program that might be able to help estimate run-time and cost (to possible help with topic **1a)**, **in the long-term**)? Since `gcp_deepvariant_runner` avoids the possibility of delays between running steps (and has an exist status depending upon whether variant calling was successful), perhaps some sort of optional reporting to an anonymized database could be provided as a parameter for that? **2)** While I realize it could be considered a cross-post, I am trying to test running each of the 3 steps run separately on Google Cloud (instead of",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:2987,energy efficiency,Cloud,Cloud,2987,"rent than my own experience (**24 hours**). I believe my upload times for my WGS datasets was also about a day. However, I also noticed that the upload from cloud servers to the precisionFDA was much quicker (so, maybe part of this is my apartment internet connection). To be fair, with the $300 credit, DeepVariant is essentially free to use on Google Cloud. However, if I was evaluating Cloud options in the long term (prior to conducting my own test), I think the examples of $0.20 per Exome (my own was <$1.00, but I think it was more than $0.20) and $2-3 per WGS (compared to $10+) may not be representative for somebody who is thinking about using the Cloud (versus local server + storage) for processing 100s or 1000s of samples (or even more, although I assume that would probably be for more than most individual labs or citizen scientists). I think 24 hour run-time was similar to running GATK on my local computer (with 8 GB of RAM and 4 cores), so Im not really complaining about the Cloud run-time that I encountered (I am just saying that the estimates provided on the README didnt match my own experience, even with an almost identical command on Google Cloud). **1b)** I realize that it would take some time (and Im not sure what would be the benefits versus other projects). However, have you considered allowing users to upload their run-time information (and estimated costs) to a program that might be able to help estimate run-time and cost (to possible help with topic **1a)**, **in the long-term**)? Since `gcp_deepvariant_runner` avoids the possibility of delays between running steps (and has an exist status depending upon whether variant calling was successful), perhaps some sort of optional reporting to an anonymized database could be provided as a parameter for that? **2)** While I realize it could be considered a cross-post, I am trying to test running each of the 3 steps run separately on Google Cloud (instead of using `gcloud alpha genomics pipelines`). I hav",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:3048,energy efficiency,estimat,estimates,3048," times for my WGS datasets was also about a day. However, I also noticed that the upload from cloud servers to the precisionFDA was much quicker (so, maybe part of this is my apartment internet connection). To be fair, with the $300 credit, DeepVariant is essentially free to use on Google Cloud. However, if I was evaluating Cloud options in the long term (prior to conducting my own test), I think the examples of $0.20 per Exome (my own was <$1.00, but I think it was more than $0.20) and $2-3 per WGS (compared to $10+) may not be representative for somebody who is thinking about using the Cloud (versus local server + storage) for processing 100s or 1000s of samples (or even more, although I assume that would probably be for more than most individual labs or citizen scientists). I think 24 hour run-time was similar to running GATK on my local computer (with 8 GB of RAM and 4 cores), so Im not really complaining about the Cloud run-time that I encountered (I am just saying that the estimates provided on the README didnt match my own experience, even with an almost identical command on Google Cloud). **1b)** I realize that it would take some time (and Im not sure what would be the benefits versus other projects). However, have you considered allowing users to upload their run-time information (and estimated costs) to a program that might be able to help estimate run-time and cost (to possible help with topic **1a)**, **in the long-term**)? Since `gcp_deepvariant_runner` avoids the possibility of delays between running steps (and has an exist status depending upon whether variant calling was successful), perhaps some sort of optional reporting to an anonymized database could be provided as a parameter for that? **2)** While I realize it could be considered a cross-post, I am trying to test running each of the 3 steps run separately on Google Cloud (instead of using `gcloud alpha genomics pipelines`). I have some notes on this [Stack Overflow post]( https://stackoverfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:3161,energy efficiency,Cloud,Cloud,3161,"the precisionFDA was much quicker (so, maybe part of this is my apartment internet connection). To be fair, with the $300 credit, DeepVariant is essentially free to use on Google Cloud. However, if I was evaluating Cloud options in the long term (prior to conducting my own test), I think the examples of $0.20 per Exome (my own was <$1.00, but I think it was more than $0.20) and $2-3 per WGS (compared to $10+) may not be representative for somebody who is thinking about using the Cloud (versus local server + storage) for processing 100s or 1000s of samples (or even more, although I assume that would probably be for more than most individual labs or citizen scientists). I think 24 hour run-time was similar to running GATK on my local computer (with 8 GB of RAM and 4 cores), so Im not really complaining about the Cloud run-time that I encountered (I am just saying that the estimates provided on the README didnt match my own experience, even with an almost identical command on Google Cloud). **1b)** I realize that it would take some time (and Im not sure what would be the benefits versus other projects). However, have you considered allowing users to upload their run-time information (and estimated costs) to a program that might be able to help estimate run-time and cost (to possible help with topic **1a)**, **in the long-term**)? Since `gcp_deepvariant_runner` avoids the possibility of delays between running steps (and has an exist status depending upon whether variant calling was successful), perhaps some sort of optional reporting to an anonymized database could be provided as a parameter for that? **2)** While I realize it could be considered a cross-post, I am trying to test running each of the 3 steps run separately on Google Cloud (instead of using `gcloud alpha genomics pipelines`). I have some notes on this [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about th",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:3371,energy efficiency,estimat,estimated,3371,"ng Cloud options in the long term (prior to conducting my own test), I think the examples of $0.20 per Exome (my own was <$1.00, but I think it was more than $0.20) and $2-3 per WGS (compared to $10+) may not be representative for somebody who is thinking about using the Cloud (versus local server + storage) for processing 100s or 1000s of samples (or even more, although I assume that would probably be for more than most individual labs or citizen scientists). I think 24 hour run-time was similar to running GATK on my local computer (with 8 GB of RAM and 4 cores), so Im not really complaining about the Cloud run-time that I encountered (I am just saying that the estimates provided on the README didnt match my own experience, even with an almost identical command on Google Cloud). **1b)** I realize that it would take some time (and Im not sure what would be the benefits versus other projects). However, have you considered allowing users to upload their run-time information (and estimated costs) to a program that might be able to help estimate run-time and cost (to possible help with topic **1a)**, **in the long-term**)? Since `gcp_deepvariant_runner` avoids the possibility of delays between running steps (and has an exist status depending upon whether variant calling was successful), perhaps some sort of optional reporting to an anonymized database could be provided as a parameter for that? **2)** While I realize it could be considered a cross-post, I am trying to test running each of the 3 steps run separately on Google Cloud (instead of using `gcloud alpha genomics pipelines`). I have some notes on this [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a G",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:3428,energy efficiency,estimat,estimate,3428,"y own test), I think the examples of $0.20 per Exome (my own was <$1.00, but I think it was more than $0.20) and $2-3 per WGS (compared to $10+) may not be representative for somebody who is thinking about using the Cloud (versus local server + storage) for processing 100s or 1000s of samples (or even more, although I assume that would probably be for more than most individual labs or citizen scientists). I think 24 hour run-time was similar to running GATK on my local computer (with 8 GB of RAM and 4 cores), so Im not really complaining about the Cloud run-time that I encountered (I am just saying that the estimates provided on the README didnt match my own experience, even with an almost identical command on Google Cloud). **1b)** I realize that it would take some time (and Im not sure what would be the benefits versus other projects). However, have you considered allowing users to upload their run-time information (and estimated costs) to a program that might be able to help estimate run-time and cost (to possible help with topic **1a)**, **in the long-term**)? Since `gcp_deepvariant_runner` avoids the possibility of delays between running steps (and has an exist status depending upon whether variant calling was successful), perhaps some sort of optional reporting to an anonymized database could be provided as a parameter for that? **2)** While I realize it could be considered a cross-post, I am trying to test running each of the 3 steps run separately on Google Cloud (instead of using `gcloud alpha genomics pipelines`). I have some notes on this [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are di",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:3925,energy efficiency,Cloud,Cloud,3925," RAM and 4 cores), so Im not really complaining about the Cloud run-time that I encountered (I am just saying that the estimates provided on the README didnt match my own experience, even with an almost identical command on Google Cloud). **1b)** I realize that it would take some time (and Im not sure what would be the benefits versus other projects). However, have you considered allowing users to upload their run-time information (and estimated costs) to a program that might be able to help estimate run-time and cost (to possible help with topic **1a)**, **in the long-term**)? Since `gcp_deepvariant_runner` avoids the possibility of delays between running steps (and has an exist status depending upon whether variant calling was successful), perhaps some sort of optional reporting to an anonymized database could be provided as a parameter for that? **2)** While I realize it could be considered a cross-post, I am trying to test running each of the 3 steps run separately on Google Cloud (instead of using `gcloud alpha genomics pipelines`). I have some notes on this [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_ru",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:4104,energy efficiency,cloud,cloud-instance-with-data-in-gcsfuse-mounted-bucket,4104,"ost identical command on Google Cloud). **1b)** I realize that it would take some time (and Im not sure what would be the benefits versus other projects). However, have you considered allowing users to upload their run-time information (and estimated costs) to a program that might be able to help estimate run-time and cost (to possible help with topic **1a)**, **in the long-term**)? Since `gcp_deepvariant_runner` avoids the possibility of delays between running steps (and has an exist status depending upon whether variant calling was successful), perhaps some sort of optional reporting to an anonymized database could be provided as a parameter for that? **2)** While I realize it could be considered a cross-post, I am trying to test running each of the 3 steps run separately on Google Cloud (instead of using `gcloud alpha genomics pipelines`). I have some notes on this [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:4225,energy efficiency,Cloud,Cloud,4225," sure what would be the benefits versus other projects). However, have you considered allowing users to upload their run-time information (and estimated costs) to a program that might be able to help estimate run-time and cost (to possible help with topic **1a)**, **in the long-term**)? Since `gcp_deepvariant_runner` avoids the possibility of delays between running steps (and has an exist status depending upon whether variant calling was successful), perhaps some sort of optional reporting to an anonymized database could be provided as a parameter for that? **2)** While I realize it could be considered a cross-post, I am trying to test running each of the 3 steps run separately on Google Cloud (instead of using `gcloud alpha genomics pipelines`). I have some notes on this [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading st",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:4344,energy efficiency,Cloud,Cloud,4344,"n-time information (and estimated costs) to a program that might be able to help estimate run-time and cost (to possible help with topic **1a)**, **in the long-term**)? Since `gcp_deepvariant_runner` avoids the possibility of delays between running steps (and has an exist status depending upon whether variant calling was successful), perhaps some sort of optional reporting to an anonymized database could be provided as a parameter for that? **2)** While I realize it could be considered a cross-post, I am trying to test running each of the 3 steps run separately on Google Cloud (instead of using `gcloud alpha genomics pipelines`). I have some notes on this [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remov",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:4382,energy efficiency,Cloud,Cloud,4382,"s) to a program that might be able to help estimate run-time and cost (to possible help with topic **1a)**, **in the long-term**)? Since `gcp_deepvariant_runner` avoids the possibility of delays between running steps (and has an exist status depending upon whether variant calling was successful), perhaps some sort of optional reporting to an anonymized database could be provided as a parameter for that? **2)** While I realize it could be considered a cross-post, I am trying to test running each of the 3 steps run separately on Google Cloud (instead of using `gcloud alpha genomics pipelines`). I have some notes on this [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot cr",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:4686,energy efficiency,Cloud,Cloud,4686,"s some sort of optional reporting to an anonymized database could be provided as a parameter for that? **2)** While I realize it could be considered a cross-post, I am trying to test running each of the 3 steps run separately on Google Cloud (instead of using `gcloud alpha genomics pipelines`). I have some notes on this [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to pro",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:5475,energy efficiency,CPU,CPU,5475,"versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled""",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:5779,energy efficiency,Power,Power,5779,"tps://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:6062,energy efficiency,CPU,CPU,6062,"the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:6066,energy efficiency,core,cores,6066,"ewest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:7345,energy efficiency,current,currently,7345,"ile creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), but I only copied one representative example above, for the repeated part. If I try to run the DeepVariant container in interactive mode (to try and understand what is going on), I get the following message (which is a note, without actually going into interactive mode):. ```. docker run -it -v /home/user/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant. See https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md. ```. I do have the `gcloud alpha genomics pipelines` example working, so this isnt absolutely essential for running DeepVariant on Google Cloud. However, if you can help provide me some guidance for running the [linked script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud, I would very much appreciate it. Thank you very much,. Charles.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:8020,energy efficiency,Cloud,Cloud,8020,"ile creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), but I only copied one representative example above, for the repeated part. If I try to run the DeepVariant container in interactive mode (to try and understand what is going on), I get the following message (which is a note, without actually going into interactive mode):. ```. docker run -it -v /home/user/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant. See https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md. ```. I do have the `gcloud alpha genomics pipelines` example working, so this isnt absolutely essential for running DeepVariant on Google Cloud. However, if you can help provide me some guidance for running the [linked script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud, I would very much appreciate it. Thank you very much,. Charles.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:8212,energy efficiency,Cloud,Cloud,8212,"ile creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), but I only copied one representative example above, for the repeated part. If I try to run the DeepVariant container in interactive mode (to try and understand what is going on), I get the following message (which is a note, without actually going into interactive mode):. ```. docker run -it -v /home/user/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant. See https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md. ```. I do have the `gcloud alpha genomics pipelines` example working, so this isnt absolutely essential for running DeepVariant on Google Cloud. However, if you can help provide me some guidance for running the [linked script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud, I would very much appreciate it. Thank you very much,. Charles.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:3478,integrability,topic,topic,3478,"ome (my own was <$1.00, but I think it was more than $0.20) and $2-3 per WGS (compared to $10+) may not be representative for somebody who is thinking about using the Cloud (versus local server + storage) for processing 100s or 1000s of samples (or even more, although I assume that would probably be for more than most individual labs or citizen scientists). I think 24 hour run-time was similar to running GATK on my local computer (with 8 GB of RAM and 4 cores), so Im not really complaining about the Cloud run-time that I encountered (I am just saying that the estimates provided on the README didnt match my own experience, even with an almost identical command on Google Cloud). **1b)** I realize that it would take some time (and Im not sure what would be the benefits versus other projects). However, have you considered allowing users to upload their run-time information (and estimated costs) to a program that might be able to help estimate run-time and cost (to possible help with topic **1a)**, **in the long-term**)? Since `gcp_deepvariant_runner` avoids the possibility of delays between running steps (and has an exist status depending upon whether variant calling was successful), perhaps some sort of optional reporting to an anonymized database could be provided as a parameter for that? **2)** While I realize it could be considered a cross-post, I am trying to test running each of the 3 steps run separately on Google Cloud (instead of using `gcloud alpha genomics pipelines`). I have some notes on this [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container vers",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:3627,integrability,depend,depending,3627,"about using the Cloud (versus local server + storage) for processing 100s or 1000s of samples (or even more, although I assume that would probably be for more than most individual labs or citizen scientists). I think 24 hour run-time was similar to running GATK on my local computer (with 8 GB of RAM and 4 cores), so Im not really complaining about the Cloud run-time that I encountered (I am just saying that the estimates provided on the README didnt match my own experience, even with an almost identical command on Google Cloud). **1b)** I realize that it would take some time (and Im not sure what would be the benefits versus other projects). However, have you considered allowing users to upload their run-time information (and estimated costs) to a program that might be able to help estimate run-time and cost (to possible help with topic **1a)**, **in the long-term**)? Since `gcp_deepvariant_runner` avoids the possibility of delays between running steps (and has an exist status depending upon whether variant calling was successful), perhaps some sort of optional reporting to an anonymized database could be provided as a parameter for that? **2)** While I realize it could be considered a cross-post, I am trying to test running each of the 3 steps run separately on Google Cloud (instead of using `gcloud alpha genomics pipelines`). I have some notes on this [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/ma",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:3972,integrability,pipelin,pipelines,3972,"about the Cloud run-time that I encountered (I am just saying that the estimates provided on the README didnt match my own experience, even with an almost identical command on Google Cloud). **1b)** I realize that it would take some time (and Im not sure what would be the benefits versus other projects). However, have you considered allowing users to upload their run-time information (and estimated costs) to a program that might be able to help estimate run-time and cost (to possible help with topic **1a)**, **in the long-term**)? Since `gcp_deepvariant_runner` avoids the possibility of delays between running steps (and has an exist status depending upon whether variant calling was successful), perhaps some sort of optional reporting to an anonymized database could be provided as a parameter for that? **2)** While I realize it could be considered a cross-post, I am trying to test running each of the 3 steps run separately on Google Cloud (instead of using `gcloud alpha genomics pipelines`). I have some notes on this [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:4406,integrability,messag,messages,4406,"t be able to help estimate run-time and cost (to possible help with topic **1a)**, **in the long-term**)? Since `gcp_deepvariant_runner` avoids the possibility of delays between running steps (and has an exist status depending upon whether variant calling was successful), perhaps some sort of optional reporting to an anonymized database could be provided as a parameter for that? **2)** While I realize it could be considered a cross-post, I am trying to test running each of the 3 steps run separately on Google Cloud (instead of using `gcloud alpha genomics pipelines`). I have some notes on this [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:4894,integrability,messag,message,4894,"s run separately on Google Cloud (instead of using `gcloud alpha genomics pipelines`). I have some notes on this [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COS",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:4996,integrability,depend,dependency,4996,"s on this [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:5021,integrability,state,state,5021,"ow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: r",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:5075,integrability,version,version,5075,"running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max j",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:5201,integrability,depend,dependency,5201,"f Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:5226,integrability,state,state,5226,"d. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/1",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:5284,integrability,version,version,5284,"ed to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while cr",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:5703,integrability,pub,publication,5703,"o AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:7385,integrability,messag,message,7385,"ile creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), but I only copied one representative example above, for the repeated part. If I try to run the DeepVariant container in interactive mode (to try and understand what is going on), I get the following message (which is a note, without actually going into interactive mode):. ```. docker run -it -v /home/user/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant. See https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md. ```. I do have the `gcloud alpha genomics pipelines` example working, so this isnt absolutely essential for running DeepVariant on Google Cloud. However, if you can help provide me some guidance for running the [linked script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud, I would very much appreciate it. Thank you very much,. Charles.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:7622,integrability,messag,message,7622,"ile creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), but I only copied one representative example above, for the repeated part. If I try to run the DeepVariant container in interactive mode (to try and understand what is going on), I get the following message (which is a note, without actually going into interactive mode):. ```. docker run -it -v /home/user/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant. See https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md. ```. I do have the `gcloud alpha genomics pipelines` example working, so this isnt absolutely essential for running DeepVariant on Google Cloud. However, if you can help provide me some guidance for running the [linked script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud, I would very much appreciate it. Thank you very much,. Charles.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:7923,integrability,pipelin,pipelines,7923,"ile creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), but I only copied one representative example above, for the repeated part. If I try to run the DeepVariant container in interactive mode (to try and understand what is going on), I get the following message (which is a note, without actually going into interactive mode):. ```. docker run -it -v /home/user/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant. See https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md. ```. I do have the `gcloud alpha genomics pipelines` example working, so this isnt absolutely essential for running DeepVariant on Google Cloud. However, if you can help provide me some guidance for running the [linked script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud, I would very much appreciate it. Thank you very much,. Charles.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:4406,interoperability,messag,messages,4406,"t be able to help estimate run-time and cost (to possible help with topic **1a)**, **in the long-term**)? Since `gcp_deepvariant_runner` avoids the possibility of delays between running steps (and has an exist status depending upon whether variant calling was successful), perhaps some sort of optional reporting to an anonymized database could be provided as a parameter for that? **2)** While I realize it could be considered a cross-post, I am trying to test running each of the 3 steps run separately on Google Cloud (instead of using `gcloud alpha genomics pipelines`). I have some notes on this [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:4894,interoperability,messag,message,4894,"s run separately on Google Cloud (instead of using `gcloud alpha genomics pipelines`). I have some notes on this [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COS",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:7385,interoperability,messag,message,7385,"ile creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), but I only copied one representative example above, for the repeated part. If I try to run the DeepVariant container in interactive mode (to try and understand what is going on), I get the following message (which is a note, without actually going into interactive mode):. ```. docker run -it -v /home/user/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant. See https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md. ```. I do have the `gcloud alpha genomics pipelines` example working, so this isnt absolutely essential for running DeepVariant on Google Cloud. However, if you can help provide me some guidance for running the [linked script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud, I would very much appreciate it. Thank you very much,. Charles.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:7622,interoperability,messag,message,7622,"ile creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), but I only copied one representative example above, for the repeated part. If I try to run the DeepVariant container in interactive mode (to try and understand what is going on), I get the following message (which is a note, without actually going into interactive mode):. ```. docker run -it -v /home/user/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant. See https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md. ```. I do have the `gcloud alpha genomics pipelines` example working, so this isnt absolutely essential for running DeepVariant on Google Cloud. However, if you can help provide me some guidance for running the [linked script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud, I would very much appreciate it. Thank you very much,. Charles.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:1278,modifiability,paramet,parameter,1278,"ads (paired-end, 150 bp x 150 bp), so it has ~25x coverage. Using the code based upon the [Google Cloud DeepVariant example]( https://cloud.google.com/genomics/docs/tutorials/deepvariant), I could process this 25X WGS samples in almost exactly 24 hours at a cost of approximately $10. On AWS, I probably should have kept more complete notes, but I believe it completed in ~18 hours (with a cost of approximately $15). This includes some cost of storage for the exome sample (and the WGS .fastq.gz files), although read storage would need to be taken into consideration if performing all analysis on the clould. Nevertheless, I previously reported [a much larger number]( https://github.com/google/deepvariant/issues/167#issuecomment-479522653) when I was learning how to use AWS, and these cost figures are now much more similar (as probably should be expected). One reason that I found running each step separately on AWS to be helpful was that I could figure out that I needed to add an extra parameter (`--sample_name VeritasProvided `) for the WGS dataset (for the provided alignment from Veritas) that wasnt necessary for the Exome dataset (for the provided alignment from Genos). Ive re-processed each sample locally, so I would also like to compare variant calls from a BWA-MEM alignment. Plus, I would like to make my comparison to AWS as fair as possible. So, here are my thoughts moving forward:. **1a)** I think it is good that you have changed the example WGS run time from [70 minutes]( https://github.com/google/deepvariant/blob/9d24133fc83e0423b3d5cf125a710bbefa864bbb/README.md) minutes to [5 hours](https://github.com/google/deepvariant/blob/r0.8/README.md), but this is still quite different than my own experience (**24 hours**). I believe my upload times for my WGS datasets was also about a day. However, I also noticed that the upload from cloud servers to the precisionFDA was much quicker (so, maybe part of this is my apartment internet connection). To be fair, with the $3",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:3627,modifiability,depend,depending,3627,"about using the Cloud (versus local server + storage) for processing 100s or 1000s of samples (or even more, although I assume that would probably be for more than most individual labs or citizen scientists). I think 24 hour run-time was similar to running GATK on my local computer (with 8 GB of RAM and 4 cores), so Im not really complaining about the Cloud run-time that I encountered (I am just saying that the estimates provided on the README didnt match my own experience, even with an almost identical command on Google Cloud). **1b)** I realize that it would take some time (and Im not sure what would be the benefits versus other projects). However, have you considered allowing users to upload their run-time information (and estimated costs) to a program that might be able to help estimate run-time and cost (to possible help with topic **1a)**, **in the long-term**)? Since `gcp_deepvariant_runner` avoids the possibility of delays between running steps (and has an exist status depending upon whether variant calling was successful), perhaps some sort of optional reporting to an anonymized database could be provided as a parameter for that? **2)** While I realize it could be considered a cross-post, I am trying to test running each of the 3 steps run separately on Google Cloud (instead of using `gcloud alpha genomics pipelines`). I have some notes on this [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/ma",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:3772,modifiability,paramet,parameter,3772,"y be for more than most individual labs or citizen scientists). I think 24 hour run-time was similar to running GATK on my local computer (with 8 GB of RAM and 4 cores), so Im not really complaining about the Cloud run-time that I encountered (I am just saying that the estimates provided on the README didnt match my own experience, even with an almost identical command on Google Cloud). **1b)** I realize that it would take some time (and Im not sure what would be the benefits versus other projects). However, have you considered allowing users to upload their run-time information (and estimated costs) to a program that might be able to help estimate run-time and cost (to possible help with topic **1a)**, **in the long-term**)? Since `gcp_deepvariant_runner` avoids the possibility of delays between running steps (and has an exist status depending upon whether variant calling was successful), perhaps some sort of optional reporting to an anonymized database could be provided as a parameter for that? **2)** While I realize it could be considered a cross-post, I am trying to test running each of the 3 steps run separately on Google Cloud (instead of using `gcloud alpha genomics pipelines`). I have some notes on this [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:4964,modifiability,pac,package,4964,"ics pipelines`). I have some notes on this [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Par",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:4996,modifiability,depend,dependency,4996,"s on this [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:5075,modifiability,version,version,5075,"running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max j",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:5100,modifiability,upgrad,upgraded,5100,"cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 /",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:5151,modifiability,upgrad,upgraded,5151," about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of starte",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:5169,modifiability,pac,package,5169," of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average sec",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:5201,modifiability,depend,dependency,5201,"f Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:5284,modifiability,version,version,5284,"ed to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while cr",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:5308,modifiability,upgrad,upgraded,5308,"rms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:5359,modifiability,upgrad,upgraded,5359,"stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cd",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:10,performance,Time,Time,10,"Cloud Run-Time and Cost (including Docker on Google Cloud with Data in Bucket); Hi Again,. Thank you again for your help (on previous issues like #166 and #167). I have successfully run one Exome sample and one WGS sample (on AWS and Google Cloud). The WGS sample has ~250 million reads (paired-end, 150 bp x 150 bp), so it has ~25x coverage. Using the code based upon the [Google Cloud DeepVariant example]( https://cloud.google.com/genomics/docs/tutorials/deepvariant), I could process this 25X WGS samples in almost exactly 24 hours at a cost of approximately $10. On AWS, I probably should have kept more complete notes, but I believe it completed in ~18 hours (with a cost of approximately $15). This includes some cost of storage for the exome sample (and the WGS .fastq.gz files), although read storage would need to be taken into consideration if performing all analysis on the clould. Nevertheless, I previously reported [a much larger number]( https://github.com/google/deepvariant/issues/167#issuecomment-479522653) when I was learning how to use AWS, and these cost figures are now much more similar (as probably should be expected). One reason that I found running each step separately on AWS to be helpful was that I could figure out that I needed to add an extra parameter (`--sample_name VeritasProvided `) for the WGS dataset (for the provided alignment from Veritas) that wasnt necessary for the Exome dataset (for the provided alignment from Genos). Ive re-processed each sample locally, so I would also like to compare variant calls from a BWA-MEM alignment. Plus, I would like to make my comparison to AWS as fair as possible. So, here are my thoughts moving forward:. **1a)** I think it is good that you have changed the example WGS run time from [70 minutes]( https://github.com/google/deepvariant/blob/9d24133fc83e0423b3d5cf125a710bbefa864bbb/README.md) minutes to [5 hours](https://github.com/google/deepvariant/blob/r0.8/README.md), but this is still quite different than ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:855,performance,perform,performing,855,"Cloud Run-Time and Cost (including Docker on Google Cloud with Data in Bucket); Hi Again,. Thank you again for your help (on previous issues like #166 and #167). I have successfully run one Exome sample and one WGS sample (on AWS and Google Cloud). The WGS sample has ~250 million reads (paired-end, 150 bp x 150 bp), so it has ~25x coverage. Using the code based upon the [Google Cloud DeepVariant example]( https://cloud.google.com/genomics/docs/tutorials/deepvariant), I could process this 25X WGS samples in almost exactly 24 hours at a cost of approximately $10. On AWS, I probably should have kept more complete notes, but I believe it completed in ~18 hours (with a cost of approximately $15). This includes some cost of storage for the exome sample (and the WGS .fastq.gz files), although read storage would need to be taken into consideration if performing all analysis on the clould. Nevertheless, I previously reported [a much larger number]( https://github.com/google/deepvariant/issues/167#issuecomment-479522653) when I was learning how to use AWS, and these cost figures are now much more similar (as probably should be expected). One reason that I found running each step separately on AWS to be helpful was that I could figure out that I needed to add an extra parameter (`--sample_name VeritasProvided `) for the WGS dataset (for the provided alignment from Veritas) that wasnt necessary for the Exome dataset (for the provided alignment from Genos). Ive re-processed each sample locally, so I would also like to compare variant calls from a BWA-MEM alignment. Plus, I would like to make my comparison to AWS as fair as possible. So, here are my thoughts moving forward:. **1a)** I think it is good that you have changed the example WGS run time from [70 minutes]( https://github.com/google/deepvariant/blob/9d24133fc83e0423b3d5cf125a710bbefa864bbb/README.md) minutes to [5 hours](https://github.com/google/deepvariant/blob/r0.8/README.md), but this is still quite different than ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:1761,performance,time,time,1761,"he WGS .fastq.gz files), although read storage would need to be taken into consideration if performing all analysis on the clould. Nevertheless, I previously reported [a much larger number]( https://github.com/google/deepvariant/issues/167#issuecomment-479522653) when I was learning how to use AWS, and these cost figures are now much more similar (as probably should be expected). One reason that I found running each step separately on AWS to be helpful was that I could figure out that I needed to add an extra parameter (`--sample_name VeritasProvided `) for the WGS dataset (for the provided alignment from Veritas) that wasnt necessary for the Exome dataset (for the provided alignment from Genos). Ive re-processed each sample locally, so I would also like to compare variant calls from a BWA-MEM alignment. Plus, I would like to make my comparison to AWS as fair as possible. So, here are my thoughts moving forward:. **1a)** I think it is good that you have changed the example WGS run time from [70 minutes]( https://github.com/google/deepvariant/blob/9d24133fc83e0423b3d5cf125a710bbefa864bbb/README.md) minutes to [5 hours](https://github.com/google/deepvariant/blob/r0.8/README.md), but this is still quite different than my own experience (**24 hours**). I believe my upload times for my WGS datasets was also about a day. However, I also noticed that the upload from cloud servers to the precisionFDA was much quicker (so, maybe part of this is my apartment internet connection). To be fair, with the $300 credit, DeepVariant is essentially free to use on Google Cloud. However, if I was evaluating Cloud options in the long term (prior to conducting my own test), I think the examples of $0.20 per Exome (my own was <$1.00, but I think it was more than $0.20) and $2-3 per WGS (compared to $10+) may not be representative for somebody who is thinking about using the Cloud (versus local server + storage) for processing 100s or 1000s of samples (or even more, although I assume that",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:2054,performance,time,times,2054," AWS, and these cost figures are now much more similar (as probably should be expected). One reason that I found running each step separately on AWS to be helpful was that I could figure out that I needed to add an extra parameter (`--sample_name VeritasProvided `) for the WGS dataset (for the provided alignment from Veritas) that wasnt necessary for the Exome dataset (for the provided alignment from Genos). Ive re-processed each sample locally, so I would also like to compare variant calls from a BWA-MEM alignment. Plus, I would like to make my comparison to AWS as fair as possible. So, here are my thoughts moving forward:. **1a)** I think it is good that you have changed the example WGS run time from [70 minutes]( https://github.com/google/deepvariant/blob/9d24133fc83e0423b3d5cf125a710bbefa864bbb/README.md) minutes to [5 hours](https://github.com/google/deepvariant/blob/r0.8/README.md), but this is still quite different than my own experience (**24 hours**). I believe my upload times for my WGS datasets was also about a day. However, I also noticed that the upload from cloud servers to the precisionFDA was much quicker (so, maybe part of this is my apartment internet connection). To be fair, with the $300 credit, DeepVariant is essentially free to use on Google Cloud. However, if I was evaluating Cloud options in the long term (prior to conducting my own test), I think the examples of $0.20 per Exome (my own was <$1.00, but I think it was more than $0.20) and $2-3 per WGS (compared to $10+) may not be representative for somebody who is thinking about using the Cloud (versus local server + storage) for processing 100s or 1000s of samples (or even more, although I assume that would probably be for more than most individual labs or citizen scientists). I think 24 hour run-time was similar to running GATK on my local computer (with 8 GB of RAM and 4 cores), so Im not really complaining about the Cloud run-time that I encountered (I am just saying that the estimates",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:2861,performance,time,time,2861,"64bbb/README.md) minutes to [5 hours](https://github.com/google/deepvariant/blob/r0.8/README.md), but this is still quite different than my own experience (**24 hours**). I believe my upload times for my WGS datasets was also about a day. However, I also noticed that the upload from cloud servers to the precisionFDA was much quicker (so, maybe part of this is my apartment internet connection). To be fair, with the $300 credit, DeepVariant is essentially free to use on Google Cloud. However, if I was evaluating Cloud options in the long term (prior to conducting my own test), I think the examples of $0.20 per Exome (my own was <$1.00, but I think it was more than $0.20) and $2-3 per WGS (compared to $10+) may not be representative for somebody who is thinking about using the Cloud (versus local server + storage) for processing 100s or 1000s of samples (or even more, although I assume that would probably be for more than most individual labs or citizen scientists). I think 24 hour run-time was similar to running GATK on my local computer (with 8 GB of RAM and 4 cores), so Im not really complaining about the Cloud run-time that I encountered (I am just saying that the estimates provided on the README didnt match my own experience, even with an almost identical command on Google Cloud). **1b)** I realize that it would take some time (and Im not sure what would be the benefits versus other projects). However, have you considered allowing users to upload their run-time information (and estimated costs) to a program that might be able to help estimate run-time and cost (to possible help with topic **1a)**, **in the long-term**)? Since `gcp_deepvariant_runner` avoids the possibility of delays between running steps (and has an exist status depending upon whether variant calling was successful), perhaps some sort of optional reporting to an anonymized database could be provided as a parameter for that? **2)** While I realize it could be considered a cross-post, I am trying",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:2997,performance,time,time,2997," my own experience (**24 hours**). I believe my upload times for my WGS datasets was also about a day. However, I also noticed that the upload from cloud servers to the precisionFDA was much quicker (so, maybe part of this is my apartment internet connection). To be fair, with the $300 credit, DeepVariant is essentially free to use on Google Cloud. However, if I was evaluating Cloud options in the long term (prior to conducting my own test), I think the examples of $0.20 per Exome (my own was <$1.00, but I think it was more than $0.20) and $2-3 per WGS (compared to $10+) may not be representative for somebody who is thinking about using the Cloud (versus local server + storage) for processing 100s or 1000s of samples (or even more, although I assume that would probably be for more than most individual labs or citizen scientists). I think 24 hour run-time was similar to running GATK on my local computer (with 8 GB of RAM and 4 cores), so Im not really complaining about the Cloud run-time that I encountered (I am just saying that the estimates provided on the README didnt match my own experience, even with an almost identical command on Google Cloud). **1b)** I realize that it would take some time (and Im not sure what would be the benefits versus other projects). However, have you considered allowing users to upload their run-time information (and estimated costs) to a program that might be able to help estimate run-time and cost (to possible help with topic **1a)**, **in the long-term**)? Since `gcp_deepvariant_runner` avoids the possibility of delays between running steps (and has an exist status depending upon whether variant calling was successful), perhaps some sort of optional reporting to an anonymized database could be provided as a parameter for that? **2)** While I realize it could be considered a cross-post, I am trying to test running each of the 3 steps run separately on Google Cloud (instead of using `gcloud alpha genomics pipelines`). I have some no",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:3211,performance,time,time,3211," of this is my apartment internet connection). To be fair, with the $300 credit, DeepVariant is essentially free to use on Google Cloud. However, if I was evaluating Cloud options in the long term (prior to conducting my own test), I think the examples of $0.20 per Exome (my own was <$1.00, but I think it was more than $0.20) and $2-3 per WGS (compared to $10+) may not be representative for somebody who is thinking about using the Cloud (versus local server + storage) for processing 100s or 1000s of samples (or even more, although I assume that would probably be for more than most individual labs or citizen scientists). I think 24 hour run-time was similar to running GATK on my local computer (with 8 GB of RAM and 4 cores), so Im not really complaining about the Cloud run-time that I encountered (I am just saying that the estimates provided on the README didnt match my own experience, even with an almost identical command on Google Cloud). **1b)** I realize that it would take some time (and Im not sure what would be the benefits versus other projects). However, have you considered allowing users to upload their run-time information (and estimated costs) to a program that might be able to help estimate run-time and cost (to possible help with topic **1a)**, **in the long-term**)? Since `gcp_deepvariant_runner` avoids the possibility of delays between running steps (and has an exist status depending upon whether variant calling was successful), perhaps some sort of optional reporting to an anonymized database could be provided as a parameter for that? **2)** While I realize it could be considered a cross-post, I am trying to test running each of the 3 steps run separately on Google Cloud (instead of using `gcloud alpha genomics pipelines`). I have some notes on this [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docke",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:3349,performance,time,time,3349,"owever, if I was evaluating Cloud options in the long term (prior to conducting my own test), I think the examples of $0.20 per Exome (my own was <$1.00, but I think it was more than $0.20) and $2-3 per WGS (compared to $10+) may not be representative for somebody who is thinking about using the Cloud (versus local server + storage) for processing 100s or 1000s of samples (or even more, although I assume that would probably be for more than most individual labs or citizen scientists). I think 24 hour run-time was similar to running GATK on my local computer (with 8 GB of RAM and 4 cores), so Im not really complaining about the Cloud run-time that I encountered (I am just saying that the estimates provided on the README didnt match my own experience, even with an almost identical command on Google Cloud). **1b)** I realize that it would take some time (and Im not sure what would be the benefits versus other projects). However, have you considered allowing users to upload their run-time information (and estimated costs) to a program that might be able to help estimate run-time and cost (to possible help with topic **1a)**, **in the long-term**)? Since `gcp_deepvariant_runner` avoids the possibility of delays between running steps (and has an exist status depending upon whether variant calling was successful), perhaps some sort of optional reporting to an anonymized database could be provided as a parameter for that? **2)** While I realize it could be considered a cross-post, I am trying to test running each of the 3 steps run separately on Google Cloud (instead of using `gcloud alpha genomics pipelines`). I have some notes on this [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:3441,performance,time,time,3441,", I think the examples of $0.20 per Exome (my own was <$1.00, but I think it was more than $0.20) and $2-3 per WGS (compared to $10+) may not be representative for somebody who is thinking about using the Cloud (versus local server + storage) for processing 100s or 1000s of samples (or even more, although I assume that would probably be for more than most individual labs or citizen scientists). I think 24 hour run-time was similar to running GATK on my local computer (with 8 GB of RAM and 4 cores), so Im not really complaining about the Cloud run-time that I encountered (I am just saying that the estimates provided on the README didnt match my own experience, even with an almost identical command on Google Cloud). **1b)** I realize that it would take some time (and Im not sure what would be the benefits versus other projects). However, have you considered allowing users to upload their run-time information (and estimated costs) to a program that might be able to help estimate run-time and cost (to possible help with topic **1a)**, **in the long-term**)? Since `gcp_deepvariant_runner` avoids the possibility of delays between running steps (and has an exist status depending upon whether variant calling was successful), perhaps some sort of optional reporting to an anonymized database could be provided as a parameter for that? **2)** While I realize it could be considered a cross-post, I am trying to test running each of the 3 steps run separately on Google Cloud (instead of using `gcloud alpha genomics pipelines`). I have some notes on this [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different whe",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:4888,performance,error,error,4888," 3 steps run separately on Google Cloud (instead of using `gcloud alpha genomics pipelines`). I have some notes on this [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WO",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:5048,performance,time,time,5048,"rflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:5253,performance,parallel,parallel,5253,"me more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error respo",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:5475,performance,CPU,CPU,5475,"versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled""",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:5674,performance,Parallel,Parallel,5674," Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:5751,performance,Parallel,Parallel,5751,"riant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Align",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:5965,performance,Parallel,Parallel,5965,"ge lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount s",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:6028,performance,parallel,parallel,6028,"mation... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:6062,performance,CPU,CPU,6062,"the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:6246,performance,Error,Error,6246,"ne. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alter",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:6274,performance,error,error,6274,"newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:6386,performance,time,time,6386,"e directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error mes",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:6420,performance,error,error,6420,"ission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:6431,performance,error,error,6431,"ed. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), but I only ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:6479,performance,parallel,parallel,6479,"gtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), but I only copied one representative example above, for the ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:6913,performance,Error,Error,6913,"ou pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), but I only copied one representative example above, for the repeated part. If I try to run the DeepVariant container in interactive mode (to try and understand what is going on), I get the following message (which is a note, without actually going into interactive mode):. ```. docker run -it -v /home/user/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant. See https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md. ```. I do have the `gcloud alpha ge",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:6941,performance,error,error,6941,"feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), but I only copied one representative example above, for the repeated part. If I try to run the DeepVariant container in interactive mode (to try and understand what is going on), I get the following message (which is a note, without actually going into interactive mode):. ```. docker run -it -v /home/user/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant. See https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md. ```. I do have the `gcloud alpha genomics pipelines` example wo",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:7053,performance,time,time,7053,"ters / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), but I only copied one representative example above, for the repeated part. If I try to run the DeepVariant container in interactive mode (to try and understand what is going on), I get the following message (which is a note, without actually going into interactive mode):. ```. docker run -it -v /home/user/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant. See https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md. ```. I do have the `gcloud alpha genomics pipelines` example working, so this isnt absolutely essential for running DeepVariant on Google Cloud. However, if you can help pro",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:7087,performance,error,error,7087," 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), but I only copied one representative example above, for the repeated part. If I try to run the DeepVariant container in interactive mode (to try and understand what is going on), I get the following message (which is a note, without actually going into interactive mode):. ```. docker run -it -v /home/user/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant. See https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md. ```. I do have the `gcloud alpha genomics pipelines` example working, so this isnt absolutely essential for running DeepVariant on Google Cloud. However, if you can help provide me some guidance for running t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:7098,performance,error,error,7098,"8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), but I only copied one representative example above, for the repeated part. If I try to run the DeepVariant container in interactive mode (to try and understand what is going on), I get the following message (which is a note, without actually going into interactive mode):. ```. docker run -it -v /home/user/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant. See https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md. ```. I do have the `gcloud alpha genomics pipelines` example working, so this isnt absolutely essential for running DeepVariant on Google Cloud. However, if you can help provide me some guidance for running the [linked ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:7334,performance,content,content,7334,"ile creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), but I only copied one representative example above, for the repeated part. If I try to run the DeepVariant container in interactive mode (to try and understand what is going on), I get the following message (which is a note, without actually going into interactive mode):. ```. docker run -it -v /home/user/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant. See https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md. ```. I do have the `gcloud alpha genomics pipelines` example working, so this isnt absolutely essential for running DeepVariant on Google Cloud. However, if you can help provide me some guidance for running the [linked script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud, I would very much appreciate it. Thank you very much,. Charles.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:7379,performance,error,error,7379,"ile creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), but I only copied one representative example above, for the repeated part. If I try to run the DeepVariant container in interactive mode (to try and understand what is going on), I get the following message (which is a note, without actually going into interactive mode):. ```. docker run -it -v /home/user/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant. See https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md. ```. I do have the `gcloud alpha genomics pipelines` example working, so this isnt absolutely essential for running DeepVariant on Google Cloud. However, if you can help provide me some guidance for running the [linked script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud, I would very much appreciate it. Thank you very much,. Charles.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:6498,reliability,fail,failed,6498,"2maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), but I only copied one representative example above, for the repeated part. If ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:609,safety,compl,complete,609,"Cloud Run-Time and Cost (including Docker on Google Cloud with Data in Bucket); Hi Again,. Thank you again for your help (on previous issues like #166 and #167). I have successfully run one Exome sample and one WGS sample (on AWS and Google Cloud). The WGS sample has ~250 million reads (paired-end, 150 bp x 150 bp), so it has ~25x coverage. Using the code based upon the [Google Cloud DeepVariant example]( https://cloud.google.com/genomics/docs/tutorials/deepvariant), I could process this 25X WGS samples in almost exactly 24 hours at a cost of approximately $10. On AWS, I probably should have kept more complete notes, but I believe it completed in ~18 hours (with a cost of approximately $15). This includes some cost of storage for the exome sample (and the WGS .fastq.gz files), although read storage would need to be taken into consideration if performing all analysis on the clould. Nevertheless, I previously reported [a much larger number]( https://github.com/google/deepvariant/issues/167#issuecomment-479522653) when I was learning how to use AWS, and these cost figures are now much more similar (as probably should be expected). One reason that I found running each step separately on AWS to be helpful was that I could figure out that I needed to add an extra parameter (`--sample_name VeritasProvided `) for the WGS dataset (for the provided alignment from Veritas) that wasnt necessary for the Exome dataset (for the provided alignment from Genos). Ive re-processed each sample locally, so I would also like to compare variant calls from a BWA-MEM alignment. Plus, I would like to make my comparison to AWS as fair as possible. So, here are my thoughts moving forward:. **1a)** I think it is good that you have changed the example WGS run time from [70 minutes]( https://github.com/google/deepvariant/blob/9d24133fc83e0423b3d5cf125a710bbefa864bbb/README.md) minutes to [5 hours](https://github.com/google/deepvariant/blob/r0.8/README.md), but this is still quite different than ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:642,safety,compl,completed,642,"Cloud Run-Time and Cost (including Docker on Google Cloud with Data in Bucket); Hi Again,. Thank you again for your help (on previous issues like #166 and #167). I have successfully run one Exome sample and one WGS sample (on AWS and Google Cloud). The WGS sample has ~250 million reads (paired-end, 150 bp x 150 bp), so it has ~25x coverage. Using the code based upon the [Google Cloud DeepVariant example]( https://cloud.google.com/genomics/docs/tutorials/deepvariant), I could process this 25X WGS samples in almost exactly 24 hours at a cost of approximately $10. On AWS, I probably should have kept more complete notes, but I believe it completed in ~18 hours (with a cost of approximately $15). This includes some cost of storage for the exome sample (and the WGS .fastq.gz files), although read storage would need to be taken into consideration if performing all analysis on the clould. Nevertheless, I previously reported [a much larger number]( https://github.com/google/deepvariant/issues/167#issuecomment-479522653) when I was learning how to use AWS, and these cost figures are now much more similar (as probably should be expected). One reason that I found running each step separately on AWS to be helpful was that I could figure out that I needed to add an extra parameter (`--sample_name VeritasProvided `) for the WGS dataset (for the provided alignment from Veritas) that wasnt necessary for the Exome dataset (for the provided alignment from Genos). Ive re-processed each sample locally, so I would also like to compare variant calls from a BWA-MEM alignment. Plus, I would like to make my comparison to AWS as fair as possible. So, here are my thoughts moving forward:. **1a)** I think it is good that you have changed the example WGS run time from [70 minutes]( https://github.com/google/deepvariant/blob/9d24133fc83e0423b3d5cf125a710bbefa864bbb/README.md) minutes to [5 hours](https://github.com/google/deepvariant/blob/r0.8/README.md), but this is still quite different than ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:2438,safety,test,test,2438,"ovided alignment from Genos). Ive re-processed each sample locally, so I would also like to compare variant calls from a BWA-MEM alignment. Plus, I would like to make my comparison to AWS as fair as possible. So, here are my thoughts moving forward:. **1a)** I think it is good that you have changed the example WGS run time from [70 minutes]( https://github.com/google/deepvariant/blob/9d24133fc83e0423b3d5cf125a710bbefa864bbb/README.md) minutes to [5 hours](https://github.com/google/deepvariant/blob/r0.8/README.md), but this is still quite different than my own experience (**24 hours**). I believe my upload times for my WGS datasets was also about a day. However, I also noticed that the upload from cloud servers to the precisionFDA was much quicker (so, maybe part of this is my apartment internet connection). To be fair, with the $300 credit, DeepVariant is essentially free to use on Google Cloud. However, if I was evaluating Cloud options in the long term (prior to conducting my own test), I think the examples of $0.20 per Exome (my own was <$1.00, but I think it was more than $0.20) and $2-3 per WGS (compared to $10+) may not be representative for somebody who is thinking about using the Cloud (versus local server + storage) for processing 100s or 1000s of samples (or even more, although I assume that would probably be for more than most individual labs or citizen scientists). I think 24 hour run-time was similar to running GATK on my local computer (with 8 GB of RAM and 4 cores), so Im not really complaining about the Cloud run-time that I encountered (I am just saying that the estimates provided on the README didnt match my own experience, even with an almost identical command on Google Cloud). **1b)** I realize that it would take some time (and Im not sure what would be the benefits versus other projects). However, have you considered allowing users to upload their run-time information (and estimated costs) to a program that might be able to help estimate run",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:2965,safety,compl,complaining,2965,"s still quite different than my own experience (**24 hours**). I believe my upload times for my WGS datasets was also about a day. However, I also noticed that the upload from cloud servers to the precisionFDA was much quicker (so, maybe part of this is my apartment internet connection). To be fair, with the $300 credit, DeepVariant is essentially free to use on Google Cloud. However, if I was evaluating Cloud options in the long term (prior to conducting my own test), I think the examples of $0.20 per Exome (my own was <$1.00, but I think it was more than $0.20) and $2-3 per WGS (compared to $10+) may not be representative for somebody who is thinking about using the Cloud (versus local server + storage) for processing 100s or 1000s of samples (or even more, although I assume that would probably be for more than most individual labs or citizen scientists). I think 24 hour run-time was similar to running GATK on my local computer (with 8 GB of RAM and 4 cores), so Im not really complaining about the Cloud run-time that I encountered (I am just saying that the estimates provided on the README didnt match my own experience, even with an almost identical command on Google Cloud). **1b)** I realize that it would take some time (and Im not sure what would be the benefits versus other projects). However, have you considered allowing users to upload their run-time information (and estimated costs) to a program that might be able to help estimate run-time and cost (to possible help with topic **1a)**, **in the long-term**)? Since `gcp_deepvariant_runner` avoids the possibility of delays between running steps (and has an exist status depending upon whether variant calling was successful), perhaps some sort of optional reporting to an anonymized database could be provided as a parameter for that? **2)** While I realize it could be considered a cross-post, I am trying to test running each of the 3 steps run separately on Google Cloud (instead of using `gcloud alpha genomics",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:3547,safety,avoid,avoids,3547,"per WGS (compared to $10+) may not be representative for somebody who is thinking about using the Cloud (versus local server + storage) for processing 100s or 1000s of samples (or even more, although I assume that would probably be for more than most individual labs or citizen scientists). I think 24 hour run-time was similar to running GATK on my local computer (with 8 GB of RAM and 4 cores), so Im not really complaining about the Cloud run-time that I encountered (I am just saying that the estimates provided on the README didnt match my own experience, even with an almost identical command on Google Cloud). **1b)** I realize that it would take some time (and Im not sure what would be the benefits versus other projects). However, have you considered allowing users to upload their run-time information (and estimated costs) to a program that might be able to help estimate run-time and cost (to possible help with topic **1a)**, **in the long-term**)? Since `gcp_deepvariant_runner` avoids the possibility of delays between running steps (and has an exist status depending upon whether variant calling was successful), perhaps some sort of optional reporting to an anonymized database could be provided as a parameter for that? **2)** While I realize it could be considered a cross-post, I am trying to test running each of the 3 steps run separately on Google Cloud (instead of using `gcloud alpha genomics pipelines`). I have some notes on this [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:3627,safety,depend,depending,3627,"about using the Cloud (versus local server + storage) for processing 100s or 1000s of samples (or even more, although I assume that would probably be for more than most individual labs or citizen scientists). I think 24 hour run-time was similar to running GATK on my local computer (with 8 GB of RAM and 4 cores), so Im not really complaining about the Cloud run-time that I encountered (I am just saying that the estimates provided on the README didnt match my own experience, even with an almost identical command on Google Cloud). **1b)** I realize that it would take some time (and Im not sure what would be the benefits versus other projects). However, have you considered allowing users to upload their run-time information (and estimated costs) to a program that might be able to help estimate run-time and cost (to possible help with topic **1a)**, **in the long-term**)? Since `gcp_deepvariant_runner` avoids the possibility of delays between running steps (and has an exist status depending upon whether variant calling was successful), perhaps some sort of optional reporting to an anonymized database could be provided as a parameter for that? **2)** While I realize it could be considered a cross-post, I am trying to test running each of the 3 steps run separately on Google Cloud (instead of using `gcloud alpha genomics pipelines`). I have some notes on this [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/ma",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:3867,safety,test,test,3867," similar to running GATK on my local computer (with 8 GB of RAM and 4 cores), so Im not really complaining about the Cloud run-time that I encountered (I am just saying that the estimates provided on the README didnt match my own experience, even with an almost identical command on Google Cloud). **1b)** I realize that it would take some time (and Im not sure what would be the benefits versus other projects). However, have you considered allowing users to upload their run-time information (and estimated costs) to a program that might be able to help estimate run-time and cost (to possible help with topic **1a)**, **in the long-term**)? Since `gcp_deepvariant_runner` avoids the possibility of delays between running steps (and has an exist status depending upon whether variant calling was successful), perhaps some sort of optional reporting to an anonymized database could be provided as a parameter for that? **2)** While I realize it could be considered a cross-post, I am trying to test running each of the 3 steps run separately on Google Cloud (instead of using `gcloud alpha genomics pipelines`). I have some notes on this [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:4265,safety,compl,complications,4265,"r projects). However, have you considered allowing users to upload their run-time information (and estimated costs) to a program that might be able to help estimate run-time and cost (to possible help with topic **1a)**, **in the long-term**)? Since `gcp_deepvariant_runner` avoids the possibility of delays between running steps (and has an exist status depending upon whether variant calling was successful), perhaps some sort of optional reporting to an anonymized database could be provided as a parameter for that? **2)** While I realize it could be considered a cross-post, I am trying to test running each of the 3 steps run separately on Google Cloud (instead of using `gcloud alpha genomics pipelines`). I have some notes on this [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:4888,safety,error,error,4888," 3 steps run separately on Google Cloud (instead of using `gcloud alpha genomics pipelines`). I have some notes on this [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WO",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:4996,safety,depend,dependency,4996,"s on this [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:5201,safety,depend,dependency,5201,"f Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:5419,safety,Permiss,Permission,5419,"t are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=erro",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:5793,safety,log,login,5793,"om/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --example",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:6135,safety,compl,completed,6135," and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context cance",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:6181,safety,compl,complete,6181," Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternat",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:6246,safety,Error,Error,6246,"ne. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alter",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:6274,safety,error,error,6274,"newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:6420,safety,error,error,6420,"ission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:6431,safety,error,error,6431,"ed. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), but I only ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:6913,safety,Error,Error,6913,"ou pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), but I only copied one representative example above, for the repeated part. If I try to run the DeepVariant container in interactive mode (to try and understand what is going on), I get the following message (which is a note, without actually going into interactive mode):. ```. docker run -it -v /home/user/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant. See https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md. ```. I do have the `gcloud alpha ge",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:6941,safety,error,error,6941,"feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), but I only copied one representative example above, for the repeated part. If I try to run the DeepVariant container in interactive mode (to try and understand what is going on), I get the following message (which is a note, without actually going into interactive mode):. ```. docker run -it -v /home/user/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant. See https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md. ```. I do have the `gcloud alpha genomics pipelines` example wo",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:7087,safety,error,error,7087," 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), but I only copied one representative example above, for the repeated part. If I try to run the DeepVariant container in interactive mode (to try and understand what is going on), I get the following message (which is a note, without actually going into interactive mode):. ```. docker run -it -v /home/user/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant. See https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md. ```. I do have the `gcloud alpha genomics pipelines` example working, so this isnt absolutely essential for running DeepVariant on Google Cloud. However, if you can help provide me some guidance for running t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:7098,safety,error,error,7098,"8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), but I only copied one representative example above, for the repeated part. If I try to run the DeepVariant container in interactive mode (to try and understand what is going on), I get the following message (which is a note, without actually going into interactive mode):. ```. docker run -it -v /home/user/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant. See https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md. ```. I do have the `gcloud alpha genomics pipelines` example working, so this isnt absolutely essential for running DeepVariant on Google Cloud. However, if you can help provide me some guidance for running the [linked ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:7209,safety,test,test,7209," 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), but I only copied one representative example above, for the repeated part. If I try to run the DeepVariant container in interactive mode (to try and understand what is going on), I get the following message (which is a note, without actually going into interactive mode):. ```. docker run -it -v /home/user/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant. See https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md. ```. I do have the `gcloud alpha genomics pipelines` example working, so this isnt absolutely essential for running DeepVariant on Google Cloud. However, if you can help provide me some guidance for running the [linked script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:7273,safety,test,test,7273,"rror while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), but I only copied one representative example above, for the repeated part. If I try to run the DeepVariant container in interactive mode (to try and understand what is going on), I get the following message (which is a note, without actually going into interactive mode):. ```. docker run -it -v /home/user/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant. See https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md. ```. I do have the `gcloud alpha genomics pipelines` example working, so this isnt absolutely essential for running DeepVariant on Google Cloud. However, if you can help provide me some guidance for running the [linked script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud, I would very much appreciate it. Thank you very much,. C",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:7379,safety,error,error,7379,"ile creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), but I only copied one representative example above, for the repeated part. If I try to run the DeepVariant container in interactive mode (to try and understand what is going on), I get the following message (which is a note, without actually going into interactive mode):. ```. docker run -it -v /home/user/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant. See https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md. ```. I do have the `gcloud alpha genomics pipelines` example working, so this isnt absolutely essential for running DeepVariant on Google Cloud. However, if you can help provide me some guidance for running the [linked script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud, I would very much appreciate it. Thank you very much,. Charles.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:609,security,compl,complete,609,"Cloud Run-Time and Cost (including Docker on Google Cloud with Data in Bucket); Hi Again,. Thank you again for your help (on previous issues like #166 and #167). I have successfully run one Exome sample and one WGS sample (on AWS and Google Cloud). The WGS sample has ~250 million reads (paired-end, 150 bp x 150 bp), so it has ~25x coverage. Using the code based upon the [Google Cloud DeepVariant example]( https://cloud.google.com/genomics/docs/tutorials/deepvariant), I could process this 25X WGS samples in almost exactly 24 hours at a cost of approximately $10. On AWS, I probably should have kept more complete notes, but I believe it completed in ~18 hours (with a cost of approximately $15). This includes some cost of storage for the exome sample (and the WGS .fastq.gz files), although read storage would need to be taken into consideration if performing all analysis on the clould. Nevertheless, I previously reported [a much larger number]( https://github.com/google/deepvariant/issues/167#issuecomment-479522653) when I was learning how to use AWS, and these cost figures are now much more similar (as probably should be expected). One reason that I found running each step separately on AWS to be helpful was that I could figure out that I needed to add an extra parameter (`--sample_name VeritasProvided `) for the WGS dataset (for the provided alignment from Veritas) that wasnt necessary for the Exome dataset (for the provided alignment from Genos). Ive re-processed each sample locally, so I would also like to compare variant calls from a BWA-MEM alignment. Plus, I would like to make my comparison to AWS as fair as possible. So, here are my thoughts moving forward:. **1a)** I think it is good that you have changed the example WGS run time from [70 minutes]( https://github.com/google/deepvariant/blob/9d24133fc83e0423b3d5cf125a710bbefa864bbb/README.md) minutes to [5 hours](https://github.com/google/deepvariant/blob/r0.8/README.md), but this is still quite different than ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:642,security,compl,completed,642,"Cloud Run-Time and Cost (including Docker on Google Cloud with Data in Bucket); Hi Again,. Thank you again for your help (on previous issues like #166 and #167). I have successfully run one Exome sample and one WGS sample (on AWS and Google Cloud). The WGS sample has ~250 million reads (paired-end, 150 bp x 150 bp), so it has ~25x coverage. Using the code based upon the [Google Cloud DeepVariant example]( https://cloud.google.com/genomics/docs/tutorials/deepvariant), I could process this 25X WGS samples in almost exactly 24 hours at a cost of approximately $10. On AWS, I probably should have kept more complete notes, but I believe it completed in ~18 hours (with a cost of approximately $15). This includes some cost of storage for the exome sample (and the WGS .fastq.gz files), although read storage would need to be taken into consideration if performing all analysis on the clould. Nevertheless, I previously reported [a much larger number]( https://github.com/google/deepvariant/issues/167#issuecomment-479522653) when I was learning how to use AWS, and these cost figures are now much more similar (as probably should be expected). One reason that I found running each step separately on AWS to be helpful was that I could figure out that I needed to add an extra parameter (`--sample_name VeritasProvided `) for the WGS dataset (for the provided alignment from Veritas) that wasnt necessary for the Exome dataset (for the provided alignment from Genos). Ive re-processed each sample locally, so I would also like to compare variant calls from a BWA-MEM alignment. Plus, I would like to make my comparison to AWS as fair as possible. So, here are my thoughts moving forward:. **1a)** I think it is good that you have changed the example WGS run time from [70 minutes]( https://github.com/google/deepvariant/blob/9d24133fc83e0423b3d5cf125a710bbefa864bbb/README.md) minutes to [5 hours](https://github.com/google/deepvariant/blob/r0.8/README.md), but this is still quite different than ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:2965,security,compl,complaining,2965,"s still quite different than my own experience (**24 hours**). I believe my upload times for my WGS datasets was also about a day. However, I also noticed that the upload from cloud servers to the precisionFDA was much quicker (so, maybe part of this is my apartment internet connection). To be fair, with the $300 credit, DeepVariant is essentially free to use on Google Cloud. However, if I was evaluating Cloud options in the long term (prior to conducting my own test), I think the examples of $0.20 per Exome (my own was <$1.00, but I think it was more than $0.20) and $2-3 per WGS (compared to $10+) may not be representative for somebody who is thinking about using the Cloud (versus local server + storage) for processing 100s or 1000s of samples (or even more, although I assume that would probably be for more than most individual labs or citizen scientists). I think 24 hour run-time was similar to running GATK on my local computer (with 8 GB of RAM and 4 cores), so Im not really complaining about the Cloud run-time that I encountered (I am just saying that the estimates provided on the README didnt match my own experience, even with an almost identical command on Google Cloud). **1b)** I realize that it would take some time (and Im not sure what would be the benefits versus other projects). However, have you considered allowing users to upload their run-time information (and estimated costs) to a program that might be able to help estimate run-time and cost (to possible help with topic **1a)**, **in the long-term**)? Since `gcp_deepvariant_runner` avoids the possibility of delays between running steps (and has an exist status depending upon whether variant calling was successful), perhaps some sort of optional reporting to an anonymized database could be provided as a parameter for that? **2)** While I realize it could be considered a cross-post, I am trying to test running each of the 3 steps run separately on Google Cloud (instead of using `gcloud alpha genomics",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:3133,security,ident,identical,3133,"oad from cloud servers to the precisionFDA was much quicker (so, maybe part of this is my apartment internet connection). To be fair, with the $300 credit, DeepVariant is essentially free to use on Google Cloud. However, if I was evaluating Cloud options in the long term (prior to conducting my own test), I think the examples of $0.20 per Exome (my own was <$1.00, but I think it was more than $0.20) and $2-3 per WGS (compared to $10+) may not be representative for somebody who is thinking about using the Cloud (versus local server + storage) for processing 100s or 1000s of samples (or even more, although I assume that would probably be for more than most individual labs or citizen scientists). I think 24 hour run-time was similar to running GATK on my local computer (with 8 GB of RAM and 4 cores), so Im not really complaining about the Cloud run-time that I encountered (I am just saying that the estimates provided on the README didnt match my own experience, even with an almost identical command on Google Cloud). **1b)** I realize that it would take some time (and Im not sure what would be the benefits versus other projects). However, have you considered allowing users to upload their run-time information (and estimated costs) to a program that might be able to help estimate run-time and cost (to possible help with topic **1a)**, **in the long-term**)? Since `gcp_deepvariant_runner` avoids the possibility of delays between running steps (and has an exist status depending upon whether variant calling was successful), perhaps some sort of optional reporting to an anonymized database could be provided as a parameter for that? **2)** While I realize it could be considered a cross-post, I am trying to test running each of the 3 steps run separately on Google Cloud (instead of using `gcloud alpha genomics pipelines`). I have some notes on this [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfus",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:4265,security,compl,complications,4265,"r projects). However, have you considered allowing users to upload their run-time information (and estimated costs) to a program that might be able to help estimate run-time and cost (to possible help with topic **1a)**, **in the long-term**)? Since `gcp_deepvariant_runner` avoids the possibility of delays between running steps (and has an exist status depending upon whether variant calling was successful), perhaps some sort of optional reporting to an anonymized database could be provided as a parameter for that? **2)** While I realize it could be considered a cross-post, I am trying to test running each of the 3 steps run separately on Google Cloud (instead of using `gcloud alpha genomics pipelines`). I have some notes on this [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:5766,security,Command-Lin,Command-Line,5766,"Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_real",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:5793,security,log,login,5793,"om/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --example",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:6135,security,compl,completed,6135," and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context cance",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:6181,security,compl,complete,6181," Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternat",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:333,testability,coverag,coverage,333,"Cloud Run-Time and Cost (including Docker on Google Cloud with Data in Bucket); Hi Again,. Thank you again for your help (on previous issues like #166 and #167). I have successfully run one Exome sample and one WGS sample (on AWS and Google Cloud). The WGS sample has ~250 million reads (paired-end, 150 bp x 150 bp), so it has ~25x coverage. Using the code based upon the [Google Cloud DeepVariant example]( https://cloud.google.com/genomics/docs/tutorials/deepvariant), I could process this 25X WGS samples in almost exactly 24 hours at a cost of approximately $10. On AWS, I probably should have kept more complete notes, but I believe it completed in ~18 hours (with a cost of approximately $15). This includes some cost of storage for the exome sample (and the WGS .fastq.gz files), although read storage would need to be taken into consideration if performing all analysis on the clould. Nevertheless, I previously reported [a much larger number]( https://github.com/google/deepvariant/issues/167#issuecomment-479522653) when I was learning how to use AWS, and these cost figures are now much more similar (as probably should be expected). One reason that I found running each step separately on AWS to be helpful was that I could figure out that I needed to add an extra parameter (`--sample_name VeritasProvided `) for the WGS dataset (for the provided alignment from Veritas) that wasnt necessary for the Exome dataset (for the provided alignment from Genos). Ive re-processed each sample locally, so I would also like to compare variant calls from a BWA-MEM alignment. Plus, I would like to make my comparison to AWS as fair as possible. So, here are my thoughts moving forward:. **1a)** I think it is good that you have changed the example WGS run time from [70 minutes]( https://github.com/google/deepvariant/blob/9d24133fc83e0423b3d5cf125a710bbefa864bbb/README.md) minutes to [5 hours](https://github.com/google/deepvariant/blob/r0.8/README.md), but this is still quite different than ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:2438,testability,test,test,2438,"ovided alignment from Genos). Ive re-processed each sample locally, so I would also like to compare variant calls from a BWA-MEM alignment. Plus, I would like to make my comparison to AWS as fair as possible. So, here are my thoughts moving forward:. **1a)** I think it is good that you have changed the example WGS run time from [70 minutes]( https://github.com/google/deepvariant/blob/9d24133fc83e0423b3d5cf125a710bbefa864bbb/README.md) minutes to [5 hours](https://github.com/google/deepvariant/blob/r0.8/README.md), but this is still quite different than my own experience (**24 hours**). I believe my upload times for my WGS datasets was also about a day. However, I also noticed that the upload from cloud servers to the precisionFDA was much quicker (so, maybe part of this is my apartment internet connection). To be fair, with the $300 credit, DeepVariant is essentially free to use on Google Cloud. However, if I was evaluating Cloud options in the long term (prior to conducting my own test), I think the examples of $0.20 per Exome (my own was <$1.00, but I think it was more than $0.20) and $2-3 per WGS (compared to $10+) may not be representative for somebody who is thinking about using the Cloud (versus local server + storage) for processing 100s or 1000s of samples (or even more, although I assume that would probably be for more than most individual labs or citizen scientists). I think 24 hour run-time was similar to running GATK on my local computer (with 8 GB of RAM and 4 cores), so Im not really complaining about the Cloud run-time that I encountered (I am just saying that the estimates provided on the README didnt match my own experience, even with an almost identical command on Google Cloud). **1b)** I realize that it would take some time (and Im not sure what would be the benefits versus other projects). However, have you considered allowing users to upload their run-time information (and estimated costs) to a program that might be able to help estimate run",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:3627,testability,depend,depending,3627,"about using the Cloud (versus local server + storage) for processing 100s or 1000s of samples (or even more, although I assume that would probably be for more than most individual labs or citizen scientists). I think 24 hour run-time was similar to running GATK on my local computer (with 8 GB of RAM and 4 cores), so Im not really complaining about the Cloud run-time that I encountered (I am just saying that the estimates provided on the README didnt match my own experience, even with an almost identical command on Google Cloud). **1b)** I realize that it would take some time (and Im not sure what would be the benefits versus other projects). However, have you considered allowing users to upload their run-time information (and estimated costs) to a program that might be able to help estimate run-time and cost (to possible help with topic **1a)**, **in the long-term**)? Since `gcp_deepvariant_runner` avoids the possibility of delays between running steps (and has an exist status depending upon whether variant calling was successful), perhaps some sort of optional reporting to an anonymized database could be provided as a parameter for that? **2)** While I realize it could be considered a cross-post, I am trying to test running each of the 3 steps run separately on Google Cloud (instead of using `gcloud alpha genomics pipelines`). I have some notes on this [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/ma",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:3867,testability,test,test,3867," similar to running GATK on my local computer (with 8 GB of RAM and 4 cores), so Im not really complaining about the Cloud run-time that I encountered (I am just saying that the estimates provided on the README didnt match my own experience, even with an almost identical command on Google Cloud). **1b)** I realize that it would take some time (and Im not sure what would be the benefits versus other projects). However, have you considered allowing users to upload their run-time information (and estimated costs) to a program that might be able to help estimate run-time and cost (to possible help with topic **1a)**, **in the long-term**)? Since `gcp_deepvariant_runner` avoids the possibility of delays between running steps (and has an exist status depending upon whether variant calling was successful), perhaps some sort of optional reporting to an anonymized database could be provided as a parameter for that? **2)** While I realize it could be considered a cross-post, I am trying to test running each of the 3 steps run separately on Google Cloud (instead of using `gcloud alpha genomics pipelines`). I have some notes on this [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:4996,testability,depend,dependency,4996,"s on this [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:5201,testability,depend,dependency,5201,"f Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:5793,testability,log,login,5793,"om/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --example",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:6460,testability,context,context,6460,"0elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), but I only copied one representative exam",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:7127,testability,context,context,7127,"obs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), but I only copied one representative example above, for the repeated part. If I try to run the DeepVariant container in interactive mode (to try and understand what is going on), I get the following message (which is a note, without actually going into interactive mode):. ```. docker run -it -v /home/user/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant. See https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md. ```. I do have the `gcloud alpha genomics pipelines` example working, so this isnt absolutely essential for running DeepVariant on Google Cloud. However, if you can help provide me some guidance for running the [linked script]( https://github.com/cw",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:7209,testability,test,test,7209," 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), but I only copied one representative example above, for the repeated part. If I try to run the DeepVariant container in interactive mode (to try and understand what is going on), I get the following message (which is a note, without actually going into interactive mode):. ```. docker run -it -v /home/user/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant. See https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md. ```. I do have the `gcloud alpha genomics pipelines` example working, so this isnt absolutely essential for running DeepVariant on Google Cloud. However, if you can help provide me some guidance for running the [linked script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:7273,testability,test,test,7273,"rror while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), but I only copied one representative example above, for the repeated part. If I try to run the DeepVariant container in interactive mode (to try and understand what is going on), I get the following message (which is a note, without actually going into interactive mode):. ```. docker run -it -v /home/user/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant. See https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md. ```. I do have the `gcloud alpha genomics pipelines` example working, so this isnt absolutely essential for running DeepVariant on Google Cloud. However, if you can help provide me some guidance for running the [linked script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud, I would very much appreciate it. Thank you very much,. C",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:7572,testability,understand,understand,7572,"ile creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), but I only copied one representative example above, for the repeated part. If I try to run the DeepVariant container in interactive mode (to try and understand what is going on), I get the following message (which is a note, without actually going into interactive mode):. ```. docker run -it -v /home/user/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant. See https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md. ```. I do have the `gcloud alpha genomics pipelines` example working, so this isnt absolutely essential for running DeepVariant on Google Cloud. However, if you can help provide me some guidance for running the [linked script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud, I would very much appreciate it. Thank you very much,. Charles.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:116,usability,help,help,116,"Cloud Run-Time and Cost (including Docker on Google Cloud with Data in Bucket); Hi Again,. Thank you again for your help (on previous issues like #166 and #167). I have successfully run one Exome sample and one WGS sample (on AWS and Google Cloud). The WGS sample has ~250 million reads (paired-end, 150 bp x 150 bp), so it has ~25x coverage. Using the code based upon the [Google Cloud DeepVariant example]( https://cloud.google.com/genomics/docs/tutorials/deepvariant), I could process this 25X WGS samples in almost exactly 24 hours at a cost of approximately $10. On AWS, I probably should have kept more complete notes, but I believe it completed in ~18 hours (with a cost of approximately $15). This includes some cost of storage for the exome sample (and the WGS .fastq.gz files), although read storage would need to be taken into consideration if performing all analysis on the clould. Nevertheless, I previously reported [a much larger number]( https://github.com/google/deepvariant/issues/167#issuecomment-479522653) when I was learning how to use AWS, and these cost figures are now much more similar (as probably should be expected). One reason that I found running each step separately on AWS to be helpful was that I could figure out that I needed to add an extra parameter (`--sample_name VeritasProvided `) for the WGS dataset (for the provided alignment from Veritas) that wasnt necessary for the Exome dataset (for the provided alignment from Genos). Ive re-processed each sample locally, so I would also like to compare variant calls from a BWA-MEM alignment. Plus, I would like to make my comparison to AWS as fair as possible. So, here are my thoughts moving forward:. **1a)** I think it is good that you have changed the example WGS run time from [70 minutes]( https://github.com/google/deepvariant/blob/9d24133fc83e0423b3d5cf125a710bbefa864bbb/README.md) minutes to [5 hours](https://github.com/google/deepvariant/blob/r0.8/README.md), but this is still quite different than ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:855,usability,perform,performing,855,"Cloud Run-Time and Cost (including Docker on Google Cloud with Data in Bucket); Hi Again,. Thank you again for your help (on previous issues like #166 and #167). I have successfully run one Exome sample and one WGS sample (on AWS and Google Cloud). The WGS sample has ~250 million reads (paired-end, 150 bp x 150 bp), so it has ~25x coverage. Using the code based upon the [Google Cloud DeepVariant example]( https://cloud.google.com/genomics/docs/tutorials/deepvariant), I could process this 25X WGS samples in almost exactly 24 hours at a cost of approximately $10. On AWS, I probably should have kept more complete notes, but I believe it completed in ~18 hours (with a cost of approximately $15). This includes some cost of storage for the exome sample (and the WGS .fastq.gz files), although read storage would need to be taken into consideration if performing all analysis on the clould. Nevertheless, I previously reported [a much larger number]( https://github.com/google/deepvariant/issues/167#issuecomment-479522653) when I was learning how to use AWS, and these cost figures are now much more similar (as probably should be expected). One reason that I found running each step separately on AWS to be helpful was that I could figure out that I needed to add an extra parameter (`--sample_name VeritasProvided `) for the WGS dataset (for the provided alignment from Veritas) that wasnt necessary for the Exome dataset (for the provided alignment from Genos). Ive re-processed each sample locally, so I would also like to compare variant calls from a BWA-MEM alignment. Plus, I would like to make my comparison to AWS as fair as possible. So, here are my thoughts moving forward:. **1a)** I think it is good that you have changed the example WGS run time from [70 minutes]( https://github.com/google/deepvariant/blob/9d24133fc83e0423b3d5cf125a710bbefa864bbb/README.md) minutes to [5 hours](https://github.com/google/deepvariant/blob/r0.8/README.md), but this is still quite different than ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:1038,usability,learn,learning,1038,"on Google Cloud with Data in Bucket); Hi Again,. Thank you again for your help (on previous issues like #166 and #167). I have successfully run one Exome sample and one WGS sample (on AWS and Google Cloud). The WGS sample has ~250 million reads (paired-end, 150 bp x 150 bp), so it has ~25x coverage. Using the code based upon the [Google Cloud DeepVariant example]( https://cloud.google.com/genomics/docs/tutorials/deepvariant), I could process this 25X WGS samples in almost exactly 24 hours at a cost of approximately $10. On AWS, I probably should have kept more complete notes, but I believe it completed in ~18 hours (with a cost of approximately $15). This includes some cost of storage for the exome sample (and the WGS .fastq.gz files), although read storage would need to be taken into consideration if performing all analysis on the clould. Nevertheless, I previously reported [a much larger number]( https://github.com/google/deepvariant/issues/167#issuecomment-479522653) when I was learning how to use AWS, and these cost figures are now much more similar (as probably should be expected). One reason that I found running each step separately on AWS to be helpful was that I could figure out that I needed to add an extra parameter (`--sample_name VeritasProvided `) for the WGS dataset (for the provided alignment from Veritas) that wasnt necessary for the Exome dataset (for the provided alignment from Genos). Ive re-processed each sample locally, so I would also like to compare variant calls from a BWA-MEM alignment. Plus, I would like to make my comparison to AWS as fair as possible. So, here are my thoughts moving forward:. **1a)** I think it is good that you have changed the example WGS run time from [70 minutes]( https://github.com/google/deepvariant/blob/9d24133fc83e0423b3d5cf125a710bbefa864bbb/README.md) minutes to [5 hours](https://github.com/google/deepvariant/blob/r0.8/README.md), but this is still quite different than my own experience (**24 hours**). I believ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:1212,usability,help,helpful,1212,"ample (on AWS and Google Cloud). The WGS sample has ~250 million reads (paired-end, 150 bp x 150 bp), so it has ~25x coverage. Using the code based upon the [Google Cloud DeepVariant example]( https://cloud.google.com/genomics/docs/tutorials/deepvariant), I could process this 25X WGS samples in almost exactly 24 hours at a cost of approximately $10. On AWS, I probably should have kept more complete notes, but I believe it completed in ~18 hours (with a cost of approximately $15). This includes some cost of storage for the exome sample (and the WGS .fastq.gz files), although read storage would need to be taken into consideration if performing all analysis on the clould. Nevertheless, I previously reported [a much larger number]( https://github.com/google/deepvariant/issues/167#issuecomment-479522653) when I was learning how to use AWS, and these cost figures are now much more similar (as probably should be expected). One reason that I found running each step separately on AWS to be helpful was that I could figure out that I needed to add an extra parameter (`--sample_name VeritasProvided `) for the WGS dataset (for the provided alignment from Veritas) that wasnt necessary for the Exome dataset (for the provided alignment from Genos). Ive re-processed each sample locally, so I would also like to compare variant calls from a BWA-MEM alignment. Plus, I would like to make my comparison to AWS as fair as possible. So, here are my thoughts moving forward:. **1a)** I think it is good that you have changed the example WGS run time from [70 minutes]( https://github.com/google/deepvariant/blob/9d24133fc83e0423b3d5cf125a710bbefa864bbb/README.md) minutes to [5 hours](https://github.com/google/deepvariant/blob/r0.8/README.md), but this is still quite different than my own experience (**24 hours**). I believe my upload times for my WGS datasets was also about a day. However, I also noticed that the upload from cloud servers to the precisionFDA was much quicker (so, maybe part of",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:2007,usability,experien,experience,2007,"ent-479522653) when I was learning how to use AWS, and these cost figures are now much more similar (as probably should be expected). One reason that I found running each step separately on AWS to be helpful was that I could figure out that I needed to add an extra parameter (`--sample_name VeritasProvided `) for the WGS dataset (for the provided alignment from Veritas) that wasnt necessary for the Exome dataset (for the provided alignment from Genos). Ive re-processed each sample locally, so I would also like to compare variant calls from a BWA-MEM alignment. Plus, I would like to make my comparison to AWS as fair as possible. So, here are my thoughts moving forward:. **1a)** I think it is good that you have changed the example WGS run time from [70 minutes]( https://github.com/google/deepvariant/blob/9d24133fc83e0423b3d5cf125a710bbefa864bbb/README.md) minutes to [5 hours](https://github.com/google/deepvariant/blob/r0.8/README.md), but this is still quite different than my own experience (**24 hours**). I believe my upload times for my WGS datasets was also about a day. However, I also noticed that the upload from cloud servers to the precisionFDA was much quicker (so, maybe part of this is my apartment internet connection). To be fair, with the $300 credit, DeepVariant is essentially free to use on Google Cloud. However, if I was evaluating Cloud options in the long term (prior to conducting my own test), I think the examples of $0.20 per Exome (my own was <$1.00, but I think it was more than $0.20) and $2-3 per WGS (compared to $10+) may not be representative for somebody who is thinking about using the Cloud (versus local server + storage) for processing 100s or 1000s of samples (or even more, although I assume that would probably be for more than most individual labs or citizen scientists). I think 24 hour run-time was similar to running GATK on my local computer (with 8 GB of RAM and 4 cores), so Im not really complaining about the Cloud run-time that I enc",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:3101,usability,experien,experience,3101,"ver, I also noticed that the upload from cloud servers to the precisionFDA was much quicker (so, maybe part of this is my apartment internet connection). To be fair, with the $300 credit, DeepVariant is essentially free to use on Google Cloud. However, if I was evaluating Cloud options in the long term (prior to conducting my own test), I think the examples of $0.20 per Exome (my own was <$1.00, but I think it was more than $0.20) and $2-3 per WGS (compared to $10+) may not be representative for somebody who is thinking about using the Cloud (versus local server + storage) for processing 100s or 1000s of samples (or even more, although I assume that would probably be for more than most individual labs or citizen scientists). I think 24 hour run-time was similar to running GATK on my local computer (with 8 GB of RAM and 4 cores), so Im not really complaining about the Cloud run-time that I encountered (I am just saying that the estimates provided on the README didnt match my own experience, even with an almost identical command on Google Cloud). **1b)** I realize that it would take some time (and Im not sure what would be the benefits versus other projects). However, have you considered allowing users to upload their run-time information (and estimated costs) to a program that might be able to help estimate run-time and cost (to possible help with topic **1a)**, **in the long-term**)? Since `gcp_deepvariant_runner` avoids the possibility of delays between running steps (and has an exist status depending upon whether variant calling was successful), perhaps some sort of optional reporting to an anonymized database could be provided as a parameter for that? **2)** While I realize it could be considered a cross-post, I am trying to test running each of the 3 steps run separately on Google Cloud (instead of using `gcloud alpha genomics pipelines`). I have some notes on this [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cl",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:3143,usability,command,command,3143,"cloud servers to the precisionFDA was much quicker (so, maybe part of this is my apartment internet connection). To be fair, with the $300 credit, DeepVariant is essentially free to use on Google Cloud. However, if I was evaluating Cloud options in the long term (prior to conducting my own test), I think the examples of $0.20 per Exome (my own was <$1.00, but I think it was more than $0.20) and $2-3 per WGS (compared to $10+) may not be representative for somebody who is thinking about using the Cloud (versus local server + storage) for processing 100s or 1000s of samples (or even more, although I assume that would probably be for more than most individual labs or citizen scientists). I think 24 hour run-time was similar to running GATK on my local computer (with 8 GB of RAM and 4 cores), so Im not really complaining about the Cloud run-time that I encountered (I am just saying that the estimates provided on the README didnt match my own experience, even with an almost identical command on Google Cloud). **1b)** I realize that it would take some time (and Im not sure what would be the benefits versus other projects). However, have you considered allowing users to upload their run-time information (and estimated costs) to a program that might be able to help estimate run-time and cost (to possible help with topic **1a)**, **in the long-term**)? Since `gcp_deepvariant_runner` avoids the possibility of delays between running steps (and has an exist status depending upon whether variant calling was successful), perhaps some sort of optional reporting to an anonymized database could be provided as a parameter for that? **2)** While I realize it could be considered a cross-post, I am trying to test running each of the 3 steps run separately on Google Cloud (instead of using `gcloud alpha genomics pipelines`). I have some notes on this [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:3323,usability,user,users,3323,"to use on Google Cloud. However, if I was evaluating Cloud options in the long term (prior to conducting my own test), I think the examples of $0.20 per Exome (my own was <$1.00, but I think it was more than $0.20) and $2-3 per WGS (compared to $10+) may not be representative for somebody who is thinking about using the Cloud (versus local server + storage) for processing 100s or 1000s of samples (or even more, although I assume that would probably be for more than most individual labs or citizen scientists). I think 24 hour run-time was similar to running GATK on my local computer (with 8 GB of RAM and 4 cores), so Im not really complaining about the Cloud run-time that I encountered (I am just saying that the estimates provided on the README didnt match my own experience, even with an almost identical command on Google Cloud). **1b)** I realize that it would take some time (and Im not sure what would be the benefits versus other projects). However, have you considered allowing users to upload their run-time information (and estimated costs) to a program that might be able to help estimate run-time and cost (to possible help with topic **1a)**, **in the long-term**)? Since `gcp_deepvariant_runner` avoids the possibility of delays between running steps (and has an exist status depending upon whether variant calling was successful), perhaps some sort of optional reporting to an anonymized database could be provided as a parameter for that? **2)** While I realize it could be considered a cross-post, I am trying to test running each of the 3 steps run separately on Google Cloud (instead of using `gcloud alpha genomics pipelines`). I have some notes on this [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:3423,usability,help,help,3423,"cting my own test), I think the examples of $0.20 per Exome (my own was <$1.00, but I think it was more than $0.20) and $2-3 per WGS (compared to $10+) may not be representative for somebody who is thinking about using the Cloud (versus local server + storage) for processing 100s or 1000s of samples (or even more, although I assume that would probably be for more than most individual labs or citizen scientists). I think 24 hour run-time was similar to running GATK on my local computer (with 8 GB of RAM and 4 cores), so Im not really complaining about the Cloud run-time that I encountered (I am just saying that the estimates provided on the README didnt match my own experience, even with an almost identical command on Google Cloud). **1b)** I realize that it would take some time (and Im not sure what would be the benefits versus other projects). However, have you considered allowing users to upload their run-time information (and estimated costs) to a program that might be able to help estimate run-time and cost (to possible help with topic **1a)**, **in the long-term**)? Since `gcp_deepvariant_runner` avoids the possibility of delays between running steps (and has an exist status depending upon whether variant calling was successful), perhaps some sort of optional reporting to an anonymized database could be provided as a parameter for that? **2)** While I realize it could be considered a cross-post, I am trying to test running each of the 3 steps run separately on Google Cloud (instead of using `gcloud alpha genomics pipelines`). I have some notes on this [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:3468,usability,help,help,3468,"0.20 per Exome (my own was <$1.00, but I think it was more than $0.20) and $2-3 per WGS (compared to $10+) may not be representative for somebody who is thinking about using the Cloud (versus local server + storage) for processing 100s or 1000s of samples (or even more, although I assume that would probably be for more than most individual labs or citizen scientists). I think 24 hour run-time was similar to running GATK on my local computer (with 8 GB of RAM and 4 cores), so Im not really complaining about the Cloud run-time that I encountered (I am just saying that the estimates provided on the README didnt match my own experience, even with an almost identical command on Google Cloud). **1b)** I realize that it would take some time (and Im not sure what would be the benefits versus other projects). However, have you considered allowing users to upload their run-time information (and estimated costs) to a program that might be able to help estimate run-time and cost (to possible help with topic **1a)**, **in the long-term**)? Since `gcp_deepvariant_runner` avoids the possibility of delays between running steps (and has an exist status depending upon whether variant calling was successful), perhaps some sort of optional reporting to an anonymized database could be provided as a parameter for that? **2)** While I realize it could be considered a cross-post, I am trying to test running each of the 3 steps run separately on Google Cloud (instead of using `gcloud alpha genomics pipelines`). I have some notes on this [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant con",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:3620,usability,statu,status,3620,"thinking about using the Cloud (versus local server + storage) for processing 100s or 1000s of samples (or even more, although I assume that would probably be for more than most individual labs or citizen scientists). I think 24 hour run-time was similar to running GATK on my local computer (with 8 GB of RAM and 4 cores), so Im not really complaining about the Cloud run-time that I encountered (I am just saying that the estimates provided on the README didnt match my own experience, even with an almost identical command on Google Cloud). **1b)** I realize that it would take some time (and Im not sure what would be the benefits versus other projects). However, have you considered allowing users to upload their run-time information (and estimated costs) to a program that might be able to help estimate run-time and cost (to possible help with topic **1a)**, **in the long-term**)? Since `gcp_deepvariant_runner` avoids the possibility of delays between running steps (and has an exist status depending upon whether variant calling was successful), perhaps some sort of optional reporting to an anonymized database could be provided as a parameter for that? **2)** While I realize it could be considered a cross-post, I am trying to test running each of the 3 steps run separately on Google Cloud (instead of using `gcloud alpha genomics pipelines`). I have some notes on this [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Script",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:4294,usability,learn,learn,4294,"e you considered allowing users to upload their run-time information (and estimated costs) to a program that might be able to help estimate run-time and cost (to possible help with topic **1a)**, **in the long-term**)? Since `gcp_deepvariant_runner` avoids the possibility of delays between running steps (and has an exist status depending upon whether variant calling was successful), perhaps some sort of optional reporting to an anonymized database could be provided as a parameter for that? **2)** While I realize it could be considered a cross-post, I am trying to test running each of the 3 steps run separately on Google Cloud (instead of using `gcloud alpha genomics pipelines`). I have some notes on this [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (2016",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:4740,usability,help,helpful,4740,"base could be provided as a parameter for that? **2)** While I realize it could be considered a cross-post, I am trying to test running each of the 3 steps run separately on Google Cloud (instead of using `gcloud alpha genomics pipelines`). I have some notes on this [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:4888,usability,error,error,4888," 3 steps run separately on Google Cloud (instead of using `gcloud alpha genomics pipelines`). I have some notes on this [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WO",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:5766,usability,Command,Command-Line,5766,"Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_real",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:5785,usability,Tool,Tool,5785,"/github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:5847,usability,help,helps,5847,"-case-study.md)), I get the following error message:. ```. $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh. Reading package lists... Done. Building dependency tree. Reading state information... Done. time is already the newest version (1.7-25.1+b1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. Reading package lists... Done. Building dependency tree. Reading state information... Done. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Al",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:6246,usability,Error,Error,6246,"ne. parallel is already the newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alter",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:6274,usability,error,error,6274,"newest version (20161222-1). 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded. mkdir: cannot create directory /mnt/cdw-genome: Permission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:6420,usability,error,error,6420,"ission denied. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:6431,usability,error,error,6431,"ed. 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), but I only ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:6468,usability,cancel,canceled,6468," 0%CPU (0avgtext+0avgdata 1692maxresident)k. 0inputs+0outputs (0major+73minor)pagefaults 0swaps. Academic tradition requires you to cite works you base your article on. When using programs that use GNU Parallel to process data for publication. please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,. ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT. If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), but I only copied one representative example abov",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:6913,usability,Error,Error,6913,"ou pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), but I only copied one representative example above, for the repeated part. If I try to run the DeepVariant container in interactive mode (to try and understand what is going on), I get the following message (which is a note, without actually going into interactive mode):. ```. docker run -it -v /home/user/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant. See https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md. ```. I do have the `gcloud alpha ge",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:6941,usability,error,error,6941,"feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run. 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), but I only copied one representative example above, for the repeated part. If I try to run the DeepVariant container in interactive mode (to try and understand what is going on), I get the following message (which is a note, without actually going into interactive mode):. ```. docker run -it -v /home/user/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant. See https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md. ```. I do have the `gcloud alpha genomics pipelines` example wo",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:7087,usability,error,error,7087," 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), but I only copied one representative example above, for the repeated part. If I try to run the DeepVariant container in interactive mode (to try and understand what is going on), I get the following message (which is a note, without actually going into interactive mode):. ```. docker run -it -v /home/user/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant. See https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md. ```. I do have the `gcloud alpha genomics pipelines` example working, so this isnt absolutely essential for running DeepVariant on Google Cloud. However, if you can help provide me some guidance for running t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:7098,usability,error,error,7098,"8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), but I only copied one representative example above, for the repeated part. If I try to run the DeepVariant container in interactive mode (to try and understand what is going on), I get the following message (which is a note, without actually going into interactive mode):. ```. docker run -it -v /home/user/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant. See https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md. ```. I do have the `gcloud alpha genomics pipelines` example working, so this isnt absolutely essential for running DeepVariant on Google Cloud. However, if you can help provide me some guidance for running the [linked ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:7135,usability,cancel,canceled,7135,"leted/%of started jobs/Average seconds to complete. ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), but I only copied one representative example above, for the repeated part. If I try to run the DeepVariant container in interactive mode (to try and understand what is going on), I get the following message (which is a note, without actually going into interactive mode):. ```. docker run -it -v /home/user/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant. See https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md. ```. I do have the `gcloud alpha genomics pipelines` example working, so this isnt absolutely essential for running DeepVariant on Google Cloud. However, if you can help provide me some guidance for running the [linked script]( https://github.com/cwarden45/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:7379,usability,error,error,7379,"ile creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), but I only copied one representative example above, for the repeated part. If I try to run the DeepVariant container in interactive mode (to try and understand what is going on), I get the following message (which is a note, without actually going into interactive mode):. ```. docker run -it -v /home/user/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant. See https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md. ```. I do have the `gcloud alpha genomics pipelines` example working, so this isnt absolutely essential for running DeepVariant on Google Cloud. However, if you can help provide me some guidance for running the [linked script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud, I would very much appreciate it. Thank you very much,. Charles.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:7543,usability,interact,interactive,7543,"ile creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), but I only copied one representative example above, for the repeated part. If I try to run the DeepVariant container in interactive mode (to try and understand what is going on), I get the following message (which is a note, without actually going into interactive mode):. ```. docker run -it -v /home/user/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant. See https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md. ```. I do have the `gcloud alpha genomics pipelines` example working, so this isnt absolutely essential for running DeepVariant on Google Cloud. However, if you can help provide me some guidance for running the [linked script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud, I would very much appreciate it. Thank you very much,. Charles.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:7676,usability,interact,interactive,7676,"ile creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), but I only copied one representative example above, for the repeated part. If I try to run the DeepVariant container in interactive mode (to try and understand what is going on), I get the following message (which is a note, without actually going into interactive mode):. ```. docker run -it -v /home/user/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant. See https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md. ```. I do have the `gcloud alpha genomics pipelines` example working, so this isnt absolutely essential for running DeepVariant on Google Cloud. However, if you can help provide me some guidance for running the [linked script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud, I would very much appreciate it. Thank you very much,. Charles.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:7725,usability,user,user,7725,"ile creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), but I only copied one representative example above, for the repeated part. If I try to run the DeepVariant container in interactive mode (to try and understand what is going on), I get the following message (which is a note, without actually going into interactive mode):. ```. docker run -it -v /home/user/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant. See https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md. ```. I do have the `gcloud alpha genomics pipelines` example working, so this isnt absolutely essential for running DeepVariant on Google Cloud. However, if you can help provide me some guidance for running the [linked script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud, I would very much appreciate it. Thank you very much,. Charles.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:8047,usability,help,help,8047,"ile creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), but I only copied one representative example above, for the repeated part. If I try to run the DeepVariant container in interactive mode (to try and understand what is going on), I get the following message (which is a note, without actually going into interactive mode):. ```. docker run -it -v /home/user/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant. See https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md. ```. I do have the `gcloud alpha genomics pipelines` example working, so this isnt absolutely essential for running DeepVariant on Google Cloud. However, if you can help provide me some guidance for running the [linked script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud, I would very much appreciate it. Thank you very much,. Charles.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/171:8068,usability,guidanc,guidance,8068,"ile creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled"". parallel: This job failed:. docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0. docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists. time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled"". ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), but I only copied one representative example above, for the repeated part. If I try to run the DeepVariant container in interactive mode (to try and understand what is going on), I get the following message (which is a note, without actually going into interactive mode):. ```. docker run -it -v /home/user/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant. See https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md. ```. I do have the `gcloud alpha genomics pipelines` example working, so this isnt absolutely essential for running DeepVariant on Google Cloud. However, if you can help provide me some guidance for running the [linked script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud, I would very much appreciate it. Thank you very much,. Charles.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/171
https://github.com/google/deepvariant/issues/172:426,availability,error,error,426,"ValueError: Must specify steps > 0, given: 0 deepvariant version: 0.8; Hi, I am new to deepvariant. I tried to train my deepvariant model with our own data within docker on my personal computer. Up until model_eval and model_train, every process was working fine. When I try to run model_train and model_eval in my computer, model_train seemed to work but model_eval returned ValueError: Must specify steps > 0, given: 0. The error came from estimator.py file in tensorflow. In Advanced Case Study: Train a customized SNP and small indel variant caller for BGISEQ-500 data, it says that . > At the same time, start model_eval on CPUs. Since I don't have a TPU, so the following is the code I used and attempt to run model_train and model_eval on CPU simultaneously. The following is the code I used:. `(time python /home/bin/model_train.zip \. --dataset_config_pbtxt=/data/output/training_data/customized_training/training_set_with_label_shuffled/training_set.dataset_config.pbtxt \. --train_dir=/data/output/trained_model \. --model_name=""inception_v3"" \. --number_of_steps=10 \. --save_interval_secs=3000 \. --batch_size=32 \. --learning_rate=0.008 \. --start_from_checkpoint=/home/models/model.ckpt) >/data/output/log/model_training/model_train.log 2>&1\. & (time python2 /home/bin/model_eval.zip \. --dataset_config_pbtxt=/data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.dataset_config.pbtxt \. --checkpoint_dir=/data/output/trained_model \. --number_of_steps=10 \. --batch_size=32) >/data/output/log/model_training/model_eval.log 2>&1`. The following is the message from model_eval log :. > I0415 07:34:19.493486 140713377441536 model_eval.py:141] Set KMP_BLOCKTIME to 0. I0415 07:34:19.495834 140713377441536 model_eval.py:177] Running fixed eval for: /data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.dataset_config.pbtxt. W0415 07:34:19.536698 140713377441536 deprecation.py:323] From /tmp/Bazel.r",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:3261,availability,Cluster,ClusterSpec,3261,"a.TFRecordDataset(path)`. I0415 07:34:19.584646 140713377441536 model_eval.py:190] Running evaluations on DeepVariantInput(name=HG001, input_file_spec=/data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz, num_examples=8, mode=eval with model DeepVariantModel(name=inception_v3). I0415 07:34:19.585236 140713377441536 model_eval.py:198] Dataset has 8 samples, doing eval over 0; max_examples is 1000000, num_batches is 0. I0415 07:34:19.585772 140713377441536 modeling.py:357] Initializing model with random parameters. I0415 07:34:19.586724 140713377441536 estimator.py:201] Using config: {'_save_checkpoints_secs': 1000, '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ffa3b29ad50>, '_model_dir': '/data/output/trained_model', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_cluster': 0, '_master': ''}. I0415 07:34:19.587389 140713377441536 evaluation.py:189] Waiting for new checkpoint at /data/output/trained_model. I0415 07:35:14.785435 140713377441536 evaluation.py:198] Found new checkpoint at /data/output/trained_model/model.ckpt-0. I0415 07:35:14.787266 140713377441536 model_eval.py:225] Starting to evaluate. WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0. For more information, please see:. * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.m",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:3826,availability,checkpoint,checkpoint,3826,"7] Initializing model with random parameters. I0415 07:34:19.586724 140713377441536 estimator.py:201] Using config: {'_save_checkpoints_secs': 1000, '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ffa3b29ad50>, '_model_dir': '/data/output/trained_model', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_cluster': 0, '_master': ''}. I0415 07:34:19.587389 140713377441536 evaluation.py:189] Waiting for new checkpoint at /data/output/trained_model. I0415 07:35:14.785435 140713377441536 evaluation.py:198] Found new checkpoint at /data/output/trained_model/model.ckpt-0. I0415 07:35:14.787266 140713377441536 model_eval.py:225] Starting to evaluate. WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0. For more information, please see:. * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md. * https://github.com/tensorflow/addons. If you depend on functionality not listed there, please file an issue. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/deepvariant/model_eval.py"", line 362, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/deepvariant/model_eval.py"", line 154, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:3935,availability,checkpoint,checkpoint,3935,"onfig: {'_save_checkpoints_secs': 1000, '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ffa3b29ad50>, '_model_dir': '/data/output/trained_model', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_cluster': 0, '_master': ''}. I0415 07:34:19.587389 140713377441536 evaluation.py:189] Waiting for new checkpoint at /data/output/trained_model. I0415 07:35:14.785435 140713377441536 evaluation.py:198] Found new checkpoint at /data/output/trained_model/model.ckpt-0. I0415 07:35:14.787266 140713377441536 model_eval.py:225] Starting to evaluate. WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0. For more information, please see:. * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md. * https://github.com/tensorflow/addons. If you depend on functionality not listed there, please file an issue. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/deepvariant/model_eval.py"", line 362, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/deepvariant/model_eval.py"", line 154, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/deepvariant/model_eval.py"", line 235, in eval_loop. name=eval_name). F",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:7269,availability,checkpoint,checkpoint,7269,"628 140368878327552 model_train.py:310] Set KMP_BLOCKTIME to 0. I0415 07:34:19.469649 140368878327552 model_train.py:244] TF_CONFIG None. W0415 07:34:19.491456 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py:307: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version. Instructions for updating:. Use eager execution and: . `tf.data.TFRecordDataset(path)`. I0415 07:34:19.549700 140368878327552 model_train.py:193] Running training on DeepVariantInput(name=HG001, input_file_spec=/data/output/training_data/customized_training/training_set_with_label_shuffled/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz, num_examples=33, mode=train with model inception_v3 and tpu False. I0415 07:34:19.550825 140368878327552 model_train.py:196] Batches per epoch 1. I0415 07:34:19.551630 140368878327552 modeling.py:330] Initializing model from checkpoint at /home/models/model.ckpt. I0415 07:34:19.564393 140368878327552 modeling.py:336] The model checkpoint to warm start from has the same number of classes. If this is in training, we will clear excluded_scopes_for_incompatible_shapes so we include everything for warm starting.... I0415 07:34:19.568434 140368878327552 estimator.py:201] Using config: {'_save_checkpoints_secs': 3000, '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7faa056a5210>, '_model_dir': '/data/output/trained_model', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:7373,availability,checkpoint,checkpoint,7373,"del_train.py:244] TF_CONFIG None. W0415 07:34:19.491456 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py:307: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version. Instructions for updating:. Use eager execution and: . `tf.data.TFRecordDataset(path)`. I0415 07:34:19.549700 140368878327552 model_train.py:193] Running training on DeepVariantInput(name=HG001, input_file_spec=/data/output/training_data/customized_training/training_set_with_label_shuffled/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz, num_examples=33, mode=train with model inception_v3 and tpu False. I0415 07:34:19.550825 140368878327552 model_train.py:196] Batches per epoch 1. I0415 07:34:19.551630 140368878327552 modeling.py:330] Initializing model from checkpoint at /home/models/model.ckpt. I0415 07:34:19.564393 140368878327552 modeling.py:336] The model checkpoint to warm start from has the same number of classes. If this is in training, we will clear excluded_scopes_for_incompatible_shapes so we include everything for warm starting.... I0415 07:34:19.568434 140368878327552 estimator.py:201] Using config: {'_save_checkpoints_secs': 3000, '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7faa056a5210>, '_model_dir': '/data/output/trained_model', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distr",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:7944,availability,Cluster,ClusterSpec,7944,"training_set_with_label_shuffled/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz, num_examples=33, mode=train with model inception_v3 and tpu False. I0415 07:34:19.550825 140368878327552 model_train.py:196] Batches per epoch 1. I0415 07:34:19.551630 140368878327552 modeling.py:330] Initializing model from checkpoint at /home/models/model.ckpt. I0415 07:34:19.564393 140368878327552 modeling.py:336] The model checkpoint to warm start from has the same number of classes. If this is in training, we will clear excluded_scopes_for_incompatible_shapes so we include everything for warm starting.... I0415 07:34:19.568434 140368878327552 estimator.py:201] Using config: {'_save_checkpoints_secs': 3000, '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7faa056a5210>, '_model_dir': '/data/output/trained_model', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_cluster': 0, '_master': ''}. W0415 07:34:19.583559 140368878327552 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version. Instructions for updating:. Colocations handled automatically by placer. W0415 07:34:19.617336 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/deepvariant/data_providers.py:286: parallel_interleave (fr",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:9816,availability,operat,operator,9816,"py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/deepvariant/data_providers.py:286: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.experimental.parallel_interleave(...)`. I0415 07:34:19.674496 140368878327552 estimator.py:1111] Calling model_fn. W0415 07:34:19.675647 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/deepvariant/modeling.py:669: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use tf.cast instead. W0415 07:34:19.682488 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/deepvariant/modeling.py:671: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. W0415 07:34:23.556665 140368878327552 deprecation.py:506] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version. Instructions for updating:. Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`. I0415 07:34:37.681962 140368878327552 estimator.py:1113] Done calling model_fn. I0415 07:34:37.683280 140368878327552 estimator.py:1294] Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from='/home/models/model.ckpt', vars_to_warm_start='InceptionV3/Mixed_6d/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_6d/Branch_1/Conv2d_0a_1x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_6d/Branch_1/Conv2d_0c_7x1/BatchNorm/beta/RMSProp|InceptionV3/Mixed_5d/Branch_2/Conv2d_0a_1x1/weights|InceptionV3/Mixed_7b/Branch_2/Conv2d_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:122549,availability,Checkpoint,CheckpointSaverHook,122549,"x7/weights; prev_var_name: Unchanged. I0415 07:34:38.065579 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged. I0415 07:34:38.065984 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged. I0415 07:34:38.066359 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged. I0415 07:34:38.066709 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged. I0415 07:34:38.067050 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/BatchNorm/beta; prev_var_name: Unchanged. I0415 07:34:38.657821 140368878327552 basic_session_run_hooks.py:527] Create CheckpointSaverHook. I0415 07:34:45.316857 140368878327552 monitored_session.py:222] Graph was finalized. 2019-04-15 07:34:45.317978: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2019-04-15 07:34:45.322541: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2304000000 Hz. 2019-04-15 07:34:45.323247: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x175ebd50 executing computations on platform Host. Devices:. 2019-04-15 07:34:45.323718: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. I0415 07:34:52.317267 140368878327552 session_manager.py:491] Running local_init_op. I0415 07:34:52.780421 140368878327552 session_manager.py:493] Done running local_init_op. I0415 07:35:11.098021 140368878327552 basic_session_run_hooks.py:594] Saving checkpoints for 0 into /data/output/trained_model/model.ckpt. 2019-04-15 07:35",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:123000,availability,servic,service,123000,"py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged. I0415 07:34:38.066709 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged. I0415 07:34:38.067050 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/BatchNorm/beta; prev_var_name: Unchanged. I0415 07:34:38.657821 140368878327552 basic_session_run_hooks.py:527] Create CheckpointSaverHook. I0415 07:34:45.316857 140368878327552 monitored_session.py:222] Graph was finalized. 2019-04-15 07:34:45.317978: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2019-04-15 07:34:45.322541: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2304000000 Hz. 2019-04-15 07:34:45.323247: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x175ebd50 executing computations on platform Host. Devices:. 2019-04-15 07:34:45.323718: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. I0415 07:34:52.317267 140368878327552 session_manager.py:491] Running local_init_op. I0415 07:34:52.780421 140368878327552 session_manager.py:493] Done running local_init_op. I0415 07:35:11.098021 140368878327552 basic_session_run_hooks.py:594] Saving checkpoints for 0 into /data/output/trained_model/model.ckpt. 2019-04-15 07:35:25.684776: W tensorflow/core/framework/allocator.cc:124] Allocation of 16972800 exceeds 10% of system memory. 2019-04-15 07:35:25.703962: W tensorflow/core/framework/allocator.cc:124] Allocation of 22077440 exceeds 10% of system memory. 2019-04-15 07:35:25.779693: W tensorflow/core/framework/allocator.cc:124] Allocation of 22077440 exceeds 10% of system memory. 2019-04-15 07:35:25.836485: W tensorflow/core/framework/allocator.cc:124] Alloca",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:123008,availability,servic,service,123008,"Warm-starting variable: InceptionV3/Mixed_5d/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged. I0415 07:34:38.066709 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged. I0415 07:34:38.067050 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/BatchNorm/beta; prev_var_name: Unchanged. I0415 07:34:38.657821 140368878327552 basic_session_run_hooks.py:527] Create CheckpointSaverHook. I0415 07:34:45.316857 140368878327552 monitored_session.py:222] Graph was finalized. 2019-04-15 07:34:45.317978: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2019-04-15 07:34:45.322541: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2304000000 Hz. 2019-04-15 07:34:45.323247: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x175ebd50 executing computations on platform Host. Devices:. 2019-04-15 07:34:45.323718: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. I0415 07:34:52.317267 140368878327552 session_manager.py:491] Running local_init_op. I0415 07:34:52.780421 140368878327552 session_manager.py:493] Done running local_init_op. I0415 07:35:11.098021 140368878327552 basic_session_run_hooks.py:594] Saving checkpoints for 0 into /data/output/trained_model/model.ckpt. 2019-04-15 07:35:25.684776: W tensorflow/core/framework/allocator.cc:124] Allocation of 16972800 exceeds 10% of system memory. 2019-04-15 07:35:25.703962: W tensorflow/core/framework/allocator.cc:124] Allocation of 22077440 exceeds 10% of system memory. 2019-04-15 07:35:25.779693: W tensorflow/core/framework/allocator.cc:124] Allocation of 22077440 exceeds 10% of system memory. 2019-04-15 07:35:25.836485: W tensorflow/core/framework/allocator.cc:124] Allocation of ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:123028,availability,servic,service,123028,"le: InceptionV3/Mixed_5d/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged. I0415 07:34:38.066709 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged. I0415 07:34:38.067050 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/BatchNorm/beta; prev_var_name: Unchanged. I0415 07:34:38.657821 140368878327552 basic_session_run_hooks.py:527] Create CheckpointSaverHook. I0415 07:34:45.316857 140368878327552 monitored_session.py:222] Graph was finalized. 2019-04-15 07:34:45.317978: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2019-04-15 07:34:45.322541: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2304000000 Hz. 2019-04-15 07:34:45.323247: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x175ebd50 executing computations on platform Host. Devices:. 2019-04-15 07:34:45.323718: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. I0415 07:34:52.317267 140368878327552 session_manager.py:491] Running local_init_op. I0415 07:34:52.780421 140368878327552 session_manager.py:493] Done running local_init_op. I0415 07:35:11.098021 140368878327552 basic_session_run_hooks.py:594] Saving checkpoints for 0 into /data/output/trained_model/model.ckpt. 2019-04-15 07:35:25.684776: W tensorflow/core/framework/allocator.cc:124] Allocation of 16972800 exceeds 10% of system memory. 2019-04-15 07:35:25.703962: W tensorflow/core/framework/allocator.cc:124] Allocation of 22077440 exceeds 10% of system memory. 2019-04-15 07:35:25.779693: W tensorflow/core/framework/allocator.cc:124] Allocation of 22077440 exceeds 10% of system memory. 2019-04-15 07:35:25.836485: W tensorflow/core/framework/allocator.cc:124] Allocation of 20791296 exceeds 10%",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:123152,availability,servic,service,123152,"552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged. I0415 07:34:38.067050 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/BatchNorm/beta; prev_var_name: Unchanged. I0415 07:34:38.657821 140368878327552 basic_session_run_hooks.py:527] Create CheckpointSaverHook. I0415 07:34:45.316857 140368878327552 monitored_session.py:222] Graph was finalized. 2019-04-15 07:34:45.317978: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2019-04-15 07:34:45.322541: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2304000000 Hz. 2019-04-15 07:34:45.323247: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x175ebd50 executing computations on platform Host. Devices:. 2019-04-15 07:34:45.323718: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. I0415 07:34:52.317267 140368878327552 session_manager.py:491] Running local_init_op. I0415 07:34:52.780421 140368878327552 session_manager.py:493] Done running local_init_op. I0415 07:35:11.098021 140368878327552 basic_session_run_hooks.py:594] Saving checkpoints for 0 into /data/output/trained_model/model.ckpt. 2019-04-15 07:35:25.684776: W tensorflow/core/framework/allocator.cc:124] Allocation of 16972800 exceeds 10% of system memory. 2019-04-15 07:35:25.703962: W tensorflow/core/framework/allocator.cc:124] Allocation of 22077440 exceeds 10% of system memory. 2019-04-15 07:35:25.779693: W tensorflow/core/framework/allocator.cc:124] Allocation of 22077440 exceeds 10% of system memory. 2019-04-15 07:35:25.836485: W tensorflow/core/framework/allocator.cc:124] Allocation of 20791296 exceeds 10% of system memory. 2019-04-15 07:35:26.261185: W tensorflow/core/framework/allocator.cc:124] Allocation of 20791296 exceeds ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:123160,availability,servic,service,123160,"_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged. I0415 07:34:38.067050 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/BatchNorm/beta; prev_var_name: Unchanged. I0415 07:34:38.657821 140368878327552 basic_session_run_hooks.py:527] Create CheckpointSaverHook. I0415 07:34:45.316857 140368878327552 monitored_session.py:222] Graph was finalized. 2019-04-15 07:34:45.317978: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2019-04-15 07:34:45.322541: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2304000000 Hz. 2019-04-15 07:34:45.323247: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x175ebd50 executing computations on platform Host. Devices:. 2019-04-15 07:34:45.323718: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. I0415 07:34:52.317267 140368878327552 session_manager.py:491] Running local_init_op. I0415 07:34:52.780421 140368878327552 session_manager.py:493] Done running local_init_op. I0415 07:35:11.098021 140368878327552 basic_session_run_hooks.py:594] Saving checkpoints for 0 into /data/output/trained_model/model.ckpt. 2019-04-15 07:35:25.684776: W tensorflow/core/framework/allocator.cc:124] Allocation of 16972800 exceeds 10% of system memory. 2019-04-15 07:35:25.703962: W tensorflow/core/framework/allocator.cc:124] Allocation of 22077440 exceeds 10% of system memory. 2019-04-15 07:35:25.779693: W tensorflow/core/framework/allocator.cc:124] Allocation of 22077440 exceeds 10% of system memory. 2019-04-15 07:35:25.836485: W tensorflow/core/framework/allocator.cc:124] Allocation of 20791296 exceeds 10% of system memory. 2019-04-15 07:35:26.261185: W tensorflow/core/framework/allocator.cc:124] Allocation of 20791296 exceeds 10% of s",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:123481,availability,checkpoint,checkpoints,123481,"657821 140368878327552 basic_session_run_hooks.py:527] Create CheckpointSaverHook. I0415 07:34:45.316857 140368878327552 monitored_session.py:222] Graph was finalized. 2019-04-15 07:34:45.317978: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2019-04-15 07:34:45.322541: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2304000000 Hz. 2019-04-15 07:34:45.323247: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x175ebd50 executing computations on platform Host. Devices:. 2019-04-15 07:34:45.323718: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. I0415 07:34:52.317267 140368878327552 session_manager.py:491] Running local_init_op. I0415 07:34:52.780421 140368878327552 session_manager.py:493] Done running local_init_op. I0415 07:35:11.098021 140368878327552 basic_session_run_hooks.py:594] Saving checkpoints for 0 into /data/output/trained_model/model.ckpt. 2019-04-15 07:35:25.684776: W tensorflow/core/framework/allocator.cc:124] Allocation of 16972800 exceeds 10% of system memory. 2019-04-15 07:35:25.703962: W tensorflow/core/framework/allocator.cc:124] Allocation of 22077440 exceeds 10% of system memory. 2019-04-15 07:35:25.779693: W tensorflow/core/framework/allocator.cc:124] Allocation of 22077440 exceeds 10% of system memory. 2019-04-15 07:35:25.836485: W tensorflow/core/framework/allocator.cc:124] Allocation of 20791296 exceeds 10% of system memory. 2019-04-15 07:35:26.261185: W tensorflow/core/framework/allocator.cc:124] Allocation of 20791296 exceeds 10% of system memory. I0415 07:35:36.190104 140368878327552 basic_session_run_hooks.py:249] loss = 0.039415985, step = 1. I0415 07:36:50.684401 140368878327552 basic_session_run_hooks.py:594] Saving checkpoints for 10 into /data/output/trained_model/model.ckpt. I0415 07:37:20.374263 140368878327552 estimator.py:359] Loss for fina",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:124355,availability,checkpoint,checkpoints,124355,"flow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2304000000 Hz. 2019-04-15 07:34:45.323247: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x175ebd50 executing computations on platform Host. Devices:. 2019-04-15 07:34:45.323718: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. I0415 07:34:52.317267 140368878327552 session_manager.py:491] Running local_init_op. I0415 07:34:52.780421 140368878327552 session_manager.py:493] Done running local_init_op. I0415 07:35:11.098021 140368878327552 basic_session_run_hooks.py:594] Saving checkpoints for 0 into /data/output/trained_model/model.ckpt. 2019-04-15 07:35:25.684776: W tensorflow/core/framework/allocator.cc:124] Allocation of 16972800 exceeds 10% of system memory. 2019-04-15 07:35:25.703962: W tensorflow/core/framework/allocator.cc:124] Allocation of 22077440 exceeds 10% of system memory. 2019-04-15 07:35:25.779693: W tensorflow/core/framework/allocator.cc:124] Allocation of 22077440 exceeds 10% of system memory. 2019-04-15 07:35:25.836485: W tensorflow/core/framework/allocator.cc:124] Allocation of 20791296 exceeds 10% of system memory. 2019-04-15 07:35:26.261185: W tensorflow/core/framework/allocator.cc:124] Allocation of 20791296 exceeds 10% of system memory. I0415 07:35:36.190104 140368878327552 basic_session_run_hooks.py:249] loss = 0.039415985, step = 1. I0415 07:36:50.684401 140368878327552 basic_session_run_hooks.py:594] Saving checkpoints for 10 into /data/output/trained_model/model.ckpt. I0415 07:37:20.374263 140368878327552 estimator.py:359] Loss for final step: 0.0037548377. WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0. For more information, please see:. * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md. * https://github.com/tensorflow/addons. If you depend on functionality not listed there, please file an issue. real	3m6.726s. user	2m58.710s. sys	0m25.780s.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:57,deployability,version,version,57,"ValueError: Must specify steps > 0, given: 0 deepvariant version: 0.8; Hi, I am new to deepvariant. I tried to train my deepvariant model with our own data within docker on my personal computer. Up until model_eval and model_train, every process was working fine. When I try to run model_train and model_eval in my computer, model_train seemed to work but model_eval returned ValueError: Must specify steps > 0, given: 0. The error came from estimator.py file in tensorflow. In Advanced Case Study: Train a customized SNP and small indel variant caller for BGISEQ-500 data, it says that . > At the same time, start model_eval on CPUs. Since I don't have a TPU, so the following is the code I used and attempt to run model_train and model_eval on CPU simultaneously. The following is the code I used:. `(time python /home/bin/model_train.zip \. --dataset_config_pbtxt=/data/output/training_data/customized_training/training_set_with_label_shuffled/training_set.dataset_config.pbtxt \. --train_dir=/data/output/trained_model \. --model_name=""inception_v3"" \. --number_of_steps=10 \. --save_interval_secs=3000 \. --batch_size=32 \. --learning_rate=0.008 \. --start_from_checkpoint=/home/models/model.ckpt) >/data/output/log/model_training/model_train.log 2>&1\. & (time python2 /home/bin/model_eval.zip \. --dataset_config_pbtxt=/data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.dataset_config.pbtxt \. --checkpoint_dir=/data/output/trained_model \. --number_of_steps=10 \. --batch_size=32) >/data/output/log/model_training/model_eval.log 2>&1`. The following is the message from model_eval log :. > I0415 07:34:19.493486 140713377441536 model_eval.py:141] Set KMP_BLOCKTIME to 0. I0415 07:34:19.495834 140713377441536 model_eval.py:177] Running fixed eval for: /data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.dataset_config.pbtxt. W0415 07:34:19.536698 140713377441536 deprecation.py:323] From /tmp/Bazel.r",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:1217,deployability,log,log,1217,"model_train, every process was working fine. When I try to run model_train and model_eval in my computer, model_train seemed to work but model_eval returned ValueError: Must specify steps > 0, given: 0. The error came from estimator.py file in tensorflow. In Advanced Case Study: Train a customized SNP and small indel variant caller for BGISEQ-500 data, it says that . > At the same time, start model_eval on CPUs. Since I don't have a TPU, so the following is the code I used and attempt to run model_train and model_eval on CPU simultaneously. The following is the code I used:. `(time python /home/bin/model_train.zip \. --dataset_config_pbtxt=/data/output/training_data/customized_training/training_set_with_label_shuffled/training_set.dataset_config.pbtxt \. --train_dir=/data/output/trained_model \. --model_name=""inception_v3"" \. --number_of_steps=10 \. --save_interval_secs=3000 \. --batch_size=32 \. --learning_rate=0.008 \. --start_from_checkpoint=/home/models/model.ckpt) >/data/output/log/model_training/model_train.log 2>&1\. & (time python2 /home/bin/model_eval.zip \. --dataset_config_pbtxt=/data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.dataset_config.pbtxt \. --checkpoint_dir=/data/output/trained_model \. --number_of_steps=10 \. --batch_size=32) >/data/output/log/model_training/model_eval.log 2>&1`. The following is the message from model_eval log :. > I0415 07:34:19.493486 140713377441536 model_eval.py:141] Set KMP_BLOCKTIME to 0. I0415 07:34:19.495834 140713377441536 model_eval.py:177] Running fixed eval for: /data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.dataset_config.pbtxt. W0415 07:34:19.536698 140713377441536 deprecation.py:323] From /tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py:307: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version. Instructions f",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:1248,deployability,log,log,1248,"working fine. When I try to run model_train and model_eval in my computer, model_train seemed to work but model_eval returned ValueError: Must specify steps > 0, given: 0. The error came from estimator.py file in tensorflow. In Advanced Case Study: Train a customized SNP and small indel variant caller for BGISEQ-500 data, it says that . > At the same time, start model_eval on CPUs. Since I don't have a TPU, so the following is the code I used and attempt to run model_train and model_eval on CPU simultaneously. The following is the code I used:. `(time python /home/bin/model_train.zip \. --dataset_config_pbtxt=/data/output/training_data/customized_training/training_set_with_label_shuffled/training_set.dataset_config.pbtxt \. --train_dir=/data/output/trained_model \. --model_name=""inception_v3"" \. --number_of_steps=10 \. --save_interval_secs=3000 \. --batch_size=32 \. --learning_rate=0.008 \. --start_from_checkpoint=/home/models/model.ckpt) >/data/output/log/model_training/model_train.log 2>&1\. & (time python2 /home/bin/model_eval.zip \. --dataset_config_pbtxt=/data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.dataset_config.pbtxt \. --checkpoint_dir=/data/output/trained_model \. --number_of_steps=10 \. --batch_size=32) >/data/output/log/model_training/model_eval.log 2>&1`. The following is the message from model_eval log :. > I0415 07:34:19.493486 140713377441536 model_eval.py:141] Set KMP_BLOCKTIME to 0. I0415 07:34:19.495834 140713377441536 model_eval.py:177] Running fixed eval for: /data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.dataset_config.pbtxt. W0415 07:34:19.536698 140713377441536 deprecation.py:323] From /tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py:307: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version. Instructions for updating:. Use eager executi",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:1549,deployability,log,log,1549,"r for BGISEQ-500 data, it says that . > At the same time, start model_eval on CPUs. Since I don't have a TPU, so the following is the code I used and attempt to run model_train and model_eval on CPU simultaneously. The following is the code I used:. `(time python /home/bin/model_train.zip \. --dataset_config_pbtxt=/data/output/training_data/customized_training/training_set_with_label_shuffled/training_set.dataset_config.pbtxt \. --train_dir=/data/output/trained_model \. --model_name=""inception_v3"" \. --number_of_steps=10 \. --save_interval_secs=3000 \. --batch_size=32 \. --learning_rate=0.008 \. --start_from_checkpoint=/home/models/model.ckpt) >/data/output/log/model_training/model_train.log 2>&1\. & (time python2 /home/bin/model_eval.zip \. --dataset_config_pbtxt=/data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.dataset_config.pbtxt \. --checkpoint_dir=/data/output/trained_model \. --number_of_steps=10 \. --batch_size=32) >/data/output/log/model_training/model_eval.log 2>&1`. The following is the message from model_eval log :. > I0415 07:34:19.493486 140713377441536 model_eval.py:141] Set KMP_BLOCKTIME to 0. I0415 07:34:19.495834 140713377441536 model_eval.py:177] Running fixed eval for: /data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.dataset_config.pbtxt. W0415 07:34:19.536698 140713377441536 deprecation.py:323] From /tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py:307: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version. Instructions for updating:. Use eager execution and: . `tf.data.TFRecordDataset(path)`. I0415 07:34:19.584646 140713377441536 model_eval.py:190] Running evaluations on DeepVariantInput(name=HG001, input_file_spec=/data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.with_label.shuffled-?????-of-?????.t",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:1579,deployability,log,log,1579," that . > At the same time, start model_eval on CPUs. Since I don't have a TPU, so the following is the code I used and attempt to run model_train and model_eval on CPU simultaneously. The following is the code I used:. `(time python /home/bin/model_train.zip \. --dataset_config_pbtxt=/data/output/training_data/customized_training/training_set_with_label_shuffled/training_set.dataset_config.pbtxt \. --train_dir=/data/output/trained_model \. --model_name=""inception_v3"" \. --number_of_steps=10 \. --save_interval_secs=3000 \. --batch_size=32 \. --learning_rate=0.008 \. --start_from_checkpoint=/home/models/model.ckpt) >/data/output/log/model_training/model_train.log 2>&1\. & (time python2 /home/bin/model_eval.zip \. --dataset_config_pbtxt=/data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.dataset_config.pbtxt \. --checkpoint_dir=/data/output/trained_model \. --number_of_steps=10 \. --batch_size=32) >/data/output/log/model_training/model_eval.log 2>&1`. The following is the message from model_eval log :. > I0415 07:34:19.493486 140713377441536 model_eval.py:141] Set KMP_BLOCKTIME to 0. I0415 07:34:19.495834 140713377441536 model_eval.py:177] Running fixed eval for: /data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.dataset_config.pbtxt. W0415 07:34:19.536698 140713377441536 deprecation.py:323] From /tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py:307: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version. Instructions for updating:. Use eager execution and: . `tf.data.TFRecordDataset(path)`. I0415 07:34:19.584646 140713377441536 model_eval.py:190] Running evaluations on DeepVariantInput(name=HG001, input_file_spec=/data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz, num_examples=8, mo",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:1635,deployability,log,log,1635,"nce I don't have a TPU, so the following is the code I used and attempt to run model_train and model_eval on CPU simultaneously. The following is the code I used:. `(time python /home/bin/model_train.zip \. --dataset_config_pbtxt=/data/output/training_data/customized_training/training_set_with_label_shuffled/training_set.dataset_config.pbtxt \. --train_dir=/data/output/trained_model \. --model_name=""inception_v3"" \. --number_of_steps=10 \. --save_interval_secs=3000 \. --batch_size=32 \. --learning_rate=0.008 \. --start_from_checkpoint=/home/models/model.ckpt) >/data/output/log/model_training/model_train.log 2>&1\. & (time python2 /home/bin/model_eval.zip \. --dataset_config_pbtxt=/data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.dataset_config.pbtxt \. --checkpoint_dir=/data/output/trained_model \. --number_of_steps=10 \. --batch_size=32) >/data/output/log/model_training/model_eval.log 2>&1`. The following is the message from model_eval log :. > I0415 07:34:19.493486 140713377441536 model_eval.py:141] Set KMP_BLOCKTIME to 0. I0415 07:34:19.495834 140713377441536 model_eval.py:177] Running fixed eval for: /data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.dataset_config.pbtxt. W0415 07:34:19.536698 140713377441536 deprecation.py:323] From /tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py:307: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version. Instructions for updating:. Use eager execution and: . `tf.data.TFRecordDataset(path)`. I0415 07:34:19.584646 140713377441536 model_eval.py:190] Running evaluations on DeepVariantInput(name=HG001, input_file_spec=/data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz, num_examples=8, mode=eval with model DeepVariantModel(name=inception_v3). ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:2196,deployability,version,version,2196,"t) >/data/output/log/model_training/model_train.log 2>&1\. & (time python2 /home/bin/model_eval.zip \. --dataset_config_pbtxt=/data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.dataset_config.pbtxt \. --checkpoint_dir=/data/output/trained_model \. --number_of_steps=10 \. --batch_size=32) >/data/output/log/model_training/model_eval.log 2>&1`. The following is the message from model_eval log :. > I0415 07:34:19.493486 140713377441536 model_eval.py:141] Set KMP_BLOCKTIME to 0. I0415 07:34:19.495834 140713377441536 model_eval.py:177] Running fixed eval for: /data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.dataset_config.pbtxt. W0415 07:34:19.536698 140713377441536 deprecation.py:323] From /tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py:307: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version. Instructions for updating:. Use eager execution and: . `tf.data.TFRecordDataset(path)`. I0415 07:34:19.584646 140713377441536 model_eval.py:190] Running evaluations on DeepVariantInput(name=HG001, input_file_spec=/data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz, num_examples=8, mode=eval with model DeepVariantModel(name=inception_v3). I0415 07:34:19.585236 140713377441536 model_eval.py:198] Dataset has 8 samples, doing eval over 0; max_examples is 1000000, num_batches is 0. I0415 07:34:19.585772 140713377441536 modeling.py:357] Initializing model with random parameters. I0415 07:34:19.586724 140713377441536 estimator.py:201] Using config: {'_save_checkpoints_secs': 1000, '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_train_distribute': None, '_is_chief': T",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:2222,deployability,updat,updating,2222,"_training/model_train.log 2>&1\. & (time python2 /home/bin/model_eval.zip \. --dataset_config_pbtxt=/data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.dataset_config.pbtxt \. --checkpoint_dir=/data/output/trained_model \. --number_of_steps=10 \. --batch_size=32) >/data/output/log/model_training/model_eval.log 2>&1`. The following is the message from model_eval log :. > I0415 07:34:19.493486 140713377441536 model_eval.py:141] Set KMP_BLOCKTIME to 0. I0415 07:34:19.495834 140713377441536 model_eval.py:177] Running fixed eval for: /data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.dataset_config.pbtxt. W0415 07:34:19.536698 140713377441536 deprecation.py:323] From /tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py:307: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version. Instructions for updating:. Use eager execution and: . `tf.data.TFRecordDataset(path)`. I0415 07:34:19.584646 140713377441536 model_eval.py:190] Running evaluations on DeepVariantInput(name=HG001, input_file_spec=/data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz, num_examples=8, mode=eval with model DeepVariantModel(name=inception_v3). I0415 07:34:19.585236 140713377441536 model_eval.py:198] Dataset has 8 samples, doing eval over 0; max_examples is 1000000, num_batches is 0. I0415 07:34:19.585772 140713377441536 modeling.py:357] Initializing model with random parameters. I0415 07:34:19.586724 140713377441536 estimator.py:201] Using config: {'_save_checkpoints_secs': 1000, '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <ten",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:3261,deployability,Cluster,ClusterSpec,3261,"a.TFRecordDataset(path)`. I0415 07:34:19.584646 140713377441536 model_eval.py:190] Running evaluations on DeepVariantInput(name=HG001, input_file_spec=/data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz, num_examples=8, mode=eval with model DeepVariantModel(name=inception_v3). I0415 07:34:19.585236 140713377441536 model_eval.py:198] Dataset has 8 samples, doing eval over 0; max_examples is 1000000, num_batches is 0. I0415 07:34:19.585772 140713377441536 modeling.py:357] Initializing model with random parameters. I0415 07:34:19.586724 140713377441536 estimator.py:201] Using config: {'_save_checkpoints_secs': 1000, '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ffa3b29ad50>, '_model_dir': '/data/output/trained_model', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_cluster': 0, '_master': ''}. I0415 07:34:19.587389 140713377441536 evaluation.py:189] Waiting for new checkpoint at /data/output/trained_model. I0415 07:35:14.785435 140713377441536 evaluation.py:198] Found new checkpoint at /data/output/trained_model/model.ckpt-0. I0415 07:35:14.787266 140713377441536 model_eval.py:225] Starting to evaluate. WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0. For more information, please see:. * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.m",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:4101,deployability,modul,module,4101,"_keep_checkpoint_max': 100000, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ffa3b29ad50>, '_model_dir': '/data/output/trained_model', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_cluster': 0, '_master': ''}. I0415 07:34:19.587389 140713377441536 evaluation.py:189] Waiting for new checkpoint at /data/output/trained_model. I0415 07:35:14.785435 140713377441536 evaluation.py:198] Found new checkpoint at /data/output/trained_model/model.ckpt-0. I0415 07:35:14.787266 140713377441536 model_eval.py:225] Starting to evaluate. WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0. For more information, please see:. * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md. * https://github.com/tensorflow/addons. If you depend on functionality not listed there, please file an issue. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/deepvariant/model_eval.py"", line 362, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/deepvariant/model_eval.py"", line 154, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/deepvariant/model_eval.py"", line 235, in eval_loop. name=eval_name). File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 469, in evaluate. name=name). File ""/usr/local/lib/python2.7/d",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:4317,deployability,depend,depend,4317,"/output/trained_model', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_cluster': 0, '_master': ''}. I0415 07:34:19.587389 140713377441536 evaluation.py:189] Waiting for new checkpoint at /data/output/trained_model. I0415 07:35:14.785435 140713377441536 evaluation.py:198] Found new checkpoint at /data/output/trained_model/model.ckpt-0. I0415 07:35:14.787266 140713377441536 model_eval.py:225] Starting to evaluate. WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0. For more information, please see:. * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md. * https://github.com/tensorflow/addons. If you depend on functionality not listed there, please file an issue. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/deepvariant/model_eval.py"", line 362, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/deepvariant/model_eval.py"", line 154, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/deepvariant/model_eval.py"", line 235, in eval_loop. name=eval_name). File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 469, in evaluate. name=name). File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 481, in _actual_eval. hooks.extend(self._convert_eval_steps_to_hooks(steps)). File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/p",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:4524,deployability,modul,module,4524,"00, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_cluster': 0, '_master': ''}. I0415 07:34:19.587389 140713377441536 evaluation.py:189] Waiting for new checkpoint at /data/output/trained_model. I0415 07:35:14.785435 140713377441536 evaluation.py:198] Found new checkpoint at /data/output/trained_model/model.ckpt-0. I0415 07:35:14.787266 140713377441536 model_eval.py:225] Starting to evaluate. WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0. For more information, please see:. * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md. * https://github.com/tensorflow/addons. If you depend on functionality not listed there, please file an issue. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/deepvariant/model_eval.py"", line 362, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/deepvariant/model_eval.py"", line 154, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/deepvariant/model_eval.py"", line 235, in eval_loop. name=eval_name). File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 469, in evaluate. name=name). File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 481, in _actual_eval. hooks.extend(self._convert_eval_steps_to_hooks(steps)). File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 529, in _convert_eval_steps_to_hooks. raise ValueError('Must specify steps > 0, given: {}'.format(steps)). ValueError: Must specify steps > 0, given: 0. real	0m58.116s. us",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:5895,deployability,instal,install,5895,"ine 235, in eval_loop. name=eval_name). File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 469, in evaluate. name=name). File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 481, in _actual_eval. hooks.extend(self._convert_eval_steps_to_hooks(steps)). File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 529, in _convert_eval_steps_to_hooks. raise ValueError('Must specify steps > 0, given: {}'.format(steps)). ValueError: Must specify steps > 0, given: 0. real	0m58.116s. user	0m1.590s. sys	0m0.590s. The following is my docker setting:. > FROM ubuntu:16.04. LABEL maintainer=""aaronarchiblad@gmail.com"". ENV DV_USE_GCP_OPTIMIZED_TF_WHL=0. ADD ./deepvariant /home/deepvariant. ADD ./bin /home/bin. ADD ./models /home/models. ADD ./oss_clif.ubuntu-16.latest.tgz /. WORKDIR /home/deepvariant. RUN /home/deepvariant/run-prereq.sh. RUN apt-get -y install python2.7. RUN apt-get -y install python-dev python-pip. RUN apt-get -y install parallel. RUN python2 -m pip install --upgrade --force-reinstall pip. RUN python2 -m pip install apache-beam. RUN python2 -m pip install google-cloud-dataflow. Just in case, if this is needed, I also quote the message from model_train, not sure if this will be useful. . > I0415 07:34:19.468628 140368878327552 model_train.py:310] Set KMP_BLOCKTIME to 0. I0415 07:34:19.469649 140368878327552 model_train.py:244] TF_CONFIG None. W0415 07:34:19.491456 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py:307: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version. Instructions for updating:. Use eager execution and: . `tf.data.TFRecordDataset(path)`. I0415 07:34:19.549700 140368878327552 model_train.py:193] Running training on DeepVariantInput(name=HG001, input_file_s",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:5929,deployability,instal,install,5929,"ame). File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 469, in evaluate. name=name). File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 481, in _actual_eval. hooks.extend(self._convert_eval_steps_to_hooks(steps)). File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 529, in _convert_eval_steps_to_hooks. raise ValueError('Must specify steps > 0, given: {}'.format(steps)). ValueError: Must specify steps > 0, given: 0. real	0m58.116s. user	0m1.590s. sys	0m0.590s. The following is my docker setting:. > FROM ubuntu:16.04. LABEL maintainer=""aaronarchiblad@gmail.com"". ENV DV_USE_GCP_OPTIMIZED_TF_WHL=0. ADD ./deepvariant /home/deepvariant. ADD ./bin /home/bin. ADD ./models /home/models. ADD ./oss_clif.ubuntu-16.latest.tgz /. WORKDIR /home/deepvariant. RUN /home/deepvariant/run-prereq.sh. RUN apt-get -y install python2.7. RUN apt-get -y install python-dev python-pip. RUN apt-get -y install parallel. RUN python2 -m pip install --upgrade --force-reinstall pip. RUN python2 -m pip install apache-beam. RUN python2 -m pip install google-cloud-dataflow. Just in case, if this is needed, I also quote the message from model_train, not sure if this will be useful. . > I0415 07:34:19.468628 140368878327552 model_train.py:310] Set KMP_BLOCKTIME to 0. I0415 07:34:19.469649 140368878327552 model_train.py:244] TF_CONFIG None. W0415 07:34:19.491456 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py:307: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version. Instructions for updating:. Use eager execution and: . `tf.data.TFRecordDataset(path)`. I0415 07:34:19.549700 140368878327552 model_train.py:193] Running training on DeepVariantInput(name=HG001, input_file_spec=/data/output/training_data/cus",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:5975,deployability,instal,install,5975,"ages/tensorflow_estimator/python/estimator/estimator.py"", line 469, in evaluate. name=name). File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 481, in _actual_eval. hooks.extend(self._convert_eval_steps_to_hooks(steps)). File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 529, in _convert_eval_steps_to_hooks. raise ValueError('Must specify steps > 0, given: {}'.format(steps)). ValueError: Must specify steps > 0, given: 0. real	0m58.116s. user	0m1.590s. sys	0m0.590s. The following is my docker setting:. > FROM ubuntu:16.04. LABEL maintainer=""aaronarchiblad@gmail.com"". ENV DV_USE_GCP_OPTIMIZED_TF_WHL=0. ADD ./deepvariant /home/deepvariant. ADD ./bin /home/bin. ADD ./models /home/models. ADD ./oss_clif.ubuntu-16.latest.tgz /. WORKDIR /home/deepvariant. RUN /home/deepvariant/run-prereq.sh. RUN apt-get -y install python2.7. RUN apt-get -y install python-dev python-pip. RUN apt-get -y install parallel. RUN python2 -m pip install --upgrade --force-reinstall pip. RUN python2 -m pip install apache-beam. RUN python2 -m pip install google-cloud-dataflow. Just in case, if this is needed, I also quote the message from model_train, not sure if this will be useful. . > I0415 07:34:19.468628 140368878327552 model_train.py:310] Set KMP_BLOCKTIME to 0. I0415 07:34:19.469649 140368878327552 model_train.py:244] TF_CONFIG None. W0415 07:34:19.491456 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py:307: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version. Instructions for updating:. Use eager execution and: . `tf.data.TFRecordDataset(path)`. I0415 07:34:19.549700 140368878327552 model_train.py:193] Running training on DeepVariantInput(name=HG001, input_file_spec=/data/output/training_data/customized_training/training_set_with_label_shuff",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:6012,deployability,instal,install,6012,"mator/estimator.py"", line 469, in evaluate. name=name). File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 481, in _actual_eval. hooks.extend(self._convert_eval_steps_to_hooks(steps)). File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 529, in _convert_eval_steps_to_hooks. raise ValueError('Must specify steps > 0, given: {}'.format(steps)). ValueError: Must specify steps > 0, given: 0. real	0m58.116s. user	0m1.590s. sys	0m0.590s. The following is my docker setting:. > FROM ubuntu:16.04. LABEL maintainer=""aaronarchiblad@gmail.com"". ENV DV_USE_GCP_OPTIMIZED_TF_WHL=0. ADD ./deepvariant /home/deepvariant. ADD ./bin /home/bin. ADD ./models /home/models. ADD ./oss_clif.ubuntu-16.latest.tgz /. WORKDIR /home/deepvariant. RUN /home/deepvariant/run-prereq.sh. RUN apt-get -y install python2.7. RUN apt-get -y install python-dev python-pip. RUN apt-get -y install parallel. RUN python2 -m pip install --upgrade --force-reinstall pip. RUN python2 -m pip install apache-beam. RUN python2 -m pip install google-cloud-dataflow. Just in case, if this is needed, I also quote the message from model_train, not sure if this will be useful. . > I0415 07:34:19.468628 140368878327552 model_train.py:310] Set KMP_BLOCKTIME to 0. I0415 07:34:19.469649 140368878327552 model_train.py:244] TF_CONFIG None. W0415 07:34:19.491456 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py:307: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version. Instructions for updating:. Use eager execution and: . `tf.data.TFRecordDataset(path)`. I0415 07:34:19.549700 140368878327552 model_train.py:193] Running training on DeepVariantInput(name=HG001, input_file_spec=/data/output/training_data/customized_training/training_set_with_label_shuffled/training_set.with_label.shuffled-",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:6022,deployability,upgrad,upgrade,6022,"mator.py"", line 469, in evaluate. name=name). File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 481, in _actual_eval. hooks.extend(self._convert_eval_steps_to_hooks(steps)). File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 529, in _convert_eval_steps_to_hooks. raise ValueError('Must specify steps > 0, given: {}'.format(steps)). ValueError: Must specify steps > 0, given: 0. real	0m58.116s. user	0m1.590s. sys	0m0.590s. The following is my docker setting:. > FROM ubuntu:16.04. LABEL maintainer=""aaronarchiblad@gmail.com"". ENV DV_USE_GCP_OPTIMIZED_TF_WHL=0. ADD ./deepvariant /home/deepvariant. ADD ./bin /home/bin. ADD ./models /home/models. ADD ./oss_clif.ubuntu-16.latest.tgz /. WORKDIR /home/deepvariant. RUN /home/deepvariant/run-prereq.sh. RUN apt-get -y install python2.7. RUN apt-get -y install python-dev python-pip. RUN apt-get -y install parallel. RUN python2 -m pip install --upgrade --force-reinstall pip. RUN python2 -m pip install apache-beam. RUN python2 -m pip install google-cloud-dataflow. Just in case, if this is needed, I also quote the message from model_train, not sure if this will be useful. . > I0415 07:34:19.468628 140368878327552 model_train.py:310] Set KMP_BLOCKTIME to 0. I0415 07:34:19.469649 140368878327552 model_train.py:244] TF_CONFIG None. W0415 07:34:19.491456 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py:307: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version. Instructions for updating:. Use eager execution and: . `tf.data.TFRecordDataset(path)`. I0415 07:34:19.549700 140368878327552 model_train.py:193] Running training on DeepVariantInput(name=HG001, input_file_spec=/data/output/training_data/customized_training/training_set_with_label_shuffled/training_set.with_label.shuffled-?????-of-?",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:6072,deployability,instal,install,6072," ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 481, in _actual_eval. hooks.extend(self._convert_eval_steps_to_hooks(steps)). File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 529, in _convert_eval_steps_to_hooks. raise ValueError('Must specify steps > 0, given: {}'.format(steps)). ValueError: Must specify steps > 0, given: 0. real	0m58.116s. user	0m1.590s. sys	0m0.590s. The following is my docker setting:. > FROM ubuntu:16.04. LABEL maintainer=""aaronarchiblad@gmail.com"". ENV DV_USE_GCP_OPTIMIZED_TF_WHL=0. ADD ./deepvariant /home/deepvariant. ADD ./bin /home/bin. ADD ./models /home/models. ADD ./oss_clif.ubuntu-16.latest.tgz /. WORKDIR /home/deepvariant. RUN /home/deepvariant/run-prereq.sh. RUN apt-get -y install python2.7. RUN apt-get -y install python-dev python-pip. RUN apt-get -y install parallel. RUN python2 -m pip install --upgrade --force-reinstall pip. RUN python2 -m pip install apache-beam. RUN python2 -m pip install google-cloud-dataflow. Just in case, if this is needed, I also quote the message from model_train, not sure if this will be useful. . > I0415 07:34:19.468628 140368878327552 model_train.py:310] Set KMP_BLOCKTIME to 0. I0415 07:34:19.469649 140368878327552 model_train.py:244] TF_CONFIG None. W0415 07:34:19.491456 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py:307: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version. Instructions for updating:. Use eager execution and: . `tf.data.TFRecordDataset(path)`. I0415 07:34:19.549700 140368878327552 model_train.py:193] Running training on DeepVariantInput(name=HG001, input_file_spec=/data/output/training_data/customized_training/training_set_with_label_shuffled/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz, num_examples=33, mode=train with",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:6112,deployability,instal,install,6112,"/tensorflow_estimator/python/estimator/estimator.py"", line 481, in _actual_eval. hooks.extend(self._convert_eval_steps_to_hooks(steps)). File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 529, in _convert_eval_steps_to_hooks. raise ValueError('Must specify steps > 0, given: {}'.format(steps)). ValueError: Must specify steps > 0, given: 0. real	0m58.116s. user	0m1.590s. sys	0m0.590s. The following is my docker setting:. > FROM ubuntu:16.04. LABEL maintainer=""aaronarchiblad@gmail.com"". ENV DV_USE_GCP_OPTIMIZED_TF_WHL=0. ADD ./deepvariant /home/deepvariant. ADD ./bin /home/bin. ADD ./models /home/models. ADD ./oss_clif.ubuntu-16.latest.tgz /. WORKDIR /home/deepvariant. RUN /home/deepvariant/run-prereq.sh. RUN apt-get -y install python2.7. RUN apt-get -y install python-dev python-pip. RUN apt-get -y install parallel. RUN python2 -m pip install --upgrade --force-reinstall pip. RUN python2 -m pip install apache-beam. RUN python2 -m pip install google-cloud-dataflow. Just in case, if this is needed, I also quote the message from model_train, not sure if this will be useful. . > I0415 07:34:19.468628 140368878327552 model_train.py:310] Set KMP_BLOCKTIME to 0. I0415 07:34:19.469649 140368878327552 model_train.py:244] TF_CONFIG None. W0415 07:34:19.491456 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py:307: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version. Instructions for updating:. Use eager execution and: . `tf.data.TFRecordDataset(path)`. I0415 07:34:19.549700 140368878327552 model_train.py:193] Running training on DeepVariantInput(name=HG001, input_file_spec=/data/output/training_data/customized_training/training_set_with_label_shuffled/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz, num_examples=33, mode=train with model inception_v3 and tpu False. I0415",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:6683,deployability,version,version,6683,"L=0. ADD ./deepvariant /home/deepvariant. ADD ./bin /home/bin. ADD ./models /home/models. ADD ./oss_clif.ubuntu-16.latest.tgz /. WORKDIR /home/deepvariant. RUN /home/deepvariant/run-prereq.sh. RUN apt-get -y install python2.7. RUN apt-get -y install python-dev python-pip. RUN apt-get -y install parallel. RUN python2 -m pip install --upgrade --force-reinstall pip. RUN python2 -m pip install apache-beam. RUN python2 -m pip install google-cloud-dataflow. Just in case, if this is needed, I also quote the message from model_train, not sure if this will be useful. . > I0415 07:34:19.468628 140368878327552 model_train.py:310] Set KMP_BLOCKTIME to 0. I0415 07:34:19.469649 140368878327552 model_train.py:244] TF_CONFIG None. W0415 07:34:19.491456 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py:307: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version. Instructions for updating:. Use eager execution and: . `tf.data.TFRecordDataset(path)`. I0415 07:34:19.549700 140368878327552 model_train.py:193] Running training on DeepVariantInput(name=HG001, input_file_spec=/data/output/training_data/customized_training/training_set_with_label_shuffled/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz, num_examples=33, mode=train with model inception_v3 and tpu False. I0415 07:34:19.550825 140368878327552 model_train.py:196] Batches per epoch 1. I0415 07:34:19.551630 140368878327552 modeling.py:330] Initializing model from checkpoint at /home/models/model.ckpt. I0415 07:34:19.564393 140368878327552 modeling.py:336] The model checkpoint to warm start from has the same number of classes. If this is in training, we will clear excluded_scopes_for_incompatible_shapes so we include everything for warm starting.... I0415 07:34:19.568434 140368878327552 estimator.py:201] Using config: {'_save_checkpoints_secs': 3000, '_session_config': allow",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:6709,deployability,updat,updating,6709,"me/deepvariant. ADD ./bin /home/bin. ADD ./models /home/models. ADD ./oss_clif.ubuntu-16.latest.tgz /. WORKDIR /home/deepvariant. RUN /home/deepvariant/run-prereq.sh. RUN apt-get -y install python2.7. RUN apt-get -y install python-dev python-pip. RUN apt-get -y install parallel. RUN python2 -m pip install --upgrade --force-reinstall pip. RUN python2 -m pip install apache-beam. RUN python2 -m pip install google-cloud-dataflow. Just in case, if this is needed, I also quote the message from model_train, not sure if this will be useful. . > I0415 07:34:19.468628 140368878327552 model_train.py:310] Set KMP_BLOCKTIME to 0. I0415 07:34:19.469649 140368878327552 model_train.py:244] TF_CONFIG None. W0415 07:34:19.491456 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py:307: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version. Instructions for updating:. Use eager execution and: . `tf.data.TFRecordDataset(path)`. I0415 07:34:19.549700 140368878327552 model_train.py:193] Running training on DeepVariantInput(name=HG001, input_file_spec=/data/output/training_data/customized_training/training_set_with_label_shuffled/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz, num_examples=33, mode=train with model inception_v3 and tpu False. I0415 07:34:19.550825 140368878327552 model_train.py:196] Batches per epoch 1. I0415 07:34:19.551630 140368878327552 modeling.py:330] Initializing model from checkpoint at /home/models/model.ckpt. I0415 07:34:19.564393 140368878327552 modeling.py:336] The model checkpoint to warm start from has the same number of classes. If this is in training, we will clear excluded_scopes_for_incompatible_shapes so we include everything for warm starting.... I0415 07:34:19.568434 140368878327552 estimator.py:201] Using config: {'_save_checkpoints_secs': 3000, '_session_config': allow_soft_placement: true. gra",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:7944,deployability,Cluster,ClusterSpec,7944,"training_set_with_label_shuffled/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz, num_examples=33, mode=train with model inception_v3 and tpu False. I0415 07:34:19.550825 140368878327552 model_train.py:196] Batches per epoch 1. I0415 07:34:19.551630 140368878327552 modeling.py:330] Initializing model from checkpoint at /home/models/model.ckpt. I0415 07:34:19.564393 140368878327552 modeling.py:336] The model checkpoint to warm start from has the same number of classes. If this is in training, we will clear excluded_scopes_for_incompatible_shapes so we include everything for warm starting.... I0415 07:34:19.568434 140368878327552 estimator.py:201] Using config: {'_save_checkpoints_secs': 3000, '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7faa056a5210>, '_model_dir': '/data/output/trained_model', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_cluster': 0, '_master': ''}. W0415 07:34:19.583559 140368878327552 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version. Instructions for updating:. Colocations handled automatically by placer. W0415 07:34:19.617336 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/deepvariant/data_providers.py:286: parallel_interleave (fr",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:8688,deployability,version,version,8688,"_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7faa056a5210>, '_model_dir': '/data/output/trained_model', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_cluster': 0, '_master': ''}. W0415 07:34:19.583559 140368878327552 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version. Instructions for updating:. Colocations handled automatically by placer. W0415 07:34:19.617336 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/deepvariant/data_providers.py:286: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.experimental.parallel_interleave(...)`. I0415 07:34:19.674496 140368878327552 estimator.py:1111] Calling model_fn. W0415 07:34:19.675647 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/deepvariant/modeling.py:669: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use tf.cast instead. W0415 07:34:19.682488 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/deepvariant/modeling.py:671: div (from tensorflow.pyt",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:8714,deployability,updat,updating,8714,"tions {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7faa056a5210>, '_model_dir': '/data/output/trained_model', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_cluster': 0, '_master': ''}. W0415 07:34:19.583559 140368878327552 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version. Instructions for updating:. Colocations handled automatically by placer. W0415 07:34:19.617336 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/deepvariant/data_providers.py:286: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.experimental.parallel_interleave(...)`. I0415 07:34:19.674496 140368878327552 estimator.py:1111] Calling model_fn. W0415 07:34:19.675647 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/deepvariant/modeling.py:669: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use tf.cast instead. W0415 07:34:19.682488 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/deepvariant/modeling.py:671: div (from tensorflow.python.ops.math_ops) is depre",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:8745,deployability,automat,automatically,8745,"ptimizer_iterations: ONE. }. }. , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7faa056a5210>, '_model_dir': '/data/output/trained_model', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_cluster': 0, '_master': ''}. W0415 07:34:19.583559 140368878327552 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version. Instructions for updating:. Colocations handled automatically by placer. W0415 07:34:19.617336 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/deepvariant/data_providers.py:286: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.experimental.parallel_interleave(...)`. I0415 07:34:19.674496 140368878327552 estimator.py:1111] Calling model_fn. W0415 07:34:19.675647 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/deepvariant/modeling.py:669: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use tf.cast instead. W0415 07:34:19.682488 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/deepvariant/modeling.py:671: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a fut",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:9050,deployability,version,version,9050,"heckpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_cluster': 0, '_master': ''}. W0415 07:34:19.583559 140368878327552 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version. Instructions for updating:. Colocations handled automatically by placer. W0415 07:34:19.617336 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/deepvariant/data_providers.py:286: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.experimental.parallel_interleave(...)`. I0415 07:34:19.674496 140368878327552 estimator.py:1111] Calling model_fn. W0415 07:34:19.675647 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/deepvariant/modeling.py:669: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use tf.cast instead. W0415 07:34:19.682488 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/deepvariant/modeling.py:671: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. W0415 07:34:23.556665 140368878327552 deprecation.py:506] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_pr",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:9076,deployability,updat,updating,9076,"_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_cluster': 0, '_master': ''}. W0415 07:34:19.583559 140368878327552 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version. Instructions for updating:. Colocations handled automatically by placer. W0415 07:34:19.617336 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/deepvariant/data_providers.py:286: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.experimental.parallel_interleave(...)`. I0415 07:34:19.674496 140368878327552 estimator.py:1111] Calling model_fn. W0415 07:34:19.675647 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/deepvariant/modeling.py:669: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use tf.cast instead. W0415 07:34:19.682488 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/deepvariant/modeling.py:671: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. W0415 07:34:23.556665 140368878327552 deprecation.py:506] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:9459,deployability,version,version,9459,8878327552 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version. Instructions for updating:. Colocations handled automatically by placer. W0415 07:34:19.617336 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/deepvariant/data_providers.py:286: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.experimental.parallel_interleave(...)`. I0415 07:34:19.674496 140368878327552 estimator.py:1111] Calling model_fn. W0415 07:34:19.675647 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/deepvariant/modeling.py:669: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use tf.cast instead. W0415 07:34:19.682488 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/deepvariant/modeling.py:671: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. W0415 07:34:23.556665 140368878327552 deprecation.py:506] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version. Instructions for updating:. Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`. I0415 07:34:37.681962 140368878327552 estimator.py:1113] Done calling model_fn. I0415 07:34:37.683280 140368878327552 estimator.py:1294] Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from='/home/models/mode,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:9485,deployability,updat,updating,9485,"323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version. Instructions for updating:. Colocations handled automatically by placer. W0415 07:34:19.617336 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/deepvariant/data_providers.py:286: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.experimental.parallel_interleave(...)`. I0415 07:34:19.674496 140368878327552 estimator.py:1111] Calling model_fn. W0415 07:34:19.675647 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/deepvariant/modeling.py:669: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use tf.cast instead. W0415 07:34:19.682488 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/deepvariant/modeling.py:671: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. W0415 07:34:23.556665 140368878327552 deprecation.py:506] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version. Instructions for updating:. Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`. I0415 07:34:37.681962 140368878327552 estimator.py:1113] Done calling model_fn. I0415 07:34:37.683280 140368878327552 estimator.py:1294] Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from='/home/models/model.ckpt', vars_to_warm_star",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:9756,deployability,version,version,9756,"y placer. W0415 07:34:19.617336 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/deepvariant/data_providers.py:286: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.experimental.parallel_interleave(...)`. I0415 07:34:19.674496 140368878327552 estimator.py:1111] Calling model_fn. W0415 07:34:19.675647 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/deepvariant/modeling.py:669: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use tf.cast instead. W0415 07:34:19.682488 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/deepvariant/modeling.py:671: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. W0415 07:34:23.556665 140368878327552 deprecation.py:506] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version. Instructions for updating:. Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`. I0415 07:34:37.681962 140368878327552 estimator.py:1113] Done calling model_fn. I0415 07:34:37.683280 140368878327552 estimator.py:1294] Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from='/home/models/model.ckpt', vars_to_warm_start='InceptionV3/Mixed_6d/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_6d/Branch_1/Conv2d_0a_1x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_6d/Branch_1/Conv2d_0c_7x1/BatchNorm/beta/RMSProp|InceptionV3/Mixed_5d/Branch_2",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:9782,deployability,updat,updating,9782,"17336 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/deepvariant/data_providers.py:286: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.experimental.parallel_interleave(...)`. I0415 07:34:19.674496 140368878327552 estimator.py:1111] Calling model_fn. W0415 07:34:19.675647 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/deepvariant/modeling.py:669: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use tf.cast instead. W0415 07:34:19.682488 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/deepvariant/modeling.py:671: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. W0415 07:34:23.556665 140368878327552 deprecation.py:506] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version. Instructions for updating:. Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`. I0415 07:34:37.681962 140368878327552 estimator.py:1113] Done calling model_fn. I0415 07:34:37.683280 140368878327552 estimator.py:1294] Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from='/home/models/model.ckpt', vars_to_warm_start='InceptionV3/Mixed_6d/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_6d/Branch_1/Conv2d_0a_1x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_6d/Branch_1/Conv2d_0c_7x1/BatchNorm/beta/RMSProp|InceptionV3/Mixed_5d/Branch_2/Conv2d_0a_1x1/weights|Inc",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:10103,deployability,version,version,10103,"ental.parallel_interleave(...)`. I0415 07:34:19.674496 140368878327552 estimator.py:1111] Calling model_fn. W0415 07:34:19.675647 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/deepvariant/modeling.py:669: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use tf.cast instead. W0415 07:34:19.682488 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/deepvariant/modeling.py:671: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. W0415 07:34:23.556665 140368878327552 deprecation.py:506] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version. Instructions for updating:. Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`. I0415 07:34:37.681962 140368878327552 estimator.py:1113] Done calling model_fn. I0415 07:34:37.683280 140368878327552 estimator.py:1294] Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from='/home/models/model.ckpt', vars_to_warm_start='InceptionV3/Mixed_6d/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_6d/Branch_1/Conv2d_0a_1x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_6d/Branch_1/Conv2d_0c_7x1/BatchNorm/beta/RMSProp|InceptionV3/Mixed_5d/Branch_2/Conv2d_0a_1x1/weights|InceptionV3/Mixed_7b/Branch_2/Conv2d_0b_3x3/weights|InceptionV3/Mixed_5b/Branch_1/Conv2d_0b_5x5/BatchNorm/beta/RMSProp|InceptionV3/Mixed_6a/Branch_0/Conv2d_1a_1x1/weights/RMSProp|InceptionV3/Mixed_6a/Branch_1/Conv2d_0b_3x3/weights/ExponentialMovingAverage|InceptionV3/Mixed_6e/Branch_2/Conv2d_0a_1x1/weights/RMSProp_1|Incept",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:10129,deployability,updat,updating,10129,"...)`. I0415 07:34:19.674496 140368878327552 estimator.py:1111] Calling model_fn. W0415 07:34:19.675647 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/deepvariant/modeling.py:669: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use tf.cast instead. W0415 07:34:19.682488 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/deepvariant/modeling.py:671: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. W0415 07:34:23.556665 140368878327552 deprecation.py:506] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version. Instructions for updating:. Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`. I0415 07:34:37.681962 140368878327552 estimator.py:1113] Done calling model_fn. I0415 07:34:37.683280 140368878327552 estimator.py:1294] Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from='/home/models/model.ckpt', vars_to_warm_start='InceptionV3/Mixed_6d/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_6d/Branch_1/Conv2d_0a_1x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_6d/Branch_1/Conv2d_0c_7x1/BatchNorm/beta/RMSProp|InceptionV3/Mixed_5d/Branch_2/Conv2d_0a_1x1/weights|InceptionV3/Mixed_7b/Branch_2/Conv2d_0b_3x3/weights|InceptionV3/Mixed_5b/Branch_1/Conv2d_0b_5x5/BatchNorm/beta/RMSProp|InceptionV3/Mixed_6a/Branch_0/Conv2d_1a_1x1/weights/RMSProp|InceptionV3/Mixed_6a/Branch_1/Conv2d_0b_3x3/weights/ExponentialMovingAverage|InceptionV3/Mixed_6e/Branch_2/Conv2d_0a_1x1/weights/RMSProp_1|InceptionV3/Mixed_6d/Branch_0/Co",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:26334,deployability,Log,Logits,26334,atchNorm/moving_variance|InceptionV3/Mixed_6d/Branch_2/Conv2d_0a_1x1/weights|InceptionV3/Mixed_7c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_6e/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_6c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1|InceptionV3/Mixed_6b/Branch_2/Conv2d_0c_1x7/BatchNorm/beta/RMSProp_1|InceptionV3/Mixed_6e/Branch_2/Conv2d_0e_1x7/weights/RMSProp|InceptionV3/Mixed_5d/Branch_1/Conv2d_0b_5x5/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_7c/Branch_2/Conv2d_0d_3x1/BatchNorm/moving_mean|InceptionV3/Mixed_5d/Branch_1/Conv2d_0b_5x5/BatchNorm/moving_variance|InceptionV3/Mixed_7b/Branch_2/Conv2d_0a_1x1/weights|InceptionV3/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_7a/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_6d/Branch_2/Conv2d_0e_1x7/BatchNorm/moving_mean|InceptionV3/Logits/Conv2d_1c_1x1/biases|InceptionV3/Conv2d_1a_3x3/weights/RMSProp_1|InceptionV3/Mixed_6b/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_mean|InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/BatchNorm/beta/RMSProp|InceptionV3/Mixed_6e/Branch_1/Conv2d_0c_7x1/BatchNorm/beta/RMSProp_1|InceptionV3/Mixed_7c/Branch_2/Conv2d_0d_3x1/BatchNorm/beta/RMSProp|InceptionV3/Mixed_6d/Branch_2/Conv2d_0c_1x7/weights/RMSProp_1|InceptionV3/Mixed_6d/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_mean|InceptionV3/Mixed_6d/Branch_2/Conv2d_0d_7x1/BatchNorm/beta/RMSProp|InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/weights/RMSProp_1|InceptionV3/Mixed_6c/Branch_2/Conv2d_0d_7x1/weights/RMSProp|InceptionV3/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_6d/Branch_2/Conv2d_0b_7x1/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_6c/Branch_2/Conv2d_0b_7x1/BatchNorm/moving_mean|InceptionV3/Mixed_6d/Branch_1/Conv2d_0c_7x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_7c/Br,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:36748,deployability,Log,Logits,36748,Branch_2/Conv2d_0b_3x3/BatchNorm/beta/RMSProp|InceptionV3/Mixed_5c/Branch_1/Conv2d_0b_1x1/weights/RMSProp_1|InceptionV3/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_7c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/RMSProp|InceptionV3/Mixed_7a/Branch_1/Conv2d_0c_7x1/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_5d/Branch_2/Conv2d_0b_3x3/BatchNorm/beta|InceptionV3/Mixed_5c/Branch_1/Conv2d_0b_1x1/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_7b/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_mean|InceptionV3/Mixed_6e/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance|InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_1x3/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_6a/Branch_1/Conv2d_1a_1x1/BatchNorm/moving_mean|InceptionV3/Mixed_6e/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_7b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta|InceptionV3/Mixed_6b/Branch_2/Conv2d_0b_7x1/weights|InceptionV3/Logits/Conv2d_1c_1x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_5c/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/RMSProp_1|InceptionV3/Mixed_7b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/RMSProp|InceptionV3/Mixed_5b/Branch_2/Conv2d_0b_3x3/weights/RMSProp|InceptionV3/Conv2d_2a_3x3/weights/RMSProp_1|InceptionV3/Mixed_6c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta|InceptionV3/Mixed_7b/Branch_2/Conv2d_0c_1x3/weights|InceptionV3/Mixed_7c/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_7b/Branch_0/Conv2d_0a_1x1/weights|InceptionV3/Mixed_5c/Branch_1/Conv2d_0b_1x1/weights/RMSProp|InceptionV3/Mixed_5d/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_6c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_6d/Branch_1/Conv2d_0a_1x1/weights|InceptionV3/Mixed_6d/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_6b/Branch_2/Conv2d_0b_7x1/BatchNorm/moving_variance/Exp,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:42500,deployability,Log,Logits,42500,tionV3/Mixed_5d/Branch_2/Conv2d_0c_3x3/weights|InceptionV3/Mixed_5c/Branch_3/Conv2d_0b_1x1/weights/RMSProp|InceptionV3/Mixed_6c/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_3x1/BatchNorm/beta/RMSProp|InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_6e/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/RMSProp|InceptionV3/Mixed_6e/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_mean|InceptionV3/Mixed_5c/Branch_2/Conv2d_0b_3x3/weights/ExponentialMovingAverage|InceptionV3/Mixed_6c/Branch_2/Conv2d_0b_7x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_6d/Branch_1/Conv2d_0b_1x7/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_6e/Branch_2/Conv2d_0e_1x7/BatchNorm/moving_variance|InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/BatchNorm/beta/RMSProp_1|InceptionV3/Mixed_5b/Branch_1/Conv2d_0a_1x1/weights|InceptionV3/Mixed_6d/Branch_1/Conv2d_0a_1x1/BatchNorm/beta|InceptionV3/Logits/Conv2d_1c_1x1/biases/ExponentialMovingAverage|InceptionV3/Mixed_5b/Branch_1/Conv2d_0b_5x5/weights/RMSProp_1|InceptionV3/Mixed_6d/Branch_2/Conv2d_0b_7x1/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_mean|InceptionV3/Mixed_5c/Branch_1/Conv2d_0b_1x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_6b/Branch_1/Conv2d_0c_7x1/BatchNorm/beta/RMSProp_1|InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/BatchNorm/beta|InceptionV3/Mixed_6b/Branch_3/Conv2d_0b_1x1/weights/RMSProp|InceptionV3/Mixed_7a/Branch_1/Conv2d_0b_1x7/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_6c/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_6c/Branch_2/Conv2d_0d_7x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_6c/Branch_2/Conv2,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:43631,deployability,Log,Logits,43631,d/Branch_2/Conv2d_0b_7x1/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_mean|InceptionV3/Mixed_5c/Branch_1/Conv2d_0b_1x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_6b/Branch_1/Conv2d_0c_7x1/BatchNorm/beta/RMSProp_1|InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/BatchNorm/beta|InceptionV3/Mixed_6b/Branch_3/Conv2d_0b_1x1/weights/RMSProp|InceptionV3/Mixed_7a/Branch_1/Conv2d_0b_1x7/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_6c/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_6c/Branch_2/Conv2d_0d_7x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_6c/Branch_2/Conv2d_0d_7x1/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_6c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/RMSProp|InceptionV3/Logits/Conv2d_1c_1x1/weights|InceptionV3/Mixed_5c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/RMSProp_1|InceptionV3/Mixed_6d/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance|InceptionV3/Conv2d_2b_3x3/BatchNorm/moving_mean|InceptionV3/Mixed_6c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1|InceptionV3/Mixed_6c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta|InceptionV3/Mixed_5d/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/RMSProp|InceptionV3/Mixed_6e/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_mean|InceptionV3/Mixed_7b/Branch_1/Conv2d_0a_1x1/weights/RMSProp_1|InceptionV3/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_mean|InceptionV3/Mixed_7b/Branch_2/Conv2d_0c_1x3/BatchNorm/beta/RMSProp|InceptionV3/Mixed_6a/Branch_0/Conv2d_1a_1x1/BatchNorm/beta/RMSProp_1|InceptionV3/Mixed_6c/Branch_3/Conv2d_0b_1x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_6a/Branch_1/Conv2d_0b_3x3/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_7a/Branch_1/Conv2d_0c_7x1/BatchNorm/beta|InceptionV3/Mixed_6e/Branch_1/C,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:52282,deployability,Log,Logits,52282,p|InceptionV3/Mixed_7b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/RMSProp|InceptionV3/Mixed_6c/Branch_1/Conv2d_0b_1x7/weights/RMSProp|InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_7a/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_variance|InceptionV3/Mixed_6c/Branch_2/Conv2d_0b_7x1/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_6d/Branch_2/Conv2d_0d_7x1/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_5c/Branch_3/Conv2d_0b_1x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_7b/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean|InceptionV3/Mixed_5c/Branch_1/Conv2d_0b_1x1/BatchNorm/moving_mean|InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/weights|InceptionV3/Mixed_6e/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_6a/Branch_1/Conv2d_0a_1x1/weights/RMSProp_1|InceptionV3/Logits/Conv2d_1c_1x1/biases/RMSProp|InceptionV3/Mixed_5d/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/RMSProp|InceptionV3/Mixed_6e/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1|InceptionV3/Mixed_6c/Branch_2/Conv2d_0c_1x7/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_3x1/weights|InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/weights/ExponentialMovingAverage|InceptionV3/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/RMSProp_1|InceptionV3/Mixed_5c/Branch_2/Conv2d_0b_3x3/weights/RMSProp_1|InceptionV3/Mixed_6c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta|InceptionV3/Mixed_6b/Branch_2/Conv2d_0e_1x7/weights/RMSProp_1|InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_3x1/BatchNorm/moving_mean|InceptionV3/Mixed_5c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_5c/Branch_2/Conv2d_0a_1x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_7b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_6c/Branch_2/Conv2d_0b_7x1/weig,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:59621,deployability,Log,Logits,59621,nce/ExponentialMovingAverage|InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_mean|InceptionV3/Mixed_6d/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_mean|InceptionV3/Mixed_5b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta|InceptionV3/Mixed_5c/Branch_2/Conv2d_0c_3x3/weights/RMSProp|InceptionV3/Mixed_5c/Branch_2/Conv2d_0a_1x1/weights|InceptionV3/Mixed_7a/Branch_1/Conv2d_0b_1x7/BatchNorm/beta/RMSProp|InceptionV3/Mixed_6c/Branch_2/Conv2d_0c_1x7/weights/ExponentialMovingAverage|InceptionV3/Mixed_6d/Branch_3/Conv2d_0b_1x1/BatchNorm/beta|InceptionV3/Mixed_6e/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_5d/Branch_1/Conv2d_0a_1x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_6d/Branch_2/Conv2d_0c_1x7/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_6e/Branch_2/Conv2d_0b_7x1/weights|InceptionV3/Mixed_6c/Branch_2/Conv2d_0e_1x7/weights/ExponentialMovingAverage|InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/BatchNorm/beta/RMSProp_1|InceptionV3/Logits/Conv2d_1c_1x1/biases/RMSProp_1|InceptionV3/Mixed_7c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1|InceptionV3/Mixed_6e/Branch_0/Conv2d_0a_1x1/weights/RMSProp|InceptionV3/Mixed_5c/Branch_2/Conv2d_0c_3x3/weights/ExponentialMovingAverage|InceptionV3/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_variance|InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_3x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_6c/Branch_2/Conv2d_0e_1x7/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/weights|InceptionV3/Mixed_7c/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_mean|InceptionV3/Mixed_6a/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_6b/Branch_2/Conv2d_0d_7x1/weights|InceptionV3/Mixed_5b/Branch_1/Conv2d_0b_5x5/BatchNorm/moving_variance|InceptionV3/Mixed_7b/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_variance|InceptionV3/Mixed_6c/Bran,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:64467,deployability,Log,Logits,64467,d_0a_1x1/BatchNorm/beta|InceptionV3/Mixed_7b/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_variance|InceptionV3/Mixed_5b/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Conv2d_2a_3x3/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_6c/Branch_1/Conv2d_0b_1x7/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_7c/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean|InceptionV3/Mixed_6c/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_7b/Branch_2/Conv2d_0c_1x3/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/weights/RMSProp|InceptionV3/Mixed_6d/Branch_1/Conv2d_0b_1x7/weights/RMSProp_1|InceptionV3/Mixed_5c/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/RMSProp|InceptionV3/Mixed_5b/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/RMSProp_1|InceptionV3/Mixed_6a/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/RMSProp_1|InceptionV3/Mixed_6e/Branch_1/Conv2d_0c_7x1/weights/RMSProp|InceptionV3/Logits/Conv2d_1c_1x1/weights/RMSProp|InceptionV3/Mixed_5b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/RMSProp|InceptionV3/Mixed_7b/Branch_2/Conv2d_0c_1x3/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta|InceptionV3/Mixed_6c/Branch_2/Conv2d_0e_1x7/weights|InceptionV3/Conv2d_2b_3x3/weights/RMSProp|InceptionV3/Mixed_6a/Branch_1/Conv2d_1a_1x1/BatchNorm/beta/RMSProp|InceptionV3/Mixed_6c/Branch_2/Conv2d_0d_7x1/weights/RMSProp_1|InceptionV3/Mixed_6b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta|InceptionV3/Mixed_7b/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean|InceptionV3/Mixed_5d/Branch_2/Conv2d_0a_1x1/BatchNorm/beta|InceptionV3/Mixed_6b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/RMSProp|InceptionV3/Mixed_6c/Branch_1/Conv2d_0a_1x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_6b/Branch_1/Conv2d_0a_1x1/weights/RMSProp_1|InceptionV3/Mixed_7b/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_1,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:88798,deployability,Log,Logits,88798,hNorm/beta|InceptionV3/Mixed_6c/Branch_2/Conv2d_0d_7x1/weights|InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_variance|InceptionV3/Mixed_6a/Branch_1/Conv2d_0b_3x3/weights|InceptionV3/Mixed_7a/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_mean|InceptionV3/Mixed_5d/Branch_1/Conv2d_0b_5x5/weights/ExponentialMovingAverage|InceptionV3/Mixed_6d/Branch_0/Conv2d_0a_1x1/weights|InceptionV3/Mixed_5d/Branch_0/Conv2d_0a_1x1/weights/RMSProp_1|InceptionV3/Mixed_6a/Branch_0/Conv2d_1a_1x1/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_6a/Branch_1/Conv2d_1a_1x1/weights/RMSProp_1|InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_1x3/BatchNorm/beta|InceptionV3/Mixed_6e/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/weights|InceptionV3/Conv2d_4a_3x3/weights/RMSProp_1|InceptionV3/Mixed_6c/Branch_2/Conv2d_0d_7x1/BatchNorm/moving_variance|InceptionV3/Mixed_5d/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance|InceptionV3/Logits/Conv2d_1c_1x1/weights/RMSProp_1|InceptionV3/Mixed_6a/Branch_0/Conv2d_1a_1x1/BatchNorm/beta/RMSProp|InceptionV3/Mixed_6c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_7b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta|InceptionV3/Mixed_6d/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_mean|InceptionV3/Mixed_7c/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_variance|InceptionV3/Mixed_6b/Branch_2/Conv2d_0b_7x1/BatchNorm/moving_mean|InceptionV3/Mixed_5d/Branch_1/Conv2d_0a_1x1/weights|InceptionV3/Mixed_6c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_5b/Branch_3/Conv2d_0b_1x1/weights|InceptionV3/Mixed_5d/Branch_2/Conv2d_0a_1x1/weights/RMSProp_1|InceptionV3/Mixed_6b/Branch_3/Conv2d_0b_1x1/weights/ExponentialMovingAverage|InceptionV3/Conv2d_3b_1x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_7c/Branch_2/Conv2d_0b_3x3/weights/ExponentialMovingAverage|InceptionV3/Conv2d_4a_3x3/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixe,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:110499,deployability,Log,Logits,110499,rm/beta; prev_var_name: Unchanged. I0415 07:34:38.032978 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0d_7x1/BatchNorm/beta; prev_var_name: Unchanged. I0415 07:34:38.033324 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged. I0415 07:34:38.033725 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/BatchNorm/beta; prev_var_name: Unchanged. I0415 07:34:38.034166 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged. I0415 07:34:38.034531 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged. I0415 07:34:38.034941 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Logits/Conv2d_1c_1x1/weights; prev_var_name: Unchanged. I0415 07:34:38.035353 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_1a_1x1/weights; prev_var_name: Unchanged. I0415 07:34:38.035823 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged. I0415 07:34:38.036331 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged. I0415 07:34:38.036871 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged. I0415 07:34:38.037236 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged. I0415 07:34:38.037589 140368878327552 warm_starting_util.py:466] Warm-starting variabl,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:115268,deployability,Log,Logits,115268,BatchNorm/beta; prev_var_name: Unchanged. I0415 07:34:38.044384 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged. I0415 07:34:38.047698 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged. I0415 07:34:38.048196 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged. I0415 07:34:38.048751 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0b_7x1/weights; prev_var_name: Unchanged. I0415 07:34:38.049181 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/weights; prev_var_name: Unchanged. I0415 07:34:38.049621 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Logits/Conv2d_1c_1x1/biases; prev_var_name: Unchanged. I0415 07:34:38.050132 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged. I0415 07:34:38.050539 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0d_7x1/weights; prev_var_name: Unchanged. I0415 07:34:38.050942 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_1/Conv2d_0b_5x5/weights; prev_var_name: Unchanged. I0415 07:34:38.051342 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged. I0415 07:34:38.051747 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0e_1x7/weights; prev_var_name: Unchanged. I0415 07:34:38.052149 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/,MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:123000,deployability,servic,service,123000,"py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged. I0415 07:34:38.066709 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged. I0415 07:34:38.067050 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/BatchNorm/beta; prev_var_name: Unchanged. I0415 07:34:38.657821 140368878327552 basic_session_run_hooks.py:527] Create CheckpointSaverHook. I0415 07:34:45.316857 140368878327552 monitored_session.py:222] Graph was finalized. 2019-04-15 07:34:45.317978: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2019-04-15 07:34:45.322541: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2304000000 Hz. 2019-04-15 07:34:45.323247: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x175ebd50 executing computations on platform Host. Devices:. 2019-04-15 07:34:45.323718: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. I0415 07:34:52.317267 140368878327552 session_manager.py:491] Running local_init_op. I0415 07:34:52.780421 140368878327552 session_manager.py:493] Done running local_init_op. I0415 07:35:11.098021 140368878327552 basic_session_run_hooks.py:594] Saving checkpoints for 0 into /data/output/trained_model/model.ckpt. 2019-04-15 07:35:25.684776: W tensorflow/core/framework/allocator.cc:124] Allocation of 16972800 exceeds 10% of system memory. 2019-04-15 07:35:25.703962: W tensorflow/core/framework/allocator.cc:124] Allocation of 22077440 exceeds 10% of system memory. 2019-04-15 07:35:25.779693: W tensorflow/core/framework/allocator.cc:124] Allocation of 22077440 exceeds 10% of system memory. 2019-04-15 07:35:25.836485: W tensorflow/core/framework/allocator.cc:124] Alloca",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:123008,deployability,servic,service,123008,"Warm-starting variable: InceptionV3/Mixed_5d/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged. I0415 07:34:38.066709 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged. I0415 07:34:38.067050 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/BatchNorm/beta; prev_var_name: Unchanged. I0415 07:34:38.657821 140368878327552 basic_session_run_hooks.py:527] Create CheckpointSaverHook. I0415 07:34:45.316857 140368878327552 monitored_session.py:222] Graph was finalized. 2019-04-15 07:34:45.317978: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2019-04-15 07:34:45.322541: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2304000000 Hz. 2019-04-15 07:34:45.323247: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x175ebd50 executing computations on platform Host. Devices:. 2019-04-15 07:34:45.323718: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. I0415 07:34:52.317267 140368878327552 session_manager.py:491] Running local_init_op. I0415 07:34:52.780421 140368878327552 session_manager.py:493] Done running local_init_op. I0415 07:35:11.098021 140368878327552 basic_session_run_hooks.py:594] Saving checkpoints for 0 into /data/output/trained_model/model.ckpt. 2019-04-15 07:35:25.684776: W tensorflow/core/framework/allocator.cc:124] Allocation of 16972800 exceeds 10% of system memory. 2019-04-15 07:35:25.703962: W tensorflow/core/framework/allocator.cc:124] Allocation of 22077440 exceeds 10% of system memory. 2019-04-15 07:35:25.779693: W tensorflow/core/framework/allocator.cc:124] Allocation of 22077440 exceeds 10% of system memory. 2019-04-15 07:35:25.836485: W tensorflow/core/framework/allocator.cc:124] Allocation of ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:123028,deployability,servic,service,123028,"le: InceptionV3/Mixed_5d/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged. I0415 07:34:38.066709 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged. I0415 07:34:38.067050 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/BatchNorm/beta; prev_var_name: Unchanged. I0415 07:34:38.657821 140368878327552 basic_session_run_hooks.py:527] Create CheckpointSaverHook. I0415 07:34:45.316857 140368878327552 monitored_session.py:222] Graph was finalized. 2019-04-15 07:34:45.317978: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2019-04-15 07:34:45.322541: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2304000000 Hz. 2019-04-15 07:34:45.323247: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x175ebd50 executing computations on platform Host. Devices:. 2019-04-15 07:34:45.323718: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. I0415 07:34:52.317267 140368878327552 session_manager.py:491] Running local_init_op. I0415 07:34:52.780421 140368878327552 session_manager.py:493] Done running local_init_op. I0415 07:35:11.098021 140368878327552 basic_session_run_hooks.py:594] Saving checkpoints for 0 into /data/output/trained_model/model.ckpt. 2019-04-15 07:35:25.684776: W tensorflow/core/framework/allocator.cc:124] Allocation of 16972800 exceeds 10% of system memory. 2019-04-15 07:35:25.703962: W tensorflow/core/framework/allocator.cc:124] Allocation of 22077440 exceeds 10% of system memory. 2019-04-15 07:35:25.779693: W tensorflow/core/framework/allocator.cc:124] Allocation of 22077440 exceeds 10% of system memory. 2019-04-15 07:35:25.836485: W tensorflow/core/framework/allocator.cc:124] Allocation of 20791296 exceeds 10%",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:123152,deployability,servic,service,123152,"552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged. I0415 07:34:38.067050 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/BatchNorm/beta; prev_var_name: Unchanged. I0415 07:34:38.657821 140368878327552 basic_session_run_hooks.py:527] Create CheckpointSaverHook. I0415 07:34:45.316857 140368878327552 monitored_session.py:222] Graph was finalized. 2019-04-15 07:34:45.317978: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2019-04-15 07:34:45.322541: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2304000000 Hz. 2019-04-15 07:34:45.323247: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x175ebd50 executing computations on platform Host. Devices:. 2019-04-15 07:34:45.323718: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. I0415 07:34:52.317267 140368878327552 session_manager.py:491] Running local_init_op. I0415 07:34:52.780421 140368878327552 session_manager.py:493] Done running local_init_op. I0415 07:35:11.098021 140368878327552 basic_session_run_hooks.py:594] Saving checkpoints for 0 into /data/output/trained_model/model.ckpt. 2019-04-15 07:35:25.684776: W tensorflow/core/framework/allocator.cc:124] Allocation of 16972800 exceeds 10% of system memory. 2019-04-15 07:35:25.703962: W tensorflow/core/framework/allocator.cc:124] Allocation of 22077440 exceeds 10% of system memory. 2019-04-15 07:35:25.779693: W tensorflow/core/framework/allocator.cc:124] Allocation of 22077440 exceeds 10% of system memory. 2019-04-15 07:35:25.836485: W tensorflow/core/framework/allocator.cc:124] Allocation of 20791296 exceeds 10% of system memory. 2019-04-15 07:35:26.261185: W tensorflow/core/framework/allocator.cc:124] Allocation of 20791296 exceeds ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:123160,deployability,servic,service,123160,"_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged. I0415 07:34:38.067050 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/BatchNorm/beta; prev_var_name: Unchanged. I0415 07:34:38.657821 140368878327552 basic_session_run_hooks.py:527] Create CheckpointSaverHook. I0415 07:34:45.316857 140368878327552 monitored_session.py:222] Graph was finalized. 2019-04-15 07:34:45.317978: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2019-04-15 07:34:45.322541: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2304000000 Hz. 2019-04-15 07:34:45.323247: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x175ebd50 executing computations on platform Host. Devices:. 2019-04-15 07:34:45.323718: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. I0415 07:34:52.317267 140368878327552 session_manager.py:491] Running local_init_op. I0415 07:34:52.780421 140368878327552 session_manager.py:493] Done running local_init_op. I0415 07:35:11.098021 140368878327552 basic_session_run_hooks.py:594] Saving checkpoints for 0 into /data/output/trained_model/model.ckpt. 2019-04-15 07:35:25.684776: W tensorflow/core/framework/allocator.cc:124] Allocation of 16972800 exceeds 10% of system memory. 2019-04-15 07:35:25.703962: W tensorflow/core/framework/allocator.cc:124] Allocation of 22077440 exceeds 10% of system memory. 2019-04-15 07:35:25.779693: W tensorflow/core/framework/allocator.cc:124] Allocation of 22077440 exceeds 10% of system memory. 2019-04-15 07:35:25.836485: W tensorflow/core/framework/allocator.cc:124] Allocation of 20791296 exceeds 10% of system memory. 2019-04-15 07:35:26.261185: W tensorflow/core/framework/allocator.cc:124] Allocation of 20791296 exceeds 10% of s",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:124541,deployability,modul,module,124541,"flow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2304000000 Hz. 2019-04-15 07:34:45.323247: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x175ebd50 executing computations on platform Host. Devices:. 2019-04-15 07:34:45.323718: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. I0415 07:34:52.317267 140368878327552 session_manager.py:491] Running local_init_op. I0415 07:34:52.780421 140368878327552 session_manager.py:493] Done running local_init_op. I0415 07:35:11.098021 140368878327552 basic_session_run_hooks.py:594] Saving checkpoints for 0 into /data/output/trained_model/model.ckpt. 2019-04-15 07:35:25.684776: W tensorflow/core/framework/allocator.cc:124] Allocation of 16972800 exceeds 10% of system memory. 2019-04-15 07:35:25.703962: W tensorflow/core/framework/allocator.cc:124] Allocation of 22077440 exceeds 10% of system memory. 2019-04-15 07:35:25.779693: W tensorflow/core/framework/allocator.cc:124] Allocation of 22077440 exceeds 10% of system memory. 2019-04-15 07:35:25.836485: W tensorflow/core/framework/allocator.cc:124] Allocation of 20791296 exceeds 10% of system memory. 2019-04-15 07:35:26.261185: W tensorflow/core/framework/allocator.cc:124] Allocation of 20791296 exceeds 10% of system memory. I0415 07:35:36.190104 140368878327552 basic_session_run_hooks.py:249] loss = 0.039415985, step = 1. I0415 07:36:50.684401 140368878327552 basic_session_run_hooks.py:594] Saving checkpoints for 10 into /data/output/trained_model/model.ckpt. I0415 07:37:20.374263 140368878327552 estimator.py:359] Loss for final step: 0.0037548377. WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0. For more information, please see:. * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md. * https://github.com/tensorflow/addons. If you depend on functionality not listed there, please file an issue. real	3m6.726s. user	2m58.710s. sys	0m25.780s.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:124757,deployability,depend,depend,124757,"flow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2304000000 Hz. 2019-04-15 07:34:45.323247: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x175ebd50 executing computations on platform Host. Devices:. 2019-04-15 07:34:45.323718: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. I0415 07:34:52.317267 140368878327552 session_manager.py:491] Running local_init_op. I0415 07:34:52.780421 140368878327552 session_manager.py:493] Done running local_init_op. I0415 07:35:11.098021 140368878327552 basic_session_run_hooks.py:594] Saving checkpoints for 0 into /data/output/trained_model/model.ckpt. 2019-04-15 07:35:25.684776: W tensorflow/core/framework/allocator.cc:124] Allocation of 16972800 exceeds 10% of system memory. 2019-04-15 07:35:25.703962: W tensorflow/core/framework/allocator.cc:124] Allocation of 22077440 exceeds 10% of system memory. 2019-04-15 07:35:25.779693: W tensorflow/core/framework/allocator.cc:124] Allocation of 22077440 exceeds 10% of system memory. 2019-04-15 07:35:25.836485: W tensorflow/core/framework/allocator.cc:124] Allocation of 20791296 exceeds 10% of system memory. 2019-04-15 07:35:26.261185: W tensorflow/core/framework/allocator.cc:124] Allocation of 20791296 exceeds 10% of system memory. I0415 07:35:36.190104 140368878327552 basic_session_run_hooks.py:249] loss = 0.039415985, step = 1. I0415 07:36:50.684401 140368878327552 basic_session_run_hooks.py:594] Saving checkpoints for 10 into /data/output/trained_model/model.ckpt. I0415 07:37:20.374263 140368878327552 estimator.py:359] Loss for final step: 0.0037548377. WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0. For more information, please see:. * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md. * https://github.com/tensorflow/addons. If you depend on functionality not listed there, please file an issue. real	3m6.726s. user	2m58.710s. sys	0m25.780s.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:132,energy efficiency,model,model,132,"ValueError: Must specify steps > 0, given: 0 deepvariant version: 0.8; Hi, I am new to deepvariant. I tried to train my deepvariant model with our own data within docker on my personal computer. Up until model_eval and model_train, every process was working fine. When I try to run model_train and model_eval in my computer, model_train seemed to work but model_eval returned ValueError: Must specify steps > 0, given: 0. The error came from estimator.py file in tensorflow. In Advanced Case Study: Train a customized SNP and small indel variant caller for BGISEQ-500 data, it says that . > At the same time, start model_eval on CPUs. Since I don't have a TPU, so the following is the code I used and attempt to run model_train and model_eval on CPU simultaneously. The following is the code I used:. `(time python /home/bin/model_train.zip \. --dataset_config_pbtxt=/data/output/training_data/customized_training/training_set_with_label_shuffled/training_set.dataset_config.pbtxt \. --train_dir=/data/output/trained_model \. --model_name=""inception_v3"" \. --number_of_steps=10 \. --save_interval_secs=3000 \. --batch_size=32 \. --learning_rate=0.008 \. --start_from_checkpoint=/home/models/model.ckpt) >/data/output/log/model_training/model_train.log 2>&1\. & (time python2 /home/bin/model_eval.zip \. --dataset_config_pbtxt=/data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.dataset_config.pbtxt \. --checkpoint_dir=/data/output/trained_model \. --number_of_steps=10 \. --batch_size=32) >/data/output/log/model_training/model_eval.log 2>&1`. The following is the message from model_eval log :. > I0415 07:34:19.493486 140713377441536 model_eval.py:141] Set KMP_BLOCKTIME to 0. I0415 07:34:19.495834 140713377441536 model_eval.py:177] Running fixed eval for: /data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.dataset_config.pbtxt. W0415 07:34:19.536698 140713377441536 deprecation.py:323] From /tmp/Bazel.r",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:442,energy efficiency,estimat,estimator,442,"ValueError: Must specify steps > 0, given: 0 deepvariant version: 0.8; Hi, I am new to deepvariant. I tried to train my deepvariant model with our own data within docker on my personal computer. Up until model_eval and model_train, every process was working fine. When I try to run model_train and model_eval in my computer, model_train seemed to work but model_eval returned ValueError: Must specify steps > 0, given: 0. The error came from estimator.py file in tensorflow. In Advanced Case Study: Train a customized SNP and small indel variant caller for BGISEQ-500 data, it says that . > At the same time, start model_eval on CPUs. Since I don't have a TPU, so the following is the code I used and attempt to run model_train and model_eval on CPU simultaneously. The following is the code I used:. `(time python /home/bin/model_train.zip \. --dataset_config_pbtxt=/data/output/training_data/customized_training/training_set_with_label_shuffled/training_set.dataset_config.pbtxt \. --train_dir=/data/output/trained_model \. --model_name=""inception_v3"" \. --number_of_steps=10 \. --save_interval_secs=3000 \. --batch_size=32 \. --learning_rate=0.008 \. --start_from_checkpoint=/home/models/model.ckpt) >/data/output/log/model_training/model_train.log 2>&1\. & (time python2 /home/bin/model_eval.zip \. --dataset_config_pbtxt=/data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.dataset_config.pbtxt \. --checkpoint_dir=/data/output/trained_model \. --number_of_steps=10 \. --batch_size=32) >/data/output/log/model_training/model_eval.log 2>&1`. The following is the message from model_eval log :. > I0415 07:34:19.493486 140713377441536 model_eval.py:141] Set KMP_BLOCKTIME to 0. I0415 07:34:19.495834 140713377441536 model_eval.py:177] Running fixed eval for: /data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.dataset_config.pbtxt. W0415 07:34:19.536698 140713377441536 deprecation.py:323] From /tmp/Bazel.r",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:629,energy efficiency,CPU,CPUs,629,"ValueError: Must specify steps > 0, given: 0 deepvariant version: 0.8; Hi, I am new to deepvariant. I tried to train my deepvariant model with our own data within docker on my personal computer. Up until model_eval and model_train, every process was working fine. When I try to run model_train and model_eval in my computer, model_train seemed to work but model_eval returned ValueError: Must specify steps > 0, given: 0. The error came from estimator.py file in tensorflow. In Advanced Case Study: Train a customized SNP and small indel variant caller for BGISEQ-500 data, it says that . > At the same time, start model_eval on CPUs. Since I don't have a TPU, so the following is the code I used and attempt to run model_train and model_eval on CPU simultaneously. The following is the code I used:. `(time python /home/bin/model_train.zip \. --dataset_config_pbtxt=/data/output/training_data/customized_training/training_set_with_label_shuffled/training_set.dataset_config.pbtxt \. --train_dir=/data/output/trained_model \. --model_name=""inception_v3"" \. --number_of_steps=10 \. --save_interval_secs=3000 \. --batch_size=32 \. --learning_rate=0.008 \. --start_from_checkpoint=/home/models/model.ckpt) >/data/output/log/model_training/model_train.log 2>&1\. & (time python2 /home/bin/model_eval.zip \. --dataset_config_pbtxt=/data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.dataset_config.pbtxt \. --checkpoint_dir=/data/output/trained_model \. --number_of_steps=10 \. --batch_size=32) >/data/output/log/model_training/model_eval.log 2>&1`. The following is the message from model_eval log :. > I0415 07:34:19.493486 140713377441536 model_eval.py:141] Set KMP_BLOCKTIME to 0. I0415 07:34:19.495834 140713377441536 model_eval.py:177] Running fixed eval for: /data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.dataset_config.pbtxt. W0415 07:34:19.536698 140713377441536 deprecation.py:323] From /tmp/Bazel.r",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:746,energy efficiency,CPU,CPU,746,"ValueError: Must specify steps > 0, given: 0 deepvariant version: 0.8; Hi, I am new to deepvariant. I tried to train my deepvariant model with our own data within docker on my personal computer. Up until model_eval and model_train, every process was working fine. When I try to run model_train and model_eval in my computer, model_train seemed to work but model_eval returned ValueError: Must specify steps > 0, given: 0. The error came from estimator.py file in tensorflow. In Advanced Case Study: Train a customized SNP and small indel variant caller for BGISEQ-500 data, it says that . > At the same time, start model_eval on CPUs. Since I don't have a TPU, so the following is the code I used and attempt to run model_train and model_eval on CPU simultaneously. The following is the code I used:. `(time python /home/bin/model_train.zip \. --dataset_config_pbtxt=/data/output/training_data/customized_training/training_set_with_label_shuffled/training_set.dataset_config.pbtxt \. --train_dir=/data/output/trained_model \. --model_name=""inception_v3"" \. --number_of_steps=10 \. --save_interval_secs=3000 \. --batch_size=32 \. --learning_rate=0.008 \. --start_from_checkpoint=/home/models/model.ckpt) >/data/output/log/model_training/model_train.log 2>&1\. & (time python2 /home/bin/model_eval.zip \. --dataset_config_pbtxt=/data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.dataset_config.pbtxt \. --checkpoint_dir=/data/output/trained_model \. --number_of_steps=10 \. --batch_size=32) >/data/output/log/model_training/model_eval.log 2>&1`. The following is the message from model_eval log :. > I0415 07:34:19.493486 140713377441536 model_eval.py:141] Set KMP_BLOCKTIME to 0. I0415 07:34:19.495834 140713377441536 model_eval.py:177] Running fixed eval for: /data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.dataset_config.pbtxt. W0415 07:34:19.536698 140713377441536 deprecation.py:323] From /tmp/Bazel.r",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:1184,energy efficiency,model,models,1184,"mputer. Up until model_eval and model_train, every process was working fine. When I try to run model_train and model_eval in my computer, model_train seemed to work but model_eval returned ValueError: Must specify steps > 0, given: 0. The error came from estimator.py file in tensorflow. In Advanced Case Study: Train a customized SNP and small indel variant caller for BGISEQ-500 data, it says that . > At the same time, start model_eval on CPUs. Since I don't have a TPU, so the following is the code I used and attempt to run model_train and model_eval on CPU simultaneously. The following is the code I used:. `(time python /home/bin/model_train.zip \. --dataset_config_pbtxt=/data/output/training_data/customized_training/training_set_with_label_shuffled/training_set.dataset_config.pbtxt \. --train_dir=/data/output/trained_model \. --model_name=""inception_v3"" \. --number_of_steps=10 \. --save_interval_secs=3000 \. --batch_size=32 \. --learning_rate=0.008 \. --start_from_checkpoint=/home/models/model.ckpt) >/data/output/log/model_training/model_train.log 2>&1\. & (time python2 /home/bin/model_eval.zip \. --dataset_config_pbtxt=/data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.dataset_config.pbtxt \. --checkpoint_dir=/data/output/trained_model \. --number_of_steps=10 \. --batch_size=32) >/data/output/log/model_training/model_eval.log 2>&1`. The following is the message from model_eval log :. > I0415 07:34:19.493486 140713377441536 model_eval.py:141] Set KMP_BLOCKTIME to 0. I0415 07:34:19.495834 140713377441536 model_eval.py:177] Running fixed eval for: /data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.dataset_config.pbtxt. W0415 07:34:19.536698 140713377441536 deprecation.py:323] From /tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py:307: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:1191,energy efficiency,model,model,1191," Up until model_eval and model_train, every process was working fine. When I try to run model_train and model_eval in my computer, model_train seemed to work but model_eval returned ValueError: Must specify steps > 0, given: 0. The error came from estimator.py file in tensorflow. In Advanced Case Study: Train a customized SNP and small indel variant caller for BGISEQ-500 data, it says that . > At the same time, start model_eval on CPUs. Since I don't have a TPU, so the following is the code I used and attempt to run model_train and model_eval on CPU simultaneously. The following is the code I used:. `(time python /home/bin/model_train.zip \. --dataset_config_pbtxt=/data/output/training_data/customized_training/training_set_with_label_shuffled/training_set.dataset_config.pbtxt \. --train_dir=/data/output/trained_model \. --model_name=""inception_v3"" \. --number_of_steps=10 \. --save_interval_secs=3000 \. --batch_size=32 \. --learning_rate=0.008 \. --start_from_checkpoint=/home/models/model.ckpt) >/data/output/log/model_training/model_train.log 2>&1\. & (time python2 /home/bin/model_eval.zip \. --dataset_config_pbtxt=/data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.dataset_config.pbtxt \. --checkpoint_dir=/data/output/trained_model \. --number_of_steps=10 \. --batch_size=32) >/data/output/log/model_training/model_eval.log 2>&1`. The following is the message from model_eval log :. > I0415 07:34:19.493486 140713377441536 model_eval.py:141] Set KMP_BLOCKTIME to 0. I0415 07:34:19.495834 140713377441536 model_eval.py:177] Running fixed eval for: /data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.dataset_config.pbtxt. W0415 07:34:19.536698 140713377441536 deprecation.py:323] From /tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py:307: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a futur",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:2594,energy efficiency,model,model,2594,"lowing is the message from model_eval log :. > I0415 07:34:19.493486 140713377441536 model_eval.py:141] Set KMP_BLOCKTIME to 0. I0415 07:34:19.495834 140713377441536 model_eval.py:177] Running fixed eval for: /data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.dataset_config.pbtxt. W0415 07:34:19.536698 140713377441536 deprecation.py:323] From /tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py:307: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version. Instructions for updating:. Use eager execution and: . `tf.data.TFRecordDataset(path)`. I0415 07:34:19.584646 140713377441536 model_eval.py:190] Running evaluations on DeepVariantInput(name=HG001, input_file_spec=/data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz, num_examples=8, mode=eval with model DeepVariantModel(name=inception_v3). I0415 07:34:19.585236 140713377441536 model_eval.py:198] Dataset has 8 samples, doing eval over 0; max_examples is 1000000, num_batches is 0. I0415 07:34:19.585772 140713377441536 modeling.py:357] Initializing model with random parameters. I0415 07:34:19.586724 140713377441536 estimator.py:201] Using config: {'_save_checkpoints_secs': 1000, '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ffa3b29ad50>, '_model_dir': '/data/output/trained_model', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:2817,energy efficiency,model,modeling,2817,"aining_data/customized_training/validation_set_with_label_shuffled/validation_set.dataset_config.pbtxt. W0415 07:34:19.536698 140713377441536 deprecation.py:323] From /tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py:307: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version. Instructions for updating:. Use eager execution and: . `tf.data.TFRecordDataset(path)`. I0415 07:34:19.584646 140713377441536 model_eval.py:190] Running evaluations on DeepVariantInput(name=HG001, input_file_spec=/data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz, num_examples=8, mode=eval with model DeepVariantModel(name=inception_v3). I0415 07:34:19.585236 140713377441536 model_eval.py:198] Dataset has 8 samples, doing eval over 0; max_examples is 1000000, num_batches is 0. I0415 07:34:19.585772 140713377441536 modeling.py:357] Initializing model with random parameters. I0415 07:34:19.586724 140713377441536 estimator.py:201] Using config: {'_save_checkpoints_secs': 1000, '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ffa3b29ad50>, '_model_dir': '/data/output/trained_model', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_cluster': 0, '_master': ''}. I0415 07:34:19.587389 140713377441536 evaluation.py:189] Waiting for",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:2847,energy efficiency,model,model,2847,"ng/validation_set_with_label_shuffled/validation_set.dataset_config.pbtxt. W0415 07:34:19.536698 140713377441536 deprecation.py:323] From /tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py:307: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version. Instructions for updating:. Use eager execution and: . `tf.data.TFRecordDataset(path)`. I0415 07:34:19.584646 140713377441536 model_eval.py:190] Running evaluations on DeepVariantInput(name=HG001, input_file_spec=/data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz, num_examples=8, mode=eval with model DeepVariantModel(name=inception_v3). I0415 07:34:19.585236 140713377441536 model_eval.py:198] Dataset has 8 samples, doing eval over 0; max_examples is 1000000, num_batches is 0. I0415 07:34:19.585772 140713377441536 modeling.py:357] Initializing model with random parameters. I0415 07:34:19.586724 140713377441536 estimator.py:201] Using config: {'_save_checkpoints_secs': 1000, '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ffa3b29ad50>, '_model_dir': '/data/output/trained_model', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_cluster': 0, '_master': ''}. I0415 07:34:19.587389 140713377441536 evaluation.py:189] Waiting for new checkpoint at /data/outp",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:2915,energy efficiency,estimat,estimator,2915,"txt. W0415 07:34:19.536698 140713377441536 deprecation.py:323] From /tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py:307: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version. Instructions for updating:. Use eager execution and: . `tf.data.TFRecordDataset(path)`. I0415 07:34:19.584646 140713377441536 model_eval.py:190] Running evaluations on DeepVariantInput(name=HG001, input_file_spec=/data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz, num_examples=8, mode=eval with model DeepVariantModel(name=inception_v3). I0415 07:34:19.585236 140713377441536 model_eval.py:198] Dataset has 8 samples, doing eval over 0; max_examples is 1000000, num_batches is 0. I0415 07:34:19.585772 140713377441536 modeling.py:357] Initializing model with random parameters. I0415 07:34:19.586724 140713377441536 estimator.py:201] Using config: {'_save_checkpoints_secs': 1000, '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ffa3b29ad50>, '_model_dir': '/data/output/trained_model', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_cluster': 0, '_master': ''}. I0415 07:34:19.587389 140713377441536 evaluation.py:189] Waiting for new checkpoint at /data/output/trained_model. I0415 07:35:14.785435 140713377441536 evaluation.py:",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:3976,energy efficiency,model,model,3976," '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ffa3b29ad50>, '_model_dir': '/data/output/trained_model', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_cluster': 0, '_master': ''}. I0415 07:34:19.587389 140713377441536 evaluation.py:189] Waiting for new checkpoint at /data/output/trained_model. I0415 07:35:14.785435 140713377441536 evaluation.py:198] Found new checkpoint at /data/output/trained_model/model.ckpt-0. I0415 07:35:14.787266 140713377441536 model_eval.py:225] Starting to evaluate. WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0. For more information, please see:. * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md. * https://github.com/tensorflow/addons. If you depend on functionality not listed there, please file an issue. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/deepvariant/model_eval.py"", line 362, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/deepvariant/model_eval.py"", line 154, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/deepvariant/model_eval.py"", line 235, in eval_loop. name=eval_name). File ""/usr/local/lib/python2.7/dist-pack",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:5012,energy efficiency,estimat,estimator,5012,"3377441536 model_eval.py:225] Starting to evaluate. WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0. For more information, please see:. * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md. * https://github.com/tensorflow/addons. If you depend on functionality not listed there, please file an issue. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/deepvariant/model_eval.py"", line 362, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/deepvariant/model_eval.py"", line 154, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/deepvariant/model_eval.py"", line 235, in eval_loop. name=eval_name). File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 469, in evaluate. name=name). File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 481, in _actual_eval. hooks.extend(self._convert_eval_steps_to_hooks(steps)). File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 529, in _convert_eval_steps_to_hooks. raise ValueError('Must specify steps > 0, given: {}'.format(steps)). ValueError: Must specify steps > 0, given: 0. real	0m58.116s. user	0m1.590s. sys	0m0.590s. The following is my docker setting:. > FROM ubuntu:16.04. LABEL maintainer=""aaronarchiblad@gmail.com"". ENV DV_USE_GCP_OPTIMIZED_TF_WHL=0. ADD ./deepvariant /home/deepvariant. ADD ./bin /home/bin. ADD ./models /home/models. ADD ./oss_clif.ubuntu-16.latest.tgz /. WORKDIR /home/deepvariant. RUN /home/deepvariant/run-prereq.sh. RUN apt-get -y install python2.7. RUN apt-get -y install python-dev python-pip. RUN apt-get -y install parallel. RUN python2 -m pip insta",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:5022,energy efficiency,estimat,estimator,5022," model_eval.py:225] Starting to evaluate. WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0. For more information, please see:. * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md. * https://github.com/tensorflow/addons. If you depend on functionality not listed there, please file an issue. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/deepvariant/model_eval.py"", line 362, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/deepvariant/model_eval.py"", line 154, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/deepvariant/model_eval.py"", line 235, in eval_loop. name=eval_name). File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 469, in evaluate. name=name). File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 481, in _actual_eval. hooks.extend(self._convert_eval_steps_to_hooks(steps)). File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 529, in _convert_eval_steps_to_hooks. raise ValueError('Must specify steps > 0, given: {}'.format(steps)). ValueError: Must specify steps > 0, given: 0. real	0m58.116s. user	0m1.590s. sys	0m0.590s. The following is my docker setting:. > FROM ubuntu:16.04. LABEL maintainer=""aaronarchiblad@gmail.com"". ENV DV_USE_GCP_OPTIMIZED_TF_WHL=0. ADD ./deepvariant /home/deepvariant. ADD ./bin /home/bin. ADD ./models /home/models. ADD ./oss_clif.ubuntu-16.latest.tgz /. WORKDIR /home/deepvariant. RUN /home/deepvariant/run-prereq.sh. RUN apt-get -y install python2.7. RUN apt-get -y install python-dev python-pip. RUN apt-get -y install parallel. RUN python2 -m pip install --upgra",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:5145,energy efficiency,estimat,estimator,5145,"r more information, please see:. * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md. * https://github.com/tensorflow/addons. If you depend on functionality not listed there, please file an issue. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/deepvariant/model_eval.py"", line 362, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/deepvariant/model_eval.py"", line 154, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/deepvariant/model_eval.py"", line 235, in eval_loop. name=eval_name). File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 469, in evaluate. name=name). File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 481, in _actual_eval. hooks.extend(self._convert_eval_steps_to_hooks(steps)). File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 529, in _convert_eval_steps_to_hooks. raise ValueError('Must specify steps > 0, given: {}'.format(steps)). ValueError: Must specify steps > 0, given: 0. real	0m58.116s. user	0m1.590s. sys	0m0.590s. The following is my docker setting:. > FROM ubuntu:16.04. LABEL maintainer=""aaronarchiblad@gmail.com"". ENV DV_USE_GCP_OPTIMIZED_TF_WHL=0. ADD ./deepvariant /home/deepvariant. ADD ./bin /home/bin. ADD ./models /home/models. ADD ./oss_clif.ubuntu-16.latest.tgz /. WORKDIR /home/deepvariant. RUN /home/deepvariant/run-prereq.sh. RUN apt-get -y install python2.7. RUN apt-get -y install python-dev python-pip. RUN apt-get -y install parallel. RUN python2 -m pip install --upgrade --force-reinstall pip. RUN python2 -m pip install apache-beam. RUN python2 -m pip install google-cloud-dataflow. Just in",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:5155,energy efficiency,estimat,estimator,5155,"ormation, please see:. * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md. * https://github.com/tensorflow/addons. If you depend on functionality not listed there, please file an issue. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/deepvariant/model_eval.py"", line 362, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/deepvariant/model_eval.py"", line 154, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/deepvariant/model_eval.py"", line 235, in eval_loop. name=eval_name). File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 469, in evaluate. name=name). File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 481, in _actual_eval. hooks.extend(self._convert_eval_steps_to_hooks(steps)). File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 529, in _convert_eval_steps_to_hooks. raise ValueError('Must specify steps > 0, given: {}'.format(steps)). ValueError: Must specify steps > 0, given: 0. real	0m58.116s. user	0m1.590s. sys	0m0.590s. The following is my docker setting:. > FROM ubuntu:16.04. LABEL maintainer=""aaronarchiblad@gmail.com"". ENV DV_USE_GCP_OPTIMIZED_TF_WHL=0. ADD ./deepvariant /home/deepvariant. ADD ./bin /home/bin. ADD ./models /home/models. ADD ./oss_clif.ubuntu-16.latest.tgz /. WORKDIR /home/deepvariant. RUN /home/deepvariant/run-prereq.sh. RUN apt-get -y install python2.7. RUN apt-get -y install python-dev python-pip. RUN apt-get -y install parallel. RUN python2 -m pip install --upgrade --force-reinstall pip. RUN python2 -m pip install apache-beam. RUN python2 -m pip install google-cloud-dataflow. Just in case, if ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:5326,energy efficiency,estimat,estimator,5326,"tionality not listed there, please file an issue. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/deepvariant/model_eval.py"", line 362, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/deepvariant/model_eval.py"", line 154, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/deepvariant/model_eval.py"", line 235, in eval_loop. name=eval_name). File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 469, in evaluate. name=name). File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 481, in _actual_eval. hooks.extend(self._convert_eval_steps_to_hooks(steps)). File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 529, in _convert_eval_steps_to_hooks. raise ValueError('Must specify steps > 0, given: {}'.format(steps)). ValueError: Must specify steps > 0, given: 0. real	0m58.116s. user	0m1.590s. sys	0m0.590s. The following is my docker setting:. > FROM ubuntu:16.04. LABEL maintainer=""aaronarchiblad@gmail.com"". ENV DV_USE_GCP_OPTIMIZED_TF_WHL=0. ADD ./deepvariant /home/deepvariant. ADD ./bin /home/bin. ADD ./models /home/models. ADD ./oss_clif.ubuntu-16.latest.tgz /. WORKDIR /home/deepvariant. RUN /home/deepvariant/run-prereq.sh. RUN apt-get -y install python2.7. RUN apt-get -y install python-dev python-pip. RUN apt-get -y install parallel. RUN python2 -m pip install --upgrade --force-reinstall pip. RUN python2 -m pip install apache-beam. RUN python2 -m pip install google-cloud-dataflow. Just in case, if this is needed, I also quote the message from model_train, not sure if this will be useful. . > I0415 07:34:19.468628 140368878327552 model_train.py:310] Set KMP_BLOCKTIME",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:5336,energy efficiency,estimat,estimator,5336,"not listed there, please file an issue. Traceback (most recent call last):. File ""/tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/deepvariant/model_eval.py"", line 362, in <module>. tf.app.run(). File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run. _sys.exit(main(argv)). File ""/tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/deepvariant/model_eval.py"", line 154, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/deepvariant/model_eval.py"", line 235, in eval_loop. name=eval_name). File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 469, in evaluate. name=name). File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 481, in _actual_eval. hooks.extend(self._convert_eval_steps_to_hooks(steps)). File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 529, in _convert_eval_steps_to_hooks. raise ValueError('Must specify steps > 0, given: {}'.format(steps)). ValueError: Must specify steps > 0, given: 0. real	0m58.116s. user	0m1.590s. sys	0m0.590s. The following is my docker setting:. > FROM ubuntu:16.04. LABEL maintainer=""aaronarchiblad@gmail.com"". ENV DV_USE_GCP_OPTIMIZED_TF_WHL=0. ADD ./deepvariant /home/deepvariant. ADD ./bin /home/bin. ADD ./models /home/models. ADD ./oss_clif.ubuntu-16.latest.tgz /. WORKDIR /home/deepvariant. RUN /home/deepvariant/run-prereq.sh. RUN apt-get -y install python2.7. RUN apt-get -y install python-dev python-pip. RUN apt-get -y install parallel. RUN python2 -m pip install --upgrade --force-reinstall pip. RUN python2 -m pip install apache-beam. RUN python2 -m pip install google-cloud-dataflow. Just in case, if this is needed, I also quote the message from model_train, not sure if this will be useful. . > I0415 07:34:19.468628 140368878327552 model_train.py:310] Set KMP_BLOCKTIME to 0. I04",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:5756,energy efficiency,model,models,5756,""", line 154, in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/deepvariant/model_eval.py"", line 235, in eval_loop. name=eval_name). File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 469, in evaluate. name=name). File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 481, in _actual_eval. hooks.extend(self._convert_eval_steps_to_hooks(steps)). File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 529, in _convert_eval_steps_to_hooks. raise ValueError('Must specify steps > 0, given: {}'.format(steps)). ValueError: Must specify steps > 0, given: 0. real	0m58.116s. user	0m1.590s. sys	0m0.590s. The following is my docker setting:. > FROM ubuntu:16.04. LABEL maintainer=""aaronarchiblad@gmail.com"". ENV DV_USE_GCP_OPTIMIZED_TF_WHL=0. ADD ./deepvariant /home/deepvariant. ADD ./bin /home/bin. ADD ./models /home/models. ADD ./oss_clif.ubuntu-16.latest.tgz /. WORKDIR /home/deepvariant. RUN /home/deepvariant/run-prereq.sh. RUN apt-get -y install python2.7. RUN apt-get -y install python-dev python-pip. RUN apt-get -y install parallel. RUN python2 -m pip install --upgrade --force-reinstall pip. RUN python2 -m pip install apache-beam. RUN python2 -m pip install google-cloud-dataflow. Just in case, if this is needed, I also quote the message from model_train, not sure if this will be useful. . > I0415 07:34:19.468628 140368878327552 model_train.py:310] Set KMP_BLOCKTIME to 0. I0415 07:34:19.469649 140368878327552 model_train.py:244] TF_CONFIG None. W0415 07:34:19.491456 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py:307: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version. Instructions for updating:. Use eager execution and: . `tf.data.TFR",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:5769,energy efficiency,model,models,5769,"in main. use_tpu=FLAGS.use_tpu,. File ""/tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/deepvariant/model_eval.py"", line 235, in eval_loop. name=eval_name). File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 469, in evaluate. name=name). File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 481, in _actual_eval. hooks.extend(self._convert_eval_steps_to_hooks(steps)). File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 529, in _convert_eval_steps_to_hooks. raise ValueError('Must specify steps > 0, given: {}'.format(steps)). ValueError: Must specify steps > 0, given: 0. real	0m58.116s. user	0m1.590s. sys	0m0.590s. The following is my docker setting:. > FROM ubuntu:16.04. LABEL maintainer=""aaronarchiblad@gmail.com"". ENV DV_USE_GCP_OPTIMIZED_TF_WHL=0. ADD ./deepvariant /home/deepvariant. ADD ./bin /home/bin. ADD ./models /home/models. ADD ./oss_clif.ubuntu-16.latest.tgz /. WORKDIR /home/deepvariant. RUN /home/deepvariant/run-prereq.sh. RUN apt-get -y install python2.7. RUN apt-get -y install python-dev python-pip. RUN apt-get -y install parallel. RUN python2 -m pip install --upgrade --force-reinstall pip. RUN python2 -m pip install apache-beam. RUN python2 -m pip install google-cloud-dataflow. Just in case, if this is needed, I also quote the message from model_train, not sure if this will be useful. . > I0415 07:34:19.468628 140368878327552 model_train.py:310] Set KMP_BLOCKTIME to 0. I0415 07:34:19.469649 140368878327552 model_train.py:244] TF_CONFIG None. W0415 07:34:19.491456 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py:307: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version. Instructions for updating:. Use eager execution and: . `tf.data.TFRecordDataset(",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:6127,energy efficiency,cloud,cloud-dataflow,6127,"tor/python/estimator/estimator.py"", line 481, in _actual_eval. hooks.extend(self._convert_eval_steps_to_hooks(steps)). File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 529, in _convert_eval_steps_to_hooks. raise ValueError('Must specify steps > 0, given: {}'.format(steps)). ValueError: Must specify steps > 0, given: 0. real	0m58.116s. user	0m1.590s. sys	0m0.590s. The following is my docker setting:. > FROM ubuntu:16.04. LABEL maintainer=""aaronarchiblad@gmail.com"". ENV DV_USE_GCP_OPTIMIZED_TF_WHL=0. ADD ./deepvariant /home/deepvariant. ADD ./bin /home/bin. ADD ./models /home/models. ADD ./oss_clif.ubuntu-16.latest.tgz /. WORKDIR /home/deepvariant. RUN /home/deepvariant/run-prereq.sh. RUN apt-get -y install python2.7. RUN apt-get -y install python-dev python-pip. RUN apt-get -y install parallel. RUN python2 -m pip install --upgrade --force-reinstall pip. RUN python2 -m pip install apache-beam. RUN python2 -m pip install google-cloud-dataflow. Just in case, if this is needed, I also quote the message from model_train, not sure if this will be useful. . > I0415 07:34:19.468628 140368878327552 model_train.py:310] Set KMP_BLOCKTIME to 0. I0415 07:34:19.469649 140368878327552 model_train.py:244] TF_CONFIG None. W0415 07:34:19.491456 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py:307: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version. Instructions for updating:. Use eager execution and: . `tf.data.TFRecordDataset(path)`. I0415 07:34:19.549700 140368878327552 model_train.py:193] Running training on DeepVariantInput(name=HG001, input_file_spec=/data/output/training_data/customized_training/training_set_with_label_shuffled/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz, num_examples=33, mode=train with model inception_v3 and tpu False. I0415 07:34:19.550825 1",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:7077,energy efficiency,model,model,7077,"apache-beam. RUN python2 -m pip install google-cloud-dataflow. Just in case, if this is needed, I also quote the message from model_train, not sure if this will be useful. . > I0415 07:34:19.468628 140368878327552 model_train.py:310] Set KMP_BLOCKTIME to 0. I0415 07:34:19.469649 140368878327552 model_train.py:244] TF_CONFIG None. W0415 07:34:19.491456 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py:307: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version. Instructions for updating:. Use eager execution and: . `tf.data.TFRecordDataset(path)`. I0415 07:34:19.549700 140368878327552 model_train.py:193] Running training on DeepVariantInput(name=HG001, input_file_spec=/data/output/training_data/customized_training/training_set_with_label_shuffled/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz, num_examples=33, mode=train with model inception_v3 and tpu False. I0415 07:34:19.550825 140368878327552 model_train.py:196] Batches per epoch 1. I0415 07:34:19.551630 140368878327552 modeling.py:330] Initializing model from checkpoint at /home/models/model.ckpt. I0415 07:34:19.564393 140368878327552 modeling.py:336] The model checkpoint to warm start from has the same number of classes. If this is in training, we will clear excluded_scopes_for_incompatible_shapes so we include everything for warm starting.... I0415 07:34:19.568434 140368878327552 estimator.py:201] Using config: {'_save_checkpoints_secs': 3000, '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7faa056a5210>, '_model_dir': '/data/output/trained_model', '_protocol': None, '_save_checkpoints_steps': None, '",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:7228,energy efficiency,model,modeling,7228,"his will be useful. . > I0415 07:34:19.468628 140368878327552 model_train.py:310] Set KMP_BLOCKTIME to 0. I0415 07:34:19.469649 140368878327552 model_train.py:244] TF_CONFIG None. W0415 07:34:19.491456 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py:307: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version. Instructions for updating:. Use eager execution and: . `tf.data.TFRecordDataset(path)`. I0415 07:34:19.549700 140368878327552 model_train.py:193] Running training on DeepVariantInput(name=HG001, input_file_spec=/data/output/training_data/customized_training/training_set_with_label_shuffled/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz, num_examples=33, mode=train with model inception_v3 and tpu False. I0415 07:34:19.550825 140368878327552 model_train.py:196] Batches per epoch 1. I0415 07:34:19.551630 140368878327552 modeling.py:330] Initializing model from checkpoint at /home/models/model.ckpt. I0415 07:34:19.564393 140368878327552 modeling.py:336] The model checkpoint to warm start from has the same number of classes. If this is in training, we will clear excluded_scopes_for_incompatible_shapes so we include everything for warm starting.... I0415 07:34:19.568434 140368878327552 estimator.py:201] Using config: {'_save_checkpoints_secs': 3000, '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7faa056a5210>, '_model_dir': '/data/output/trained_model', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:7258,energy efficiency,model,model,7258," 07:34:19.468628 140368878327552 model_train.py:310] Set KMP_BLOCKTIME to 0. I0415 07:34:19.469649 140368878327552 model_train.py:244] TF_CONFIG None. W0415 07:34:19.491456 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py:307: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version. Instructions for updating:. Use eager execution and: . `tf.data.TFRecordDataset(path)`. I0415 07:34:19.549700 140368878327552 model_train.py:193] Running training on DeepVariantInput(name=HG001, input_file_spec=/data/output/training_data/customized_training/training_set_with_label_shuffled/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz, num_examples=33, mode=train with model inception_v3 and tpu False. I0415 07:34:19.550825 140368878327552 model_train.py:196] Batches per epoch 1. I0415 07:34:19.551630 140368878327552 modeling.py:330] Initializing model from checkpoint at /home/models/model.ckpt. I0415 07:34:19.564393 140368878327552 modeling.py:336] The model checkpoint to warm start from has the same number of classes. If this is in training, we will clear excluded_scopes_for_incompatible_shapes so we include everything for warm starting.... I0415 07:34:19.568434 140368878327552 estimator.py:201] Using config: {'_save_checkpoints_secs': 3000, '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7faa056a5210>, '_model_dir': '/data/output/trained_model', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute':",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:7289,energy efficiency,model,models,7289,"2 model_train.py:310] Set KMP_BLOCKTIME to 0. I0415 07:34:19.469649 140368878327552 model_train.py:244] TF_CONFIG None. W0415 07:34:19.491456 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py:307: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version. Instructions for updating:. Use eager execution and: . `tf.data.TFRecordDataset(path)`. I0415 07:34:19.549700 140368878327552 model_train.py:193] Running training on DeepVariantInput(name=HG001, input_file_spec=/data/output/training_data/customized_training/training_set_with_label_shuffled/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz, num_examples=33, mode=train with model inception_v3 and tpu False. I0415 07:34:19.550825 140368878327552 model_train.py:196] Batches per epoch 1. I0415 07:34:19.551630 140368878327552 modeling.py:330] Initializing model from checkpoint at /home/models/model.ckpt. I0415 07:34:19.564393 140368878327552 modeling.py:336] The model checkpoint to warm start from has the same number of classes. If this is in training, we will clear excluded_scopes_for_incompatible_shapes so we include everything for warm starting.... I0415 07:34:19.568434 140368878327552 estimator.py:201] Using config: {'_save_checkpoints_secs': 3000, '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7faa056a5210>, '_model_dir': '/data/output/trained_model', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:7296,energy efficiency,model,model,7296,"_train.py:310] Set KMP_BLOCKTIME to 0. I0415 07:34:19.469649 140368878327552 model_train.py:244] TF_CONFIG None. W0415 07:34:19.491456 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py:307: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version. Instructions for updating:. Use eager execution and: . `tf.data.TFRecordDataset(path)`. I0415 07:34:19.549700 140368878327552 model_train.py:193] Running training on DeepVariantInput(name=HG001, input_file_spec=/data/output/training_data/customized_training/training_set_with_label_shuffled/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz, num_examples=33, mode=train with model inception_v3 and tpu False. I0415 07:34:19.550825 140368878327552 model_train.py:196] Batches per epoch 1. I0415 07:34:19.551630 140368878327552 modeling.py:330] Initializing model from checkpoint at /home/models/model.ckpt. I0415 07:34:19.564393 140368878327552 modeling.py:336] The model checkpoint to warm start from has the same number of classes. If this is in training, we will clear excluded_scopes_for_incompatible_shapes so we include everything for warm starting.... I0415 07:34:19.568434 140368878327552 estimator.py:201] Using config: {'_save_checkpoints_secs': 3000, '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7faa056a5210>, '_model_dir': '/data/output/trained_model', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_ta",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:7346,energy efficiency,model,modeling,7346,"19.469649 140368878327552 model_train.py:244] TF_CONFIG None. W0415 07:34:19.491456 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py:307: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version. Instructions for updating:. Use eager execution and: . `tf.data.TFRecordDataset(path)`. I0415 07:34:19.549700 140368878327552 model_train.py:193] Running training on DeepVariantInput(name=HG001, input_file_spec=/data/output/training_data/customized_training/training_set_with_label_shuffled/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz, num_examples=33, mode=train with model inception_v3 and tpu False. I0415 07:34:19.550825 140368878327552 model_train.py:196] Batches per epoch 1. I0415 07:34:19.551630 140368878327552 modeling.py:330] Initializing model from checkpoint at /home/models/model.ckpt. I0415 07:34:19.564393 140368878327552 modeling.py:336] The model checkpoint to warm start from has the same number of classes. If this is in training, we will clear excluded_scopes_for_incompatible_shapes so we include everything for warm starting.... I0415 07:34:19.568434 140368878327552 estimator.py:201] Using config: {'_save_checkpoints_secs': 3000, '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7faa056a5210>, '_model_dir': '/data/output/trained_model', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluati",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:7367,energy efficiency,model,model,7367,"27552 model_train.py:244] TF_CONFIG None. W0415 07:34:19.491456 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py:307: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version. Instructions for updating:. Use eager execution and: . `tf.data.TFRecordDataset(path)`. I0415 07:34:19.549700 140368878327552 model_train.py:193] Running training on DeepVariantInput(name=HG001, input_file_spec=/data/output/training_data/customized_training/training_set_with_label_shuffled/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz, num_examples=33, mode=train with model inception_v3 and tpu False. I0415 07:34:19.550825 140368878327552 model_train.py:196] Batches per epoch 1. I0415 07:34:19.551630 140368878327552 modeling.py:330] Initializing model from checkpoint at /home/models/model.ckpt. I0415 07:34:19.564393 140368878327552 modeling.py:336] The model checkpoint to warm start from has the same number of classes. If this is in training, we will clear excluded_scopes_for_incompatible_shapes so we include everything for warm starting.... I0415 07:34:19.568434 140368878327552 estimator.py:201] Using config: {'_save_checkpoints_secs': 3000, '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7faa056a5210>, '_model_dir': '/data/output/trained_model', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_ev",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:7598,energy efficiency,estimat,estimator,7598,"nsorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version. Instructions for updating:. Use eager execution and: . `tf.data.TFRecordDataset(path)`. I0415 07:34:19.549700 140368878327552 model_train.py:193] Running training on DeepVariantInput(name=HG001, input_file_spec=/data/output/training_data/customized_training/training_set_with_label_shuffled/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz, num_examples=33, mode=train with model inception_v3 and tpu False. I0415 07:34:19.550825 140368878327552 model_train.py:196] Batches per epoch 1. I0415 07:34:19.551630 140368878327552 modeling.py:330] Initializing model from checkpoint at /home/models/model.ckpt. I0415 07:34:19.564393 140368878327552 modeling.py:336] The model checkpoint to warm start from has the same number of classes. If this is in training, we will clear excluded_scopes_for_incompatible_shapes so we include everything for warm starting.... I0415 07:34:19.568434 140368878327552 estimator.py:201] Using config: {'_save_checkpoints_secs': 3000, '_session_config': allow_soft_placement: true. graph_options {. rewrite_options {. meta_optimizer_iterations: ONE. }. }. , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7faa056a5210>, '_model_dir': '/data/output/trained_model', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_cluster': 0, '_master': ''}. W0415 07:34:19.583559 140368878327552 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:9178,energy efficiency,estimat,estimator,9178,"e, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_cluster': 0, '_master': ''}. W0415 07:34:19.583559 140368878327552 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version. Instructions for updating:. Colocations handled automatically by placer. W0415 07:34:19.617336 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/deepvariant/data_providers.py:286: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.experimental.parallel_interleave(...)`. I0415 07:34:19.674496 140368878327552 estimator.py:1111] Calling model_fn. W0415 07:34:19.675647 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/deepvariant/modeling.py:669: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use tf.cast instead. W0415 07:34:19.682488 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/deepvariant/modeling.py:671: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. W0415 07:34:23.556665 140368878327552 deprecation.py:506] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version. Instructions for updating:. Please use `rate` instead of `keep_prob`. R",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:9349,energy efficiency,model,modeling,9349,"master': '', '_eval_distribute': None, '_global_id_in_cluster': 0, '_master': ''}. W0415 07:34:19.583559 140368878327552 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version. Instructions for updating:. Colocations handled automatically by placer. W0415 07:34:19.617336 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/deepvariant/data_providers.py:286: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.experimental.parallel_interleave(...)`. I0415 07:34:19.674496 140368878327552 estimator.py:1111] Calling model_fn. W0415 07:34:19.675647 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/deepvariant/modeling.py:669: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use tf.cast instead. W0415 07:34:19.682488 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/deepvariant/modeling.py:671: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. W0415 07:34:23.556665 140368878327552 deprecation.py:506] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version. Instructions for updating:. Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`. I0415 07:34:37.681962 140368878327552 estimator.py:1113] Done calling model_fn. I0415 07:34:37.683280 140368878327552 estimat",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:9651,energy efficiency,model,modeling,9651," and will be removed in a future version. Instructions for updating:. Colocations handled automatically by placer. W0415 07:34:19.617336 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/deepvariant/data_providers.py:286: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.experimental.parallel_interleave(...)`. I0415 07:34:19.674496 140368878327552 estimator.py:1111] Calling model_fn. W0415 07:34:19.675647 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/deepvariant/modeling.py:669: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use tf.cast instead. W0415 07:34:19.682488 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/deepvariant/modeling.py:671: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. W0415 07:34:23.556665 140368878327552 deprecation.py:506] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version. Instructions for updating:. Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`. I0415 07:34:37.681962 140368878327552 estimator.py:1113] Done calling model_fn. I0415 07:34:37.683280 140368878327552 estimator.py:1294] Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from='/home/models/model.ckpt', vars_to_warm_start='InceptionV3/Mixed_6d/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_6d/Branch_1/Conv2d_0a_1x1/weights/ExponentialMovin",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:9977,energy efficiency,core,core,9977,"thon.ops.interleave_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use `tf.data.experimental.parallel_interleave(...)`. I0415 07:34:19.674496 140368878327552 estimator.py:1111] Calling model_fn. W0415 07:34:19.675647 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/deepvariant/modeling.py:669: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use tf.cast instead. W0415 07:34:19.682488 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/deepvariant/modeling.py:671: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. W0415 07:34:23.556665 140368878327552 deprecation.py:506] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version. Instructions for updating:. Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`. I0415 07:34:37.681962 140368878327552 estimator.py:1113] Done calling model_fn. I0415 07:34:37.683280 140368878327552 estimator.py:1294] Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from='/home/models/model.ckpt', vars_to_warm_start='InceptionV3/Mixed_6d/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_6d/Branch_1/Conv2d_0a_1x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_6d/Branch_1/Conv2d_0c_7x1/BatchNorm/beta/RMSProp|InceptionV3/Mixed_5d/Branch_2/Conv2d_0a_1x1/weights|InceptionV3/Mixed_7b/Branch_2/Conv2d_0b_3x3/weights|InceptionV3/Mixed_5b/Branch_1/Conv2d_0b_5x5/BatchNorm/beta/RMSProp|InceptionV3/Mixed_6a/Branch_0/Conv2d_1a_1x1/weights/RMSProp|InceptionV3/Mixed",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:10266,energy efficiency,estimat,estimator,10266,"] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/deepvariant/modeling.py:669: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use tf.cast instead. W0415 07:34:19.682488 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/deepvariant/modeling.py:671: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. W0415 07:34:23.556665 140368878327552 deprecation.py:506] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version. Instructions for updating:. Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`. I0415 07:34:37.681962 140368878327552 estimator.py:1113] Done calling model_fn. I0415 07:34:37.683280 140368878327552 estimator.py:1294] Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from='/home/models/model.ckpt', vars_to_warm_start='InceptionV3/Mixed_6d/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_6d/Branch_1/Conv2d_0a_1x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_6d/Branch_1/Conv2d_0c_7x1/BatchNorm/beta/RMSProp|InceptionV3/Mixed_5d/Branch_2/Conv2d_0a_1x1/weights|InceptionV3/Mixed_7b/Branch_2/Conv2d_0b_3x3/weights|InceptionV3/Mixed_5b/Branch_1/Conv2d_0b_5x5/BatchNorm/beta/RMSProp|InceptionV3/Mixed_6a/Branch_0/Conv2d_1a_1x1/weights/RMSProp|InceptionV3/Mixed_6a/Branch_1/Conv2d_0b_3x3/weights/ExponentialMovingAverage|InceptionV3/Mixed_6e/Branch_2/Conv2d_0a_1x1/weights/RMSProp_1|InceptionV3/Mixed_6d/Branch_0/Conv2d_0a_1x1/weights/RMSProp|InceptionV3/Mixed_6d/Branch_2/Conv2d_0d_7x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_5d/Branch_2/Con",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:10346,energy efficiency,estimat,estimator,10346,"deling.py:669: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Use tf.cast instead. W0415 07:34:19.682488 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/deepvariant/modeling.py:671: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. W0415 07:34:23.556665 140368878327552 deprecation.py:506] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version. Instructions for updating:. Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`. I0415 07:34:37.681962 140368878327552 estimator.py:1113] Done calling model_fn. I0415 07:34:37.683280 140368878327552 estimator.py:1294] Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from='/home/models/model.ckpt', vars_to_warm_start='InceptionV3/Mixed_6d/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_6d/Branch_1/Conv2d_0a_1x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_6d/Branch_1/Conv2d_0c_7x1/BatchNorm/beta/RMSProp|InceptionV3/Mixed_5d/Branch_2/Conv2d_0a_1x1/weights|InceptionV3/Mixed_7b/Branch_2/Conv2d_0b_3x3/weights|InceptionV3/Mixed_5b/Branch_1/Conv2d_0b_5x5/BatchNorm/beta/RMSProp|InceptionV3/Mixed_6a/Branch_0/Conv2d_1a_1x1/weights/RMSProp|InceptionV3/Mixed_6a/Branch_1/Conv2d_0b_3x3/weights/ExponentialMovingAverage|InceptionV3/Mixed_6e/Branch_2/Conv2d_0a_1x1/weights/RMSProp_1|InceptionV3/Mixed_6d/Branch_0/Conv2d_0a_1x1/weights/RMSProp|InceptionV3/Mixed_6d/Branch_2/Conv2d_0d_7x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_5d/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:10452,energy efficiency,model,models,10452,"ure version. Instructions for updating:. Use tf.cast instead. W0415 07:34:19.682488 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/deepvariant/modeling.py:671: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. W0415 07:34:23.556665 140368878327552 deprecation.py:506] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version. Instructions for updating:. Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`. I0415 07:34:37.681962 140368878327552 estimator.py:1113] Done calling model_fn. I0415 07:34:37.683280 140368878327552 estimator.py:1294] Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from='/home/models/model.ckpt', vars_to_warm_start='InceptionV3/Mixed_6d/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_6d/Branch_1/Conv2d_0a_1x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_6d/Branch_1/Conv2d_0c_7x1/BatchNorm/beta/RMSProp|InceptionV3/Mixed_5d/Branch_2/Conv2d_0a_1x1/weights|InceptionV3/Mixed_7b/Branch_2/Conv2d_0b_3x3/weights|InceptionV3/Mixed_5b/Branch_1/Conv2d_0b_5x5/BatchNorm/beta/RMSProp|InceptionV3/Mixed_6a/Branch_0/Conv2d_1a_1x1/weights/RMSProp|InceptionV3/Mixed_6a/Branch_1/Conv2d_0b_3x3/weights/ExponentialMovingAverage|InceptionV3/Mixed_6e/Branch_2/Conv2d_0a_1x1/weights/RMSProp_1|InceptionV3/Mixed_6d/Branch_0/Conv2d_0a_1x1/weights/RMSProp|InceptionV3/Mixed_6d/Branch_2/Conv2d_0d_7x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_5d/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_5b/Branch_1/Conv2d_0b_5x5/BatchNorm/beta|InceptionV3/Mixed_6e/Branch_1/Conv2d_0a_1x1/weights/RMSProp_1|I",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:10459,energy efficiency,model,model,10459,"sion. Instructions for updating:. Use tf.cast instead. W0415 07:34:19.682488 140368878327552 deprecation.py:323] From /tmp/Bazel.runfiles_9ZA81B/runfiles/com_google_deepvariant/deepvariant/modeling.py:671: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating:. Deprecated in favor of operator or tf.math.divide. W0415 07:34:23.556665 140368878327552 deprecation.py:506] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version. Instructions for updating:. Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`. I0415 07:34:37.681962 140368878327552 estimator.py:1113] Done calling model_fn. I0415 07:34:37.683280 140368878327552 estimator.py:1294] Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from='/home/models/model.ckpt', vars_to_warm_start='InceptionV3/Mixed_6d/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_6d/Branch_1/Conv2d_0a_1x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_6d/Branch_1/Conv2d_0c_7x1/BatchNorm/beta/RMSProp|InceptionV3/Mixed_5d/Branch_2/Conv2d_0a_1x1/weights|InceptionV3/Mixed_7b/Branch_2/Conv2d_0b_3x3/weights|InceptionV3/Mixed_5b/Branch_1/Conv2d_0b_5x5/BatchNorm/beta/RMSProp|InceptionV3/Mixed_6a/Branch_0/Conv2d_1a_1x1/weights/RMSProp|InceptionV3/Mixed_6a/Branch_1/Conv2d_0b_3x3/weights/ExponentialMovingAverage|InceptionV3/Mixed_6e/Branch_2/Conv2d_0a_1x1/weights/RMSProp_1|InceptionV3/Mixed_6d/Branch_0/Conv2d_0a_1x1/weights/RMSProp|InceptionV3/Mixed_6d/Branch_2/Conv2d_0d_7x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_5d/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_5b/Branch_1/Conv2d_0b_5x5/BatchNorm/beta|InceptionV3/Mixed_6e/Branch_1/Conv2d_0a_1x1/weights/RMSProp_1|Inceptio",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:90075,energy efficiency,model,models,90075,"onv2d_0b_1x1/BatchNorm/moving_mean|InceptionV3/Mixed_7c/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_variance|InceptionV3/Mixed_6b/Branch_2/Conv2d_0b_7x1/BatchNorm/moving_mean|InceptionV3/Mixed_5d/Branch_1/Conv2d_0a_1x1/weights|InceptionV3/Mixed_6c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_5b/Branch_3/Conv2d_0b_1x1/weights|InceptionV3/Mixed_5d/Branch_2/Conv2d_0a_1x1/weights/RMSProp_1|InceptionV3/Mixed_6b/Branch_3/Conv2d_0b_1x1/weights/ExponentialMovingAverage|InceptionV3/Conv2d_3b_1x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_7c/Branch_2/Conv2d_0b_3x3/weights/ExponentialMovingAverage|InceptionV3/Conv2d_4a_3x3/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_7a/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_variance|InceptionV3/Mixed_5c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/RMSProp', var_name_to_vocab_info={}, var_name_to_prev_var_name={}). I0415 07:34:37.687702 140368878327552 warm_starting_util.py:417] Warm-starting from: ('/home/models/model.ckpt',). I0415 07:34:37.964245 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0c_7x1/weights; prev_var_name: Unchanged. I0415 07:34:37.965248 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_3b_1x1/weights; prev_var_name: Unchanged. I0415 07:34:37.966092 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged. I0415 07:34:37.966933 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged. I0415 07:34:37.967533 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_0/Conv2d_1a_1x1/weights; prev_var_name: Unchanged. I0415 07:34:37.968360 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Un",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:90082,energy efficiency,model,model,90082,"b_1x1/BatchNorm/moving_mean|InceptionV3/Mixed_7c/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_variance|InceptionV3/Mixed_6b/Branch_2/Conv2d_0b_7x1/BatchNorm/moving_mean|InceptionV3/Mixed_5d/Branch_1/Conv2d_0a_1x1/weights|InceptionV3/Mixed_6c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_5b/Branch_3/Conv2d_0b_1x1/weights|InceptionV3/Mixed_5d/Branch_2/Conv2d_0a_1x1/weights/RMSProp_1|InceptionV3/Mixed_6b/Branch_3/Conv2d_0b_1x1/weights/ExponentialMovingAverage|InceptionV3/Conv2d_3b_1x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_7c/Branch_2/Conv2d_0b_3x3/weights/ExponentialMovingAverage|InceptionV3/Conv2d_4a_3x3/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_7a/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_variance|InceptionV3/Mixed_5c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/RMSProp', var_name_to_vocab_info={}, var_name_to_prev_var_name={}). I0415 07:34:37.687702 140368878327552 warm_starting_util.py:417] Warm-starting from: ('/home/models/model.ckpt',). I0415 07:34:37.964245 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0c_7x1/weights; prev_var_name: Unchanged. I0415 07:34:37.965248 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Conv2d_3b_1x1/weights; prev_var_name: Unchanged. I0415 07:34:37.966092 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged. I0415 07:34:37.966933 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0b_3x3/weights; prev_var_name: Unchanged. I0415 07:34:37.967533 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_0/Conv2d_1a_1x1/weights; prev_var_name: Unchanged. I0415 07:34:37.968360 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_3/Conv2d_0b_1x1/weights; prev_var_name: Unchanged",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:122696,energy efficiency,core,core,122696,"Mixed_7a/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged. I0415 07:34:38.065984 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged. I0415 07:34:38.066359 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged. I0415 07:34:38.066709 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged. I0415 07:34:38.067050 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/BatchNorm/beta; prev_var_name: Unchanged. I0415 07:34:38.657821 140368878327552 basic_session_run_hooks.py:527] Create CheckpointSaverHook. I0415 07:34:45.316857 140368878327552 monitored_session.py:222] Graph was finalized. 2019-04-15 07:34:45.317978: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2019-04-15 07:34:45.322541: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2304000000 Hz. 2019-04-15 07:34:45.323247: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x175ebd50 executing computations on platform Host. Devices:. 2019-04-15 07:34:45.323718: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. I0415 07:34:52.317267 140368878327552 session_manager.py:491] Running local_init_op. I0415 07:34:52.780421 140368878327552 session_manager.py:493] Done running local_init_op. I0415 07:35:11.098021 140368878327552 basic_session_run_hooks.py:594] Saving checkpoints for 0 into /data/output/trained_model/model.ckpt. 2019-04-15 07:35:25.684776: W tensorflow/core/framework/allocator.cc:124] Allocation of 16972800 exceeds 10% of system memory. 2019-04-15 07:35:25.703962: ",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:122741,energy efficiency,CPU,CPU,122741,"a; prev_var_name: Unchanged. I0415 07:34:38.065984 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged. I0415 07:34:38.066359 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged. I0415 07:34:38.066709 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged. I0415 07:34:38.067050 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/BatchNorm/beta; prev_var_name: Unchanged. I0415 07:34:38.657821 140368878327552 basic_session_run_hooks.py:527] Create CheckpointSaverHook. I0415 07:34:45.316857 140368878327552 monitored_session.py:222] Graph was finalized. 2019-04-15 07:34:45.317978: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2019-04-15 07:34:45.322541: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2304000000 Hz. 2019-04-15 07:34:45.323247: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x175ebd50 executing computations on platform Host. Devices:. 2019-04-15 07:34:45.323718: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. I0415 07:34:52.317267 140368878327552 session_manager.py:491] Running local_init_op. I0415 07:34:52.780421 140368878327552 session_manager.py:493] Done running local_init_op. I0415 07:35:11.098021 140368878327552 basic_session_run_hooks.py:594] Saving checkpoints for 0 into /data/output/trained_model/model.ckpt. 2019-04-15 07:35:25.684776: W tensorflow/core/framework/allocator.cc:124] Allocation of 16972800 exceeds 10% of system memory. 2019-04-15 07:35:25.703962: W tensorflow/core/framework/allocator.cc:124]",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:122871,energy efficiency,core,core,122871,"Mixed_7b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged. I0415 07:34:38.066359 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged. I0415 07:34:38.066709 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged. I0415 07:34:38.067050 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/BatchNorm/beta; prev_var_name: Unchanged. I0415 07:34:38.657821 140368878327552 basic_session_run_hooks.py:527] Create CheckpointSaverHook. I0415 07:34:45.316857 140368878327552 monitored_session.py:222] Graph was finalized. 2019-04-15 07:34:45.317978: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2019-04-15 07:34:45.322541: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2304000000 Hz. 2019-04-15 07:34:45.323247: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x175ebd50 executing computations on platform Host. Devices:. 2019-04-15 07:34:45.323718: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. I0415 07:34:52.317267 140368878327552 session_manager.py:491] Running local_init_op. I0415 07:34:52.780421 140368878327552 session_manager.py:493] Done running local_init_op. I0415 07:35:11.098021 140368878327552 basic_session_run_hooks.py:594] Saving checkpoints for 0 into /data/output/trained_model/model.ckpt. 2019-04-15 07:35:25.684776: W tensorflow/core/framework/allocator.cc:124] Allocation of 16972800 exceeds 10% of system memory. 2019-04-15 07:35:25.703962: W tensorflow/core/framework/allocator.cc:124] Allocation of 22077440 exceeds 10% of system memory. 2019-04-15 07:35:25.779693: W tensorflow/core/framework/allocator.cc:124] Al",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:122916,energy efficiency,CPU,CPU,122916,"a; prev_var_name: Unchanged. I0415 07:34:38.066359 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged. I0415 07:34:38.066709 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged. I0415 07:34:38.067050 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/BatchNorm/beta; prev_var_name: Unchanged. I0415 07:34:38.657821 140368878327552 basic_session_run_hooks.py:527] Create CheckpointSaverHook. I0415 07:34:45.316857 140368878327552 monitored_session.py:222] Graph was finalized. 2019-04-15 07:34:45.317978: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2019-04-15 07:34:45.322541: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2304000000 Hz. 2019-04-15 07:34:45.323247: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x175ebd50 executing computations on platform Host. Devices:. 2019-04-15 07:34:45.323718: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. I0415 07:34:52.317267 140368878327552 session_manager.py:491] Running local_init_op. I0415 07:34:52.780421 140368878327552 session_manager.py:493] Done running local_init_op. I0415 07:35:11.098021 140368878327552 basic_session_run_hooks.py:594] Saving checkpoints for 0 into /data/output/trained_model/model.ckpt. 2019-04-15 07:35:25.684776: W tensorflow/core/framework/allocator.cc:124] Allocation of 16972800 exceeds 10% of system memory. 2019-04-15 07:35:25.703962: W tensorflow/core/framework/allocator.cc:124] Allocation of 22077440 exceeds 10% of system memory. 2019-04-15 07:35:25.779693: W tensorflow/core/framework/allocator.cc:124] Allocation of 22077440 exceeds 10% of system me",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:122920,energy efficiency,Frequenc,Frequency,122920,"_var_name: Unchanged. I0415 07:34:38.066359 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5d/Branch_3/Conv2d_0b_1x1/BatchNorm/beta; prev_var_name: Unchanged. I0415 07:34:38.066709 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/BatchNorm/beta; prev_var_name: Unchanged. I0415 07:34:38.067050 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/BatchNorm/beta; prev_var_name: Unchanged. I0415 07:34:38.657821 140368878327552 basic_session_run_hooks.py:527] Create CheckpointSaverHook. I0415 07:34:45.316857 140368878327552 monitored_session.py:222] Graph was finalized. 2019-04-15 07:34:45.317978: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2019-04-15 07:34:45.322541: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2304000000 Hz. 2019-04-15 07:34:45.323247: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x175ebd50 executing computations on platform Host. Devices:. 2019-04-15 07:34:45.323718: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. I0415 07:34:52.317267 140368878327552 session_manager.py:491] Running local_init_op. I0415 07:34:52.780421 140368878327552 session_manager.py:493] Done running local_init_op. I0415 07:35:11.098021 140368878327552 basic_session_run_hooks.py:594] Saving checkpoints for 0 into /data/output/trained_model/model.ckpt. 2019-04-15 07:35:25.684776: W tensorflow/core/framework/allocator.cc:124] Allocation of 16972800 exceeds 10% of system memory. 2019-04-15 07:35:25.703962: W tensorflow/core/framework/allocator.cc:124] Allocation of 22077440 exceeds 10% of system memory. 2019-04-15 07:35:25.779693: W tensorflow/core/framework/allocator.cc:124] Allocation of 22077440 exceeds 10% of system memory. 2",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:123531,energy efficiency,model,model,123531,"py:527] Create CheckpointSaverHook. I0415 07:34:45.316857 140368878327552 monitored_session.py:222] Graph was finalized. 2019-04-15 07:34:45.317978: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2019-04-15 07:34:45.322541: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2304000000 Hz. 2019-04-15 07:34:45.323247: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x175ebd50 executing computations on platform Host. Devices:. 2019-04-15 07:34:45.323718: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. I0415 07:34:52.317267 140368878327552 session_manager.py:491] Running local_init_op. I0415 07:34:52.780421 140368878327552 session_manager.py:493] Done running local_init_op. I0415 07:35:11.098021 140368878327552 basic_session_run_hooks.py:594] Saving checkpoints for 0 into /data/output/trained_model/model.ckpt. 2019-04-15 07:35:25.684776: W tensorflow/core/framework/allocator.cc:124] Allocation of 16972800 exceeds 10% of system memory. 2019-04-15 07:35:25.703962: W tensorflow/core/framework/allocator.cc:124] Allocation of 22077440 exceeds 10% of system memory. 2019-04-15 07:35:25.779693: W tensorflow/core/framework/allocator.cc:124] Allocation of 22077440 exceeds 10% of system memory. 2019-04-15 07:35:25.836485: W tensorflow/core/framework/allocator.cc:124] Allocation of 20791296 exceeds 10% of system memory. 2019-04-15 07:35:26.261185: W tensorflow/core/framework/allocator.cc:124] Allocation of 20791296 exceeds 10% of system memory. I0415 07:35:36.190104 140368878327552 basic_session_run_hooks.py:249] loss = 0.039415985, step = 1. I0415 07:36:50.684401 140368878327552 basic_session_run_hooks.py:594] Saving checkpoints for 10 into /data/output/trained_model/model.ckpt. I0415 07:37:20.374263 140368878327552 estimator.py:359] Loss for final step: 0.0037548377. WARNING: The TensorFlow c",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:123584,energy efficiency,core,core,123584,"16857 140368878327552 monitored_session.py:222] Graph was finalized. 2019-04-15 07:34:45.317978: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2019-04-15 07:34:45.322541: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2304000000 Hz. 2019-04-15 07:34:45.323247: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x175ebd50 executing computations on platform Host. Devices:. 2019-04-15 07:34:45.323718: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. I0415 07:34:52.317267 140368878327552 session_manager.py:491] Running local_init_op. I0415 07:34:52.780421 140368878327552 session_manager.py:493] Done running local_init_op. I0415 07:35:11.098021 140368878327552 basic_session_run_hooks.py:594] Saving checkpoints for 0 into /data/output/trained_model/model.ckpt. 2019-04-15 07:35:25.684776: W tensorflow/core/framework/allocator.cc:124] Allocation of 16972800 exceeds 10% of system memory. 2019-04-15 07:35:25.703962: W tensorflow/core/framework/allocator.cc:124] Allocation of 22077440 exceeds 10% of system memory. 2019-04-15 07:35:25.779693: W tensorflow/core/framework/allocator.cc:124] Allocation of 22077440 exceeds 10% of system memory. 2019-04-15 07:35:25.836485: W tensorflow/core/framework/allocator.cc:124] Allocation of 20791296 exceeds 10% of system memory. 2019-04-15 07:35:26.261185: W tensorflow/core/framework/allocator.cc:124] Allocation of 20791296 exceeds 10% of system memory. I0415 07:35:36.190104 140368878327552 basic_session_run_hooks.py:249] loss = 0.039415985, step = 1. I0415 07:36:50.684401 140368878327552 basic_session_run_hooks.py:594] Saving checkpoints for 10 into /data/output/trained_model/model.ckpt. I0415 07:37:20.374263 140368878327552 estimator.py:359] Loss for final step: 0.0037548377. WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:123599,energy efficiency,alloc,allocator,123599,"552 monitored_session.py:222] Graph was finalized. 2019-04-15 07:34:45.317978: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2019-04-15 07:34:45.322541: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2304000000 Hz. 2019-04-15 07:34:45.323247: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x175ebd50 executing computations on platform Host. Devices:. 2019-04-15 07:34:45.323718: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. I0415 07:34:52.317267 140368878327552 session_manager.py:491] Running local_init_op. I0415 07:34:52.780421 140368878327552 session_manager.py:493] Done running local_init_op. I0415 07:35:11.098021 140368878327552 basic_session_run_hooks.py:594] Saving checkpoints for 0 into /data/output/trained_model/model.ckpt. 2019-04-15 07:35:25.684776: W tensorflow/core/framework/allocator.cc:124] Allocation of 16972800 exceeds 10% of system memory. 2019-04-15 07:35:25.703962: W tensorflow/core/framework/allocator.cc:124] Allocation of 22077440 exceeds 10% of system memory. 2019-04-15 07:35:25.779693: W tensorflow/core/framework/allocator.cc:124] Allocation of 22077440 exceeds 10% of system memory. 2019-04-15 07:35:25.836485: W tensorflow/core/framework/allocator.cc:124] Allocation of 20791296 exceeds 10% of system memory. 2019-04-15 07:35:26.261185: W tensorflow/core/framework/allocator.cc:124] Allocation of 20791296 exceeds 10% of system memory. I0415 07:35:36.190104 140368878327552 basic_session_run_hooks.py:249] loss = 0.039415985, step = 1. I0415 07:36:50.684401 140368878327552 basic_session_run_hooks.py:594] Saving checkpoints for 10 into /data/output/trained_model/model.ckpt. I0415 07:37:20.374263 140368878327552 estimator.py:359] Loss for final step: 0.0037548377. WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0. For more informa",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:123617,energy efficiency,Alloc,Allocation,123617,"ion.py:222] Graph was finalized. 2019-04-15 07:34:45.317978: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2019-04-15 07:34:45.322541: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2304000000 Hz. 2019-04-15 07:34:45.323247: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x175ebd50 executing computations on platform Host. Devices:. 2019-04-15 07:34:45.323718: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. I0415 07:34:52.317267 140368878327552 session_manager.py:491] Running local_init_op. I0415 07:34:52.780421 140368878327552 session_manager.py:493] Done running local_init_op. I0415 07:35:11.098021 140368878327552 basic_session_run_hooks.py:594] Saving checkpoints for 0 into /data/output/trained_model/model.ckpt. 2019-04-15 07:35:25.684776: W tensorflow/core/framework/allocator.cc:124] Allocation of 16972800 exceeds 10% of system memory. 2019-04-15 07:35:25.703962: W tensorflow/core/framework/allocator.cc:124] Allocation of 22077440 exceeds 10% of system memory. 2019-04-15 07:35:25.779693: W tensorflow/core/framework/allocator.cc:124] Allocation of 22077440 exceeds 10% of system memory. 2019-04-15 07:35:25.836485: W tensorflow/core/framework/allocator.cc:124] Allocation of 20791296 exceeds 10% of system memory. 2019-04-15 07:35:26.261185: W tensorflow/core/framework/allocator.cc:124] Allocation of 20791296 exceeds 10% of system memory. I0415 07:35:36.190104 140368878327552 basic_session_run_hooks.py:249] loss = 0.039415985, step = 1. I0415 07:36:50.684401 140368878327552 basic_session_run_hooks.py:594] Saving checkpoints for 10 into /data/output/trained_model/model.ckpt. I0415 07:37:20.374263 140368878327552 estimator.py:359] Loss for final step: 0.0037548377. WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0. For more information, please see:.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:123711,energy efficiency,core,core,123711,"_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2019-04-15 07:34:45.322541: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2304000000 Hz. 2019-04-15 07:34:45.323247: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x175ebd50 executing computations on platform Host. Devices:. 2019-04-15 07:34:45.323718: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. I0415 07:34:52.317267 140368878327552 session_manager.py:491] Running local_init_op. I0415 07:34:52.780421 140368878327552 session_manager.py:493] Done running local_init_op. I0415 07:35:11.098021 140368878327552 basic_session_run_hooks.py:594] Saving checkpoints for 0 into /data/output/trained_model/model.ckpt. 2019-04-15 07:35:25.684776: W tensorflow/core/framework/allocator.cc:124] Allocation of 16972800 exceeds 10% of system memory. 2019-04-15 07:35:25.703962: W tensorflow/core/framework/allocator.cc:124] Allocation of 22077440 exceeds 10% of system memory. 2019-04-15 07:35:25.779693: W tensorflow/core/framework/allocator.cc:124] Allocation of 22077440 exceeds 10% of system memory. 2019-04-15 07:35:25.836485: W tensorflow/core/framework/allocator.cc:124] Allocation of 20791296 exceeds 10% of system memory. 2019-04-15 07:35:26.261185: W tensorflow/core/framework/allocator.cc:124] Allocation of 20791296 exceeds 10% of system memory. I0415 07:35:36.190104 140368878327552 basic_session_run_hooks.py:249] loss = 0.039415985, step = 1. I0415 07:36:50.684401 140368878327552 basic_session_run_hooks.py:594] Saving checkpoints for 10 into /data/output/trained_model/model.ckpt. I0415 07:37:20.374263 140368878327552 estimator.py:359] Loss for final step: 0.0037548377. WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0. For more information, please see:. * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md. * h",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:123726,energy efficiency,alloc,allocator,123726,"141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2019-04-15 07:34:45.322541: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2304000000 Hz. 2019-04-15 07:34:45.323247: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x175ebd50 executing computations on platform Host. Devices:. 2019-04-15 07:34:45.323718: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. I0415 07:34:52.317267 140368878327552 session_manager.py:491] Running local_init_op. I0415 07:34:52.780421 140368878327552 session_manager.py:493] Done running local_init_op. I0415 07:35:11.098021 140368878327552 basic_session_run_hooks.py:594] Saving checkpoints for 0 into /data/output/trained_model/model.ckpt. 2019-04-15 07:35:25.684776: W tensorflow/core/framework/allocator.cc:124] Allocation of 16972800 exceeds 10% of system memory. 2019-04-15 07:35:25.703962: W tensorflow/core/framework/allocator.cc:124] Allocation of 22077440 exceeds 10% of system memory. 2019-04-15 07:35:25.779693: W tensorflow/core/framework/allocator.cc:124] Allocation of 22077440 exceeds 10% of system memory. 2019-04-15 07:35:25.836485: W tensorflow/core/framework/allocator.cc:124] Allocation of 20791296 exceeds 10% of system memory. 2019-04-15 07:35:26.261185: W tensorflow/core/framework/allocator.cc:124] Allocation of 20791296 exceeds 10% of system memory. I0415 07:35:36.190104 140368878327552 basic_session_run_hooks.py:249] loss = 0.039415985, step = 1. I0415 07:36:50.684401 140368878327552 basic_session_run_hooks.py:594] Saving checkpoints for 10 into /data/output/trained_model/model.ckpt. I0415 07:37:20.374263 140368878327552 estimator.py:359] Loss for final step: 0.0037548377. WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0. For more information, please see:. * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md. * https://github.com/",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
https://github.com/google/deepvariant/issues/172:123744,energy efficiency,Alloc,Allocation,123744,"orts instructions that this TensorFlow binary was not compiled to use: AVX2 FMA. 2019-04-15 07:34:45.322541: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2304000000 Hz. 2019-04-15 07:34:45.323247: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x175ebd50 executing computations on platform Host. Devices:. 2019-04-15 07:34:45.323718: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>. I0415 07:34:52.317267 140368878327552 session_manager.py:491] Running local_init_op. I0415 07:34:52.780421 140368878327552 session_manager.py:493] Done running local_init_op. I0415 07:35:11.098021 140368878327552 basic_session_run_hooks.py:594] Saving checkpoints for 0 into /data/output/trained_model/model.ckpt. 2019-04-15 07:35:25.684776: W tensorflow/core/framework/allocator.cc:124] Allocation of 16972800 exceeds 10% of system memory. 2019-04-15 07:35:25.703962: W tensorflow/core/framework/allocator.cc:124] Allocation of 22077440 exceeds 10% of system memory. 2019-04-15 07:35:25.779693: W tensorflow/core/framework/allocator.cc:124] Allocation of 22077440 exceeds 10% of system memory. 2019-04-15 07:35:25.836485: W tensorflow/core/framework/allocator.cc:124] Allocation of 20791296 exceeds 10% of system memory. 2019-04-15 07:35:26.261185: W tensorflow/core/framework/allocator.cc:124] Allocation of 20791296 exceeds 10% of system memory. I0415 07:35:36.190104 140368878327552 basic_session_run_hooks.py:249] loss = 0.039415985, step = 1. I0415 07:36:50.684401 140368878327552 basic_session_run_hooks.py:594] Saving checkpoints for 10 into /data/output/trained_model/model.ckpt. I0415 07:37:20.374263 140368878327552 estimator.py:359] Loss for final step: 0.0037548377. WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0. For more information, please see:. * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md. * https://github.com/tensorflow/addons.",MatchSource.ISSUE,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/issues/172
